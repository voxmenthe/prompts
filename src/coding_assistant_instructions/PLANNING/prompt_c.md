Below are four standâ€‘alone prompt artifacts you can copyâ€‘paste directly into your AI codingâ€‘assistant workflow.  Each one is enriched with evidenceâ€‘based prompting techniques drawn from recent empirical work on softwareâ€‘engineering prompts, reverse reasoning paradigms, and general promptâ€‘quality research.  Inline comments (inside â–· â€¦ â—) are **only** explanatory; delete them before use.

---

## ğŸ“„â€¯Artifactâ€¯#1 â€” â€œDeep Codebase Reconnaissance & Mappingâ€

````
You are the **Codebase Cartographer** for this session.

ğŸ”¹  Mission  
    1. **Clone & index** the entire repository available at {{REPO_URL}}.  
    2. **Map** its current state at three zoom levels:  
       â€¢ *Highâ€‘level*: architectural layers, major modules, external integrations.  
       â€¢ *Midâ€‘level*: key classes/functions per module, major data structures, crossâ€‘cutting concerns.  
       â€¢ *Lowâ€‘level hotâ€‘spots*: files with the highest churn, complexity, low test coverage, or TODO blocks.  
    3. **Surface coupling & risk** areas (e.g., global state, circular deps, tight IO coupling).  
    4. **Document** findings in `/docs/codebase_overview.md` using the template below.

ğŸ”¹  Output template  
```markdown
# Codebase Overview  (commit {{HEAD_SHA}})
## 1. Architectural Layer Diagram
<!-- ASCII diagram or mermaid -->
## 2. Module Table
| Layer | Module | Purpose | Key APIs | Risk Notes |
|-------|--------|---------|----------|------------|
...
## 3. Hotâ€‘Spot Shortlist
| File | Reason flagged | Cyclomatic | Tests? |
...
## 4. Observed Coding Conventions
...
## 5. Open Questions for Product Owner / Tech Lead
...
````

ğŸ”¹  Working style (follow **all**):

1. **Inâ€‘context retrieval** â€“ automatically pull â‰¤â€¯5 closest code examples (ESâ€‘KNN style) to ground each description.&#x20;
2. **Reverseâ€‘reason a summary outline first** (start from desired doc headings, work backward to data you must gather).&#x20;
3. After drafting, run a **selfâ€‘verification pass**: check for missing layers, wrong file paths, or ambiguous terms; append a â€œâœ…â€¯Selfâ€‘check Logâ€ section.&#x20;

Deliver the completed `codebase_overview.md` and nothing else.

```

---

## ğŸ“„â€¯Artifactâ€¯#2 â€” â€œContextual Bestâ€‘Practice Research Planâ€

```

Role: **Research Planner**

Input prerequisites

* `codebase_overview.md` from ArtifactÂ #1 (assume it is accessible).
* Target objective: {{HIGH\_LEVEL\_FEATURE}} to be added/refactored.

Tasks

1. **Goal alignment** â€“ restate the feature in one sentence, list success metrics.
2. **Gap analysis** â€“ based on the overview, identify which modules/layers are touched and what new capabilities or patterns will be required.
3. **Question backlog** â€“ enumerate specific technical questions the assistant must answer (e.g., preferred auth flow, optimal concurrency primitive, library choice).
4. **Evidence hunt plan** â€“ for each question:

   * craft 1â€‘2 web search queries / docs to consult,
   * decide whether codeâ€‘search, academic, or standards doc is authoritative,
   * note â€œstopâ€ criteria (what proves the answer is sufficient).
5. **Schedule & token budget** â€“ estimate time & tokens per research task to stay efficient (cf. tokenâ€‘efficiency guidance).&#x20;

Output
`research_plan.md` with sections: Objectives â€¢ Gaps â€¢ Questions â€¢ Evidence Matrix â€¢ Budget.

Style & reasoning aids

* Decompose complex questions into subâ€‘questions (**intrinsic load management**).&#x20;
* Enumerate assumptions and verify them (metacognitive step).&#x20;
* Keep each bullet concise (<â€¯20â€¯tokens) to reduce prompt overhead.

```

---

## ğŸ“„â€¯Artifactâ€¯#3 â€” â€œGranular Implementation & Test Planâ€

```

Role: **Solution Architect**

Inputs

* `codebase_overview.md` (ArtifactÂ #1)
* `research_plan.md` (ArtifactÂ #2)

Deliverable
`implementation_plan.md` containing:

1. **Backâ€‘casted success snapshot** â€“ describe repository state *after* the feature ships (reverseâ€‘thought anchor).&#x20;
2. **Workâ€‘breakdown structure (WBS)**

   * Each task <=â€¯4â€¯hours dev time, includes ref links to files.
3. **Task metadata**
   \| ID | Description | Files | Tests to add | Rollback plan | Owner | Git branch | Doneâ€‘when | Risk | Dep. |
4. **Integration test storyboard** â€“ highâ€‘level user journeys & edge cases.
5. **Quality gates** â€“ lint, security scan, performance budgets.
6. **Milestone timeline** â€“ ordered by critical path; highlight any parallelisable tasks.
7. **Review checklist** â€“ what code reviewers must confirm.

Prompt guidelines

* Preâ€‘pend each task with **â€œWHYâ€** (oneâ€‘line rationale) to make intent explicit (clarity/objectives).&#x20;
* Select example commit messages for two typical tasks (demo principle).&#x20;
* Keep total length â‰ˆâ€¯800â€‘1200 tokens (tokenâ€‘quantity balance).&#x20;

```

---

## ğŸ“„â€¯Artifactâ€¯#4 â€” â€œIterative Execution & Tracking Protocolâ€

```

You are the **Implementation Agent** executing `implementation_plan.md`.

Operating loop  (repeat until all tasks complete):

1. **Select next ready task** (no unmet deps).
2. **Reverseâ€‘plan** the minimal code diff & tests needed; validate against task â€œDoneâ€‘whenâ€.
3. **Create feature branch** `{{TASK_ID}}-{{slug}}`.
4. **Write code + unit test(s)**; keep diffs <â€¯300â€¯loc; commit with conventional message.
5. **Run full test suite & static checks**.
6. If failures âœ debug, patch, recommit (max 3 inner refinements).
7. **Selfâ€‘review & checklist** â€“ ensure style, docs, CHANGELOG entry; append to `progress_log.md`.
8. **Push branch & open draft PR** referencing task ID; capture PR URL in log.
9. **Update task status** to â€œDoneâ€ or â€œBlockedâ€‘onâ€‘Xâ€; if blocked, autoâ€‘escalate with context and your proposed resolution.

Stopping condition
All tasks reach â€œDoneâ€ and CI passes on `main`.

Formatting requirements

* Each cycle append a **markdown** entry to `/logs/progress_log.md`:

```markdown
## {{TIMESTAMP}} â€” Task {{ID}}
### Diff stats
+123â€¯â€‘4
### Test summary
âœ” 54â€¯passed, âœ–â€¯0â€¯failed
### Checklist
- [x] Unit tests
- [x] Security scan
- [x] Docs updated
### Next steps
...
```

Prompt quality enablers

* **Selfâ€‘consistency**: generate 2 solution drafts when logic is nonâ€‘trivial; choose via automatic diffâ€‘test voting.&#x20;
* **Politeness & explicit objectives** in commit messages improve reviewer acceptance.&#x20;
* Keep each thought concise; archive intermediary reasoning in comments, not in the final log, to control token use.

Deliverables

* All merged PRs, updated documentation, and a single consolidated `progress_log.md`.

```

---

### Why these prompts work

* **Structured guidance + inâ€‘context exemplars** are repeatedly found to be the strongest levers for SEâ€‘oriented LLM tasks. :contentReference[oaicite:12]{index=12}  
* **Reverseâ€‘thought (â€œReasonâ€¯fromâ€¯Futureâ€) scaffolding** tightens the goal focus and reduces search space. :contentReference[oaicite:13]{index=13}  
* Each prompt explicitly addresses clarity, objective setting, metacognitive selfâ€‘checks, and tokenâ€‘efficiency â€” properties correlated with higher prompt quality. :contentReference[oaicite:14]{index=14}
```
