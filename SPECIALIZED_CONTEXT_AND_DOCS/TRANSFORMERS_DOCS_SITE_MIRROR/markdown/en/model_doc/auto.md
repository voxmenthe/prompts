# Auto Classes

In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `from_pretrained()` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.

Instantiating one of [AutoConfig](/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig), [AutoModel](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel), and
[AutoTokenizer](/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) will directly create a class of the relevant architecture. For instance

```python
model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

will create a model that is an instance of [BertModel](/docs/transformers/main/en/model_doc/bert#transformers.BertModel).

There is one class of `AutoModel` for each task.

## Extending the Auto Classes

Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `NewModel`, make sure you have a `NewModelConfig` then you can add those to the auto
classes like this:

```python
from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)
```

You will then be able to use the auto classes like you would usually do!

If your `NewModelConfig` is a subclass of [PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), make sure its
`model_type` attribute is set to the same key you use when registering the config (here `"new-model"`).

Likewise, if your `NewModel` is a subclass of [PreTrainedModel](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel), make sure its
`config_class` attribute is set to the same class you use when registering the model (here
`NewModelConfig`).

## AutoConfig[[transformers.AutoConfig]]

#### transformers.AutoConfig[[transformers.AutoConfig]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1196)

This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoConfig.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1219[{"name": "pretrained_model_name_or_path", "val": ": typing.Union[str, os.PathLike[str]]"}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  Can be either:

  - A string, the *model id* of a pretrained model configuration hosted inside a model repo on
    huggingface.co.
  - A path to a *directory* containing a configuration file saved using the
    [save_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.save_pretrained) method, or the [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) method,
    e.g., `./my_model_directory/`.
  - A path or url to a saved configuration JSON *file*, e.g.,
    `./my_model_directory/configuration.json`.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model configuration should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force the (re-)download the model weights and configuration files and override the
  cached versions if they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **return_unused_kwargs** (`bool`, *optional*, defaults to `False`) --
  If `False`, then this function returns just the final configuration object.

  If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a
  dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
  part of `kwargs` which has not been used to update `config` and is otherwise ignored.
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **kwargs(additional** keyword arguments, *optional*) --
  The values in kwargs of any keys which are configuration attributes will be used to override the loaded
  values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled
  by the `return_unused_kwargs` keyword parameter.0

Instantiate one of the configuration classes of the library from a pretrained model configuration.

The configuration class to instantiate is selected based on the `model_type` property of the config object that
is loaded, or when it's missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:

- **afmoe** -- [AfmoeConfig](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeConfig) (AFMoE model)
- **aimv2** -- [Aimv2Config](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Config) (AIMv2 model)
- **aimv2_vision_model** -- [Aimv2VisionConfig](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionConfig) (Aimv2VisionModel model)
- **albert** -- [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) (ALBERT model)
- **align** -- [AlignConfig](/docs/transformers/main/en/model_doc/align#transformers.AlignConfig) (ALIGN model)
- **altclip** -- [AltCLIPConfig](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig) (AltCLIP model)
- **apertus** -- [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) (Apertus model)
- **arcee** -- [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) (Arcee model)
- **aria** -- [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) (Aria model)
- **aria_text** -- [AriaTextConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextConfig) (AriaText model)
- **audio-spectrogram-transformer** -- [ASTConfig](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig) (Audio Spectrogram Transformer model)
- **audioflamingo3** -- [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) (AudioFlamingo3 model)
- **audioflamingo3_encoder** -- [AudioFlamingo3EncoderConfig](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3EncoderConfig) (AudioFlamingo3Encoder model)
- **autoformer** -- [AutoformerConfig](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerConfig) (Autoformer model)
- **aya_vision** -- [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) (AyaVision model)
- **bamba** -- [BambaConfig](/docs/transformers/main/en/model_doc/bamba#transformers.BambaConfig) (Bamba model)
- **bark** -- [BarkConfig](/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig) (Bark model)
- **bart** -- [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) (BART model)
- **beit** -- [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) (BEiT model)
- **bert** -- [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) (BERT model)
- **bert-generation** -- [BertGenerationConfig](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig) (Bert Generation model)
- **big_bird** -- [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) (BigBird model)
- **bigbird_pegasus** -- [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) (BigBird-Pegasus model)
- **biogpt** -- [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) (BioGpt model)
- **bit** -- [BitConfig](/docs/transformers/main/en/model_doc/bit#transformers.BitConfig) (BiT model)
- **bitnet** -- [BitNetConfig](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetConfig) (BitNet model)
- **blenderbot** -- [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) (Blenderbot model)
- **blenderbot-small** -- [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) (BlenderbotSmall model)
- **blip** -- [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) (BLIP model)
- **blip-2** -- [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) (BLIP-2 model)
- **blip_2_qformer** -- [Blip2QFormerConfig](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerConfig) (BLIP-2 QFormer model)
- **bloom** -- [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) (BLOOM model)
- **blt** -- [BltConfig](/docs/transformers/main/en/model_doc/blt#transformers.BltConfig) (Blt model)
- **bridgetower** -- [BridgeTowerConfig](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerConfig) (BridgeTower model)
- **bros** -- [BrosConfig](/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig) (BROS model)
- **camembert** -- [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) (CamemBERT model)
- **canine** -- [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) (CANINE model)
- **chameleon** -- [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) (Chameleon model)
- **chinese_clip** -- [ChineseCLIPConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig) (Chinese-CLIP model)
- **chinese_clip_vision_model** -- [ChineseCLIPVisionConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionConfig) (ChineseCLIPVisionModel model)
- **clap** -- [ClapConfig](/docs/transformers/main/en/model_doc/clap#transformers.ClapConfig) (CLAP model)
- **clip** -- [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) (CLIP model)
- **clip_text_model** -- [CLIPTextConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextConfig) (CLIPTextModel model)
- **clip_vision_model** -- [CLIPVisionConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionConfig) (CLIPVisionModel model)
- **clipseg** -- [CLIPSegConfig](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig) (CLIPSeg model)
- **clvp** -- [ClvpConfig](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpConfig) (CLVP model)
- **code_llama** -- [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) (CodeLlama model)
- **codegen** -- [CodeGenConfig](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig) (CodeGen model)
- **cohere** -- [CohereConfig](/docs/transformers/main/en/model_doc/cohere#transformers.CohereConfig) (Cohere model)
- **cohere2** -- [Cohere2Config](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Config) (Cohere2 model)
- **cohere2_vision** -- [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) (Cohere2Vision model)
- **colpali** -- [ColPaliConfig](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliConfig) (ColPali model)
- **colqwen2** -- [ColQwen2Config](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2Config) (ColQwen2 model)
- **conditional_detr** -- [ConditionalDetrConfig](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) (Conditional DETR model)
- **convbert** -- [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) (ConvBERT model)
- **convnext** -- [ConvNextConfig](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig) (ConvNeXT model)
- **convnextv2** -- [ConvNextV2Config](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config) (ConvNeXTV2 model)
- **cpmant** -- [CpmAntConfig](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig) (CPM-Ant model)
- **csm** -- [CsmConfig](/docs/transformers/main/en/model_doc/csm#transformers.CsmConfig) (CSM model)
- **ctrl** -- [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) (CTRL model)
- **cvt** -- [CvtConfig](/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig) (CvT model)
- **cwm** -- [CwmConfig](/docs/transformers/main/en/model_doc/cwm#transformers.CwmConfig) (Code World Model (CWM) model)
- **d_fine** -- [DFineConfig](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineConfig) (D-FINE model)
- **dab-detr** -- [DabDetrConfig](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrConfig) (DAB-DETR model)
- **dac** -- [DacConfig](/docs/transformers/main/en/model_doc/dac#transformers.DacConfig) (DAC model)
- **data2vec-audio** -- [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) (Data2VecAudio model)
- **data2vec-text** -- [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) (Data2VecText model)
- **data2vec-vision** -- [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) (Data2VecVision model)
- **dbrx** -- [DbrxConfig](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxConfig) (DBRX model)
- **deberta** -- [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) (DeBERTa model)
- **deberta-v2** -- [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) (DeBERTa-v2 model)
- **decision_transformer** -- [DecisionTransformerConfig](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig) (Decision Transformer model)
- **deepseek_v2** -- [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) (DeepSeek-V2 model)
- **deepseek_v3** -- [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) (DeepSeek-V3 model)
- **deepseek_vl** -- [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) (DeepseekVLHybrid model)
- **deformable_detr** -- [DeformableDetrConfig](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig) (Deformable DETR model)
- **deit** -- [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) (DeiT model)
- **depth_anything** -- [DepthAnythingConfig](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingConfig) (Depth Anything model)
- **depth_pro** -- [DepthProConfig](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProConfig) (DepthPro model)
- **detr** -- [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) (DETR model)
- **dia** -- [DiaConfig](/docs/transformers/main/en/model_doc/dia#transformers.DiaConfig) (Dia model)
- **diffllama** -- [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) (DiffLlama model)
- **dinat** -- [DinatConfig](/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig) (DiNAT model)
- **dinov2** -- [Dinov2Config](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config) (DINOv2 model)
- **dinov2_with_registers** -- [Dinov2WithRegistersConfig](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig) (DINOv2 with Registers model)
- **dinov3_convnext** -- [DINOv3ConvNextConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextConfig) (DINOv3 ConvNext model)
- **dinov3_vit** -- [DINOv3ViTConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTConfig) (DINOv3 ViT model)
- **distilbert** -- [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) (DistilBERT model)
- **doge** -- [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) (Doge model)
- **donut-swin** -- [DonutSwinConfig](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig) (DonutSwin model)
- **dots1** -- [Dots1Config](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Config) (dots1 model)
- **dpr** -- [DPRConfig](/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig) (DPR model)
- **dpt** -- [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) (DPT model)
- **edgetam** -- [EdgeTamConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamConfig) (EdgeTAM model)
- **edgetam_video** -- [EdgeTamVideoConfig](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoConfig) (EdgeTamVideo model)
- **edgetam_vision_model** -- [EdgeTamVisionConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionConfig) (EdgeTamVisionModel model)
- **efficientloftr** -- [EfficientLoFTRConfig](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig) (EfficientLoFTR model)
- **efficientnet** -- [EfficientNetConfig](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig) (EfficientNet model)
- **electra** -- [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) (ELECTRA model)
- **emu3** -- [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) (Emu3 model)
- **encodec** -- [EncodecConfig](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecConfig) (EnCodec model)
- **encoder-decoder** -- [EncoderDecoderConfig](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig) (Encoder decoder model)
- **eomt** -- [EomtConfig](/docs/transformers/main/en/model_doc/eomt#transformers.EomtConfig) (EoMT model)
- **ernie** -- [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) (ERNIE model)
- **ernie4_5** -- [Ernie4_5Config](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Config) (Ernie4_5 model)
- **ernie4_5_moe** -- [Ernie4_5_MoeConfig](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig) (Ernie4_5_MoE model)
- **esm** -- [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) (ESM model)
- **evolla** -- [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) (Evolla model)
- **exaone4** -- [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) (EXAONE-4.0 model)
- **falcon** -- [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) (Falcon model)
- **falcon_h1** -- [FalconH1Config](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Config) (FalconH1 model)
- **falcon_mamba** -- [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) (FalconMamba model)
- **fast_vlm** -- [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) (FastVlm model)
- **fastspeech2_conformer** -- [FastSpeech2ConformerConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig) (FastSpeech2Conformer model)
- **fastspeech2_conformer_with_hifigan** -- [FastSpeech2ConformerWithHifiGanConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGanConfig) (FastSpeech2ConformerWithHifiGan model)
- **flaubert** -- [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) (FlauBERT model)
- **flava** -- [FlavaConfig](/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig) (FLAVA model)
- **flex_olmo** -- [FlexOlmoConfig](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoConfig) (FlexOlmo model)
- **florence2** -- [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) (Florence2 model)
- **fnet** -- [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) (FNet model)
- **focalnet** -- [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) (FocalNet model)
- **fsmt** -- [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) (FairSeq Machine-Translation model)
- **funnel** -- [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) (Funnel Transformer model)
- **fuyu** -- [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) (Fuyu model)
- **gemma** -- [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) (Gemma model)
- **gemma2** -- [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) (Gemma2 model)
- **gemma3** -- [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) (Gemma3ForConditionalGeneration model)
- **gemma3_text** -- [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) (Gemma3ForCausalLM model)
- **gemma3n** -- [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) (Gemma3nForConditionalGeneration model)
- **gemma3n_audio** -- [Gemma3nAudioConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nAudioConfig) (Gemma3nAudioEncoder model)
- **gemma3n_text** -- [Gemma3nTextConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextConfig) (Gemma3nForCausalLM model)
- **gemma3n_vision** -- [Gemma3nVisionConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nVisionConfig) (TimmWrapperModel model)
- **git** -- [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) (GIT model)
- **glm** -- [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) (GLM model)
- **glm4** -- [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) (GLM4 model)
- **glm46v** -- [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) (Glm46V model)
- **glm4_moe** -- [Glm4MoeConfig](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeConfig) (Glm4MoE model)
- **glm4v** -- [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) (GLM4V model)
- **glm4v_moe** -- [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) (GLM4VMOE model)
- **glm4v_moe_text** -- [Glm4vMoeTextConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextConfig) (GLM4VMOE model)
- **glm4v_moe_vision** -- [Glm4vMoeVisionConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionConfig) (Glm4vMoeVisionModel model)
- **glm4v_text** -- [Glm4vTextConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextConfig) (GLM4V model)
- **glm4v_vision** -- [Glm4vVisionConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionConfig) (Glm4vVisionModel model)
- **glpn** -- [GLPNConfig](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig) (GLPN model)
- **got_ocr2** -- [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) (GOT-OCR2 model)
- **gpt-sw3** -- [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) (GPT-Sw3 model)
- **gpt2** -- [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) (GPTBigCode model)
- **gpt_neo** -- [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) (GPT NeoX model)
- **gpt_neox_japanese** -- [GPTNeoXJapaneseConfig](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig) (GPT NeoX Japanese model)
- **gpt_oss** -- [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) (GptOss model)
- **gptj** -- [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) (GPT-J model)
- **granite** -- [GraniteConfig](/docs/transformers/main/en/model_doc/granite#transformers.GraniteConfig) (Granite model)
- **granite_speech** -- [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) (GraniteSpeech model)
- **granitemoe** -- [GraniteMoeConfig](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeConfig) (GraniteMoeMoe model)
- **granitemoehybrid** -- [GraniteMoeHybridConfig](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig) (GraniteMoeHybrid model)
- **granitemoeshared** -- [GraniteMoeSharedConfig](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig) (GraniteMoeSharedMoe model)
- **granitevision** -- [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) (LLaVA-NeXT model)
- **grounding-dino** -- [GroundingDinoConfig](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoConfig) (Grounding DINO model)
- **groupvit** -- [GroupViTConfig](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig) (GroupViT model)
- **helium** -- [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) (Helium model)
- **hgnet_v2** -- [HGNetV2Config](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Config) (HGNet-V2 model)
- **hiera** -- [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) (Hiera model)
- **hubert** -- [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) (Hubert model)
- **hunyuan_v1_dense** -- [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) (HunYuanDenseV1 model)
- **hunyuan_v1_moe** -- [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) (HunYuanMoeV1 model)
- **ibert** -- [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) (I-BERT model)
- **idefics** -- [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) (IDEFICS model)
- **idefics2** -- [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) (Idefics2 model)
- **idefics3** -- [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) (Idefics3 model)
- **idefics3_vision** -- [Idefics3VisionConfig](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionConfig) (Idefics3VisionTransformer model)
- **ijepa** -- [IJepaConfig](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaConfig) (I-JEPA model)
- **imagegpt** -- [ImageGPTConfig](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig) (ImageGPT model)
- **informer** -- [InformerConfig](/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig) (Informer model)
- **instructblip** -- [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) (InstructBLIP model)
- **instructblipvideo** -- [InstructBlipVideoConfig](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig) (InstructBlipVideo model)
- **internvl** -- [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) (InternVL model)
- **internvl_vision** -- [InternVLVisionConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionConfig) (InternVLVision model)
- **jamba** -- [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) (Jamba model)
- **janus** -- [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) (Janus model)
- **jetmoe** -- [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) (JetMoe model)
- **kosmos-2** -- [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) (KOSMOS-2.5 model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) (KyutaiSpeechToText model)
- **lasr_ctc** -- [LasrCTCConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrCTCConfig) (Lasr model)
- **lasr_encoder** -- [LasrEncoderConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoderConfig) (LasrEncoder model)
- **layoutlm** -- [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) (LayoutLMv3 model)
- **layoutxlm** -- [LayoutXLMConfig](/docs/transformers/main/en/model_doc/layoutxlm#transformers.LayoutXLMConfig) (LayoutXLM model)
- **led** -- [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) (LED model)
- **levit** -- [LevitConfig](/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig) (LeViT model)
- **lfm2** -- [Lfm2Config](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Config) (Lfm2 model)
- **lfm2_moe** -- [Lfm2MoeConfig](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeConfig) (Lfm2Moe model)
- **lfm2_vl** -- [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) (Lfm2Vl model)
- **lightglue** -- [LightGlueConfig](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueConfig) (LightGlue model)
- **lilt** -- [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) (LiLT model)
- **llama** -- [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) (LLaMA model)
- **llama4** -- [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) (Llama4 model)
- **llama4_text** -- [Llama4TextConfig](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextConfig) (Llama4ForCausalLM model)
- **llava** -- [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) (LLaVa model)
- **llava_next** -- [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) (LLaVA-Onevision model)
- **longcat_flash** -- [LongcatFlashConfig](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashConfig) (LongCatFlash model)
- **longformer** -- [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) (Longformer model)
- **longt5** -- [LongT5Config](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config) (LongT5 model)
- **luke** -- [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) (LUKE model)
- **lxmert** -- [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) (LXMERT model)
- **m2m_100** -- [M2M100Config](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config) (M2M100 model)
- **mamba** -- [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) (Mamba model)
- **mamba2** -- [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) (mamba2 model)
- **marian** -- [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) (Marian model)
- **markuplm** -- [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) (MarkupLM model)
- **mask2former** -- [Mask2FormerConfig](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig) (Mask2Former model)
- **maskformer** -- [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) (MaskFormer model)
- **maskformer-swin** -- `MaskFormerSwinConfig` (MaskFormerSwin model)
- **mbart** -- [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) (mBART model)
- **megatron-bert** -- [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) (Megatron-BERT model)
- **metaclip_2** -- [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) (MetaCLIP 2 model)
- **mgp-str** -- [MgpstrConfig](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrConfig) (MGP-STR model)
- **mimi** -- [MimiConfig](/docs/transformers/main/en/model_doc/mimi#transformers.MimiConfig) (Mimi model)
- **minimax** -- [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) (MiniMax model)
- **ministral** -- [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) (Ministral model)
- **ministral3** -- [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) (Ministral3 model)
- **mistral** -- [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) (Mistral model)
- **mistral3** -- [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) (Mistral3 model)
- **mixtral** -- [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) (Mixtral model)
- **mlcd** -- [MLCDVisionConfig](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionConfig) (MLCD model)
- **mllama** -- [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) (Mllama model)
- **mm-grounding-dino** -- [MMGroundingDinoConfig](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig) (MM Grounding DINO model)
- **mobilebert** -- [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) (MobileBERT model)
- **mobilenet_v1** -- [MobileNetV1Config](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config) (MobileNetV1 model)
- **mobilenet_v2** -- [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) (MobileNetV2 model)
- **mobilevit** -- [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) (MobileViT model)
- **mobilevitv2** -- [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) (MobileViTV2 model)
- **modernbert** -- [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) (ModernBERT model)
- **modernbert-decoder** -- [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) (ModernBertDecoder model)
- **moonshine** -- [MoonshineConfig](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineConfig) (Moonshine model)
- **moshi** -- [MoshiConfig](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiConfig) (Moshi model)
- **mpnet** -- [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) (MPNet model)
- **mpt** -- [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) (MPT model)
- **mra** -- [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) (MRA model)
- **mt5** -- [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) (MT5 model)
- **musicgen** -- [MusicgenConfig](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig) (MusicGen model)
- **musicgen_melody** -- [MusicgenMelodyConfig](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig) (MusicGen Melody model)
- **mvp** -- [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) (MVP model)
- **nanochat** -- [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) (NanoChat model)
- **nemotron** -- [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) (Nemotron model)
- **nllb-moe** -- [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) (NLLB-MOE model)
- **nougat** -- [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) (Nougat model)
- **nystromformer** -- [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) (Nystr√∂mformer model)
- **olmo** -- [OlmoConfig](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoConfig) (OLMo model)
- **olmo2** -- [Olmo2Config](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Config) (OLMo2 model)
- **olmo3** -- [Olmo3Config](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Config) (Olmo3 model)
- **olmoe** -- [OlmoeConfig](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeConfig) (OLMoE model)
- **omdet-turbo** -- [OmDetTurboConfig](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig) (OmDet-Turbo model)
- **oneformer** -- [OneFormerConfig](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig) (OneFormer model)
- **openai-gpt** -- [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) (OpenAI GPT model)
- **opt** -- [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) (OPT model)
- **ovis2** -- [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) (Ovis2 model)
- **owlv2** -- [Owlv2Config](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config) (OWLv2 model)
- **owlvit** -- [OwlViTConfig](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig) (OWL-ViT model)
- **paddleocr_vl** -- [PaddleOCRVLConfig](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLConfig) (PaddleOCRVL model)
- **paligemma** -- [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) (PaliGemma model)
- **parakeet_ctc** -- [ParakeetCTCConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetCTCConfig) (Parakeet model)
- **parakeet_encoder** -- [ParakeetEncoderConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoderConfig) (ParakeetEncoder model)
- **patchtsmixer** -- [PatchTSMixerConfig](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig) (PatchTSMixer model)
- **patchtst** -- [PatchTSTConfig](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTConfig) (PatchTST model)
- **pegasus** -- [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) (Pegasus model)
- **pegasus_x** -- [PegasusXConfig](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig) (PEGASUS-X model)
- **perceiver** -- [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) (Perceiver model)
- **perception_lm** -- [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) (PerceptionLM model)
- **persimmon** -- [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) (Persimmon model)
- **phi** -- [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) (Phi model)
- **phi3** -- [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) (Phi3 model)
- **phi4_multimodal** -- [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) (Phi4Multimodal model)
- **phimoe** -- [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) (Phimoe model)
- **pix2struct** -- [Pix2StructConfig](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig) (Pix2Struct model)
- **pixtral** -- [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) (Pixtral model)
- **plbart** -- [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) (PLBart model)
- **poolformer** -- [PoolFormerConfig](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig) (PoolFormer model)
- **pop2piano** -- [Pop2PianoConfig](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoConfig) (Pop2Piano model)
- **prompt_depth_anything** -- [PromptDepthAnythingConfig](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig) (PromptDepthAnything model)
- **prophetnet** -- [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) (ProphetNet model)
- **pvt** -- [PvtConfig](/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig) (PVT model)
- **pvt_v2** -- [PvtV2Config](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Config) (PVTv2 model)
- **qwen2** -- [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) (Qwen2 model)
- **qwen2_5_omni** -- [Qwen2_5OmniConfig](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) (Qwen2_5_VL model)
- **qwen2_5_vl_text** -- [Qwen2_5_VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextConfig) (Qwen2_5_VL model)
- **qwen2_audio** -- [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) (Qwen2Audio model)
- **qwen2_audio_encoder** -- [Qwen2AudioEncoderConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig) (Qwen2AudioEncoder model)
- **qwen2_moe** -- [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) (Qwen2MoE model)
- **qwen2_vl** -- [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) (Qwen2VL model)
- **qwen2_vl_text** -- [Qwen2VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextConfig) (Qwen2VL model)
- **qwen3** -- [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) (Qwen3Next model)
- **qwen3_omni_moe** -- [Qwen3OmniMoeConfig](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeConfig) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) (Qwen3VLMoe model)
- **qwen3_vl_moe_text** -- [Qwen3VLMoeTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextConfig) (Qwen3VLMoe model)
- **qwen3_vl_text** -- [Qwen3VLTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextConfig) (Qwen3VL model)
- **rag** -- [RagConfig](/docs/transformers/main/en/model_doc/rag#transformers.RagConfig) (RAG model)
- **recurrent_gemma** -- [RecurrentGemmaConfig](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig) (RecurrentGemma model)
- **reformer** -- [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) (Reformer model)
- **regnet** -- [RegNetConfig](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig) (RegNet model)
- **rembert** -- [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) (RemBERT model)
- **resnet** -- [ResNetConfig](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig) (ResNet model)
- **roberta** -- [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) (RoCBert model)
- **roformer** -- [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) (RoFormer model)
- **rt_detr** -- [RTDetrConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrConfig) (RT-DETR model)
- **rt_detr_resnet** -- [RTDetrResNetConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrResNetConfig) (RT-DETR-ResNet model)
- **rt_detr_v2** -- [RTDetrV2Config](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) (RT-DETRv2 model)
- **rwkv** -- [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) (RWKV model)
- **sam** -- [SamConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamConfig) (SAM model)
- **sam2** -- [Sam2Config](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Config) (SAM2 model)
- **sam2_hiera_det_model** -- [Sam2HieraDetConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetConfig) (Sam2HieraDetModel model)
- **sam2_video** -- [Sam2VideoConfig](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoConfig) (Sam2VideoModel model)
- **sam2_vision_model** -- [Sam2VisionConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionConfig) (Sam2VisionModel model)
- **sam3** -- [Sam3Config](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Config) (SAM3 model)
- **sam3_tracker** -- [Sam3TrackerConfig](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerConfig) (Sam3Tracker model)
- **sam3_tracker_video** -- [Sam3TrackerVideoConfig](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoConfig) (Sam3TrackerVideo model)
- **sam3_video** -- [Sam3VideoConfig](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoConfig) (Sam3VideoModel model)
- **sam3_vision_model** -- [Sam3VisionConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionConfig) (Sam3VisionModel model)
- **sam3_vit_model** -- [Sam3ViTConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTConfig) (Sam3ViTModel model)
- **sam_hq** -- [SamHQConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQConfig) (SAM-HQ model)
- **sam_hq_vision_model** -- [SamHQVisionConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionConfig) (SamHQVisionModel model)
- **sam_vision_model** -- [SamVisionConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionConfig) (SamVisionModel model)
- **seamless_m4t** -- [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) (SeamlessM4Tv2 model)
- **seed_oss** -- [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) (SeedOss model)
- **segformer** -- [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) (SegFormer model)
- **seggpt** -- [SegGptConfig](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptConfig) (SegGPT model)
- **sew** -- [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) (SEW model)
- **sew-d** -- [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) (SEW-D model)
- **shieldgemma2** -- [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) (Shieldgemma2 model)
- **siglip** -- [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) (SigLIP model)
- **siglip2** -- [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) (SigLIP2 model)
- **siglip2_vision_model** -- [Siglip2VisionConfig](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionConfig) (Siglip2VisionModel model)
- **siglip_vision_model** -- [SiglipVisionConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionConfig) (SiglipVisionModel model)
- **smollm3** -- [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) (SmolLM3 model)
- **smolvlm** -- [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) (SmolVLM model)
- **smolvlm_vision** -- [SmolVLMVisionConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionConfig) (SmolVLMVisionTransformer model)
- **speech-encoder-decoder** -- [SpeechEncoderDecoderConfig](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig) (Speech Encoder decoder model)
- **speech_to_text** -- [Speech2TextConfig](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig) (Speech2Text model)
- **speecht5** -- [SpeechT5Config](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config) (SpeechT5 model)
- **splinter** -- [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) (Splinter model)
- **squeezebert** -- [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) (SqueezeBERT model)
- **stablelm** -- [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) (StableLm model)
- **starcoder2** -- [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) (Starcoder2 model)
- **superglue** -- [SuperGlueConfig](/docs/transformers/main/en/model_doc/superglue#transformers.SuperGlueConfig) (SuperGlue model)
- **superpoint** -- [SuperPointConfig](/docs/transformers/main/en/model_doc/superpoint#transformers.SuperPointConfig) (SuperPoint model)
- **swiftformer** -- [SwiftFormerConfig](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig) (SwiftFormer model)
- **swin** -- [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) (Swin Transformer model)
- **swin2sr** -- [Swin2SRConfig](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRConfig) (Swin2SR model)
- **swinv2** -- [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) (Swin Transformer V2 model)
- **switch_transformers** -- [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) (SwitchTransformers model)
- **t5** -- [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) (T5 model)
- **t5gemma** -- [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) (T5Gemma2 model)
- **table-transformer** -- [TableTransformerConfig](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig) (Table Transformer model)
- **tapas** -- [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) (TAPAS model)
- **textnet** -- [TextNetConfig](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetConfig) (TextNet model)
- **time_series_transformer** -- [TimeSeriesTransformerConfig](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig) (Time Series Transformer model)
- **timesfm** -- [TimesFmConfig](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmConfig) (TimesFm model)
- **timesformer** -- [TimesformerConfig](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig) (TimeSformer model)
- **timm_backbone** -- [TimmBackboneConfig](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackboneConfig) (TimmBackbone model)
- **timm_wrapper** -- [TimmWrapperConfig](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig) (TimmWrapperModel model)
- **trocr** -- [TrOCRConfig](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRConfig) (TrOCR model)
- **tvp** -- [TvpConfig](/docs/transformers/main/en/model_doc/tvp#transformers.TvpConfig) (TVP model)
- **udop** -- [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) (UDOP model)
- **umt5** -- [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) (UMT5 model)
- **unispeech** -- [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) (UniSpeech model)
- **unispeech-sat** -- [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) (UniSpeechSat model)
- **univnet** -- [UnivNetConfig](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetConfig) (UnivNet model)
- **upernet** -- [UperNetConfig](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig) (UPerNet model)
- **vaultgemma** -- [VaultGemmaConfig](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaConfig) (VaultGemma model)
- **video_llama_3** -- [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) (VideoLlama3 model)
- **video_llama_3_vision** -- [VideoLlama3VisionConfig](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionConfig) (VideoLlama3Vision model)
- **video_llava** -- [VideoLlavaConfig](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaConfig) (VideoLlava model)
- **videomae** -- [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) (VideoMAE model)
- **vilt** -- [ViltConfig](/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig) (ViLT model)
- **vipllava** -- [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) (VipLlava model)
- **vision-encoder-decoder** -- [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) (Vision Encoder decoder model)
- **vision-text-dual-encoder** -- [VisionTextDualEncoderConfig](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig) (VisionTextDualEncoder model)
- **visual_bert** -- [VisualBertConfig](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig) (VisualBERT model)
- **vit** -- [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) (ViT model)
- **vit_mae** -- [ViTMAEConfig](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig) (ViTMAE model)
- **vit_msn** -- [ViTMSNConfig](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig) (ViTMSN model)
- **vitdet** -- [VitDetConfig](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig) (VitDet model)
- **vitmatte** -- [VitMatteConfig](/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteConfig) (ViTMatte model)
- **vitpose** -- [VitPoseConfig](/docs/transformers/main/en/model_doc/vitpose#transformers.VitPoseConfig) (ViTPose model)
- **vitpose_backbone** -- `VitPoseBackboneConfig` (ViTPoseBackbone model)
- **vits** -- [VitsConfig](/docs/transformers/main/en/model_doc/vits#transformers.VitsConfig) (VITS model)
- **vivit** -- [VivitConfig](/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig) (ViViT model)
- **vjepa2** -- [VJEPA2Config](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Config) (VJEPA2Model model)
- **voxtral** -- [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) (Voxtral model)
- **voxtral_encoder** -- [VoxtralEncoderConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoderConfig) (Voxtral Encoder model)
- **wav2vec2** -- [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) (WavLM model)
- **whisper** -- [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) (Whisper model)
- **xclip** -- [XCLIPConfig](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPConfig) (X-CLIP model)
- **xcodec** -- [XcodecConfig](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecConfig) (X-CODEC model)
- **xglm** -- [XGLMConfig](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig) (XGLM model)
- **xlm** -- [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) (XLM model)
- **xlm-roberta** -- [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) (XLNet model)
- **xlstm** -- [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) (xLSTM model)
- **xmod** -- [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) (X-MOD model)
- **yolos** -- [YolosConfig](/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig) (YOLOS model)
- **yoso** -- [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) (YOSO model)
- **zamba** -- [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) (Zamba model)
- **zamba2** -- [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) (Zamba2 model)
- **zoedepth** -- [ZoeDepthConfig](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthConfig) (ZoeDepth model)

Examples:

```python
>>> from transformers import AutoConfig

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-uncased")

>>> # Download configuration from huggingface.co (user-uploaded) and cache.
>>> config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

>>> # If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
>>> config = AutoConfig.from_pretrained("./test/bert_saved_model/")

>>> # Load a specific configuration file.
>>> config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

>>> # Change some config attributes when loading a pretrained config.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-uncased", output_attentions=True, foo=False)
>>> config.output_attentions
True

>>> config, unused_kwargs = AutoConfig.from_pretrained(
...     "google-bert/bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
... )
>>> config.output_attentions
True

>>> unused_kwargs
{'foo': False}
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model configuration hosted inside a model repo on huggingface.co. - A path to a *directory* containing a configuration file saved using the [save_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.save_pretrained) method, or the [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) method, e.g., `./my_model_directory/`. - A path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download the model weights and configuration files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

return_unused_kwargs (`bool`, *optional*, defaults to `False`) : If `False`, then this function returns just the final configuration object.  If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the part of `kwargs` which has not been used to update `config` and is otherwise ignored.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

kwargs(additional keyword arguments, *optional*) : The values in kwargs of any keys which are configuration attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.
#### register[[transformers.AutoConfig.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1360)

Register a new configuration for this class.

**Parameters:**

model_type (`str`) : The model type like "bert" or "gpt".

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The config to register.

## AutoTokenizer[[transformers.AutoTokenizer]]

#### transformers.AutoTokenizer[[transformers.AutoTokenizer]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L531)

This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the [AutoTokenizer.from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoTokenizer.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L545[{"name": "pretrained_model_name_or_path", "val": ""}, {"name": "*inputs", "val": ""}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  Can be either:

  - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.
  - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved
    using the [save_pretrained()](/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained) method, e.g., `./my_model_directory/`.
  - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
    single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not
    applicable to all derived classes)
- **inputs** (additional positional arguments, *optional*) --
  Will be passed along to the Tokenizer `__init__()` method.
- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) --
  The configuration object used to determine the tokenizer class to instantiate.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model configuration should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force the (re-)download the model weights and configuration files and override the
  cached versions if they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **subfolder** (`str`, *optional*) --
  In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
  facebook/rag-token-base), specify it here.
- **tokenizer_type** (`str`, *optional*) --
  Tokenizer type to be loaded.
- **backend** (`str`, *optional*, defaults to `"tokenizers"`) --
  Backend to use for tokenization. Valid options are:
  - `"tokenizers"`: Use the HuggingFace tokenizers library backend (default)
  - `"sentencepiece"`: Use the SentencePiece backend
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **kwargs** (additional keyword arguments, *optional*) --
  Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like
  `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,
  `additional_special_tokens`. See parameters in the `__init__()` for more details.0

Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.

The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **aimv2** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (AIMv2 model)
- **albert** -- [AlbertTokenizer](/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer) (ALBERT model)
- **align** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (ALIGN model)
- **arcee** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Arcee model)
- **aria** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Aria model)
- **aya_vision** -- [CohereTokenizer](/docs/transformers/main/en/model_doc/cohere#transformers.CohereTokenizer) (AyaVision model)
- **bark** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (Bark model)
- **bart** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (BART model)
- **barthez** -- [BarthezTokenizer](/docs/transformers/main/en/model_doc/barthez#transformers.BarthezTokenizer) (BARThez model)
- **bartpho** -- [BartphoTokenizer](/docs/transformers/main/en/model_doc/bartpho#transformers.BartphoTokenizer) (BARTpho model)
- **bert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (BERT model)
- **bert-generation** -- [BertGenerationTokenizer](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationTokenizer) (Bert Generation model)
- **bert-japanese** -- [BertJapaneseTokenizer](/docs/transformers/main/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer) (BertJapanese model)
- **bertweet** -- [BertweetTokenizer](/docs/transformers/main/en/model_doc/bertweet#transformers.BertweetTokenizer) (BERTweet model)
- **big_bird** -- [BigBirdTokenizer](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdTokenizer) (BigBird model)
- **bigbird_pegasus** -- [PegasusTokenizer](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer) (BigBird-Pegasus model)
- **biogpt** -- [BioGptTokenizer](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptTokenizer) (BioGpt model)
- **bitnet** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (BitNet model)
- **blenderbot** -- [BlenderbotTokenizer](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotTokenizer) (Blenderbot model)
- **blenderbot-small** -- [BlenderbotSmallTokenizer](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer) (BlenderbotSmall model)
- **blip** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (BLIP model)
- **blip-2** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (BLIP-2 model)
- **bloom** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (BLOOM model)
- **blt** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Blt model)
- **bridgetower** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (BridgeTower model)
- **bros** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (BROS model)
- **byt5** -- [ByT5Tokenizer](/docs/transformers/main/en/model_doc/byt5#transformers.ByT5Tokenizer) (ByT5 model)
- **camembert** -- [CamembertTokenizer](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertTokenizer) (CamemBERT model)
- **canine** -- [CanineTokenizer](/docs/transformers/main/en/model_doc/canine#transformers.CanineTokenizer) (CANINE model)
- **chameleon** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Chameleon model)
- **chinese_clip** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (Chinese-CLIP model)
- **clap** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (CLAP model)
- **clip** -- [CLIPTokenizer](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (CLIP model)
- **clipseg** -- [CLIPTokenizer](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (CLIPSeg model)
- **clvp** -- [ClvpTokenizer](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpTokenizer) (CLVP model)
- **code_llama** -- [CodeLlamaTokenizer](/docs/transformers/main/en/model_doc/code_llama#transformers.CodeLlamaTokenizer) (CodeLlama model)
- **codegen** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (CodeGen model)
- **cohere** -- [CohereTokenizer](/docs/transformers/main/en/model_doc/cohere#transformers.CohereTokenizer) (Cohere model)
- **cohere2** -- [CohereTokenizer](/docs/transformers/main/en/model_doc/cohere#transformers.CohereTokenizer) (Cohere2 model)
- **colpali** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (ColPali model)
- **colqwen2** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (ColQwen2 model)
- **convbert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (ConvBERT model)
- **cpm** -- [CpmTokenizer](/docs/transformers/main/en/model_doc/cpm#transformers.CpmTokenizer) (CPM model)
- **cpmant** -- [CpmAntTokenizer](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntTokenizer) (CPM-Ant model)
- **csm** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (CSM model)
- **ctrl** -- [CTRLTokenizer](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLTokenizer) (CTRL model)
- **data2vec-audio** -- [Wav2Vec2CTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) (Data2VecAudio model)
- **data2vec-text** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (Data2VecText model)
- **dbrx** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (DBRX model)
- **deberta** -- [DebertaTokenizer](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaTokenizer) (DeBERTa model)
- **deberta-v2** -- [DebertaV2Tokenizer](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer) (DeBERTa-v2 model)
- **deepseek_v2** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (DeepSeek-V2 model)
- **deepseek_v3** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (DeepSeek-V3 model)
- **deepseek_vl** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (DeepseekVLHybrid model)
- **dia** -- [DiaTokenizer](/docs/transformers/main/en/model_doc/dia#transformers.DiaTokenizer) (Dia model)
- **diffllama** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (DiffLlama model)
- **distilbert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (DistilBERT model)
- **dpr** -- [DPRQuestionEncoderTokenizerFast](/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast) (DPR model)
- **electra** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (ELECTRA model)
- **emu3** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (Emu3 model)
- **ernie** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (ERNIE model)
- **ernie4_5** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Ernie4_5 model)
- **ernie4_5_moe** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Ernie4_5_MoE model)
- **esm** -- [EsmTokenizer](/docs/transformers/main/en/model_doc/esm#transformers.EsmTokenizer) (ESM model)
- **exaone4** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (EXAONE-4.0 model)
- **falcon** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Falcon model)
- **falcon_mamba** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (FalconMamba model)
- **fastspeech2_conformer** -- `None` (FastSpeech2Conformer model)
- **flaubert** -- [FlaubertTokenizer](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertTokenizer) (FlauBERT model)
- **flava** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (FLAVA model)
- **flex_olmo** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (FlexOlmo model)
- **florence2** -- [BartTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (Florence2 model)
- **fnet** -- [FNetTokenizerFast](/docs/transformers/main/en/model_doc/fnet#transformers.FNetTokenizer) (FNet model)
- **fsmt** -- [FSMTTokenizer](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTTokenizer) (FairSeq Machine-Translation model)
- **funnel** -- [FunnelTokenizer](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer) (Funnel Transformer model)
- **fuyu** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Fuyu model)
- **gemma** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma model)
- **gemma2** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma2 model)
- **gemma3** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma3ForConditionalGeneration model)
- **gemma3_text** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma3ForCausalLM model)
- **gemma3n** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma3nForConditionalGeneration model)
- **gemma3n_text** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Gemma3nForCausalLM model)
- **git** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (GIT model)
- **glm** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GLM model)
- **glm4** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GLM4 model)
- **glm4_moe** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Glm4MoE model)
- **glm4v** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GLM4V model)
- **glm4v_moe** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GLM4VMOE model)
- **got_ocr2** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GOT-OCR2 model)
- **gpt-sw3** -- [GPTSw3Tokenizer](/docs/transformers/main/en/model_doc/gpt-sw3#transformers.GPTSw3Tokenizer) (GPT-Sw3 model)
- **gpt2** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GPTBigCode model)
- **gpt_neo** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXTokenizer](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (GPT NeoX model)
- **gpt_neox_japanese** -- [GPTNeoXJapaneseTokenizer](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseTokenizer) (GPT NeoX Japanese model)
- **gpt_oss** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (GptOss model)
- **gptj** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GPT-J model)
- **granite** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (Granite model)
- **granitemoe** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GraniteMoeMoe model)
- **granitemoehybrid** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GraniteMoeHybrid model)
- **granitemoeshared** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (GraniteMoeSharedMoe model)
- **grounding-dino** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (Grounding DINO model)
- **groupvit** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (GroupViT model)
- **helium** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Helium model)
- **herbert** -- [HerbertTokenizer](/docs/transformers/main/en/model_doc/herbert#transformers.HerbertTokenizer) (HerBERT model)
- **hubert** -- [Wav2Vec2CTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) (Hubert model)
- **ibert** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (I-BERT model)
- **idefics** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (IDEFICS model)
- **idefics2** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Idefics2 model)
- **idefics3** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Idefics3 model)
- **instructblip** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (InstructBLIP model)
- **instructblipvideo** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (InstructBlipVideo model)
- **internvl** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (InternVL model)
- **jamba** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Jamba model)
- **janus** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Janus model)
- **jetmoe** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (JetMoe model)
- **kosmos-2** -- [XLMRobertaTokenizer](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (KOSMOS-2 model)
- **kosmos-2.5** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (KOSMOS-2.5 model)
- **layoutlm** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2Tokenizer](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3Tokenizer](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer) (LayoutLMv3 model)
- **layoutxlm** -- [LayoutXLMTokenizer](/docs/transformers/main/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer) (LayoutXLM model)
- **led** -- [LEDTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (LED model)
- **lfm2_vl** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Lfm2Vl model)
- **lilt** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (LiLT model)
- **llama** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (LLaMA model)
- **llama4** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Llama4 model)
- **llama4_text** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Llama4ForCausalLM model)
- **llava** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (LLaVa model)
- **llava_next** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (LLaVA-NeXT model)
- **llava_next_video** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (LLaVA-Onevision model)
- **longformer** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (Longformer model)
- **longt5** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (LongT5 model)
- **luke** -- [LukeTokenizer](/docs/transformers/main/en/model_doc/luke#transformers.LukeTokenizer) (LUKE model)
- **lxmert** -- [LxmertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (LXMERT model)
- **m2m_100** -- [M2M100Tokenizer](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Tokenizer) (M2M100 model)
- **mamba** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (Mamba model)
- **mamba2** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (mamba2 model)
- **marian** -- [MarianTokenizer](/docs/transformers/main/en/model_doc/marian#transformers.MarianTokenizer) (Marian model)
- **mbart** -- [MBartTokenizer](/docs/transformers/main/en/model_doc/mbart#transformers.MBartTokenizer) (mBART model)
- **mbart50** -- [MBart50Tokenizer](/docs/transformers/main/en/model_doc/mbart#transformers.MBart50Tokenizer) (mBART-50 model)
- **megatron-bert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (Megatron-BERT model)
- **metaclip_2** -- [XLMRobertaTokenizerFast](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (MetaCLIP 2 model)
- **mgp-str** -- [MgpstrTokenizer](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrTokenizer) (MGP-STR model)
- **minimax** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (MiniMax model)
- **ministral3** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Ministral3 model)
- **mistral** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Mistral model)
- **mistral3** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Mistral3 model)
- **mixtral** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Mixtral model)
- **mllama** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Mllama model)
- **mluke** -- [MLukeTokenizer](/docs/transformers/main/en/model_doc/mluke#transformers.MLukeTokenizer) (mLUKE model)
- **mm-grounding-dino** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (MM Grounding DINO model)
- **mobilebert** -- [MobileBertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (MobileBERT model)
- **modernbert** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (ModernBERT model)
- **moonshine** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Moonshine model)
- **moshi** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Moshi model)
- **mpnet** -- [MPNetTokenizer](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetTokenizer) (MPNet model)
- **mpt** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (MPT model)
- **mra** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (MRA model)
- **mt5** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (MT5 model)
- **musicgen** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (MusicGen model)
- **musicgen_melody** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (MusicGen Melody model)
- **mvp** -- [MvpTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (MVP model)
- **myt5** -- [MyT5Tokenizer](/docs/transformers/main/en/model_doc/myt5#transformers.MyT5Tokenizer) (myt5 model)
- **nemotron** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (Nemotron model)
- **nllb** -- [NllbTokenizer](/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizer) (NLLB model)
- **nllb-moe** -- [NllbTokenizer](/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizer) (NLLB-MOE model)
- **nougat** -- [NougatTokenizer](/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizer) (Nougat model)
- **nystromformer** -- [AlbertTokenizerFast](/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer) (Nystr√∂mformer model)
- **olmo** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (OLMo model)
- **olmo2** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (OLMo2 model)
- **olmo3** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (Olmo3 model)
- **olmoe** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (OLMoE model)
- **omdet-turbo** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (OmDet-Turbo model)
- **oneformer** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (OneFormer model)
- **openai-gpt** -- [OpenAIGPTTokenizer](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer) (OpenAI GPT model)
- **opt** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (OPT model)
- **ovis2** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Ovis2 model)
- **owlv2** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (OWLv2 model)
- **owlvit** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (OWL-ViT model)
- **paddleocr_vl** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (PaddleOCRVL model)
- **paligemma** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (PaliGemma model)
- **pegasus** -- [PegasusTokenizer](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer) (Pegasus model)
- **pegasus_x** -- [PegasusTokenizer](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer) (PEGASUS-X model)
- **perceiver** -- [PerceiverTokenizer](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverTokenizer) (Perceiver model)
- **persimmon** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Persimmon model)
- **phi** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (Phi model)
- **phi3** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Phi3 model)
- **phimoe** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Phimoe model)
- **phobert** -- [PhobertTokenizer](/docs/transformers/main/en/model_doc/phobert#transformers.PhobertTokenizer) (PhoBERT model)
- **pix2struct** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (Pix2Struct model)
- **pixtral** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Pixtral model)
- **plbart** -- [PLBartTokenizer](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartTokenizer) (PLBart model)
- **prophetnet** -- [ProphetNetTokenizer](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer) (ProphetNet model)
- **qwen2** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2 model)
- **qwen2_5_omni** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2_5_VL model)
- **qwen2_audio** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2Audio model)
- **qwen2_moe** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2MoE model)
- **qwen2_vl** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen2VL model)
- **qwen3** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3 model)
- **qwen3_moe** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3MoE model)
- **qwen3_next** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3Next model)
- **qwen3_omni_moe** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen2TokenizerFast](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer) (Qwen3VLMoe model)
- **rag** -- [RagTokenizer](/docs/transformers/main/en/model_doc/rag#transformers.RagTokenizer) (RAG model)
- **recurrent_gemma** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (RecurrentGemma model)
- **reformer** -- [ReformerTokenizer](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerTokenizer) (Reformer model)
- **rembert** -- [RemBertTokenizer](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertTokenizer) (RemBERT model)
- **roberta** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaTokenizer](/docs/transformers/main/en/model_doc/longformer#transformers.RobertaTokenizer) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertTokenizer](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertTokenizer) (RoCBert model)
- **roformer** -- [RoFormerTokenizer](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerTokenizer) (RoFormer model)
- **rwkv** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (RWKV model)
- **seamless_m4t** -- [SeamlessM4TTokenizer](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4TTokenizer](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer) (SeamlessM4Tv2 model)
- **shieldgemma2** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (Shieldgemma2 model)
- **siglip** -- [SiglipTokenizer](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipTokenizer) (SigLIP model)
- **siglip2** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (SigLIP2 model)
- **smollm3** -- [TokenizersBackend](/docs/transformers/main/en/main_classes/tokenizer#transformers.TokenizersBackend) (SmolLM3 model)
- **speech_to_text** -- [Speech2TextTokenizer](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer) (Speech2Text model)
- **speecht5** -- [SpeechT5Tokenizer](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer) (SpeechT5 model)
- **splinter** -- [SplinterTokenizer](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterTokenizer) (Splinter model)
- **squeezebert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (SqueezeBERT model)
- **stablelm** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (StableLm model)
- **starcoder2** -- [GPT2Tokenizer](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer) (Starcoder2 model)
- **switch_transformers** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (SwitchTransformers model)
- **t5** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (T5 model)
- **t5gemma** -- [GemmaTokenizerFast](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaTokenizer) (T5Gemma model)
- **tapas** -- [TapasTokenizer](/docs/transformers/main/en/model_doc/tapas#transformers.TapasTokenizer) (TAPAS model)
- **trocr** -- [XLMRobertaTokenizer](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (TrOCR model)
- **tvp** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (TVP model)
- **udop** -- [UdopTokenizer](/docs/transformers/main/en/model_doc/udop#transformers.UdopTokenizer) (UDOP model)
- **umt5** -- [T5Tokenizer](/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer) (UMT5 model)
- **video_llava** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (VideoLlava model)
- **vilt** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (ViLT model)
- **vipllava** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (VipLlava model)
- **visual_bert** -- [BertTokenizer](/docs/transformers/main/en/model_doc/electra#transformers.BertTokenizer) (VisualBERT model)
- **vits** -- [VitsTokenizer](/docs/transformers/main/en/model_doc/vits#transformers.VitsTokenizer) (VITS model)
- **voxtral** -- [MistralCommonBackend](/docs/transformers/main/en/model_doc/mistral3#transformers.MistralCommonBackend) (Voxtral model)
- **wav2vec2** -- [Wav2Vec2CTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2CTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2CTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer) (Wav2Vec2-Conformer model)
- **wav2vec2_phoneme** -- [Wav2Vec2PhonemeCTCTokenizer](/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer) (Wav2Vec2Phoneme model)
- **whisper** -- [WhisperTokenizer](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperTokenizer) (Whisper model)
- **xclip** -- [CLIPTokenizerFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) (X-CLIP model)
- **xglm** -- [XGLMTokenizer](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMTokenizer) (XGLM model)
- **xlm** -- [XLMTokenizer](/docs/transformers/main/en/model_doc/xlm#transformers.XLMTokenizer) (XLM model)
- **xlm-roberta** -- [XLMRobertaTokenizer](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaTokenizer](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetTokenizer](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizer) (XLNet model)
- **xlstm** -- [GPTNeoXTokenizerFast](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizer) (xLSTM model)
- **xmod** -- [XLMRobertaTokenizerFast](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer) (X-MOD model)
- **yoso** -- [AlbertTokenizer](/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer) (YOSO model)
- **zamba** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Zamba model)
- **zamba2** -- [LlamaTokenizer](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer) (Zamba2 model)

Examples:

```python
>>> from transformers import AutoTokenizer

>>> # Download vocabulary from huggingface.co and cache.
>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

>>> # Download vocabulary from huggingface.co (user-uploaded) and cache.
>>> tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

>>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
>>> # tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")

>>> # Download vocabulary from huggingface.co and define model-specific arguments
>>> tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True)

>>> # Explicitly use the tokenizers backend
>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", backend="tokenizers")

>>> # Explicitly use the sentencepiece backend
>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", backend="sentencepiece")
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co. - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved using the [save_pretrained()](/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained) method, e.g., `./my_model_directory/`. - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not applicable to all derived classes)

inputs (additional positional arguments, *optional*) : Will be passed along to the Tokenizer `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : The configuration object used to determine the tokenizer class to instantiate.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download the model weights and configuration files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

subfolder (`str`, *optional*) : In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for facebook/rag-token-base), specify it here.

tokenizer_type (`str`, *optional*) : Tokenizer type to be loaded.

backend (`str`, *optional*, defaults to `"tokenizers"`) : Backend to use for tokenization. Valid options are: - `"tokenizers"`: Use the HuggingFace tokenizers library backend (default) - `"sentencepiece"`: Use the SentencePiece backend

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

kwargs (additional keyword arguments, *optional*) : Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`. See parameters in the `__init__()` for more details.
#### register[[transformers.AutoTokenizer.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L766)

Register a new tokenizer in this mapping.

**Parameters:**

config_class ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The configuration corresponding to the model to register.

tokenizer_class : The tokenizer class to register (V5 - preferred parameter).

slow_tokenizer_class : (Deprecated) The slow tokenizer to register.

fast_tokenizer_class : (Deprecated) The fast tokenizer to register.

## AutoFeatureExtractor[[transformers.AutoFeatureExtractor]]

#### transformers.AutoFeatureExtractor[[transformers.AutoFeatureExtractor]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L223)

This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the [AutoFeatureExtractor.from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoFeatureExtractor.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L237[{"name": "pretrained_model_name_or_path", "val": ""}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  This can be either:

  - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on
    huggingface.co.
  - a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained) method, e.g.,
    `./my_model_directory/`.
  - a path or url to a saved feature extractor JSON *file*, e.g.,
    `./my_model_directory/preprocessor_config.json`.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force to (re-)download the feature extractor files and override the cached versions
  if they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
- **token** (`str` or *bool*, *optional*) --
  The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
  when running `hf auth login` (stored in `~/.huggingface`).
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **return_unused_kwargs** (`bool`, *optional*, defaults to `False`) --
  If `False`, then this function returns just the final feature extractor object. If `True`, then this
  functions returns a `Tuple(feature_extractor, unused_kwargs)` where *unused_kwargs* is a dictionary
  consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
  `kwargs` which has not been used to update `feature_extractor` and is otherwise ignored.
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **kwargs** (`dict[str, Any]`, *optional*) --
  The values in kwargs of any keys which are feature extractor attributes will be used to override the
  loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is
  controlled by the `return_unused_kwargs` keyword parameter.0

Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary.

The feature extractor class to instantiate is selected based on the `model_type` property of the config object
(either passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's
missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:

- **audio-spectrogram-transformer** -- [ASTFeatureExtractor](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor) (Audio Spectrogram Transformer model)
- **clap** -- [ClapFeatureExtractor](/docs/transformers/main/en/model_doc/clap#transformers.ClapFeatureExtractor) (CLAP model)
- **clvp** -- [ClvpFeatureExtractor](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpFeatureExtractor) (CLVP model)
- **csm** -- [EncodecFeatureExtractor](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor) (CSM model)
- **dac** -- [DacFeatureExtractor](/docs/transformers/main/en/model_doc/dac#transformers.DacFeatureExtractor) (DAC model)
- **data2vec-audio** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Data2VecAudio model)
- **dia** -- [DiaFeatureExtractor](/docs/transformers/main/en/model_doc/dia#transformers.DiaFeatureExtractor) (Dia model)
- **encodec** -- [EncodecFeatureExtractor](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor) (EnCodec model)
- **gemma3n** -- [Gemma3nAudioFeatureExtractor](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nAudioFeatureExtractor) (Gemma3nForConditionalGeneration model)
- **granite_speech** -- [GraniteSpeechFeatureExtractor](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechFeatureExtractor) (GraniteSpeech model)
- **hubert** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Hubert model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextFeatureExtractor](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextFeatureExtractor) (KyutaiSpeechToText model)
- **lasr_ctc** -- [LasrFeatureExtractor](/docs/transformers/main/en/model_doc/lasr#transformers.LasrFeatureExtractor) (Lasr model)
- **lasr_encoder** -- [LasrFeatureExtractor](/docs/transformers/main/en/model_doc/lasr#transformers.LasrFeatureExtractor) (LasrEncoder model)
- **markuplm** -- [MarkupLMFeatureExtractor](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor) (MarkupLM model)
- **mimi** -- [EncodecFeatureExtractor](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor) (Mimi model)
- **moonshine** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Moonshine model)
- **moshi** -- [EncodecFeatureExtractor](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor) (Moshi model)
- **musicgen** -- [EncodecFeatureExtractor](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor) (MusicGen model)
- **musicgen_melody** -- [MusicgenMelodyFeatureExtractor](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyFeatureExtractor) (MusicGen Melody model)
- **parakeet_ctc** -- [ParakeetFeatureExtractor](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetFeatureExtractor) (Parakeet model)
- **parakeet_encoder** -- [ParakeetFeatureExtractor](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetFeatureExtractor) (ParakeetEncoder model)
- **phi4_multimodal** -- [Phi4MultimodalFeatureExtractor](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalFeatureExtractor) (Phi4Multimodal model)
- **pop2piano** -- [Pop2PianoFeatureExtractor](/docs/transformers/main/en/model_doc/pop2piano#transformers.models.pop2piano.feature_extraction_pop2piano._LazyModule.__getattr__..Placeholder) (Pop2Piano model)
- **qwen2_5_omni** -- [WhisperFeatureExtractor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor) (Qwen2_5Omni model)
- **qwen2_audio** -- [WhisperFeatureExtractor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor) (Qwen2Audio model)
- **qwen3_omni_moe** -- [WhisperFeatureExtractor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor) (Qwen3OmniMoE model)
- **seamless_m4t** -- [SeamlessM4TFeatureExtractor](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4TFeatureExtractor](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor) (SeamlessM4Tv2 model)
- **sew** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (SEW model)
- **sew-d** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (SEW-D model)
- **speech_to_text** -- [Speech2TextFeatureExtractor](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor) (Speech2Text model)
- **speecht5** -- [SpeechT5FeatureExtractor](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor) (SpeechT5 model)
- **unispeech** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (UniSpeech model)
- **unispeech-sat** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (UniSpeechSat model)
- **univnet** -- [UnivNetFeatureExtractor](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetFeatureExtractor) (UnivNet model)
- **voxtral** -- [WhisperFeatureExtractor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor) (Voxtral model)
- **wav2vec2** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (Wav2Vec2-Conformer model)
- **wavlm** -- [Wav2Vec2FeatureExtractor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) (WavLM model)
- **whisper** -- [WhisperFeatureExtractor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor) (Whisper model)
- **xcodec** -- [DacFeatureExtractor](/docs/transformers/main/en/model_doc/dac#transformers.DacFeatureExtractor) (X-CODEC model)

Passing `token=True` is required when you want to use a private model.

Examples:

```python
>>> from transformers import AutoFeatureExtractor

>>> # Download feature extractor from huggingface.co and cache.
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

>>> # If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
>>> # feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : This can be either:  - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on huggingface.co. - a path to a *directory* containing a feature extractor file saved using the [save_pretrained()](/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained) method, e.g., `./my_model_directory/`. - a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model feature extractor should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force to (re-)download the feature extractor files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.

token (`str` or *bool*, *optional*) : The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `hf auth login` (stored in `~/.huggingface`).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

return_unused_kwargs (`bool`, *optional*, defaults to `False`) : If `False`, then this function returns just the final feature extractor object. If `True`, then this functions returns a `Tuple(feature_extractor, unused_kwargs)` where *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of `kwargs` which has not been used to update `feature_extractor` and is otherwise ignored.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

kwargs (`dict[str, Any]`, *optional*) : The values in kwargs of any keys which are feature extractor attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is controlled by the `return_unused_kwargs` keyword parameter.
#### register[[transformers.AutoFeatureExtractor.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L362)

Register a new feature extractor for this class.

**Parameters:**

config_class ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The configuration corresponding to the model to register.

feature_extractor_class (`FeatureExtractorMixin`) : The feature extractor to register.

## AutoImageProcessor[[transformers.AutoImageProcessor]]

#### transformers.AutoImageProcessor[[transformers.AutoImageProcessor]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L377)

This is a generic image processor class that will be instantiated as one of the image processor classes of the
library when created with the [AutoImageProcessor.from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoImageProcessor.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L391[{"name": "pretrained_model_name_or_path", "val": ""}, {"name": "*inputs", "val": ""}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  This can be either:

  - a string, the *model id* of a pretrained image_processor hosted inside a model repo on
    huggingface.co.
  - a path to a *directory* containing a image processor file saved using the
    [save_pretrained()](/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained) method, e.g.,
    `./my_model_directory/`.
  - a path or url to a saved image processor JSON *file*, e.g.,
    `./my_model_directory/preprocessor_config.json`.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model image processor should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force to (re-)download the image processor files and override the cached versions if
  they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
- **token** (`str` or *bool*, *optional*) --
  The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
  when running `hf auth login` (stored in `~/.huggingface`).
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **use_fast** (`bool`, *optional*, defaults to `False`) --
  Use a fast torchvision-base image processor if it is supported for a given model.
  If a fast image processor is not available for a given model, a normal numpy-based image processor
  is returned instead.
- **return_unused_kwargs** (`bool`, *optional*, defaults to `False`) --
  If `False`, then this function returns just the final image processor object. If `True`, then this
  functions returns a `Tuple(image_processor, unused_kwargs)` where *unused_kwargs* is a dictionary
  consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of
  `kwargs` which has not been used to update `image_processor` and is otherwise ignored.
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **image_processor_filename** (`str`, *optional*, defaults to `"config.json"`) --
  The name of the file in the model directory to use for the image processor config.
- **kwargs** (`dict[str, Any]`, *optional*) --
  The values in kwargs of any keys which are image processor attributes will be used to override the
  loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is
  controlled by the `return_unused_kwargs` keyword parameter.0

Instantiate one of the image processor classes of the library from a pretrained model vocabulary.

The image processor class to instantiate is selected based on the `model_type` property of the config object
(either passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's
missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:

- **aimv2** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (AIMv2 model)
- **aimv2_vision_model** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (Aimv2VisionModel model)
- **align** -- [EfficientNetImageProcessor](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor) or [EfficientNetImageProcessorFast](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessorFast) (ALIGN model)
- **altclip** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (AltCLIP model)
- **aria** -- [AriaImageProcessor](/docs/transformers/main/en/model_doc/aria#transformers.AriaImageProcessor) (Aria model)
- **aya_vision** -- [GotOcr2ImageProcessor](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessor) or [GotOcr2ImageProcessorFast](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessorFast) (AyaVision model)
- **beit** -- [BeitImageProcessor](/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessor) or [BeitImageProcessorFast](/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessorFast) (BEiT model)
- **bit** -- [BitImageProcessor](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor) or [BitImageProcessorFast](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessorFast) (BiT model)
- **blip** -- [BlipImageProcessor](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor) or [BlipImageProcessorFast](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessorFast) (BLIP model)
- **blip-2** -- [BlipImageProcessor](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor) or [BlipImageProcessorFast](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessorFast) (BLIP-2 model)
- **bridgetower** -- [BridgeTowerImageProcessor](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor) or [BridgeTowerImageProcessorFast](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessorFast) (BridgeTower model)
- **chameleon** -- [ChameleonImageProcessor](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonImageProcessor) or [ChameleonImageProcessorFast](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonImageProcessorFast) (Chameleon model)
- **chinese_clip** -- [ChineseCLIPImageProcessor](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessor) or [ChineseCLIPImageProcessorFast](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessorFast) (Chinese-CLIP model)
- **clip** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (CLIP model)
- **clipseg** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (CLIPSeg model)
- **cohere2_vision** -- [Cohere2VisionImageProcessorFast](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionImageProcessorFast) (Cohere2Vision model)
- **colpali** -- [SiglipImageProcessor](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessor) or [SiglipImageProcessorFast](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessorFast) (ColPali model)
- **colqwen2** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (ColQwen2 model)
- **conditional_detr** -- [ConditionalDetrImageProcessor](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor) or [ConditionalDetrImageProcessorFast](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessorFast) (Conditional DETR model)
- **convnext** -- [ConvNextImageProcessor](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor) or [ConvNextImageProcessorFast](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessorFast) (ConvNeXT model)
- **convnextv2** -- [ConvNextImageProcessor](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor) or [ConvNextImageProcessorFast](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessorFast) (ConvNeXTV2 model)
- **cvt** -- [ConvNextImageProcessor](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor) or [ConvNextImageProcessorFast](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessorFast) (CvT model)
- **data2vec-vision** -- [BeitImageProcessor](/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessor) or [BeitImageProcessorFast](/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessorFast) (Data2VecVision model)
- **deepseek_vl** -- [DeepseekVLImageProcessor](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLImageProcessor) or [DeepseekVLImageProcessorFast](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLImageProcessorFast) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridImageProcessor](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridImageProcessor) or [DeepseekVLHybridImageProcessorFast](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridImageProcessorFast) (DeepseekVLHybrid model)
- **deformable_detr** -- [DeformableDetrImageProcessor](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrImageProcessor) or [DeformableDetrImageProcessorFast](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrImageProcessorFast) (Deformable DETR model)
- **deit** -- [DeiTImageProcessor](/docs/transformers/main/en/model_doc/deit#transformers.DeiTImageProcessor) or [DeiTImageProcessorFast](/docs/transformers/main/en/model_doc/deit#transformers.DeiTImageProcessorFast) (DeiT model)
- **depth_anything** -- [DPTImageProcessor](/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessor) or [DPTImageProcessorFast](/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessorFast) (Depth Anything model)
- **depth_pro** -- [DepthProImageProcessor](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProImageProcessor) or [DepthProImageProcessorFast](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProImageProcessorFast) (DepthPro model)
- **detr** -- [DetrImageProcessor](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessor) or [DetrImageProcessorFast](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessorFast) (DETR model)
- **dinat** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (DiNAT model)
- **dinov2** -- [BitImageProcessor](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor) or [BitImageProcessorFast](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessorFast) (DINOv2 model)
- **dinov3_vit** -- [DINOv3ViTImageProcessorFast](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTImageProcessorFast) (DINOv3 ViT model)
- **donut-swin** -- [DonutImageProcessor](/docs/transformers/main/en/model_doc/donut#transformers.DonutImageProcessor) or [DonutImageProcessorFast](/docs/transformers/main/en/model_doc/donut#transformers.DonutImageProcessorFast) (DonutSwin model)
- **dpt** -- [DPTImageProcessor](/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessor) or [DPTImageProcessorFast](/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessorFast) (DPT model)
- **edgetam** -- [Sam2ImageProcessorFast](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2ImageProcessorFast) (EdgeTAM model)
- **efficientloftr** -- [EfficientLoFTRImageProcessor](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessor) or [EfficientLoFTRImageProcessorFast](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessorFast) (EfficientLoFTR model)
- **efficientnet** -- [EfficientNetImageProcessor](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor) or [EfficientNetImageProcessorFast](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessorFast) (EfficientNet model)
- **emu3** -- [Emu3ImageProcessor](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ImageProcessor) (Emu3 model)
- **eomt** -- [EomtImageProcessor](/docs/transformers/main/en/model_doc/eomt#transformers.EomtImageProcessor) or [EomtImageProcessorFast](/docs/transformers/main/en/model_doc/eomt#transformers.EomtImageProcessorFast) (EoMT model)
- **flava** -- [FlavaImageProcessor](/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor) or [FlavaImageProcessorFast](/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessorFast) (FLAVA model)
- **florence2** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (Florence2 model)
- **focalnet** -- [BitImageProcessor](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor) or [BitImageProcessorFast](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessorFast) (FocalNet model)
- **fuyu** -- [FuyuImageProcessor](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuImageProcessor) or [FuyuImageProcessorFast](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuImageProcessorFast) (Fuyu model)
- **gemma3** -- [Gemma3ImageProcessor](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessor) or [Gemma3ImageProcessorFast](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessorFast) (Gemma3ForConditionalGeneration model)
- **gemma3n** -- [SiglipImageProcessor](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessor) or [SiglipImageProcessorFast](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessorFast) (Gemma3nForConditionalGeneration model)
- **git** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (GIT model)
- **glm46v** -- [Glm46VImageProcessor](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VImageProcessor) or [Glm46VImageProcessorFast](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VImageProcessorFast) (Glm46V model)
- **glm4v** -- [Glm4vImageProcessor](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vImageProcessor) or [Glm4vImageProcessorFast](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vImageProcessorFast) (GLM4V model)
- **glpn** -- [GLPNImageProcessor](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNImageProcessor) or [GLPNImageProcessorFast](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNImageProcessorFast) (GLPN model)
- **got_ocr2** -- [GotOcr2ImageProcessor](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessor) or [GotOcr2ImageProcessorFast](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessorFast) (GOT-OCR2 model)
- **grounding-dino** -- [GroundingDinoImageProcessor](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor) or [GroundingDinoImageProcessorFast](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessorFast) (Grounding DINO model)
- **groupvit** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (GroupViT model)
- **hiera** -- [BitImageProcessor](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor) or [BitImageProcessorFast](/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessorFast) (Hiera model)
- **idefics** -- [IdeficsImageProcessor](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsImageProcessor) (IDEFICS model)
- **idefics2** -- [Idefics2ImageProcessor](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ImageProcessor) or [Idefics2ImageProcessorFast](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ImageProcessorFast) (Idefics2 model)
- **idefics3** -- [Idefics3ImageProcessor](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ImageProcessor) or [Idefics3ImageProcessorFast](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ImageProcessorFast) (Idefics3 model)
- **ijepa** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (I-JEPA model)
- **imagegpt** -- [ImageGPTImageProcessor](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor) or [ImageGPTImageProcessorFast](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTImageProcessorFast) (ImageGPT model)
- **instructblip** -- [BlipImageProcessor](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor) or [BlipImageProcessorFast](/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessorFast) (InstructBLIP model)
- **internvl** -- [GotOcr2ImageProcessor](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessor) or [GotOcr2ImageProcessorFast](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessorFast) (InternVL model)
- **janus** -- [JanusImageProcessor](/docs/transformers/main/en/model_doc/janus#transformers.JanusImageProcessor) or [JanusImageProcessorFast](/docs/transformers/main/en/model_doc/janus#transformers.JanusImageProcessorFast) (Janus model)
- **kosmos-2** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5ImageProcessor](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessor) or [Kosmos2_5ImageProcessorFast](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessorFast) (KOSMOS-2.5 model)
- **layoutlmv2** -- [LayoutLMv2ImageProcessor](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor) or [LayoutLMv2ImageProcessorFast](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessorFast) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3ImageProcessor](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor) or [LayoutLMv3ImageProcessorFast](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessorFast) (LayoutLMv3 model)
- **layoutxlm** -- [LayoutLMv2ImageProcessor](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor) or [LayoutLMv2ImageProcessor](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor) (LayoutXLM model)
- **levit** -- [LevitImageProcessor](/docs/transformers/main/en/model_doc/levit#transformers.LevitImageProcessor) or [LevitImageProcessorFast](/docs/transformers/main/en/model_doc/levit#transformers.LevitImageProcessorFast) (LeViT model)
- **lfm2_vl** -- [Lfm2VlImageProcessorFast](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlImageProcessorFast) (Lfm2Vl model)
- **lightglue** -- [LightGlueImageProcessor](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueImageProcessor) or [LightGlueImageProcessorFast](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueImageProcessorFast) (LightGlue model)
- **llama4** -- [Llama4ImageProcessorFast](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ImageProcessorFast) (Llama4 model)
- **llava** -- [LlavaImageProcessor](/docs/transformers/main/en/model_doc/llava#transformers.LlavaImageProcessor) or [LlavaImageProcessorFast](/docs/transformers/main/en/model_doc/llava#transformers.LlavaImageProcessorFast) (LLaVa model)
- **llava_next** -- [LlavaNextImageProcessor](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextImageProcessor) or [LlavaNextImageProcessorFast](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextImageProcessorFast) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextImageProcessor](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextImageProcessor) or [LlavaNextImageProcessorFast](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextImageProcessorFast) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionImageProcessor](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionImageProcessor) or [LlavaOnevisionImageProcessorFast](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionImageProcessorFast) (LLaVA-Onevision model)
- **mask2former** -- [Mask2FormerImageProcessor](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerImageProcessor) or [Mask2FormerImageProcessorFast](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerImageProcessorFast) (Mask2Former model)
- **maskformer** -- [MaskFormerImageProcessor](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerImageProcessor) or [MaskFormerImageProcessorFast](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerImageProcessorFast) (MaskFormer model)
- **metaclip_2** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (MetaCLIP 2 model)
- **mgp-str** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (MGP-STR model)
- **mistral3** -- [PixtralImageProcessor](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralImageProcessor) or [PixtralImageProcessorFast](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralImageProcessorFast) (Mistral3 model)
- **mlcd** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (MLCD model)
- **mllama** -- [MllamaImageProcessor](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaImageProcessor) or [MllamaImageProcessorFast](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaImageProcessorFast) (Mllama model)
- **mm-grounding-dino** -- [GroundingDinoImageProcessor](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor) or [GroundingDinoImageProcessorFast](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessorFast) (MM Grounding DINO model)
- **mobilenet_v1** -- [MobileNetV1ImageProcessor](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ImageProcessor) or [MobileNetV1ImageProcessorFast](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ImageProcessorFast) (MobileNetV1 model)
- **mobilenet_v2** -- [MobileNetV2ImageProcessor](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessor) or [MobileNetV2ImageProcessorFast](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessorFast) (MobileNetV2 model)
- **mobilevit** -- [MobileViTImageProcessor](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessor) or [MobileViTImageProcessorFast](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessorFast) (MobileViT model)
- **mobilevitv2** -- [MobileViTImageProcessor](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessor) or [MobileViTImageProcessorFast](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessorFast) (MobileViTV2 model)
- **nougat** -- [NougatImageProcessor](/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor) or [NougatImageProcessorFast](/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessorFast) (Nougat model)
- **omdet-turbo** -- [DetrImageProcessor](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessor) or [DetrImageProcessorFast](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessorFast) (OmDet-Turbo model)
- **oneformer** -- [OneFormerImageProcessor](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerImageProcessor) or [OneFormerImageProcessorFast](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerImageProcessorFast) (OneFormer model)
- **ovis2** -- [Ovis2ImageProcessor](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ImageProcessor) or [Ovis2ImageProcessorFast](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ImageProcessorFast) (Ovis2 model)
- **owlv2** -- [Owlv2ImageProcessor](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ImageProcessor) or [Owlv2ImageProcessorFast](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ImageProcessorFast) (OWLv2 model)
- **owlvit** -- [OwlViTImageProcessor](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTImageProcessor) or [OwlViTImageProcessorFast](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTImageProcessorFast) (OWL-ViT model)
- **paddleocr_vl** -- [PaddleOCRVLImageProcessor](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLImageProcessor) or [PaddleOCRVLImageProcessorFast](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLImageProcessorFast) (PaddleOCRVL model)
- **paligemma** -- [SiglipImageProcessor](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessor) or [SiglipImageProcessorFast](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessorFast) (PaliGemma model)
- **perceiver** -- [PerceiverImageProcessor](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverImageProcessor) or [PerceiverImageProcessorFast](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverImageProcessorFast) (Perceiver model)
- **perception_lm** -- [PerceptionLMImageProcessorFast](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMImageProcessorFast) (PerceptionLM model)
- **phi4_multimodal** -- [Phi4MultimodalImageProcessorFast](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalImageProcessorFast) (Phi4Multimodal model)
- **pix2struct** -- [Pix2StructImageProcessor](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor) or [Pix2StructImageProcessorFast](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructImageProcessorFast) (Pix2Struct model)
- **pixtral** -- [PixtralImageProcessor](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralImageProcessor) or [PixtralImageProcessorFast](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralImageProcessorFast) (Pixtral model)
- **poolformer** -- [PoolFormerImageProcessor](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerImageProcessor) or [PoolFormerImageProcessorFast](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerImageProcessorFast) (PoolFormer model)
- **prompt_depth_anything** -- [PromptDepthAnythingImageProcessor](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingImageProcessor) or [PromptDepthAnythingImageProcessorFast](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingImageProcessorFast) (PromptDepthAnything model)
- **pvt** -- [PvtImageProcessor](/docs/transformers/main/en/model_doc/pvt#transformers.PvtImageProcessor) or [PvtImageProcessorFast](/docs/transformers/main/en/model_doc/pvt#transformers.PvtImageProcessorFast) (PVT model)
- **pvt_v2** -- [PvtImageProcessor](/docs/transformers/main/en/model_doc/pvt#transformers.PvtImageProcessor) or [PvtImageProcessorFast](/docs/transformers/main/en/model_doc/pvt#transformers.PvtImageProcessorFast) (PVTv2 model)
- **qwen2_5_omni** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (Qwen2_5_VL model)
- **qwen2_vl** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (Qwen2VL model)
- **qwen3_omni_moe** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen2VLImageProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor) or [Qwen2VLImageProcessorFast](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast) (Qwen3VL model)
- **regnet** -- [ConvNextImageProcessor](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor) or [ConvNextImageProcessorFast](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessorFast) (RegNet model)
- **resnet** -- [ConvNextImageProcessor](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor) or [ConvNextImageProcessorFast](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessorFast) (ResNet model)
- **rt_detr** -- [RTDetrImageProcessor](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrImageProcessor) or [RTDetrImageProcessorFast](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrImageProcessorFast) (RT-DETR model)
- **sam** -- [SamImageProcessor](/docs/transformers/main/en/model_doc/sam#transformers.SamImageProcessor) or [SamImageProcessorFast](/docs/transformers/main/en/model_doc/sam#transformers.SamImageProcessorFast) (SAM model)
- **sam2** -- [Sam2ImageProcessorFast](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2ImageProcessorFast) (SAM2 model)
- **sam2_video** -- [Sam2ImageProcessorFast](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2ImageProcessorFast) (Sam2VideoModel model)
- **sam3** -- [Sam3ImageProcessorFast](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ImageProcessorFast) (SAM3 model)
- **sam3_video** -- [Sam3ImageProcessorFast](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ImageProcessorFast) (Sam3VideoModel model)
- **sam_hq** -- [SamImageProcessor](/docs/transformers/main/en/model_doc/sam#transformers.SamImageProcessor) or [SamImageProcessorFast](/docs/transformers/main/en/model_doc/sam#transformers.SamImageProcessorFast) (SAM-HQ model)
- **segformer** -- [SegformerImageProcessor](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessor) or [SegformerImageProcessorFast](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessorFast) (SegFormer model)
- **seggpt** -- [SegGptImageProcessor](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptImageProcessor) (SegGPT model)
- **shieldgemma2** -- [Gemma3ImageProcessor](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessor) or [Gemma3ImageProcessorFast](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessorFast) (Shieldgemma2 model)
- **siglip** -- [SiglipImageProcessor](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessor) or [SiglipImageProcessorFast](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessorFast) (SigLIP model)
- **siglip2** -- [Siglip2ImageProcessor](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2ImageProcessor) or [Siglip2ImageProcessorFast](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2ImageProcessorFast) (SigLIP2 model)
- **smolvlm** -- [SmolVLMImageProcessor](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMImageProcessor) or [SmolVLMImageProcessorFast](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMImageProcessorFast) (SmolVLM model)
- **superglue** -- [SuperGlueImageProcessor](/docs/transformers/main/en/model_doc/superglue#transformers.SuperGlueImageProcessor) or [SuperGlueImageProcessorFast](/docs/transformers/main/en/model_doc/superglue#transformers.SuperGlueImageProcessorFast) (SuperGlue model)
- **superpoint** -- [SuperPointImageProcessor](/docs/transformers/main/en/model_doc/superpoint#transformers.SuperPointImageProcessor) or [SuperPointImageProcessorFast](/docs/transformers/main/en/model_doc/superpoint#transformers.SuperPointImageProcessorFast) (SuperPoint model)
- **swiftformer** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (SwiftFormer model)
- **swin** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (Swin Transformer model)
- **swin2sr** -- [Swin2SRImageProcessor](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRImageProcessor) or [Swin2SRImageProcessorFast](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRImageProcessorFast) (Swin2SR model)
- **swinv2** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (Swin Transformer V2 model)
- **t5gemma2** -- [Gemma3ImageProcessor](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessor) or [Gemma3ImageProcessorFast](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ImageProcessorFast) (T5Gemma2 model)
- **table-transformer** -- [DetrImageProcessor](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessor) or [DetrImageProcessorFast](/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessorFast) (Table Transformer model)
- **textnet** -- [TextNetImageProcessor](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetImageProcessor) or [TextNetImageProcessorFast](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetImageProcessorFast) (TextNet model)
- **timesformer** -- [VideoMAEImageProcessor](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEImageProcessor) (TimeSformer model)
- **timm_wrapper** -- [TimmWrapperImageProcessor](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor) (TimmWrapperModel model)
- **trocr** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (TrOCR model)
- **tvp** -- [TvpImageProcessor](/docs/transformers/main/en/model_doc/tvp#transformers.TvpImageProcessor) or [TvpImageProcessorFast](/docs/transformers/main/en/model_doc/tvp#transformers.TvpImageProcessorFast) (TVP model)
- **udop** -- [LayoutLMv3ImageProcessor](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor) or [LayoutLMv3ImageProcessorFast](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessorFast) (UDOP model)
- **upernet** -- [SegformerImageProcessor](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessor) or [SegformerImageProcessorFast](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessorFast) (UPerNet model)
- **video_llama_3** -- [VideoLlama3ImageProcessor](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ImageProcessor) or [VideoLlama3ImageProcessorFast](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ImageProcessorFast) (VideoLlama3 model)
- **video_llava** -- [VideoLlavaImageProcessor](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaImageProcessor) (VideoLlava model)
- **videomae** -- [VideoMAEImageProcessor](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEImageProcessor) (VideoMAE model)
- **vilt** -- [ViltImageProcessor](/docs/transformers/main/en/model_doc/vilt#transformers.ViltImageProcessor) or [ViltImageProcessorFast](/docs/transformers/main/en/model_doc/vilt#transformers.ViltImageProcessorFast) (ViLT model)
- **vipllava** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (VipLlava model)
- **vit** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (ViT model)
- **vit_mae** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (ViTMAE model)
- **vit_msn** -- [ViTImageProcessor](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor) or [ViTImageProcessorFast](/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessorFast) (ViTMSN model)
- **vitmatte** -- [VitMatteImageProcessor](/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteImageProcessor) or [VitMatteImageProcessorFast](/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteImageProcessorFast) (ViTMatte model)
- **vitpose** -- [VitPoseImageProcessor](/docs/transformers/main/en/model_doc/vitpose#transformers.VitPoseImageProcessor) or [VitPoseImageProcessorFast](/docs/transformers/main/en/model_doc/vitpose#transformers.VitPoseImageProcessorFast) (ViTPose model)
- **xclip** -- [CLIPImageProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor) or [CLIPImageProcessorFast](/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessorFast) (X-CLIP model)
- **yolos** -- [YolosImageProcessor](/docs/transformers/main/en/model_doc/yolos#transformers.YolosImageProcessor) or [YolosImageProcessorFast](/docs/transformers/main/en/model_doc/yolos#transformers.YolosImageProcessorFast) (YOLOS model)
- **zoedepth** -- [ZoeDepthImageProcessor](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthImageProcessor) or [ZoeDepthImageProcessorFast](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthImageProcessorFast) (ZoeDepth model)

Passing `token=True` is required when you want to use a private model.

Examples:

```python
>>> from transformers import AutoImageProcessor

>>> # Download image processor from huggingface.co and cache.
>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

>>> # If image processor files are in a directory (e.g. image processor was saved using *save_pretrained('./test/saved_model/')*)
>>> # image_processor = AutoImageProcessor.from_pretrained("./test/saved_model/")
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : This can be either:  - a string, the *model id* of a pretrained image_processor hosted inside a model repo on huggingface.co. - a path to a *directory* containing a image processor file saved using the [save_pretrained()](/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained) method, e.g., `./my_model_directory/`. - a path or url to a saved image processor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model image processor should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force to (re-)download the image processor files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.

token (`str` or *bool*, *optional*) : The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `hf auth login` (stored in `~/.huggingface`).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

use_fast (`bool`, *optional*, defaults to `False`) : Use a fast torchvision-base image processor if it is supported for a given model. If a fast image processor is not available for a given model, a normal numpy-based image processor is returned instead.

return_unused_kwargs (`bool`, *optional*, defaults to `False`) : If `False`, then this function returns just the final image processor object. If `True`, then this functions returns a `Tuple(image_processor, unused_kwargs)` where *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of `kwargs` which has not been used to update `image_processor` and is otherwise ignored.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

image_processor_filename (`str`, *optional*, defaults to `"config.json"`) : The name of the file in the model directory to use for the image processor config.

kwargs (`dict[str, Any]`, *optional*) : The values in kwargs of any keys which are image processor attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is controlled by the `return_unused_kwargs` keyword parameter.
#### register[[transformers.AutoImageProcessor.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L635)

Register a new image processor for this class.

**Parameters:**

config_class ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The configuration corresponding to the model to register.

image_processor_class ([ImageProcessingMixin](/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin)) : The image processor to register.

## AutoVideoProcessor[[transformers.AutoVideoProcessor]]

#### transformers.AutoVideoProcessor[[transformers.AutoVideoProcessor]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/video_processing_auto.py#L237)

This is a generic video processor class that will be instantiated as one of the video processor classes of the
library when created with the [AutoVideoProcessor.from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoVideoProcessor.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoVideoProcessor.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/video_processing_auto.py#L251[{"name": "pretrained_model_name_or_path", "val": ""}, {"name": "*inputs", "val": ""}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  This can be either:

  - a string, the *model id* of a pretrained video_processor hosted inside a model repo on
    huggingface.co.
  - a path to a *directory* containing a video processor file saved using the
    [save_pretrained()](/docs/transformers/main/en/main_classes/video_processor#transformers.BaseVideoProcessor.save_pretrained) method, e.g.,
    `./my_model_directory/`.
  - a path or url to a saved video processor JSON *file*, e.g.,
    `./my_model_directory/preprocessor_config.json`.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model video processor should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force to (re-)download the video processor files and override the cached versions if
  they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
- **token** (`str` or *bool*, *optional*) --
  The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
  when running `hf auth login` (stored in `~/.huggingface`).
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **return_unused_kwargs** (`bool`, *optional*, defaults to `False`) --
  If `False`, then this function returns just the final video processor object. If `True`, then this
  functions returns a `Tuple(video_processor, unused_kwargs)` where *unused_kwargs* is a dictionary
  consisting of the key/value pairs whose keys are not video processor attributes: i.e., the part of
  `kwargs` which has not been used to update `video_processor` and is otherwise ignored.
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **kwargs** (`dict[str, Any]`, *optional*) --
  The values in kwargs of any keys which are video processor attributes will be used to override the
  loaded values. Behavior concerning key/value pairs whose keys are *not* video processor attributes is
  controlled by the `return_unused_kwargs` keyword parameter.0

Instantiate one of the video processor classes of the library from a pretrained model vocabulary.

The video processor class to instantiate is selected based on the `model_type` property of the config object
(either passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's
missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:

- **glm46v** -- [Glm46VVideoProcessor](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VVideoProcessor) (Glm46V model)
- **glm4v** -- [Glm4vVideoProcessor](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVideoProcessor) (GLM4V model)
- **instructblip** -- [InstructBlipVideoVideoProcessor](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVideoProcessor) (InstructBLIP model)
- **instructblipvideo** -- [InstructBlipVideoVideoProcessor](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVideoProcessor) (InstructBlipVideo model)
- **internvl** -- [InternVLVideoProcessor](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVideoProcessor) (InternVL model)
- **llava_next_video** -- [LlavaNextVideoVideoProcessor](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoVideoProcessor) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionVideoProcessor](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionVideoProcessor) (LLaVA-Onevision model)
- **perception_lm** -- [PerceptionLMVideoProcessor](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMVideoProcessor) (PerceptionLM model)
- **qwen2_5_omni** -- [Qwen2VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor) (Qwen2_5_VL model)
- **qwen2_vl** -- [Qwen2VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor) (Qwen2VL model)
- **qwen3_omni_moe** -- [Qwen2VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen3VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLVideoProcessor) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLVideoProcessor](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLVideoProcessor) (Qwen3VLMoe model)
- **sam2_video** -- [Sam2VideoVideoProcessor](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoVideoProcessor) (Sam2VideoModel model)
- **sam3_video** -- `Sam3VideoVideoProcessor` (Sam3VideoModel model)
- **smolvlm** -- [SmolVLMVideoProcessor](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVideoProcessor) (SmolVLM model)
- **video_llama_3** -- [VideoLlama3VideoProcessor](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VideoProcessor) (VideoLlama3 model)
- **video_llava** -- [VideoLlavaVideoProcessor](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaVideoProcessor) (VideoLlava model)
- **videomae** -- [VideoMAEVideoProcessor](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEVideoProcessor) (VideoMAE model)
- **vjepa2** -- [VJEPA2VideoProcessor](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor) (VJEPA2Model model)

Passing `token=True` is required when you want to use a private model.

Examples:

```python
>>> from transformers import AutoVideoProcessor

>>> # Download video processor from huggingface.co and cache.
>>> video_processor = AutoVideoProcessor.from_pretrained("llava-hf/llava-onevision-qwen2-0.5b-ov-hf")

>>> # If video processor files are in a directory (e.g. video processor was saved using *save_pretrained('./test/saved_model/')*)
>>> # video_processor = AutoVideoProcessor.from_pretrained("./test/saved_model/")
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : This can be either:  - a string, the *model id* of a pretrained video_processor hosted inside a model repo on huggingface.co. - a path to a *directory* containing a video processor file saved using the [save_pretrained()](/docs/transformers/main/en/main_classes/video_processor#transformers.BaseVideoProcessor.save_pretrained) method, e.g., `./my_model_directory/`. - a path or url to a saved video processor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model video processor should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force to (re-)download the video processor files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.

token (`str` or *bool*, *optional*) : The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `hf auth login` (stored in `~/.huggingface`).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

return_unused_kwargs (`bool`, *optional*, defaults to `False`) : If `False`, then this function returns just the final video processor object. If `True`, then this functions returns a `Tuple(video_processor, unused_kwargs)` where *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are not video processor attributes: i.e., the part of `kwargs` which has not been used to update `video_processor` and is otherwise ignored.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

kwargs (`dict[str, Any]`, *optional*) : The values in kwargs of any keys which are video processor attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are *not* video processor attributes is controlled by the `return_unused_kwargs` keyword parameter.
#### register[[transformers.AutoVideoProcessor.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/video_processing_auto.py#L396)

Register a new video processor for this class.

**Parameters:**

config_class ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The configuration corresponding to the model to register.

video_processor_class ([BaseVideoProcessor](/docs/transformers/main/en/main_classes/video_processor#transformers.BaseVideoProcessor)) : The video processor to register.

## AutoProcessor[[transformers.AutoProcessor]]

#### transformers.AutoProcessor[[transformers.AutoProcessor]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/processing_auto.py#L191)

This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the [AutoProcessor.from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor.from_pretrained) class method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_pretrainedtransformers.AutoProcessor.from_pretrainedhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/processing_auto.py#L205[{"name": "pretrained_model_name_or_path", "val": ""}, {"name": "**kwargs", "val": ""}]- **pretrained_model_name_or_path** (`str` or `os.PathLike`) --
  This can be either:

  - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on
    huggingface.co.
  - a path to a *directory* containing a processor files saved using the `save_pretrained()` method,
    e.g., `./my_model_directory/`.
- **cache_dir** (`str` or `os.PathLike`, *optional*) --
  Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
  standard cache should not be used.
- **force_download** (`bool`, *optional*, defaults to `False`) --
  Whether or not to force to (re-)download the feature extractor files and override the cached versions
  if they exist.
- **proxies** (`dict[str, str]`, *optional*) --
  A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
  'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
- **token** (`str` or *bool*, *optional*) --
  The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
  when running `hf auth login` (stored in `~/.huggingface`).
- **revision** (`str`, *optional*, defaults to `"main"`) --
  The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
  git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
  identifier allowed by git.
- **return_unused_kwargs** (`bool`, *optional*, defaults to `False`) --
  If `False`, then this function returns just the final feature extractor object. If `True`, then this
  functions returns a `Tuple(feature_extractor, unused_kwargs)` where *unused_kwargs* is a dictionary
  consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
  `kwargs` which has not been used to update `feature_extractor` and is otherwise ignored.
- **trust_remote_code** (`bool`, *optional*, defaults to `False`) --
  Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
  should only be set to `True` for repositories you trust and in which you have read the code, as it will
  execute code present on the Hub on your local machine.
- **kwargs** (`dict[str, Any]`, *optional*) --
  The values in kwargs of any keys which are feature extractor attributes will be used to override the
  loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is
  controlled by the `return_unused_kwargs` keyword parameter.0

Instantiate one of the processor classes of the library from a pretrained model vocabulary.

The processor class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible):

- **aimv2** -- [CLIPProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor) (AIMv2 model)
- **align** -- [AlignProcessor](/docs/transformers/main/en/model_doc/align#transformers.AlignProcessor) (ALIGN model)
- **altclip** -- [AltCLIPProcessor](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPProcessor) (AltCLIP model)
- **aria** -- [AriaProcessor](/docs/transformers/main/en/model_doc/aria#transformers.AriaProcessor) (Aria model)
- **audioflamingo3** -- [AudioFlamingo3Processor](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Processor) (AudioFlamingo3 model)
- **aya_vision** -- [AyaVisionProcessor](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionProcessor) (AyaVision model)
- **bark** -- [BarkProcessor](/docs/transformers/main/en/model_doc/bark#transformers.BarkProcessor) (Bark model)
- **blip** -- [BlipProcessor](/docs/transformers/main/en/model_doc/blip#transformers.BlipProcessor) (BLIP model)
- **blip-2** -- [Blip2Processor](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor) (BLIP-2 model)
- **bridgetower** -- [BridgeTowerProcessor](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerProcessor) (BridgeTower model)
- **chameleon** -- [ChameleonProcessor](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonProcessor) (Chameleon model)
- **chinese_clip** -- [ChineseCLIPProcessor](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPProcessor) (Chinese-CLIP model)
- **clap** -- [ClapProcessor](/docs/transformers/main/en/model_doc/clap#transformers.ClapProcessor) (CLAP model)
- **clip** -- [CLIPProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor) (CLIP model)
- **clipseg** -- [CLIPSegProcessor](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor) (CLIPSeg model)
- **clvp** -- [ClvpProcessor](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpProcessor) (CLVP model)
- **cohere2_vision** -- [Cohere2VisionProcessor](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionProcessor) (Cohere2Vision model)
- **colpali** -- [ColPaliProcessor](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliProcessor) (ColPali model)
- **colqwen2** -- [ColQwen2Processor](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2Processor) (ColQwen2 model)
- **deepseek_vl** -- [DeepseekVLProcessor](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLProcessor) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridProcessor](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridProcessor) (DeepseekVLHybrid model)
- **dia** -- [DiaProcessor](/docs/transformers/main/en/model_doc/dia#transformers.DiaProcessor) (Dia model)
- **edgetam** -- [Sam2Processor](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Processor) (EdgeTAM model)
- **emu3** -- [Emu3Processor](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Processor) (Emu3 model)
- **evolla** -- [EvollaProcessor](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaProcessor) (Evolla model)
- **flava** -- [FlavaProcessor](/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor) (FLAVA model)
- **florence2** -- [Florence2Processor](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Processor) (Florence2 model)
- **fuyu** -- [FuyuProcessor](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuProcessor) (Fuyu model)
- **gemma3** -- [Gemma3Processor](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Processor) (Gemma3ForConditionalGeneration model)
- **gemma3n** -- [Gemma3nProcessor](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nProcessor) (Gemma3nForConditionalGeneration model)
- **git** -- [GitProcessor](/docs/transformers/main/en/model_doc/git#transformers.GitProcessor) (GIT model)
- **glm46v** -- [Glm46VProcessor](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VProcessor) (Glm46V model)
- **glm4v** -- [Glm4vProcessor](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vProcessor) (GLM4V model)
- **glm4v_moe** -- [Glm4vProcessor](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vProcessor) (GLM4VMOE model)
- **got_ocr2** -- [GotOcr2Processor](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Processor) (GOT-OCR2 model)
- **granite_speech** -- [GraniteSpeechProcessor](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechProcessor) (GraniteSpeech model)
- **grounding-dino** -- [GroundingDinoProcessor](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor) (Grounding DINO model)
- **groupvit** -- [CLIPProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor) (GroupViT model)
- **hubert** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (Hubert model)
- **idefics** -- [IdeficsProcessor](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsProcessor) (IDEFICS model)
- **idefics2** -- [Idefics2Processor](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Processor) (Idefics2 model)
- **idefics3** -- [Idefics3Processor](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Processor) (Idefics3 model)
- **instructblip** -- [InstructBlipProcessor](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipProcessor) (InstructBLIP model)
- **instructblipvideo** -- [InstructBlipVideoProcessor](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor) (InstructBlipVideo model)
- **internvl** -- [InternVLProcessor](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLProcessor) (InternVL model)
- **janus** -- [JanusProcessor](/docs/transformers/main/en/model_doc/janus#transformers.JanusProcessor) (Janus model)
- **kosmos-2** -- [Kosmos2Processor](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Processor) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5Processor](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Processor) (KOSMOS-2.5 model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextProcessor](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextProcessor) (KyutaiSpeechToText model)
- **layoutlmv2** -- [LayoutLMv2Processor](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3Processor](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor) (LayoutLMv3 model)
- **layoutxlm** -- [LayoutXLMProcessor](/docs/transformers/main/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor) (LayoutXLM model)
- **lfm2_vl** -- [Lfm2VlProcessor](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlProcessor) (Lfm2Vl model)
- **llama4** -- [Llama4Processor](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Processor) (Llama4 model)
- **llava** -- [LlavaProcessor](/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor) (LLaVa model)
- **llava_next** -- [LlavaNextProcessor](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextProcessor) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoProcessor](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoProcessor) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionProcessor](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionProcessor) (LLaVA-Onevision model)
- **markuplm** -- [MarkupLMProcessor](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMProcessor) (MarkupLM model)
- **metaclip_2** -- [CLIPProcessor](/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor) (MetaCLIP 2 model)
- **mgp-str** -- [MgpstrProcessor](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrProcessor) (MGP-STR model)
- **mistral3** -- [PixtralProcessor](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralProcessor) (Mistral3 model)
- **mllama** -- [MllamaProcessor](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaProcessor) (Mllama model)
- **mm-grounding-dino** -- [GroundingDinoProcessor](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor) (MM Grounding DINO model)
- **moonshine** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (Moonshine model)
- **omdet-turbo** -- [OmDetTurboProcessor](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboProcessor) (OmDet-Turbo model)
- **oneformer** -- [OneFormerProcessor](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerProcessor) (OneFormer model)
- **ovis2** -- [Ovis2Processor](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Processor) (Ovis2 model)
- **owlv2** -- [Owlv2Processor](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Processor) (OWLv2 model)
- **owlvit** -- [OwlViTProcessor](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor) (OWL-ViT model)
- **paddleocr_vl** -- [PaddleOCRVLProcessor](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLProcessor) (PaddleOCRVL model)
- **paligemma** -- [PaliGemmaProcessor](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaProcessor) (PaliGemma model)
- **perception_lm** -- [PerceptionLMProcessor](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMProcessor) (PerceptionLM model)
- **phi4_multimodal** -- [Phi4MultimodalProcessor](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalProcessor) (Phi4Multimodal model)
- **pix2struct** -- [Pix2StructProcessor](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructProcessor) (Pix2Struct model)
- **pixtral** -- [PixtralProcessor](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralProcessor) (Pixtral model)
- **pop2piano** -- [Pop2PianoProcessor](/docs/transformers/main/en/model_doc/pop2piano#transformers.models.pop2piano.processing_pop2piano._LazyModule.__getattr__..Placeholder) (Pop2Piano model)
- **qwen2_5_omni** -- [Qwen2_5OmniProcessor](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2_5_VLProcessor](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLProcessor) (Qwen2_5_VL model)
- **qwen2_audio** -- [Qwen2AudioProcessor](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioProcessor) (Qwen2Audio model)
- **qwen2_vl** -- [Qwen2VLProcessor](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLProcessor) (Qwen2VL model)
- **qwen3_omni_moe** -- [Qwen3OmniMoeProcessor](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeProcessor) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen3VLProcessor](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLProcessor) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLProcessor](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLProcessor) (Qwen3VLMoe model)
- **sam** -- [SamProcessor](/docs/transformers/main/en/model_doc/sam#transformers.SamProcessor) (SAM model)
- **sam2** -- [Sam2Processor](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Processor) (SAM2 model)
- **sam3** -- [Sam3Processor](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Processor) (SAM3 model)
- **sam_hq** -- [SamHQProcessor](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQProcessor) (SAM-HQ model)
- **seamless_m4t** -- [SeamlessM4TProcessor](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor) (SeamlessM4T model)
- **sew** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (SEW model)
- **sew-d** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (SEW-D model)
- **shieldgemma2** -- [ShieldGemma2Processor](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Processor) (Shieldgemma2 model)
- **siglip** -- [SiglipProcessor](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipProcessor) (SigLIP model)
- **siglip2** -- [Siglip2Processor](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Processor) (SigLIP2 model)
- **smolvlm** -- [SmolVLMProcessor](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMProcessor) (SmolVLM model)
- **speech_to_text** -- [Speech2TextProcessor](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextProcessor) (Speech2Text model)
- **speecht5** -- [SpeechT5Processor](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Processor) (SpeechT5 model)
- **t5gemma2** -- [Gemma3Processor](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Processor) (T5Gemma2 model)
- **trocr** -- [TrOCRProcessor](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRProcessor) (TrOCR model)
- **tvp** -- [TvpProcessor](/docs/transformers/main/en/model_doc/tvp#transformers.TvpProcessor) (TVP model)
- **udop** -- [UdopProcessor](/docs/transformers/main/en/model_doc/udop#transformers.UdopProcessor) (UDOP model)
- **unispeech** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (UniSpeech model)
- **unispeech-sat** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (UniSpeechSat model)
- **video_llava** -- [VideoLlavaProcessor](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaProcessor) (VideoLlava model)
- **vilt** -- [ViltProcessor](/docs/transformers/main/en/model_doc/vilt#transformers.ViltProcessor) (ViLT model)
- **vipllava** -- [LlavaProcessor](/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor) (VipLlava model)
- **vision-text-dual-encoder** -- [VisionTextDualEncoderProcessor](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor) (VisionTextDualEncoder model)
- **voxtral** -- [VoxtralProcessor](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralProcessor) (Voxtral model)
- **wav2vec2** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (Wav2Vec2-Conformer model)
- **wavlm** -- [Wav2Vec2Processor](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) (WavLM model)
- **whisper** -- [WhisperProcessor](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperProcessor) (Whisper model)
- **xclip** -- [XCLIPProcessor](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPProcessor) (X-CLIP model)

Passing `token=True` is required when you want to use a private model.

Examples:

```python
>>> from transformers import AutoProcessor

>>> # Download processor from huggingface.co and cache.
>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

>>> # If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
>>> # processor = AutoProcessor.from_pretrained("./test/saved_model/")
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : This can be either:  - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on huggingface.co. - a path to a *directory* containing a processor files saved using the `save_pretrained()` method, e.g., `./my_model_directory/`.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model feature extractor should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force to (re-)download the feature extractor files and override the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.

token (`str` or *bool*, *optional*) : The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `hf auth login` (stored in `~/.huggingface`).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

return_unused_kwargs (`bool`, *optional*, defaults to `False`) : If `False`, then this function returns just the final feature extractor object. If `True`, then this functions returns a `Tuple(feature_extractor, unused_kwargs)` where *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of `kwargs` which has not been used to update `feature_extractor` and is otherwise ignored.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

kwargs (`dict[str, Any]`, *optional*) : The values in kwargs of any keys which are feature extractor attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is controlled by the `return_unused_kwargs` keyword parameter.
#### register[[transformers.AutoProcessor.register]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/processing_auto.py#L425)

Register a new processor for this class.

**Parameters:**

config_class ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The configuration corresponding to the model to register.

processor_class ([ProcessorMixin](/docs/transformers/main/en/main_classes/processors#transformers.ProcessorMixin)) : The processor to register.

## Generic model classes

The following auto classes are available for instantiating a base model class without a specific head.

### AutoModel[[transformers.AutoModel]]

#### transformers.AutoModel[[transformers.AutoModel]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1925)

This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModel.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [ASTConfig](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig) configuration class: [ASTModel](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel) (Audio Spectrogram Transformer model)
  - [AfmoeConfig](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeConfig) configuration class: [AfmoeModel](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeModel) (AFMoE model)
  - [Aimv2Config](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Config) configuration class: [Aimv2Model](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Model) (AIMv2 model)
  - [Aimv2VisionConfig](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionConfig) configuration class: [Aimv2VisionModel](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionModel) (Aimv2VisionModel model)
  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertModel` (ALBERT model)
  - [AlignConfig](/docs/transformers/main/en/model_doc/align#transformers.AlignConfig) configuration class: [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model)
  - [AltCLIPConfig](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig) configuration class: [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model)
  - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusModel](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusModel) (Apertus model)
  - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeModel](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeModel) (Arcee model)
  - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaModel) (Aria model)
  - [AriaTextConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextConfig) configuration class: [AriaTextModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextModel) (AriaText model)
  - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
  - [AudioFlamingo3EncoderConfig](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3EncoderConfig) configuration class: [AudioFlamingo3Encoder](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Encoder) (AudioFlamingo3Encoder model)
  - [AutoformerConfig](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerConfig) configuration class: [AutoformerModel](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerModel) (Autoformer model)
  - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionModel](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionModel) (AyaVision model)
  - [BambaConfig](/docs/transformers/main/en/model_doc/bamba#transformers.BambaConfig) configuration class: [BambaModel](/docs/transformers/main/en/model_doc/bamba#transformers.BambaModel) (Bamba model)
  - [BarkConfig](/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig) configuration class: [BarkModel](/docs/transformers/main/en/model_doc/bark#transformers.BarkModel) (Bark model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartModel](/docs/transformers/main/en/model_doc/bart#transformers.BartModel) (BART model)
  - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitModel](/docs/transformers/main/en/model_doc/beit#transformers.BeitModel) (BEiT model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertModel](/docs/transformers/main/en/model_doc/bert#transformers.BertModel) (BERT model)
  - [BertGenerationConfig](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig) configuration class: [BertGenerationEncoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationEncoder) (Bert Generation model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdModel](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdModel) (BigBird model)
  - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusModel](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel) (BigBird-Pegasus model)
  - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptModel](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel) (BioGpt model)
  - [BitConfig](/docs/transformers/main/en/model_doc/bit#transformers.BitConfig) configuration class: [BitModel](/docs/transformers/main/en/model_doc/bit#transformers.BitModel) (BiT model)
  - [BitNetConfig](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetConfig) configuration class: [BitNetModel](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetModel) (BitNet model)
  - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotModel](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotModel) (Blenderbot model)
  - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallModel](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel) (BlenderbotSmall model)
  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2Model](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model) (BLIP-2 model)
  - [Blip2QFormerConfig](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerConfig) configuration class: [Blip2QFormerModel](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerModel) (BLIP-2 QFormer model)
  - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomModel](/docs/transformers/main/en/model_doc/bloom#transformers.BloomModel) (BLOOM model)
  - [BltConfig](/docs/transformers/main/en/model_doc/blt#transformers.BltConfig) configuration class: [BltModel](/docs/transformers/main/en/model_doc/blt#transformers.BltModel) (Blt model)
  - [BridgeTowerConfig](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerConfig) configuration class: [BridgeTowerModel](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerModel) (BridgeTower model)
  - [BrosConfig](/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig) configuration class: [BrosModel](/docs/transformers/main/en/model_doc/bros#transformers.BrosModel) (BROS model)
  - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model)
  - [CLIPSegConfig](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig) configuration class: [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model)
  - [CLIPTextConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextConfig) configuration class: [CLIPTextModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextModel) (CLIPTextModel model)
  - [CLIPVisionConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionConfig) configuration class: [CLIPVisionModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionModel) (CLIPVisionModel model)
  - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLModel) (CTRL model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertModel](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertModel) (CamemBERT model)
  - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineModel](/docs/transformers/main/en/model_doc/canine#transformers.CanineModel) (CANINE model)
  - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonModel](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonModel) (Chameleon model)
  - [ChineseCLIPConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig) configuration class: [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model)
  - [ChineseCLIPVisionConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionConfig) configuration class: [ChineseCLIPVisionModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionModel) (ChineseCLIPVisionModel model)
  - [ClapConfig](/docs/transformers/main/en/model_doc/clap#transformers.ClapConfig) configuration class: [ClapModel](/docs/transformers/main/en/model_doc/clap#transformers.ClapModel) (CLAP model)
  - [ClvpConfig](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpConfig) configuration class: [ClvpModelForConditionalGeneration](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration) (CLVP model)
  - [CodeGenConfig](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig) configuration class: [CodeGenModel](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenModel) (CodeGen model)
  - [Cohere2Config](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Config) configuration class: [Cohere2Model](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Model) (Cohere2 model)
  - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionModel](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionModel) (Cohere2Vision model)
  - [CohereConfig](/docs/transformers/main/en/model_doc/cohere#transformers.CohereConfig) configuration class: [CohereModel](/docs/transformers/main/en/model_doc/cohere#transformers.CohereModel) (Cohere model)
  - [ConditionalDetrConfig](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) configuration class: [ConditionalDetrModel](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel) (Conditional DETR model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertModel](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertModel) (ConvBERT model)
  - [ConvNextConfig](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig) configuration class: [ConvNextModel](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextModel) (ConvNeXT model)
  - [ConvNextV2Config](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config) configuration class: [ConvNextV2Model](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Model) (ConvNeXTV2 model)
  - [CpmAntConfig](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig) configuration class: [CpmAntModel](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntModel) (CPM-Ant model)
  - [CsmConfig](/docs/transformers/main/en/model_doc/csm#transformers.CsmConfig) configuration class: [CsmForConditionalGeneration](/docs/transformers/main/en/model_doc/csm#transformers.CsmForConditionalGeneration) (CSM model)
  - [CvtConfig](/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig) configuration class: [CvtModel](/docs/transformers/main/en/model_doc/cvt#transformers.CvtModel) (CvT model)
  - [CwmConfig](/docs/transformers/main/en/model_doc/cwm#transformers.CwmConfig) configuration class: [CwmModel](/docs/transformers/main/en/model_doc/cwm#transformers.CwmModel) (Code World Model (CWM) model)
  - [DFineConfig](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineConfig) configuration class: [DFineModel](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineModel) (D-FINE model)
  - [DINOv3ConvNextConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextConfig) configuration class: [DINOv3ConvNextModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextModel) (DINOv3 ConvNext model)
  - [DINOv3ViTConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTConfig) configuration class: [DINOv3ViTModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTModel) (DINOv3 ViT model)
  - [DPRConfig](/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig) configuration class: [DPRQuestionEncoder](/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder) (DPR model)
  - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTModel](/docs/transformers/main/en/model_doc/dpt#transformers.DPTModel) (DPT model)
  - [DabDetrConfig](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrConfig) configuration class: [DabDetrModel](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrModel) (DAB-DETR model)
  - [DacConfig](/docs/transformers/main/en/model_doc/dac#transformers.DacConfig) configuration class: [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model)
  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel) (Data2VecAudio model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextModel) (Data2VecText model)
  - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel) (Data2VecVision model)
  - [DbrxConfig](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxConfig) configuration class: [DbrxModel](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxModel) (DBRX model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaModel](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaModel) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2Model](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Model) (DeBERTa-v2 model)
  - [DecisionTransformerConfig](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig) configuration class: [DecisionTransformerModel](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerModel) (Decision Transformer model)
  - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2Model](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Model) (DeepSeek-V2 model)
  - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3Model](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Model) (DeepSeek-V3 model)
  - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLModel](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLModel) (DeepseekVL model)
  - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridModel](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridModel) (DeepseekVLHybrid model)
  - [DeformableDetrConfig](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig) configuration class: [DeformableDetrModel](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrModel) (Deformable DETR model)
  - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTModel](/docs/transformers/main/en/model_doc/deit#transformers.DeiTModel) (DeiT model)
  - [DepthProConfig](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProConfig) configuration class: [DepthProModel](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProModel) (DepthPro model)
  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrModel](/docs/transformers/main/en/model_doc/detr#transformers.DetrModel) (DETR model)
  - [DiaConfig](/docs/transformers/main/en/model_doc/dia#transformers.DiaConfig) configuration class: [DiaModel](/docs/transformers/main/en/model_doc/dia#transformers.DiaModel) (Dia model)
  - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaModel](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaModel) (DiffLlama model)
  - [DinatConfig](/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig) configuration class: [DinatModel](/docs/transformers/main/en/model_doc/dinat#transformers.DinatModel) (DiNAT model)
  - [Dinov2Config](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config) configuration class: [Dinov2Model](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model) (DINOv2 model)
  - [Dinov2WithRegistersConfig](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig) configuration class: [Dinov2WithRegistersModel](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersModel) (DINOv2 with Registers model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertModel](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel) (DistilBERT model)
  - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeModel](/docs/transformers/main/en/model_doc/doge#transformers.DogeModel) (Doge model)
  - [DonutSwinConfig](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig) configuration class: [DonutSwinModel](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinModel) (DonutSwin model)
  - [Dots1Config](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Config) configuration class: [Dots1Model](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Model) (dots1 model)
  - [EdgeTamConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamConfig) configuration class: [EdgeTamModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamModel) (EdgeTAM model)
  - [EdgeTamVideoConfig](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoConfig) configuration class: [EdgeTamVideoModel](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoModel) (EdgeTamVideo model)
  - [EdgeTamVisionConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionConfig) configuration class: [EdgeTamVisionModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionModel) (EdgeTamVisionModel model)
  - [EfficientLoFTRConfig](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig) configuration class: [EfficientLoFTRModel](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel) (EfficientLoFTR model)
  - [EfficientNetConfig](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig) configuration class: [EfficientNetModel](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetModel) (EfficientNet model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraModel](/docs/transformers/main/en/model_doc/electra#transformers.ElectraModel) (ELECTRA model)
  - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3Model](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Model) (Emu3 model)
  - [EncodecConfig](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecConfig) configuration class: [EncodecModel](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel) (EnCodec model)
  - [Ernie4_5Config](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Config) configuration class: [Ernie4_5Model](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Model) (Ernie4_5 model)
  - [Ernie4_5_MoeConfig](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig) configuration class: [Ernie4_5_MoeModel](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeModel) (Ernie4_5_MoE model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieModel](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieModel) (ERNIE model)
  - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmModel](/docs/transformers/main/en/model_doc/esm#transformers.EsmModel) (ESM model)
  - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaModel](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaModel) (Evolla model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4Model](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Model) (EXAONE-4.0 model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetModel](/docs/transformers/main/en/model_doc/fnet#transformers.FNetModel) (FNet model)
  - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTModel](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTModel) (FairSeq Machine-Translation model)
  - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconModel](/docs/transformers/main/en/model_doc/falcon#transformers.FalconModel) (Falcon model)
  - [FalconH1Config](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Config) configuration class: [FalconH1Model](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Model) (FalconH1 model)
  - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaModel](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaModel) (FalconMamba model)
  - [FastSpeech2ConformerConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig) configuration class: [FastSpeech2ConformerModel](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel) (FastSpeech2Conformer model)
  - [FastSpeech2ConformerWithHifiGanConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGanConfig) configuration class: [FastSpeech2ConformerWithHifiGan](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGan) (FastSpeech2ConformerWithHifiGan model)
  - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmModel](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmModel) (FastVlm model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertModel) (FlauBERT model)
  - [FlavaConfig](/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig) configuration class: [FlavaModel](/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel) (FLAVA model)
  - [FlexOlmoConfig](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoConfig) configuration class: [FlexOlmoModel](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoModel) (FlexOlmo model)
  - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2Model](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Model) (Florence2 model)
  - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetModel](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel) (FocalNet model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel) or [FunnelBaseModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel) (Funnel Transformer model)
  - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuModel](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuModel) (Fuyu model)
  - [GLPNConfig](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig) configuration class: [GLPNModel](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNModel) (GLPN model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2Model](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model) (OpenAI GPT-2 model)
  - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeModel](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel) (GPTBigCode model)
  - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJModel](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJModel) (GPT-J model)
  - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoModel](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoModel) (GPT Neo model)
  - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXModel](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXModel) (GPT NeoX model)
  - [GPTNeoXJapaneseConfig](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig) configuration class: [GPTNeoXJapaneseModel](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel) (GPT NeoX Japanese model)
  - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2Model](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Model) (Gemma2 model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3Model](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Model) (Gemma3ForConditionalGeneration model)
  - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3TextModel](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextModel) (Gemma3ForCausalLM model)
  - [Gemma3nAudioConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nAudioConfig) configuration class: `Gemma3nAudioEncoder` (Gemma3nAudioEncoder model)
  - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nModel) (Gemma3nForConditionalGeneration model)
  - [Gemma3nTextConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextConfig) configuration class: [Gemma3nTextModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextModel) (Gemma3nForCausalLM model)
  - [Gemma3nVisionConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nVisionConfig) configuration class: [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model)
  - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaModel](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaModel) (Gemma model)
  - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitModel](/docs/transformers/main/en/model_doc/git#transformers.GitModel) (GIT model)
  - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VModel](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VModel) (Glm46V model)
  - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4Model](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Model) (GLM4 model)
  - [Glm4MoeConfig](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeConfig) configuration class: [Glm4MoeModel](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeModel) (Glm4MoE model)
  - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vModel) (GLM4V model)
  - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeModel) (GLM4VMOE model)
  - [Glm4vMoeTextConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextConfig) configuration class: [Glm4vMoeTextModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextModel) (GLM4VMOE model)
  - [Glm4vMoeVisionConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionConfig) configuration class: [Glm4vMoeVisionModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionModel) (Glm4vMoeVisionModel model)
  - [Glm4vTextConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextConfig) configuration class: [Glm4vTextModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextModel) (GLM4V model)
  - [Glm4vVisionConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionConfig) configuration class: [Glm4vVisionModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionModel) (Glm4vVisionModel model)
  - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmModel](/docs/transformers/main/en/model_doc/glm#transformers.GlmModel) (GLM model)
  - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2Model](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Model) (GOT-OCR2 model)
  - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssModel](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssModel) (GptOss model)
  - [GraniteConfig](/docs/transformers/main/en/model_doc/granite#transformers.GraniteConfig) configuration class: [GraniteModel](/docs/transformers/main/en/model_doc/granite#transformers.GraniteModel) (Granite model)
  - [GraniteMoeConfig](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeConfig) configuration class: [GraniteMoeModel](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeModel) (GraniteMoeMoe model)
  - [GraniteMoeHybridConfig](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig) configuration class: [GraniteMoeHybridModel](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridModel) (GraniteMoeHybrid model)
  - [GraniteMoeSharedConfig](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig) configuration class: [GraniteMoeSharedModel](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedModel) (GraniteMoeSharedMoe model)
  - [GroundingDinoConfig](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoConfig) configuration class: [GroundingDinoModel](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoModel) (Grounding DINO model)
  - [GroupViTConfig](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig) configuration class: [GroupViTModel](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTModel) (GroupViT model)
  - [HGNetV2Config](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Config) configuration class: [HGNetV2Backbone](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone) (HGNet-V2 model)
  - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumModel](/docs/transformers/main/en/model_doc/helium#transformers.HeliumModel) (Helium model)
  - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraModel](/docs/transformers/main/en/model_doc/hiera#transformers.HieraModel) (Hiera model)
  - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertModel](/docs/transformers/main/en/model_doc/hubert#transformers.HubertModel) (Hubert model)
  - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Model) (HunYuanDenseV1 model)
  - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Model) (HunYuanMoeV1 model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertModel](/docs/transformers/main/en/model_doc/ibert#transformers.IBertModel) (I-BERT model)
  - [IJepaConfig](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaConfig) configuration class: [IJepaModel](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaModel) (I-JEPA model)
  - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2Model](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Model) (Idefics2 model)
  - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3Model](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Model) (Idefics3 model)
  - [Idefics3VisionConfig](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionConfig) configuration class: [Idefics3VisionTransformer](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionTransformer) (Idefics3VisionTransformer model)
  - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsModel](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsModel) (IDEFICS model)
  - [ImageGPTConfig](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig) configuration class: [ImageGPTModel](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTModel) (ImageGPT model)
  - [InformerConfig](/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig) configuration class: [InformerModel](/docs/transformers/main/en/model_doc/informer#transformers.InformerModel) (Informer model)
  - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipModel](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipModel) (InstructBLIP model)
  - [InstructBlipVideoConfig](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig) configuration class: [InstructBlipVideoModel](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel) (InstructBlipVideo model)
  - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLModel) (InternVL model)
  - [InternVLVisionConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionConfig) configuration class: [InternVLVisionModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionModel) (InternVLVision model)
  - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaModel](/docs/transformers/main/en/model_doc/jamba#transformers.JambaModel) (Jamba model)
  - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusModel](/docs/transformers/main/en/model_doc/janus#transformers.JanusModel) (Janus model)
  - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeModel](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeModel) (JetMoe model)
  - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2Model](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Model) (KOSMOS-2 model)
  - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5Model](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model) (KOSMOS-2.5 model)
  - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextModel](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextModel) (KyutaiSpeechToText model)
  - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDModel](/docs/transformers/main/en/model_doc/led#transformers.LEDModel) (LED model)
  - [LasrCTCConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrCTCConfig) configuration class: [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model)
  - [LasrEncoderConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoderConfig) configuration class: [LasrEncoder](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoder) (LasrEncoder model)
  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMModel](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMModel) (LayoutLM model)
  - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2Model](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model) (LayoutLMv2 model)
  - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3Model](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model) (LayoutLMv3 model)
  - [LevitConfig](/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig) configuration class: [LevitModel](/docs/transformers/main/en/model_doc/levit#transformers.LevitModel) (LeViT model)
  - [Lfm2Config](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Config) configuration class: [Lfm2Model](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Model) (Lfm2 model)
  - [Lfm2MoeConfig](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeConfig) configuration class: [Lfm2MoeModel](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeModel) (Lfm2Moe model)
  - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlModel](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlModel) (Lfm2Vl model)
  - [LightGlueConfig](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueConfig) configuration class: [LightGlueForKeypointMatching](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching) (LightGlue model)
  - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltModel](/docs/transformers/main/en/model_doc/lilt#transformers.LiltModel) (LiLT model)
  - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
  - [Llama4TextConfig](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextConfig) configuration class: [Llama4TextModel](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextModel) (Llama4ForCausalLM model)
  - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaModel](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel) (LLaMA model)
  - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaModel](/docs/transformers/main/en/model_doc/llava#transformers.LlavaModel) (LLaVa model)
  - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextModel](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextModel) (LLaVA-NeXT model)
  - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoModel](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoModel) (LLaVa-NeXT-Video model)
  - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionModel](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionModel) (LLaVA-Onevision model)
  - [LongT5Config](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config) configuration class: [LongT5Model](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Model) (LongT5 model)
  - [LongcatFlashConfig](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashConfig) configuration class: [LongcatFlashModel](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashModel) (LongCatFlash model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerModel](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerModel) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeModel](/docs/transformers/main/en/model_doc/luke#transformers.LukeModel) (LUKE model)
  - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertModel](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertModel) (LXMERT model)
  - [M2M100Config](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config) configuration class: [M2M100Model](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Model) (M2M100 model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartModel](/docs/transformers/main/en/model_doc/mbart#transformers.MBartModel) (mBART model)
  - [MLCDVisionConfig](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionConfig) configuration class: [MLCDVisionModel](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionModel) (MLCD model)
  - [MMGroundingDinoConfig](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig) configuration class: [MMGroundingDinoModel](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel) (MM Grounding DINO model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetModel](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetModel) (MPNet model)
  - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5Model](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model) (MT5 model)
  - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2Model](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Model) (mamba2 model)
  - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaModel](/docs/transformers/main/en/model_doc/mamba#transformers.MambaModel) (Mamba model)
  - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianModel) (Marian model)
  - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMModel](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMModel) (MarkupLM model)
  - [Mask2FormerConfig](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig) configuration class: [Mask2FormerModel](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerModel) (Mask2Former model)
  - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerModel](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerModel) (MaskFormer model)
  - `MaskFormerSwinConfig` configuration class: `MaskFormerSwinModel` (MaskFormerSwin model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertModel](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertModel) (Megatron-BERT model)
  - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model)
  - [MgpstrConfig](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrConfig) configuration class: [MgpstrForSceneTextRecognition](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition) (MGP-STR model)
  - [MimiConfig](/docs/transformers/main/en/model_doc/mimi#transformers.MimiConfig) configuration class: [MimiModel](/docs/transformers/main/en/model_doc/mimi#transformers.MimiModel) (Mimi model)
  - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxModel](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxModel) (MiniMax model)
  - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3Model](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Model) (Ministral3 model)
  - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralModel](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralModel) (Ministral model)
  - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3Model](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Model) (Mistral3 model)
  - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralModel](/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel) (Mistral model)
  - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralModel](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralModel) (Mixtral model)
  - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaModel](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaModel) (Mllama model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertModel](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertModel) (MobileBERT model)
  - [MobileNetV1Config](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config) configuration class: [MobileNetV1Model](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model) (MobileNetV1 model)
  - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2Model](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model) (MobileNetV2 model)
  - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTModel](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTModel) (MobileViT model)
  - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2Model](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model) (MobileViTV2 model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertModel](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertModel) (ModernBERT model)
  - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderModel](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderModel) (ModernBertDecoder model)
  - [MoonshineConfig](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineConfig) configuration class: [MoonshineModel](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineModel) (Moonshine model)
  - [MoshiConfig](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiConfig) configuration class: [MoshiModel](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiModel) (Moshi model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptModel](/docs/transformers/main/en/model_doc/mpt#transformers.MptModel) (MPT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraModel](/docs/transformers/main/en/model_doc/mra#transformers.MraModel) (MRA model)
  - [MusicgenConfig](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig) configuration class: [MusicgenModel](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenModel) (MusicGen model)
  - [MusicgenMelodyConfig](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig) configuration class: [MusicgenMelodyModel](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyModel) (MusicGen Melody model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpModel](/docs/transformers/main/en/model_doc/mvp#transformers.MvpModel) (MVP model)
  - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatModel](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatModel) (NanoChat model)
  - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronModel](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronModel) (Nemotron model)
  - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeModel](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeModel) (NLLB-MOE model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerModel](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerModel) (Nystr√∂mformer model)
  - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTModel](/docs/transformers/main/en/model_doc/opt#transformers.OPTModel) (OPT model)
  - [Olmo2Config](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Config) configuration class: [Olmo2Model](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Model) (OLMo2 model)
  - [Olmo3Config](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Config) configuration class: [Olmo3Model](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Model) (Olmo3 model)
  - [OlmoConfig](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoConfig) configuration class: [OlmoModel](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoModel) (OLMo model)
  - [OlmoeConfig](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeConfig) configuration class: [OlmoeModel](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeModel) (OLMoE model)
  - [OmDetTurboConfig](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig) configuration class: [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model)
  - [OneFormerConfig](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig) configuration class: [OneFormerModel](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerModel) (OneFormer model)
  - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTModel) (OpenAI GPT model)
  - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2Model](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Model) (Ovis2 model)
  - [OwlViTConfig](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig) configuration class: [OwlViTModel](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel) (OWL-ViT model)
  - [Owlv2Config](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config) configuration class: [Owlv2Model](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Model) (OWLv2 model)
  - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartModel](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartModel) (PLBart model)
  - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaModel](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaModel) (PaliGemma model)
  - [ParakeetCTCConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetCTCConfig) configuration class: [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model)
  - [ParakeetEncoderConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoderConfig) configuration class: [ParakeetEncoder](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoder) (ParakeetEncoder model)
  - [PatchTSMixerConfig](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig) configuration class: [PatchTSMixerModel](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel) (PatchTSMixer model)
  - [PatchTSTConfig](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTConfig) configuration class: [PatchTSTModel](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTModel) (PatchTST model)
  - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusModel](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusModel) (Pegasus model)
  - [PegasusXConfig](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig) configuration class: [PegasusXModel](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXModel) (PEGASUS-X model)
  - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverModel](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverModel) (Perceiver model)
  - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMModel](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMModel) (PerceptionLM model)
  - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonModel](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonModel) (Persimmon model)
  - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3Model](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Model) (Phi3 model)
  - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalModel](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalModel) (Phi4Multimodal model)
  - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiModel](/docs/transformers/main/en/model_doc/phi#transformers.PhiModel) (Phi model)
  - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeModel](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeModel) (Phimoe model)
  - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [PixtralVisionModel](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionModel) (Pixtral model)
  - [PoolFormerConfig](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig) configuration class: [PoolFormerModel](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerModel) (PoolFormer model)
  - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetModel](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel) (ProphetNet model)
  - [PvtConfig](/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig) configuration class: [PvtModel](/docs/transformers/main/en/model_doc/pvt#transformers.PvtModel) (PVT model)
  - [PvtV2Config](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Config) configuration class: [PvtV2Model](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Model) (PVTv2 model)
  - [Qwen2AudioEncoderConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig) configuration class: [Qwen2AudioEncoder](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder) (Qwen2AudioEncoder model)
  - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2Model](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Model) (Qwen2 model)
  - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeModel](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeModel) (Qwen2MoE model)
  - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLModel) (Qwen2VL model)
  - [Qwen2VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextConfig) configuration class: [Qwen2VLTextModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextModel) (Qwen2VL model)
  - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLModel) (Qwen2_5_VL model)
  - [Qwen2_5_VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextConfig) configuration class: [Qwen2_5_VLTextModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextModel) (Qwen2_5_VL model)
  - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3Model](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Model) (Qwen3 model)
  - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeModel](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeModel) (Qwen3MoE model)
  - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextModel](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextModel) (Qwen3Next model)
  - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLModel) (Qwen3VL model)
  - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeModel) (Qwen3VLMoe model)
  - [Qwen3VLMoeTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextConfig) configuration class: [Qwen3VLMoeTextModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextModel) (Qwen3VLMoe model)
  - [Qwen3VLTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextConfig) configuration class: [Qwen3VLTextModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextModel) (Qwen3VL model)
  - [RTDetrConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrConfig) configuration class: [RTDetrModel](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrModel) (RT-DETR model)
  - [RTDetrV2Config](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) configuration class: [RTDetrV2Model](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model) (RT-DETRv2 model)
  - [RecurrentGemmaConfig](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig) configuration class: [RecurrentGemmaModel](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel) (RecurrentGemma model)
  - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerModel](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModel) (Reformer model)
  - [RegNetConfig](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig) configuration class: [RegNetModel](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetModel) (RegNet model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertModel](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertModel) (RemBERT model)
  - [ResNetConfig](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig) configuration class: [ResNetModel](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel) (ResNet model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertModel](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertModel) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerModel](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerModel) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaModel](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormModel](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel) (RoBERTa-PreLayerNorm model)
  - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvModel](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvModel) (RWKV model)
  - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWModel](/docs/transformers/main/en/model_doc/sew#transformers.SEWModel) (SEW model)
  - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDModel](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDModel) (SEW-D model)
  - [Sam2Config](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Config) configuration class: [Sam2Model](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Model) (SAM2 model)
  - [Sam2HieraDetConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetConfig) configuration class: [Sam2HieraDetModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetModel) (Sam2HieraDetModel model)
  - [Sam2VideoConfig](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoConfig) configuration class: [Sam2VideoModel](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoModel) (Sam2VideoModel model)
  - [Sam2VisionConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionConfig) configuration class: [Sam2VisionModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionModel) (Sam2VisionModel model)
  - [Sam3Config](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Config) configuration class: [Sam3Model](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Model) (SAM3 model)
  - [Sam3TrackerConfig](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerConfig) configuration class: [Sam3TrackerModel](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerModel) (Sam3Tracker model)
  - [Sam3TrackerVideoConfig](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoConfig) configuration class: [Sam3TrackerVideoModel](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoModel) (Sam3TrackerVideo model)
  - [Sam3ViTConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTConfig) configuration class: [Sam3ViTModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTModel) (Sam3ViTModel model)
  - [Sam3VideoConfig](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoConfig) configuration class: [Sam3VideoModel](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoModel) (Sam3VideoModel model)
  - [Sam3VisionConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionConfig) configuration class: [Sam3VisionModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionModel) (Sam3VisionModel model)
  - [SamConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamConfig) configuration class: [SamModel](/docs/transformers/main/en/model_doc/sam#transformers.SamModel) (SAM model)
  - [SamHQConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQConfig) configuration class: [SamHQModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQModel) (SAM-HQ model)
  - [SamHQVisionConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionConfig) configuration class: [SamHQVisionModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionModel) (SamHQVisionModel model)
  - [SamVisionConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionConfig) configuration class: [SamVisionModel](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionModel) (SamVisionModel model)
  - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TModel](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel) (SeamlessM4T model)
  - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2Model](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model) (SeamlessM4Tv2 model)
  - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssModel](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssModel) (SeedOss model)
  - [SegGptConfig](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptConfig) configuration class: [SegGptModel](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptModel) (SegGPT model)
  - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerModel](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerModel) (SegFormer model)
  - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model)
  - [Siglip2VisionConfig](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionConfig) configuration class: [Siglip2VisionModel](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionModel) (Siglip2VisionModel model)
  - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model)
  - [SiglipVisionConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionConfig) configuration class: [SiglipVisionModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionModel) (SiglipVisionModel model)
  - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3Model](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Model) (SmolLM3 model)
  - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMModel](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMModel) (SmolVLM model)
  - [SmolVLMVisionConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionConfig) configuration class: [SmolVLMVisionTransformer](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionTransformer) (SmolVLMVisionTransformer model)
  - [Speech2TextConfig](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig) configuration class: [Speech2TextModel](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextModel) (Speech2Text model)
  - [SpeechT5Config](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config) configuration class: [SpeechT5Model](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Model) (SpeechT5 model)
  - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterModel](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterModel) (Splinter model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertModel](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertModel) (SqueezeBERT model)
  - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmModel](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel) (StableLm model)
  - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2Model](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Model) (Starcoder2 model)
  - [SwiftFormerConfig](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig) configuration class: [SwiftFormerModel](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel) (SwiftFormer model)
  - [Swin2SRConfig](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRConfig) configuration class: [Swin2SRModel](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRModel) (Swin2SR model)
  - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinModel](/docs/transformers/main/en/model_doc/swin#transformers.SwinModel) (Swin Transformer model)
  - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2Model](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model) (Swin Transformer V2 model)
  - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersModel](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersModel) (SwitchTransformers model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5Model](/docs/transformers/main/en/model_doc/t5#transformers.T5Model) (T5 model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2Model](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Model) (T5Gemma2 model)
  - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaModel](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaModel) (T5Gemma model)
  - [TableTransformerConfig](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig) configuration class: [TableTransformerModel](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerModel) (Table Transformer model)
  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasModel](/docs/transformers/main/en/model_doc/tapas#transformers.TapasModel) (TAPAS model)
  - [TextNetConfig](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetConfig) configuration class: [TextNetModel](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetModel) (TextNet model)
  - [TimeSeriesTransformerConfig](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig) configuration class: [TimeSeriesTransformerModel](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel) (Time Series Transformer model)
  - [TimesFmConfig](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmConfig) configuration class: [TimesFmModel](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModel) (TimesFm model)
  - [TimesformerConfig](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig) configuration class: [TimesformerModel](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerModel) (TimeSformer model)
  - [TimmBackboneConfig](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackboneConfig) configuration class: [TimmBackbone](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackbone) (TimmBackbone model)
  - [TimmWrapperConfig](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig) configuration class: [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model)
  - [TvpConfig](/docs/transformers/main/en/model_doc/tvp#transformers.TvpConfig) configuration class: [TvpModel](/docs/transformers/main/en/model_doc/tvp#transformers.TvpModel) (TVP model)
  - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5Model](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Model) (UMT5 model)
  - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopModel](/docs/transformers/main/en/model_doc/udop#transformers.UdopModel) (UDOP model)
  - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechModel](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechModel) (UniSpeech model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatModel](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel) (UniSpeechSat model)
  - [UnivNetConfig](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetConfig) configuration class: [UnivNetModel](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetModel) (UnivNet model)
  - [VJEPA2Config](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Config) configuration class: [VJEPA2Model](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Model) (VJEPA2Model model)
  - [VaultGemmaConfig](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaConfig) configuration class: [VaultGemmaModel](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaModel) (VaultGemma model)
  - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTModel](/docs/transformers/main/en/model_doc/vit#transformers.ViTModel) (ViT model)
  - [ViTMAEConfig](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig) configuration class: [ViTMAEModel](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEModel) (ViTMAE model)
  - [ViTMSNConfig](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig) configuration class: [ViTMSNModel](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel) (ViTMSN model)
  - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3Model](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Model) (VideoLlama3 model)
  - [VideoLlama3VisionConfig](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionConfig) configuration class: [VideoLlama3VisionModel](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionModel) (VideoLlama3Vision model)
  - [VideoLlavaConfig](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaConfig) configuration class: [VideoLlavaModel](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaModel) (VideoLlava model)
  - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEModel](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEModel) (VideoMAE model)
  - [ViltConfig](/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig) configuration class: [ViltModel](/docs/transformers/main/en/model_doc/vilt#transformers.ViltModel) (ViLT model)
  - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaModel](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaModel) (VipLlava model)
  - [VisionTextDualEncoderConfig](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig) configuration class: [VisionTextDualEncoderModel](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) (VisionTextDualEncoder model)
  - [VisualBertConfig](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig) configuration class: [VisualBertModel](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertModel) (VisualBERT model)
  - [VitDetConfig](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig) configuration class: [VitDetModel](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel) (VitDet model)
  - [VitsConfig](/docs/transformers/main/en/model_doc/vits#transformers.VitsConfig) configuration class: [VitsModel](/docs/transformers/main/en/model_doc/vits#transformers.VitsModel) (VITS model)
  - [VivitConfig](/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig) configuration class: [VivitModel](/docs/transformers/main/en/model_doc/vivit#transformers.VivitModel) (ViViT model)
  - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
  - [VoxtralEncoderConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoderConfig) configuration class: [VoxtralEncoder](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoder) (Voxtral Encoder model)
  - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertModel](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel) (Wav2Vec2-BERT model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2Model](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerModel](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel) (Wav2Vec2-Conformer model)
  - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMModel](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMModel) (WavLM model)
  - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperModel](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperModel) (Whisper model)
  - [XCLIPConfig](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPConfig) configuration class: [XCLIPModel](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPModel) (X-CLIP model)
  - [XGLMConfig](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig) configuration class: [XGLMModel](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMModel) (XGLM model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMModel) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaModel](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaModel) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLModel](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel) (XLNet model)
  - [XcodecConfig](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecConfig) configuration class: [XcodecModel](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecModel) (X-CODEC model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodModel](/docs/transformers/main/en/model_doc/xmod#transformers.XmodModel) (X-MOD model)
  - [YolosConfig](/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig) configuration class: [YolosModel](/docs/transformers/main/en/model_doc/yolos#transformers.YolosModel) (YOLOS model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoModel](/docs/transformers/main/en/model_doc/yoso#transformers.YosoModel) (YOSO model)
  - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2Model](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Model) (Zamba2 model)
  - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaModel](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaModel) (Zamba model)
  - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMModel](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMModel) (xLSTM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the base model classes of the library from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModel

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModel.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [ASTConfig](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig) configuration class: [ASTModel](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel) (Audio Spectrogram Transformer model) - [AfmoeConfig](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeConfig) configuration class: [AfmoeModel](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeModel) (AFMoE model) - [Aimv2Config](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Config) configuration class: [Aimv2Model](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Model) (AIMv2 model) - [Aimv2VisionConfig](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionConfig) configuration class: [Aimv2VisionModel](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionModel) (Aimv2VisionModel model) - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertModel` (ALBERT model) - [AlignConfig](/docs/transformers/main/en/model_doc/align#transformers.AlignConfig) configuration class: [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model) - [AltCLIPConfig](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig) configuration class: [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model) - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusModel](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusModel) (Apertus model) - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeModel](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeModel) (Arcee model) - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaModel) (Aria model) - [AriaTextConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextConfig) configuration class: [AriaTextModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextModel) (AriaText model) - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model) - [AudioFlamingo3EncoderConfig](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3EncoderConfig) configuration class: [AudioFlamingo3Encoder](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Encoder) (AudioFlamingo3Encoder model) - [AutoformerConfig](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerConfig) configuration class: [AutoformerModel](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerModel) (Autoformer model) - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionModel](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionModel) (AyaVision model) - [BambaConfig](/docs/transformers/main/en/model_doc/bamba#transformers.BambaConfig) configuration class: [BambaModel](/docs/transformers/main/en/model_doc/bamba#transformers.BambaModel) (Bamba model) - [BarkConfig](/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig) configuration class: [BarkModel](/docs/transformers/main/en/model_doc/bark#transformers.BarkModel) (Bark model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartModel](/docs/transformers/main/en/model_doc/bart#transformers.BartModel) (BART model) - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitModel](/docs/transformers/main/en/model_doc/beit#transformers.BeitModel) (BEiT model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertModel](/docs/transformers/main/en/model_doc/bert#transformers.BertModel) (BERT model) - [BertGenerationConfig](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig) configuration class: [BertGenerationEncoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationEncoder) (Bert Generation model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdModel](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdModel) (BigBird model) - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusModel](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel) (BigBird-Pegasus model) - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptModel](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel) (BioGpt model) - [BitConfig](/docs/transformers/main/en/model_doc/bit#transformers.BitConfig) configuration class: [BitModel](/docs/transformers/main/en/model_doc/bit#transformers.BitModel) (BiT model) - [BitNetConfig](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetConfig) configuration class: [BitNetModel](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetModel) (BitNet model) - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotModel](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotModel) (Blenderbot model) - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallModel](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel) (BlenderbotSmall model) - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2Model](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model) (BLIP-2 model) - [Blip2QFormerConfig](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerConfig) configuration class: [Blip2QFormerModel](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerModel) (BLIP-2 QFormer model) - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomModel](/docs/transformers/main/en/model_doc/bloom#transformers.BloomModel) (BLOOM model) - [BltConfig](/docs/transformers/main/en/model_doc/blt#transformers.BltConfig) configuration class: [BltModel](/docs/transformers/main/en/model_doc/blt#transformers.BltModel) (Blt model) - [BridgeTowerConfig](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerConfig) configuration class: [BridgeTowerModel](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerModel) (BridgeTower model) - [BrosConfig](/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig) configuration class: [BrosModel](/docs/transformers/main/en/model_doc/bros#transformers.BrosModel) (BROS model) - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model) - [CLIPSegConfig](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig) configuration class: [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model) - [CLIPTextConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextConfig) configuration class: [CLIPTextModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextModel) (CLIPTextModel model) - [CLIPVisionConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionConfig) configuration class: [CLIPVisionModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionModel) (CLIPVisionModel model) - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLModel) (CTRL model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertModel](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertModel) (CamemBERT model) - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineModel](/docs/transformers/main/en/model_doc/canine#transformers.CanineModel) (CANINE model) - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonModel](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonModel) (Chameleon model) - [ChineseCLIPConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig) configuration class: [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model) - [ChineseCLIPVisionConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionConfig) configuration class: [ChineseCLIPVisionModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionModel) (ChineseCLIPVisionModel model) - [ClapConfig](/docs/transformers/main/en/model_doc/clap#transformers.ClapConfig) configuration class: [ClapModel](/docs/transformers/main/en/model_doc/clap#transformers.ClapModel) (CLAP model) - [ClvpConfig](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpConfig) configuration class: [ClvpModelForConditionalGeneration](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration) (CLVP model) - [CodeGenConfig](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig) configuration class: [CodeGenModel](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenModel) (CodeGen model) - [Cohere2Config](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Config) configuration class: [Cohere2Model](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Model) (Cohere2 model) - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionModel](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionModel) (Cohere2Vision model) - [CohereConfig](/docs/transformers/main/en/model_doc/cohere#transformers.CohereConfig) configuration class: [CohereModel](/docs/transformers/main/en/model_doc/cohere#transformers.CohereModel) (Cohere model) - [ConditionalDetrConfig](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) configuration class: [ConditionalDetrModel](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel) (Conditional DETR model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertModel](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertModel) (ConvBERT model) - [ConvNextConfig](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig) configuration class: [ConvNextModel](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextModel) (ConvNeXT model) - [ConvNextV2Config](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config) configuration class: [ConvNextV2Model](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Model) (ConvNeXTV2 model) - [CpmAntConfig](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig) configuration class: [CpmAntModel](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntModel) (CPM-Ant model) - [CsmConfig](/docs/transformers/main/en/model_doc/csm#transformers.CsmConfig) configuration class: [CsmForConditionalGeneration](/docs/transformers/main/en/model_doc/csm#transformers.CsmForConditionalGeneration) (CSM model) - [CvtConfig](/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig) configuration class: [CvtModel](/docs/transformers/main/en/model_doc/cvt#transformers.CvtModel) (CvT model) - [CwmConfig](/docs/transformers/main/en/model_doc/cwm#transformers.CwmConfig) configuration class: [CwmModel](/docs/transformers/main/en/model_doc/cwm#transformers.CwmModel) (Code World Model (CWM) model) - [DFineConfig](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineConfig) configuration class: [DFineModel](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineModel) (D-FINE model) - [DINOv3ConvNextConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextConfig) configuration class: [DINOv3ConvNextModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextModel) (DINOv3 ConvNext model) - [DINOv3ViTConfig](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTConfig) configuration class: [DINOv3ViTModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTModel) (DINOv3 ViT model) - [DPRConfig](/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig) configuration class: [DPRQuestionEncoder](/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder) (DPR model) - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTModel](/docs/transformers/main/en/model_doc/dpt#transformers.DPTModel) (DPT model) - [DabDetrConfig](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrConfig) configuration class: [DabDetrModel](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrModel) (DAB-DETR model) - [DacConfig](/docs/transformers/main/en/model_doc/dac#transformers.DacConfig) configuration class: [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model) - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel) (Data2VecAudio model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextModel) (Data2VecText model) - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel) (Data2VecVision model) - [DbrxConfig](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxConfig) configuration class: [DbrxModel](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxModel) (DBRX model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaModel](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaModel) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2Model](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Model) (DeBERTa-v2 model) - [DecisionTransformerConfig](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig) configuration class: [DecisionTransformerModel](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerModel) (Decision Transformer model) - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2Model](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Model) (DeepSeek-V2 model) - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3Model](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Model) (DeepSeek-V3 model) - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLModel](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLModel) (DeepseekVL model) - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridModel](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridModel) (DeepseekVLHybrid model) - [DeformableDetrConfig](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig) configuration class: [DeformableDetrModel](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrModel) (Deformable DETR model) - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTModel](/docs/transformers/main/en/model_doc/deit#transformers.DeiTModel) (DeiT model) - [DepthProConfig](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProConfig) configuration class: [DepthProModel](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProModel) (DepthPro model) - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrModel](/docs/transformers/main/en/model_doc/detr#transformers.DetrModel) (DETR model) - [DiaConfig](/docs/transformers/main/en/model_doc/dia#transformers.DiaConfig) configuration class: [DiaModel](/docs/transformers/main/en/model_doc/dia#transformers.DiaModel) (Dia model) - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaModel](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaModel) (DiffLlama model) - [DinatConfig](/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig) configuration class: [DinatModel](/docs/transformers/main/en/model_doc/dinat#transformers.DinatModel) (DiNAT model) - [Dinov2Config](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config) configuration class: [Dinov2Model](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model) (DINOv2 model) - [Dinov2WithRegistersConfig](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig) configuration class: [Dinov2WithRegistersModel](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersModel) (DINOv2 with Registers model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertModel](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel) (DistilBERT model) - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeModel](/docs/transformers/main/en/model_doc/doge#transformers.DogeModel) (Doge model) - [DonutSwinConfig](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig) configuration class: [DonutSwinModel](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinModel) (DonutSwin model) - [Dots1Config](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Config) configuration class: [Dots1Model](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Model) (dots1 model) - [EdgeTamConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamConfig) configuration class: [EdgeTamModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamModel) (EdgeTAM model) - [EdgeTamVideoConfig](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoConfig) configuration class: [EdgeTamVideoModel](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoModel) (EdgeTamVideo model) - [EdgeTamVisionConfig](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionConfig) configuration class: [EdgeTamVisionModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionModel) (EdgeTamVisionModel model) - [EfficientLoFTRConfig](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig) configuration class: [EfficientLoFTRModel](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel) (EfficientLoFTR model) - [EfficientNetConfig](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig) configuration class: [EfficientNetModel](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetModel) (EfficientNet model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraModel](/docs/transformers/main/en/model_doc/electra#transformers.ElectraModel) (ELECTRA model) - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3Model](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Model) (Emu3 model) - [EncodecConfig](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecConfig) configuration class: [EncodecModel](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel) (EnCodec model) - [Ernie4_5Config](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Config) configuration class: [Ernie4_5Model](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Model) (Ernie4_5 model) - [Ernie4_5_MoeConfig](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig) configuration class: [Ernie4_5_MoeModel](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeModel) (Ernie4_5_MoE model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieModel](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieModel) (ERNIE model) - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmModel](/docs/transformers/main/en/model_doc/esm#transformers.EsmModel) (ESM model) - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaModel](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaModel) (Evolla model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4Model](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Model) (EXAONE-4.0 model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetModel](/docs/transformers/main/en/model_doc/fnet#transformers.FNetModel) (FNet model) - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTModel](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTModel) (FairSeq Machine-Translation model) - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconModel](/docs/transformers/main/en/model_doc/falcon#transformers.FalconModel) (Falcon model) - [FalconH1Config](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Config) configuration class: [FalconH1Model](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Model) (FalconH1 model) - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaModel](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaModel) (FalconMamba model) - [FastSpeech2ConformerConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig) configuration class: [FastSpeech2ConformerModel](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel) (FastSpeech2Conformer model) - [FastSpeech2ConformerWithHifiGanConfig](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGanConfig) configuration class: [FastSpeech2ConformerWithHifiGan](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGan) (FastSpeech2ConformerWithHifiGan model) - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmModel](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmModel) (FastVlm model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertModel) (FlauBERT model) - [FlavaConfig](/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig) configuration class: [FlavaModel](/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel) (FLAVA model) - [FlexOlmoConfig](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoConfig) configuration class: [FlexOlmoModel](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoModel) (FlexOlmo model) - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2Model](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Model) (Florence2 model) - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetModel](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel) (FocalNet model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel) or [FunnelBaseModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel) (Funnel Transformer model) - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuModel](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuModel) (Fuyu model) - [GLPNConfig](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig) configuration class: [GLPNModel](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNModel) (GLPN model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2Model](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model) (OpenAI GPT-2 model) - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeModel](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel) (GPTBigCode model) - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJModel](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJModel) (GPT-J model) - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoModel](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoModel) (GPT Neo model) - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXModel](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXModel) (GPT NeoX model) - [GPTNeoXJapaneseConfig](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig) configuration class: [GPTNeoXJapaneseModel](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel) (GPT NeoX Japanese model) - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2Model](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Model) (Gemma2 model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3Model](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Model) (Gemma3ForConditionalGeneration model) - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3TextModel](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextModel) (Gemma3ForCausalLM model) - [Gemma3nAudioConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nAudioConfig) configuration class: `Gemma3nAudioEncoder` (Gemma3nAudioEncoder model) - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nModel) (Gemma3nForConditionalGeneration model) - [Gemma3nTextConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextConfig) configuration class: [Gemma3nTextModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextModel) (Gemma3nForCausalLM model) - [Gemma3nVisionConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nVisionConfig) configuration class: [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model) - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaModel](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaModel) (Gemma model) - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitModel](/docs/transformers/main/en/model_doc/git#transformers.GitModel) (GIT model) - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VModel](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VModel) (Glm46V model) - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4Model](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Model) (GLM4 model) - [Glm4MoeConfig](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeConfig) configuration class: [Glm4MoeModel](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeModel) (Glm4MoE model) - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vModel) (GLM4V model) - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeModel) (GLM4VMOE model) - [Glm4vMoeTextConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextConfig) configuration class: [Glm4vMoeTextModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextModel) (GLM4VMOE model) - [Glm4vMoeVisionConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionConfig) configuration class: [Glm4vMoeVisionModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionModel) (Glm4vMoeVisionModel model) - [Glm4vTextConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextConfig) configuration class: [Glm4vTextModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextModel) (GLM4V model) - [Glm4vVisionConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionConfig) configuration class: [Glm4vVisionModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionModel) (Glm4vVisionModel model) - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmModel](/docs/transformers/main/en/model_doc/glm#transformers.GlmModel) (GLM model) - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2Model](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Model) (GOT-OCR2 model) - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssModel](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssModel) (GptOss model) - [GraniteConfig](/docs/transformers/main/en/model_doc/granite#transformers.GraniteConfig) configuration class: [GraniteModel](/docs/transformers/main/en/model_doc/granite#transformers.GraniteModel) (Granite model) - [GraniteMoeConfig](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeConfig) configuration class: [GraniteMoeModel](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeModel) (GraniteMoeMoe model) - [GraniteMoeHybridConfig](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig) configuration class: [GraniteMoeHybridModel](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridModel) (GraniteMoeHybrid model) - [GraniteMoeSharedConfig](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig) configuration class: [GraniteMoeSharedModel](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedModel) (GraniteMoeSharedMoe model) - [GroundingDinoConfig](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoConfig) configuration class: [GroundingDinoModel](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoModel) (Grounding DINO model) - [GroupViTConfig](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig) configuration class: [GroupViTModel](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTModel) (GroupViT model) - [HGNetV2Config](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Config) configuration class: [HGNetV2Backbone](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone) (HGNet-V2 model) - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumModel](/docs/transformers/main/en/model_doc/helium#transformers.HeliumModel) (Helium model) - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraModel](/docs/transformers/main/en/model_doc/hiera#transformers.HieraModel) (Hiera model) - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertModel](/docs/transformers/main/en/model_doc/hubert#transformers.HubertModel) (Hubert model) - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Model) (HunYuanDenseV1 model) - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Model) (HunYuanMoeV1 model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertModel](/docs/transformers/main/en/model_doc/ibert#transformers.IBertModel) (I-BERT model) - [IJepaConfig](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaConfig) configuration class: [IJepaModel](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaModel) (I-JEPA model) - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2Model](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Model) (Idefics2 model) - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3Model](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Model) (Idefics3 model) - [Idefics3VisionConfig](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionConfig) configuration class: [Idefics3VisionTransformer](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionTransformer) (Idefics3VisionTransformer model) - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsModel](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsModel) (IDEFICS model) - [ImageGPTConfig](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig) configuration class: [ImageGPTModel](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTModel) (ImageGPT model) - [InformerConfig](/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig) configuration class: [InformerModel](/docs/transformers/main/en/model_doc/informer#transformers.InformerModel) (Informer model) - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipModel](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipModel) (InstructBLIP model) - [InstructBlipVideoConfig](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig) configuration class: [InstructBlipVideoModel](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel) (InstructBlipVideo model) - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLModel) (InternVL model) - [InternVLVisionConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionConfig) configuration class: [InternVLVisionModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionModel) (InternVLVision model) - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaModel](/docs/transformers/main/en/model_doc/jamba#transformers.JambaModel) (Jamba model) - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusModel](/docs/transformers/main/en/model_doc/janus#transformers.JanusModel) (Janus model) - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeModel](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeModel) (JetMoe model) - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2Model](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Model) (KOSMOS-2 model) - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5Model](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model) (KOSMOS-2.5 model) - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextModel](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextModel) (KyutaiSpeechToText model) - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDModel](/docs/transformers/main/en/model_doc/led#transformers.LEDModel) (LED model) - [LasrCTCConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrCTCConfig) configuration class: [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model) - [LasrEncoderConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoderConfig) configuration class: [LasrEncoder](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoder) (LasrEncoder model) - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMModel](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMModel) (LayoutLM model) - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2Model](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model) (LayoutLMv2 model) - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3Model](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model) (LayoutLMv3 model) - [LevitConfig](/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig) configuration class: [LevitModel](/docs/transformers/main/en/model_doc/levit#transformers.LevitModel) (LeViT model) - [Lfm2Config](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Config) configuration class: [Lfm2Model](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Model) (Lfm2 model) - [Lfm2MoeConfig](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeConfig) configuration class: [Lfm2MoeModel](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeModel) (Lfm2Moe model) - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlModel](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlModel) (Lfm2Vl model) - [LightGlueConfig](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueConfig) configuration class: [LightGlueForKeypointMatching](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching) (LightGlue model) - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltModel](/docs/transformers/main/en/model_doc/lilt#transformers.LiltModel) (LiLT model) - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model) - [Llama4TextConfig](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextConfig) configuration class: [Llama4TextModel](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextModel) (Llama4ForCausalLM model) - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaModel](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel) (LLaMA model) - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaModel](/docs/transformers/main/en/model_doc/llava#transformers.LlavaModel) (LLaVa model) - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextModel](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextModel) (LLaVA-NeXT model) - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoModel](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoModel) (LLaVa-NeXT-Video model) - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionModel](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionModel) (LLaVA-Onevision model) - [LongT5Config](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config) configuration class: [LongT5Model](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Model) (LongT5 model) - [LongcatFlashConfig](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashConfig) configuration class: [LongcatFlashModel](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashModel) (LongCatFlash model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerModel](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerModel) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeModel](/docs/transformers/main/en/model_doc/luke#transformers.LukeModel) (LUKE model) - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertModel](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertModel) (LXMERT model) - [M2M100Config](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config) configuration class: [M2M100Model](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Model) (M2M100 model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartModel](/docs/transformers/main/en/model_doc/mbart#transformers.MBartModel) (mBART model) - [MLCDVisionConfig](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionConfig) configuration class: [MLCDVisionModel](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionModel) (MLCD model) - [MMGroundingDinoConfig](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig) configuration class: [MMGroundingDinoModel](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel) (MM Grounding DINO model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetModel](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetModel) (MPNet model) - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5Model](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model) (MT5 model) - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2Model](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Model) (mamba2 model) - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaModel](/docs/transformers/main/en/model_doc/mamba#transformers.MambaModel) (Mamba model) - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianModel) (Marian model) - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMModel](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMModel) (MarkupLM model) - [Mask2FormerConfig](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig) configuration class: [Mask2FormerModel](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerModel) (Mask2Former model) - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerModel](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerModel) (MaskFormer model) - `MaskFormerSwinConfig` configuration class: `MaskFormerSwinModel` (MaskFormerSwin model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertModel](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertModel) (Megatron-BERT model) - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model) - [MgpstrConfig](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrConfig) configuration class: [MgpstrForSceneTextRecognition](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition) (MGP-STR model) - [MimiConfig](/docs/transformers/main/en/model_doc/mimi#transformers.MimiConfig) configuration class: [MimiModel](/docs/transformers/main/en/model_doc/mimi#transformers.MimiModel) (Mimi model) - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxModel](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxModel) (MiniMax model) - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3Model](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Model) (Ministral3 model) - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralModel](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralModel) (Ministral model) - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3Model](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Model) (Mistral3 model) - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralModel](/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel) (Mistral model) - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralModel](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralModel) (Mixtral model) - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaModel](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaModel) (Mllama model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertModel](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertModel) (MobileBERT model) - [MobileNetV1Config](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config) configuration class: [MobileNetV1Model](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model) (MobileNetV1 model) - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2Model](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model) (MobileNetV2 model) - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTModel](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTModel) (MobileViT model) - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2Model](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model) (MobileViTV2 model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertModel](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertModel) (ModernBERT model) - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderModel](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderModel) (ModernBertDecoder model) - [MoonshineConfig](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineConfig) configuration class: [MoonshineModel](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineModel) (Moonshine model) - [MoshiConfig](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiConfig) configuration class: [MoshiModel](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiModel) (Moshi model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptModel](/docs/transformers/main/en/model_doc/mpt#transformers.MptModel) (MPT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraModel](/docs/transformers/main/en/model_doc/mra#transformers.MraModel) (MRA model) - [MusicgenConfig](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig) configuration class: [MusicgenModel](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenModel) (MusicGen model) - [MusicgenMelodyConfig](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig) configuration class: [MusicgenMelodyModel](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyModel) (MusicGen Melody model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpModel](/docs/transformers/main/en/model_doc/mvp#transformers.MvpModel) (MVP model) - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatModel](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatModel) (NanoChat model) - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronModel](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronModel) (Nemotron model) - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeModel](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeModel) (NLLB-MOE model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerModel](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerModel) (Nystr√∂mformer model) - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTModel](/docs/transformers/main/en/model_doc/opt#transformers.OPTModel) (OPT model) - [Olmo2Config](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Config) configuration class: [Olmo2Model](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Model) (OLMo2 model) - [Olmo3Config](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Config) configuration class: [Olmo3Model](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Model) (Olmo3 model) - [OlmoConfig](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoConfig) configuration class: [OlmoModel](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoModel) (OLMo model) - [OlmoeConfig](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeConfig) configuration class: [OlmoeModel](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeModel) (OLMoE model) - [OmDetTurboConfig](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig) configuration class: [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model) - [OneFormerConfig](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig) configuration class: [OneFormerModel](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerModel) (OneFormer model) - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTModel) (OpenAI GPT model) - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2Model](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Model) (Ovis2 model) - [OwlViTConfig](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig) configuration class: [OwlViTModel](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel) (OWL-ViT model) - [Owlv2Config](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config) configuration class: [Owlv2Model](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Model) (OWLv2 model) - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartModel](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartModel) (PLBart model) - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaModel](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaModel) (PaliGemma model) - [ParakeetCTCConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetCTCConfig) configuration class: [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model) - [ParakeetEncoderConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoderConfig) configuration class: [ParakeetEncoder](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoder) (ParakeetEncoder model) - [PatchTSMixerConfig](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig) configuration class: [PatchTSMixerModel](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel) (PatchTSMixer model) - [PatchTSTConfig](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTConfig) configuration class: [PatchTSTModel](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTModel) (PatchTST model) - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusModel](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusModel) (Pegasus model) - [PegasusXConfig](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig) configuration class: [PegasusXModel](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXModel) (PEGASUS-X model) - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverModel](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverModel) (Perceiver model) - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMModel](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMModel) (PerceptionLM model) - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonModel](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonModel) (Persimmon model) - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3Model](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Model) (Phi3 model) - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalModel](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalModel) (Phi4Multimodal model) - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiModel](/docs/transformers/main/en/model_doc/phi#transformers.PhiModel) (Phi model) - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeModel](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeModel) (Phimoe model) - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [PixtralVisionModel](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionModel) (Pixtral model) - [PoolFormerConfig](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig) configuration class: [PoolFormerModel](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerModel) (PoolFormer model) - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetModel](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel) (ProphetNet model) - [PvtConfig](/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig) configuration class: [PvtModel](/docs/transformers/main/en/model_doc/pvt#transformers.PvtModel) (PVT model) - [PvtV2Config](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Config) configuration class: [PvtV2Model](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Model) (PVTv2 model) - [Qwen2AudioEncoderConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig) configuration class: [Qwen2AudioEncoder](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder) (Qwen2AudioEncoder model) - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2Model](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Model) (Qwen2 model) - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeModel](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeModel) (Qwen2MoE model) - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLModel) (Qwen2VL model) - [Qwen2VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextConfig) configuration class: [Qwen2VLTextModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextModel) (Qwen2VL model) - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLModel) (Qwen2_5_VL model) - [Qwen2_5_VLTextConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextConfig) configuration class: [Qwen2_5_VLTextModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextModel) (Qwen2_5_VL model) - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3Model](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Model) (Qwen3 model) - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeModel](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeModel) (Qwen3MoE model) - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextModel](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextModel) (Qwen3Next model) - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLModel) (Qwen3VL model) - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeModel) (Qwen3VLMoe model) - [Qwen3VLMoeTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextConfig) configuration class: [Qwen3VLMoeTextModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextModel) (Qwen3VLMoe model) - [Qwen3VLTextConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextConfig) configuration class: [Qwen3VLTextModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextModel) (Qwen3VL model) - [RTDetrConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrConfig) configuration class: [RTDetrModel](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrModel) (RT-DETR model) - [RTDetrV2Config](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) configuration class: [RTDetrV2Model](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model) (RT-DETRv2 model) - [RecurrentGemmaConfig](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig) configuration class: [RecurrentGemmaModel](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel) (RecurrentGemma model) - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerModel](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModel) (Reformer model) - [RegNetConfig](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig) configuration class: [RegNetModel](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetModel) (RegNet model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertModel](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertModel) (RemBERT model) - [ResNetConfig](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig) configuration class: [ResNetModel](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel) (ResNet model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertModel](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertModel) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerModel](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerModel) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaModel](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormModel](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel) (RoBERTa-PreLayerNorm model) - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvModel](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvModel) (RWKV model) - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWModel](/docs/transformers/main/en/model_doc/sew#transformers.SEWModel) (SEW model) - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDModel](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDModel) (SEW-D model) - [Sam2Config](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Config) configuration class: [Sam2Model](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Model) (SAM2 model) - [Sam2HieraDetConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetConfig) configuration class: [Sam2HieraDetModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetModel) (Sam2HieraDetModel model) - [Sam2VideoConfig](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoConfig) configuration class: [Sam2VideoModel](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoModel) (Sam2VideoModel model) - [Sam2VisionConfig](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionConfig) configuration class: [Sam2VisionModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionModel) (Sam2VisionModel model) - [Sam3Config](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Config) configuration class: [Sam3Model](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Model) (SAM3 model) - [Sam3TrackerConfig](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerConfig) configuration class: [Sam3TrackerModel](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerModel) (Sam3Tracker model) - [Sam3TrackerVideoConfig](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoConfig) configuration class: [Sam3TrackerVideoModel](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoModel) (Sam3TrackerVideo model) - [Sam3ViTConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTConfig) configuration class: [Sam3ViTModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTModel) (Sam3ViTModel model) - [Sam3VideoConfig](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoConfig) configuration class: [Sam3VideoModel](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoModel) (Sam3VideoModel model) - [Sam3VisionConfig](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionConfig) configuration class: [Sam3VisionModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionModel) (Sam3VisionModel model) - [SamConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamConfig) configuration class: [SamModel](/docs/transformers/main/en/model_doc/sam#transformers.SamModel) (SAM model) - [SamHQConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQConfig) configuration class: [SamHQModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQModel) (SAM-HQ model) - [SamHQVisionConfig](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionConfig) configuration class: [SamHQVisionModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionModel) (SamHQVisionModel model) - [SamVisionConfig](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionConfig) configuration class: [SamVisionModel](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionModel) (SamVisionModel model) - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TModel](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel) (SeamlessM4T model) - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2Model](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model) (SeamlessM4Tv2 model) - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssModel](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssModel) (SeedOss model) - [SegGptConfig](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptConfig) configuration class: [SegGptModel](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptModel) (SegGPT model) - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerModel](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerModel) (SegFormer model) - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model) - [Siglip2VisionConfig](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionConfig) configuration class: [Siglip2VisionModel](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionModel) (Siglip2VisionModel model) - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model) - [SiglipVisionConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionConfig) configuration class: [SiglipVisionModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionModel) (SiglipVisionModel model) - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3Model](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Model) (SmolLM3 model) - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMModel](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMModel) (SmolVLM model) - [SmolVLMVisionConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionConfig) configuration class: [SmolVLMVisionTransformer](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionTransformer) (SmolVLMVisionTransformer model) - [Speech2TextConfig](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig) configuration class: [Speech2TextModel](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextModel) (Speech2Text model) - [SpeechT5Config](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config) configuration class: [SpeechT5Model](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Model) (SpeechT5 model) - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterModel](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterModel) (Splinter model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertModel](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertModel) (SqueezeBERT model) - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmModel](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel) (StableLm model) - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2Model](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Model) (Starcoder2 model) - [SwiftFormerConfig](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig) configuration class: [SwiftFormerModel](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel) (SwiftFormer model) - [Swin2SRConfig](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRConfig) configuration class: [Swin2SRModel](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRModel) (Swin2SR model) - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinModel](/docs/transformers/main/en/model_doc/swin#transformers.SwinModel) (Swin Transformer model) - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2Model](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model) (Swin Transformer V2 model) - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersModel](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersModel) (SwitchTransformers model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5Model](/docs/transformers/main/en/model_doc/t5#transformers.T5Model) (T5 model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2Model](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Model) (T5Gemma2 model) - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaModel](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaModel) (T5Gemma model) - [TableTransformerConfig](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig) configuration class: [TableTransformerModel](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerModel) (Table Transformer model) - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasModel](/docs/transformers/main/en/model_doc/tapas#transformers.TapasModel) (TAPAS model) - [TextNetConfig](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetConfig) configuration class: [TextNetModel](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetModel) (TextNet model) - [TimeSeriesTransformerConfig](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig) configuration class: [TimeSeriesTransformerModel](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel) (Time Series Transformer model) - [TimesFmConfig](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmConfig) configuration class: [TimesFmModel](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModel) (TimesFm model) - [TimesformerConfig](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig) configuration class: [TimesformerModel](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerModel) (TimeSformer model) - [TimmBackboneConfig](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackboneConfig) configuration class: [TimmBackbone](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackbone) (TimmBackbone model) - [TimmWrapperConfig](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig) configuration class: [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model) - [TvpConfig](/docs/transformers/main/en/model_doc/tvp#transformers.TvpConfig) configuration class: [TvpModel](/docs/transformers/main/en/model_doc/tvp#transformers.TvpModel) (TVP model) - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5Model](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Model) (UMT5 model) - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopModel](/docs/transformers/main/en/model_doc/udop#transformers.UdopModel) (UDOP model) - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechModel](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechModel) (UniSpeech model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatModel](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel) (UniSpeechSat model) - [UnivNetConfig](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetConfig) configuration class: [UnivNetModel](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetModel) (UnivNet model) - [VJEPA2Config](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Config) configuration class: [VJEPA2Model](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Model) (VJEPA2Model model) - [VaultGemmaConfig](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaConfig) configuration class: [VaultGemmaModel](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaModel) (VaultGemma model) - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTModel](/docs/transformers/main/en/model_doc/vit#transformers.ViTModel) (ViT model) - [ViTMAEConfig](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig) configuration class: [ViTMAEModel](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEModel) (ViTMAE model) - [ViTMSNConfig](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig) configuration class: [ViTMSNModel](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel) (ViTMSN model) - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3Model](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Model) (VideoLlama3 model) - [VideoLlama3VisionConfig](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionConfig) configuration class: [VideoLlama3VisionModel](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionModel) (VideoLlama3Vision model) - [VideoLlavaConfig](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaConfig) configuration class: [VideoLlavaModel](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaModel) (VideoLlava model) - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEModel](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEModel) (VideoMAE model) - [ViltConfig](/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig) configuration class: [ViltModel](/docs/transformers/main/en/model_doc/vilt#transformers.ViltModel) (ViLT model) - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaModel](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaModel) (VipLlava model) - [VisionTextDualEncoderConfig](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig) configuration class: [VisionTextDualEncoderModel](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) (VisionTextDualEncoder model) - [VisualBertConfig](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig) configuration class: [VisualBertModel](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertModel) (VisualBERT model) - [VitDetConfig](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig) configuration class: [VitDetModel](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel) (VitDet model) - [VitsConfig](/docs/transformers/main/en/model_doc/vits#transformers.VitsConfig) configuration class: [VitsModel](/docs/transformers/main/en/model_doc/vits#transformers.VitsModel) (VITS model) - [VivitConfig](/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig) configuration class: [VivitModel](/docs/transformers/main/en/model_doc/vivit#transformers.VivitModel) (ViViT model) - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model) - [VoxtralEncoderConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoderConfig) configuration class: [VoxtralEncoder](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoder) (Voxtral Encoder model) - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertModel](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel) (Wav2Vec2-BERT model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2Model](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerModel](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel) (Wav2Vec2-Conformer model) - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMModel](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMModel) (WavLM model) - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperModel](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperModel) (Whisper model) - [XCLIPConfig](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPConfig) configuration class: [XCLIPModel](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPModel) (X-CLIP model) - [XGLMConfig](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig) configuration class: [XGLMModel](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMModel) (XGLM model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMModel) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaModel](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaModel) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLModel](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel) (XLNet model) - [XcodecConfig](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecConfig) configuration class: [XcodecModel](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecModel) (X-CODEC model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodModel](/docs/transformers/main/en/model_doc/xmod#transformers.XmodModel) (X-MOD model) - [YolosConfig](/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig) configuration class: [YolosModel](/docs/transformers/main/en/model_doc/yolos#transformers.YolosModel) (YOLOS model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoModel](/docs/transformers/main/en/model_doc/yoso#transformers.YosoModel) (YOSO model) - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2Model](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Model) (Zamba2 model) - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaModel](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaModel) (Zamba model) - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMModel](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMModel) (xLSTM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModel.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the base model classes of the library from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **afmoe** -- [AfmoeModel](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeModel) (AFMoE model)
- **aimv2** -- [Aimv2Model](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2Model) (AIMv2 model)
- **aimv2_vision_model** -- [Aimv2VisionModel](/docs/transformers/main/en/model_doc/aimv2#transformers.Aimv2VisionModel) (Aimv2VisionModel model)
- **albert** -- `AlbertModel` (ALBERT model)
- **align** -- [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model)
- **altclip** -- [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model)
- **apertus** -- [ApertusModel](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusModel) (Apertus model)
- **arcee** -- [ArceeModel](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeModel) (Arcee model)
- **aria** -- [AriaModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaModel) (Aria model)
- **aria_text** -- [AriaTextModel](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextModel) (AriaText model)
- **audio-spectrogram-transformer** -- [ASTModel](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel) (Audio Spectrogram Transformer model)
- **audioflamingo3** -- [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
- **audioflamingo3_encoder** -- [AudioFlamingo3Encoder](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Encoder) (AudioFlamingo3Encoder model)
- **autoformer** -- [AutoformerModel](/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerModel) (Autoformer model)
- **aya_vision** -- [AyaVisionModel](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionModel) (AyaVision model)
- **bamba** -- [BambaModel](/docs/transformers/main/en/model_doc/bamba#transformers.BambaModel) (Bamba model)
- **bark** -- [BarkModel](/docs/transformers/main/en/model_doc/bark#transformers.BarkModel) (Bark model)
- **bart** -- [BartModel](/docs/transformers/main/en/model_doc/bart#transformers.BartModel) (BART model)
- **beit** -- [BeitModel](/docs/transformers/main/en/model_doc/beit#transformers.BeitModel) (BEiT model)
- **bert** -- [BertModel](/docs/transformers/main/en/model_doc/bert#transformers.BertModel) (BERT model)
- **bert-generation** -- [BertGenerationEncoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationEncoder) (Bert Generation model)
- **big_bird** -- [BigBirdModel](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdModel) (BigBird model)
- **bigbird_pegasus** -- [BigBirdPegasusModel](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel) (BigBird-Pegasus model)
- **biogpt** -- [BioGptModel](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel) (BioGpt model)
- **bit** -- [BitModel](/docs/transformers/main/en/model_doc/bit#transformers.BitModel) (BiT model)
- **bitnet** -- [BitNetModel](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetModel) (BitNet model)
- **blenderbot** -- [BlenderbotModel](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotModel) (Blenderbot model)
- **blenderbot-small** -- [BlenderbotSmallModel](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel) (BlenderbotSmall model)
- **blip** -- [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model)
- **blip-2** -- [Blip2Model](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model) (BLIP-2 model)
- **blip_2_qformer** -- [Blip2QFormerModel](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerModel) (BLIP-2 QFormer model)
- **bloom** -- [BloomModel](/docs/transformers/main/en/model_doc/bloom#transformers.BloomModel) (BLOOM model)
- **blt** -- [BltModel](/docs/transformers/main/en/model_doc/blt#transformers.BltModel) (Blt model)
- **bridgetower** -- [BridgeTowerModel](/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerModel) (BridgeTower model)
- **bros** -- [BrosModel](/docs/transformers/main/en/model_doc/bros#transformers.BrosModel) (BROS model)
- **camembert** -- [CamembertModel](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertModel) (CamemBERT model)
- **canine** -- [CanineModel](/docs/transformers/main/en/model_doc/canine#transformers.CanineModel) (CANINE model)
- **chameleon** -- [ChameleonModel](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonModel) (Chameleon model)
- **chinese_clip** -- [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model)
- **chinese_clip_vision_model** -- [ChineseCLIPVisionModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionModel) (ChineseCLIPVisionModel model)
- **clap** -- [ClapModel](/docs/transformers/main/en/model_doc/clap#transformers.ClapModel) (CLAP model)
- **clip** -- [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model)
- **clip_text_model** -- [CLIPTextModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextModel) (CLIPTextModel model)
- **clip_vision_model** -- [CLIPVisionModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionModel) (CLIPVisionModel model)
- **clipseg** -- [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model)
- **clvp** -- [ClvpModelForConditionalGeneration](/docs/transformers/main/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration) (CLVP model)
- **code_llama** -- [LlamaModel](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel) (CodeLlama model)
- **codegen** -- [CodeGenModel](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenModel) (CodeGen model)
- **cohere** -- [CohereModel](/docs/transformers/main/en/model_doc/cohere#transformers.CohereModel) (Cohere model)
- **cohere2** -- [Cohere2Model](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Model) (Cohere2 model)
- **cohere2_vision** -- [Cohere2VisionModel](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionModel) (Cohere2Vision model)
- **conditional_detr** -- [ConditionalDetrModel](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel) (Conditional DETR model)
- **convbert** -- [ConvBertModel](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertModel) (ConvBERT model)
- **convnext** -- [ConvNextModel](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextModel) (ConvNeXT model)
- **convnextv2** -- [ConvNextV2Model](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Model) (ConvNeXTV2 model)
- **cpmant** -- [CpmAntModel](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntModel) (CPM-Ant model)
- **csm** -- [CsmForConditionalGeneration](/docs/transformers/main/en/model_doc/csm#transformers.CsmForConditionalGeneration) (CSM model)
- **ctrl** -- [CTRLModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLModel) (CTRL model)
- **cvt** -- [CvtModel](/docs/transformers/main/en/model_doc/cvt#transformers.CvtModel) (CvT model)
- **cwm** -- [CwmModel](/docs/transformers/main/en/model_doc/cwm#transformers.CwmModel) (Code World Model (CWM) model)
- **d_fine** -- [DFineModel](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineModel) (D-FINE model)
- **dab-detr** -- [DabDetrModel](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrModel) (DAB-DETR model)
- **dac** -- [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model)
- **data2vec-audio** -- [Data2VecAudioModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel) (Data2VecAudio model)
- **data2vec-text** -- [Data2VecTextModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextModel) (Data2VecText model)
- **data2vec-vision** -- [Data2VecVisionModel](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel) (Data2VecVision model)
- **dbrx** -- [DbrxModel](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxModel) (DBRX model)
- **deberta** -- [DebertaModel](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaModel) (DeBERTa model)
- **deberta-v2** -- [DebertaV2Model](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Model) (DeBERTa-v2 model)
- **decision_transformer** -- [DecisionTransformerModel](/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerModel) (Decision Transformer model)
- **deepseek_v2** -- [DeepseekV2Model](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Model) (DeepSeek-V2 model)
- **deepseek_v3** -- [DeepseekV3Model](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Model) (DeepSeek-V3 model)
- **deepseek_vl** -- [DeepseekVLModel](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLModel) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridModel](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridModel) (DeepseekVLHybrid model)
- **deformable_detr** -- [DeformableDetrModel](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrModel) (Deformable DETR model)
- **deit** -- [DeiTModel](/docs/transformers/main/en/model_doc/deit#transformers.DeiTModel) (DeiT model)
- **depth_pro** -- [DepthProModel](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProModel) (DepthPro model)
- **detr** -- [DetrModel](/docs/transformers/main/en/model_doc/detr#transformers.DetrModel) (DETR model)
- **dia** -- [DiaModel](/docs/transformers/main/en/model_doc/dia#transformers.DiaModel) (Dia model)
- **diffllama** -- [DiffLlamaModel](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaModel) (DiffLlama model)
- **dinat** -- [DinatModel](/docs/transformers/main/en/model_doc/dinat#transformers.DinatModel) (DiNAT model)
- **dinov2** -- [Dinov2Model](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model) (DINOv2 model)
- **dinov2_with_registers** -- [Dinov2WithRegistersModel](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersModel) (DINOv2 with Registers model)
- **dinov3_convnext** -- [DINOv3ConvNextModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ConvNextModel) (DINOv3 ConvNext model)
- **dinov3_vit** -- [DINOv3ViTModel](/docs/transformers/main/en/model_doc/dinov3#transformers.DINOv3ViTModel) (DINOv3 ViT model)
- **distilbert** -- [DistilBertModel](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel) (DistilBERT model)
- **doge** -- [DogeModel](/docs/transformers/main/en/model_doc/doge#transformers.DogeModel) (Doge model)
- **donut-swin** -- [DonutSwinModel](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinModel) (DonutSwin model)
- **dots1** -- [Dots1Model](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Model) (dots1 model)
- **dpr** -- [DPRQuestionEncoder](/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder) (DPR model)
- **dpt** -- [DPTModel](/docs/transformers/main/en/model_doc/dpt#transformers.DPTModel) (DPT model)
- **edgetam** -- [EdgeTamModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamModel) (EdgeTAM model)
- **edgetam_video** -- [EdgeTamVideoModel](/docs/transformers/main/en/model_doc/edgetam_video#transformers.EdgeTamVideoModel) (EdgeTamVideo model)
- **edgetam_vision_model** -- [EdgeTamVisionModel](/docs/transformers/main/en/model_doc/edgetam#transformers.EdgeTamVisionModel) (EdgeTamVisionModel model)
- **efficientloftr** -- [EfficientLoFTRModel](/docs/transformers/main/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel) (EfficientLoFTR model)
- **efficientnet** -- [EfficientNetModel](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetModel) (EfficientNet model)
- **electra** -- [ElectraModel](/docs/transformers/main/en/model_doc/electra#transformers.ElectraModel) (ELECTRA model)
- **emu3** -- [Emu3Model](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Model) (Emu3 model)
- **encodec** -- [EncodecModel](/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel) (EnCodec model)
- **ernie** -- [ErnieModel](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieModel) (ERNIE model)
- **ernie4_5** -- [Ernie4_5Model](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Model) (Ernie4_5 model)
- **ernie4_5_moe** -- [Ernie4_5_MoeModel](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeModel) (Ernie4_5_MoE model)
- **esm** -- [EsmModel](/docs/transformers/main/en/model_doc/esm#transformers.EsmModel) (ESM model)
- **evolla** -- [EvollaModel](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaModel) (Evolla model)
- **exaone4** -- [Exaone4Model](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Model) (EXAONE-4.0 model)
- **falcon** -- [FalconModel](/docs/transformers/main/en/model_doc/falcon#transformers.FalconModel) (Falcon model)
- **falcon_h1** -- [FalconH1Model](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Model) (FalconH1 model)
- **falcon_mamba** -- [FalconMambaModel](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaModel) (FalconMamba model)
- **fast_vlm** -- [FastVlmModel](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmModel) (FastVlm model)
- **fastspeech2_conformer** -- [FastSpeech2ConformerModel](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel) (FastSpeech2Conformer model)
- **fastspeech2_conformer_with_hifigan** -- [FastSpeech2ConformerWithHifiGan](/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGan) (FastSpeech2ConformerWithHifiGan model)
- **flaubert** -- [FlaubertModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertModel) (FlauBERT model)
- **flava** -- [FlavaModel](/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel) (FLAVA model)
- **flex_olmo** -- [FlexOlmoModel](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoModel) (FlexOlmo model)
- **florence2** -- [Florence2Model](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Model) (Florence2 model)
- **fnet** -- [FNetModel](/docs/transformers/main/en/model_doc/fnet#transformers.FNetModel) (FNet model)
- **focalnet** -- [FocalNetModel](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel) (FocalNet model)
- **fsmt** -- [FSMTModel](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTModel) (FairSeq Machine-Translation model)
- **funnel** -- [FunnelModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel) or [FunnelBaseModel](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel) (Funnel Transformer model)
- **fuyu** -- [FuyuModel](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuModel) (Fuyu model)
- **gemma** -- [GemmaModel](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaModel) (Gemma model)
- **gemma2** -- [Gemma2Model](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Model) (Gemma2 model)
- **gemma3** -- [Gemma3Model](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Model) (Gemma3ForConditionalGeneration model)
- **gemma3_text** -- [Gemma3TextModel](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextModel) (Gemma3ForCausalLM model)
- **gemma3n** -- [Gemma3nModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nModel) (Gemma3nForConditionalGeneration model)
- **gemma3n_audio** -- `Gemma3nAudioEncoder` (Gemma3nAudioEncoder model)
- **gemma3n_text** -- [Gemma3nTextModel](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextModel) (Gemma3nForCausalLM model)
- **gemma3n_vision** -- [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model)
- **git** -- [GitModel](/docs/transformers/main/en/model_doc/git#transformers.GitModel) (GIT model)
- **glm** -- [GlmModel](/docs/transformers/main/en/model_doc/glm#transformers.GlmModel) (GLM model)
- **glm4** -- [Glm4Model](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Model) (GLM4 model)
- **glm46v** -- [Glm46VModel](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VModel) (Glm46V model)
- **glm4_moe** -- [Glm4MoeModel](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeModel) (Glm4MoE model)
- **glm4v** -- [Glm4vModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vModel) (GLM4V model)
- **glm4v_moe** -- [Glm4vMoeModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeModel) (GLM4VMOE model)
- **glm4v_moe_text** -- [Glm4vMoeTextModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextModel) (GLM4VMOE model)
- **glm4v_moe_vision** -- [Glm4vMoeVisionModel](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeVisionModel) (Glm4vMoeVisionModel model)
- **glm4v_text** -- [Glm4vTextModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vTextModel) (GLM4V model)
- **glm4v_vision** -- [Glm4vVisionModel](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vVisionModel) (Glm4vVisionModel model)
- **glpn** -- [GLPNModel](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNModel) (GLPN model)
- **got_ocr2** -- [GotOcr2Model](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Model) (GOT-OCR2 model)
- **gpt-sw3** -- [GPT2Model](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model) (GPT-Sw3 model)
- **gpt2** -- [GPT2Model](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeModel](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel) (GPTBigCode model)
- **gpt_neo** -- [GPTNeoModel](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoModel) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXModel](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXModel) (GPT NeoX model)
- **gpt_neox_japanese** -- [GPTNeoXJapaneseModel](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel) (GPT NeoX Japanese model)
- **gpt_oss** -- [GptOssModel](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssModel) (GptOss model)
- **gptj** -- [GPTJModel](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJModel) (GPT-J model)
- **granite** -- [GraniteModel](/docs/transformers/main/en/model_doc/granite#transformers.GraniteModel) (Granite model)
- **granitemoe** -- [GraniteMoeModel](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeModel) (GraniteMoeMoe model)
- **granitemoehybrid** -- [GraniteMoeHybridModel](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridModel) (GraniteMoeHybrid model)
- **granitemoeshared** -- [GraniteMoeSharedModel](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedModel) (GraniteMoeSharedMoe model)
- **grounding-dino** -- [GroundingDinoModel](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoModel) (Grounding DINO model)
- **groupvit** -- [GroupViTModel](/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTModel) (GroupViT model)
- **helium** -- [HeliumModel](/docs/transformers/main/en/model_doc/helium#transformers.HeliumModel) (Helium model)
- **hgnet_v2** -- [HGNetV2Backbone](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone) (HGNet-V2 model)
- **hiera** -- [HieraModel](/docs/transformers/main/en/model_doc/hiera#transformers.HieraModel) (Hiera model)
- **hubert** -- [HubertModel](/docs/transformers/main/en/model_doc/hubert#transformers.HubertModel) (Hubert model)
- **hunyuan_v1_dense** -- [HunYuanDenseV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Model) (HunYuanDenseV1 model)
- **hunyuan_v1_moe** -- [HunYuanMoEV1Model](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Model) (HunYuanMoeV1 model)
- **ibert** -- [IBertModel](/docs/transformers/main/en/model_doc/ibert#transformers.IBertModel) (I-BERT model)
- **idefics** -- [IdeficsModel](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsModel) (IDEFICS model)
- **idefics2** -- [Idefics2Model](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Model) (Idefics2 model)
- **idefics3** -- [Idefics3Model](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Model) (Idefics3 model)
- **idefics3_vision** -- [Idefics3VisionTransformer](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3VisionTransformer) (Idefics3VisionTransformer model)
- **ijepa** -- [IJepaModel](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaModel) (I-JEPA model)
- **imagegpt** -- [ImageGPTModel](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTModel) (ImageGPT model)
- **informer** -- [InformerModel](/docs/transformers/main/en/model_doc/informer#transformers.InformerModel) (Informer model)
- **instructblip** -- [InstructBlipModel](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipModel) (InstructBLIP model)
- **instructblipvideo** -- [InstructBlipVideoModel](/docs/transformers/main/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel) (InstructBlipVideo model)
- **internvl** -- [InternVLModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLModel) (InternVL model)
- **internvl_vision** -- [InternVLVisionModel](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLVisionModel) (InternVLVision model)
- **jamba** -- [JambaModel](/docs/transformers/main/en/model_doc/jamba#transformers.JambaModel) (Jamba model)
- **janus** -- [JanusModel](/docs/transformers/main/en/model_doc/janus#transformers.JanusModel) (Janus model)
- **jetmoe** -- [JetMoeModel](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeModel) (JetMoe model)
- **kosmos-2** -- [Kosmos2Model](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Model) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5Model](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model) (KOSMOS-2.5 model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextModel](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextModel) (KyutaiSpeechToText model)
- **lasr_ctc** -- [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model)
- **lasr_encoder** -- [LasrEncoder](/docs/transformers/main/en/model_doc/lasr#transformers.LasrEncoder) (LasrEncoder model)
- **layoutlm** -- [LayoutLMModel](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMModel) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2Model](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3Model](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model) (LayoutLMv3 model)
- **led** -- [LEDModel](/docs/transformers/main/en/model_doc/led#transformers.LEDModel) (LED model)
- **levit** -- [LevitModel](/docs/transformers/main/en/model_doc/levit#transformers.LevitModel) (LeViT model)
- **lfm2** -- [Lfm2Model](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Model) (Lfm2 model)
- **lfm2_moe** -- [Lfm2MoeModel](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeModel) (Lfm2Moe model)
- **lfm2_vl** -- [Lfm2VlModel](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlModel) (Lfm2Vl model)
- **lightglue** -- [LightGlueForKeypointMatching](/docs/transformers/main/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching) (LightGlue model)
- **lilt** -- [LiltModel](/docs/transformers/main/en/model_doc/lilt#transformers.LiltModel) (LiLT model)
- **llama** -- [LlamaModel](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel) (LLaMA model)
- **llama4** -- [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
- **llama4_text** -- [Llama4TextModel](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextModel) (Llama4ForCausalLM model)
- **llava** -- [LlavaModel](/docs/transformers/main/en/model_doc/llava#transformers.LlavaModel) (LLaVa model)
- **llava_next** -- [LlavaNextModel](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextModel) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoModel](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoModel) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionModel](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionModel) (LLaVA-Onevision model)
- **longcat_flash** -- [LongcatFlashModel](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashModel) (LongCatFlash model)
- **longformer** -- [LongformerModel](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerModel) (Longformer model)
- **longt5** -- [LongT5Model](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Model) (LongT5 model)
- **luke** -- [LukeModel](/docs/transformers/main/en/model_doc/luke#transformers.LukeModel) (LUKE model)
- **lxmert** -- [LxmertModel](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertModel) (LXMERT model)
- **m2m_100** -- [M2M100Model](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Model) (M2M100 model)
- **mamba** -- [MambaModel](/docs/transformers/main/en/model_doc/mamba#transformers.MambaModel) (Mamba model)
- **mamba2** -- [Mamba2Model](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Model) (mamba2 model)
- **marian** -- [MarianModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianModel) (Marian model)
- **markuplm** -- [MarkupLMModel](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMModel) (MarkupLM model)
- **mask2former** -- [Mask2FormerModel](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerModel) (Mask2Former model)
- **maskformer** -- [MaskFormerModel](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerModel) (MaskFormer model)
- **maskformer-swin** -- `MaskFormerSwinModel` (MaskFormerSwin model)
- **mbart** -- [MBartModel](/docs/transformers/main/en/model_doc/mbart#transformers.MBartModel) (mBART model)
- **megatron-bert** -- [MegatronBertModel](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertModel) (Megatron-BERT model)
- **metaclip_2** -- [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model)
- **mgp-str** -- [MgpstrForSceneTextRecognition](/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition) (MGP-STR model)
- **mimi** -- [MimiModel](/docs/transformers/main/en/model_doc/mimi#transformers.MimiModel) (Mimi model)
- **minimax** -- [MiniMaxModel](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxModel) (MiniMax model)
- **ministral** -- [MinistralModel](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralModel) (Ministral model)
- **ministral3** -- [Ministral3Model](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Model) (Ministral3 model)
- **mistral** -- [MistralModel](/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel) (Mistral model)
- **mistral3** -- [Mistral3Model](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Model) (Mistral3 model)
- **mixtral** -- [MixtralModel](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralModel) (Mixtral model)
- **mlcd** -- [MLCDVisionModel](/docs/transformers/main/en/model_doc/mlcd#transformers.MLCDVisionModel) (MLCD model)
- **mllama** -- [MllamaModel](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaModel) (Mllama model)
- **mm-grounding-dino** -- [MMGroundingDinoModel](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel) (MM Grounding DINO model)
- **mobilebert** -- [MobileBertModel](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertModel) (MobileBERT model)
- **mobilenet_v1** -- [MobileNetV1Model](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model) (MobileNetV1 model)
- **mobilenet_v2** -- [MobileNetV2Model](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model) (MobileNetV2 model)
- **mobilevit** -- [MobileViTModel](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTModel) (MobileViT model)
- **mobilevitv2** -- [MobileViTV2Model](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model) (MobileViTV2 model)
- **modernbert** -- [ModernBertModel](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertModel) (ModernBERT model)
- **modernbert-decoder** -- [ModernBertDecoderModel](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderModel) (ModernBertDecoder model)
- **moonshine** -- [MoonshineModel](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineModel) (Moonshine model)
- **moshi** -- [MoshiModel](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiModel) (Moshi model)
- **mpnet** -- [MPNetModel](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetModel) (MPNet model)
- **mpt** -- [MptModel](/docs/transformers/main/en/model_doc/mpt#transformers.MptModel) (MPT model)
- **mra** -- [MraModel](/docs/transformers/main/en/model_doc/mra#transformers.MraModel) (MRA model)
- **mt5** -- [MT5Model](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model) (MT5 model)
- **musicgen** -- [MusicgenModel](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenModel) (MusicGen model)
- **musicgen_melody** -- [MusicgenMelodyModel](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyModel) (MusicGen Melody model)
- **mvp** -- [MvpModel](/docs/transformers/main/en/model_doc/mvp#transformers.MvpModel) (MVP model)
- **nanochat** -- [NanoChatModel](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatModel) (NanoChat model)
- **nemotron** -- [NemotronModel](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronModel) (Nemotron model)
- **nllb-moe** -- [NllbMoeModel](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeModel) (NLLB-MOE model)
- **nystromformer** -- [NystromformerModel](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerModel) (Nystr√∂mformer model)
- **olmo** -- [OlmoModel](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoModel) (OLMo model)
- **olmo2** -- [Olmo2Model](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Model) (OLMo2 model)
- **olmo3** -- [Olmo3Model](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Model) (Olmo3 model)
- **olmoe** -- [OlmoeModel](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeModel) (OLMoE model)
- **omdet-turbo** -- [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model)
- **oneformer** -- [OneFormerModel](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerModel) (OneFormer model)
- **openai-gpt** -- [OpenAIGPTModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTModel) (OpenAI GPT model)
- **opt** -- [OPTModel](/docs/transformers/main/en/model_doc/opt#transformers.OPTModel) (OPT model)
- **ovis2** -- [Ovis2Model](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Model) (Ovis2 model)
- **owlv2** -- [Owlv2Model](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Model) (OWLv2 model)
- **owlvit** -- [OwlViTModel](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel) (OWL-ViT model)
- **paligemma** -- [PaliGemmaModel](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaModel) (PaliGemma model)
- **parakeet_ctc** -- [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model)
- **parakeet_encoder** -- [ParakeetEncoder](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetEncoder) (ParakeetEncoder model)
- **patchtsmixer** -- [PatchTSMixerModel](/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel) (PatchTSMixer model)
- **patchtst** -- [PatchTSTModel](/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTModel) (PatchTST model)
- **pegasus** -- [PegasusModel](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusModel) (Pegasus model)
- **pegasus_x** -- [PegasusXModel](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXModel) (PEGASUS-X model)
- **perceiver** -- [PerceiverModel](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverModel) (Perceiver model)
- **perception_lm** -- [PerceptionLMModel](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMModel) (PerceptionLM model)
- **persimmon** -- [PersimmonModel](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonModel) (Persimmon model)
- **phi** -- [PhiModel](/docs/transformers/main/en/model_doc/phi#transformers.PhiModel) (Phi model)
- **phi3** -- [Phi3Model](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Model) (Phi3 model)
- **phi4_multimodal** -- [Phi4MultimodalModel](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalModel) (Phi4Multimodal model)
- **phimoe** -- [PhimoeModel](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeModel) (Phimoe model)
- **pixtral** -- [PixtralVisionModel](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionModel) (Pixtral model)
- **plbart** -- [PLBartModel](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartModel) (PLBart model)
- **poolformer** -- [PoolFormerModel](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerModel) (PoolFormer model)
- **prophetnet** -- [ProphetNetModel](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel) (ProphetNet model)
- **pvt** -- [PvtModel](/docs/transformers/main/en/model_doc/pvt#transformers.PvtModel) (PVT model)
- **pvt_v2** -- [PvtV2Model](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Model) (PVTv2 model)
- **qwen2** -- [Qwen2Model](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Model) (Qwen2 model)
- **qwen2_5_vl** -- [Qwen2_5_VLModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLModel) (Qwen2_5_VL model)
- **qwen2_5_vl_text** -- [Qwen2_5_VLTextModel](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextModel) (Qwen2_5_VL model)
- **qwen2_audio_encoder** -- [Qwen2AudioEncoder](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder) (Qwen2AudioEncoder model)
- **qwen2_moe** -- [Qwen2MoeModel](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeModel) (Qwen2MoE model)
- **qwen2_vl** -- [Qwen2VLModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLModel) (Qwen2VL model)
- **qwen2_vl_text** -- [Qwen2VLTextModel](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLTextModel) (Qwen2VL model)
- **qwen3** -- [Qwen3Model](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Model) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeModel](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeModel) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextModel](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextModel) (Qwen3Next model)
- **qwen3_vl** -- [Qwen3VLModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLModel) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLMoeModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeModel) (Qwen3VLMoe model)
- **qwen3_vl_moe_text** -- [Qwen3VLMoeTextModel](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeTextModel) (Qwen3VLMoe model)
- **qwen3_vl_text** -- [Qwen3VLTextModel](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLTextModel) (Qwen3VL model)
- **recurrent_gemma** -- [RecurrentGemmaModel](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel) (RecurrentGemma model)
- **reformer** -- [ReformerModel](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModel) (Reformer model)
- **regnet** -- [RegNetModel](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetModel) (RegNet model)
- **rembert** -- [RemBertModel](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertModel) (RemBERT model)
- **resnet** -- [ResNetModel](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel) (ResNet model)
- **roberta** -- [RobertaModel](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormModel](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertModel](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertModel) (RoCBert model)
- **roformer** -- [RoFormerModel](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerModel) (RoFormer model)
- **rt_detr** -- [RTDetrModel](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrModel) (RT-DETR model)
- **rt_detr_v2** -- [RTDetrV2Model](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model) (RT-DETRv2 model)
- **rwkv** -- [RwkvModel](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvModel) (RWKV model)
- **sam** -- [SamModel](/docs/transformers/main/en/model_doc/sam#transformers.SamModel) (SAM model)
- **sam2** -- [Sam2Model](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2Model) (SAM2 model)
- **sam2_hiera_det_model** -- [Sam2HieraDetModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2HieraDetModel) (Sam2HieraDetModel model)
- **sam2_video** -- [Sam2VideoModel](/docs/transformers/main/en/model_doc/sam2_video#transformers.Sam2VideoModel) (Sam2VideoModel model)
- **sam2_vision_model** -- [Sam2VisionModel](/docs/transformers/main/en/model_doc/sam2#transformers.Sam2VisionModel) (Sam2VisionModel model)
- **sam3** -- [Sam3Model](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3Model) (SAM3 model)
- **sam3_tracker** -- [Sam3TrackerModel](/docs/transformers/main/en/model_doc/sam3_tracker#transformers.Sam3TrackerModel) (Sam3Tracker model)
- **sam3_tracker_video** -- [Sam3TrackerVideoModel](/docs/transformers/main/en/model_doc/sam3_tracker_video#transformers.Sam3TrackerVideoModel) (Sam3TrackerVideo model)
- **sam3_video** -- [Sam3VideoModel](/docs/transformers/main/en/model_doc/sam3_video#transformers.Sam3VideoModel) (Sam3VideoModel model)
- **sam3_vision_model** -- [Sam3VisionModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3VisionModel) (Sam3VisionModel model)
- **sam3_vit_model** -- [Sam3ViTModel](/docs/transformers/main/en/model_doc/sam3#transformers.Sam3ViTModel) (Sam3ViTModel model)
- **sam_hq** -- [SamHQModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQModel) (SAM-HQ model)
- **sam_hq_vision_model** -- [SamHQVisionModel](/docs/transformers/main/en/model_doc/sam_hq#transformers.SamHQVisionModel) (SamHQVisionModel model)
- **sam_vision_model** -- [SamVisionModel](/docs/transformers/main/en/model_doc/sam#transformers.SamVisionModel) (SamVisionModel model)
- **seamless_m4t** -- [SeamlessM4TModel](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4Tv2Model](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model) (SeamlessM4Tv2 model)
- **seed_oss** -- [SeedOssModel](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssModel) (SeedOss model)
- **segformer** -- [SegformerModel](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerModel) (SegFormer model)
- **seggpt** -- [SegGptModel](/docs/transformers/main/en/model_doc/seggpt#transformers.SegGptModel) (SegGPT model)
- **sew** -- [SEWModel](/docs/transformers/main/en/model_doc/sew#transformers.SEWModel) (SEW model)
- **sew-d** -- [SEWDModel](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDModel) (SEW-D model)
- **siglip** -- [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model)
- **siglip2** -- [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model)
- **siglip2_vision_model** -- [Siglip2VisionModel](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2VisionModel) (Siglip2VisionModel model)
- **siglip_vision_model** -- [SiglipVisionModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionModel) (SiglipVisionModel model)
- **smollm3** -- [SmolLM3Model](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Model) (SmolLM3 model)
- **smolvlm** -- [SmolVLMModel](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMModel) (SmolVLM model)
- **smolvlm_vision** -- [SmolVLMVisionTransformer](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMVisionTransformer) (SmolVLMVisionTransformer model)
- **speech_to_text** -- [Speech2TextModel](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextModel) (Speech2Text model)
- **speecht5** -- [SpeechT5Model](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Model) (SpeechT5 model)
- **splinter** -- [SplinterModel](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterModel) (Splinter model)
- **squeezebert** -- [SqueezeBertModel](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertModel) (SqueezeBERT model)
- **stablelm** -- [StableLmModel](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel) (StableLm model)
- **starcoder2** -- [Starcoder2Model](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Model) (Starcoder2 model)
- **swiftformer** -- [SwiftFormerModel](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel) (SwiftFormer model)
- **swin** -- [SwinModel](/docs/transformers/main/en/model_doc/swin#transformers.SwinModel) (Swin Transformer model)
- **swin2sr** -- [Swin2SRModel](/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRModel) (Swin2SR model)
- **swinv2** -- [Swinv2Model](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model) (Swin Transformer V2 model)
- **switch_transformers** -- [SwitchTransformersModel](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersModel) (SwitchTransformers model)
- **t5** -- [T5Model](/docs/transformers/main/en/model_doc/t5#transformers.T5Model) (T5 model)
- **t5gemma** -- [T5GemmaModel](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaModel) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2Model](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Model) (T5Gemma2 model)
- **table-transformer** -- [TableTransformerModel](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerModel) (Table Transformer model)
- **tapas** -- [TapasModel](/docs/transformers/main/en/model_doc/tapas#transformers.TapasModel) (TAPAS model)
- **textnet** -- [TextNetModel](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetModel) (TextNet model)
- **time_series_transformer** -- [TimeSeriesTransformerModel](/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel) (Time Series Transformer model)
- **timesfm** -- [TimesFmModel](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModel) (TimesFm model)
- **timesformer** -- [TimesformerModel](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerModel) (TimeSformer model)
- **timm_backbone** -- [TimmBackbone](/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackbone) (TimmBackbone model)
- **timm_wrapper** -- [TimmWrapperModel](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperModel) (TimmWrapperModel model)
- **tvp** -- [TvpModel](/docs/transformers/main/en/model_doc/tvp#transformers.TvpModel) (TVP model)
- **udop** -- [UdopModel](/docs/transformers/main/en/model_doc/udop#transformers.UdopModel) (UDOP model)
- **umt5** -- [UMT5Model](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Model) (UMT5 model)
- **unispeech** -- [UniSpeechModel](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechModel) (UniSpeech model)
- **unispeech-sat** -- [UniSpeechSatModel](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel) (UniSpeechSat model)
- **univnet** -- [UnivNetModel](/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetModel) (UnivNet model)
- **vaultgemma** -- [VaultGemmaModel](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaModel) (VaultGemma model)
- **video_llama_3** -- [VideoLlama3Model](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Model) (VideoLlama3 model)
- **video_llama_3_vision** -- [VideoLlama3VisionModel](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3VisionModel) (VideoLlama3Vision model)
- **video_llava** -- [VideoLlavaModel](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaModel) (VideoLlava model)
- **videomae** -- [VideoMAEModel](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEModel) (VideoMAE model)
- **vilt** -- [ViltModel](/docs/transformers/main/en/model_doc/vilt#transformers.ViltModel) (ViLT model)
- **vipllava** -- [VipLlavaModel](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaModel) (VipLlava model)
- **vision-text-dual-encoder** -- [VisionTextDualEncoderModel](/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) (VisionTextDualEncoder model)
- **visual_bert** -- [VisualBertModel](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertModel) (VisualBERT model)
- **vit** -- [ViTModel](/docs/transformers/main/en/model_doc/vit#transformers.ViTModel) (ViT model)
- **vit_mae** -- [ViTMAEModel](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEModel) (ViTMAE model)
- **vit_msn** -- [ViTMSNModel](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel) (ViTMSN model)
- **vitdet** -- [VitDetModel](/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel) (VitDet model)
- **vits** -- [VitsModel](/docs/transformers/main/en/model_doc/vits#transformers.VitsModel) (VITS model)
- **vivit** -- [VivitModel](/docs/transformers/main/en/model_doc/vivit#transformers.VivitModel) (ViViT model)
- **vjepa2** -- [VJEPA2Model](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Model) (VJEPA2Model model)
- **voxtral** -- [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
- **voxtral_encoder** -- [VoxtralEncoder](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralEncoder) (Voxtral Encoder model)
- **wav2vec2** -- [Wav2Vec2Model](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertModel](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerModel](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMModel](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMModel) (WavLM model)
- **whisper** -- [WhisperModel](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperModel) (Whisper model)
- **xclip** -- [XCLIPModel](/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPModel) (X-CLIP model)
- **xcodec** -- [XcodecModel](/docs/transformers/main/en/model_doc/xcodec#transformers.XcodecModel) (X-CODEC model)
- **xglm** -- [XGLMModel](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMModel) (XGLM model)
- **xlm** -- [XLMModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMModel) (XLM model)
- **xlm-roberta** -- [XLMRobertaModel](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaModel) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLModel](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel) (XLNet model)
- **xlstm** -- [xLSTMModel](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMModel) (xLSTM model)
- **xmod** -- [XmodModel](/docs/transformers/main/en/model_doc/xmod#transformers.XmodModel) (X-MOD model)
- **yolos** -- [YolosModel](/docs/transformers/main/en/model_doc/yolos#transformers.YolosModel) (YOLOS model)
- **yoso** -- [YosoModel](/docs/transformers/main/en/model_doc/yoso#transformers.YosoModel) (YOSO model)
- **zamba** -- [ZambaModel](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaModel) (Zamba model)
- **zamba2** -- [Zamba2Model](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Model) (Zamba2 model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModel

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModel.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModel.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

## Generic pretraining classes

The following auto classes are available for instantiating a model with a pretraining head.

### AutoModelForPreTraining[[transformers.AutoModelForPreTraining]]

#### transformers.AutoModelForPreTraining[[transformers.AutoModelForPreTraining]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1932)

This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForPreTraining.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForPreTraining` (ALBERT model)
  - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForPreTraining](/docs/transformers/main/en/model_doc/bert#transformers.BertForPreTraining) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForPreTraining](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForPreTraining) (BigBird model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model)
  - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model)
  - [ColPaliConfig](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliConfig) configuration class: [ColPaliForRetrieval](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliForRetrieval) (ColPali model)
  - [ColQwen2Config](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2Config) configuration class: [ColQwen2ForRetrieval](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2ForRetrieval) (ColQwen2 model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForPreTraining](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForPreTraining) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForPreTraining](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForPreTraining) (ERNIE model)
  - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForPreTraining](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForPreTraining) (FNet model)
  - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model)
  - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model)
  - [FlavaConfig](/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig) configuration class: [FlavaForPreTraining](/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining) (FLAVA model)
  - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForPreTraining](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining) (Funnel Transformer model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model)
  - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
  - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraForPreTraining](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForPreTraining) (Hiera model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model)
  - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
  - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
  - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
  - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model)
  - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
  - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
  - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
  - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model)
  - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertForPreTraining](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForPreTraining) (LXMERT model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model)
  - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model)
  - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForPreTraining](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining) (Megatron-BERT model)
  - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
  - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForPreTraining](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForPreTraining) (MobileBERT model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
  - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model)
  - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model)
  - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model)
  - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
  - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForPreTraining](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForPreTraining) (RoCBert model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model)
  - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model)
  - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterForPreTraining](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForPreTraining) (Splinter model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model)
  - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
  - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model)
  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model)
  - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForPreTraining](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForPreTraining) (UniSpeech model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForPreTraining](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining) (UniSpeechSat model)
  - [ViTMAEConfig](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig) configuration class: [ViTMAEForPreTraining](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining) (ViTMAE model)
  - [VideoLlavaConfig](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaConfig) configuration class: [VideoLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaForConditionalGeneration) (VideoLlava model)
  - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEForPreTraining](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForPreTraining) (VideoMAE model)
  - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
  - [VisualBertConfig](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig) configuration class: [VisualBertForPreTraining](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertForPreTraining) (VisualBERT model)
  - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining) (Wav2Vec2-Conformer model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model)
  - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a pretraining head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForPreTraining

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForPreTraining.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForPreTraining` (ALBERT model) - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForPreTraining](/docs/transformers/main/en/model_doc/bert#transformers.BertForPreTraining) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForPreTraining](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForPreTraining) (BigBird model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model) - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model) - [ColPaliConfig](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliConfig) configuration class: [ColPaliForRetrieval](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliForRetrieval) (ColPali model) - [ColQwen2Config](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2Config) configuration class: [ColQwen2ForRetrieval](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2ForRetrieval) (ColQwen2 model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForPreTraining](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForPreTraining) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForPreTraining](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForPreTraining) (ERNIE model) - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForPreTraining](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForPreTraining) (FNet model) - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model) - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model) - [FlavaConfig](/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig) configuration class: [FlavaForPreTraining](/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining) (FLAVA model) - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForPreTraining](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining) (Funnel Transformer model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model) - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model) - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraForPreTraining](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForPreTraining) (Hiera model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model) - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model) - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model) - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model) - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model) - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model) - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model) - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model) - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model) - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model) - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertForPreTraining](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForPreTraining) (LXMERT model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model) - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model) - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForPreTraining](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining) (Megatron-BERT model) - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model) - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForPreTraining](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForPreTraining) (MobileBERT model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model) - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model) - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model) - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model) - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model) - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForPreTraining](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForPreTraining) (RoCBert model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model) - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model) - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterForPreTraining](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForPreTraining) (Splinter model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model) - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model) - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model) - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model) - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForPreTraining](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForPreTraining) (UniSpeech model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForPreTraining](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining) (UniSpeechSat model) - [ViTMAEConfig](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig) configuration class: [ViTMAEForPreTraining](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining) (ViTMAE model) - [VideoLlavaConfig](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaConfig) configuration class: [VideoLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaForConditionalGeneration) (VideoLlava model) - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEForPreTraining](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForPreTraining) (VideoMAE model) - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model) - [VisualBertConfig](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig) configuration class: [VisualBertForPreTraining](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertForPreTraining) (VisualBERT model) - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining) (Wav2Vec2-Conformer model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model) - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForPreTraining.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- `AlbertForPreTraining` (ALBERT model)
- **audioflamingo3** -- [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
- **bart** -- [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
- **bert** -- [BertForPreTraining](/docs/transformers/main/en/model_doc/bert#transformers.BertForPreTraining) (BERT model)
- **big_bird** -- [BigBirdForPreTraining](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForPreTraining) (BigBird model)
- **bloom** -- [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model)
- **camembert** -- [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model)
- **colpali** -- [ColPaliForRetrieval](/docs/transformers/main/en/model_doc/colpali#transformers.ColPaliForRetrieval) (ColPali model)
- **colqwen2** -- [ColQwen2ForRetrieval](/docs/transformers/main/en/model_doc/colqwen2#transformers.ColQwen2ForRetrieval) (ColQwen2 model)
- **ctrl** -- [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model)
- **data2vec-text** -- [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model)
- **deberta** -- [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model)
- **deberta-v2** -- [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model)
- **distilbert** -- [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model)
- **electra** -- [ElectraForPreTraining](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForPreTraining) (ELECTRA model)
- **ernie** -- [ErnieForPreTraining](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForPreTraining) (ERNIE model)
- **evolla** -- [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
- **exaone4** -- [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model)
- **falcon_mamba** -- [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model)
- **flaubert** -- [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model)
- **flava** -- [FlavaForPreTraining](/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining) (FLAVA model)
- **florence2** -- [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
- **fnet** -- [FNetForPreTraining](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForPreTraining) (FNet model)
- **fsmt** -- [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model)
- **funnel** -- [FunnelForPreTraining](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining) (Funnel Transformer model)
- **gemma3** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
- **gpt-sw3** -- [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (GPT-Sw3 model)
- **gpt2** -- [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model)
- **hiera** -- [HieraForPreTraining](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForPreTraining) (Hiera model)
- **ibert** -- [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model)
- **idefics** -- [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
- **idefics2** -- [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
- **idefics3** -- [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
- **janus** -- [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
- **layoutlm** -- [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model)
- **llava** -- [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
- **llava_next** -- [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
- **longformer** -- [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model)
- **luke** -- [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model)
- **lxmert** -- [LxmertForPreTraining](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForPreTraining) (LXMERT model)
- **mamba** -- [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model)
- **mamba2** -- [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model)
- **megatron-bert** -- [MegatronBertForPreTraining](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining) (Megatron-BERT model)
- **mistral3** -- [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
- **mllama** -- [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
- **mobilebert** -- [MobileBertForPreTraining](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForPreTraining) (MobileBERT model)
- **mpnet** -- [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model)
- **mpt** -- [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model)
- **mra** -- [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model)
- **mvp** -- [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
- **nanochat** -- [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model)
- **nllb-moe** -- [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model)
- **openai-gpt** -- [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model)
- **paligemma** -- [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
- **qwen2_audio** -- [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
- **roberta** -- [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForPreTraining](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForPreTraining) (RoCBert model)
- **rwkv** -- [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model)
- **splinter** -- [SplinterForPreTraining](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForPreTraining) (Splinter model)
- **squeezebert** -- [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model)
- **switch_transformers** -- [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model)
- **t5** -- [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model)
- **t5gemma** -- [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
- **tapas** -- [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model)
- **unispeech** -- [UniSpeechForPreTraining](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForPreTraining) (UniSpeech model)
- **unispeech-sat** -- [UniSpeechSatForPreTraining](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining) (UniSpeechSat model)
- **video_llava** -- [VideoLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llava#transformers.VideoLlavaForConditionalGeneration) (VideoLlava model)
- **videomae** -- [VideoMAEForPreTraining](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForPreTraining) (VideoMAE model)
- **vipllava** -- [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
- **visual_bert** -- [VisualBertForPreTraining](/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertForPreTraining) (VisualBERT model)
- **vit_mae** -- [ViTMAEForPreTraining](/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining) (ViTMAE model)
- **voxtral** -- [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
- **wav2vec2** -- [Wav2Vec2ForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining) (Wav2Vec2 model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerForPreTraining](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining) (Wav2Vec2-Conformer model)
- **xlm** -- [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
- **xlm-roberta** -- [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model)
- **xlstm** -- [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)
- **xmod** -- [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForPreTraining

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForPreTraining.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForPreTraining.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

## Natural Language Processing

The following auto classes are available for the following natural language processing tasks.

### AutoModelForCausalLM[[transformers.AutoModelForCausalLM]]

#### transformers.AutoModelForCausalLM[[transformers.AutoModelForCausalLM]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1947)

This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForCausalLM.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AfmoeConfig](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeConfig) configuration class: [AfmoeForCausalLM](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeForCausalLM) (AFMoE model)
  - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusForCausalLM](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForCausalLM) (Apertus model)
  - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForCausalLM](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForCausalLM) (Arcee model)
  - [AriaTextConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextConfig) configuration class: [AriaTextForCausalLM](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextForCausalLM) (AriaText model)
  - [BambaConfig](/docs/transformers/main/en/model_doc/bamba#transformers.BambaConfig) configuration class: [BambaForCausalLM](/docs/transformers/main/en/model_doc/bamba#transformers.BambaForCausalLM) (Bamba model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForCausalLM](/docs/transformers/main/en/model_doc/bart#transformers.BartForCausalLM) (BART model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertLMHeadModel](/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel) (BERT model)
  - [BertGenerationConfig](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig) configuration class: [BertGenerationDecoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationDecoder) (Bert Generation model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForCausalLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForCausalLM) (BigBird model)
  - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForCausalLM](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM) (BigBird-Pegasus model)
  - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForCausalLM](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForCausalLM) (BioGpt model)
  - [BitNetConfig](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetConfig) configuration class: [BitNetForCausalLM](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetForCausalLM) (BitNet model)
  - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotForCausalLM](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM) (Blenderbot model)
  - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallForCausalLM](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM) (BlenderbotSmall model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model)
  - [BltConfig](/docs/transformers/main/en/model_doc/blt#transformers.BltConfig) configuration class: [BltForCausalLM](/docs/transformers/main/en/model_doc/blt#transformers.BltForCausalLM) (Blt model)
  - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForCausalLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForCausalLM) (CamemBERT model)
  - [CodeGenConfig](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig) configuration class: [CodeGenForCausalLM](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenForCausalLM) (CodeGen model)
  - [Cohere2Config](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Config) configuration class: [Cohere2ForCausalLM](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2ForCausalLM) (Cohere2 model)
  - [CohereConfig](/docs/transformers/main/en/model_doc/cohere#transformers.CohereConfig) configuration class: [CohereForCausalLM](/docs/transformers/main/en/model_doc/cohere#transformers.CohereForCausalLM) (Cohere model)
  - [CpmAntConfig](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig) configuration class: [CpmAntForCausalLM](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntForCausalLM) (CPM-Ant model)
  - [CwmConfig](/docs/transformers/main/en/model_doc/cwm#transformers.CwmConfig) configuration class: [CwmForCausalLM](/docs/transformers/main/en/model_doc/cwm#transformers.CwmForCausalLM) (Code World Model (CWM) model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForCausalLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM) (Data2VecText model)
  - [DbrxConfig](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxConfig) configuration class: [DbrxForCausalLM](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxForCausalLM) (DBRX model)
  - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForCausalLM) (DeepSeek-V2 model)
  - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForCausalLM) (DeepSeek-V3 model)
  - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForCausalLM](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForCausalLM) (DiffLlama model)
  - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeForCausalLM](/docs/transformers/main/en/model_doc/doge#transformers.DogeForCausalLM) (Doge model)
  - [Dots1Config](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Config) configuration class: [Dots1ForCausalLM](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1ForCausalLM) (dots1 model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForCausalLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForCausalLM) (ELECTRA model)
  - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForCausalLM](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForCausalLM) (Emu3 model)
  - [Ernie4_5Config](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Config) configuration class: [Ernie4_5ForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5ForCausalLM) (Ernie4_5 model)
  - [Ernie4_5_MoeConfig](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig) configuration class: [Ernie4_5_MoeForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeForCausalLM) (Ernie4_5_MoE model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForCausalLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForCausalLM) (ERNIE model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model)
  - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForCausalLM](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForCausalLM) (Falcon model)
  - [FalconH1Config](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Config) configuration class: [FalconH1ForCausalLM](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1ForCausalLM) (FalconH1 model)
  - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model)
  - [FlexOlmoConfig](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoConfig) configuration class: [FlexOlmoForCausalLM](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoForCausalLM) (FlexOlmo model)
  - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model)
  - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model)
  - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForCausalLM](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForCausalLM) (GPT-J model)
  - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForCausalLM](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM) (GPT Neo model)
  - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM) (GPT NeoX model)
  - [GPTNeoXJapaneseConfig](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig) configuration class: [GPTNeoXJapaneseForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM) (GPT NeoX Japanese model)
  - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForCausalLM](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForCausalLM) (Gemma2 model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
  - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3ForCausalLM](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForCausalLM) (Gemma3ForCausalLM model)
  - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
  - [Gemma3nTextConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextConfig) configuration class: [Gemma3nForCausalLM](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForCausalLM) (Gemma3nForCausalLM model)
  - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForCausalLM](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForCausalLM) (Gemma model)
  - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
  - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForCausalLM](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForCausalLM) (GLM4 model)
  - [Glm4MoeConfig](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeConfig) configuration class: [Glm4MoeForCausalLM](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeForCausalLM) (Glm4MoE model)
  - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForCausalLM](/docs/transformers/main/en/model_doc/glm#transformers.GlmForCausalLM) (GLM model)
  - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
  - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForCausalLM](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForCausalLM) (GptOss model)
  - [GraniteConfig](/docs/transformers/main/en/model_doc/granite#transformers.GraniteConfig) configuration class: [GraniteForCausalLM](/docs/transformers/main/en/model_doc/granite#transformers.GraniteForCausalLM) (Granite model)
  - [GraniteMoeConfig](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeConfig) configuration class: [GraniteMoeForCausalLM](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeForCausalLM) (GraniteMoeMoe model)
  - [GraniteMoeHybridConfig](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig) configuration class: [GraniteMoeHybridForCausalLM](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridForCausalLM) (GraniteMoeHybrid model)
  - [GraniteMoeSharedConfig](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig) configuration class: [GraniteMoeSharedForCausalLM](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedForCausalLM) (GraniteMoeSharedMoe model)
  - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForCausalLM](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForCausalLM) (Helium model)
  - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForCausalLM) (HunYuanDenseV1 model)
  - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForCausalLM) (HunYuanMoeV1 model)
  - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaForCausalLM](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForCausalLM) (Jamba model)
  - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeForCausalLM](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForCausalLM) (JetMoe model)
  - [Lfm2Config](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Config) configuration class: [Lfm2ForCausalLM](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2ForCausalLM) (Lfm2 model)
  - [Lfm2MoeConfig](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeConfig) configuration class: [Lfm2MoeForCausalLM](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeForCausalLM) (Lfm2Moe model)
  - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4 model)
  - [Llama4TextConfig](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextConfig) configuration class: [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4ForCausalLM model)
  - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForCausalLM](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM) (LLaMA model)
  - [LongcatFlashConfig](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashConfig) configuration class: [LongcatFlashForCausalLM](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashForCausalLM) (LongCatFlash model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForCausalLM](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForCausalLM) (mBART model)
  - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model)
  - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model)
  - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianForCausalLM](/docs/transformers/main/en/model_doc/marian#transformers.MarianForCausalLM) (Marian model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForCausalLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM) (Megatron-BERT model)
  - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForCausalLM](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForCausalLM) (MiniMax model)
  - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForCausalLM](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForCausalLM) (Ministral3 model)
  - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForCausalLM](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForCausalLM) (Ministral model)
  - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForCausalLM](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM) (Mistral model)
  - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForCausalLM](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForCausalLM) (Mixtral model)
  - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForCausalLM](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForCausalLM) (Mllama model)
  - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderForCausalLM](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForCausalLM) (ModernBertDecoder model)
  - [MoshiConfig](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiConfig) configuration class: [MoshiForCausalLM](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiForCausalLM) (Moshi model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model)
  - [MusicgenConfig](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig) configuration class: [MusicgenForCausalLM](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenForCausalLM) (MusicGen model)
  - [MusicgenMelodyConfig](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig) configuration class: [MusicgenMelodyForCausalLM](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyForCausalLM) (MusicGen Melody model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForCausalLM](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForCausalLM) (MVP model)
  - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model)
  - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForCausalLM](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForCausalLM) (Nemotron model)
  - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForCausalLM](/docs/transformers/main/en/model_doc/opt#transformers.OPTForCausalLM) (OPT model)
  - [Olmo2Config](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Config) configuration class: [Olmo2ForCausalLM](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2ForCausalLM) (OLMo2 model)
  - [Olmo3Config](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Config) configuration class: [Olmo3ForCausalLM](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3ForCausalLM) (Olmo3 model)
  - [OlmoConfig](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoConfig) configuration class: [OlmoForCausalLM](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoForCausalLM) (OLMo model)
  - [OlmoeConfig](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeConfig) configuration class: [OlmoeForCausalLM](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeForCausalLM) (OLMoE model)
  - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model)
  - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForCausalLM](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForCausalLM) (PLBart model)
  - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusForCausalLM](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForCausalLM) (Pegasus model)
  - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForCausalLM](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForCausalLM) (Persimmon model)
  - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForCausalLM](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForCausalLM) (Phi3 model)
  - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model)
  - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForCausalLM](/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM) (Phi model)
  - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeForCausalLM](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForCausalLM) (Phimoe model)
  - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetForCausalLM](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM) (ProphetNet model)
  - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForCausalLM](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForCausalLM) (Qwen2 model)
  - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForCausalLM) (Qwen2MoE model)
  - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForCausalLM](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForCausalLM) (Qwen3 model)
  - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForCausalLM) (Qwen3MoE model)
  - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForCausalLM](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForCausalLM) (Qwen3Next model)
  - [RecurrentGemmaConfig](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig) configuration class: [RecurrentGemmaForCausalLM](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM) (RecurrentGemma model)
  - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerModelWithLMHead](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModelWithLMHead) (Reformer model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForCausalLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForCausalLM) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForCausalLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForCausalLM) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForCausalLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForCausalLM) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForCausalLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForCausalLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM) (RoBERTa-PreLayerNorm model)
  - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model)
  - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForCausalLM](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForCausalLM) (SeedOss model)
  - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForCausalLM](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForCausalLM) (SmolLM3 model)
  - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForCausalLM](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM) (StableLm model)
  - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForCausalLM](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForCausalLM) (Starcoder2 model)
  - [TrOCRConfig](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRConfig) configuration class: [TrOCRForCausalLM](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRForCausalLM) (TrOCR model)
  - [VaultGemmaConfig](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaConfig) configuration class: [VaultGemmaForCausalLM](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaForCausalLM) (VaultGemma model)
  - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForCausalLM](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForCausalLM) (Whisper model)
  - [XGLMConfig](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig) configuration class: [XGLMForCausalLM](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMForCausalLM) (XGLM model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForCausalLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForCausalLM) (X-MOD model)
  - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2ForCausalLM](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForCausalLM) (Zamba2 model)
  - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaForCausalLM](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForCausalLM) (Zamba model)
  - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForCausalLM

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForCausalLM.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AfmoeConfig](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeConfig) configuration class: [AfmoeForCausalLM](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeForCausalLM) (AFMoE model) - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusForCausalLM](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForCausalLM) (Apertus model) - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForCausalLM](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForCausalLM) (Arcee model) - [AriaTextConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextConfig) configuration class: [AriaTextForCausalLM](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextForCausalLM) (AriaText model) - [BambaConfig](/docs/transformers/main/en/model_doc/bamba#transformers.BambaConfig) configuration class: [BambaForCausalLM](/docs/transformers/main/en/model_doc/bamba#transformers.BambaForCausalLM) (Bamba model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForCausalLM](/docs/transformers/main/en/model_doc/bart#transformers.BartForCausalLM) (BART model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertLMHeadModel](/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel) (BERT model) - [BertGenerationConfig](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig) configuration class: [BertGenerationDecoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationDecoder) (Bert Generation model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForCausalLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForCausalLM) (BigBird model) - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForCausalLM](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM) (BigBird-Pegasus model) - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForCausalLM](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForCausalLM) (BioGpt model) - [BitNetConfig](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetConfig) configuration class: [BitNetForCausalLM](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetForCausalLM) (BitNet model) - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotForCausalLM](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM) (Blenderbot model) - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallForCausalLM](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM) (BlenderbotSmall model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model) - [BltConfig](/docs/transformers/main/en/model_doc/blt#transformers.BltConfig) configuration class: [BltForCausalLM](/docs/transformers/main/en/model_doc/blt#transformers.BltForCausalLM) (Blt model) - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForCausalLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForCausalLM) (CamemBERT model) - [CodeGenConfig](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig) configuration class: [CodeGenForCausalLM](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenForCausalLM) (CodeGen model) - [Cohere2Config](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2Config) configuration class: [Cohere2ForCausalLM](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2ForCausalLM) (Cohere2 model) - [CohereConfig](/docs/transformers/main/en/model_doc/cohere#transformers.CohereConfig) configuration class: [CohereForCausalLM](/docs/transformers/main/en/model_doc/cohere#transformers.CohereForCausalLM) (Cohere model) - [CpmAntConfig](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig) configuration class: [CpmAntForCausalLM](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntForCausalLM) (CPM-Ant model) - [CwmConfig](/docs/transformers/main/en/model_doc/cwm#transformers.CwmConfig) configuration class: [CwmForCausalLM](/docs/transformers/main/en/model_doc/cwm#transformers.CwmForCausalLM) (Code World Model (CWM) model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForCausalLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM) (Data2VecText model) - [DbrxConfig](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxConfig) configuration class: [DbrxForCausalLM](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxForCausalLM) (DBRX model) - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForCausalLM) (DeepSeek-V2 model) - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForCausalLM) (DeepSeek-V3 model) - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForCausalLM](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForCausalLM) (DiffLlama model) - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeForCausalLM](/docs/transformers/main/en/model_doc/doge#transformers.DogeForCausalLM) (Doge model) - [Dots1Config](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1Config) configuration class: [Dots1ForCausalLM](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1ForCausalLM) (dots1 model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForCausalLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForCausalLM) (ELECTRA model) - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForCausalLM](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForCausalLM) (Emu3 model) - [Ernie4_5Config](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5Config) configuration class: [Ernie4_5ForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5ForCausalLM) (Ernie4_5 model) - [Ernie4_5_MoeConfig](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig) configuration class: [Ernie4_5_MoeForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeForCausalLM) (Ernie4_5_MoE model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForCausalLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForCausalLM) (ERNIE model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model) - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForCausalLM](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForCausalLM) (Falcon model) - [FalconH1Config](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1Config) configuration class: [FalconH1ForCausalLM](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1ForCausalLM) (FalconH1 model) - [FalconMambaConfig](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaConfig) configuration class: [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model) - [FlexOlmoConfig](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoConfig) configuration class: [FlexOlmoForCausalLM](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoForCausalLM) (FlexOlmo model) - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model) - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model) - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForCausalLM](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForCausalLM) (GPT-J model) - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForCausalLM](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM) (GPT Neo model) - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM) (GPT NeoX model) - [GPTNeoXJapaneseConfig](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig) configuration class: [GPTNeoXJapaneseForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM) (GPT NeoX Japanese model) - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForCausalLM](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForCausalLM) (Gemma2 model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model) - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3ForCausalLM](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForCausalLM) (Gemma3ForCausalLM model) - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model) - [Gemma3nTextConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nTextConfig) configuration class: [Gemma3nForCausalLM](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForCausalLM) (Gemma3nForCausalLM model) - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForCausalLM](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForCausalLM) (Gemma model) - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model) - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForCausalLM](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForCausalLM) (GLM4 model) - [Glm4MoeConfig](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeConfig) configuration class: [Glm4MoeForCausalLM](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeForCausalLM) (Glm4MoE model) - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForCausalLM](/docs/transformers/main/en/model_doc/glm#transformers.GlmForCausalLM) (GLM model) - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model) - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForCausalLM](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForCausalLM) (GptOss model) - [GraniteConfig](/docs/transformers/main/en/model_doc/granite#transformers.GraniteConfig) configuration class: [GraniteForCausalLM](/docs/transformers/main/en/model_doc/granite#transformers.GraniteForCausalLM) (Granite model) - [GraniteMoeConfig](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeConfig) configuration class: [GraniteMoeForCausalLM](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeForCausalLM) (GraniteMoeMoe model) - [GraniteMoeHybridConfig](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig) configuration class: [GraniteMoeHybridForCausalLM](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridForCausalLM) (GraniteMoeHybrid model) - [GraniteMoeSharedConfig](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig) configuration class: [GraniteMoeSharedForCausalLM](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedForCausalLM) (GraniteMoeSharedMoe model) - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForCausalLM](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForCausalLM) (Helium model) - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForCausalLM) (HunYuanDenseV1 model) - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForCausalLM) (HunYuanMoeV1 model) - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaForCausalLM](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForCausalLM) (Jamba model) - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeForCausalLM](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForCausalLM) (JetMoe model) - [Lfm2Config](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2Config) configuration class: [Lfm2ForCausalLM](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2ForCausalLM) (Lfm2 model) - [Lfm2MoeConfig](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeConfig) configuration class: [Lfm2MoeForCausalLM](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeForCausalLM) (Lfm2Moe model) - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4 model) - [Llama4TextConfig](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4TextConfig) configuration class: [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4ForCausalLM model) - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForCausalLM](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM) (LLaMA model) - [LongcatFlashConfig](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashConfig) configuration class: [LongcatFlashForCausalLM](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashForCausalLM) (LongCatFlash model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForCausalLM](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForCausalLM) (mBART model) - [Mamba2Config](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2Config) configuration class: [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model) - [MambaConfig](/docs/transformers/main/en/model_doc/mamba#transformers.MambaConfig) configuration class: [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model) - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianForCausalLM](/docs/transformers/main/en/model_doc/marian#transformers.MarianForCausalLM) (Marian model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForCausalLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM) (Megatron-BERT model) - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForCausalLM](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForCausalLM) (MiniMax model) - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForCausalLM](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForCausalLM) (Ministral3 model) - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForCausalLM](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForCausalLM) (Ministral model) - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForCausalLM](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM) (Mistral model) - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForCausalLM](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForCausalLM) (Mixtral model) - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForCausalLM](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForCausalLM) (Mllama model) - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderForCausalLM](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForCausalLM) (ModernBertDecoder model) - [MoshiConfig](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiConfig) configuration class: [MoshiForCausalLM](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiForCausalLM) (Moshi model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model) - [MusicgenConfig](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig) configuration class: [MusicgenForCausalLM](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenForCausalLM) (MusicGen model) - [MusicgenMelodyConfig](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig) configuration class: [MusicgenMelodyForCausalLM](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyForCausalLM) (MusicGen Melody model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForCausalLM](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForCausalLM) (MVP model) - [NanoChatConfig](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatConfig) configuration class: [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model) - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForCausalLM](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForCausalLM) (Nemotron model) - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForCausalLM](/docs/transformers/main/en/model_doc/opt#transformers.OPTForCausalLM) (OPT model) - [Olmo2Config](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2Config) configuration class: [Olmo2ForCausalLM](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2ForCausalLM) (OLMo2 model) - [Olmo3Config](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3Config) configuration class: [Olmo3ForCausalLM](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3ForCausalLM) (Olmo3 model) - [OlmoConfig](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoConfig) configuration class: [OlmoForCausalLM](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoForCausalLM) (OLMo model) - [OlmoeConfig](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeConfig) configuration class: [OlmoeForCausalLM](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeForCausalLM) (OLMoE model) - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model) - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForCausalLM](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForCausalLM) (PLBart model) - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusForCausalLM](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForCausalLM) (Pegasus model) - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForCausalLM](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForCausalLM) (Persimmon model) - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForCausalLM](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForCausalLM) (Phi3 model) - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model) - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForCausalLM](/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM) (Phi model) - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeForCausalLM](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForCausalLM) (Phimoe model) - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetForCausalLM](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM) (ProphetNet model) - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForCausalLM](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForCausalLM) (Qwen2 model) - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForCausalLM) (Qwen2MoE model) - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForCausalLM](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForCausalLM) (Qwen3 model) - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForCausalLM) (Qwen3MoE model) - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForCausalLM](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForCausalLM) (Qwen3Next model) - [RecurrentGemmaConfig](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig) configuration class: [RecurrentGemmaForCausalLM](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM) (RecurrentGemma model) - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerModelWithLMHead](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModelWithLMHead) (Reformer model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForCausalLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForCausalLM) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForCausalLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForCausalLM) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForCausalLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForCausalLM) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForCausalLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForCausalLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM) (RoBERTa-PreLayerNorm model) - [RwkvConfig](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig) configuration class: [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model) - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForCausalLM](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForCausalLM) (SeedOss model) - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForCausalLM](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForCausalLM) (SmolLM3 model) - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForCausalLM](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM) (StableLm model) - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForCausalLM](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForCausalLM) (Starcoder2 model) - [TrOCRConfig](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRConfig) configuration class: [TrOCRForCausalLM](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRForCausalLM) (TrOCR model) - [VaultGemmaConfig](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaConfig) configuration class: [VaultGemmaForCausalLM](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaForCausalLM) (VaultGemma model) - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForCausalLM](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForCausalLM) (Whisper model) - [XGLMConfig](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig) configuration class: [XGLMForCausalLM](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMForCausalLM) (XGLM model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForCausalLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForCausalLM) (X-MOD model) - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2ForCausalLM](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForCausalLM) (Zamba2 model) - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaForCausalLM](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForCausalLM) (Zamba model) - [xLSTMConfig](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMConfig) configuration class: [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForCausalLM.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **afmoe** -- [AfmoeForCausalLM](/docs/transformers/main/en/model_doc/afmoe#transformers.AfmoeForCausalLM) (AFMoE model)
- **apertus** -- [ApertusForCausalLM](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForCausalLM) (Apertus model)
- **arcee** -- [ArceeForCausalLM](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForCausalLM) (Arcee model)
- **aria_text** -- [AriaTextForCausalLM](/docs/transformers/main/en/model_doc/aria#transformers.AriaTextForCausalLM) (AriaText model)
- **bamba** -- [BambaForCausalLM](/docs/transformers/main/en/model_doc/bamba#transformers.BambaForCausalLM) (Bamba model)
- **bart** -- [BartForCausalLM](/docs/transformers/main/en/model_doc/bart#transformers.BartForCausalLM) (BART model)
- **bert** -- [BertLMHeadModel](/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel) (BERT model)
- **bert-generation** -- [BertGenerationDecoder](/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationDecoder) (Bert Generation model)
- **big_bird** -- [BigBirdForCausalLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForCausalLM) (BigBird model)
- **bigbird_pegasus** -- [BigBirdPegasusForCausalLM](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM) (BigBird-Pegasus model)
- **biogpt** -- [BioGptForCausalLM](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForCausalLM) (BioGpt model)
- **bitnet** -- [BitNetForCausalLM](/docs/transformers/main/en/model_doc/bitnet#transformers.BitNetForCausalLM) (BitNet model)
- **blenderbot** -- [BlenderbotForCausalLM](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM) (Blenderbot model)
- **blenderbot-small** -- [BlenderbotSmallForCausalLM](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM) (BlenderbotSmall model)
- **bloom** -- [BloomForCausalLM](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM) (BLOOM model)
- **blt** -- [BltForCausalLM](/docs/transformers/main/en/model_doc/blt#transformers.BltForCausalLM) (Blt model)
- **camembert** -- [CamembertForCausalLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForCausalLM) (CamemBERT model)
- **code_llama** -- [LlamaForCausalLM](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM) (CodeLlama model)
- **codegen** -- [CodeGenForCausalLM](/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenForCausalLM) (CodeGen model)
- **cohere** -- [CohereForCausalLM](/docs/transformers/main/en/model_doc/cohere#transformers.CohereForCausalLM) (Cohere model)
- **cohere2** -- [Cohere2ForCausalLM](/docs/transformers/main/en/model_doc/cohere2#transformers.Cohere2ForCausalLM) (Cohere2 model)
- **cpmant** -- [CpmAntForCausalLM](/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntForCausalLM) (CPM-Ant model)
- **ctrl** -- [CTRLLMHeadModel](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel) (CTRL model)
- **cwm** -- [CwmForCausalLM](/docs/transformers/main/en/model_doc/cwm#transformers.CwmForCausalLM) (Code World Model (CWM) model)
- **data2vec-text** -- [Data2VecTextForCausalLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM) (Data2VecText model)
- **dbrx** -- [DbrxForCausalLM](/docs/transformers/main/en/model_doc/dbrx#transformers.DbrxForCausalLM) (DBRX model)
- **deepseek_v2** -- [DeepseekV2ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForCausalLM) (DeepSeek-V2 model)
- **deepseek_v3** -- [DeepseekV3ForCausalLM](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForCausalLM) (DeepSeek-V3 model)
- **diffllama** -- [DiffLlamaForCausalLM](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForCausalLM) (DiffLlama model)
- **doge** -- [DogeForCausalLM](/docs/transformers/main/en/model_doc/doge#transformers.DogeForCausalLM) (Doge model)
- **dots1** -- [Dots1ForCausalLM](/docs/transformers/main/en/model_doc/dots1#transformers.Dots1ForCausalLM) (dots1 model)
- **electra** -- [ElectraForCausalLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForCausalLM) (ELECTRA model)
- **emu3** -- [Emu3ForCausalLM](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForCausalLM) (Emu3 model)
- **ernie** -- [ErnieForCausalLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForCausalLM) (ERNIE model)
- **ernie4_5** -- [Ernie4_5ForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5#transformers.Ernie4_5ForCausalLM) (Ernie4_5 model)
- **ernie4_5_moe** -- [Ernie4_5_MoeForCausalLM](/docs/transformers/main/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeForCausalLM) (Ernie4_5_MoE model)
- **exaone4** -- [Exaone4ForCausalLM](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForCausalLM) (EXAONE-4.0 model)
- **falcon** -- [FalconForCausalLM](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForCausalLM) (Falcon model)
- **falcon_h1** -- [FalconH1ForCausalLM](/docs/transformers/main/en/model_doc/falcon_h1#transformers.FalconH1ForCausalLM) (FalconH1 model)
- **falcon_mamba** -- [FalconMambaForCausalLM](/docs/transformers/main/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM) (FalconMamba model)
- **flex_olmo** -- [FlexOlmoForCausalLM](/docs/transformers/main/en/model_doc/flex_olmo#transformers.FlexOlmoForCausalLM) (FlexOlmo model)
- **fuyu** -- [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
- **gemma** -- [GemmaForCausalLM](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForCausalLM) (Gemma model)
- **gemma2** -- [Gemma2ForCausalLM](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForCausalLM) (Gemma2 model)
- **gemma3** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
- **gemma3_text** -- [Gemma3ForCausalLM](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForCausalLM) (Gemma3ForCausalLM model)
- **gemma3n** -- [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
- **gemma3n_text** -- [Gemma3nForCausalLM](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForCausalLM) (Gemma3nForCausalLM model)
- **git** -- [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
- **glm** -- [GlmForCausalLM](/docs/transformers/main/en/model_doc/glm#transformers.GlmForCausalLM) (GLM model)
- **glm4** -- [Glm4ForCausalLM](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForCausalLM) (GLM4 model)
- **glm4_moe** -- [Glm4MoeForCausalLM](/docs/transformers/main/en/model_doc/glm4_moe#transformers.Glm4MoeForCausalLM) (Glm4MoE model)
- **got_ocr2** -- [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
- **gpt-sw3** -- [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (GPT-Sw3 model)
- **gpt2** -- [GPT2LMHeadModel](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeForCausalLM](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM) (GPTBigCode model)
- **gpt_neo** -- [GPTNeoForCausalLM](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM) (GPT NeoX model)
- **gpt_neox_japanese** -- [GPTNeoXJapaneseForCausalLM](/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM) (GPT NeoX Japanese model)
- **gpt_oss** -- [GptOssForCausalLM](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForCausalLM) (GptOss model)
- **gptj** -- [GPTJForCausalLM](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForCausalLM) (GPT-J model)
- **granite** -- [GraniteForCausalLM](/docs/transformers/main/en/model_doc/granite#transformers.GraniteForCausalLM) (Granite model)
- **granitemoe** -- [GraniteMoeForCausalLM](/docs/transformers/main/en/model_doc/granitemoe#transformers.GraniteMoeForCausalLM) (GraniteMoeMoe model)
- **granitemoehybrid** -- [GraniteMoeHybridForCausalLM](/docs/transformers/main/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridForCausalLM) (GraniteMoeHybrid model)
- **granitemoeshared** -- [GraniteMoeSharedForCausalLM](/docs/transformers/main/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedForCausalLM) (GraniteMoeSharedMoe model)
- **helium** -- [HeliumForCausalLM](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForCausalLM) (Helium model)
- **hunyuan_v1_dense** -- [HunYuanDenseV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForCausalLM) (HunYuanDenseV1 model)
- **hunyuan_v1_moe** -- [HunYuanMoEV1ForCausalLM](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForCausalLM) (HunYuanMoeV1 model)
- **jamba** -- [JambaForCausalLM](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForCausalLM) (Jamba model)
- **jetmoe** -- [JetMoeForCausalLM](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForCausalLM) (JetMoe model)
- **lfm2** -- [Lfm2ForCausalLM](/docs/transformers/main/en/model_doc/lfm2#transformers.Lfm2ForCausalLM) (Lfm2 model)
- **lfm2_moe** -- [Lfm2MoeForCausalLM](/docs/transformers/main/en/model_doc/lfm2_moe#transformers.Lfm2MoeForCausalLM) (Lfm2Moe model)
- **llama** -- [LlamaForCausalLM](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM) (LLaMA model)
- **llama4** -- [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4 model)
- **llama4_text** -- [Llama4ForCausalLM](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForCausalLM) (Llama4ForCausalLM model)
- **longcat_flash** -- [LongcatFlashForCausalLM](/docs/transformers/main/en/model_doc/longcat_flash#transformers.LongcatFlashForCausalLM) (LongCatFlash model)
- **mamba** -- [MambaForCausalLM](/docs/transformers/main/en/model_doc/mamba#transformers.MambaForCausalLM) (Mamba model)
- **mamba2** -- [Mamba2ForCausalLM](/docs/transformers/main/en/model_doc/mamba2#transformers.Mamba2ForCausalLM) (mamba2 model)
- **marian** -- [MarianForCausalLM](/docs/transformers/main/en/model_doc/marian#transformers.MarianForCausalLM) (Marian model)
- **mbart** -- [MBartForCausalLM](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForCausalLM) (mBART model)
- **megatron-bert** -- [MegatronBertForCausalLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM) (Megatron-BERT model)
- **minimax** -- [MiniMaxForCausalLM](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForCausalLM) (MiniMax model)
- **ministral** -- [MinistralForCausalLM](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForCausalLM) (Ministral model)
- **ministral3** -- [Ministral3ForCausalLM](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForCausalLM) (Ministral3 model)
- **mistral** -- [MistralForCausalLM](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM) (Mistral model)
- **mixtral** -- [MixtralForCausalLM](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForCausalLM) (Mixtral model)
- **mllama** -- [MllamaForCausalLM](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForCausalLM) (Mllama model)
- **modernbert-decoder** -- [ModernBertDecoderForCausalLM](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForCausalLM) (ModernBertDecoder model)
- **moshi** -- [MoshiForCausalLM](/docs/transformers/main/en/model_doc/moshi#transformers.MoshiForCausalLM) (Moshi model)
- **mpt** -- [MptForCausalLM](/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM) (MPT model)
- **musicgen** -- [MusicgenForCausalLM](/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenForCausalLM) (MusicGen model)
- **musicgen_melody** -- [MusicgenMelodyForCausalLM](/docs/transformers/main/en/model_doc/musicgen_melody#transformers.MusicgenMelodyForCausalLM) (MusicGen Melody model)
- **mvp** -- [MvpForCausalLM](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForCausalLM) (MVP model)
- **nanochat** -- [NanoChatForCausalLM](/docs/transformers/main/en/model_doc/nanochat#transformers.NanoChatForCausalLM) (NanoChat model)
- **nemotron** -- [NemotronForCausalLM](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForCausalLM) (Nemotron model)
- **olmo** -- [OlmoForCausalLM](/docs/transformers/main/en/model_doc/olmo#transformers.OlmoForCausalLM) (OLMo model)
- **olmo2** -- [Olmo2ForCausalLM](/docs/transformers/main/en/model_doc/olmo2#transformers.Olmo2ForCausalLM) (OLMo2 model)
- **olmo3** -- [Olmo3ForCausalLM](/docs/transformers/main/en/model_doc/olmo3#transformers.Olmo3ForCausalLM) (Olmo3 model)
- **olmoe** -- [OlmoeForCausalLM](/docs/transformers/main/en/model_doc/olmoe#transformers.OlmoeForCausalLM) (OLMoE model)
- **openai-gpt** -- [OpenAIGPTLMHeadModel](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) (OpenAI GPT model)
- **opt** -- [OPTForCausalLM](/docs/transformers/main/en/model_doc/opt#transformers.OPTForCausalLM) (OPT model)
- **pegasus** -- [PegasusForCausalLM](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForCausalLM) (Pegasus model)
- **persimmon** -- [PersimmonForCausalLM](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForCausalLM) (Persimmon model)
- **phi** -- [PhiForCausalLM](/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM) (Phi model)
- **phi3** -- [Phi3ForCausalLM](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForCausalLM) (Phi3 model)
- **phi4_multimodal** -- [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model)
- **phimoe** -- [PhimoeForCausalLM](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForCausalLM) (Phimoe model)
- **plbart** -- [PLBartForCausalLM](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForCausalLM) (PLBart model)
- **prophetnet** -- [ProphetNetForCausalLM](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM) (ProphetNet model)
- **qwen2** -- [Qwen2ForCausalLM](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForCausalLM) (Qwen2 model)
- **qwen2_moe** -- [Qwen2MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForCausalLM) (Qwen2MoE model)
- **qwen3** -- [Qwen3ForCausalLM](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForCausalLM) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeForCausalLM](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForCausalLM) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextForCausalLM](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForCausalLM) (Qwen3Next model)
- **recurrent_gemma** -- [RecurrentGemmaForCausalLM](/docs/transformers/main/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM) (RecurrentGemma model)
- **reformer** -- [ReformerModelWithLMHead](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModelWithLMHead) (Reformer model)
- **rembert** -- [RemBertForCausalLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForCausalLM) (RemBERT model)
- **roberta** -- [RobertaForCausalLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForCausalLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForCausalLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForCausalLM) (RoCBert model)
- **roformer** -- [RoFormerForCausalLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForCausalLM) (RoFormer model)
- **rwkv** -- [RwkvForCausalLM](/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM) (RWKV model)
- **seed_oss** -- [SeedOssForCausalLM](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForCausalLM) (SeedOss model)
- **smollm3** -- [SmolLM3ForCausalLM](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForCausalLM) (SmolLM3 model)
- **stablelm** -- [StableLmForCausalLM](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM) (StableLm model)
- **starcoder2** -- [Starcoder2ForCausalLM](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForCausalLM) (Starcoder2 model)
- **trocr** -- [TrOCRForCausalLM](/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRForCausalLM) (TrOCR model)
- **vaultgemma** -- [VaultGemmaForCausalLM](/docs/transformers/main/en/model_doc/vaultgemma#transformers.VaultGemmaForCausalLM) (VaultGemma model)
- **whisper** -- [WhisperForCausalLM](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForCausalLM) (Whisper model)
- **xglm** -- [XGLMForCausalLM](/docs/transformers/main/en/model_doc/xglm#transformers.XGLMForCausalLM) (XGLM model)
- **xlm** -- [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
- **xlm-roberta** -- [XLMRobertaForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForCausalLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetLMHeadModel](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel) (XLNet model)
- **xlstm** -- [xLSTMForCausalLM](/docs/transformers/main/en/model_doc/xlstm#transformers.xLSTMForCausalLM) (xLSTM model)
- **xmod** -- [XmodForCausalLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForCausalLM) (X-MOD model)
- **zamba** -- [ZambaForCausalLM](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForCausalLM) (Zamba model)
- **zamba2** -- [Zamba2ForCausalLM](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForCausalLM) (Zamba2 model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForCausalLM

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForCausalLM.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForCausalLM.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForMaskedLM[[transformers.AutoModelForMaskedLM]]

#### transformers.AutoModelForMaskedLM[[transformers.AutoModelForMaskedLM]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1964)

This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForMaskedLM.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForMaskedLM` (ALBERT model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForMaskedLM](/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForMaskedLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMaskedLM) (BigBird model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForMaskedLM](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMaskedLM) (ConvBERT model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForMaskedLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMaskedLM) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForMaskedLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMaskedLM) (ERNIE model)
  - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForMaskedLM](/docs/transformers/main/en/model_doc/esm#transformers.EsmForMaskedLM) (ESM model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForMaskedLM](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMaskedLM) (FNet model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForMaskedLM](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM) (Funnel Transformer model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model)
  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForMaskedLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM) (Megatron-BERT model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForMaskedLM](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM) (MobileBERT model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForMaskedLM](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMaskedLM) (ModernBERT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForMaskedLM](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM) (Nystr√∂mformer model)
  - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForMaskedLM](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForMaskedLM) (Perceiver model)
  - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForMaskedLM](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForMaskedLM) (Reformer model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForMaskedLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMaskedLM) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForMaskedLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForMaskedLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMaskedLM) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model)
  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: `Wav2Vec2ForMaskedLM` (Wav2Vec2 model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForMaskedLM](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMaskedLM) (YOSO model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMaskedLM

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForMaskedLM.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForMaskedLM` (ALBERT model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForMaskedLM](/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForMaskedLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMaskedLM) (BigBird model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForMaskedLM](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMaskedLM) (ConvBERT model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForMaskedLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMaskedLM) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForMaskedLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMaskedLM) (ERNIE model) - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForMaskedLM](/docs/transformers/main/en/model_doc/esm#transformers.EsmForMaskedLM) (ESM model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForMaskedLM](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMaskedLM) (FNet model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForMaskedLM](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM) (Funnel Transformer model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model) - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForMaskedLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM) (Megatron-BERT model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForMaskedLM](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM) (MobileBERT model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForMaskedLM](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMaskedLM) (ModernBERT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForMaskedLM](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM) (Nystr√∂mformer model) - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForMaskedLM](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForMaskedLM) (Perceiver model) - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForMaskedLM](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForMaskedLM) (Reformer model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForMaskedLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMaskedLM) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForMaskedLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForMaskedLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMaskedLM) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model) - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: `Wav2Vec2ForMaskedLM` (Wav2Vec2 model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForMaskedLM](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMaskedLM) (YOSO model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForMaskedLM.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- `AlbertForMaskedLM` (ALBERT model)
- **bart** -- [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
- **bert** -- [BertForMaskedLM](/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM) (BERT model)
- **big_bird** -- [BigBirdForMaskedLM](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMaskedLM) (BigBird model)
- **camembert** -- [CamembertForMaskedLM](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM) (CamemBERT model)
- **convbert** -- [ConvBertForMaskedLM](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMaskedLM) (ConvBERT model)
- **data2vec-text** -- [Data2VecTextForMaskedLM](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM) (Data2VecText model)
- **deberta** -- [DebertaForMaskedLM](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM) (DeBERTa model)
- **deberta-v2** -- [DebertaV2ForMaskedLM](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM) (DeBERTa-v2 model)
- **distilbert** -- [DistilBertForMaskedLM](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM) (DistilBERT model)
- **electra** -- [ElectraForMaskedLM](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMaskedLM) (ELECTRA model)
- **ernie** -- [ErnieForMaskedLM](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMaskedLM) (ERNIE model)
- **esm** -- [EsmForMaskedLM](/docs/transformers/main/en/model_doc/esm#transformers.EsmForMaskedLM) (ESM model)
- **flaubert** -- [FlaubertWithLMHeadModel](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel) (FlauBERT model)
- **fnet** -- [FNetForMaskedLM](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMaskedLM) (FNet model)
- **funnel** -- [FunnelForMaskedLM](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM) (Funnel Transformer model)
- **ibert** -- [IBertForMaskedLM](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM) (I-BERT model)
- **layoutlm** -- [LayoutLMForMaskedLM](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM) (LayoutLM model)
- **longformer** -- [LongformerForMaskedLM](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM) (Longformer model)
- **luke** -- [LukeForMaskedLM](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM) (LUKE model)
- **mbart** -- [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model)
- **megatron-bert** -- [MegatronBertForMaskedLM](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM) (Megatron-BERT model)
- **mobilebert** -- [MobileBertForMaskedLM](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM) (MobileBERT model)
- **modernbert** -- [ModernBertForMaskedLM](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMaskedLM) (ModernBERT model)
- **mpnet** -- [MPNetForMaskedLM](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM) (MPNet model)
- **mra** -- [MraForMaskedLM](/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM) (MRA model)
- **mvp** -- [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
- **nystromformer** -- [NystromformerForMaskedLM](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM) (Nystr√∂mformer model)
- **perceiver** -- [PerceiverForMaskedLM](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForMaskedLM) (Perceiver model)
- **reformer** -- [ReformerForMaskedLM](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForMaskedLM) (Reformer model)
- **rembert** -- [RemBertForMaskedLM](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMaskedLM) (RemBERT model)
- **roberta** -- [RobertaForMaskedLM](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForMaskedLM](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForMaskedLM](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM) (RoCBert model)
- **roformer** -- [RoFormerForMaskedLM](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMaskedLM) (RoFormer model)
- **squeezebert** -- [SqueezeBertForMaskedLM](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM) (SqueezeBERT model)
- **tapas** -- [TapasForMaskedLM](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM) (TAPAS model)
- **wav2vec2** -- `Wav2Vec2ForMaskedLM` (Wav2Vec2 model)
- **xlm** -- [XLMWithLMHeadModel](/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel) (XLM model)
- **xlm-roberta** -- [XLMRobertaForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForMaskedLM](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM) (XLM-RoBERTa-XL model)
- **xmod** -- [XmodForMaskedLM](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM) (X-MOD model)
- **yoso** -- [YosoForMaskedLM](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMaskedLM) (YOSO model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMaskedLM

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForMaskedLM.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForMaskedLM.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForMaskGeneration[[transformers.AutoModelForMaskGeneration]]

#### transformers.AutoModelForMaskGeneration[[transformers.AutoModelForMaskGeneration]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1905)

### AutoModelForSeq2SeqLM[[transformers.AutoModelForSeq2SeqLM]]

#### transformers.AutoModelForSeq2SeqLM[[transformers.AutoModelForSeq2SeqLM]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1971)

This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForSeq2SeqLM.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
  - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration) (BigBird-Pegasus model)
  - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration) (Blenderbot model)
  - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration) (BlenderbotSmall model)
  - [EncoderDecoderConfig](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig) configuration class: [EncoderDecoderModel](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) (Encoder decoder model)
  - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model)
  - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
  - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForConditionalGeneration](/docs/transformers/main/en/model_doc/led#transformers.LEDForConditionalGeneration) (LED model)
  - [LongT5Config](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config) configuration class: [LongT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration) (LongT5 model)
  - [M2M100Config](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config) configuration class: [M2M100ForConditionalGeneration](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration) (M2M100 model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model)
  - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForConditionalGeneration) (MT5 model)
  - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianMTModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianMTModel) (Marian model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
  - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model)
  - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForConditionalGeneration](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForConditionalGeneration) (PLBart model)
  - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration) (Pegasus model)
  - [PegasusXConfig](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig) configuration class: [PegasusXForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration) (PEGASUS-X model)
  - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetForConditionalGeneration](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration) (ProphetNet model)
  - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
  - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText) (SeamlessM4T model)
  - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2ForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText) (SeamlessM4Tv2 model)
  - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
  - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model)
  - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration) (UMT5 model)
  - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSeq2SeqLM

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-t5/t5-base")
>>> model = AutoModelForSeq2SeqLM.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AudioFlamingo3Config](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3Config) configuration class: [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model) - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration) (BigBird-Pegasus model) - [BlenderbotConfig](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig) configuration class: [BlenderbotForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration) (Blenderbot model) - [BlenderbotSmallConfig](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig) configuration class: [BlenderbotSmallForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration) (BlenderbotSmall model) - [EncoderDecoderConfig](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig) configuration class: [EncoderDecoderModel](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) (Encoder decoder model) - [FSMTConfig](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig) configuration class: [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model) - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model) - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForConditionalGeneration](/docs/transformers/main/en/model_doc/led#transformers.LEDForConditionalGeneration) (LED model) - [LongT5Config](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config) configuration class: [LongT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration) (LongT5 model) - [M2M100Config](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config) configuration class: [M2M100ForConditionalGeneration](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration) (M2M100 model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model) - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForConditionalGeneration) (MT5 model) - [MarianConfig](/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig) configuration class: [MarianMTModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianMTModel) (Marian model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model) - [NllbMoeConfig](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig) configuration class: [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model) - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForConditionalGeneration](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForConditionalGeneration) (PLBart model) - [PegasusConfig](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig) configuration class: [PegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration) (Pegasus model) - [PegasusXConfig](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig) configuration class: [PegasusXForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration) (PEGASUS-X model) - [ProphetNetConfig](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig) configuration class: [ProphetNetForConditionalGeneration](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration) (ProphetNet model) - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model) - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText) (SeamlessM4T model) - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2ForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText) (SeamlessM4Tv2 model) - [SwitchTransformersConfig](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig) configuration class: [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model) - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model) - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration) (UMT5 model) - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForSeq2SeqLM.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **audioflamingo3** -- [AudioFlamingo3ForConditionalGeneration](/docs/transformers/main/en/model_doc/audioflamingo3#transformers.AudioFlamingo3ForConditionalGeneration) (AudioFlamingo3 model)
- **bart** -- [BartForConditionalGeneration](/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration) (BART model)
- **bigbird_pegasus** -- [BigBirdPegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration) (BigBird-Pegasus model)
- **blenderbot** -- [BlenderbotForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration) (Blenderbot model)
- **blenderbot-small** -- [BlenderbotSmallForConditionalGeneration](/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration) (BlenderbotSmall model)
- **encoder-decoder** -- [EncoderDecoderModel](/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) (Encoder decoder model)
- **fsmt** -- [FSMTForConditionalGeneration](/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration) (FairSeq Machine-Translation model)
- **granite_speech** -- [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
- **led** -- [LEDForConditionalGeneration](/docs/transformers/main/en/model_doc/led#transformers.LEDForConditionalGeneration) (LED model)
- **longt5** -- [LongT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration) (LongT5 model)
- **m2m_100** -- [M2M100ForConditionalGeneration](/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration) (M2M100 model)
- **marian** -- [MarianMTModel](/docs/transformers/main/en/model_doc/marian#transformers.MarianMTModel) (Marian model)
- **mbart** -- [MBartForConditionalGeneration](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration) (mBART model)
- **mt5** -- [MT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForConditionalGeneration) (MT5 model)
- **mvp** -- [MvpForConditionalGeneration](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration) (MVP model)
- **nllb-moe** -- [NllbMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration) (NLLB-MOE model)
- **pegasus** -- [PegasusForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration) (Pegasus model)
- **pegasus_x** -- [PegasusXForConditionalGeneration](/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration) (PEGASUS-X model)
- **plbart** -- [PLBartForConditionalGeneration](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForConditionalGeneration) (PLBart model)
- **prophetnet** -- [ProphetNetForConditionalGeneration](/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration) (ProphetNet model)
- **qwen2_audio** -- [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
- **seamless_m4t** -- [SeamlessM4TForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4Tv2ForTextToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText) (SeamlessM4Tv2 model)
- **switch_transformers** -- [SwitchTransformersForConditionalGeneration](/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration) (SwitchTransformers model)
- **t5** -- [T5ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration) (T5 model)
- **t5gemma** -- [T5GemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
- **umt5** -- [UMT5ForConditionalGeneration](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration) (UMT5 model)
- **voxtral** -- [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSeq2SeqLM

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-base")

>>> # Update configuration during loading
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-base", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForSequenceClassification[[transformers.AutoModelForSequenceClassification]]

#### transformers.AutoModelForSequenceClassification[[transformers.AutoModelForSequenceClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1982)

This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForSequenceClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForSequenceClassification` (ALBERT model)
  - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForSequenceClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForSequenceClassification) (Arcee model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForSequenceClassification](/docs/transformers/main/en/model_doc/bart#transformers.BartForSequenceClassification) (BART model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForSequenceClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForSequenceClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification) (BigBird model)
  - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForSequenceClassification](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification) (BigBird-Pegasus model)
  - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForSequenceClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForSequenceClassification) (BioGpt model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForSequenceClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForSequenceClassification) (BLOOM model)
  - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLForSequenceClassification](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLForSequenceClassification) (CTRL model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForSequenceClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForSequenceClassification) (CamemBERT model)
  - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForSequenceClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForSequenceClassification) (CANINE model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForSequenceClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForSequenceClassification) (ConvBERT model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification) (Data2VecText model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForSequenceClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForSequenceClassification) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification) (DeBERTa-v2 model)
  - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForSequenceClassification) (DeepSeek-V2 model)
  - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForSequenceClassification) (DeepSeek-V3 model)
  - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForSequenceClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForSequenceClassification) (DiffLlama model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForSequenceClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) (DistilBERT model)
  - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeForSequenceClassification](/docs/transformers/main/en/model_doc/doge#transformers.DogeForSequenceClassification) (Doge model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForSequenceClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForSequenceClassification) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForSequenceClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForSequenceClassification) (ERNIE model)
  - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForSequenceClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForSequenceClassification) (ESM model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForSequenceClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForSequenceClassification) (EXAONE-4.0 model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForSequenceClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForSequenceClassification) (FNet model)
  - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForSequenceClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForSequenceClassification) (Falcon model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForSequenceClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification) (FlauBERT model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForSequenceClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification) (Funnel Transformer model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForSequenceClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification) (OpenAI GPT-2 model)
  - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification) (GPTBigCode model)
  - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForSequenceClassification](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForSequenceClassification) (GPT-J model)
  - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification) (GPT Neo model)
  - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification) (GPT NeoX model)
  - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForSequenceClassification) (Gemma2 model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForSequenceClassification) (Gemma3ForConditionalGeneration model)
  - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3TextForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextForSequenceClassification) (Gemma3ForCausalLM model)
  - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForSequenceClassification) (Gemma model)
  - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForSequenceClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForSequenceClassification) (GLM4 model)
  - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForSequenceClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForSequenceClassification) (GLM model)
  - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForSequenceClassification) (GptOss model)
  - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForSequenceClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForSequenceClassification) (Helium model)
  - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForSequenceClassification) (HunYuanDenseV1 model)
  - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForSequenceClassification) (HunYuanMoeV1 model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForSequenceClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForSequenceClassification) (I-BERT model)
  - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaForSequenceClassification](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForSequenceClassification) (Jamba model)
  - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeForSequenceClassification](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForSequenceClassification) (JetMoe model)
  - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForSequenceClassification](/docs/transformers/main/en/model_doc/led#transformers.LEDForSequenceClassification) (LED model)
  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification) (LayoutLM model)
  - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification) (LayoutLMv2 model)
  - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification) (LayoutLMv3 model)
  - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForSequenceClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForSequenceClassification) (LiLT model)
  - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForSequenceClassification](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification) (LLaMA model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForSequenceClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForSequenceClassification) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForSequenceClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForSequenceClassification) (LUKE model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForSequenceClassification](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForSequenceClassification) (mBART model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForSequenceClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForSequenceClassification) (MPNet model)
  - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForSequenceClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForSequenceClassification) (MT5 model)
  - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForSequenceClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification) (MarkupLM model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForSequenceClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification) (Megatron-BERT model)
  - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForSequenceClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForSequenceClassification) (MiniMax model)
  - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForSequenceClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForSequenceClassification) (Ministral3 model)
  - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForSequenceClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForSequenceClassification) (Ministral model)
  - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForSequenceClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification) (Mistral model)
  - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForSequenceClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForSequenceClassification) (Mixtral model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForSequenceClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification) (MobileBERT model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification) (ModernBERT model)
  - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForSequenceClassification) (ModernBertDecoder model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForSequenceClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForSequenceClassification) (MPT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForSequenceClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForSequenceClassification) (MRA model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForSequenceClassification](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForSequenceClassification) (MVP model)
  - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForSequenceClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForSequenceClassification) (Nemotron model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForSequenceClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification) (Nystr√∂mformer model)
  - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForSequenceClassification](/docs/transformers/main/en/model_doc/opt#transformers.OPTForSequenceClassification) (OPT model)
  - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTForSequenceClassification](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification) (OpenAI GPT model)
  - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForSequenceClassification](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForSequenceClassification) (PLBart model)
  - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForSequenceClassification](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification) (Perceiver model)
  - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForSequenceClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification) (Persimmon model)
  - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForSequenceClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForSequenceClassification) (Phi3 model)
  - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForSequenceClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForSequenceClassification) (Phi model)
  - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeForSequenceClassification](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForSequenceClassification) (Phimoe model)
  - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification) (Qwen2 model)
  - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForSequenceClassification) (Qwen2MoE model)
  - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForSequenceClassification) (Qwen3 model)
  - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForSequenceClassification) (Qwen3MoE model)
  - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForSequenceClassification) (Qwen3Next model)
  - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForSequenceClassification](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForSequenceClassification) (Reformer model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForSequenceClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForSequenceClassification) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForSequenceClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForSequenceClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForSequenceClassification) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForSequenceClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForSequenceClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification) (RoBERTa-PreLayerNorm model)
  - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForSequenceClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForSequenceClassification) (SeedOss model)
  - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForSequenceClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForSequenceClassification) (SmolLM3 model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForSequenceClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification) (SqueezeBERT model)
  - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForSequenceClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification) (StableLm model)
  - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForSequenceClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForSequenceClassification) (Starcoder2 model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForSequenceClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForSequenceClassification) (T5 model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForSequenceClassification) (T5Gemma2 model)
  - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForSequenceClassification) (T5Gemma model)
  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForSequenceClassification](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForSequenceClassification) (TAPAS model)
  - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForSequenceClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForSequenceClassification) (UMT5 model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForSequenceClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForSequenceClassification) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForSequenceClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForSequenceClassification) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForSequenceClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForSequenceClassification) (X-MOD model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForSequenceClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForSequenceClassification) (YOSO model)
  - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2ForSequenceClassification](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForSequenceClassification) (Zamba2 model)
  - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaForSequenceClassification](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForSequenceClassification) (Zamba model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSequenceClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForSequenceClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForSequenceClassification` (ALBERT model) - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForSequenceClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForSequenceClassification) (Arcee model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForSequenceClassification](/docs/transformers/main/en/model_doc/bart#transformers.BartForSequenceClassification) (BART model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForSequenceClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForSequenceClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification) (BigBird model) - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForSequenceClassification](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification) (BigBird-Pegasus model) - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForSequenceClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForSequenceClassification) (BioGpt model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForSequenceClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForSequenceClassification) (BLOOM model) - [CTRLConfig](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig) configuration class: [CTRLForSequenceClassification](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLForSequenceClassification) (CTRL model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForSequenceClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForSequenceClassification) (CamemBERT model) - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForSequenceClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForSequenceClassification) (CANINE model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForSequenceClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForSequenceClassification) (ConvBERT model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification) (Data2VecText model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForSequenceClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForSequenceClassification) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification) (DeBERTa-v2 model) - [DeepseekV2Config](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2Config) configuration class: [DeepseekV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForSequenceClassification) (DeepSeek-V2 model) - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForSequenceClassification) (DeepSeek-V3 model) - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForSequenceClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForSequenceClassification) (DiffLlama model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForSequenceClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) (DistilBERT model) - [DogeConfig](/docs/transformers/main/en/model_doc/doge#transformers.DogeConfig) configuration class: [DogeForSequenceClassification](/docs/transformers/main/en/model_doc/doge#transformers.DogeForSequenceClassification) (Doge model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForSequenceClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForSequenceClassification) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForSequenceClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForSequenceClassification) (ERNIE model) - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForSequenceClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForSequenceClassification) (ESM model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForSequenceClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForSequenceClassification) (EXAONE-4.0 model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForSequenceClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForSequenceClassification) (FNet model) - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForSequenceClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForSequenceClassification) (Falcon model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForSequenceClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification) (FlauBERT model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForSequenceClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification) (Funnel Transformer model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForSequenceClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification) (OpenAI GPT-2 model) - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification) (GPTBigCode model) - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForSequenceClassification](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForSequenceClassification) (GPT-J model) - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification) (GPT Neo model) - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification) (GPT NeoX model) - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForSequenceClassification) (Gemma2 model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForSequenceClassification) (Gemma3ForConditionalGeneration model) - [Gemma3TextConfig](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextConfig) configuration class: [Gemma3TextForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextForSequenceClassification) (Gemma3ForCausalLM model) - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForSequenceClassification) (Gemma model) - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForSequenceClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForSequenceClassification) (GLM4 model) - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForSequenceClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForSequenceClassification) (GLM model) - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForSequenceClassification) (GptOss model) - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForSequenceClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForSequenceClassification) (Helium model) - [HunYuanDenseV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config) configuration class: [HunYuanDenseV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForSequenceClassification) (HunYuanDenseV1 model) - [HunYuanMoEV1Config](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config) configuration class: [HunYuanMoEV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForSequenceClassification) (HunYuanMoeV1 model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForSequenceClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForSequenceClassification) (I-BERT model) - [JambaConfig](/docs/transformers/main/en/model_doc/jamba#transformers.JambaConfig) configuration class: [JambaForSequenceClassification](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForSequenceClassification) (Jamba model) - [JetMoeConfig](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeConfig) configuration class: [JetMoeForSequenceClassification](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForSequenceClassification) (JetMoe model) - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForSequenceClassification](/docs/transformers/main/en/model_doc/led#transformers.LEDForSequenceClassification) (LED model) - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification) (LayoutLM model) - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification) (LayoutLMv2 model) - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification) (LayoutLMv3 model) - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForSequenceClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForSequenceClassification) (LiLT model) - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForSequenceClassification](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification) (LLaMA model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForSequenceClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForSequenceClassification) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForSequenceClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForSequenceClassification) (LUKE model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForSequenceClassification](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForSequenceClassification) (mBART model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForSequenceClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForSequenceClassification) (MPNet model) - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForSequenceClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForSequenceClassification) (MT5 model) - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForSequenceClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification) (MarkupLM model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForSequenceClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification) (Megatron-BERT model) - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForSequenceClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForSequenceClassification) (MiniMax model) - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForSequenceClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForSequenceClassification) (Ministral3 model) - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForSequenceClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForSequenceClassification) (Ministral model) - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForSequenceClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification) (Mistral model) - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForSequenceClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForSequenceClassification) (Mixtral model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForSequenceClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification) (MobileBERT model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification) (ModernBERT model) - [ModernBertDecoderConfig](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig) configuration class: [ModernBertDecoderForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForSequenceClassification) (ModernBertDecoder model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForSequenceClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForSequenceClassification) (MPT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForSequenceClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForSequenceClassification) (MRA model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForSequenceClassification](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForSequenceClassification) (MVP model) - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForSequenceClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForSequenceClassification) (Nemotron model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForSequenceClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification) (Nystr√∂mformer model) - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForSequenceClassification](/docs/transformers/main/en/model_doc/opt#transformers.OPTForSequenceClassification) (OPT model) - [OpenAIGPTConfig](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig) configuration class: [OpenAIGPTForSequenceClassification](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification) (OpenAI GPT model) - [PLBartConfig](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig) configuration class: [PLBartForSequenceClassification](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForSequenceClassification) (PLBart model) - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForSequenceClassification](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification) (Perceiver model) - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForSequenceClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification) (Persimmon model) - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForSequenceClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForSequenceClassification) (Phi3 model) - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForSequenceClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForSequenceClassification) (Phi model) - [PhimoeConfig](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeConfig) configuration class: [PhimoeForSequenceClassification](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForSequenceClassification) (Phimoe model) - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification) (Qwen2 model) - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForSequenceClassification) (Qwen2MoE model) - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForSequenceClassification) (Qwen3 model) - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForSequenceClassification) (Qwen3MoE model) - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForSequenceClassification) (Qwen3Next model) - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForSequenceClassification](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForSequenceClassification) (Reformer model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForSequenceClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForSequenceClassification) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForSequenceClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForSequenceClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForSequenceClassification) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForSequenceClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForSequenceClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification) (RoBERTa-PreLayerNorm model) - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForSequenceClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForSequenceClassification) (SeedOss model) - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForSequenceClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForSequenceClassification) (SmolLM3 model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForSequenceClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification) (SqueezeBERT model) - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForSequenceClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification) (StableLm model) - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForSequenceClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForSequenceClassification) (Starcoder2 model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForSequenceClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForSequenceClassification) (T5 model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForSequenceClassification) (T5Gemma2 model) - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForSequenceClassification) (T5Gemma model) - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForSequenceClassification](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForSequenceClassification) (TAPAS model) - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForSequenceClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForSequenceClassification) (UMT5 model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForSequenceClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForSequenceClassification) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForSequenceClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForSequenceClassification) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForSequenceClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForSequenceClassification) (X-MOD model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForSequenceClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForSequenceClassification) (YOSO model) - [Zamba2Config](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2Config) configuration class: [Zamba2ForSequenceClassification](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForSequenceClassification) (Zamba2 model) - [ZambaConfig](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaConfig) configuration class: [ZambaForSequenceClassification](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForSequenceClassification) (Zamba model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForSequenceClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- `AlbertForSequenceClassification` (ALBERT model)
- **arcee** -- [ArceeForSequenceClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForSequenceClassification) (Arcee model)
- **bart** -- [BartForSequenceClassification](/docs/transformers/main/en/model_doc/bart#transformers.BartForSequenceClassification) (BART model)
- **bert** -- [BertForSequenceClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification) (BERT model)
- **big_bird** -- [BigBirdForSequenceClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification) (BigBird model)
- **bigbird_pegasus** -- [BigBirdPegasusForSequenceClassification](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification) (BigBird-Pegasus model)
- **biogpt** -- [BioGptForSequenceClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForSequenceClassification) (BioGpt model)
- **bloom** -- [BloomForSequenceClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForSequenceClassification) (BLOOM model)
- **camembert** -- [CamembertForSequenceClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForSequenceClassification) (CamemBERT model)
- **canine** -- [CanineForSequenceClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForSequenceClassification) (CANINE model)
- **code_llama** -- [LlamaForSequenceClassification](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification) (CodeLlama model)
- **convbert** -- [ConvBertForSequenceClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForSequenceClassification) (ConvBERT model)
- **ctrl** -- [CTRLForSequenceClassification](/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLForSequenceClassification) (CTRL model)
- **data2vec-text** -- [Data2VecTextForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification) (Data2VecText model)
- **deberta** -- [DebertaForSequenceClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForSequenceClassification) (DeBERTa model)
- **deberta-v2** -- [DebertaV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification) (DeBERTa-v2 model)
- **deepseek_v2** -- [DeepseekV2ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v2#transformers.DeepseekV2ForSequenceClassification) (DeepSeek-V2 model)
- **deepseek_v3** -- [DeepseekV3ForSequenceClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForSequenceClassification) (DeepSeek-V3 model)
- **diffllama** -- [DiffLlamaForSequenceClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForSequenceClassification) (DiffLlama model)
- **distilbert** -- [DistilBertForSequenceClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) (DistilBERT model)
- **doge** -- [DogeForSequenceClassification](/docs/transformers/main/en/model_doc/doge#transformers.DogeForSequenceClassification) (Doge model)
- **electra** -- [ElectraForSequenceClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForSequenceClassification) (ELECTRA model)
- **ernie** -- [ErnieForSequenceClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForSequenceClassification) (ERNIE model)
- **esm** -- [EsmForSequenceClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForSequenceClassification) (ESM model)
- **exaone4** -- [Exaone4ForSequenceClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForSequenceClassification) (EXAONE-4.0 model)
- **falcon** -- [FalconForSequenceClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForSequenceClassification) (Falcon model)
- **flaubert** -- [FlaubertForSequenceClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification) (FlauBERT model)
- **fnet** -- [FNetForSequenceClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForSequenceClassification) (FNet model)
- **funnel** -- [FunnelForSequenceClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification) (Funnel Transformer model)
- **gemma** -- [GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForSequenceClassification) (Gemma model)
- **gemma2** -- [Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForSequenceClassification) (Gemma2 model)
- **gemma3** -- [Gemma3ForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForSequenceClassification) (Gemma3ForConditionalGeneration model)
- **gemma3_text** -- [Gemma3TextForSequenceClassification](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3TextForSequenceClassification) (Gemma3ForCausalLM model)
- **glm** -- [GlmForSequenceClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForSequenceClassification) (GLM model)
- **glm4** -- [Glm4ForSequenceClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForSequenceClassification) (GLM4 model)
- **gpt-sw3** -- [GPT2ForSequenceClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification) (GPT-Sw3 model)
- **gpt2** -- [GPT2ForSequenceClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification) (GPTBigCode model)
- **gpt_neo** -- [GPTNeoForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification) (GPT NeoX model)
- **gpt_oss** -- [GptOssForSequenceClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForSequenceClassification) (GptOss model)
- **gptj** -- [GPTJForSequenceClassification](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForSequenceClassification) (GPT-J model)
- **helium** -- [HeliumForSequenceClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForSequenceClassification) (Helium model)
- **hunyuan_v1_dense** -- [HunYuanDenseV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForSequenceClassification) (HunYuanDenseV1 model)
- **hunyuan_v1_moe** -- [HunYuanMoEV1ForSequenceClassification](/docs/transformers/main/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForSequenceClassification) (HunYuanMoeV1 model)
- **ibert** -- [IBertForSequenceClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForSequenceClassification) (I-BERT model)
- **jamba** -- [JambaForSequenceClassification](/docs/transformers/main/en/model_doc/jamba#transformers.JambaForSequenceClassification) (Jamba model)
- **jetmoe** -- [JetMoeForSequenceClassification](/docs/transformers/main/en/model_doc/jetmoe#transformers.JetMoeForSequenceClassification) (JetMoe model)
- **layoutlm** -- [LayoutLMForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3ForSequenceClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification) (LayoutLMv3 model)
- **led** -- [LEDForSequenceClassification](/docs/transformers/main/en/model_doc/led#transformers.LEDForSequenceClassification) (LED model)
- **lilt** -- [LiltForSequenceClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForSequenceClassification) (LiLT model)
- **llama** -- [LlamaForSequenceClassification](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification) (LLaMA model)
- **longformer** -- [LongformerForSequenceClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForSequenceClassification) (Longformer model)
- **luke** -- [LukeForSequenceClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForSequenceClassification) (LUKE model)
- **markuplm** -- [MarkupLMForSequenceClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification) (MarkupLM model)
- **mbart** -- [MBartForSequenceClassification](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForSequenceClassification) (mBART model)
- **megatron-bert** -- [MegatronBertForSequenceClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification) (Megatron-BERT model)
- **minimax** -- [MiniMaxForSequenceClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForSequenceClassification) (MiniMax model)
- **ministral** -- [MinistralForSequenceClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForSequenceClassification) (Ministral model)
- **ministral3** -- [Ministral3ForSequenceClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForSequenceClassification) (Ministral3 model)
- **mistral** -- [MistralForSequenceClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification) (Mistral model)
- **mixtral** -- [MixtralForSequenceClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForSequenceClassification) (Mixtral model)
- **mobilebert** -- [MobileBertForSequenceClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification) (MobileBERT model)
- **modernbert** -- [ModernBertForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification) (ModernBERT model)
- **modernbert-decoder** -- [ModernBertDecoderForSequenceClassification](/docs/transformers/main/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForSequenceClassification) (ModernBertDecoder model)
- **mpnet** -- [MPNetForSequenceClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForSequenceClassification) (MPNet model)
- **mpt** -- [MptForSequenceClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForSequenceClassification) (MPT model)
- **mra** -- [MraForSequenceClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForSequenceClassification) (MRA model)
- **mt5** -- [MT5ForSequenceClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForSequenceClassification) (MT5 model)
- **mvp** -- [MvpForSequenceClassification](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForSequenceClassification) (MVP model)
- **nemotron** -- [NemotronForSequenceClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForSequenceClassification) (Nemotron model)
- **nystromformer** -- [NystromformerForSequenceClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification) (Nystr√∂mformer model)
- **openai-gpt** -- [OpenAIGPTForSequenceClassification](/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification) (OpenAI GPT model)
- **opt** -- [OPTForSequenceClassification](/docs/transformers/main/en/model_doc/opt#transformers.OPTForSequenceClassification) (OPT model)
- **perceiver** -- [PerceiverForSequenceClassification](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification) (Perceiver model)
- **persimmon** -- [PersimmonForSequenceClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification) (Persimmon model)
- **phi** -- [PhiForSequenceClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForSequenceClassification) (Phi model)
- **phi3** -- [Phi3ForSequenceClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForSequenceClassification) (Phi3 model)
- **phimoe** -- [PhimoeForSequenceClassification](/docs/transformers/main/en/model_doc/phimoe#transformers.PhimoeForSequenceClassification) (Phimoe model)
- **plbart** -- [PLBartForSequenceClassification](/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForSequenceClassification) (PLBart model)
- **qwen2** -- [Qwen2ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification) (Qwen2 model)
- **qwen2_moe** -- [Qwen2MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForSequenceClassification) (Qwen2MoE model)
- **qwen3** -- [Qwen3ForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForSequenceClassification) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForSequenceClassification) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextForSequenceClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForSequenceClassification) (Qwen3Next model)
- **reformer** -- [ReformerForSequenceClassification](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForSequenceClassification) (Reformer model)
- **rembert** -- [RemBertForSequenceClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForSequenceClassification) (RemBERT model)
- **roberta** -- [RobertaForSequenceClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForSequenceClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForSequenceClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification) (RoCBert model)
- **roformer** -- [RoFormerForSequenceClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForSequenceClassification) (RoFormer model)
- **seed_oss** -- [SeedOssForSequenceClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForSequenceClassification) (SeedOss model)
- **smollm3** -- [SmolLM3ForSequenceClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForSequenceClassification) (SmolLM3 model)
- **squeezebert** -- [SqueezeBertForSequenceClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification) (SqueezeBERT model)
- **stablelm** -- [StableLmForSequenceClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification) (StableLm model)
- **starcoder2** -- [Starcoder2ForSequenceClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForSequenceClassification) (Starcoder2 model)
- **t5** -- [T5ForSequenceClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForSequenceClassification) (T5 model)
- **t5gemma** -- [T5GemmaForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForSequenceClassification) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2ForSequenceClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForSequenceClassification) (T5Gemma2 model)
- **tapas** -- [TapasForSequenceClassification](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForSequenceClassification) (TAPAS model)
- **umt5** -- [UMT5ForSequenceClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForSequenceClassification) (UMT5 model)
- **xlm** -- [XLMForSequenceClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForSequenceClassification) (XLM model)
- **xlm-roberta** -- [XLMRobertaForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForSequenceClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetForSequenceClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForSequenceClassification) (XLNet model)
- **xmod** -- [XmodForSequenceClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForSequenceClassification) (X-MOD model)
- **yoso** -- [YosoForSequenceClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForSequenceClassification) (YOSO model)
- **zamba** -- [ZambaForSequenceClassification](/docs/transformers/main/en/model_doc/zamba#transformers.ZambaForSequenceClassification) (Zamba model)
- **zamba2** -- [Zamba2ForSequenceClassification](/docs/transformers/main/en/model_doc/zamba2#transformers.Zamba2ForSequenceClassification) (Zamba2 model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSequenceClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForMultipleChoice[[transformers.AutoModelForMultipleChoice]]

#### transformers.AutoModelForMultipleChoice[[transformers.AutoModelForMultipleChoice]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2038)

This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForMultipleChoice.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: [AlbertForMultipleChoice](/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice) (ALBERT model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForMultipleChoice](/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForMultipleChoice](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice) (BigBird model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMultipleChoice](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMultipleChoice) (CamemBERT model)
  - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForMultipleChoice](/docs/transformers/main/en/model_doc/canine#transformers.CanineForMultipleChoice) (CANINE model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForMultipleChoice](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMultipleChoice) (ConvBERT model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMultipleChoice](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice) (Data2VecText model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMultipleChoice](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice) (DeBERTa-v2 model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMultipleChoice](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice) (DistilBERT model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForMultipleChoice](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMultipleChoice) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForMultipleChoice](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMultipleChoice) (ERNIE model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForMultipleChoice](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMultipleChoice) (FNet model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForMultipleChoice](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice) (FlauBERT model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForMultipleChoice](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice) (Funnel Transformer model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMultipleChoice](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMultipleChoice) (I-BERT model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMultipleChoice](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMultipleChoice) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMultipleChoice](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMultipleChoice) (LUKE model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMultipleChoice](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMultipleChoice) (MPNet model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForMultipleChoice](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice) (Megatron-BERT model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForMultipleChoice](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice) (MobileBERT model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForMultipleChoice](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice) (ModernBERT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMultipleChoice](/docs/transformers/main/en/model_doc/mra#transformers.MraForMultipleChoice) (MRA model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForMultipleChoice](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice) (Nystr√∂mformer model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForMultipleChoice](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMultipleChoice) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForMultipleChoice](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForMultipleChoice](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMultipleChoice) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMultipleChoice](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMultipleChoice](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice) (RoBERTa-PreLayerNorm model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMultipleChoice](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice) (SqueezeBERT model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForMultipleChoice](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForMultipleChoice) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForMultipleChoice](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForMultipleChoice) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMultipleChoice](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMultipleChoice) (X-MOD model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForMultipleChoice](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMultipleChoice) (YOSO model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMultipleChoice

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForMultipleChoice.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: [AlbertForMultipleChoice](/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice) (ALBERT model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForMultipleChoice](/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForMultipleChoice](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice) (BigBird model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForMultipleChoice](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMultipleChoice) (CamemBERT model) - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForMultipleChoice](/docs/transformers/main/en/model_doc/canine#transformers.CanineForMultipleChoice) (CANINE model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForMultipleChoice](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMultipleChoice) (ConvBERT model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForMultipleChoice](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice) (Data2VecText model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForMultipleChoice](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice) (DeBERTa-v2 model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForMultipleChoice](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice) (DistilBERT model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForMultipleChoice](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMultipleChoice) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForMultipleChoice](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMultipleChoice) (ERNIE model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForMultipleChoice](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMultipleChoice) (FNet model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForMultipleChoice](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice) (FlauBERT model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForMultipleChoice](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice) (Funnel Transformer model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForMultipleChoice](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMultipleChoice) (I-BERT model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForMultipleChoice](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMultipleChoice) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForMultipleChoice](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMultipleChoice) (LUKE model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForMultipleChoice](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMultipleChoice) (MPNet model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForMultipleChoice](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice) (Megatron-BERT model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForMultipleChoice](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice) (MobileBERT model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForMultipleChoice](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice) (ModernBERT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForMultipleChoice](/docs/transformers/main/en/model_doc/mra#transformers.MraForMultipleChoice) (MRA model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForMultipleChoice](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice) (Nystr√∂mformer model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForMultipleChoice](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMultipleChoice) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForMultipleChoice](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForMultipleChoice](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMultipleChoice) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForMultipleChoice](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForMultipleChoice](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice) (RoBERTa-PreLayerNorm model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForMultipleChoice](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice) (SqueezeBERT model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForMultipleChoice](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForMultipleChoice) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForMultipleChoice](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForMultipleChoice) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForMultipleChoice](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMultipleChoice) (X-MOD model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForMultipleChoice](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMultipleChoice) (YOSO model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForMultipleChoice.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- [AlbertForMultipleChoice](/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice) (ALBERT model)
- **bert** -- [BertForMultipleChoice](/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice) (BERT model)
- **big_bird** -- [BigBirdForMultipleChoice](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice) (BigBird model)
- **camembert** -- [CamembertForMultipleChoice](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMultipleChoice) (CamemBERT model)
- **canine** -- [CanineForMultipleChoice](/docs/transformers/main/en/model_doc/canine#transformers.CanineForMultipleChoice) (CANINE model)
- **convbert** -- [ConvBertForMultipleChoice](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMultipleChoice) (ConvBERT model)
- **data2vec-text** -- [Data2VecTextForMultipleChoice](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice) (Data2VecText model)
- **deberta-v2** -- [DebertaV2ForMultipleChoice](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice) (DeBERTa-v2 model)
- **distilbert** -- [DistilBertForMultipleChoice](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice) (DistilBERT model)
- **electra** -- [ElectraForMultipleChoice](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMultipleChoice) (ELECTRA model)
- **ernie** -- [ErnieForMultipleChoice](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMultipleChoice) (ERNIE model)
- **flaubert** -- [FlaubertForMultipleChoice](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice) (FlauBERT model)
- **fnet** -- [FNetForMultipleChoice](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMultipleChoice) (FNet model)
- **funnel** -- [FunnelForMultipleChoice](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice) (Funnel Transformer model)
- **ibert** -- [IBertForMultipleChoice](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMultipleChoice) (I-BERT model)
- **longformer** -- [LongformerForMultipleChoice](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMultipleChoice) (Longformer model)
- **luke** -- [LukeForMultipleChoice](/docs/transformers/main/en/model_doc/luke#transformers.LukeForMultipleChoice) (LUKE model)
- **megatron-bert** -- [MegatronBertForMultipleChoice](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice) (Megatron-BERT model)
- **mobilebert** -- [MobileBertForMultipleChoice](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice) (MobileBERT model)
- **modernbert** -- [ModernBertForMultipleChoice](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice) (ModernBERT model)
- **mpnet** -- [MPNetForMultipleChoice](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMultipleChoice) (MPNet model)
- **mra** -- [MraForMultipleChoice](/docs/transformers/main/en/model_doc/mra#transformers.MraForMultipleChoice) (MRA model)
- **nystromformer** -- [NystromformerForMultipleChoice](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice) (Nystr√∂mformer model)
- **rembert** -- [RemBertForMultipleChoice](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMultipleChoice) (RemBERT model)
- **roberta** -- [RobertaForMultipleChoice](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForMultipleChoice](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForMultipleChoice](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice) (RoCBert model)
- **roformer** -- [RoFormerForMultipleChoice](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMultipleChoice) (RoFormer model)
- **squeezebert** -- [SqueezeBertForMultipleChoice](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice) (SqueezeBERT model)
- **xlm** -- [XLMForMultipleChoice](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForMultipleChoice) (XLM model)
- **xlm-roberta** -- [XLMRobertaForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForMultipleChoice](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetForMultipleChoice](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForMultipleChoice) (XLNet model)
- **xmod** -- [XmodForMultipleChoice](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMultipleChoice) (X-MOD model)
- **yoso** -- [YosoForMultipleChoice](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMultipleChoice) (YOSO model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMultipleChoice

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForNextSentencePrediction[[transformers.AutoModelForNextSentencePrediction]]

#### transformers.AutoModelForNextSentencePrediction[[transformers.AutoModelForNextSentencePrediction]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2045)

This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForNextSentencePrediction.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForNextSentencePrediction](/docs/transformers/main/en/model_doc/bert#transformers.BertForNextSentencePrediction) (BERT model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForNextSentencePrediction](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction) (ERNIE model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForNextSentencePrediction](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForNextSentencePrediction) (FNet model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction) (Megatron-BERT model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction) (MobileBERT model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForNextSentencePrediction

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForNextSentencePrediction.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForNextSentencePrediction](/docs/transformers/main/en/model_doc/bert#transformers.BertForNextSentencePrediction) (BERT model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForNextSentencePrediction](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction) (ERNIE model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForNextSentencePrediction](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForNextSentencePrediction) (FNet model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction) (Megatron-BERT model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction) (MobileBERT model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForNextSentencePrediction.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **bert** -- [BertForNextSentencePrediction](/docs/transformers/main/en/model_doc/bert#transformers.BertForNextSentencePrediction) (BERT model)
- **ernie** -- [ErnieForNextSentencePrediction](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction) (ERNIE model)
- **fnet** -- [FNetForNextSentencePrediction](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForNextSentencePrediction) (FNet model)
- **megatron-bert** -- [MegatronBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction) (Megatron-BERT model)
- **mobilebert** -- [MobileBertForNextSentencePrediction](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction) (MobileBERT model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForNextSentencePrediction

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForNextSentencePrediction.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForNextSentencePrediction.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForTokenClassification[[transformers.AutoModelForTokenClassification]]

#### transformers.AutoModelForTokenClassification[[transformers.AutoModelForTokenClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2031)

This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForTokenClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForTokenClassification` (ALBERT model)
  - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusForTokenClassification](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForTokenClassification) (Apertus model)
  - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForTokenClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForTokenClassification) (Arcee model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForTokenClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForTokenClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForTokenClassification) (BigBird model)
  - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForTokenClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForTokenClassification) (BioGpt model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForTokenClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForTokenClassification) (BLOOM model)
  - [BrosConfig](/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig) configuration class: [BrosForTokenClassification](/docs/transformers/main/en/model_doc/bros#transformers.BrosForTokenClassification) (BROS model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForTokenClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForTokenClassification) (CamemBERT model)
  - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForTokenClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForTokenClassification) (CANINE model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForTokenClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForTokenClassification) (ConvBERT model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForTokenClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification) (Data2VecText model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForTokenClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForTokenClassification) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForTokenClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification) (DeBERTa-v2 model)
  - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForTokenClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForTokenClassification) (DeepSeek-V3 model)
  - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForTokenClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForTokenClassification) (DiffLlama model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForTokenClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification) (DistilBERT model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForTokenClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForTokenClassification) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForTokenClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForTokenClassification) (ERNIE model)
  - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForTokenClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForTokenClassification) (ESM model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForTokenClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForTokenClassification) (EXAONE-4.0 model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForTokenClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForTokenClassification) (FNet model)
  - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForTokenClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForTokenClassification) (Falcon model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForTokenClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForTokenClassification) (FlauBERT model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForTokenClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification) (Funnel Transformer model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForTokenClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification) (OpenAI GPT-2 model)
  - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForTokenClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification) (GPTBigCode model)
  - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification) (GPT Neo model)
  - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification) (GPT NeoX model)
  - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForTokenClassification) (Gemma2 model)
  - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForTokenClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForTokenClassification) (Gemma model)
  - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForTokenClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForTokenClassification) (GLM4 model)
  - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForTokenClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForTokenClassification) (GLM model)
  - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForTokenClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForTokenClassification) (GptOss model)
  - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForTokenClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForTokenClassification) (Helium model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForTokenClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForTokenClassification) (I-BERT model)
  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForTokenClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification) (LayoutLM model)
  - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification) (LayoutLMv2 model)
  - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification) (LayoutLMv3 model)
  - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForTokenClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForTokenClassification) (LiLT model)
  - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForTokenClassification](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForTokenClassification) (LLaMA model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForTokenClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForTokenClassification) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForTokenClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForTokenClassification) (LUKE model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForTokenClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForTokenClassification) (MPNet model)
  - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForTokenClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForTokenClassification) (MT5 model)
  - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForTokenClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification) (MarkupLM model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForTokenClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification) (Megatron-BERT model)
  - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForTokenClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForTokenClassification) (MiniMax model)
  - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForTokenClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForTokenClassification) (Ministral3 model)
  - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForTokenClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForTokenClassification) (Ministral model)
  - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForTokenClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForTokenClassification) (Mistral model)
  - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForTokenClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForTokenClassification) (Mixtral model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForTokenClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification) (MobileBERT model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForTokenClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForTokenClassification) (ModernBERT model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForTokenClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForTokenClassification) (MPT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForTokenClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForTokenClassification) (MRA model)
  - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForTokenClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForTokenClassification) (Nemotron model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForTokenClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification) (Nystr√∂mformer model)
  - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForTokenClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForTokenClassification) (Persimmon model)
  - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForTokenClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForTokenClassification) (Phi3 model)
  - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForTokenClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForTokenClassification) (Phi model)
  - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForTokenClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForTokenClassification) (Qwen2 model)
  - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForTokenClassification) (Qwen2MoE model)
  - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForTokenClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForTokenClassification) (Qwen3 model)
  - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForTokenClassification) (Qwen3MoE model)
  - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForTokenClassification) (Qwen3Next model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForTokenClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForTokenClassification) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForTokenClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForTokenClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForTokenClassification) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForTokenClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForTokenClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification) (RoBERTa-PreLayerNorm model)
  - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForTokenClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForTokenClassification) (SeedOss model)
  - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForTokenClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForTokenClassification) (SmolLM3 model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForTokenClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification) (SqueezeBERT model)
  - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForTokenClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForTokenClassification) (StableLm model)
  - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForTokenClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForTokenClassification) (Starcoder2 model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForTokenClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForTokenClassification) (T5 model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForTokenClassification) (T5Gemma2 model)
  - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForTokenClassification) (T5Gemma model)
  - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForTokenClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForTokenClassification) (UMT5 model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForTokenClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForTokenClassification) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForTokenClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForTokenClassification) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForTokenClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForTokenClassification) (X-MOD model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForTokenClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForTokenClassification) (YOSO model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a token classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTokenClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForTokenClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForTokenClassification` (ALBERT model) - [ApertusConfig](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusConfig) configuration class: [ApertusForTokenClassification](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForTokenClassification) (Apertus model) - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForTokenClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForTokenClassification) (Arcee model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForTokenClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForTokenClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForTokenClassification) (BigBird model) - [BioGptConfig](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig) configuration class: [BioGptForTokenClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForTokenClassification) (BioGpt model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForTokenClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForTokenClassification) (BLOOM model) - [BrosConfig](/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig) configuration class: [BrosForTokenClassification](/docs/transformers/main/en/model_doc/bros#transformers.BrosForTokenClassification) (BROS model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForTokenClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForTokenClassification) (CamemBERT model) - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForTokenClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForTokenClassification) (CANINE model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForTokenClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForTokenClassification) (ConvBERT model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForTokenClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification) (Data2VecText model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForTokenClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForTokenClassification) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForTokenClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification) (DeBERTa-v2 model) - [DeepseekV3Config](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3Config) configuration class: [DeepseekV3ForTokenClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForTokenClassification) (DeepSeek-V3 model) - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForTokenClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForTokenClassification) (DiffLlama model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForTokenClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification) (DistilBERT model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForTokenClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForTokenClassification) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForTokenClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForTokenClassification) (ERNIE model) - [EsmConfig](/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig) configuration class: [EsmForTokenClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForTokenClassification) (ESM model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForTokenClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForTokenClassification) (EXAONE-4.0 model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForTokenClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForTokenClassification) (FNet model) - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForTokenClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForTokenClassification) (Falcon model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForTokenClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForTokenClassification) (FlauBERT model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForTokenClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification) (Funnel Transformer model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForTokenClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification) (OpenAI GPT-2 model) - [GPTBigCodeConfig](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig) configuration class: [GPTBigCodeForTokenClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification) (GPTBigCode model) - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification) (GPT Neo model) - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification) (GPT NeoX model) - [Gemma2Config](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2Config) configuration class: [Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForTokenClassification) (Gemma2 model) - [GemmaConfig](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaConfig) configuration class: [GemmaForTokenClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForTokenClassification) (Gemma model) - [Glm4Config](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4Config) configuration class: [Glm4ForTokenClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForTokenClassification) (GLM4 model) - [GlmConfig](/docs/transformers/main/en/model_doc/glm#transformers.GlmConfig) configuration class: [GlmForTokenClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForTokenClassification) (GLM model) - [GptOssConfig](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssConfig) configuration class: [GptOssForTokenClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForTokenClassification) (GptOss model) - [HeliumConfig](/docs/transformers/main/en/model_doc/helium#transformers.HeliumConfig) configuration class: [HeliumForTokenClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForTokenClassification) (Helium model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForTokenClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForTokenClassification) (I-BERT model) - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForTokenClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification) (LayoutLM model) - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification) (LayoutLMv2 model) - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification) (LayoutLMv3 model) - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForTokenClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForTokenClassification) (LiLT model) - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForTokenClassification](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForTokenClassification) (LLaMA model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForTokenClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForTokenClassification) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForTokenClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForTokenClassification) (LUKE model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForTokenClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForTokenClassification) (MPNet model) - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForTokenClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForTokenClassification) (MT5 model) - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForTokenClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification) (MarkupLM model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForTokenClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification) (Megatron-BERT model) - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForTokenClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForTokenClassification) (MiniMax model) - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForTokenClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForTokenClassification) (Ministral3 model) - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForTokenClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForTokenClassification) (Ministral model) - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForTokenClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForTokenClassification) (Mistral model) - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForTokenClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForTokenClassification) (Mixtral model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForTokenClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification) (MobileBERT model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForTokenClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForTokenClassification) (ModernBERT model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForTokenClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForTokenClassification) (MPT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForTokenClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForTokenClassification) (MRA model) - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForTokenClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForTokenClassification) (Nemotron model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForTokenClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification) (Nystr√∂mformer model) - [PersimmonConfig](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig) configuration class: [PersimmonForTokenClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForTokenClassification) (Persimmon model) - [Phi3Config](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3Config) configuration class: [Phi3ForTokenClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForTokenClassification) (Phi3 model) - [PhiConfig](/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig) configuration class: [PhiForTokenClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForTokenClassification) (Phi model) - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForTokenClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForTokenClassification) (Qwen2 model) - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForTokenClassification) (Qwen2MoE model) - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForTokenClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForTokenClassification) (Qwen3 model) - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForTokenClassification) (Qwen3MoE model) - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForTokenClassification) (Qwen3Next model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForTokenClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForTokenClassification) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForTokenClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForTokenClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForTokenClassification) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForTokenClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForTokenClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification) (RoBERTa-PreLayerNorm model) - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForTokenClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForTokenClassification) (SeedOss model) - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForTokenClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForTokenClassification) (SmolLM3 model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForTokenClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification) (SqueezeBERT model) - [StableLmConfig](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig) configuration class: [StableLmForTokenClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForTokenClassification) (StableLm model) - [Starcoder2Config](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2Config) configuration class: [Starcoder2ForTokenClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForTokenClassification) (Starcoder2 model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForTokenClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForTokenClassification) (T5 model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForTokenClassification) (T5Gemma2 model) - [T5GemmaConfig](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaConfig) configuration class: [T5GemmaForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForTokenClassification) (T5Gemma model) - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForTokenClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForTokenClassification) (UMT5 model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForTokenClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForTokenClassification) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForTokenClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForTokenClassification) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForTokenClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForTokenClassification) (X-MOD model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForTokenClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForTokenClassification) (YOSO model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForTokenClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- `AlbertForTokenClassification` (ALBERT model)
- **apertus** -- [ApertusForTokenClassification](/docs/transformers/main/en/model_doc/apertus#transformers.ApertusForTokenClassification) (Apertus model)
- **arcee** -- [ArceeForTokenClassification](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForTokenClassification) (Arcee model)
- **bert** -- [BertForTokenClassification](/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification) (BERT model)
- **big_bird** -- [BigBirdForTokenClassification](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForTokenClassification) (BigBird model)
- **biogpt** -- [BioGptForTokenClassification](/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForTokenClassification) (BioGpt model)
- **bloom** -- [BloomForTokenClassification](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForTokenClassification) (BLOOM model)
- **bros** -- [BrosForTokenClassification](/docs/transformers/main/en/model_doc/bros#transformers.BrosForTokenClassification) (BROS model)
- **camembert** -- [CamembertForTokenClassification](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForTokenClassification) (CamemBERT model)
- **canine** -- [CanineForTokenClassification](/docs/transformers/main/en/model_doc/canine#transformers.CanineForTokenClassification) (CANINE model)
- **convbert** -- [ConvBertForTokenClassification](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForTokenClassification) (ConvBERT model)
- **data2vec-text** -- [Data2VecTextForTokenClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification) (Data2VecText model)
- **deberta** -- [DebertaForTokenClassification](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForTokenClassification) (DeBERTa model)
- **deberta-v2** -- [DebertaV2ForTokenClassification](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification) (DeBERTa-v2 model)
- **deepseek_v3** -- [DeepseekV3ForTokenClassification](/docs/transformers/main/en/model_doc/deepseek_v3#transformers.DeepseekV3ForTokenClassification) (DeepSeek-V3 model)
- **diffllama** -- [DiffLlamaForTokenClassification](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForTokenClassification) (DiffLlama model)
- **distilbert** -- [DistilBertForTokenClassification](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification) (DistilBERT model)
- **electra** -- [ElectraForTokenClassification](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForTokenClassification) (ELECTRA model)
- **ernie** -- [ErnieForTokenClassification](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForTokenClassification) (ERNIE model)
- **esm** -- [EsmForTokenClassification](/docs/transformers/main/en/model_doc/esm#transformers.EsmForTokenClassification) (ESM model)
- **exaone4** -- [Exaone4ForTokenClassification](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForTokenClassification) (EXAONE-4.0 model)
- **falcon** -- [FalconForTokenClassification](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForTokenClassification) (Falcon model)
- **flaubert** -- [FlaubertForTokenClassification](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForTokenClassification) (FlauBERT model)
- **fnet** -- [FNetForTokenClassification](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForTokenClassification) (FNet model)
- **funnel** -- [FunnelForTokenClassification](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification) (Funnel Transformer model)
- **gemma** -- [GemmaForTokenClassification](/docs/transformers/main/en/model_doc/gemma#transformers.GemmaForTokenClassification) (Gemma model)
- **gemma2** -- [Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/gemma2#transformers.Gemma2ForTokenClassification) (Gemma2 model)
- **glm** -- [GlmForTokenClassification](/docs/transformers/main/en/model_doc/glm#transformers.GlmForTokenClassification) (GLM model)
- **glm4** -- [Glm4ForTokenClassification](/docs/transformers/main/en/model_doc/glm4#transformers.Glm4ForTokenClassification) (GLM4 model)
- **gpt-sw3** -- [GPT2ForTokenClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification) (GPT-Sw3 model)
- **gpt2** -- [GPT2ForTokenClassification](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification) (OpenAI GPT-2 model)
- **gpt_bigcode** -- [GPTBigCodeForTokenClassification](/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification) (GPTBigCode model)
- **gpt_neo** -- [GPTNeoForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXForTokenClassification](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification) (GPT NeoX model)
- **gpt_oss** -- [GptOssForTokenClassification](/docs/transformers/main/en/model_doc/gpt_oss#transformers.GptOssForTokenClassification) (GptOss model)
- **helium** -- [HeliumForTokenClassification](/docs/transformers/main/en/model_doc/helium#transformers.HeliumForTokenClassification) (Helium model)
- **ibert** -- [IBertForTokenClassification](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForTokenClassification) (I-BERT model)
- **layoutlm** -- [LayoutLMForTokenClassification](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3ForTokenClassification](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification) (LayoutLMv3 model)
- **lilt** -- [LiltForTokenClassification](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForTokenClassification) (LiLT model)
- **llama** -- [LlamaForTokenClassification](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForTokenClassification) (LLaMA model)
- **longformer** -- [LongformerForTokenClassification](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForTokenClassification) (Longformer model)
- **luke** -- [LukeForTokenClassification](/docs/transformers/main/en/model_doc/luke#transformers.LukeForTokenClassification) (LUKE model)
- **markuplm** -- [MarkupLMForTokenClassification](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification) (MarkupLM model)
- **megatron-bert** -- [MegatronBertForTokenClassification](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification) (Megatron-BERT model)
- **minimax** -- [MiniMaxForTokenClassification](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForTokenClassification) (MiniMax model)
- **ministral** -- [MinistralForTokenClassification](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForTokenClassification) (Ministral model)
- **ministral3** -- [Ministral3ForTokenClassification](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForTokenClassification) (Ministral3 model)
- **mistral** -- [MistralForTokenClassification](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForTokenClassification) (Mistral model)
- **mixtral** -- [MixtralForTokenClassification](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForTokenClassification) (Mixtral model)
- **mobilebert** -- [MobileBertForTokenClassification](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification) (MobileBERT model)
- **modernbert** -- [ModernBertForTokenClassification](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForTokenClassification) (ModernBERT model)
- **mpnet** -- [MPNetForTokenClassification](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForTokenClassification) (MPNet model)
- **mpt** -- [MptForTokenClassification](/docs/transformers/main/en/model_doc/mpt#transformers.MptForTokenClassification) (MPT model)
- **mra** -- [MraForTokenClassification](/docs/transformers/main/en/model_doc/mra#transformers.MraForTokenClassification) (MRA model)
- **mt5** -- [MT5ForTokenClassification](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForTokenClassification) (MT5 model)
- **nemotron** -- [NemotronForTokenClassification](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForTokenClassification) (Nemotron model)
- **nystromformer** -- [NystromformerForTokenClassification](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification) (Nystr√∂mformer model)
- **persimmon** -- [PersimmonForTokenClassification](/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForTokenClassification) (Persimmon model)
- **phi** -- [PhiForTokenClassification](/docs/transformers/main/en/model_doc/phi#transformers.PhiForTokenClassification) (Phi model)
- **phi3** -- [Phi3ForTokenClassification](/docs/transformers/main/en/model_doc/phi3#transformers.Phi3ForTokenClassification) (Phi3 model)
- **qwen2** -- [Qwen2ForTokenClassification](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForTokenClassification) (Qwen2 model)
- **qwen2_moe** -- [Qwen2MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForTokenClassification) (Qwen2MoE model)
- **qwen3** -- [Qwen3ForTokenClassification](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForTokenClassification) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForTokenClassification) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextForTokenClassification](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForTokenClassification) (Qwen3Next model)
- **rembert** -- [RemBertForTokenClassification](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForTokenClassification) (RemBERT model)
- **roberta** -- [RobertaForTokenClassification](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForTokenClassification](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForTokenClassification](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification) (RoCBert model)
- **roformer** -- [RoFormerForTokenClassification](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForTokenClassification) (RoFormer model)
- **seed_oss** -- [SeedOssForTokenClassification](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForTokenClassification) (SeedOss model)
- **smollm3** -- [SmolLM3ForTokenClassification](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForTokenClassification) (SmolLM3 model)
- **squeezebert** -- [SqueezeBertForTokenClassification](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification) (SqueezeBERT model)
- **stablelm** -- [StableLmForTokenClassification](/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForTokenClassification) (StableLm model)
- **starcoder2** -- [Starcoder2ForTokenClassification](/docs/transformers/main/en/model_doc/starcoder2#transformers.Starcoder2ForTokenClassification) (Starcoder2 model)
- **t5** -- [T5ForTokenClassification](/docs/transformers/main/en/model_doc/t5#transformers.T5ForTokenClassification) (T5 model)
- **t5gemma** -- [T5GemmaForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma#transformers.T5GemmaForTokenClassification) (T5Gemma model)
- **t5gemma2** -- [T5Gemma2ForTokenClassification](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForTokenClassification) (T5Gemma2 model)
- **umt5** -- [UMT5ForTokenClassification](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForTokenClassification) (UMT5 model)
- **xlm** -- [XLMForTokenClassification](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForTokenClassification) (XLM model)
- **xlm-roberta** -- [XLMRobertaForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForTokenClassification](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetForTokenClassification](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForTokenClassification) (XLNet model)
- **xmod** -- [XmodForTokenClassification](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForTokenClassification) (X-MOD model)
- **yoso** -- [YosoForTokenClassification](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForTokenClassification) (YOSO model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTokenClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForTokenClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForTokenClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForQuestionAnswering[[transformers.AutoModelForQuestionAnswering]]

#### transformers.AutoModelForQuestionAnswering[[transformers.AutoModelForQuestionAnswering]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1991)

This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForQuestionAnswering.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForQuestionAnswering` (ALBERT model)
  - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForQuestionAnswering](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForQuestionAnswering) (Arcee model)
  - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForQuestionAnswering](/docs/transformers/main/en/model_doc/bart#transformers.BartForQuestionAnswering) (BART model)
  - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForQuestionAnswering](/docs/transformers/main/en/model_doc/bert#transformers.BertForQuestionAnswering) (BERT model)
  - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForQuestionAnswering](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering) (BigBird model)
  - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForQuestionAnswering](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering) (BigBird-Pegasus model)
  - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForQuestionAnswering](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForQuestionAnswering) (BLOOM model)
  - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForQuestionAnswering](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForQuestionAnswering) (CamemBERT model)
  - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForQuestionAnswering](/docs/transformers/main/en/model_doc/canine#transformers.CanineForQuestionAnswering) (CANINE model)
  - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForQuestionAnswering](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering) (ConvBERT model)
  - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForQuestionAnswering](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering) (Data2VecText model)
  - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForQuestionAnswering) (DeBERTa model)
  - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering) (DeBERTa-v2 model)
  - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForQuestionAnswering) (DiffLlama model)
  - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForQuestionAnswering](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering) (DistilBERT model)
  - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForQuestionAnswering](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForQuestionAnswering) (ELECTRA model)
  - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForQuestionAnswering](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForQuestionAnswering) (ERNIE model)
  - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForQuestionAnswering](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForQuestionAnswering) (EXAONE-4.0 model)
  - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForQuestionAnswering](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForQuestionAnswering) (FNet model)
  - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForQuestionAnswering](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForQuestionAnswering) (Falcon model)
  - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple) (FlauBERT model)
  - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForQuestionAnswering](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering) (Funnel Transformer model)
  - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering) (OpenAI GPT-2 model)
  - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForQuestionAnswering](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForQuestionAnswering) (GPT-J model)
  - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering) (GPT Neo model)
  - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering) (GPT NeoX model)
  - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForQuestionAnswering](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForQuestionAnswering) (I-BERT model)
  - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForQuestionAnswering](/docs/transformers/main/en/model_doc/led#transformers.LEDForQuestionAnswering) (LED model)
  - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model)
  - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model)
  - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForQuestionAnswering](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForQuestionAnswering) (LiLT model)
  - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering) (LLaMA model)
  - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForQuestionAnswering](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForQuestionAnswering) (Longformer model)
  - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForQuestionAnswering](/docs/transformers/main/en/model_doc/luke#transformers.LukeForQuestionAnswering) (LUKE model)
  - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertForQuestionAnswering](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering) (LXMERT model)
  - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForQuestionAnswering](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForQuestionAnswering) (mBART model)
  - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForQuestionAnswering](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering) (MPNet model)
  - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForQuestionAnswering) (MT5 model)
  - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForQuestionAnswering](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering) (MarkupLM model)
  - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForQuestionAnswering](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering) (Megatron-BERT model)
  - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForQuestionAnswering](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForQuestionAnswering) (MiniMax model)
  - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForQuestionAnswering) (Ministral3 model)
  - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForQuestionAnswering) (Ministral model)
  - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForQuestionAnswering](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForQuestionAnswering) (Mistral model)
  - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForQuestionAnswering](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForQuestionAnswering) (Mixtral model)
  - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForQuestionAnswering](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering) (MobileBERT model)
  - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForQuestionAnswering](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering) (ModernBERT model)
  - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForQuestionAnswering](/docs/transformers/main/en/model_doc/mpt#transformers.MptForQuestionAnswering) (MPT model)
  - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForQuestionAnswering](/docs/transformers/main/en/model_doc/mra#transformers.MraForQuestionAnswering) (MRA model)
  - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForQuestionAnswering](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForQuestionAnswering) (MVP model)
  - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForQuestionAnswering](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForQuestionAnswering) (Nemotron model)
  - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForQuestionAnswering](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering) (Nystr√∂mformer model)
  - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForQuestionAnswering](/docs/transformers/main/en/model_doc/opt#transformers.OPTForQuestionAnswering) (OPT model)
  - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForQuestionAnswering) (Qwen2 model)
  - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForQuestionAnswering) (Qwen2MoE model)
  - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForQuestionAnswering) (Qwen3 model)
  - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForQuestionAnswering) (Qwen3MoE model)
  - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForQuestionAnswering) (Qwen3Next model)
  - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForQuestionAnswering](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForQuestionAnswering) (Reformer model)
  - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForQuestionAnswering](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForQuestionAnswering) (RemBERT model)
  - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForQuestionAnswering](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering) (RoCBert model)
  - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForQuestionAnswering](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering) (RoFormer model)
  - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering) (RoBERTa model)
  - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering) (RoBERTa-PreLayerNorm model)
  - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForQuestionAnswering](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForQuestionAnswering) (SeedOss model)
  - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForQuestionAnswering](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForQuestionAnswering) (SmolLM3 model)
  - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterForQuestionAnswering](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForQuestionAnswering) (Splinter model)
  - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForQuestionAnswering](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering) (SqueezeBERT model)
  - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForQuestionAnswering](/docs/transformers/main/en/model_doc/t5#transformers.T5ForQuestionAnswering) (T5 model)
  - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering) (UMT5 model)
  - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple) (XLM model)
  - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering) (XLM-RoBERTa model)
  - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering) (XLM-RoBERTa-XL model)
  - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple) (XLNet model)
  - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForQuestionAnswering](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForQuestionAnswering) (X-MOD model)
  - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForQuestionAnswering](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForQuestionAnswering) (YOSO model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a question answering head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForQuestionAnswering

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForQuestionAnswering.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlbertConfig](/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig) configuration class: `AlbertForQuestionAnswering` (ALBERT model) - [ArceeConfig](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeConfig) configuration class: [ArceeForQuestionAnswering](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForQuestionAnswering) (Arcee model) - [BartConfig](/docs/transformers/main/en/model_doc/bart#transformers.BartConfig) configuration class: [BartForQuestionAnswering](/docs/transformers/main/en/model_doc/bart#transformers.BartForQuestionAnswering) (BART model) - [BertConfig](/docs/transformers/main/en/model_doc/bert#transformers.BertConfig) configuration class: [BertForQuestionAnswering](/docs/transformers/main/en/model_doc/bert#transformers.BertForQuestionAnswering) (BERT model) - [BigBirdConfig](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig) configuration class: [BigBirdForQuestionAnswering](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering) (BigBird model) - [BigBirdPegasusConfig](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig) configuration class: [BigBirdPegasusForQuestionAnswering](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering) (BigBird-Pegasus model) - [BloomConfig](/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig) configuration class: [BloomForQuestionAnswering](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForQuestionAnswering) (BLOOM model) - [CamembertConfig](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig) configuration class: [CamembertForQuestionAnswering](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForQuestionAnswering) (CamemBERT model) - [CanineConfig](/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig) configuration class: [CanineForQuestionAnswering](/docs/transformers/main/en/model_doc/canine#transformers.CanineForQuestionAnswering) (CANINE model) - [ConvBertConfig](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig) configuration class: [ConvBertForQuestionAnswering](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering) (ConvBERT model) - [Data2VecTextConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig) configuration class: [Data2VecTextForQuestionAnswering](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering) (Data2VecText model) - [DebertaConfig](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig) configuration class: [DebertaForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForQuestionAnswering) (DeBERTa model) - [DebertaV2Config](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config) configuration class: [DebertaV2ForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering) (DeBERTa-v2 model) - [DiffLlamaConfig](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaConfig) configuration class: [DiffLlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForQuestionAnswering) (DiffLlama model) - [DistilBertConfig](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig) configuration class: [DistilBertForQuestionAnswering](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering) (DistilBERT model) - [ElectraConfig](/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig) configuration class: [ElectraForQuestionAnswering](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForQuestionAnswering) (ELECTRA model) - [ErnieConfig](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig) configuration class: [ErnieForQuestionAnswering](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForQuestionAnswering) (ERNIE model) - [Exaone4Config](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4Config) configuration class: [Exaone4ForQuestionAnswering](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForQuestionAnswering) (EXAONE-4.0 model) - [FNetConfig](/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig) configuration class: [FNetForQuestionAnswering](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForQuestionAnswering) (FNet model) - [FalconConfig](/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig) configuration class: [FalconForQuestionAnswering](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForQuestionAnswering) (Falcon model) - [FlaubertConfig](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig) configuration class: [FlaubertForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple) (FlauBERT model) - [FunnelConfig](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig) configuration class: [FunnelForQuestionAnswering](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering) (Funnel Transformer model) - [GPT2Config](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config) configuration class: [GPT2ForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering) (OpenAI GPT-2 model) - [GPTJConfig](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig) configuration class: [GPTJForQuestionAnswering](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForQuestionAnswering) (GPT-J model) - [GPTNeoConfig](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig) configuration class: [GPTNeoForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering) (GPT Neo model) - [GPTNeoXConfig](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig) configuration class: [GPTNeoXForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering) (GPT NeoX model) - [IBertConfig](/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig) configuration class: [IBertForQuestionAnswering](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForQuestionAnswering) (I-BERT model) - [LEDConfig](/docs/transformers/main/en/model_doc/led#transformers.LEDConfig) configuration class: [LEDForQuestionAnswering](/docs/transformers/main/en/model_doc/led#transformers.LEDForQuestionAnswering) (LED model) - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model) - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model) - [LiltConfig](/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig) configuration class: [LiltForQuestionAnswering](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForQuestionAnswering) (LiLT model) - [LlamaConfig](/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig) configuration class: [LlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering) (LLaMA model) - [LongformerConfig](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig) configuration class: [LongformerForQuestionAnswering](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForQuestionAnswering) (Longformer model) - [LukeConfig](/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig) configuration class: [LukeForQuestionAnswering](/docs/transformers/main/en/model_doc/luke#transformers.LukeForQuestionAnswering) (LUKE model) - [LxmertConfig](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig) configuration class: [LxmertForQuestionAnswering](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering) (LXMERT model) - [MBartConfig](/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig) configuration class: [MBartForQuestionAnswering](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForQuestionAnswering) (mBART model) - [MPNetConfig](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig) configuration class: [MPNetForQuestionAnswering](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering) (MPNet model) - [MT5Config](/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config) configuration class: [MT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForQuestionAnswering) (MT5 model) - [MarkupLMConfig](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig) configuration class: [MarkupLMForQuestionAnswering](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering) (MarkupLM model) - [MegatronBertConfig](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig) configuration class: [MegatronBertForQuestionAnswering](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering) (Megatron-BERT model) - [MiniMaxConfig](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxConfig) configuration class: [MiniMaxForQuestionAnswering](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForQuestionAnswering) (MiniMax model) - [Ministral3Config](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3Config) configuration class: [Ministral3ForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForQuestionAnswering) (Ministral3 model) - [MinistralConfig](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralConfig) configuration class: [MinistralForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForQuestionAnswering) (Ministral model) - [MistralConfig](/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig) configuration class: [MistralForQuestionAnswering](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForQuestionAnswering) (Mistral model) - [MixtralConfig](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig) configuration class: [MixtralForQuestionAnswering](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForQuestionAnswering) (Mixtral model) - [MobileBertConfig](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig) configuration class: [MobileBertForQuestionAnswering](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering) (MobileBERT model) - [ModernBertConfig](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) configuration class: [ModernBertForQuestionAnswering](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering) (ModernBERT model) - [MptConfig](/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig) configuration class: [MptForQuestionAnswering](/docs/transformers/main/en/model_doc/mpt#transformers.MptForQuestionAnswering) (MPT model) - [MraConfig](/docs/transformers/main/en/model_doc/mra#transformers.MraConfig) configuration class: [MraForQuestionAnswering](/docs/transformers/main/en/model_doc/mra#transformers.MraForQuestionAnswering) (MRA model) - [MvpConfig](/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig) configuration class: [MvpForQuestionAnswering](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForQuestionAnswering) (MVP model) - [NemotronConfig](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronConfig) configuration class: [NemotronForQuestionAnswering](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForQuestionAnswering) (Nemotron model) - [NystromformerConfig](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig) configuration class: [NystromformerForQuestionAnswering](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering) (Nystr√∂mformer model) - [OPTConfig](/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig) configuration class: [OPTForQuestionAnswering](/docs/transformers/main/en/model_doc/opt#transformers.OPTForQuestionAnswering) (OPT model) - [Qwen2Config](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config) configuration class: [Qwen2ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForQuestionAnswering) (Qwen2 model) - [Qwen2MoeConfig](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig) configuration class: [Qwen2MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForQuestionAnswering) (Qwen2MoE model) - [Qwen3Config](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3Config) configuration class: [Qwen3ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForQuestionAnswering) (Qwen3 model) - [Qwen3MoeConfig](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig) configuration class: [Qwen3MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForQuestionAnswering) (Qwen3MoE model) - [Qwen3NextConfig](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextConfig) configuration class: [Qwen3NextForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForQuestionAnswering) (Qwen3Next model) - [ReformerConfig](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig) configuration class: [ReformerForQuestionAnswering](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForQuestionAnswering) (Reformer model) - [RemBertConfig](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig) configuration class: [RemBertForQuestionAnswering](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForQuestionAnswering) (RemBERT model) - [RoCBertConfig](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig) configuration class: [RoCBertForQuestionAnswering](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering) (RoCBert model) - [RoFormerConfig](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig) configuration class: [RoFormerForQuestionAnswering](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering) (RoFormer model) - [RobertaConfig](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig) configuration class: [RobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering) (RoBERTa model) - [RobertaPreLayerNormConfig](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig) configuration class: [RobertaPreLayerNormForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering) (RoBERTa-PreLayerNorm model) - [SeedOssConfig](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssConfig) configuration class: [SeedOssForQuestionAnswering](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForQuestionAnswering) (SeedOss model) - [SmolLM3Config](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3Config) configuration class: [SmolLM3ForQuestionAnswering](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForQuestionAnswering) (SmolLM3 model) - [SplinterConfig](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig) configuration class: [SplinterForQuestionAnswering](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForQuestionAnswering) (Splinter model) - [SqueezeBertConfig](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig) configuration class: [SqueezeBertForQuestionAnswering](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering) (SqueezeBERT model) - [T5Config](/docs/transformers/main/en/model_doc/t5#transformers.T5Config) configuration class: [T5ForQuestionAnswering](/docs/transformers/main/en/model_doc/t5#transformers.T5ForQuestionAnswering) (T5 model) - [UMT5Config](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config) configuration class: [UMT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering) (UMT5 model) - [XLMConfig](/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig) configuration class: [XLMForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple) (XLM model) - [XLMRobertaConfig](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig) configuration class: [XLMRobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering) (XLM-RoBERTa model) - [XLMRobertaXLConfig](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig) configuration class: [XLMRobertaXLForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering) (XLM-RoBERTa-XL model) - [XLNetConfig](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig) configuration class: [XLNetForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple) (XLNet model) - [XmodConfig](/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig) configuration class: [XmodForQuestionAnswering](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForQuestionAnswering) (X-MOD model) - [YosoConfig](/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig) configuration class: [YosoForQuestionAnswering](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForQuestionAnswering) (YOSO model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForQuestionAnswering.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **albert** -- `AlbertForQuestionAnswering` (ALBERT model)
- **arcee** -- [ArceeForQuestionAnswering](/docs/transformers/main/en/model_doc/arcee#transformers.ArceeForQuestionAnswering) (Arcee model)
- **bart** -- [BartForQuestionAnswering](/docs/transformers/main/en/model_doc/bart#transformers.BartForQuestionAnswering) (BART model)
- **bert** -- [BertForQuestionAnswering](/docs/transformers/main/en/model_doc/bert#transformers.BertForQuestionAnswering) (BERT model)
- **big_bird** -- [BigBirdForQuestionAnswering](/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering) (BigBird model)
- **bigbird_pegasus** -- [BigBirdPegasusForQuestionAnswering](/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering) (BigBird-Pegasus model)
- **bloom** -- [BloomForQuestionAnswering](/docs/transformers/main/en/model_doc/bloom#transformers.BloomForQuestionAnswering) (BLOOM model)
- **camembert** -- [CamembertForQuestionAnswering](/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForQuestionAnswering) (CamemBERT model)
- **canine** -- [CanineForQuestionAnswering](/docs/transformers/main/en/model_doc/canine#transformers.CanineForQuestionAnswering) (CANINE model)
- **convbert** -- [ConvBertForQuestionAnswering](/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering) (ConvBERT model)
- **data2vec-text** -- [Data2VecTextForQuestionAnswering](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering) (Data2VecText model)
- **deberta** -- [DebertaForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForQuestionAnswering) (DeBERTa model)
- **deberta-v2** -- [DebertaV2ForQuestionAnswering](/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering) (DeBERTa-v2 model)
- **diffllama** -- [DiffLlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/diffllama#transformers.DiffLlamaForQuestionAnswering) (DiffLlama model)
- **distilbert** -- [DistilBertForQuestionAnswering](/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering) (DistilBERT model)
- **electra** -- [ElectraForQuestionAnswering](/docs/transformers/main/en/model_doc/electra#transformers.ElectraForQuestionAnswering) (ELECTRA model)
- **ernie** -- [ErnieForQuestionAnswering](/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForQuestionAnswering) (ERNIE model)
- **exaone4** -- [Exaone4ForQuestionAnswering](/docs/transformers/main/en/model_doc/exaone4#transformers.Exaone4ForQuestionAnswering) (EXAONE-4.0 model)
- **falcon** -- [FalconForQuestionAnswering](/docs/transformers/main/en/model_doc/falcon#transformers.FalconForQuestionAnswering) (Falcon model)
- **flaubert** -- [FlaubertForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple) (FlauBERT model)
- **fnet** -- [FNetForQuestionAnswering](/docs/transformers/main/en/model_doc/fnet#transformers.FNetForQuestionAnswering) (FNet model)
- **funnel** -- [FunnelForQuestionAnswering](/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering) (Funnel Transformer model)
- **gpt2** -- [GPT2ForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering) (OpenAI GPT-2 model)
- **gpt_neo** -- [GPTNeoForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering) (GPT Neo model)
- **gpt_neox** -- [GPTNeoXForQuestionAnswering](/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering) (GPT NeoX model)
- **gptj** -- [GPTJForQuestionAnswering](/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForQuestionAnswering) (GPT-J model)
- **ibert** -- [IBertForQuestionAnswering](/docs/transformers/main/en/model_doc/ibert#transformers.IBertForQuestionAnswering) (I-BERT model)
- **layoutlmv2** -- [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model)
- **led** -- [LEDForQuestionAnswering](/docs/transformers/main/en/model_doc/led#transformers.LEDForQuestionAnswering) (LED model)
- **lilt** -- [LiltForQuestionAnswering](/docs/transformers/main/en/model_doc/lilt#transformers.LiltForQuestionAnswering) (LiLT model)
- **llama** -- [LlamaForQuestionAnswering](/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering) (LLaMA model)
- **longformer** -- [LongformerForQuestionAnswering](/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForQuestionAnswering) (Longformer model)
- **luke** -- [LukeForQuestionAnswering](/docs/transformers/main/en/model_doc/luke#transformers.LukeForQuestionAnswering) (LUKE model)
- **lxmert** -- [LxmertForQuestionAnswering](/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering) (LXMERT model)
- **markuplm** -- [MarkupLMForQuestionAnswering](/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering) (MarkupLM model)
- **mbart** -- [MBartForQuestionAnswering](/docs/transformers/main/en/model_doc/mbart#transformers.MBartForQuestionAnswering) (mBART model)
- **megatron-bert** -- [MegatronBertForQuestionAnswering](/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering) (Megatron-BERT model)
- **minimax** -- [MiniMaxForQuestionAnswering](/docs/transformers/main/en/model_doc/minimax#transformers.MiniMaxForQuestionAnswering) (MiniMax model)
- **ministral** -- [MinistralForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral#transformers.MinistralForQuestionAnswering) (Ministral model)
- **ministral3** -- [Ministral3ForQuestionAnswering](/docs/transformers/main/en/model_doc/ministral3#transformers.Ministral3ForQuestionAnswering) (Ministral3 model)
- **mistral** -- [MistralForQuestionAnswering](/docs/transformers/main/en/model_doc/mistral#transformers.MistralForQuestionAnswering) (Mistral model)
- **mixtral** -- [MixtralForQuestionAnswering](/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForQuestionAnswering) (Mixtral model)
- **mobilebert** -- [MobileBertForQuestionAnswering](/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering) (MobileBERT model)
- **modernbert** -- [ModernBertForQuestionAnswering](/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering) (ModernBERT model)
- **mpnet** -- [MPNetForQuestionAnswering](/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering) (MPNet model)
- **mpt** -- [MptForQuestionAnswering](/docs/transformers/main/en/model_doc/mpt#transformers.MptForQuestionAnswering) (MPT model)
- **mra** -- [MraForQuestionAnswering](/docs/transformers/main/en/model_doc/mra#transformers.MraForQuestionAnswering) (MRA model)
- **mt5** -- [MT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForQuestionAnswering) (MT5 model)
- **mvp** -- [MvpForQuestionAnswering](/docs/transformers/main/en/model_doc/mvp#transformers.MvpForQuestionAnswering) (MVP model)
- **nemotron** -- [NemotronForQuestionAnswering](/docs/transformers/main/en/model_doc/nemotron#transformers.NemotronForQuestionAnswering) (Nemotron model)
- **nystromformer** -- [NystromformerForQuestionAnswering](/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering) (Nystr√∂mformer model)
- **opt** -- [OPTForQuestionAnswering](/docs/transformers/main/en/model_doc/opt#transformers.OPTForQuestionAnswering) (OPT model)
- **qwen2** -- [Qwen2ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForQuestionAnswering) (Qwen2 model)
- **qwen2_moe** -- [Qwen2MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen2_moe#transformers.Qwen2MoeForQuestionAnswering) (Qwen2MoE model)
- **qwen3** -- [Qwen3ForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3#transformers.Qwen3ForQuestionAnswering) (Qwen3 model)
- **qwen3_moe** -- [Qwen3MoeForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_moe#transformers.Qwen3MoeForQuestionAnswering) (Qwen3MoE model)
- **qwen3_next** -- [Qwen3NextForQuestionAnswering](/docs/transformers/main/en/model_doc/qwen3_next#transformers.Qwen3NextForQuestionAnswering) (Qwen3Next model)
- **reformer** -- [ReformerForQuestionAnswering](/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForQuestionAnswering) (Reformer model)
- **rembert** -- [RemBertForQuestionAnswering](/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForQuestionAnswering) (RemBERT model)
- **roberta** -- [RobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering) (RoBERTa model)
- **roberta-prelayernorm** -- [RobertaPreLayerNormForQuestionAnswering](/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering) (RoBERTa-PreLayerNorm model)
- **roc_bert** -- [RoCBertForQuestionAnswering](/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering) (RoCBert model)
- **roformer** -- [RoFormerForQuestionAnswering](/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering) (RoFormer model)
- **seed_oss** -- [SeedOssForQuestionAnswering](/docs/transformers/main/en/model_doc/seed_oss#transformers.SeedOssForQuestionAnswering) (SeedOss model)
- **smollm3** -- [SmolLM3ForQuestionAnswering](/docs/transformers/main/en/model_doc/smollm3#transformers.SmolLM3ForQuestionAnswering) (SmolLM3 model)
- **splinter** -- [SplinterForQuestionAnswering](/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForQuestionAnswering) (Splinter model)
- **squeezebert** -- [SqueezeBertForQuestionAnswering](/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering) (SqueezeBERT model)
- **t5** -- [T5ForQuestionAnswering](/docs/transformers/main/en/model_doc/t5#transformers.T5ForQuestionAnswering) (T5 model)
- **umt5** -- [UMT5ForQuestionAnswering](/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering) (UMT5 model)
- **xlm** -- [XLMForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple) (XLM model)
- **xlm-roberta** -- [XLMRobertaForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering) (XLM-RoBERTa model)
- **xlm-roberta-xl** -- [XLMRobertaXLForQuestionAnswering](/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering) (XLM-RoBERTa-XL model)
- **xlnet** -- [XLNetForQuestionAnsweringSimple](/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple) (XLNet model)
- **xmod** -- [XmodForQuestionAnswering](/docs/transformers/main/en/model_doc/xmod#transformers.XmodForQuestionAnswering) (X-MOD model)
- **yoso** -- [YosoForQuestionAnswering](/docs/transformers/main/en/model_doc/yoso#transformers.YosoForQuestionAnswering) (YOSO model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForQuestionAnswering

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForQuestionAnswering.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForQuestionAnswering.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForTextEncoding[[transformers.AutoModelForTextEncoding]]

#### transformers.AutoModelForTextEncoding[[transformers.AutoModelForTextEncoding]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1917)

## Computer vision

The following auto classes are available for the following computer vision tasks.

### AutoModelForDepthEstimation[[transformers.AutoModelForDepthEstimation]]

#### transformers.AutoModelForDepthEstimation[[transformers.AutoModelForDepthEstimation]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2129)

This is a generic model class that will be instantiated as one of the model classes of the library (with a depth estimation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForDepthEstimation.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTForDepthEstimation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForDepthEstimation) (DPT model)
  - [DepthAnythingConfig](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingConfig) configuration class: [DepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation) (Depth Anything model)
  - [DepthProConfig](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProConfig) configuration class: [DepthProForDepthEstimation](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation) (DepthPro model)
  - [GLPNConfig](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig) configuration class: [GLPNForDepthEstimation](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNForDepthEstimation) (GLPN model)
  - [PromptDepthAnythingConfig](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig) configuration class: [PromptDepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation) (PromptDepthAnything model)
  - [ZoeDepthConfig](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthConfig) configuration class: [ZoeDepthForDepthEstimation](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthForDepthEstimation) (ZoeDepth model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a depth estimation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForDepthEstimation

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForDepthEstimation.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTForDepthEstimation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForDepthEstimation) (DPT model) - [DepthAnythingConfig](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingConfig) configuration class: [DepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation) (Depth Anything model) - [DepthProConfig](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProConfig) configuration class: [DepthProForDepthEstimation](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation) (DepthPro model) - [GLPNConfig](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig) configuration class: [GLPNForDepthEstimation](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNForDepthEstimation) (GLPN model) - [PromptDepthAnythingConfig](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig) configuration class: [PromptDepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation) (PromptDepthAnything model) - [ZoeDepthConfig](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthConfig) configuration class: [ZoeDepthForDepthEstimation](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthForDepthEstimation) (ZoeDepth model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForDepthEstimation.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a depth estimation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **depth_anything** -- [DepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation) (Depth Anything model)
- **depth_pro** -- [DepthProForDepthEstimation](/docs/transformers/main/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation) (DepthPro model)
- **dpt** -- [DPTForDepthEstimation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForDepthEstimation) (DPT model)
- **glpn** -- [GLPNForDepthEstimation](/docs/transformers/main/en/model_doc/glpn#transformers.GLPNForDepthEstimation) (GLPN model)
- **prompt_depth_anything** -- [PromptDepthAnythingForDepthEstimation](/docs/transformers/main/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation) (PromptDepthAnything model)
- **zoedepth** -- [ZoeDepthForDepthEstimation](/docs/transformers/main/en/model_doc/zoedepth#transformers.ZoeDepthForDepthEstimation) (ZoeDepth model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForDepthEstimation

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForDepthEstimation.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForDepthEstimation.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForImageClassification[[transformers.AutoModelForImageClassification]]

#### transformers.AutoModelForImageClassification[[transformers.AutoModelForImageClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2054)

This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForImageClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitForImageClassification](/docs/transformers/main/en/model_doc/beit#transformers.BeitForImageClassification) (BEiT model)
  - [BitConfig](/docs/transformers/main/en/model_doc/bit#transformers.BitConfig) configuration class: [BitForImageClassification](/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification) (BiT model)
  - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPForImageClassification](/docs/transformers/main/en/model_doc/clip#transformers.CLIPForImageClassification) (CLIP model)
  - [ConvNextConfig](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig) configuration class: [ConvNextForImageClassification](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextForImageClassification) (ConvNeXT model)
  - [ConvNextV2Config](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config) configuration class: [ConvNextV2ForImageClassification](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification) (ConvNeXTV2 model)
  - [CvtConfig](/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig) configuration class: [CvtForImageClassification](/docs/transformers/main/en/model_doc/cvt#transformers.CvtForImageClassification) (CvT model)
  - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionForImageClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification) (Data2VecVision model)
  - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTForImageClassification](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassification) or [DeiTForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher) (DeiT model)
  - [DinatConfig](/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig) configuration class: [DinatForImageClassification](/docs/transformers/main/en/model_doc/dinat#transformers.DinatForImageClassification) (DiNAT model)
  - [Dinov2Config](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config) configuration class: [Dinov2ForImageClassification](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification) (DINOv2 model)
  - [Dinov2WithRegistersConfig](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig) configuration class: [Dinov2WithRegistersForImageClassification](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersForImageClassification) (DINOv2 with Registers model)
  - [DonutSwinConfig](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig) configuration class: [DonutSwinForImageClassification](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinForImageClassification) (DonutSwin model)
  - [EfficientNetConfig](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig) configuration class: [EfficientNetForImageClassification](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification) (EfficientNet model)
  - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetForImageClassification](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification) (FocalNet model)
  - [HGNetV2Config](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Config) configuration class: [HGNetV2ForImageClassification](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification) (HGNet-V2 model)
  - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraForImageClassification](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForImageClassification) (Hiera model)
  - [IJepaConfig](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaConfig) configuration class: [IJepaForImageClassification](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaForImageClassification) (I-JEPA model)
  - [ImageGPTConfig](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig) configuration class: [ImageGPTForImageClassification](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification) (ImageGPT model)
  - [LevitConfig](/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig) configuration class: [LevitForImageClassification](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassification) or [LevitForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher) (LeViT model)
  - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2ForImageClassification](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification) (MetaCLIP 2 model)
  - [MobileNetV1Config](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config) configuration class: [MobileNetV1ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification) (MobileNetV1 model)
  - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification) (MobileNetV2 model)
  - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTForImageClassification](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForImageClassification) (MobileViT model)
  - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification) (MobileViTV2 model)
  - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForImageClassificationLearned](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned) or [PerceiverForImageClassificationFourier](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier) or [PerceiverForImageClassificationConvProcessing](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing) (Perceiver model)
  - [PoolFormerConfig](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig) configuration class: [PoolFormerForImageClassification](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerForImageClassification) (PoolFormer model)
  - [PvtConfig](/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig) configuration class: [PvtForImageClassification](/docs/transformers/main/en/model_doc/pvt#transformers.PvtForImageClassification) (PVT model)
  - [PvtV2Config](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Config) configuration class: [PvtV2ForImageClassification](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification) (PVTv2 model)
  - [RegNetConfig](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig) configuration class: [RegNetForImageClassification](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetForImageClassification) (RegNet model)
  - [ResNetConfig](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig) configuration class: [ResNetForImageClassification](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetForImageClassification) (ResNet model)
  - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerForImageClassification](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForImageClassification) (SegFormer model)
  - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [ShieldGemma2ForImageClassification](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2ForImageClassification) (Shieldgemma2 model)
  - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2ForImageClassification](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2ForImageClassification) (SigLIP2 model)
  - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipForImageClassification](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipForImageClassification) (SigLIP model)
  - [SwiftFormerConfig](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig) configuration class: [SwiftFormerForImageClassification](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification) (SwiftFormer model)
  - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinForImageClassification](/docs/transformers/main/en/model_doc/swin#transformers.SwinForImageClassification) (Swin Transformer model)
  - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2ForImageClassification](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification) (Swin Transformer V2 model)
  - [TextNetConfig](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetConfig) configuration class: [TextNetForImageClassification](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetForImageClassification) (TextNet model)
  - [TimmWrapperConfig](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig) configuration class: [TimmWrapperForImageClassification](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification) (TimmWrapperModel model)
  - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTForImageClassification](/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification) (ViT model)
  - [ViTMSNConfig](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig) configuration class: [ViTMSNForImageClassification](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification) (ViTMSN model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a image classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForImageClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitForImageClassification](/docs/transformers/main/en/model_doc/beit#transformers.BeitForImageClassification) (BEiT model) - [BitConfig](/docs/transformers/main/en/model_doc/bit#transformers.BitConfig) configuration class: [BitForImageClassification](/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification) (BiT model) - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPForImageClassification](/docs/transformers/main/en/model_doc/clip#transformers.CLIPForImageClassification) (CLIP model) - [ConvNextConfig](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig) configuration class: [ConvNextForImageClassification](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextForImageClassification) (ConvNeXT model) - [ConvNextV2Config](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config) configuration class: [ConvNextV2ForImageClassification](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification) (ConvNeXTV2 model) - [CvtConfig](/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig) configuration class: [CvtForImageClassification](/docs/transformers/main/en/model_doc/cvt#transformers.CvtForImageClassification) (CvT model) - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionForImageClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification) (Data2VecVision model) - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTForImageClassification](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassification) or [DeiTForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher) (DeiT model) - [DinatConfig](/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig) configuration class: [DinatForImageClassification](/docs/transformers/main/en/model_doc/dinat#transformers.DinatForImageClassification) (DiNAT model) - [Dinov2Config](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config) configuration class: [Dinov2ForImageClassification](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification) (DINOv2 model) - [Dinov2WithRegistersConfig](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig) configuration class: [Dinov2WithRegistersForImageClassification](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersForImageClassification) (DINOv2 with Registers model) - [DonutSwinConfig](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig) configuration class: [DonutSwinForImageClassification](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinForImageClassification) (DonutSwin model) - [EfficientNetConfig](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig) configuration class: [EfficientNetForImageClassification](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification) (EfficientNet model) - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetForImageClassification](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification) (FocalNet model) - [HGNetV2Config](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2Config) configuration class: [HGNetV2ForImageClassification](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification) (HGNet-V2 model) - [HieraConfig](/docs/transformers/main/en/model_doc/hiera#transformers.HieraConfig) configuration class: [HieraForImageClassification](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForImageClassification) (Hiera model) - [IJepaConfig](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaConfig) configuration class: [IJepaForImageClassification](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaForImageClassification) (I-JEPA model) - [ImageGPTConfig](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig) configuration class: [ImageGPTForImageClassification](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification) (ImageGPT model) - [LevitConfig](/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig) configuration class: [LevitForImageClassification](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassification) or [LevitForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher) (LeViT model) - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2ForImageClassification](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification) (MetaCLIP 2 model) - [MobileNetV1Config](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config) configuration class: [MobileNetV1ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification) (MobileNetV1 model) - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification) (MobileNetV2 model) - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTForImageClassification](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForImageClassification) (MobileViT model) - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification) (MobileViTV2 model) - [PerceiverConfig](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig) configuration class: [PerceiverForImageClassificationLearned](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned) or [PerceiverForImageClassificationFourier](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier) or [PerceiverForImageClassificationConvProcessing](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing) (Perceiver model) - [PoolFormerConfig](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig) configuration class: [PoolFormerForImageClassification](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerForImageClassification) (PoolFormer model) - [PvtConfig](/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig) configuration class: [PvtForImageClassification](/docs/transformers/main/en/model_doc/pvt#transformers.PvtForImageClassification) (PVT model) - [PvtV2Config](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2Config) configuration class: [PvtV2ForImageClassification](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification) (PVTv2 model) - [RegNetConfig](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig) configuration class: [RegNetForImageClassification](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetForImageClassification) (RegNet model) - [ResNetConfig](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig) configuration class: [ResNetForImageClassification](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetForImageClassification) (ResNet model) - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerForImageClassification](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForImageClassification) (SegFormer model) - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [ShieldGemma2ForImageClassification](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2ForImageClassification) (Shieldgemma2 model) - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2ForImageClassification](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2ForImageClassification) (SigLIP2 model) - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipForImageClassification](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipForImageClassification) (SigLIP model) - [SwiftFormerConfig](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig) configuration class: [SwiftFormerForImageClassification](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification) (SwiftFormer model) - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinForImageClassification](/docs/transformers/main/en/model_doc/swin#transformers.SwinForImageClassification) (Swin Transformer model) - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2ForImageClassification](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification) (Swin Transformer V2 model) - [TextNetConfig](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetConfig) configuration class: [TextNetForImageClassification](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetForImageClassification) (TextNet model) - [TimmWrapperConfig](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig) configuration class: [TimmWrapperForImageClassification](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification) (TimmWrapperModel model) - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTForImageClassification](/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification) (ViT model) - [ViTMSNConfig](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig) configuration class: [ViTMSNForImageClassification](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification) (ViTMSN model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForImageClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **beit** -- [BeitForImageClassification](/docs/transformers/main/en/model_doc/beit#transformers.BeitForImageClassification) (BEiT model)
- **bit** -- [BitForImageClassification](/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification) (BiT model)
- **clip** -- [CLIPForImageClassification](/docs/transformers/main/en/model_doc/clip#transformers.CLIPForImageClassification) (CLIP model)
- **convnext** -- [ConvNextForImageClassification](/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextForImageClassification) (ConvNeXT model)
- **convnextv2** -- [ConvNextV2ForImageClassification](/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification) (ConvNeXTV2 model)
- **cvt** -- [CvtForImageClassification](/docs/transformers/main/en/model_doc/cvt#transformers.CvtForImageClassification) (CvT model)
- **data2vec-vision** -- [Data2VecVisionForImageClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification) (Data2VecVision model)
- **deit** -- [DeiTForImageClassification](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassification) or [DeiTForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher) (DeiT model)
- **dinat** -- [DinatForImageClassification](/docs/transformers/main/en/model_doc/dinat#transformers.DinatForImageClassification) (DiNAT model)
- **dinov2** -- [Dinov2ForImageClassification](/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification) (DINOv2 model)
- **dinov2_with_registers** -- [Dinov2WithRegistersForImageClassification](/docs/transformers/main/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersForImageClassification) (DINOv2 with Registers model)
- **donut-swin** -- [DonutSwinForImageClassification](/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinForImageClassification) (DonutSwin model)
- **efficientnet** -- [EfficientNetForImageClassification](/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification) (EfficientNet model)
- **focalnet** -- [FocalNetForImageClassification](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification) (FocalNet model)
- **hgnet_v2** -- [HGNetV2ForImageClassification](/docs/transformers/main/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification) (HGNet-V2 model)
- **hiera** -- [HieraForImageClassification](/docs/transformers/main/en/model_doc/hiera#transformers.HieraForImageClassification) (Hiera model)
- **ijepa** -- [IJepaForImageClassification](/docs/transformers/main/en/model_doc/ijepa#transformers.IJepaForImageClassification) (I-JEPA model)
- **imagegpt** -- [ImageGPTForImageClassification](/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification) (ImageGPT model)
- **levit** -- [LevitForImageClassification](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassification) or [LevitForImageClassificationWithTeacher](/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher) (LeViT model)
- **metaclip_2** -- [MetaClip2ForImageClassification](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification) (MetaCLIP 2 model)
- **mobilenet_v1** -- [MobileNetV1ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification) (MobileNetV1 model)
- **mobilenet_v2** -- [MobileNetV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification) (MobileNetV2 model)
- **mobilevit** -- [MobileViTForImageClassification](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForImageClassification) (MobileViT model)
- **mobilevitv2** -- [MobileViTV2ForImageClassification](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification) (MobileViTV2 model)
- **perceiver** -- [PerceiverForImageClassificationLearned](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned) or [PerceiverForImageClassificationFourier](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier) or [PerceiverForImageClassificationConvProcessing](/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing) (Perceiver model)
- **poolformer** -- [PoolFormerForImageClassification](/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerForImageClassification) (PoolFormer model)
- **pvt** -- [PvtForImageClassification](/docs/transformers/main/en/model_doc/pvt#transformers.PvtForImageClassification) (PVT model)
- **pvt_v2** -- [PvtV2ForImageClassification](/docs/transformers/main/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification) (PVTv2 model)
- **regnet** -- [RegNetForImageClassification](/docs/transformers/main/en/model_doc/regnet#transformers.RegNetForImageClassification) (RegNet model)
- **resnet** -- [ResNetForImageClassification](/docs/transformers/main/en/model_doc/resnet#transformers.ResNetForImageClassification) (ResNet model)
- **segformer** -- [SegformerForImageClassification](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForImageClassification) (SegFormer model)
- **shieldgemma2** -- [ShieldGemma2ForImageClassification](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2ForImageClassification) (Shieldgemma2 model)
- **siglip** -- [SiglipForImageClassification](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipForImageClassification) (SigLIP model)
- **siglip2** -- [Siglip2ForImageClassification](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2ForImageClassification) (SigLIP2 model)
- **swiftformer** -- [SwiftFormerForImageClassification](/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification) (SwiftFormer model)
- **swin** -- [SwinForImageClassification](/docs/transformers/main/en/model_doc/swin#transformers.SwinForImageClassification) (Swin Transformer model)
- **swinv2** -- [Swinv2ForImageClassification](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification) (Swin Transformer V2 model)
- **textnet** -- [TextNetForImageClassification](/docs/transformers/main/en/model_doc/textnet#transformers.TextNetForImageClassification) (TextNet model)
- **timm_wrapper** -- [TimmWrapperForImageClassification](/docs/transformers/main/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification) (TimmWrapperModel model)
- **vit** -- [ViTForImageClassification](/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification) (ViT model)
- **vit_msn** -- [ViTMSNForImageClassification](/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification) (ViTMSN model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForImageClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForImageClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForVideoClassification[[transformers.AutoModelForVideoClassification]]

#### transformers.AutoModelForVideoClassification[[transformers.AutoModelForVideoClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2136)

This is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForVideoClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [TimesformerConfig](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig) configuration class: [TimesformerForVideoClassification](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerForVideoClassification) (TimeSformer model)
  - [VJEPA2Config](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Config) configuration class: [VJEPA2ForVideoClassification](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification) (VJEPA2Model model)
  - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEForVideoClassification](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForVideoClassification) (VideoMAE model)
  - [VivitConfig](/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig) configuration class: [VivitForVideoClassification](/docs/transformers/main/en/model_doc/vivit#transformers.VivitForVideoClassification) (ViViT model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a video classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForVideoClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForVideoClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [TimesformerConfig](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig) configuration class: [TimesformerForVideoClassification](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerForVideoClassification) (TimeSformer model) - [VJEPA2Config](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2Config) configuration class: [VJEPA2ForVideoClassification](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification) (VJEPA2Model model) - [VideoMAEConfig](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig) configuration class: [VideoMAEForVideoClassification](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForVideoClassification) (VideoMAE model) - [VivitConfig](/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig) configuration class: [VivitForVideoClassification](/docs/transformers/main/en/model_doc/vivit#transformers.VivitForVideoClassification) (ViViT model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForVideoClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a video classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **timesformer** -- [TimesformerForVideoClassification](/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerForVideoClassification) (TimeSformer model)
- **videomae** -- [VideoMAEForVideoClassification](/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForVideoClassification) (VideoMAE model)
- **vivit** -- [VivitForVideoClassification](/docs/transformers/main/en/model_doc/vivit#transformers.VivitForVideoClassification) (ViViT model)
- **vjepa2** -- [VJEPA2ForVideoClassification](/docs/transformers/main/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification) (VJEPA2Model model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForVideoClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForVideoClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForVideoClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForKeypointDetection[[transformers.AutoModelForKeypointDetection]]

#### transformers.AutoModelForKeypointDetection[[transformers.AutoModelForKeypointDetection]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1909)

### AutoModelForKeypointMatching[[transformers.AutoModelForKeypointMatching]]

#### transformers.AutoModelForKeypointMatching[[transformers.AutoModelForKeypointMatching]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1913)

### AutoModelForMaskedImageModeling[[transformers.AutoModelForMaskedImageModeling]]

#### transformers.AutoModelForMaskedImageModeling[[transformers.AutoModelForMaskedImageModeling]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2226)

This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForMaskedImageModeling.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTForMaskedImageModeling](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForMaskedImageModeling) (DeiT model)
  - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetForMaskedImageModeling](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling) (FocalNet model)
  - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinForMaskedImageModeling](/docs/transformers/main/en/model_doc/swin#transformers.SwinForMaskedImageModeling) (Swin Transformer model)
  - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2ForMaskedImageModeling](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling) (Swin Transformer V2 model)
  - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTForMaskedImageModeling](/docs/transformers/main/en/model_doc/vit#transformers.ViTForMaskedImageModeling) (ViT model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMaskedImageModeling

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForMaskedImageModeling.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DeiTConfig](/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig) configuration class: [DeiTForMaskedImageModeling](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForMaskedImageModeling) (DeiT model) - [FocalNetConfig](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig) configuration class: [FocalNetForMaskedImageModeling](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling) (FocalNet model) - [SwinConfig](/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig) configuration class: [SwinForMaskedImageModeling](/docs/transformers/main/en/model_doc/swin#transformers.SwinForMaskedImageModeling) (Swin Transformer model) - [Swinv2Config](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config) configuration class: [Swinv2ForMaskedImageModeling](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling) (Swin Transformer V2 model) - [ViTConfig](/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig) configuration class: [ViTForMaskedImageModeling](/docs/transformers/main/en/model_doc/vit#transformers.ViTForMaskedImageModeling) (ViT model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForMaskedImageModeling.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **deit** -- [DeiTForMaskedImageModeling](/docs/transformers/main/en/model_doc/deit#transformers.DeiTForMaskedImageModeling) (DeiT model)
- **focalnet** -- [FocalNetForMaskedImageModeling](/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling) (FocalNet model)
- **swin** -- [SwinForMaskedImageModeling](/docs/transformers/main/en/model_doc/swin#transformers.SwinForMaskedImageModeling) (Swin Transformer model)
- **swinv2** -- [Swinv2ForMaskedImageModeling](/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling) (Swin Transformer V2 model)
- **vit** -- [ViTForMaskedImageModeling](/docs/transformers/main/en/model_doc/vit#transformers.ViTForMaskedImageModeling) (ViT model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMaskedImageModeling

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForMaskedImageModeling.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForMaskedImageModeling.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForObjectDetection[[transformers.AutoModelForObjectDetection]]

#### transformers.AutoModelForObjectDetection[[transformers.AutoModelForObjectDetection]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2113)

This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForObjectDetection.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [ConditionalDetrConfig](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) configuration class: [ConditionalDetrForObjectDetection](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection) (Conditional DETR model)
  - [DFineConfig](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineConfig) configuration class: [DFineForObjectDetection](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineForObjectDetection) (D-FINE model)
  - [DabDetrConfig](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrConfig) configuration class: [DabDetrForObjectDetection](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection) (DAB-DETR model)
  - [DeformableDetrConfig](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig) configuration class: [DeformableDetrForObjectDetection](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection) (Deformable DETR model)
  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForObjectDetection](/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection) (DETR model)
  - [RTDetrConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrConfig) configuration class: [RTDetrForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrForObjectDetection) (RT-DETR model)
  - [RTDetrV2Config](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) configuration class: [RTDetrV2ForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection) (RT-DETRv2 model)
  - [TableTransformerConfig](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig) configuration class: [TableTransformerForObjectDetection](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection) (Table Transformer model)
  - [YolosConfig](/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig) configuration class: [YolosForObjectDetection](/docs/transformers/main/en/model_doc/yolos#transformers.YolosForObjectDetection) (YOLOS model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a object detection head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForObjectDetection

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForObjectDetection.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [ConditionalDetrConfig](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) configuration class: [ConditionalDetrForObjectDetection](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection) (Conditional DETR model) - [DFineConfig](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineConfig) configuration class: [DFineForObjectDetection](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineForObjectDetection) (D-FINE model) - [DabDetrConfig](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrConfig) configuration class: [DabDetrForObjectDetection](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection) (DAB-DETR model) - [DeformableDetrConfig](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig) configuration class: [DeformableDetrForObjectDetection](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection) (Deformable DETR model) - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForObjectDetection](/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection) (DETR model) - [RTDetrConfig](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrConfig) configuration class: [RTDetrForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrForObjectDetection) (RT-DETR model) - [RTDetrV2Config](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) configuration class: [RTDetrV2ForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection) (RT-DETRv2 model) - [TableTransformerConfig](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig) configuration class: [TableTransformerForObjectDetection](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection) (Table Transformer model) - [YolosConfig](/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig) configuration class: [YolosForObjectDetection](/docs/transformers/main/en/model_doc/yolos#transformers.YolosForObjectDetection) (YOLOS model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForObjectDetection.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a object detection head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **conditional_detr** -- [ConditionalDetrForObjectDetection](/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection) (Conditional DETR model)
- **d_fine** -- [DFineForObjectDetection](/docs/transformers/main/en/model_doc/d_fine#transformers.DFineForObjectDetection) (D-FINE model)
- **dab-detr** -- [DabDetrForObjectDetection](/docs/transformers/main/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection) (DAB-DETR model)
- **deformable_detr** -- [DeformableDetrForObjectDetection](/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection) (Deformable DETR model)
- **detr** -- [DetrForObjectDetection](/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection) (DETR model)
- **rt_detr** -- [RTDetrForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrForObjectDetection) (RT-DETR model)
- **rt_detr_v2** -- [RTDetrV2ForObjectDetection](/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection) (RT-DETRv2 model)
- **table-transformer** -- [TableTransformerForObjectDetection](/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection) (Table Transformer model)
- **yolos** -- [YolosForObjectDetection](/docs/transformers/main/en/model_doc/yolos#transformers.YolosForObjectDetection) (YOLOS model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForObjectDetection

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForObjectDetection.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForObjectDetection.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForImageSegmentation[[transformers.AutoModelForImageSegmentation]]

#### transformers.AutoModelForImageSegmentation[[transformers.AutoModelForImageSegmentation]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2070)

This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForImageSegmentation.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a image segmentation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageSegmentation

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForImageSegmentation.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForImageSegmentation.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **detr** -- [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageSegmentation

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForImageSegmentation.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForImageSegmentation.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForImageToImage[[transformers.AutoModelForImageToImage]]

#### transformers.AutoModelForImageToImage[[transformers.AutoModelForImageToImage]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1921)

### AutoModelForSemanticSegmentation[[transformers.AutoModelForSemanticSegmentation]]

#### transformers.AutoModelForSemanticSegmentation[[transformers.AutoModelForSemanticSegmentation]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2077)

This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForSemanticSegmentation.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitForSemanticSegmentation](/docs/transformers/main/en/model_doc/beit#transformers.BeitForSemanticSegmentation) (BEiT model)
  - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTForSemanticSegmentation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForSemanticSegmentation) (DPT model)
  - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionForSemanticSegmentation](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation) (Data2VecVision model)
  - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation) (MobileNetV2 model)
  - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation) (MobileViT model)
  - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation) (MobileViTV2 model)
  - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerForSemanticSegmentation](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation) (SegFormer model)
  - [UperNetConfig](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig) configuration class: [UperNetForSemanticSegmentation](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation) (UPerNet model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSemanticSegmentation

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForSemanticSegmentation.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [BeitConfig](/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig) configuration class: [BeitForSemanticSegmentation](/docs/transformers/main/en/model_doc/beit#transformers.BeitForSemanticSegmentation) (BEiT model) - [DPTConfig](/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig) configuration class: [DPTForSemanticSegmentation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForSemanticSegmentation) (DPT model) - [Data2VecVisionConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig) configuration class: [Data2VecVisionForSemanticSegmentation](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation) (Data2VecVision model) - [MobileNetV2Config](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config) configuration class: [MobileNetV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation) (MobileNetV2 model) - [MobileViTConfig](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig) configuration class: [MobileViTForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation) (MobileViT model) - [MobileViTV2Config](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config) configuration class: [MobileViTV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation) (MobileViTV2 model) - [SegformerConfig](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig) configuration class: [SegformerForSemanticSegmentation](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation) (SegFormer model) - [UperNetConfig](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig) configuration class: [UperNetForSemanticSegmentation](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation) (UPerNet model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForSemanticSegmentation.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **beit** -- [BeitForSemanticSegmentation](/docs/transformers/main/en/model_doc/beit#transformers.BeitForSemanticSegmentation) (BEiT model)
- **data2vec-vision** -- [Data2VecVisionForSemanticSegmentation](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation) (Data2VecVision model)
- **dpt** -- [DPTForSemanticSegmentation](/docs/transformers/main/en/model_doc/dpt#transformers.DPTForSemanticSegmentation) (DPT model)
- **mobilenet_v2** -- [MobileNetV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation) (MobileNetV2 model)
- **mobilevit** -- [MobileViTForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation) (MobileViT model)
- **mobilevitv2** -- [MobileViTV2ForSemanticSegmentation](/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation) (MobileViTV2 model)
- **segformer** -- [SegformerForSemanticSegmentation](/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation) (SegFormer model)
- **upernet** -- [UperNetForSemanticSegmentation](/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation) (UPerNet model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSemanticSegmentation

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForSemanticSegmentation.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForSemanticSegmentation.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForInstanceSegmentation[[transformers.AutoModelForInstanceSegmentation]]

#### transformers.AutoModelForInstanceSegmentation[[transformers.AutoModelForInstanceSegmentation]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2104)

This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForInstanceSegmentation.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForInstanceSegmentation

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForInstanceSegmentation.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForInstanceSegmentation.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **maskformer** -- [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForInstanceSegmentation

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForInstanceSegmentation.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForInstanceSegmentation.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForUniversalSegmentation[[transformers.AutoModelForUniversalSegmentation]]

#### transformers.AutoModelForUniversalSegmentation[[transformers.AutoModelForUniversalSegmentation]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2095)

This is a generic model class that will be instantiated as one of the model classes of the library (with a universal image segmentation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForUniversalSegmentation.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model)
  - [EomtConfig](/docs/transformers/main/en/model_doc/eomt#transformers.EomtConfig) configuration class: [EomtForUniversalSegmentation](/docs/transformers/main/en/model_doc/eomt#transformers.EomtForUniversalSegmentation) (EoMT model)
  - [Mask2FormerConfig](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig) configuration class: [Mask2FormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation) (Mask2Former model)
  - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model)
  - [OneFormerConfig](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig) configuration class: [OneFormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation) (OneFormer model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a universal image segmentation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForUniversalSegmentation

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForUniversalSegmentation.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DetrConfig](/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig) configuration class: [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model) - [EomtConfig](/docs/transformers/main/en/model_doc/eomt#transformers.EomtConfig) configuration class: [EomtForUniversalSegmentation](/docs/transformers/main/en/model_doc/eomt#transformers.EomtForUniversalSegmentation) (EoMT model) - [Mask2FormerConfig](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig) configuration class: [Mask2FormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation) (Mask2Former model) - [MaskFormerConfig](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig) configuration class: [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model) - [OneFormerConfig](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig) configuration class: [OneFormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation) (OneFormer model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForUniversalSegmentation.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a universal image segmentation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **detr** -- [DetrForSegmentation](/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation) (DETR model)
- **eomt** -- [EomtForUniversalSegmentation](/docs/transformers/main/en/model_doc/eomt#transformers.EomtForUniversalSegmentation) (EoMT model)
- **mask2former** -- [Mask2FormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation) (Mask2Former model)
- **maskformer** -- [MaskFormerForInstanceSegmentation](/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) (MaskFormer model)
- **oneformer** -- [OneFormerForUniversalSegmentation](/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation) (OneFormer model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForUniversalSegmentation

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForUniversalSegmentation.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForUniversalSegmentation.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForZeroShotImageClassification[[transformers.AutoModelForZeroShotImageClassification]]

#### transformers.AutoModelForZeroShotImageClassification[[transformers.AutoModelForZeroShotImageClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2061)

This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForZeroShotImageClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AlignConfig](/docs/transformers/main/en/model_doc/align#transformers.AlignConfig) configuration class: [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model)
  - [AltCLIPConfig](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig) configuration class: [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model)
  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForImageTextRetrieval](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval) (BLIP-2 model)
  - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model)
  - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model)
  - [CLIPSegConfig](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig) configuration class: [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model)
  - [ChineseCLIPConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig) configuration class: [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model)
  - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model)
  - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model)
  - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForZeroShotImageClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForZeroShotImageClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AlignConfig](/docs/transformers/main/en/model_doc/align#transformers.AlignConfig) configuration class: [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model) - [AltCLIPConfig](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig) configuration class: [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model) - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForImageTextRetrieval](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval) (BLIP-2 model) - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model) - [CLIPConfig](/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig) configuration class: [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model) - [CLIPSegConfig](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig) configuration class: [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model) - [ChineseCLIPConfig](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig) configuration class: [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model) - [MetaClip2Config](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Config) configuration class: [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model) - [Siglip2Config](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Config) configuration class: [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model) - [SiglipConfig](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig) configuration class: [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForZeroShotImageClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **align** -- [AlignModel](/docs/transformers/main/en/model_doc/align#transformers.AlignModel) (ALIGN model)
- **altclip** -- [AltCLIPModel](/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel) (AltCLIP model)
- **blip** -- [BlipModel](/docs/transformers/main/en/model_doc/blip#transformers.BlipModel) (BLIP model)
- **blip-2** -- [Blip2ForImageTextRetrieval](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval) (BLIP-2 model)
- **chinese_clip** -- [ChineseCLIPModel](/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel) (Chinese-CLIP model)
- **clip** -- [CLIPModel](/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel) (CLIP model)
- **clipseg** -- [CLIPSegModel](/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel) (CLIPSeg model)
- **metaclip_2** -- [MetaClip2Model](/docs/transformers/main/en/model_doc/metaclip_2#transformers.MetaClip2Model) (MetaCLIP 2 model)
- **siglip** -- [SiglipModel](/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel) (SigLIP model)
- **siglip2** -- [Siglip2Model](/docs/transformers/main/en/model_doc/siglip2#transformers.Siglip2Model) (SigLIP2 model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForZeroShotImageClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForZeroShotImageClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForZeroShotImageClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForZeroShotObjectDetection[[transformers.AutoModelForZeroShotObjectDetection]]

#### transformers.AutoModelForZeroShotObjectDetection[[transformers.AutoModelForZeroShotObjectDetection]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2120)

This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot object detection head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForZeroShotObjectDetection.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [GroundingDinoConfig](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoConfig) configuration class: [GroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoForObjectDetection) (Grounding DINO model)
  - [MMGroundingDinoConfig](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig) configuration class: [MMGroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection) (MM Grounding DINO model)
  - [OmDetTurboConfig](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig) configuration class: [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model)
  - [OwlViTConfig](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig) configuration class: [OwlViTForObjectDetection](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection) (OWL-ViT model)
  - [Owlv2Config](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config) configuration class: [Owlv2ForObjectDetection](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection) (OWLv2 model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a zero-shot object detection head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForZeroShotObjectDetection

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForZeroShotObjectDetection.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [GroundingDinoConfig](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoConfig) configuration class: [GroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoForObjectDetection) (Grounding DINO model) - [MMGroundingDinoConfig](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig) configuration class: [MMGroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection) (MM Grounding DINO model) - [OmDetTurboConfig](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig) configuration class: [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model) - [OwlViTConfig](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig) configuration class: [OwlViTForObjectDetection](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection) (OWL-ViT model) - [Owlv2Config](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config) configuration class: [Owlv2ForObjectDetection](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection) (OWLv2 model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForZeroShotObjectDetection.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a zero-shot object detection head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **grounding-dino** -- [GroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/grounding-dino#transformers.GroundingDinoForObjectDetection) (Grounding DINO model)
- **mm-grounding-dino** -- [MMGroundingDinoForObjectDetection](/docs/transformers/main/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection) (MM Grounding DINO model)
- **omdet-turbo** -- [OmDetTurboForObjectDetection](/docs/transformers/main/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection) (OmDet-Turbo model)
- **owlv2** -- [Owlv2ForObjectDetection](/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection) (OWLv2 model)
- **owlvit** -- [OwlViTForObjectDetection](/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection) (OWL-ViT model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForZeroShotObjectDetection

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForZeroShotObjectDetection.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForZeroShotObjectDetection.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

## Audio

The following auto classes are available for the following audio tasks.

### AutoModelForAudioClassification[[transformers.AutoModelForAudioClassification]]

#### transformers.AutoModelForAudioClassification[[transformers.AutoModelForAudioClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2175)

This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForAudioClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [ASTConfig](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig) configuration class: [ASTForAudioClassification](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification) (Audio Spectrogram Transformer model)
  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification) (Data2VecAudio model)
  - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertForSequenceClassification](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForSequenceClassification) (Hubert model)
  - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWForSequenceClassification](/docs/transformers/main/en/model_doc/sew#transformers.SEWForSequenceClassification) (SEW model)
  - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDForSequenceClassification](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForSequenceClassification) (SEW-D model)
  - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification) (UniSpeech model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification) (UniSpeechSat model)
  - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification) (Wav2Vec2-BERT model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification) (Wav2Vec2-Conformer model)
  - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForSequenceClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForSequenceClassification) (WavLM model)
  - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForAudioClassification](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForAudioClassification) (Whisper model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a audio classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForAudioClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [ASTConfig](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig) configuration class: [ASTForAudioClassification](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification) (Audio Spectrogram Transformer model) - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification) (Data2VecAudio model) - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertForSequenceClassification](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForSequenceClassification) (Hubert model) - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWForSequenceClassification](/docs/transformers/main/en/model_doc/sew#transformers.SEWForSequenceClassification) (SEW model) - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDForSequenceClassification](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForSequenceClassification) (SEW-D model) - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification) (UniSpeech model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification) (UniSpeechSat model) - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification) (Wav2Vec2-BERT model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification) (Wav2Vec2-Conformer model) - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForSequenceClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForSequenceClassification) (WavLM model) - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForAudioClassification](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForAudioClassification) (Whisper model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForAudioClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **audio-spectrogram-transformer** -- [ASTForAudioClassification](/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification) (Audio Spectrogram Transformer model)
- **data2vec-audio** -- [Data2VecAudioForSequenceClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification) (Data2VecAudio model)
- **hubert** -- [HubertForSequenceClassification](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForSequenceClassification) (Hubert model)
- **sew** -- [SEWForSequenceClassification](/docs/transformers/main/en/model_doc/sew#transformers.SEWForSequenceClassification) (SEW model)
- **sew-d** -- [SEWDForSequenceClassification](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForSequenceClassification) (SEW-D model)
- **unispeech** -- [UniSpeechForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification) (UniSpeech model)
- **unispeech-sat** -- [UniSpeechSatForSequenceClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification) (UniSpeechSat model)
- **wav2vec2** -- [Wav2Vec2ForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerForSequenceClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMForSequenceClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForSequenceClassification) (WavLM model)
- **whisper** -- [WhisperForAudioClassification](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForAudioClassification) (Whisper model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForAudioClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForAudioClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForAudioFrameClassification[[transformers.AutoModelForAudioFrameClassification]]

#### transformers.AutoModelForAudioFrameClassification[[transformers.AutoModelForAudioFrameClassification]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2198)

This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForAudioFrameClassification.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForAudioFrameClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification) (Data2VecAudio model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForAudioFrameClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification) (UniSpeechSat model)
  - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification) (Wav2Vec2-BERT model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification) (Wav2Vec2-Conformer model)
  - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForAudioFrameClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification) (WavLM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioFrameClassification

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForAudioFrameClassification.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForAudioFrameClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification) (Data2VecAudio model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForAudioFrameClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification) (UniSpeechSat model) - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification) (Wav2Vec2-BERT model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification) (Wav2Vec2-Conformer model) - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForAudioFrameClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification) (WavLM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForAudioFrameClassification.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **data2vec-audio** -- [Data2VecAudioForAudioFrameClassification](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification) (Data2VecAudio model)
- **unispeech-sat** -- [UniSpeechSatForAudioFrameClassification](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification) (UniSpeechSat model)
- **wav2vec2** -- [Wav2Vec2ForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerForAudioFrameClassification](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMForAudioFrameClassification](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification) (WavLM model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioFrameClassification

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForAudioFrameClassification.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForAudioFrameClassification.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForCTC[[transformers.AutoModelForCTC]]

#### transformers.AutoModelForCTC[[transformers.AutoModelForCTC]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2182)

This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForCTC.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForCTC](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForCTC) (Data2VecAudio model)
  - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertForCTC](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForCTC) (Hubert model)
  - [LasrCTCConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrCTCConfig) configuration class: [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model)
  - [ParakeetCTCConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetCTCConfig) configuration class: [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model)
  - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWForCTC](/docs/transformers/main/en/model_doc/sew#transformers.SEWForCTC) (SEW model)
  - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDForCTC](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForCTC) (SEW-D model)
  - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForCTC](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForCTC) (UniSpeech model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForCTC](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC) (UniSpeechSat model)
  - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForCTC](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC) (Wav2Vec2-BERT model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForCTC](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForCTC](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC) (Wav2Vec2-Conformer model)
  - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForCTC](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForCTC) (WavLM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForCTC

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForCTC.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForCTC](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForCTC) (Data2VecAudio model) - [HubertConfig](/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig) configuration class: [HubertForCTC](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForCTC) (Hubert model) - [LasrCTCConfig](/docs/transformers/main/en/model_doc/lasr#transformers.LasrCTCConfig) configuration class: [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model) - [ParakeetCTCConfig](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetCTCConfig) configuration class: [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model) - [SEWConfig](/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig) configuration class: [SEWForCTC](/docs/transformers/main/en/model_doc/sew#transformers.SEWForCTC) (SEW model) - [SEWDConfig](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig) configuration class: [SEWDForCTC](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForCTC) (SEW-D model) - [UniSpeechConfig](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig) configuration class: [UniSpeechForCTC](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForCTC) (UniSpeech model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForCTC](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC) (UniSpeechSat model) - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForCTC](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC) (Wav2Vec2-BERT model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForCTC](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForCTC](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC) (Wav2Vec2-Conformer model) - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForCTC](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForCTC) (WavLM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForCTC.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **data2vec-audio** -- [Data2VecAudioForCTC](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForCTC) (Data2VecAudio model)
- **hubert** -- [HubertForCTC](/docs/transformers/main/en/model_doc/hubert#transformers.HubertForCTC) (Hubert model)
- **lasr_ctc** -- [LasrForCTC](/docs/transformers/main/en/model_doc/lasr#transformers.LasrForCTC) (Lasr model)
- **parakeet_ctc** -- [ParakeetForCTC](/docs/transformers/main/en/model_doc/parakeet#transformers.ParakeetForCTC) (Parakeet model)
- **sew** -- [SEWForCTC](/docs/transformers/main/en/model_doc/sew#transformers.SEWForCTC) (SEW model)
- **sew-d** -- [SEWDForCTC](/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForCTC) (SEW-D model)
- **unispeech** -- [UniSpeechForCTC](/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForCTC) (UniSpeech model)
- **unispeech-sat** -- [UniSpeechSatForCTC](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC) (UniSpeechSat model)
- **wav2vec2** -- [Wav2Vec2ForCTC](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertForCTC](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerForCTC](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMForCTC](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForCTC) (WavLM model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForCTC

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForCTC.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForCTC.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForSpeechSeq2Seq[[transformers.AutoModelForSpeechSeq2Seq]]

#### transformers.AutoModelForSpeechSeq2Seq[[transformers.AutoModelForSpeechSeq2Seq]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2189)

This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForSpeechSeq2Seq.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DiaConfig](/docs/transformers/main/en/model_doc/dia#transformers.DiaConfig) configuration class: [DiaForConditionalGeneration](/docs/transformers/main/en/model_doc/dia#transformers.DiaForConditionalGeneration) (Dia model)
  - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
  - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model)
  - [MoonshineConfig](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineConfig) configuration class: [MoonshineForConditionalGeneration](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineForConditionalGeneration) (Moonshine model)
  - [Pop2PianoConfig](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoConfig) configuration class: [Pop2PianoForConditionalGeneration](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration) (Pop2Piano model)
  - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText) (SeamlessM4T model)
  - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2ForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText) (SeamlessM4Tv2 model)
  - [Speech2TextConfig](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig) configuration class: [Speech2TextForConditionalGeneration](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration) (Speech2Text model)
  - [SpeechEncoderDecoderConfig](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig) configuration class: [SpeechEncoderDecoderModel](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel) (Speech Encoder decoder model)
  - [SpeechT5Config](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config) configuration class: [SpeechT5ForSpeechToText](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText) (SpeechT5 model)
  - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForConditionalGeneration](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForConditionalGeneration) (Whisper model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForSpeechSeq2Seq.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DiaConfig](/docs/transformers/main/en/model_doc/dia#transformers.DiaConfig) configuration class: [DiaForConditionalGeneration](/docs/transformers/main/en/model_doc/dia#transformers.DiaForConditionalGeneration) (Dia model) - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model) - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model) - [MoonshineConfig](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineConfig) configuration class: [MoonshineForConditionalGeneration](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineForConditionalGeneration) (Moonshine model) - [Pop2PianoConfig](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoConfig) configuration class: [Pop2PianoForConditionalGeneration](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration) (Pop2Piano model) - [SeamlessM4TConfig](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig) configuration class: [SeamlessM4TForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText) (SeamlessM4T model) - [SeamlessM4Tv2Config](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config) configuration class: [SeamlessM4Tv2ForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText) (SeamlessM4Tv2 model) - [Speech2TextConfig](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig) configuration class: [Speech2TextForConditionalGeneration](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration) (Speech2Text model) - [SpeechEncoderDecoderConfig](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig) configuration class: [SpeechEncoderDecoderModel](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel) (Speech Encoder decoder model) - [SpeechT5Config](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config) configuration class: [SpeechT5ForSpeechToText](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText) (SpeechT5 model) - [WhisperConfig](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig) configuration class: [WhisperForConditionalGeneration](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForConditionalGeneration) (Whisper model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForSpeechSeq2Seq.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **dia** -- [DiaForConditionalGeneration](/docs/transformers/main/en/model_doc/dia#transformers.DiaForConditionalGeneration) (Dia model)
- **granite_speech** -- [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model)
- **moonshine** -- [MoonshineForConditionalGeneration](/docs/transformers/main/en/model_doc/moonshine#transformers.MoonshineForConditionalGeneration) (Moonshine model)
- **pop2piano** -- [Pop2PianoForConditionalGeneration](/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration) (Pop2Piano model)
- **seamless_m4t** -- [SeamlessM4TForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText) (SeamlessM4T model)
- **seamless_m4t_v2** -- [SeamlessM4Tv2ForSpeechToText](/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText) (SeamlessM4Tv2 model)
- **speech-encoder-decoder** -- [SpeechEncoderDecoderModel](/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel) (Speech Encoder decoder model)
- **speech_to_text** -- [Speech2TextForConditionalGeneration](/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration) (Speech2Text model)
- **speecht5** -- [SpeechT5ForSpeechToText](/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText) (SpeechT5 model)
- **whisper** -- [WhisperForConditionalGeneration](/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForConditionalGeneration) (Whisper model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForSpeechSeq2Seq.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForSpeechSeq2Seq.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForAudioXVector[[transformers.AutoModelForAudioXVector]]

#### transformers.AutoModelForAudioXVector[[transformers.AutoModelForAudioXVector]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2207)

This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForAudioXVector.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForXVector](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForXVector) (Data2VecAudio model)
  - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForXVector](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector) (UniSpeechSat model)
  - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForXVector](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector) (Wav2Vec2-BERT model)
  - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForXVector](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector) (Wav2Vec2 model)
  - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForXVector](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector) (Wav2Vec2-Conformer model)
  - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForXVector](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForXVector) (WavLM model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioXVector

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForAudioXVector.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [Data2VecAudioConfig](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig) configuration class: [Data2VecAudioForXVector](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForXVector) (Data2VecAudio model) - [UniSpeechSatConfig](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig) configuration class: [UniSpeechSatForXVector](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector) (UniSpeechSat model) - [Wav2Vec2BertConfig](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig) configuration class: [Wav2Vec2BertForXVector](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector) (Wav2Vec2-BERT model) - [Wav2Vec2Config](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config) configuration class: [Wav2Vec2ForXVector](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector) (Wav2Vec2 model) - [Wav2Vec2ConformerConfig](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig) configuration class: [Wav2Vec2ConformerForXVector](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector) (Wav2Vec2-Conformer model) - [WavLMConfig](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig) configuration class: [WavLMForXVector](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForXVector) (WavLM model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForAudioXVector.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **data2vec-audio** -- [Data2VecAudioForXVector](/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForXVector) (Data2VecAudio model)
- **unispeech-sat** -- [UniSpeechSatForXVector](/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector) (UniSpeechSat model)
- **wav2vec2** -- [Wav2Vec2ForXVector](/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector) (Wav2Vec2 model)
- **wav2vec2-bert** -- [Wav2Vec2BertForXVector](/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector) (Wav2Vec2-BERT model)
- **wav2vec2-conformer** -- [Wav2Vec2ConformerForXVector](/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector) (Wav2Vec2-Conformer model)
- **wavlm** -- [WavLMForXVector](/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForXVector) (WavLM model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioXVector

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForAudioXVector.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForAudioXVector.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForTextToSpectrogram[[transformers.AutoModelForTextToSpectrogram]]

#### transformers.AutoModelForTextToSpectrogram[[transformers.AutoModelForTextToSpectrogram]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2211)

### AutoModelForTextToWaveform[[transformers.AutoModelForTextToWaveform]]

#### transformers.AutoModelForTextToWaveform[[transformers.AutoModelForTextToWaveform]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2215)

### AutoModelForAudioTokenization[[transformers.AutoModelForAudioTokenization]]

#### transformers.AutoModelForAudioTokenization[[transformers.AutoModelForAudioTokenization]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2233)

This is a generic model class that will be instantiated as one of the model classes of the library (with a audio tokenization through codebooks head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForAudioTokenization.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [DacConfig](/docs/transformers/main/en/model_doc/dac#transformers.DacConfig) configuration class: [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a audio tokenization through codebooks head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioTokenization

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForAudioTokenization.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [DacConfig](/docs/transformers/main/en/model_doc/dac#transformers.DacConfig) configuration class: [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForAudioTokenization.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a audio tokenization through codebooks head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **dac** -- [DacModel](/docs/transformers/main/en/model_doc/dac#transformers.DacModel) (DAC model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForAudioTokenization

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForAudioTokenization.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForAudioTokenization.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

## Multimodal

The following auto classes are available for the following multimodal tasks.

### AutoModelForMultimodalLM[[transformers.AutoModelForMultimodalLM]]

#### transformers.AutoModelForMultimodalLM[[transformers.AutoModelForMultimodalLM]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2168)

This is a generic model class that will be instantiated as one of the model classes of the library (with a multimodal generation head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForMultimodalLM.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model)
  - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model)
  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
  - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model)
  - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model)
  - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model)
  - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model)
  - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model)
  - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model)
  - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
  - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model)
  - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
  - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
  - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
  - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
  - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model)
  - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model)
  - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model)
  - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
  - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
  - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
  - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
  - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
  - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model)
  - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model)
  - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
  - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model)
  - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model)
  - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model)
  - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model)
  - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
  - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
  - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
  - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
  - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
  - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
  - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
  - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model)
  - [PaddleOCRVLConfig](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLConfig) configuration class: [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model)
  - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
  - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model)
  - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model)
  - [Pix2StructConfig](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig) configuration class: [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model)
  - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model)
  - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
  - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model)
  - [Qwen2_5OmniConfig](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig) configuration class: [Qwen2_5OmniForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration) (Qwen2_5Omni model)
  - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model)
  - [Qwen3OmniMoeConfig](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeConfig) configuration class: [Qwen3OmniMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeForConditionalGeneration) (Qwen3OmniMoE model)
  - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model)
  - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model)
  - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model)
  - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
  - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model)
  - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model)
  - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
  - [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) configuration class: [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model)
  - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a multimodal generation head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMultimodalLM

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForMultimodalLM.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model) - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model) - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model) - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model) - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model) - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model) - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model) - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model) - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model) - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model) - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model) - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model) - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model) - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model) - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model) - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model) - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model) - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model) - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model) - [GraniteSpeechConfig](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechConfig) configuration class: [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model) - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model) - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model) - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model) - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model) - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model) - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model) - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model) - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model) - [KyutaiSpeechToTextConfig](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig) configuration class: [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model) - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model) - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model) - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model) - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model) - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model) - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model) - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model) - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model) - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model) - [PaddleOCRVLConfig](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLConfig) configuration class: [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model) - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model) - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model) - [Phi4MultimodalConfig](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig) configuration class: [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model) - [Pix2StructConfig](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig) configuration class: [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model) - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model) - [Qwen2AudioConfig](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig) configuration class: [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model) - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model) - [Qwen2_5OmniConfig](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig) configuration class: [Qwen2_5OmniForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration) (Qwen2_5Omni model) - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model) - [Qwen3OmniMoeConfig](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeConfig) configuration class: [Qwen3OmniMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeForConditionalGeneration) (Qwen3OmniMoE model) - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model) - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model) - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model) - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model) - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model) - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model) - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model) - [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) configuration class: [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model) - [VoxtralConfig](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralConfig) configuration class: [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForMultimodalLM.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a multimodal generation head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **aria** -- [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model)
- **aya_vision** -- [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model)
- **blip** -- [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model)
- **blip-2** -- [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
- **chameleon** -- [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model)
- **cohere2_vision** -- [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model)
- **deepseek_vl** -- [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model)
- **emu3** -- [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model)
- **evolla** -- [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
- **fast_vlm** -- [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model)
- **florence2** -- [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
- **fuyu** -- [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
- **gemma3** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
- **gemma3n** -- [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
- **git** -- [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
- **glm46v** -- [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model)
- **glm4v** -- [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model)
- **glm4v_moe** -- [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model)
- **got_ocr2** -- [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
- **granite_speech** -- [GraniteSpeechForConditionalGeneration](/docs/transformers/main/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration) (GraniteSpeech model)
- **idefics** -- [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
- **idefics2** -- [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
- **idefics3** -- [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
- **instructblip** -- [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model)
- **internvl** -- [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model)
- **janus** -- [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
- **kosmos-2** -- [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model)
- **kyutai_speech_to_text** -- [KyutaiSpeechToTextForConditionalGeneration](/docs/transformers/main/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration) (KyutaiSpeechToText model)
- **lfm2_vl** -- [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model)
- **llama4** -- [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
- **llava** -- [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
- **llava_next** -- [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
- **mistral3** -- [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
- **mllama** -- [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
- **ovis2** -- [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model)
- **paddleocr_vl** -- [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model)
- **paligemma** -- [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
- **perception_lm** -- [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model)
- **phi4_multimodal** -- [Phi4MultimodalForCausalLM](/docs/transformers/main/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM) (Phi4Multimodal model)
- **pix2struct** -- [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model)
- **pixtral** -- [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model)
- **qwen2_5_omni** -- [Qwen2_5OmniForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration) (Qwen2_5Omni model)
- **qwen2_5_vl** -- [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model)
- **qwen2_audio** -- [Qwen2AudioForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration) (Qwen2Audio model)
- **qwen2_vl** -- [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model)
- **qwen3_omni_moe** -- [Qwen3OmniMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_omni_moe#transformers.Qwen3OmniMoeForConditionalGeneration) (Qwen3OmniMoE model)
- **qwen3_vl** -- [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model)
- **shieldgemma2** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model)
- **smolvlm** -- [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model)
- **t5gemma2** -- [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
- **udop** -- [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model)
- **video_llama_3** -- [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model)
- **vipllava** -- [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
- **vision-encoder-decoder** -- [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model)
- **voxtral** -- [VoxtralForConditionalGeneration](/docs/transformers/main/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration) (Voxtral model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForMultimodalLM

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForMultimodalLM.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForMultimodalLM.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForTableQuestionAnswering[[transformers.AutoModelForTableQuestionAnswering]]

#### transformers.AutoModelForTableQuestionAnswering[[transformers.AutoModelForTableQuestionAnswering]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1998)

This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForTableQuestionAnswering.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForQuestionAnswering](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForQuestionAnswering) (TAPAS model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a table question answering head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTableQuestionAnswering

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
>>> model = AutoModelForTableQuestionAnswering.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [TapasConfig](/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig) configuration class: [TapasForQuestionAnswering](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForQuestionAnswering) (TAPAS model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForTableQuestionAnswering.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **tapas** -- [TapasForQuestionAnswering](/docs/transformers/main/en/model_doc/tapas#transformers.TapasForQuestionAnswering) (TAPAS model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTableQuestionAnswering

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

>>> # Update configuration during loading
>>> model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForDocumentQuestionAnswering[[transformers.AutoModelForDocumentQuestionAnswering]]

#### transformers.AutoModelForDocumentQuestionAnswering[[transformers.AutoModelForDocumentQuestionAnswering]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2020)

This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForDocumentQuestionAnswering.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering) (LayoutLM model)
  - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model)
  - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a document question answering head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForDocumentQuestionAnswering

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("impira/layoutlm-document-qa", revision="52e01b3")
>>> model = AutoModelForDocumentQuestionAnswering.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [LayoutLMConfig](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig) configuration class: [LayoutLMForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering) (LayoutLM model) - [LayoutLMv2Config](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config) configuration class: [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model) - [LayoutLMv3Config](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config) configuration class: [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForDocumentQuestionAnswering.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **layoutlm** -- [LayoutLMForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering) (LayoutLM model)
- **layoutlmv2** -- [LayoutLMv2ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering) (LayoutLMv2 model)
- **layoutlmv3** -- [LayoutLMv3ForQuestionAnswering](/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering) (LayoutLMv3 model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForDocumentQuestionAnswering

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("impira/layoutlm-document-qa", revision="52e01b3")

>>> # Update configuration during loading
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("impira/layoutlm-document-qa", revision="52e01b3", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForVisualQuestionAnswering[[transformers.AutoModelForVisualQuestionAnswering]]

#### transformers.AutoModelForVisualQuestionAnswering[[transformers.AutoModelForVisualQuestionAnswering]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2009)

This is a generic model class that will be instantiated as one of the model classes of the library (with a visual question answering head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForVisualQuestionAnswering.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
  - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForQuestionAnswering](/docs/transformers/main/en/model_doc/blip#transformers.BlipForQuestionAnswering) (BLIP model)
  - [ViltConfig](/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig) configuration class: [ViltForQuestionAnswering](/docs/transformers/main/en/model_doc/vilt#transformers.ViltForQuestionAnswering) (ViLT model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a visual question answering head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForVisualQuestionAnswering

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
>>> model = AutoModelForVisualQuestionAnswering.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model) - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForQuestionAnswering](/docs/transformers/main/en/model_doc/blip#transformers.BlipForQuestionAnswering) (BLIP model) - [ViltConfig](/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig) configuration class: [ViltForQuestionAnswering](/docs/transformers/main/en/model_doc/vilt#transformers.ViltForQuestionAnswering) (ViLT model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForVisualQuestionAnswering.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a visual question answering head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **blip** -- [BlipForQuestionAnswering](/docs/transformers/main/en/model_doc/blip#transformers.BlipForQuestionAnswering) (BLIP model)
- **blip-2** -- [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
- **vilt** -- [ViltForQuestionAnswering](/docs/transformers/main/en/model_doc/vilt#transformers.ViltForQuestionAnswering) (ViLT model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForVisualQuestionAnswering

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForVisualQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

>>> # Update configuration during loading
>>> model = AutoModelForVisualQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

### AutoModelForVision2Seq[[transformers.AutoModelForVision2Seq]]

#### transformers.AutoModelForVision2Seq[[transformers.AutoModelForVision2Seq]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2264)

### AutoModelForImageTextToText[[transformers.AutoModelForImageTextToText]]

#### transformers.AutoModelForImageTextToText[[transformers.AutoModelForImageTextToText]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2151)

This is a generic model class that will be instantiated as one of the model classes of the library (with a image-text-to-text modeling head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForImageTextToText.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model)
  - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model)
  - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
  - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model)
  - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model)
  - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model)
  - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model)
  - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model)
  - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model)
  - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
  - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model)
  - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
  - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
  - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
  - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
  - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
  - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model)
  - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model)
  - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model)
  - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
  - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
  - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
  - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
  - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model)
  - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model)
  - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
  - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model)
  - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model)
  - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model)
  - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
  - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
  - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
  - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
  - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
  - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
  - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
  - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model)
  - [PaddleOCRVLConfig](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLConfig) configuration class: [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model)
  - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
  - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model)
  - [Pix2StructConfig](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig) configuration class: [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model)
  - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model)
  - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model)
  - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model)
  - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model)
  - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model)
  - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model)
  - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model)
  - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
  - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model)
  - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model)
  - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
  - [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) configuration class: [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a image-text-to-text modeling head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageTextToText

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForImageTextToText.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [AriaConfig](/docs/transformers/main/en/model_doc/aria#transformers.AriaConfig) configuration class: [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model) - [AyaVisionConfig](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionConfig) configuration class: [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model) - [Blip2Config](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config) configuration class: [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model) - [BlipConfig](/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig) configuration class: [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model) - [ChameleonConfig](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonConfig) configuration class: [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model) - [Cohere2VisionConfig](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig) configuration class: [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model) - [DeepseekVLConfig](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig) configuration class: [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model) - [DeepseekVLHybridConfig](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig) configuration class: [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model) - [Emu3Config](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3Config) configuration class: [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model) - [EvollaConfig](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaConfig) configuration class: [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model) - [FastVlmConfig](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmConfig) configuration class: [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model) - [Florence2Config](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2Config) configuration class: [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model) - [FuyuConfig](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig) configuration class: [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model) - [Gemma3Config](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model) - [Gemma3nConfig](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nConfig) configuration class: [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model) - [GitConfig](/docs/transformers/main/en/model_doc/git#transformers.GitConfig) configuration class: [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model) - [Glm46VConfig](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VConfig) configuration class: [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model) - [Glm4vConfig](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vConfig) configuration class: [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model) - [Glm4vMoeConfig](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig) configuration class: [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model) - [GotOcr2Config](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2Config) configuration class: [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model) - [Idefics2Config](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2Config) configuration class: [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model) - [Idefics3Config](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3Config) configuration class: [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model) - [IdeficsConfig](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig) configuration class: [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model) - [InstructBlipConfig](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig) configuration class: [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model) - [InternVLConfig](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLConfig) configuration class: [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model) - [JanusConfig](/docs/transformers/main/en/model_doc/janus#transformers.JanusConfig) configuration class: [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model) - [Kosmos2Config](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config) configuration class: [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model) - [Kosmos2_5Config](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config) configuration class: [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model) - [Lfm2VlConfig](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlConfig) configuration class: [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model) - [Llama4Config](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4Config) configuration class: [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model) - [LlavaConfig](/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model) - [LlavaNextConfig](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextConfig) configuration class: [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model) - [LlavaNextVideoConfig](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig) configuration class: [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model) - [LlavaOnevisionConfig](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig) configuration class: [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model) - [Mistral3Config](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3Config) configuration class: [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model) - [MllamaConfig](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaConfig) configuration class: [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model) - [Ovis2Config](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2Config) configuration class: [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model) - [PaddleOCRVLConfig](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLConfig) configuration class: [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model) - [PaliGemmaConfig](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaConfig) configuration class: [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model) - [PerceptionLMConfig](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMConfig) configuration class: [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model) - [Pix2StructConfig](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig) configuration class: [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model) - [PixtralVisionConfig](/docs/transformers/main/en/model_doc/pixtral#transformers.PixtralVisionConfig) configuration class: [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model) - [Qwen2VLConfig](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig) configuration class: [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model) - [Qwen2_5_VLConfig](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig) configuration class: [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model) - [Qwen3VLConfig](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLConfig) configuration class: [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model) - [Qwen3VLMoeConfig](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeConfig) configuration class: [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model) - [ShieldGemma2Config](/docs/transformers/main/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config) configuration class: [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model) - [SmolVLMConfig](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMConfig) configuration class: [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model) - [T5Gemma2Config](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2Config) configuration class: [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model) - [UdopConfig](/docs/transformers/main/en/model_doc/udop#transformers.UdopConfig) configuration class: [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model) - [VideoLlama3Config](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3Config) configuration class: [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model) - [VipLlavaConfig](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig) configuration class: [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model) - [VisionEncoderDecoderConfig](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) configuration class: [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForImageTextToText.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a image-text-to-text modeling head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **aria** -- [AriaForConditionalGeneration](/docs/transformers/main/en/model_doc/aria#transformers.AriaForConditionalGeneration) (Aria model)
- **aya_vision** -- [AyaVisionForConditionalGeneration](/docs/transformers/main/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration) (AyaVision model)
- **blip** -- [BlipForConditionalGeneration](/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration) (BLIP model)
- **blip-2** -- [Blip2ForConditionalGeneration](/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration) (BLIP-2 model)
- **chameleon** -- [ChameleonForConditionalGeneration](/docs/transformers/main/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration) (Chameleon model)
- **cohere2_vision** -- [Cohere2VisionForConditionalGeneration](/docs/transformers/main/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration) (Cohere2Vision model)
- **deepseek_vl** -- [DeepseekVLForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration) (DeepseekVL model)
- **deepseek_vl_hybrid** -- [DeepseekVLHybridForConditionalGeneration](/docs/transformers/main/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration) (DeepseekVLHybrid model)
- **emu3** -- [Emu3ForConditionalGeneration](/docs/transformers/main/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration) (Emu3 model)
- **evolla** -- [EvollaForProteinText2Text](/docs/transformers/main/en/model_doc/evolla#transformers.EvollaForProteinText2Text) (Evolla model)
- **fast_vlm** -- [FastVlmForConditionalGeneration](/docs/transformers/main/en/model_doc/fast_vlm#transformers.FastVlmForConditionalGeneration) (FastVlm model)
- **florence2** -- [Florence2ForConditionalGeneration](/docs/transformers/main/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration) (Florence2 model)
- **fuyu** -- [FuyuForCausalLM](/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM) (Fuyu model)
- **gemma3** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Gemma3ForConditionalGeneration model)
- **gemma3n** -- [Gemma3nForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration) (Gemma3nForConditionalGeneration model)
- **git** -- [GitForCausalLM](/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM) (GIT model)
- **glm46v** -- [Glm46VForConditionalGeneration](/docs/transformers/main/en/model_doc/glm46v#transformers.Glm46VForConditionalGeneration) (Glm46V model)
- **glm4v** -- [Glm4vForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration) (GLM4V model)
- **glm4v_moe** -- [Glm4vMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration) (GLM4VMOE model)
- **got_ocr2** -- [GotOcr2ForConditionalGeneration](/docs/transformers/main/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration) (GOT-OCR2 model)
- **idefics** -- [IdeficsForVisionText2Text](/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text) (IDEFICS model)
- **idefics2** -- [Idefics2ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration) (Idefics2 model)
- **idefics3** -- [Idefics3ForConditionalGeneration](/docs/transformers/main/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration) (Idefics3 model)
- **instructblip** -- [InstructBlipForConditionalGeneration](/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration) (InstructBLIP model)
- **internvl** -- [InternVLForConditionalGeneration](/docs/transformers/main/en/model_doc/internvl#transformers.InternVLForConditionalGeneration) (InternVL model)
- **janus** -- [JanusForConditionalGeneration](/docs/transformers/main/en/model_doc/janus#transformers.JanusForConditionalGeneration) (Janus model)
- **kosmos-2** -- [Kosmos2ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration) (KOSMOS-2 model)
- **kosmos-2.5** -- [Kosmos2_5ForConditionalGeneration](/docs/transformers/main/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration) (KOSMOS-2.5 model)
- **lfm2_vl** -- [Lfm2VlForConditionalGeneration](/docs/transformers/main/en/model_doc/lfm2_vl#transformers.Lfm2VlForConditionalGeneration) (Lfm2Vl model)
- **llama4** -- [Llama4ForConditionalGeneration](/docs/transformers/main/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration) (Llama4 model)
- **llava** -- [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (LLaVa model)
- **llava_next** -- [LlavaNextForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next#transformers.LlavaNextForConditionalGeneration) (LLaVA-NeXT model)
- **llava_next_video** -- [LlavaNextVideoForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration) (LLaVa-NeXT-Video model)
- **llava_onevision** -- [LlavaOnevisionForConditionalGeneration](/docs/transformers/main/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration) (LLaVA-Onevision model)
- **mistral3** -- [Mistral3ForConditionalGeneration](/docs/transformers/main/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration) (Mistral3 model)
- **mllama** -- [MllamaForConditionalGeneration](/docs/transformers/main/en/model_doc/mllama#transformers.MllamaForConditionalGeneration) (Mllama model)
- **ovis2** -- [Ovis2ForConditionalGeneration](/docs/transformers/main/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration) (Ovis2 model)
- **paddleocr_vl** -- [PaddleOCRVLForConditionalGeneration](/docs/transformers/main/en/model_doc/paddleocr_vl#transformers.PaddleOCRVLForConditionalGeneration) (PaddleOCRVL model)
- **paligemma** -- [PaliGemmaForConditionalGeneration](/docs/transformers/main/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration) (PaliGemma model)
- **perception_lm** -- [PerceptionLMForConditionalGeneration](/docs/transformers/main/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration) (PerceptionLM model)
- **pix2struct** -- [Pix2StructForConditionalGeneration](/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration) (Pix2Struct model)
- **pixtral** -- [LlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration) (Pixtral model)
- **qwen2_5_vl** -- [Qwen2_5_VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration) (Qwen2_5_VL model)
- **qwen2_vl** -- [Qwen2VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration) (Qwen2VL model)
- **qwen3_vl** -- [Qwen3VLForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl#transformers.Qwen3VLForConditionalGeneration) (Qwen3VL model)
- **qwen3_vl_moe** -- [Qwen3VLMoeForConditionalGeneration](/docs/transformers/main/en/model_doc/qwen3_vl_moe#transformers.Qwen3VLMoeForConditionalGeneration) (Qwen3VLMoe model)
- **shieldgemma2** -- [Gemma3ForConditionalGeneration](/docs/transformers/main/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration) (Shieldgemma2 model)
- **smolvlm** -- [SmolVLMForConditionalGeneration](/docs/transformers/main/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration) (SmolVLM model)
- **t5gemma2** -- [T5Gemma2ForConditionalGeneration](/docs/transformers/main/en/model_doc/t5gemma2#transformers.T5Gemma2ForConditionalGeneration) (T5Gemma2 model)
- **udop** -- [UdopForConditionalGeneration](/docs/transformers/main/en/model_doc/udop#transformers.UdopForConditionalGeneration) (UDOP model)
- **video_llama_3** -- [VideoLlama3ForConditionalGeneration](/docs/transformers/main/en/model_doc/video_llama_3#transformers.VideoLlama3ForConditionalGeneration) (VideoLlama3 model)
- **vipllava** -- [VipLlavaForConditionalGeneration](/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration) (VipLlava model)
- **vision-encoder-decoder** -- [VisionEncoderDecoderModel](/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) (Vision Encoder decoder model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForImageTextToText

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForImageTextToText.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForImageTextToText.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.

## Time Series

### AutoModelForTimeSeriesPrediction[[transformers.AutoModelForTimeSeriesPrediction]]

#### transformers.AutoModelForTimeSeriesPrediction[[transformers.AutoModelForTimeSeriesPrediction]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L2086)

This is a generic model class that will be instantiated as one of the model classes of the library (with a time-series prediction head) when created
with the [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) class method or the [from_config()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_config) class
method.

This class cannot be instantiated directly using `__init__()` (throws an error).

from_configtransformers.AutoModelForTimeSeriesPrediction.from_confighttps://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L205[{"name": "**kwargs", "val": ""}]- **config** ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) --
  The model class to instantiate is selected based on the configuration class:

  - [TimesFmConfig](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmConfig) configuration class: [TimesFmModelForPrediction](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModelForPrediction) (TimesFm model)
- **attn_implementation** (`str`, *optional*) --
  The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.0

Instantiates one of the model classes of the library (with a time-series prediction head) from a configuration.

Note:
Loading a model from its configuration file does **not** load the model weights. It only affects the
model's configuration. Use [from_pretrained()](/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) to load the model weights.

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTimeSeriesPrediction

>>> # Download configuration from huggingface.co and cache.
>>> config = AutoConfig.from_pretrained("google-bert/bert-base-cased")
>>> model = AutoModelForTimeSeriesPrediction.from_config(config)
```

**Parameters:**

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig)) : The model class to instantiate is selected based on the configuration class:  - [TimesFmConfig](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmConfig) configuration class: [TimesFmModelForPrediction](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModelForPrediction) (TimesFm model)

attn_implementation (`str`, *optional*) : The attention implementation to use in the model (if relevant). Can be any of `"eager"` (manual implementation of the attention), `"sdpa"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `"flash_attention_2"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `"eager"` implementation.
#### from_pretrained[[transformers.AutoModelForTimeSeriesPrediction.from_pretrained]]

[Source](https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L249)

Instantiate one of the model classes of the library (with a time-series prediction head) from a pretrained model.

The model class to instantiate is selected based on the `model_type` property of the config object (either
passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
falling back to using pattern matching on `pretrained_model_name_or_path`:

- **timesfm** -- [TimesFmModelForPrediction](/docs/transformers/main/en/model_doc/timesfm#transformers.TimesFmModelForPrediction) (TimesFm model)

The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `model.train()`

Examples:

```python
>>> from transformers import AutoConfig, AutoModelForTimeSeriesPrediction

>>> # Download model and configuration from huggingface.co and cache.
>>> model = AutoModelForTimeSeriesPrediction.from_pretrained("google-bert/bert-base-cased")

>>> # Update configuration during loading
>>> model = AutoModelForTimeSeriesPrediction.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
>>> model.config.output_attentions
True
```

**Parameters:**

pretrained_model_name_or_path (`str` or `os.PathLike`) : Can be either:  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co. - A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained), e.g., `./my_model_directory/`.

model_args (additional positional arguments, *optional*) : Will be passed along to the underlying model `__init__()` method.

config ([PreTrainedConfig](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig), *optional*) : Configuration for the model to use instead of an automatically loaded configuration. Configuration can be automatically loaded when:  - The model is a model provided by the library (loaded with the *model id* string of a pretrained model). - The model was saved using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and is reloaded by supplying the save directory. - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named *config.json* is found in the directory.

state_dict (*dict[str, torch.Tensor]*, *optional*) : A state dictionary to use instead of a state dictionary loaded from saved weights file.  This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) and [from_pretrained()](/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) is not a simpler option.

cache_dir (`str` or `os.PathLike`, *optional*) : Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.

force_download (`bool`, *optional*, defaults to `False`) : Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.

proxies (`dict[str, str]`, *optional*) : A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.

output_loading_info(`bool`, *optional*, defaults to `False`) : Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.

local_files_only(`bool`, *optional*, defaults to `False`) : Whether or not to only look at local files (e.g., not try downloading the model).

revision (`str`, *optional*, defaults to `"main"`) : The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

trust_remote_code (`bool`, *optional*, defaults to `False`) : Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.

code_revision (`str`, *optional*, defaults to `"main"`) : The specific revision to use for the code on the Hub, if the code leaves in a different repository than the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

kwargs (additional keyword arguments, *optional*) : Can be used to update the configuration object (after it being loaded) and initiate the model (e.g., `output_attentions=True`). Behaves differently depending on whether a `config` is provided or automatically loaded:  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the underlying model's `__init__` method (we assume all relevant updates to the configuration have already been done) - If a configuration is not provided, `kwargs` will be first passed to the configuration class initialization function ([from_pretrained()](/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.from_pretrained)). Each key of `kwargs` that corresponds to a configuration attribute will be used to override said attribute with the supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's `__init__` function.
