import{s as os,o as ss,n as Co}from"../chunks/scheduler.18a86fab.js";import{S as ts,i as rs,g as a,s as r,r as p,A as ns,h as i,f as s,c as n,j as M,u as h,x as v,k as $,y as t,a as d,v as f,d as u,t as g,w as _}from"../chunks/index.98837b22.js";import{T as es}from"../chunks/Tip.77304350.js";import{D as j}from"../chunks/Docstring.a1ef7999.js";import{C as Ue}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as as}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as no,E as is}from"../chunks/getInferenceSnippets.06c2775f.js";function ds(C){let c,V='<li><p>Specifying <code>num_frames</code> does not guarantee the output will contain exactly that number of frames. Depending on the model, the sampler may enforce minimum or maximum frame limits.</p></li> <li><p>The default decoder is <a href="https://pypi.org/project/torchcodec/" rel="nofollow"><code>torchcodec</code></a>, which must be installed.</p></li>';return{c(){c=a("ul"),c.innerHTML=V},l(b){c=i(b,"UL",{"data-svelte-h":!0}),v(c)!=="svelte-1iiliob"&&(c.innerHTML=V)},m(b,w){d(b,c,w)},p:Co,d(b){b&&s(c)}}}function cs(C){let c,V="Examples:",b,w,x;return w=new Ue({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqVmlkZW9Qcm9jZXNzb3JCYXNlKiUyMHNvJTIwbGV0J3MlMjBzaG93JTIwdGhlJTIwZXhhbXBsZXMlMjBvbiUyMGElMEElMjMlMjBkZXJpdmVkJTIwY2xhc3MlM0ElMjAqTGxhdmFPbmV2aXNpb25WaWRlb1Byb2Nlc3NvciolMEF2aWRlb19wcm9jZXNzb3IlMjAlM0QlMjBMbGF2YU9uZXZpc2lvblZpZGVvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJsbGF2YS1oZiUyRmxsYXZhLW9uZXZpc2lvbi1xd2VuMi0wLjViLW92LWhmJTIyJTBBKSUyMCUyMCUyMyUyMERvd25sb2FkJTIwdmlkZW9fcHJvY2Vzc2luZ19jb25maWclMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEF2aWRlb19wcm9jZXNzb3IlMjAlM0QlMjBMbGF2YU9uZXZpc2lvblZpZGVvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJTIyJTBBKSUyMCUyMCUyMyUyMEUuZy4lMjB2aWRlbyUyMHByb2Nlc3NvciUyMChvciUyMG1vZGVsKSUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqJTBBdmlkZW9fcHJvY2Vzc29yJTIwJTNEJTIwTGxhdmFPbmV2aXNpb25WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnZpZGVvX3ByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEF2aWRlb19wcm9jZXNzb3IlMjAlM0QlMjBMbGF2YU9uZXZpc2lvblZpZGVvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJsbGF2YS1oZiUyRmxsYXZhLW9uZXZpc2lvbi1xd2VuMi0wLjViLW92LWhmJTIyJTJDJTIwZG9fbm9ybWFsaXplJTNERmFsc2UlMkMlMjBmb28lM0RGYWxzZSUwQSklMEFhc3NlcnQlMjB2aWRlb19wcm9jZXNzb3IuZG9fbm9ybWFsaXplJTIwaXMlMjBGYWxzZSUwQXZpZGVvX3Byb2Nlc3NvciUyQyUyMHVudXNlZF9rd2FyZ3MlMjAlM0QlMjBMbGF2YU9uZXZpc2lvblZpZGVvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJsbGF2YS1oZiUyRmxsYXZhLW9uZXZpc2lvbi1xd2VuMi0wLjViLW92LWhmJTIyJTJDJTIwZG9fbm9ybWFsaXplJTNERmFsc2UlMkMlMjBmb28lM0RGYWxzZSUyQyUyMHJldHVybl91bnVzZWRfa3dhcmdzJTNEVHJ1ZSUwQSklMEFhc3NlcnQlMjB2aWRlb19wcm9jZXNzb3IuZG9fbm9ybWFsaXplJTIwaXMlMjBGYWxzZSUwQWFzc2VydCUyMHVudXNlZF9rd2FyZ3MlMjAlM0QlM0QlMjAlN0IlMjJmb28lMjIlM0ElMjBGYWxzZSU3RA==",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *VideoProcessorBase* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *LlavaOnevisionVideoProcessor*</span>
video_processor = LlavaOnevisionVideoProcessor.from_pretrained(
    <span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>
)  <span class="hljs-comment"># Download video_processing_config from huggingface.co and cache.</span>
video_processor = LlavaOnevisionVideoProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. video processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
video_processor = LlavaOnevisionVideoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/video_preprocessor_config.json&quot;</span>)
video_processor = LlavaOnevisionVideoProcessor.from_pretrained(
    <span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> video_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
video_processor, unused_kwargs = LlavaOnevisionVideoProcessor.from_pretrained(
    <span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> video_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){c=a("p"),c.textContent=V,b=r(),p(w.$$.fragment)},l(y){c=i(y,"P",{"data-svelte-h":!0}),v(c)!=="svelte-kvfsh7"&&(c.textContent=V),b=n(y),h(w.$$.fragment,y)},m(y,T){d(y,c,T),d(y,b,T),f(w,y,T),x=!0},p:Co,i(y){x||(u(w.$$.fragment,y),x=!0)},o(y){g(w.$$.fragment,y),x=!1},d(y){y&&(s(c),s(b)),_(w,y)}}}function ls(C){let c,V="This API is experimental and may have some slight breaking changes in the next releases.";return{c(){c=a("p"),c.textContent=V},l(b){c=i(b,"P",{"data-svelte-h":!0}),v(c)!=="svelte-15rpg4"&&(c.textContent=V)},m(b,w){d(b,c,w)},p:Co,d(b){b&&s(c)}}}function ms(C){let c,V,b,w,x,y,T,Uo='A <strong>Video Processor</strong> is a utility responsible for preparing input features for video models, as well as handling the post-processing of their outputs. It provides transformations such as resizing, normalization, and conversion into PyTorch. Along ith transformations the <code>VideoProcessor</code> class handles video decoding from local paths or URLs (requires <a href="https://pypi.org/project/torchcodec/" rel="nofollow"><code>torchcodec</code></a>) and frame sampling according to model-specific strategies.',Je,H,Jo="The video processor extends the functionality of image processors by allowing Vision Large Language Models (VLMs) to handle videos with a distinct set of arguments compared to images. It serves as the bridge between raw video data and the model, ensuring that input features are optimized for the VLM.",We,X,Wo="When adding a new VLM or updating an existing one to enable distinct video preprocessing, saving and reloading the processor configuration will store the video related arguments in a dedicated file named <code>video_preprocessing_config.json</code>. Don’t worry if you haven’t updated your VLM, the processor will try to load video related configurations from a file named <code>preprocessing_config.json</code>.",ke,E,Ie,F,ko='Here’s an example of how to load a video processor with <a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" rel="nofollow"><code>llava-hf/llava-onevision-qwen2-0.5b-ov-hf</code></a> model:',qe,Y,Le,S,Io='Currently, if using base image processor for videos, it processes video data by treating each frame as an individual image and applying transformations frame-by-frame. While functional, this approach is not highly efficient. Using <code>AutoVideoProcessor</code> allows us to take advantage of <strong>fast video processors</strong>, leveraging the <a href="https://pytorch.org/vision/stable/index.html" rel="nofollow">torchvision</a> library. Fast processors handle the whole batch of videos at once, without iterating over each video or frame. These updates introduce GPU acceleration and significantly enhance processing speed, especially for tasks requiring high throughput.',Re,D,qo="Fast video processors are available for all models and are loaded by default when an <code>AutoVideoProcessor</code> is initialized. When using a fast video processor, you can also set the <code>device</code> argument to specify the device on which the processing should be done. By default, the processing is done on the same device as the inputs if the inputs are tensors, or on the CPU otherwise. For even more speed improvement, we can compile the processor when using ‘cuda’ as device.",Ge,A,Ne,Q,He,O,Lo="The video processor can also sample video frames using the technique best suited for the given model. Sampling behavior is controlled with the <code>do_sample_frames</code> argument and can be configured through model-specific parameters such as <code>num_frames</code> or <code>fps</code> (the rate at which the video will be sampled). If the input video is given as a local path or URL (<code>str</code>), the processor will decode it automatically. To obtain metadata about the decoded video, such as sampled frame indices, original dimensions, duration, and fps, pass <code>return_metadata=True</code> to the processor.",Xe,U,Ee,K,Fe,ee,Ro="If you pass an already decoded video array but still want to enable model-specific frame sampling, it is strongly recommended to provide video_metadata. This allows the sampler to know the original video’s duration and FPS. You can pass metadata as a <code>VideoMetadata</code> object or as a plain dict.",Ye,oe,Se,se,De,l,te,ao,_e,Go="Constructs a base VideoProcessor.",io,J,re,co,ve,No="Converts a video to RGB format.",lo,P,ne,mo,be,Ho="Convert a single or a list of urls into the corresponding <code>np.array</code> objects.",po,ye,Xo=`If a single url is passed, the return value will be a single object. If a list is passed a list of objects is
returned.`,ho,W,ae,fo,we,Eo="Instantiates a type of <code>~video_processing_utils.VideoProcessorBase</code> from a Python dictionary of parameters.",uo,k,ie,go,Me,Fo=`Instantiates a video processor of type <code>~video_processing_utils.VideoProcessorBase</code> from the path to a JSON
file of parameters.`,_o,B,de,vo,$e,Yo="Instantiate a type of <code>~video_processing_utils.VideoProcessorBase</code> from an video processor.",bo,I,yo,q,ce,wo,Ve,So=`From a <code>pretrained_model_name_or_path</code>, resolve to a dictionary of parameters, to be used for instantiating a
video processor of type <code>~video_processing_utils.VideoProcessorBase</code> using <code>from_dict</code>.`,Mo,xe,le,$o,z,me,Vo,je,Do=`Register this class with a given auto class. This should only be used for custom video processors as the ones
in the library are already mapped with <code>AutoVideoProcessor </code>.`,xo,L,jo,R,pe,To,Te,Ao=`Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.
If <code>fps</code> is passed along with metadata, <code>fps</code> frames per second are sampled uniformty. Arguments <code>num_frames</code>
and <code>fps</code> are mutually exclusive.`,Po,G,he,Bo,Pe,Qo=`Save an video processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<code>~video_processing_utils.VideoProcessorBase.from_pretrained</code> class method.`,zo,N,fe,Zo,Be,Oo="Serializes this instance to a Python dictionary.",Ae,ue,Qe,Ce,Oe;return x=new no({props:{title:"Video Processor",local:"video-processor",headingTag:"h1"}}),E=new no({props:{title:"Usage Example",local:"usage-example",headingTag:"h3"}}),Y=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9WaWRlb1Byb2Nlc3NvciUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybGxhdmEtaGYlMkZsbGF2YS1vbmV2aXNpb24tcXdlbjItMC41Yi1vdi1oZiUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor

processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>)`,wrap:!1}}),A=new Ue({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzLnZpZGVvX3V0aWxzJTIwaW1wb3J0JTIwbG9hZF92aWRlbyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvVmlkZW9Qcm9jZXNzb3IlMEElMEF2aWRlbyUyMCUzRCUyMGxvYWRfdmlkZW8oJTIydmlkZW8ubXA0JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybGxhdmEtaGYlMkZsbGF2YS1vbmV2aXNpb24tcXdlbjItMC41Yi1vdi1oZiUyMiUyQyUyMGRldmljZSUzRCUyMmN1ZGElMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwdG9yY2guY29tcGlsZShwcm9jZXNzb3IpJTBBcHJvY2Vzc2VkX3ZpZGVvJTIwJTNEJTIwcHJvY2Vzc29yKHZpZGVvJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers.video_utils <span class="hljs-keyword">import</span> load_video
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor

video = load_video(<span class="hljs-string">&quot;video.mp4&quot;</span>)
processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)
processor = torch.<span class="hljs-built_in">compile</span>(processor)
processed_video = processor(video, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),Q=new no({props:{title:"Sampling behavior",local:"sampling-behavior",headingTag:"h4"}}),U=new es({props:{warning:!1,$$slots:{default:[ds]},$$scope:{ctx:C}}}),K=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9WaWRlb1Byb2Nlc3NvciUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybGxhdmEtaGYlMkZsbGF2YS1vbmV2aXNpb24tcXdlbjItMC41Yi1vdi1oZiUyMiUyQyUyMGRldmljZSUzRCUyMmN1ZGElMjIpJTBBcHJvY2Vzc2VkX3ZpZGVvX2lucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih2aWRlb3MlM0QlNUIlMjJ2aWRlb19wYXRoLm1wNCUyMiU1RCUyQyUyMHJldHVybl9tZXRhZGF0YSUzRFRydWUlMkMlMjBkb19zYW1wbGVfZnJhbWVzJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdmlkZW9fbWV0YWRhdGElMjAlM0QlMjBwcm9jZXNzZWRfdmlkZW9faW5wdXRzJTVCJTIydmlkZW9fbWV0YWRhdGElMjIlNUQlMEElMEElMjMlMjBTZWUlMjBob3clMjBtYW55JTIwZnJhbWVzJTIwdGhlJTIwb3JpZ2luYWwlMjB2aWRlbyUyMGhhZCUyMGFuZCUyMHdoYXQlMjB3YXMlMjB0aGUlMjBvcmlnaW5hbCUyMEZQUyUwQXByaW50KHZpZGVvX21ldGFkYXRhLnRvdGFsX251bV9mcmFtZXMlMkMlMjB2aWRlb19tZXRhZGF0YS5mcHMp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor

processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)
processed_video_inputs = processor(videos=[<span class="hljs-string">&quot;video_path.mp4&quot;</span>], return_metadata=<span class="hljs-literal">True</span>, do_sample_frames=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
video_metadata = processed_video_inputs[<span class="hljs-string">&quot;video_metadata&quot;</span>]

<span class="hljs-comment"># See how many frames the original video had and what was the original FPS</span>
<span class="hljs-built_in">print</span>(video_metadata.total_num_frames, video_metadata.fps)`,wrap:!1}}),oe=new Ue({props:{code:"JTBBJTVCMTAlMkMlMjAzJTJDJTIwMzg0JTJDJTIwMzg0JTVE",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor
<span class="hljs-keyword">from</span> transformers.video_utils <span class="hljs-keyword">import</span> VideoMetadata

processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)
my_decodec_video = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, size=(<span class="hljs-number">100</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1280</span>, <span class="hljs-number">1280</span>)) <span class="hljs-comment"># short video of 100 frames</span>
video_metadata = VideoMetadata(
    total_num_frames=<span class="hljs-number">100</span>,
    fps=<span class="hljs-number">24</span>,
    duration=<span class="hljs-number">4.1</span>, <span class="hljs-comment"># in seconds</span>
)
processed_video_inputs = processor(videos=[<span class="hljs-string">&quot;video_path.mp4&quot;</span>], video_metadata=video_metadata, do_sample_frames=<span class="hljs-literal">True</span>, num_frames=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(processed_video_inputs.pixel_values_videos.shape)
<span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>]`,wrap:!1}}),se=new no({props:{title:"BaseVideoProcessor",local:"transformers.BaseVideoProcessor",headingTag:"h2"}}),te=new j({props:{name:"class transformers.BaseVideoProcessor",anchor:"transformers.BaseVideoProcessor",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.processing_utils.VideosKwargs]"}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the video&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BaseVideoProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output video after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.BaseVideoProcessor.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.size_divisor</code>) &#x2014;
The size by which to make sure both the height and width can be divided.`,name:"size_divisor"},{anchor:"transformers.BaseVideoProcessor.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.default_to_square</code>) &#x2014;
Whether to default to a square video when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.BaseVideoProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the video. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BaseVideoProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the video to the specified <code>crop_size</code>. Can be overridden by <code>do_center_crop</code> in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.BaseVideoProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to pad the video to the <code>(max_height, max_width)</code> of the videos in the batch.`,name:"do_pad"},{anchor:"transformers.BaseVideoProcessor.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the output video after applying <code>center_crop</code>. Can be overridden by <code>crop_size</code> in the <code>preprocess</code>
method.`,name:"crop_size"},{anchor:"transformers.BaseVideoProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the video by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BaseVideoProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Scale factor to use if rescaling the video. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BaseVideoProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the video. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.BaseVideoProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Mean to use if normalizing the video. This is a float or list of floats the length of the number of
channels in the video. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BaseVideoProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Standard deviation to use if normalizing the video. This is a float or list of floats the length of the
number of channels in the video. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BaseVideoProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Whether to convert the video to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.BaseVideoProcessor.video_metadata",description:`<strong>video_metadata</strong> (<code>VideoMetadata</code>, <em>optional</em>) &#x2014;
Metadata of the video containing information about total duration, fps and total number of frames.`,name:"video_metadata"},{anchor:"transformers.BaseVideoProcessor.do_sample_frames",description:`<strong>do_sample_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.do_sample_frames</code>) &#x2014;
Whether to sample frames from the video before processing or to process the whole video.`,name:"do_sample_frames"},{anchor:"transformers.BaseVideoProcessor.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.num_frames</code>) &#x2014;
Maximum number of frames to sample when <code>do_sample_frames=True</code>.`,name:"num_frames"},{anchor:"transformers.BaseVideoProcessor.fps",description:`<strong>fps</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.fps</code>) &#x2014;
Target frames to sample per second when <code>do_sample_frames=True</code>.`,name:"fps"},{anchor:"transformers.BaseVideoProcessor.return_tensors",description:"<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.BaseVideoProcessor.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input video.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BaseVideoProcessor.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input video. If unset, the channel dimension format is inferred
from the input video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: video in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.BaseVideoProcessor.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the videos on. If unset, the device is inferred from the input videos.`,name:"device"},{anchor:"transformers.BaseVideoProcessor.return_metadata",description:`<strong>return_metadata</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return video metadata or not.`,name:"return_metadata"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L155"}}),re=new j({props:{name:"convert_to_rgb",anchor:"transformers.BaseVideoProcessor.convert_to_rgb",parameters:[{name:"video",val:": torch.Tensor"}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.convert_to_rgb.video",description:`<strong>video</strong> (<code>&quot;torch.Tensor&quot;</code>) &#x2014;
The video to convert.`,name:"video"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The converted video.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),ne=new j({props:{name:"fetch_videos",anchor:"transformers.BaseVideoProcessor.fetch_videos",parameters:[{name:"video_url_or_urls",val:": typing.Union[str, list[str], list[list[str]]]"},{name:"sample_indices_fn",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L880"}}),ae=new j({props:{name:"from_dict",anchor:"transformers.BaseVideoProcessor.from_dict",parameters:[{name:"video_processor_dict",val:": dict"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.from_dict.video_processor_dict",description:`<strong>video_processor_dict</strong> (<code>dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the video processor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<code>~video_processing_utils.VideoProcessorBase.to_dict</code> method.`,name:"video_processor_dict"},{anchor:"transformers.BaseVideoProcessor.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the video processor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L741",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The video processor object instantiated from those
parameters.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>~video_processing_utils.VideoProcessorBase</code></p>
`}}),ie=new j({props:{name:"from_json_file",anchor:"transformers.BaseVideoProcessor.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L835",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The video_processor object
instantiated from that JSON file.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A video processor of type <code>~video_processing_utils.VideoProcessorBase</code></p>
`}}),de=new j({props:{name:"from_pretrained",anchor:"transformers.BaseVideoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained video hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a video processor file saved using the
<code>~video_processing_utils.VideoProcessorBase.save_pretrained</code> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved video processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/video_preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model video processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the video processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.BaseVideoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L448",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A video processor of type <code>~video_processing_utils.ImagVideoProcessorBase</code>.</p>
`}}),I=new as({props:{anchor:"transformers.BaseVideoProcessor.from_pretrained.example",$$slots:{default:[cs]},$$scope:{ctx:C}}}),ce=new j({props:{name:"get_video_processor_dict",anchor:"transformers.BaseVideoProcessor.get_video_processor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.get_video_processor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BaseVideoProcessor.get_video_processor_dict.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&quot;</code>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.`,name:"subfolder"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L623",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The dictionary(ies) that will be used to instantiate the video processor object.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[Dict, Dict]</code></p>
`}}),le=new j({props:{name:"preprocess",anchor:"transformers.BaseVideoProcessor.preprocess",parameters:[{name:"videos",val:": typing.Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]]]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.processing_utils.VideosKwargs]"}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the video&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BaseVideoProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output video after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.BaseVideoProcessor.preprocess.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.size_divisor</code>) &#x2014;
The size by which to make sure both the height and width can be divided.`,name:"size_divisor"},{anchor:"transformers.BaseVideoProcessor.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.default_to_square</code>) &#x2014;
Whether to default to a square video when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.BaseVideoProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the video. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the video to the specified <code>crop_size</code>. Can be overridden by <code>do_center_crop</code> in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to pad the video to the <code>(max_height, max_width)</code> of the videos in the batch.`,name:"do_pad"},{anchor:"transformers.BaseVideoProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the output video after applying <code>center_crop</code>. Can be overridden by <code>crop_size</code> in the <code>preprocess</code>
method.`,name:"crop_size"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the video by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BaseVideoProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Scale factor to use if rescaling the video. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the video. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.BaseVideoProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Mean to use if normalizing the video. This is a float or list of floats the length of the number of
channels in the video. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BaseVideoProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Standard deviation to use if normalizing the video. This is a float or list of floats the length of the
number of channels in the video. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Whether to convert the video to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.BaseVideoProcessor.preprocess.video_metadata",description:`<strong>video_metadata</strong> (<code>VideoMetadata</code>, <em>optional</em>) &#x2014;
Metadata of the video containing information about total duration, fps and total number of frames.`,name:"video_metadata"},{anchor:"transformers.BaseVideoProcessor.preprocess.do_sample_frames",description:`<strong>do_sample_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.do_sample_frames</code>) &#x2014;
Whether to sample frames from the video before processing or to process the whole video.`,name:"do_sample_frames"},{anchor:"transformers.BaseVideoProcessor.preprocess.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.num_frames</code>) &#x2014;
Maximum number of frames to sample when <code>do_sample_frames=True</code>.`,name:"num_frames"},{anchor:"transformers.BaseVideoProcessor.preprocess.fps",description:`<strong>fps</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.fps</code>) &#x2014;
Target frames to sample per second when <code>do_sample_frames=True</code>.`,name:"fps"},{anchor:"transformers.BaseVideoProcessor.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.BaseVideoProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input video.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BaseVideoProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input video. If unset, the channel dimension format is inferred
from the input video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: video in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.BaseVideoProcessor.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the videos on. If unset, the device is inferred from the input videos.`,name:"device"},{anchor:"transformers.BaseVideoProcessor.preprocess.return_metadata",description:`<strong>return_metadata</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return video metadata or not.`,name:"return_metadata"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L355"}}),me=new j({props:{name:"register_for_auto_class",anchor:"transformers.BaseVideoProcessor.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoVideoProcessor'"}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoVideoProcessor &quot;</code>) &#x2014;
The auto class to register this new video processor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L854"}}),L=new es({props:{warning:!0,$$slots:{default:[ls]},$$scope:{ctx:C}}}),pe=new j({props:{name:"sample_frames",anchor:"transformers.BaseVideoProcessor.sample_frames",parameters:[{name:"metadata",val:": VideoMetadata"},{name:"num_frames",val:": typing.Optional[int] = None"},{name:"fps",val:": typing.Union[int, float, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.sample_frames.metadata",description:`<strong>metadata</strong> (<code>VideoMetadata</code>) &#x2014;
Metadata of the video containing information about total duration, fps and total number of frames.`,name:"metadata"},{anchor:"transformers.BaseVideoProcessor.sample_frames.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of frames to sample. Defaults to <code>self.num_frames</code>.`,name:"num_frames"},{anchor:"transformers.BaseVideoProcessor.sample_frames.fps",description:`<strong>fps</strong> (<code>int</code> or <code>float</code>, <em>optional</em>) &#x2014;
Target frames to sample per second. Defaults to <code>self.fps</code>.`,name:"fps"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L239",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Indices to sample video frames.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>np.ndarray</p>
`}}),he=new j({props:{name:"save_pretrained",anchor:"transformers.BaseVideoProcessor.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseVideoProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the video processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.BaseVideoProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.BaseVideoProcessor.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L562"}}),fe=new j({props:{name:"to_dict",anchor:"transformers.BaseVideoProcessor.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L786",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this video processor instance.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>dict[str, Any]</code></p>
`}}),ue=new is({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/video_processor.md"}}),{c(){c=a("meta"),V=r(),b=a("p"),w=r(),p(x.$$.fragment),y=r(),T=a("p"),T.innerHTML=Uo,Je=r(),H=a("p"),H.textContent=Jo,We=r(),X=a("p"),X.innerHTML=Wo,ke=r(),p(E.$$.fragment),Ie=r(),F=a("p"),F.innerHTML=ko,qe=r(),p(Y.$$.fragment),Le=r(),S=a("p"),S.innerHTML=Io,Re=r(),D=a("p"),D.innerHTML=qo,Ge=r(),p(A.$$.fragment),Ne=r(),p(Q.$$.fragment),He=r(),O=a("p"),O.innerHTML=Lo,Xe=r(),p(U.$$.fragment),Ee=r(),p(K.$$.fragment),Fe=r(),ee=a("p"),ee.innerHTML=Ro,Ye=r(),p(oe.$$.fragment),Se=r(),p(se.$$.fragment),De=r(),l=a("div"),p(te.$$.fragment),ao=r(),_e=a("p"),_e.textContent=Go,io=r(),J=a("div"),p(re.$$.fragment),co=r(),ve=a("p"),ve.textContent=No,lo=r(),P=a("div"),p(ne.$$.fragment),mo=r(),be=a("p"),be.innerHTML=Ho,po=r(),ye=a("p"),ye.textContent=Xo,ho=r(),W=a("div"),p(ae.$$.fragment),fo=r(),we=a("p"),we.innerHTML=Eo,uo=r(),k=a("div"),p(ie.$$.fragment),go=r(),Me=a("p"),Me.innerHTML=Fo,_o=r(),B=a("div"),p(de.$$.fragment),vo=r(),$e=a("p"),$e.innerHTML=Yo,bo=r(),p(I.$$.fragment),yo=r(),q=a("div"),p(ce.$$.fragment),wo=r(),Ve=a("p"),Ve.innerHTML=So,Mo=r(),xe=a("div"),p(le.$$.fragment),$o=r(),z=a("div"),p(me.$$.fragment),Vo=r(),je=a("p"),je.innerHTML=Do,xo=r(),p(L.$$.fragment),jo=r(),R=a("div"),p(pe.$$.fragment),To=r(),Te=a("p"),Te.innerHTML=Ao,Po=r(),G=a("div"),p(he.$$.fragment),Bo=r(),Pe=a("p"),Pe.innerHTML=Qo,zo=r(),N=a("div"),p(fe.$$.fragment),Zo=r(),Be=a("p"),Be.textContent=Oo,Ae=r(),p(ue.$$.fragment),Qe=r(),Ce=a("p"),this.h()},l(e){const o=ns("svelte-u9bgzb",document.head);c=i(o,"META",{name:!0,content:!0}),o.forEach(s),V=n(e),b=i(e,"P",{}),M(b).forEach(s),w=n(e),h(x.$$.fragment,e),y=n(e),T=i(e,"P",{"data-svelte-h":!0}),v(T)!=="svelte-1jnqgyg"&&(T.innerHTML=Uo),Je=n(e),H=i(e,"P",{"data-svelte-h":!0}),v(H)!=="svelte-dh9q2l"&&(H.textContent=Jo),We=n(e),X=i(e,"P",{"data-svelte-h":!0}),v(X)!=="svelte-u7nklz"&&(X.innerHTML=Wo),ke=n(e),h(E.$$.fragment,e),Ie=n(e),F=i(e,"P",{"data-svelte-h":!0}),v(F)!=="svelte-1ukoepa"&&(F.innerHTML=ko),qe=n(e),h(Y.$$.fragment,e),Le=n(e),S=i(e,"P",{"data-svelte-h":!0}),v(S)!=="svelte-157dwxe"&&(S.innerHTML=Io),Re=n(e),D=i(e,"P",{"data-svelte-h":!0}),v(D)!=="svelte-1wmclx8"&&(D.innerHTML=qo),Ge=n(e),h(A.$$.fragment,e),Ne=n(e),h(Q.$$.fragment,e),He=n(e),O=i(e,"P",{"data-svelte-h":!0}),v(O)!=="svelte-rprxr3"&&(O.innerHTML=Lo),Xe=n(e),h(U.$$.fragment,e),Ee=n(e),h(K.$$.fragment,e),Fe=n(e),ee=i(e,"P",{"data-svelte-h":!0}),v(ee)!=="svelte-jrrjr4"&&(ee.innerHTML=Ro),Ye=n(e),h(oe.$$.fragment,e),Se=n(e),h(se.$$.fragment,e),De=n(e),l=i(e,"DIV",{class:!0});var m=M(l);h(te.$$.fragment,m),ao=n(m),_e=i(m,"P",{"data-svelte-h":!0}),v(_e)!=="svelte-1nq3ftb"&&(_e.textContent=Go),io=n(m),J=i(m,"DIV",{class:!0});var ge=M(J);h(re.$$.fragment,ge),co=n(ge),ve=i(ge,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-zje851"&&(ve.textContent=No),ge.forEach(s),lo=n(m),P=i(m,"DIV",{class:!0});var Z=M(P);h(ne.$$.fragment,Z),mo=n(Z),be=i(Z,"P",{"data-svelte-h":!0}),v(be)!=="svelte-8rp1wb"&&(be.innerHTML=Ho),po=n(Z),ye=i(Z,"P",{"data-svelte-h":!0}),v(ye)!=="svelte-1q8uymn"&&(ye.textContent=Xo),Z.forEach(s),ho=n(m),W=i(m,"DIV",{class:!0});var Ke=M(W);h(ae.$$.fragment,Ke),fo=n(Ke),we=i(Ke,"P",{"data-svelte-h":!0}),v(we)!=="svelte-xo6w0l"&&(we.innerHTML=Eo),Ke.forEach(s),uo=n(m),k=i(m,"DIV",{class:!0});var eo=M(k);h(ie.$$.fragment,eo),go=n(eo),Me=i(eo,"P",{"data-svelte-h":!0}),v(Me)!=="svelte-ab595l"&&(Me.innerHTML=Fo),eo.forEach(s),_o=n(m),B=i(m,"DIV",{class:!0});var ze=M(B);h(de.$$.fragment,ze),vo=n(ze),$e=i(ze,"P",{"data-svelte-h":!0}),v($e)!=="svelte-1h58qqw"&&($e.innerHTML=Yo),bo=n(ze),h(I.$$.fragment,ze),ze.forEach(s),yo=n(m),q=i(m,"DIV",{class:!0});var oo=M(q);h(ce.$$.fragment,oo),wo=n(oo),Ve=i(oo,"P",{"data-svelte-h":!0}),v(Ve)!=="svelte-1l7bssv"&&(Ve.innerHTML=So),oo.forEach(s),Mo=n(m),xe=i(m,"DIV",{class:!0});var Ko=M(xe);h(le.$$.fragment,Ko),Ko.forEach(s),$o=n(m),z=i(m,"DIV",{class:!0});var Ze=M(z);h(me.$$.fragment,Ze),Vo=n(Ze),je=i(Ze,"P",{"data-svelte-h":!0}),v(je)!=="svelte-muvbve"&&(je.innerHTML=Do),xo=n(Ze),h(L.$$.fragment,Ze),Ze.forEach(s),jo=n(m),R=i(m,"DIV",{class:!0});var so=M(R);h(pe.$$.fragment,so),To=n(so),Te=i(so,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-1hhsohc"&&(Te.innerHTML=Ao),so.forEach(s),Po=n(m),G=i(m,"DIV",{class:!0});var to=M(G);h(he.$$.fragment,to),Bo=n(to),Pe=i(to,"P",{"data-svelte-h":!0}),v(Pe)!=="svelte-i81fpc"&&(Pe.innerHTML=Qo),to.forEach(s),zo=n(m),N=i(m,"DIV",{class:!0});var ro=M(N);h(fe.$$.fragment,ro),Zo=n(ro),Be=i(ro,"P",{"data-svelte-h":!0}),v(Be)!=="svelte-1ww3wqq"&&(Be.textContent=Oo),ro.forEach(s),m.forEach(s),Ae=n(e),h(ue.$$.fragment,e),Qe=n(e),Ce=i(e,"P",{}),M(Ce).forEach(s),this.h()},h(){$(c,"name","hf:doc:metadata"),$(c,"content",ps),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){t(document.head,c),d(e,V,o),d(e,b,o),d(e,w,o),f(x,e,o),d(e,y,o),d(e,T,o),d(e,Je,o),d(e,H,o),d(e,We,o),d(e,X,o),d(e,ke,o),f(E,e,o),d(e,Ie,o),d(e,F,o),d(e,qe,o),f(Y,e,o),d(e,Le,o),d(e,S,o),d(e,Re,o),d(e,D,o),d(e,Ge,o),f(A,e,o),d(e,Ne,o),f(Q,e,o),d(e,He,o),d(e,O,o),d(e,Xe,o),f(U,e,o),d(e,Ee,o),f(K,e,o),d(e,Fe,o),d(e,ee,o),d(e,Ye,o),f(oe,e,o),d(e,Se,o),f(se,e,o),d(e,De,o),d(e,l,o),f(te,l,null),t(l,ao),t(l,_e),t(l,io),t(l,J),f(re,J,null),t(J,co),t(J,ve),t(l,lo),t(l,P),f(ne,P,null),t(P,mo),t(P,be),t(P,po),t(P,ye),t(l,ho),t(l,W),f(ae,W,null),t(W,fo),t(W,we),t(l,uo),t(l,k),f(ie,k,null),t(k,go),t(k,Me),t(l,_o),t(l,B),f(de,B,null),t(B,vo),t(B,$e),t(B,bo),f(I,B,null),t(l,yo),t(l,q),f(ce,q,null),t(q,wo),t(q,Ve),t(l,Mo),t(l,xe),f(le,xe,null),t(l,$o),t(l,z),f(me,z,null),t(z,Vo),t(z,je),t(z,xo),f(L,z,null),t(l,jo),t(l,R),f(pe,R,null),t(R,To),t(R,Te),t(l,Po),t(l,G),f(he,G,null),t(G,Bo),t(G,Pe),t(l,zo),t(l,N),f(fe,N,null),t(N,Zo),t(N,Be),d(e,Ae,o),f(ue,e,o),d(e,Qe,o),d(e,Ce,o),Oe=!0},p(e,[o]){const m={};o&2&&(m.$$scope={dirty:o,ctx:e}),U.$set(m);const ge={};o&2&&(ge.$$scope={dirty:o,ctx:e}),I.$set(ge);const Z={};o&2&&(Z.$$scope={dirty:o,ctx:e}),L.$set(Z)},i(e){Oe||(u(x.$$.fragment,e),u(E.$$.fragment,e),u(Y.$$.fragment,e),u(A.$$.fragment,e),u(Q.$$.fragment,e),u(U.$$.fragment,e),u(K.$$.fragment,e),u(oe.$$.fragment,e),u(se.$$.fragment,e),u(te.$$.fragment,e),u(re.$$.fragment,e),u(ne.$$.fragment,e),u(ae.$$.fragment,e),u(ie.$$.fragment,e),u(de.$$.fragment,e),u(I.$$.fragment,e),u(ce.$$.fragment,e),u(le.$$.fragment,e),u(me.$$.fragment,e),u(L.$$.fragment,e),u(pe.$$.fragment,e),u(he.$$.fragment,e),u(fe.$$.fragment,e),u(ue.$$.fragment,e),Oe=!0)},o(e){g(x.$$.fragment,e),g(E.$$.fragment,e),g(Y.$$.fragment,e),g(A.$$.fragment,e),g(Q.$$.fragment,e),g(U.$$.fragment,e),g(K.$$.fragment,e),g(oe.$$.fragment,e),g(se.$$.fragment,e),g(te.$$.fragment,e),g(re.$$.fragment,e),g(ne.$$.fragment,e),g(ae.$$.fragment,e),g(ie.$$.fragment,e),g(de.$$.fragment,e),g(I.$$.fragment,e),g(ce.$$.fragment,e),g(le.$$.fragment,e),g(me.$$.fragment,e),g(L.$$.fragment,e),g(pe.$$.fragment,e),g(he.$$.fragment,e),g(fe.$$.fragment,e),g(ue.$$.fragment,e),Oe=!1},d(e){e&&(s(V),s(b),s(w),s(y),s(T),s(Je),s(H),s(We),s(X),s(ke),s(Ie),s(F),s(qe),s(Le),s(S),s(Re),s(D),s(Ge),s(Ne),s(He),s(O),s(Xe),s(Ee),s(Fe),s(ee),s(Ye),s(Se),s(De),s(l),s(Ae),s(Qe),s(Ce)),s(c),_(x,e),_(E,e),_(Y,e),_(A,e),_(Q,e),_(U,e),_(K,e),_(oe,e),_(se,e),_(te),_(re),_(ne),_(ae),_(ie),_(de),_(I),_(ce),_(le),_(me),_(L),_(pe),_(he),_(fe),_(ue,e)}}}const ps='{"title":"Video Processor","local":"video-processor","sections":[{"title":"Usage Example","local":"usage-example","sections":[{"title":"Sampling behavior","local":"sampling-behavior","sections":[],"depth":4}],"depth":3},{"title":"BaseVideoProcessor","local":"transformers.BaseVideoProcessor","sections":[],"depth":2}],"depth":1}';function hs(C){return ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ws extends ts{constructor(c){super(),rs(this,c,hs,ms,os,{})}}export{ws as component};
