import{s as jn,o as Un,n as He}from"../chunks/scheduler.18a86fab.js";import{S as xn,i as zn,g as i,s as o,r as h,m as $n,A as Gn,h as c,f as l,c as r,j as G,x as u,u as b,n as vn,k as F,l as Zn,y as n,a as d,v as y,d as k,t as _,w as M}from"../chunks/index.98837b22.js";import{T as qn}from"../chunks/Tip.77304350.js";import{D as _e}from"../chunks/Docstring.a1ef7999.js";import{C as B}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Jn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as gt,E as Nn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as In,a as St}from"../chunks/HfOption.6641485e.js";function Cn(j){let t,f=`This model was contributed by <a href="https://huggingface.co/lysandre" rel="nofollow">Lysandre</a>.<br/>
Click on the NLLB models in the right sidebar for more examples of how to apply NLLB to different translation tasks.`;return{c(){t=i("p"),t.innerHTML=f},l(s){t=c(s,"P",{"data-svelte-h":!0}),u(t)!=="svelte-1ez7t0j"&&(t.innerHTML=f)},m(s,g){d(s,t,g)},p:He,d(s){s&&l(t)}}}function Wn(j){let t,f;return t=new B({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJ0cmFuc2xhdGlvbiUyMiUyQyUyMG1vZGVsJTNEJTIyZmFjZWJvb2slMkZubGxiLTIwMC1kaXN0aWxsZWQtNjAwTSUyMiUyQyUyMHNyY19sYW5nJTNEJTIyZW5nX0xhdG4lMjIlMkMlMjB0Z3RfbGFuZyUzRCUyMmZyYV9MYXRuJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlJTNEMCklMEFwaXBlbGluZSglMjJVTiUyMENoaWVmJTIwc2F5cyUyMHRoZXJlJTIwaXMlMjBubyUyMG1pbGl0YXJ5JTIwc29sdXRpb24lMjBpbiUyMFN5cmlhJTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(task=<span class="hljs-string">&quot;translation&quot;</span>, model=<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>, dtype=torch.float16, device=<span class="hljs-number">0</span>)
pipeline(<span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>)`,wrap:!1}}),{c(){h(t.$$.fragment)},l(s){b(t.$$.fragment,s)},m(s,g){y(t,s,g),f=!0},p:He,i(s){f||(k(t.$$.fragment,s),f=!0)},o(s){_(t.$$.fragment,s),f=!1},d(s){M(t,s)}}}function Rn(j){let t,f;return t=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTIwYXR0bl9pbXBsZW1lbnRhaXRvbiUzRCUyMnNkcGElMjIpJTBBJTBBYXJ0aWNsZSUyMCUzRCUyMCUyMlVOJTIwQ2hpZWYlMjBzYXlzJTIwdGhlcmUlMjBpcyUyMG5vJTIwbWlsaXRhcnklMjBzb2x1dGlvbiUyMGluJTIwU3lyaWElMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoYXJ0aWNsZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBdHJhbnNsYXRlZF90b2tlbnMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSglMEElMjAlMjAlMjAlMjAqKmlucHV0cyUyQyUyMGZvcmNlZF9ib3NfdG9rZW5faWQlM0R0b2tlbml6ZXIuY29udmVydF90b2tlbnNfdG9faWRzKCUyMmZyYV9MYXRuJTIyKSUyQyUyMG1heF9sZW5ndGglM0QzMCUwQSklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKHRyYW5zbGF0ZWRfdG9rZW5zJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, dtype=<span class="hljs-string">&quot;auto&quot;</span>, attn_implementaiton=<span class="hljs-string">&quot;sdpa&quot;</span>)

article = <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>
inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

translated_tokens = model.generate(
    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(<span class="hljs-string">&quot;fra_Latn&quot;</span>), max_length=<span class="hljs-number">30</span>
)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])`,wrap:!1}}),{c(){h(t.$$.fragment)},l(s){b(t.$$.fragment,s)},m(s,g){y(t,s,g),f=!0},p:He,i(s){f||(k(t.$$.fragment,s),f=!0)},o(s){_(t.$$.fragment,s),f=!1},d(s){M(t,s)}}}function Fn(j){let t,f;return t=new B({props:{code:"ZWNobyUyMC1lJTIwJTIyVU4lMjBDaGllZiUyMHNheXMlMjB0aGVyZSUyMGlzJTIwbm8lMjBtaWxpdGFyeSUyMHNvbHV0aW9uJTIwaW4lMjBTeXJpYSUyMiUyMCU3QyUyMHRyYW5zZm9ybWVycyUyMHJ1biUyMC0tdGFzayUyMCUyMnRyYW5zbGF0aW9uX2VuX3RvX2ZyJTIyJTIwLS1tb2RlbCUyMGZhY2Vib29rJTJGbmxsYi0yMDAtZGlzdGlsbGVkLTYwME0lMjAtLWRldmljZSUyMDA=",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span> | transformers run --task <span class="hljs-string">&quot;translation_en_to_fr&quot;</span> --model facebook/nllb-200-distilled-600M --device 0',wrap:!1}}),{c(){h(t.$$.fragment)},l(s){b(t.$$.fragment,s)},m(s,g){y(t,s,g),f=!0},p:He,i(s){f||(k(t.$$.fragment,s),f=!0)},o(s){_(t.$$.fragment,s),f=!1},d(s){M(t,s)}}}function Bn(j){let t,f,s,g,T,m;return t=new St({props:{id:"usage",option:"Pipeline",$$slots:{default:[Wn]},$$scope:{ctx:j}}}),s=new St({props:{id:"usage",option:"AutoModel",$$slots:{default:[Rn]},$$scope:{ctx:j}}}),T=new St({props:{id:"usage",option:"transformers CLI",$$slots:{default:[Fn]},$$scope:{ctx:j}}}),{c(){h(t.$$.fragment),f=o(),h(s.$$.fragment),g=o(),h(T.$$.fragment)},l(p){b(t.$$.fragment,p),f=r(p),b(s.$$.fragment,p),g=r(p),b(T.$$.fragment,p)},m(p,$){y(t,p,$),d(p,f,$),y(s,p,$),d(p,g,$),y(T,p,$),m=!0},p(p,$){const Ye={};$&2&&(Ye.$$scope={dirty:$,ctx:p}),t.$set(Ye);const Q={};$&2&&(Q.$$scope={dirty:$,ctx:p}),s.$set(Q);const N={};$&2&&(N.$$scope={dirty:$,ctx:p}),T.$set(N)},i(p){m||(k(t.$$.fragment,p),k(s.$$.fragment,p),k(T.$$.fragment,p),m=!0)},o(p){_(t.$$.fragment,p),_(s.$$.fragment,p),_(T.$$.fragment,p),m=!1},d(p){p&&(l(f),l(g)),M(t,p),M(s,p),M(T,p)}}}function Ln(j){let t,f="Examples:",s,g,T;return g=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwc3JjX2xhbmclM0QlMjJlbmdfTGF0biUyMiUyQyUyMHRndF9sYW5nJTNEJTIyZnJhX0xhdG4lMjIlMEEpJTBBZXhhbXBsZV9lbmdsaXNoX3BocmFzZSUyMCUzRCUyMCUyMiUyMFVOJTIwQ2hpZWYlMjBTYXlzJTIwVGhlcmUlMjBJcyUyME5vJTIwTWlsaXRhcnklMjBTb2x1dGlvbiUyMGluJTIwU3lyaWElMjIlMEFleHBlY3RlZF90cmFuc2xhdGlvbl9mcmVuY2glMjAlM0QlMjAlMjJMZSUyMGNoZWYlMjBkZSUyMGwnT05VJTIwYWZmaXJtZSUyMHF1J2lsJTIwbid5JTIwYSUyMHBhcyUyMGRlJTIwc29sdXRpb24lMjBtaWxpdGFpcmUlMjBlbiUyMFN5cmllLiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihleGFtcGxlX2VuZ2xpc2hfcGhyYXNlJTJDJTIwdGV4dF90YXJnZXQlM0RleHBlY3RlZF90cmFuc2xhdGlvbl9mcmVuY2glMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),{c(){t=i("p"),t.textContent=f,s=o(),h(g.$$.fragment)},l(m){t=c(m,"P",{"data-svelte-h":!0}),u(t)!=="svelte-kvfsh7"&&(t.textContent=f),s=r(m),b(g.$$.fragment,m)},m(m,p){d(m,t,p),d(m,s,p),y(g,m,p),T=!0},p:He,i(m){T||(k(g.$$.fragment,m),T=!0)},o(m){_(g.$$.fragment,m),T=!1},d(m){m&&(l(t),l(s)),M(g,m)}}}function Vn(j){let t,f="Examples:",s,g,T;return g=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXJGYXN0JTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwTmxsYlRva2VuaXplckZhc3QuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmZhY2Vib29rJTJGbmxsYi0yMDAtZGlzdGlsbGVkLTYwME0lMjIlMkMlMjBzcmNfbGFuZyUzRCUyMmVuZ19MYXRuJTIyJTJDJTIwdGd0X2xhbmclM0QlMjJmcmFfTGF0biUyMiUwQSklMEFleGFtcGxlX2VuZ2xpc2hfcGhyYXNlJTIwJTNEJTIwJTIyJTIwVU4lMjBDaGllZiUyMFNheXMlMjBUaGVyZSUyMElzJTIwTm8lMjBNaWxpdGFyeSUyMFNvbHV0aW9uJTIwaW4lMjBTeXJpYSUyMiUwQWV4cGVjdGVkX3RyYW5zbGF0aW9uX2ZyZW5jaCUyMCUzRCUyMCUyMkxlJTIwY2hlZiUyMGRlJTIwbCdPTlUlMjBhZmZpcm1lJTIwcXUnaWwlMjBuJ3klMjBhJTIwcGFzJTIwZGUlMjBzb2x1dGlvbiUyMG1pbGl0YWlyZSUyMGVuJTIwU3lyaWUuJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKGV4YW1wbGVfZW5nbGlzaF9waHJhc2UlMkMlMjB0ZXh0X3RhcmdldCUzRGV4cGVjdGVkX3RyYW5zbGF0aW9uX2ZyZW5jaCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizerFast.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),{c(){t=i("p"),t.textContent=f,s=o(),h(g.$$.fragment)},l(m){t=c(m,"P",{"data-svelte-h":!0}),u(t)!=="svelte-kvfsh7"&&(t.textContent=f),s=r(m),b(g.$$.fragment,m)},m(m,p){d(m,t,p),d(m,s,p),y(g,m,p),T=!0},p:He,i(m){T||(k(g.$$.fragment,m),T=!0)},o(m){_(g.$$.fragment,m),T=!1},d(m){m&&(l(t),l(s)),M(g,m)}}}function Xn(j){let t,f,s,g,T,m='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',p,$,Ye="<em>This model was released on 2022-07-11 and added to Hugging Face Transformers on 2022-07-18.</em>",Q,N,Ae,D,At='<a href="https://huggingface.co/papers/2207.04672" rel="nofollow">NLLB: No Language Left Behind</a> is a multilingual translation model. It’s trained on data using data mining techniques tailored for low-resource languages and supports over 200 languages. NLLB features a conditional compute architecture using a Sparsely Gated Mixture of Experts.',Qe,P,Qt='You can find all the original NLLB checkpoints under the <a href="https://huggingface.co/facebook/models?search=nllb" rel="nofollow">AI at Meta</a> organization.',De,V,Pe,K,Dt='The example below demonstrates how to translate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',Ke,X,Oe,O,Pt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',et,ee,Kt='The example below uses <a href="../quantization/bitsandbytes">bitsandbytes</a> to quantize the weights to 8-bits.',tt,te,nt,ne,Ot='Use the <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/utils/attention_visualizer.py#L139" rel="nofollow">AttentionMaskVisualizer</a> to better understand what tokens the model can and cannot attend to.',st,se,at,E,en='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/NLLB-Attn-Mask.png"/>',lt,ae,ot,I,Z,Me,tn="The tokenizer was updated in April 2023 to prefix the source sequence with the source language rather than the target language. This prioritizes zero-shot performance at a minor cost to supervised performance.",ht,le,bt,Te,nn="To revert to the legacy behavior, use the code example below.",yt,oe,kt,we,sn='<p>For non-English languages, specify the language’s <a href="https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200" rel="nofollow">BCP-47</a> code with the <code>src_lang</code> keyword as shown below.</p>',_t,re,$e,an="See example below for a translation from Romanian to German.",Mt,ie,rt,ce,it,J,de,Tt,ve,ln="Construct an NLLB tokenizer.",wt,Je,on=`Adapted from <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetTokenizer">XLNetTokenizer</a>. Based on
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.`,$t,je,rn="The tokenization method is <code>&lt;tokens&gt; &lt;eos&gt; &lt;language code&gt;</code> for source language documents, and `&lt;language code&gt;",vt,H,Jt,z,pe,jt,Ue,cn=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An NLLB sequence has the following format, where <code>X</code> represents the sequence:`,Ut,xe,dn="<li><code>input_ids</code> (for encoder) <code>X [eos, src_lang_code]</code></li> <li><code>decoder_input_ids</code>: (for decoder) <code>X [eos, tgt_lang_code]</code></li>",xt,ze,pn=`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`,ct,me,dt,w,ue,zt,Ge,mn=`Construct a “fast” NLLB tokenizer (backed by HuggingFace’s <em>tokenizers</em> library). Based on
<a href="https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models" rel="nofollow">BPE</a>.`,Gt,Ze,un=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Zt,qe,fn="The tokenization method is <code>&lt;tokens&gt; &lt;eos&gt; &lt;language code&gt;</code> for source language documents, and `&lt;language code&gt;",qt,Y,Nt,x,fe,It,Ne,gn=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. The special tokens depend on calling set_lang.`,Ct,Ie,hn="An NLLB sequence has the following format, where <code>X</code> represents the sequence:",Wt,Ce,bn="<li><code>input_ids</code> (for encoder) <code>X [eos, src_lang_code]</code></li> <li><code>decoder_input_ids</code>: (for decoder) <code>X [eos, tgt_lang_code]</code></li>",Rt,We,yn=`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`,Ft,S,ge,Bt,Re,kn=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.`,Lt,C,he,Vt,Fe,_n="Reset the special tokens to the source lang setting.",Xt,Be,Mn="<li>In legacy mode: No prefix and suffix=[eos, src_lang_code].</li> <li>In default mode: Prefix=[src_lang_code], suffix = [eos]</li>",Et,W,be,Ht,Le,Tn="Reset the special tokens to the target lang setting.",Yt,Ve,wn="<li>In legacy mode: No prefix and suffix=[eos, tgt_lang_code].</li> <li>In default mode: Prefix=[tgt_lang_code], suffix = [eos]</li>",pt,ye,mt,Se,ut;return N=new gt({props:{title:"NLLB",local:"nllb",headingTag:"h1"}}),V=new qn({props:{warning:!1,$$slots:{default:[Cn]},$$scope:{ctx:j}}}),X=new In({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Bn]},$$scope:{ctx:j}}}),te=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBCaXRzQW5kQnl0ZXNDb25maWclMEElMEFibmJfY29uZmlnJTIwJTNEJTIwQml0c0FuZEJ5dGVzQ29uZmlnKGxvYWRfaW5fOGJpdCUzRFRydWUpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGbmxsYi0yMDAtZGlzdGlsbGVkLTEuM0IlMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEYm5iX2NvbmZpZyklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC0xLjNCJTIyKSUwQSUwQWFydGljbGUlMjAlM0QlMjAlMjJVTiUyMENoaWVmJTIwc2F5cyUyMHRoZXJlJTIwaXMlMjBubyUyMG1pbGl0YXJ5JTIwc29sdXRpb24lMjBpbiUyMFN5cmlhJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKGFydGljbGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBdHJhbnNsYXRlZF90b2tlbnMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSglMEElMjAlMjAlMjAlMjAqKmlucHV0cyUyQyUyMGZvcmNlZF9ib3NfdG9rZW5faWQlM0R0b2tlbml6ZXIuY29udmVydF90b2tlbnNfdG9faWRzKCUyMmZyYV9MYXRuJTIyKSUyQyUyMG1heF9sZW5ndGglM0QzMCUyQyUwQSklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKHRyYW5zbGF0ZWRfdG9rZW5zJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_8bit=<span class="hljs-literal">True</span>)
model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-1.3B&quot;</span>, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-1.3B&quot;</span>)

article = <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>
inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
translated_tokens = model.generate(
    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(<span class="hljs-string">&quot;fra_Latn&quot;</span>), max_length=<span class="hljs-number">30</span>,
)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])`,wrap:!1}}),se=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy51dGlscy5hdHRlbnRpb25fdmlzdWFsaXplciUyMGltcG9ydCUyMEF0dGVudGlvbk1hc2tWaXN1YWxpemVyJTBBJTBBdmlzdWFsaXplciUyMCUzRCUyMEF0dGVudGlvbk1hc2tWaXN1YWxpemVyKCUyMmZhY2Vib29rJTJGbmxsYi0yMDAtZGlzdGlsbGVkLTYwME0lMjIpJTBBdmlzdWFsaXplciglMjJVTiUyMENoaWVmJTIwc2F5cyUyMHRoZXJlJTIwaXMlMjBubyUyMG1pbGl0YXJ5JTIwc29sdXRpb24lMjBpbiUyMFN5cmlhJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers.utils.attention_visualizer <span class="hljs-keyword">import</span> AttentionMaskVisualizer

visualizer = AttentionMaskVisualizer(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
visualizer(<span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>)`,wrap:!1}}),ae=new gt({props:{title:"Notes",local:"notes",headingTag:"h2"}}),le=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQXRva2VuaXplciglMjJIb3clMjB3YXMlMjB5b3VyJTIwZGF5JTNGJTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;How was your day?&quot;</span>).input_ids
[<span class="hljs-number">256047</span>, <span class="hljs-number">13374</span>, <span class="hljs-number">1398</span>, <span class="hljs-number">4260</span>, <span class="hljs-number">4039</span>, <span class="hljs-number">248130</span>, <span class="hljs-number">2</span>]`,wrap:!1}}),oe=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwbGVnYWN5X2JlaGF2aW91ciUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, legacy_behaviour=<span class="hljs-literal">True</span>)`,wrap:!1}}),ie=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQSUwQWFydGljbGUlMjAlM0QlMjAlMjJVTiUyMENoaWVmJTIwc2F5cyUyMHRoZXJlJTIwaXMlMjBubyUyMG1pbGl0YXJ5JTIwc29sdXRpb24lMjBpbiUyMFN5cmlhJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKGFydGljbGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXRyYW5zbGF0ZWRfdG9rZW5zJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoJTBBJTIwJTIwJTIwJTIwKippbnB1dHMlMkMlMjBmb3JjZWRfYm9zX3Rva2VuX2lkJTNEdG9rZW5pemVyLmNvbnZlcnRfdG9rZW5zX3RvX2lkcyglMjJmcmFfTGF0biUyMiklMkMlMjBtYXhfbGVuZ3RoJTNEMzAlMEEpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZSh0cmFuc2xhdGVkX3Rva2VucyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translated_tokens = model.generate(
<span class="hljs-meta">... </span>    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(<span class="hljs-string">&quot;fra_Latn&quot;</span>), max_length=<span class="hljs-number">30</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
Le chef de l<span class="hljs-string">&#x27;ONU dit qu&#x27;</span>il n<span class="hljs-string">&#x27;y a pas de solution militaire en Syrie</span>`,wrap:!1}}),ce=new gt({props:{title:"NllbTokenizer",local:"transformers.NllbTokenizer",headingTag:"h2"}}),de=new _e({props:{name:"class transformers.NllbTokenizer",anchor:"transformers.NllbTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"tokenizer_file",val:" = None"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"sp_model_kwargs",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"additional_special_tokens",val:" = None"},{name:"legacy_behaviour",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NllbTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.NllbTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.NllbTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.NllbTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.NllbTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.NllbTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.NllbTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.NllbTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.NllbTokenizer.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The path to a tokenizer file to use instead of the vocab file.`,name:"tokenizer_file"},{anchor:"transformers.NllbTokenizer.src_lang",description:`<strong>src_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as source language for translation.`,name:"src_lang"},{anchor:"transformers.NllbTokenizer.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as target language for translation.`,name:"tgt_lang"},{anchor:"transformers.NllbTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict[str, str]</code>) &#x2014;
Additional keyword arguments to pass to the model initialization.`,name:"sp_model_kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb.py#L38"}}),H=new Jn({props:{anchor:"transformers.NllbTokenizer.example",$$slots:{default:[Ln]},$$scope:{ctx:j}}}),pe=new _e({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb.py#L245",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),me=new gt({props:{title:"NllbTokenizerFast",local:"transformers.NllbTokenizerFast",headingTag:"h2"}}),ue=new _e({props:{name:"class transformers.NllbTokenizerFast",anchor:"transformers.NllbTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"additional_special_tokens",val:" = None"},{name:"legacy_behaviour",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.NllbTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.NllbTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.NllbTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.NllbTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.NllbTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.NllbTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.NllbTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.NllbTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The path to a tokenizer file to use instead of the vocab file.`,name:"tokenizer_file"},{anchor:"transformers.NllbTokenizerFast.src_lang",description:`<strong>src_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as source language for translation.`,name:"src_lang"},{anchor:"transformers.NllbTokenizerFast.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as target language for translation.`,name:"tgt_lang"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L42"}}),Y=new Jn({props:{anchor:"transformers.NllbTokenizerFast.example",$$slots:{default:[Vn]},$$scope:{ctx:j}}}),fe=new _e({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L178",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),ge=new _e({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L207",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),he=new _e({props:{name:"set_src_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_src_lang_special_tokens",parameters:[{name:"src_lang",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L262"}}),be=new _e({props:{name:"set_tgt_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_tgt_lang_special_tokens",parameters:[{name:"lang",val:": str"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L285"}}),ye=new Nn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb.md"}}),{c(){t=i("meta"),f=o(),s=i("p"),g=o(),T=i("div"),T.innerHTML=m,p=o(),$=i("p"),$.innerHTML=Ye,Q=o(),h(N.$$.fragment),Ae=o(),D=i("p"),D.innerHTML=At,Qe=o(),P=i("p"),P.innerHTML=Qt,De=o(),h(V.$$.fragment),Pe=o(),K=i("p"),K.innerHTML=Dt,Ke=o(),h(X.$$.fragment),Oe=o(),O=i("p"),O.innerHTML=Pt,et=o(),ee=i("p"),ee.innerHTML=Kt,tt=o(),h(te.$$.fragment),nt=o(),ne=i("p"),ne.innerHTML=Ot,st=o(),h(se.$$.fragment),at=o(),E=i("div"),E.innerHTML=en,lt=o(),h(ae.$$.fragment),ot=o(),I=i("ul"),Z=i("li"),Me=i("p"),Me.textContent=tn,ht=o(),h(le.$$.fragment),bt=o(),Te=i("p"),Te.textContent=nn,yt=o(),h(oe.$$.fragment),kt=o(),we=i("li"),we.innerHTML=sn,_t=o(),re=i("li"),$e=i("p"),$e.textContent=an,Mt=o(),h(ie.$$.fragment),rt=o(),h(ce.$$.fragment),it=o(),J=i("div"),h(de.$$.fragment),Tt=o(),ve=i("p"),ve.textContent=ln,wt=o(),Je=i("p"),Je.innerHTML=on,$t=o(),je=i("p"),je.innerHTML=rn,vt=$n(`
<tokens> <eos>\` for target language documents.
`),h(H.$$.fragment),Jt=o(),z=i("div"),h(pe.$$.fragment),jt=o(),Ue=i("p"),Ue.innerHTML=cn,Ut=o(),xe=i("ul"),xe.innerHTML=dn,xt=o(),ze=i("p"),ze.textContent=pn,ct=o(),h(me.$$.fragment),dt=o(),w=i("div"),h(ue.$$.fragment),zt=o(),Ge=i("p"),Ge.innerHTML=mn,Gt=o(),Ze=i("p"),Ze.innerHTML=un,Zt=o(),qe=i("p"),qe.innerHTML=fn,qt=$n(`
<tokens> <eos>\` for target language documents.
`),h(Y.$$.fragment),Nt=o(),x=i("div"),h(fe.$$.fragment),It=o(),Ne=i("p"),Ne.textContent=gn,Ct=o(),Ie=i("p"),Ie.innerHTML=hn,Wt=o(),Ce=i("ul"),Ce.innerHTML=bn,Rt=o(),We=i("p"),We.textContent=yn,Ft=o(),S=i("div"),h(ge.$$.fragment),Bt=o(),Re=i("p"),Re.textContent=kn,Lt=o(),C=i("div"),h(he.$$.fragment),Vt=o(),Fe=i("p"),Fe.textContent=_n,Xt=o(),Be=i("ul"),Be.innerHTML=Mn,Et=o(),W=i("div"),h(be.$$.fragment),Ht=o(),Le=i("p"),Le.textContent=Tn,Yt=o(),Ve=i("ul"),Ve.innerHTML=wn,pt=o(),h(ye.$$.fragment),mt=o(),Se=i("p"),this.h()},l(e){const a=Gn("svelte-u9bgzb",document.head);t=c(a,"META",{name:!0,content:!0}),a.forEach(l),f=r(e),s=c(e,"P",{}),G(s).forEach(l),g=r(e),T=c(e,"DIV",{style:!0,"data-svelte-h":!0}),u(T)!=="svelte-2m0t7r"&&(T.innerHTML=m),p=r(e),$=c(e,"P",{"data-svelte-h":!0}),u($)!=="svelte-1x0lqm5"&&($.innerHTML=Ye),Q=r(e),b(N.$$.fragment,e),Ae=r(e),D=c(e,"P",{"data-svelte-h":!0}),u(D)!=="svelte-xdg0o1"&&(D.innerHTML=At),Qe=r(e),P=c(e,"P",{"data-svelte-h":!0}),u(P)!=="svelte-1ircyc2"&&(P.innerHTML=Qt),De=r(e),b(V.$$.fragment,e),Pe=r(e),K=c(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-1s7eaah"&&(K.innerHTML=Dt),Ke=r(e),b(X.$$.fragment,e),Oe=r(e),O=c(e,"P",{"data-svelte-h":!0}),u(O)!=="svelte-nf5ooi"&&(O.innerHTML=Pt),et=r(e),ee=c(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-u672qo"&&(ee.innerHTML=Kt),tt=r(e),b(te.$$.fragment,e),nt=r(e),ne=c(e,"P",{"data-svelte-h":!0}),u(ne)!=="svelte-vcd1zv"&&(ne.innerHTML=Ot),st=r(e),b(se.$$.fragment,e),at=r(e),E=c(e,"DIV",{class:!0,"data-svelte-h":!0}),u(E)!=="svelte-vqdv4c"&&(E.innerHTML=en),lt=r(e),b(ae.$$.fragment,e),ot=r(e),I=c(e,"UL",{});var L=G(I);Z=c(L,"LI",{});var q=G(Z);Me=c(q,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-1b470gf"&&(Me.textContent=tn),ht=r(q),b(le.$$.fragment,q),bt=r(q),Te=c(q,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-2yrv9y"&&(Te.textContent=nn),yt=r(q),b(oe.$$.fragment,q),q.forEach(l),kt=r(L),we=c(L,"LI",{"data-svelte-h":!0}),u(we)!=="svelte-roref4"&&(we.innerHTML=sn),_t=r(L),re=c(L,"LI",{});var ke=G(re);$e=c(ke,"P",{"data-svelte-h":!0}),u($e)!=="svelte-1ujz84n"&&($e.textContent=an),Mt=r(ke),b(ie.$$.fragment,ke),ke.forEach(l),L.forEach(l),rt=r(e),b(ce.$$.fragment,e),it=r(e),J=c(e,"DIV",{class:!0});var U=G(J);b(de.$$.fragment,U),Tt=r(U),ve=c(U,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-7mmiyn"&&(ve.textContent=ln),wt=r(U),Je=c(U,"P",{"data-svelte-h":!0}),u(Je)!=="svelte-19vr0qz"&&(Je.innerHTML=on),$t=r(U),je=c(U,"P",{"data-svelte-h":!0}),u(je)!=="svelte-1i8rh37"&&(je.innerHTML=rn),vt=vn(U,`
<tokens> <eos>\` for target language documents.
`),b(H.$$.fragment,U),Jt=r(U),z=c(U,"DIV",{class:!0});var A=G(z);b(pe.$$.fragment,A),jt=r(A),Ue=c(A,"P",{"data-svelte-h":!0}),u(Ue)!=="svelte-nkpot4"&&(Ue.innerHTML=cn),Ut=r(A),xe=c(A,"UL",{"data-svelte-h":!0}),u(xe)!=="svelte-mlrsks"&&(xe.innerHTML=dn),xt=r(A),ze=c(A,"P",{"data-svelte-h":!0}),u(ze)!=="svelte-46aam0"&&(ze.textContent=pn),A.forEach(l),U.forEach(l),ct=r(e),b(me.$$.fragment,e),dt=r(e),w=c(e,"DIV",{class:!0});var v=G(w);b(ue.$$.fragment,v),zt=r(v),Ge=c(v,"P",{"data-svelte-h":!0}),u(Ge)!=="svelte-g5z8ln"&&(Ge.innerHTML=mn),Gt=r(v),Ze=c(v,"P",{"data-svelte-h":!0}),u(Ze)!=="svelte-gxzj9w"&&(Ze.innerHTML=un),Zt=r(v),qe=c(v,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-1i8rh37"&&(qe.innerHTML=fn),qt=vn(v,`
<tokens> <eos>\` for target language documents.
`),b(Y.$$.fragment,v),Nt=r(v),x=c(v,"DIV",{class:!0});var R=G(x);b(fe.$$.fragment,R),It=r(R),Ne=c(R,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-1vll0v2"&&(Ne.textContent=gn),Ct=r(R),Ie=c(R,"P",{"data-svelte-h":!0}),u(Ie)!=="svelte-90np8u"&&(Ie.innerHTML=hn),Wt=r(R),Ce=c(R,"UL",{"data-svelte-h":!0}),u(Ce)!=="svelte-mlrsks"&&(Ce.innerHTML=bn),Rt=r(R),We=c(R,"P",{"data-svelte-h":!0}),u(We)!=="svelte-46aam0"&&(We.textContent=yn),R.forEach(l),Ft=r(v),S=c(v,"DIV",{class:!0});var ft=G(S);b(ge.$$.fragment,ft),Bt=r(ft),Re=c(ft,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-1bcwf97"&&(Re.textContent=kn),ft.forEach(l),Lt=r(v),C=c(v,"DIV",{class:!0});var Xe=G(C);b(he.$$.fragment,Xe),Vt=r(Xe),Fe=c(Xe,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-1euodjq"&&(Fe.textContent=_n),Xt=r(Xe),Be=c(Xe,"UL",{"data-svelte-h":!0}),u(Be)!=="svelte-rrgzfj"&&(Be.innerHTML=Mn),Xe.forEach(l),Et=r(v),W=c(v,"DIV",{class:!0});var Ee=G(W);b(be.$$.fragment,Ee),Ht=r(Ee),Le=c(Ee,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-1i6tlcm"&&(Le.textContent=Tn),Yt=r(Ee),Ve=c(Ee,"UL",{"data-svelte-h":!0}),u(Ve)!=="svelte-1durotj"&&(Ve.innerHTML=wn),Ee.forEach(l),v.forEach(l),pt=r(e),b(ye.$$.fragment,e),mt=r(e),Se=c(e,"P",{}),G(Se).forEach(l),this.h()},h(){F(t,"name","hf:doc:metadata"),F(t,"content",En),Zn(T,"float","right"),F(E,"class","flex justify-center"),F(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){n(document.head,t),d(e,f,a),d(e,s,a),d(e,g,a),d(e,T,a),d(e,p,a),d(e,$,a),d(e,Q,a),y(N,e,a),d(e,Ae,a),d(e,D,a),d(e,Qe,a),d(e,P,a),d(e,De,a),y(V,e,a),d(e,Pe,a),d(e,K,a),d(e,Ke,a),y(X,e,a),d(e,Oe,a),d(e,O,a),d(e,et,a),d(e,ee,a),d(e,tt,a),y(te,e,a),d(e,nt,a),d(e,ne,a),d(e,st,a),y(se,e,a),d(e,at,a),d(e,E,a),d(e,lt,a),y(ae,e,a),d(e,ot,a),d(e,I,a),n(I,Z),n(Z,Me),n(Z,ht),y(le,Z,null),n(Z,bt),n(Z,Te),n(Z,yt),y(oe,Z,null),n(I,kt),n(I,we),n(I,_t),n(I,re),n(re,$e),n(re,Mt),y(ie,re,null),d(e,rt,a),y(ce,e,a),d(e,it,a),d(e,J,a),y(de,J,null),n(J,Tt),n(J,ve),n(J,wt),n(J,Je),n(J,$t),n(J,je),n(J,vt),y(H,J,null),n(J,Jt),n(J,z),y(pe,z,null),n(z,jt),n(z,Ue),n(z,Ut),n(z,xe),n(z,xt),n(z,ze),d(e,ct,a),y(me,e,a),d(e,dt,a),d(e,w,a),y(ue,w,null),n(w,zt),n(w,Ge),n(w,Gt),n(w,Ze),n(w,Zt),n(w,qe),n(w,qt),y(Y,w,null),n(w,Nt),n(w,x),y(fe,x,null),n(x,It),n(x,Ne),n(x,Ct),n(x,Ie),n(x,Wt),n(x,Ce),n(x,Rt),n(x,We),n(w,Ft),n(w,S),y(ge,S,null),n(S,Bt),n(S,Re),n(w,Lt),n(w,C),y(he,C,null),n(C,Vt),n(C,Fe),n(C,Xt),n(C,Be),n(w,Et),n(w,W),y(be,W,null),n(W,Ht),n(W,Le),n(W,Yt),n(W,Ve),d(e,pt,a),y(ye,e,a),d(e,mt,a),d(e,Se,a),ut=!0},p(e,[a]){const L={};a&2&&(L.$$scope={dirty:a,ctx:e}),V.$set(L);const q={};a&2&&(q.$$scope={dirty:a,ctx:e}),X.$set(q);const ke={};a&2&&(ke.$$scope={dirty:a,ctx:e}),H.$set(ke);const U={};a&2&&(U.$$scope={dirty:a,ctx:e}),Y.$set(U)},i(e){ut||(k(N.$$.fragment,e),k(V.$$.fragment,e),k(X.$$.fragment,e),k(te.$$.fragment,e),k(se.$$.fragment,e),k(ae.$$.fragment,e),k(le.$$.fragment,e),k(oe.$$.fragment,e),k(ie.$$.fragment,e),k(ce.$$.fragment,e),k(de.$$.fragment,e),k(H.$$.fragment,e),k(pe.$$.fragment,e),k(me.$$.fragment,e),k(ue.$$.fragment,e),k(Y.$$.fragment,e),k(fe.$$.fragment,e),k(ge.$$.fragment,e),k(he.$$.fragment,e),k(be.$$.fragment,e),k(ye.$$.fragment,e),ut=!0)},o(e){_(N.$$.fragment,e),_(V.$$.fragment,e),_(X.$$.fragment,e),_(te.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(le.$$.fragment,e),_(oe.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(H.$$.fragment,e),_(pe.$$.fragment,e),_(me.$$.fragment,e),_(ue.$$.fragment,e),_(Y.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(he.$$.fragment,e),_(be.$$.fragment,e),_(ye.$$.fragment,e),ut=!1},d(e){e&&(l(f),l(s),l(g),l(T),l(p),l($),l(Q),l(Ae),l(D),l(Qe),l(P),l(De),l(Pe),l(K),l(Ke),l(Oe),l(O),l(et),l(ee),l(tt),l(nt),l(ne),l(st),l(at),l(E),l(lt),l(ot),l(I),l(rt),l(it),l(J),l(ct),l(dt),l(w),l(pt),l(mt),l(Se)),l(t),M(N,e),M(V,e),M(X,e),M(te,e),M(se,e),M(ae,e),M(le),M(oe),M(ie),M(ce,e),M(de),M(H),M(pe),M(me,e),M(ue),M(Y),M(fe),M(ge),M(he),M(be),M(ye,e)}}}const En='{"title":"NLLB","local":"nllb","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"NllbTokenizer","local":"transformers.NllbTokenizer","sections":[],"depth":2},{"title":"NllbTokenizerFast","local":"transformers.NllbTokenizerFast","sections":[],"depth":2}],"depth":1}';function Hn(j){return Un(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class es extends xn{constructor(t){super(),zn(this,t,Hn,Xn,jn,{})}}export{es as component};
