import{s as Et,o as Ct,n as Le}from"../chunks/scheduler.18a86fab.js";import{S as $t,i as It,g as c,s as a,r as h,A as Gt,h as l,f as o,c as r,j as pe,x as M,u as g,k as N,y as p,a as s,v as u,d as f,t as y,w as _}from"../chunks/index.98837b22.js";import{T as Rt}from"../chunks/Tip.77304350.js";import{D as Ze}from"../chunks/Docstring.a1ef7999.js";import{C as he}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as mt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as me,E as St}from"../chunks/getInferenceSnippets.06c2775f.js";function Nt(V){let d,v="Examples:",m,i,b;return i=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRDb25maWclMkMlMjBXYXYyVmVjMkNvbmZpZyUyQyUyMFNwZWVjaEVuY29kZXJEZWNvZGVyQ29uZmlnJTJDJTIwU3BlZWNoRW5jb2RlckRlY29kZXJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBXYXYyVmVjMiUyMCUyNiUyMEJFUlQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWdfZW5jb2RlciUyMCUzRCUyMFdhdjJWZWMyQ29uZmlnKCklMEFjb25maWdfZGVjb2RlciUyMCUzRCUyMEJlcnRDb25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyQ29uZmlnLmZyb21fZW5jb2Rlcl9kZWNvZGVyX2NvbmZpZ3MoY29uZmlnX2VuY29kZXIlMkMlMjBjb25maWdfZGVjb2RlciklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwV2F2MlZlYzJCZXJ0JTIwbW9kZWwlMjBmcm9tJTIwYSUyMFdhdjJWZWMyJTIwJTI2JTIwZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbnMlMEFtb2RlbCUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwoY29uZmlnJTNEY29uZmlnKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ19lbmNvZGVyJTIwJTNEJTIwbW9kZWwuY29uZmlnLmVuY29kZXIlMEFjb25maWdfZGVjb2RlciUyMCUzRCUyMG1vZGVsLmNvbmZpZy5kZWNvZGVyJTBBJTIzJTIwc2V0JTIwZGVjb2RlciUyMGNvbmZpZyUyMHRvJTIwY2F1c2FsJTIwbG0lMEFjb25maWdfZGVjb2Rlci5pc19kZWNvZGVyJTIwJTNEJTIwVHJ1ZSUwQWNvbmZpZ19kZWNvZGVyLmFkZF9jcm9zc19hdHRlbnRpb24lMjAlM0QlMjBUcnVlJTBBJTBBJTIzJTIwU2F2aW5nJTIwdGhlJTIwbW9kZWwlMkMlMjBpbmNsdWRpbmclMjBpdHMlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMm15LW1vZGVsJTIyKSUwQSUwQSUyMyUyMGxvYWRpbmclMjBtb2RlbCUyMGFuZCUyMGNvbmZpZyUyMGZyb20lMjBwcmV0cmFpbmVkJTIwZm9sZGVyJTBBZW5jb2Rlcl9kZWNvZGVyX2NvbmZpZyUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJteS1tb2RlbCUyMiklMEFtb2RlbCUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm15LW1vZGVsJTIyJTJDJTIwY29uZmlnJTNEZW5jb2Rlcl9kZWNvZGVyX2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Wav2Vec2 &amp; BERT style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = Wav2Vec2Config()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Wav2Vec2Bert model from a Wav2Vec2 &amp; google-bert/bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>, config=encoder_decoder_config)`,wrap:!1}}),{c(){d=c("p"),d.textContent=v,m=a(),h(i.$$.fragment)},l(n){d=l(n,"P",{"data-svelte-h":!0}),M(d)!=="svelte-kvfsh7"&&(d.textContent=v),m=r(n),g(i.$$.fragment,n)},m(n,w){s(n,d,w),s(n,m,w),u(i,n,w),b=!0},p:Le,i(n){b||(f(i.$$.fragment,n),b=!0)},o(n){y(i.$$.fragment,n),b=!1},d(n){n&&(o(d),o(m)),_(i,n)}}}function Ft(V){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=c("p"),d.innerHTML=v},l(m){d=l(m,"P",{"data-svelte-h":!0}),M(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(m,i){s(m,d,i)},p:Le,d(m){m&&o(d)}}}function xt(V){let d,v="Examples:",m,i,b;return i=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwlMkMlMjBBdXRvUHJvY2Vzc29yJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBaW1wb3J0JTIwdG9yY2glMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRndhdjJ2ZWMyLXhscy1yLTMwMG0tZW4tdG8tMTUlMjIpJTBBbW9kZWwlMjAlM0QlMjBTcGVlY2hFbmNvZGVyRGVjb2Rlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRndhdjJ2ZWMyLXhscy1yLTMwMG0tZW4tdG8tMTUlMjIpJTBBJTBBZHMlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQSUwQWlucHV0X3ZhbHVlcyUyMCUzRCUyMHByb2Nlc3NvcihkcyU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikuaW5wdXRfdmFsdWVzJTBBJTIzJTIwSW5mZXJlbmNlJTNBJTIwVHJhbnNsYXRlJTIwRW5nbGlzaCUyMHNwZWVjaCUyMHRvJTIwR2VybWFuJTBBZ2VuZXJhdGVkJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRfdmFsdWVzKSUwQWRlY29kZWQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEFkZWNvZGVkJTBBJTBBJTIzJTIwVHJhaW5pbmclM0ElMjBUcmFpbiUyMG1vZGVsJTIwb24lMjBFbmdsaXNoJTIwdHJhbnNjcmlwdGlvbiUwQWxhYmVscyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEZHMlNUIwJTVEJTVCJTIydGV4dCUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLmlucHV0X2lkcyUwQSUwQWxvc3MlMjAlM0QlMjBtb2RlbChpbnB1dF92YWx1ZXMlMkMlMjBsYWJlbHMlM0RsYWJlbHMpLmxvc3MlMEFsb3NzLmJhY2t3YXJkKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SpeechEncoderDecoderModel, AutoProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-xls-r-300m-en-to-15&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-xls-r-300m-en-to-15&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = processor(ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Inference: Translate English speech to German</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(input_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>decoded = processor.batch_decode(generated, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>decoded
<span class="hljs-string">&#x27;Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heißen zu können.&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Training: Train model on English transcription</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = processor(text=ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(input_values, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`,wrap:!1}}),{c(){d=c("p"),d.textContent=v,m=a(),h(i.$$.fragment)},l(n){d=l(n,"P",{"data-svelte-h":!0}),M(d)!=="svelte-kvfsh7"&&(d.textContent=v),m=r(n),g(i.$$.fragment,n)},m(n,w){s(n,d,w),s(n,m,w),u(i,n,w),b=!0},p:Le,i(n){b||(f(i.$$.fragment,n),b=!0)},o(n){y(i.$$.fragment,n),b=!1},d(n){n&&(o(d),o(m)),_(i,n)}}}function Xt(V){let d,v="Example:",m,i,b;return i=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwlMEElMEElMjMlMjBpbml0aWFsaXplJTIwYSUyMHdhdjJ2ZWMyYmVydCUyMGZyb20lMjBhJTIwcHJldHJhaW5lZCUyMFdhdjJWZWMyJTIwYW5kJTIwYSUyMHByZXRyYWluZWQlMjBCRVJUJTIwbW9kZWwuJTIwTm90ZSUyMHRoYXQlMjB0aGUlMjBjcm9zcy1hdHRlbnRpb24lMjBsYXllcnMlMjB3aWxsJTIwYmUlMjByYW5kb21seSUyMGluaXRpYWxpemVkJTBBbW9kZWwlMjAlM0QlMjBTcGVlY2hFbmNvZGVyRGVjb2Rlck1vZGVsLmZyb21fZW5jb2Rlcl9kZWNvZGVyX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIlMkMlMjAlMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyJTBBKSUwQSUyMyUyMHNhdmluZyUyMG1vZGVsJTIwYWZ0ZXIlMjBmaW5lLXR1bmluZyUwQW1vZGVsLnNhdmVfcHJldHJhaW5lZCglMjIuJTJGd2F2MnZlYzJiZXJ0JTIyKSUwQSUyMyUyMGxvYWQlMjBmaW5lLXR1bmVkJTIwbW9kZWwlMEFtb2RlbCUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ3YXYydmVjMmJlcnQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SpeechEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a wav2vec2bert from a pretrained Wav2Vec2 and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./wav2vec2bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./wav2vec2bert&quot;</span>)`,wrap:!1}}),{c(){d=c("p"),d.textContent=v,m=a(),h(i.$$.fragment)},l(n){d=l(n,"P",{"data-svelte-h":!0}),M(d)!=="svelte-11lpom8"&&(d.textContent=v),m=r(n),g(i.$$.fragment,n)},m(n,w){s(n,d,w),s(n,m,w),u(i,n,w),b=!0},p:Le,i(n){b||(f(i.$$.fragment,n),b=!0)},o(n){y(i.$$.fragment,n),b=!1},d(n){n&&(o(d),o(m)),_(i,n)}}}function Yt(V){let d,v,m,i,b,n="<em>This model was released on 2021-04-14 and added to Hugging Face Transformers on 2021-09-01.</em>",w,F,je,E,ht='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Je,x,gt=`The <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> can be used to initialize a speech-to-text model
with any pretrained speech autoencoding model as the encoder (<em>e.g.</em> <a href="wav2vec2">Wav2Vec2</a>, <a href="hubert">Hubert</a>) and any pretrained autoregressive model as the decoder.`,We,X,ut=`The effectiveness of initializing speech-sequence-to-text-sequence models with pretrained checkpoints for speech
recognition and speech translation has <em>e.g.</em> been shown in <a href="https://huggingface.co/papers/2104.06678" rel="nofollow">Large-Scale Self- and Semi-Supervised Learning for Speech
Translation</a> by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,
Alexis Conneau.`,ke,Y,ft='An example of how to use a <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> for inference can be seen in <a href="speech_to_text_2">Speech2Text2</a>.',Ue,B,Ve,z,yt=`<a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> configuration for the encoder
and the default <code>BertForCausalLM</code> configuration for the decoder.`,Ee,D,Ce,H,$e,q,_t=`<a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based speech model, <em>e.g.</em> <a href="wav2vec2">Wav2Vec2</a>, <a href="hubert">Hubert</a> can serve as the encoder and both pretrained auto-encoding models, <em>e.g.</em> BERT, pretrained causal language models, <em>e.g.</em> GPT2, as well as the pretrained decoder part of sequence-to-sequence models, <em>e.g.</em> decoder of BART, can be used as the decoder.
Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.
Initializing <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in <a href="https://huggingface.co/blog/warm-starting-encoder-decoder" rel="nofollow">the <em>Warm-starting-encoder-decoder blog post</em></a>.
To do so, the <code>SpeechEncoderDecoderModel</code> class provides a <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained">SpeechEncoderDecoderModel.from_encoder_decoder_pretrained()</a> method.`,Ie,Q,Ge,L,Re,P,Mt='To load fine-tuned checkpoints of the <code>SpeechEncoderDecoderModel</code> class, <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> provides the <code>from_pretrained(...)</code> method just like any other model architecture in Transformers.',Se,A,bt="To perform inference, one uses the <code>generate</code> method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling.",Ne,O,Fe,K,xe,ee,Tt=`Once the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model on a dataset of (speech, text) pairs.
As you can see, only 2 inputs are required for the model in order to compute a loss: <code>input_values</code> (which are the
speech inputs) and <code>labels</code> (which are the <code>input_ids</code> of the encoded target sequence).`,Xe,te,Ye,oe,Be,Z,ne,Pe,ge,vt=`<a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> is the configuration class to store the configuration of a
<a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a>. It is used to instantiate an Encoder Decoder model according to the specified
arguments, defining the encoder and decoder configs.`,Ae,ue,wt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Oe,C,Ke,$,se,et,fe,Zt=`Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`,ze,ae,De,T,re,tt,ye,jt="The bare Speech Encoder Decoder Model outputting raw hidden-states without any specific head on top.",ot,_e,Jt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,nt,Me,Wt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,st,W,de,at,be,kt='The <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> forward method, overrides the <code>__call__</code> special method.',rt,I,dt,G,ct,k,ce,lt,Te,Ut=`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`,it,ve,Vt=`The model is set in evaluation mode by default using <code>model.eval()</code> (Dropout modules are deactivated). To train
the model, you need to first set it back in training mode with <code>model.train()</code>.`,pt,R,He,le,qe,we,Qe;return F=new me({props:{title:"Speech Encoder Decoder Models",local:"speech-encoder-decoder-models",headingTag:"h1"}}),B=new me({props:{title:"Randomly initializing SpeechEncoderDecoderModel from model configurations.",local:"randomly-initializing-speechencoderdecodermodel-from-model-configurations",headingTag:"h2"}}),D=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRDb25maWclMkMlMjBXYXYyVmVjMkNvbmZpZyUyQyUyMFNwZWVjaEVuY29kZXJEZWNvZGVyQ29uZmlnJTJDJTIwU3BlZWNoRW5jb2RlckRlY29kZXJNb2RlbCUwQSUwQWNvbmZpZ19lbmNvZGVyJTIwJTNEJTIwV2F2MlZlYzJDb25maWcoKSUwQWNvbmZpZ19kZWNvZGVyJTIwJTNEJTIwQmVydENvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwU3BlZWNoRW5jb2RlckRlY29kZXJDb25maWcuZnJvbV9lbmNvZGVyX2RlY29kZXJfY29uZmlncyhjb25maWdfZW5jb2RlciUyQyUyMGNvbmZpZ19kZWNvZGVyKSUwQW1vZGVsJTIwJTNEJTIwU3BlZWNoRW5jb2RlckRlY29kZXJNb2RlbChjb25maWclM0Rjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = Wav2Vec2Config()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel(config=config)`,wrap:!1}}),H=new me({props:{title:"Initialising SpeechEncoderDecoderModel from a pretrained encoder and a pretrained decoder.",local:"initialising-speechencoderdecodermodel-from-a-pretrained-encoder-and-a-pretrained-decoder",headingTag:"h2"}}),Q=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwuZnJvbV9lbmNvZGVyX2RlY29kZXJfcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRmh1YmVydC1sYXJnZS1sbDYwayUyMiUyQyUyMCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLXVuY2FzZWQlMjIlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SpeechEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/hubert-large-ll60k&quot;</span>, <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)`,wrap:!1}}),L=new me({props:{title:"Loading an existing SpeechEncoderDecoderModel checkpoint and perform inference.",local:"loading-an-existing-speechencoderdecodermodel-checkpoint-and-perform-inference",headingTag:"h2"}}),O=new he({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFdhdjJWZWMyUHJvY2Vzc29yJTJDJTIwU3BlZWNoRW5jb2RlckRlY29kZXJNb2RlbCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBJTIzJTIwbG9hZCUyMGElMjBmaW5lLXR1bmVkJTIwc3BlZWNoJTIwdHJhbnNsYXRpb24lMjBtb2RlbCUyMGFuZCUyMGNvcnJlc3BvbmRpbmclMjBwcm9jZXNzb3IlMEFtb2RlbCUyMCUzRCUyMFNwZWVjaEVuY29kZXJEZWNvZGVyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzIteGxzLXItMzAwbS1lbi10by0xNSUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBXYXYyVmVjMlByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi14bHMtci0zMDBtLWVuLXRvLTE1JTIyKSUwQSUwQSUyMyUyMGxldCdzJTIwcGVyZm9ybSUyMGluZmVyZW5jZSUyMG9uJTIwYSUyMHBpZWNlJTIwb2YlMjBFbmdsaXNoJTIwc3BlZWNoJTIwKHdoaWNoJTIwd2UnbGwlMjB0cmFuc2xhdGUlMjB0byUyMEdlcm1hbiklMEFkcyUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2R1bW15JTIyJTJDJTIwJTIyY2xlYW4lMjIlMkMlMjBzcGxpdCUzRCUyMnZhbGlkYXRpb24lMjIpJTBBaW5wdXRfdmFsdWVzJTIwJTNEJTIwcHJvY2Vzc29yKGRzJTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5pbnB1dF92YWx1ZXMlMEElMEElMjMlMjBhdXRvcmVncmVzc2l2ZWx5JTIwZ2VuZXJhdGUlMjB0cmFuc2NyaXB0aW9uJTIwKHVzZXMlMjBncmVlZHklMjBkZWNvZGluZyUyMGJ5JTIwZGVmYXVsdCklMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRfdmFsdWVzKSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQXByaW50KGdlbmVyYXRlZF90ZXh0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, SpeechEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load a fine-tuned speech translation model and corresponding processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-xls-r-300m-en-to-15&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-xls-r-300m-en-to-15&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># let&#x27;s perform inference on a piece of English speech (which we&#x27;ll translate to German)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = processor(ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># autoregressively generate transcription (uses greedy decoding by default)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(input_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heißen zu können.`,wrap:!1}}),K=new me({props:{title:"Training",local:"training",headingTag:"h2"}}),te=new he({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_id = <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>  <span class="hljs-comment"># acoustic model encoder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_id = <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>  <span class="hljs-comment"># text decoder</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(decoder_id)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)

<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = tokenizer.pad_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load an audio input and pre-process (normalise mean/std to 0/1)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = feature_extractor(ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load its corresponding transcription and tokenize to generate labels</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the forward function automatically creates the correct decoder_input_ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(input_values=input_values, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`,wrap:!1}}),oe=new me({props:{title:"SpeechEncoderDecoderConfig",local:"transformers.SpeechEncoderDecoderConfig",headingTag:"h2"}}),ne=new Ze({props:{name:"class transformers.SpeechEncoderDecoderConfig",anchor:"transformers.SpeechEncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SpeechEncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the decoder config.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L26"}}),C=new mt({props:{anchor:"transformers.SpeechEncoderDecoderConfig.example",$$slots:{default:[Nt]},$$scope:{ctx:V}}}),se=new Ze({props:{name:"from_encoder_decoder_configs",anchor:"transformers.SpeechEncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py#L94",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"
>SpeechEncoderDecoderConfig</a></p>
`}}),ae=new me({props:{title:"SpeechEncoderDecoderModel",local:"transformers.SpeechEncoderDecoderModel",headingTag:"h2"}}),re=new Ze({props:{name:"class transformers.SpeechEncoderDecoderModel",anchor:"transformers.SpeechEncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],parametersDescription:[{anchor:"transformers.SpeechEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.SpeechEncoderDecoderModel.encoder",description:`<strong>encoder</strong> (<code>PreTrainedModel</code>, <em>optional</em>) &#x2014;
The encoder model to use.`,name:"encoder"},{anchor:"transformers.SpeechEncoderDecoderModel.decoder",description:`<strong>decoder</strong> (<code>PreTrainedModel</code>, <em>optional</em>) &#x2014;
The decoder model to use.`,name:"decoder"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L56"}}),de=new Ze({props:{name:"forward",anchor:"transformers.SpeechEncoderDecoderModel.forward",parameters:[{name:"inputs",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"encoder_outputs",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[tuple[tuple[torch.FloatTensor]]] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"input_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_features",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SpeechEncoderDecoderModel.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code> or <code>(batch_size, sequence_length, feature_dim)</code>, <em>optional</em>) &#x2014;
Float values of input raw speech waveform or speech features. Values can be obtained by loading a <code>.flac</code>
or <code>.wav</code> audio file into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em>
via the torchcodec library (<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>inputs</code>, either the <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> or
<a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor">Speech2TextProcessor</a> should be used for padding and conversion into a tensor of type
<code>torch.FloatTensor</code>.`,name:"inputs"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code> to the
right, replacing -100 by the <code>pad_token_id</code> and prepending them with the <code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[tuple[torch.FloatTensor]]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>list[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <em>input_values</em>, the <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> should be used for padding and conversion
into a tensor of type <em>torch.FloatTensor</em>. See <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__">Wav2Vec2Processor.<strong>call</strong>()</a> for details.`,name:"input_values"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.input_features",description:`<strong>input_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, feature_dim)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input audio features. Audio features can be obtained using
<code>feature_extractor_class</code>. See <code>feature_extractor_class.__call__</code> for details (<code>processor_class</code> uses
<code>feature_extractor_class</code> for processing audios).`,name:"input_features"},{anchor:"transformers.SpeechEncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L317",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"
>SpeechEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache"
>EncoderDecoderCache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),I=new Rt({props:{$$slots:{default:[Ft]},$$scope:{ctx:V}}}),G=new mt({props:{anchor:"transformers.SpeechEncoderDecoderModel.forward.example",$$slots:{default:[xt]},$$scope:{ctx:V}}}),ce=new Ze({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Optional[str] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Optional[str] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Information necessary to initiate the decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaining positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py#L166"}}),R=new mt({props:{anchor:"transformers.SpeechEncoderDecoderModel.from_encoder_decoder_pretrained.example",$$slots:{default:[Xt]},$$scope:{ctx:V}}}),le=new St({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech-encoder-decoder.md"}}),{c(){d=c("meta"),v=a(),m=c("p"),i=a(),b=c("p"),b.innerHTML=n,w=a(),h(F.$$.fragment),je=a(),E=c("div"),E.innerHTML=ht,Je=a(),x=c("p"),x.innerHTML=gt,We=a(),X=c("p"),X.innerHTML=ut,ke=a(),Y=c("p"),Y.innerHTML=ft,Ue=a(),h(B.$$.fragment),Ve=a(),z=c("p"),z.innerHTML=yt,Ee=a(),h(D.$$.fragment),Ce=a(),h(H.$$.fragment),$e=a(),q=c("p"),q.innerHTML=_t,Ie=a(),h(Q.$$.fragment),Ge=a(),h(L.$$.fragment),Re=a(),P=c("p"),P.innerHTML=Mt,Se=a(),A=c("p"),A.innerHTML=bt,Ne=a(),h(O.$$.fragment),Fe=a(),h(K.$$.fragment),xe=a(),ee=c("p"),ee.innerHTML=Tt,Xe=a(),h(te.$$.fragment),Ye=a(),h(oe.$$.fragment),Be=a(),Z=c("div"),h(ne.$$.fragment),Pe=a(),ge=c("p"),ge.innerHTML=vt,Ae=a(),ue=c("p"),ue.innerHTML=wt,Oe=a(),h(C.$$.fragment),Ke=a(),$=c("div"),h(se.$$.fragment),et=a(),fe=c("p"),fe.innerHTML=Zt,ze=a(),h(ae.$$.fragment),De=a(),T=c("div"),h(re.$$.fragment),tt=a(),ye=c("p"),ye.textContent=jt,ot=a(),_e=c("p"),_e.innerHTML=Jt,nt=a(),Me=c("p"),Me.innerHTML=Wt,st=a(),W=c("div"),h(de.$$.fragment),at=a(),be=c("p"),be.innerHTML=kt,rt=a(),h(I.$$.fragment),dt=a(),h(G.$$.fragment),ct=a(),k=c("div"),h(ce.$$.fragment),lt=a(),Te=c("p"),Te.textContent=Ut,it=a(),ve=c("p"),ve.innerHTML=Vt,pt=a(),h(R.$$.fragment),He=a(),h(le.$$.fragment),qe=a(),we=c("p"),this.h()},l(e){const t=Gt("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),v=r(e),m=l(e,"P",{}),pe(m).forEach(o),i=r(e),b=l(e,"P",{"data-svelte-h":!0}),M(b)!=="svelte-hfny2j"&&(b.innerHTML=n),w=r(e),g(F.$$.fragment,e),je=r(e),E=l(e,"DIV",{class:!0,"data-svelte-h":!0}),M(E)!=="svelte-b95w5j"&&(E.innerHTML=ht),Je=r(e),x=l(e,"P",{"data-svelte-h":!0}),M(x)!=="svelte-3pkzr4"&&(x.innerHTML=gt),We=r(e),X=l(e,"P",{"data-svelte-h":!0}),M(X)!=="svelte-1q8hunb"&&(X.innerHTML=ut),ke=r(e),Y=l(e,"P",{"data-svelte-h":!0}),M(Y)!=="svelte-1piu1lu"&&(Y.innerHTML=ft),Ue=r(e),g(B.$$.fragment,e),Ve=r(e),z=l(e,"P",{"data-svelte-h":!0}),M(z)!=="svelte-hx3ba6"&&(z.innerHTML=yt),Ee=r(e),g(D.$$.fragment,e),Ce=r(e),g(H.$$.fragment,e),$e=r(e),q=l(e,"P",{"data-svelte-h":!0}),M(q)!=="svelte-1x8zq9a"&&(q.innerHTML=_t),Ie=r(e),g(Q.$$.fragment,e),Ge=r(e),g(L.$$.fragment,e),Re=r(e),P=l(e,"P",{"data-svelte-h":!0}),M(P)!=="svelte-1n73hq5"&&(P.innerHTML=Mt),Se=r(e),A=l(e,"P",{"data-svelte-h":!0}),M(A)!=="svelte-otiwkm"&&(A.innerHTML=bt),Ne=r(e),g(O.$$.fragment,e),Fe=r(e),g(K.$$.fragment,e),xe=r(e),ee=l(e,"P",{"data-svelte-h":!0}),M(ee)!=="svelte-1mtzhr7"&&(ee.innerHTML=Tt),Xe=r(e),g(te.$$.fragment,e),Ye=r(e),g(oe.$$.fragment,e),Be=r(e),Z=l(e,"DIV",{class:!0});var J=pe(Z);g(ne.$$.fragment,J),Pe=r(J),ge=l(J,"P",{"data-svelte-h":!0}),M(ge)!=="svelte-1obpviu"&&(ge.innerHTML=vt),Ae=r(J),ue=l(J,"P",{"data-svelte-h":!0}),M(ue)!=="svelte-1ek1ss9"&&(ue.innerHTML=wt),Oe=r(J),g(C.$$.fragment,J),Ke=r(J),$=l(J,"DIV",{class:!0});var ie=pe($);g(se.$$.fragment,ie),et=r(ie),fe=l(ie,"P",{"data-svelte-h":!0}),M(fe)!=="svelte-vnx6bt"&&(fe.innerHTML=Zt),ie.forEach(o),J.forEach(o),ze=r(e),g(ae.$$.fragment,e),De=r(e),T=l(e,"DIV",{class:!0});var j=pe(T);g(re.$$.fragment,j),tt=r(j),ye=l(j,"P",{"data-svelte-h":!0}),M(ye)!=="svelte-1fcfhqe"&&(ye.textContent=jt),ot=r(j),_e=l(j,"P",{"data-svelte-h":!0}),M(_e)!=="svelte-q52n56"&&(_e.innerHTML=Jt),nt=r(j),Me=l(j,"P",{"data-svelte-h":!0}),M(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Wt),st=r(j),W=l(j,"DIV",{class:!0});var U=pe(W);g(de.$$.fragment,U),at=r(U),be=l(U,"P",{"data-svelte-h":!0}),M(be)!=="svelte-1a7wge9"&&(be.innerHTML=kt),rt=r(U),g(I.$$.fragment,U),dt=r(U),g(G.$$.fragment,U),U.forEach(o),ct=r(j),k=l(j,"DIV",{class:!0});var S=pe(k);g(ce.$$.fragment,S),lt=r(S),Te=l(S,"P",{"data-svelte-h":!0}),M(Te)!=="svelte-n4p3zm"&&(Te.textContent=Ut),it=r(S),ve=l(S,"P",{"data-svelte-h":!0}),M(ve)!=="svelte-ce5sus"&&(ve.innerHTML=Vt),pt=r(S),g(R.$$.fragment,S),S.forEach(o),j.forEach(o),He=r(e),g(le.$$.fragment,e),qe=r(e),we=l(e,"P",{}),pe(we).forEach(o),this.h()},h(){N(d,"name","hf:doc:metadata"),N(d,"content",Bt),N(E,"class","flex flex-wrap space-x-1"),N($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,d),s(e,v,t),s(e,m,t),s(e,i,t),s(e,b,t),s(e,w,t),u(F,e,t),s(e,je,t),s(e,E,t),s(e,Je,t),s(e,x,t),s(e,We,t),s(e,X,t),s(e,ke,t),s(e,Y,t),s(e,Ue,t),u(B,e,t),s(e,Ve,t),s(e,z,t),s(e,Ee,t),u(D,e,t),s(e,Ce,t),u(H,e,t),s(e,$e,t),s(e,q,t),s(e,Ie,t),u(Q,e,t),s(e,Ge,t),u(L,e,t),s(e,Re,t),s(e,P,t),s(e,Se,t),s(e,A,t),s(e,Ne,t),u(O,e,t),s(e,Fe,t),u(K,e,t),s(e,xe,t),s(e,ee,t),s(e,Xe,t),u(te,e,t),s(e,Ye,t),u(oe,e,t),s(e,Be,t),s(e,Z,t),u(ne,Z,null),p(Z,Pe),p(Z,ge),p(Z,Ae),p(Z,ue),p(Z,Oe),u(C,Z,null),p(Z,Ke),p(Z,$),u(se,$,null),p($,et),p($,fe),s(e,ze,t),u(ae,e,t),s(e,De,t),s(e,T,t),u(re,T,null),p(T,tt),p(T,ye),p(T,ot),p(T,_e),p(T,nt),p(T,Me),p(T,st),p(T,W),u(de,W,null),p(W,at),p(W,be),p(W,rt),u(I,W,null),p(W,dt),u(G,W,null),p(T,ct),p(T,k),u(ce,k,null),p(k,lt),p(k,Te),p(k,it),p(k,ve),p(k,pt),u(R,k,null),s(e,He,t),u(le,e,t),s(e,qe,t),s(e,we,t),Qe=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),C.$set(J);const ie={};t&2&&(ie.$$scope={dirty:t,ctx:e}),I.$set(ie);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),G.$set(j);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),R.$set(U)},i(e){Qe||(f(F.$$.fragment,e),f(B.$$.fragment,e),f(D.$$.fragment,e),f(H.$$.fragment,e),f(Q.$$.fragment,e),f(L.$$.fragment,e),f(O.$$.fragment,e),f(K.$$.fragment,e),f(te.$$.fragment,e),f(oe.$$.fragment,e),f(ne.$$.fragment,e),f(C.$$.fragment,e),f(se.$$.fragment,e),f(ae.$$.fragment,e),f(re.$$.fragment,e),f(de.$$.fragment,e),f(I.$$.fragment,e),f(G.$$.fragment,e),f(ce.$$.fragment,e),f(R.$$.fragment,e),f(le.$$.fragment,e),Qe=!0)},o(e){y(F.$$.fragment,e),y(B.$$.fragment,e),y(D.$$.fragment,e),y(H.$$.fragment,e),y(Q.$$.fragment,e),y(L.$$.fragment,e),y(O.$$.fragment,e),y(K.$$.fragment,e),y(te.$$.fragment,e),y(oe.$$.fragment,e),y(ne.$$.fragment,e),y(C.$$.fragment,e),y(se.$$.fragment,e),y(ae.$$.fragment,e),y(re.$$.fragment,e),y(de.$$.fragment,e),y(I.$$.fragment,e),y(G.$$.fragment,e),y(ce.$$.fragment,e),y(R.$$.fragment,e),y(le.$$.fragment,e),Qe=!1},d(e){e&&(o(v),o(m),o(i),o(b),o(w),o(je),o(E),o(Je),o(x),o(We),o(X),o(ke),o(Y),o(Ue),o(Ve),o(z),o(Ee),o(Ce),o($e),o(q),o(Ie),o(Ge),o(Re),o(P),o(Se),o(A),o(Ne),o(Fe),o(xe),o(ee),o(Xe),o(Ye),o(Be),o(Z),o(ze),o(De),o(T),o(He),o(qe),o(we)),o(d),_(F,e),_(B,e),_(D,e),_(H,e),_(Q,e),_(L,e),_(O,e),_(K,e),_(te,e),_(oe,e),_(ne),_(C),_(se),_(ae,e),_(re),_(de),_(I),_(G),_(ce),_(R),_(le,e)}}}const Bt='{"title":"Speech Encoder Decoder Models","local":"speech-encoder-decoder-models","sections":[{"title":"Randomly initializing SpeechEncoderDecoderModel from model configurations.","local":"randomly-initializing-speechencoderdecodermodel-from-model-configurations","sections":[],"depth":2},{"title":"Initialising SpeechEncoderDecoderModel from a pretrained encoder and a pretrained decoder.","local":"initialising-speechencoderdecodermodel-from-a-pretrained-encoder-and-a-pretrained-decoder","sections":[],"depth":2},{"title":"Loading an existing SpeechEncoderDecoderModel checkpoint and perform inference.","local":"loading-an-existing-speechencoderdecodermodel-checkpoint-and-perform-inference","sections":[],"depth":2},{"title":"Training","local":"training","sections":[],"depth":2},{"title":"SpeechEncoderDecoderConfig","local":"transformers.SpeechEncoderDecoderConfig","sections":[],"depth":2},{"title":"SpeechEncoderDecoderModel","local":"transformers.SpeechEncoderDecoderModel","sections":[],"depth":2}],"depth":1}';function zt(V){return Ct(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ot extends $t{constructor(d){super(),It(this,d,zt,Yt,Et,{})}}export{Ot as component};
