import{s as gt,o as wt,n as we}from"../chunks/scheduler.18a86fab.js";import{S as _t,i as bt,g as c,s,r as g,A as vt,h as f,f as n,c as a,j as re,x as h,u as w,k as R,y as d,a as i,v as _,d as b,t as v,w as y}from"../chunks/index.98837b22.js";import{T as ut}from"../chunks/Tip.77304350.js";import{D as ue}from"../chunks/Docstring.a1ef7999.js";import{C as Oe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as De}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ge,E as yt}from"../chunks/getInferenceSnippets.06c2775f.js";function $t(S){let o,u="Example:",l,m,p;return m=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aWZ0Rm9ybWVyQ29uZmlnJTJDJTIwU3dpZnRGb3JtZXJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBTd2lmdEZvcm1lciUyMHN3aWZ0Zm9ybWVyLWJhc2UtcGF0Y2gxNi0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwU3dpZnRGb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwc3dpZnRmb3JtZXItYmFzZS1wYXRjaDE2LTIyNCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwU3dpZnRGb3JtZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SwiftFormerConfig, SwiftFormerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SwiftFormer swiftformer-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SwiftFormerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the swiftformer-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SwiftFormerModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=c("p"),o.textContent=u,l=s(),g(m.$$.fragment)},l(r){o=f(r,"P",{"data-svelte-h":!0}),h(o)!=="svelte-11lpom8"&&(o.textContent=u),l=a(r),w(m.$$.fragment,r)},m(r,$){i(r,o,$),i(r,l,$),_(m,r,$),p=!0},p:we,i(r){p||(b(m.$$.fragment,r),p=!0)},o(r){v(m.$$.fragment,r),p=!1},d(r){r&&(n(o),n(l)),y(m,r)}}}function Tt(S){let o,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=c("p"),o.innerHTML=u},l(l){o=f(l,"P",{"data-svelte-h":!0}),h(o)!=="svelte-fincs2"&&(o.innerHTML=u)},m(l,m){i(l,o,m)},p:we,d(l){l&&n(o)}}}function Mt(S){let o,u="Example:",l,m,p;return m=new Oe({props:{code:"",highlighted:"",wrap:!1}}),{c(){o=c("p"),o.textContent=u,l=s(),g(m.$$.fragment)},l(r){o=f(r,"P",{"data-svelte-h":!0}),h(o)!=="svelte-11lpom8"&&(o.textContent=u),l=a(r),w(m.$$.fragment,r)},m(r,$){i(r,o,$),i(r,l,$),_(m,r,$),p=!0},p:we,i(r){p||(b(m.$$.fragment,r),p=!0)},o(r){v(m.$$.fragment,r),p=!1},d(r){r&&(n(o),n(l)),y(m,r)}}}function Ct(S){let o,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=c("p"),o.innerHTML=u},l(l){o=f(l,"P",{"data-svelte-h":!0}),h(o)!=="svelte-fincs2"&&(o.innerHTML=u)},m(l,m){i(l,o,m)},p:we,d(l){l&&n(o)}}}function Ft(S){let o,u="Example:",l,m,p;return m=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aWZ0Rm9ybWVyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyTUJaVUFJJTJGc3dpZnRmb3JtZXIteHMlMjIpJTBBbW9kZWwlMjAlM0QlMjBTd2lmdEZvcm1lckZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMk1CWlVBSSUyRnN3aWZ0Zm9ybWVyLXhzJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SwiftFormerForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SwiftFormerForImageClassification.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){o=c("p"),o.textContent=u,l=s(),g(m.$$.fragment)},l(r){o=f(r,"P",{"data-svelte-h":!0}),h(o)!=="svelte-11lpom8"&&(o.textContent=u),l=a(r),w(m.$$.fragment,r)},m(r,$){i(r,o,$),i(r,l,$),_(m,r,$),p=!0},p:we,i(r){p||(b(m.$$.fragment,r),p=!0)},o(r){v(m.$$.fragment,r),p=!1},d(r){r&&(n(o),n(l)),y(m,r)}}}function xt(S){let o,u,l,m,p,r="<em>This model was released on 2023-03-27 and added to Hugging Face Transformers on 2023-05-12.</em>",$,N,_e,P,Ke='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',be,L,ve,B,et='The SwiftFormer model was proposed in <a href="https://huggingface.co/papers/2303.15446" rel="nofollow">SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications</a> by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.',ye,G,tt="The SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called ‘SwiftFormer’ is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2× faster compared to MobileViT-v2.",$e,E,ot="The abstract from the paper is the following:",Te,q,rt="<em>Self-attention has become a defacto choice for capturing global context in various vision applications. However, its quadratic computational complexity with respect to image resolution limits its use in real-time applications, especially for deployment on resource-constrained mobile devices. Although hybrid approaches have been proposed to combine the advantages of convolutions and self-attention for a better speed-accuracy trade-off, the expensive matrix multiplication operations in self-attention remain a bottleneck. In this work, we introduce a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations with linear element-wise multiplications. Our design shows that the key-value interaction can be replaced with a linear layer without sacrificing any accuracy. Unlike previous state-of-the-art methods, our efficient formulation of self-attention enables its usage at all stages of the network. Using our proposed efficient additive attention, we build a series of models called “SwiftFormer” which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster compared to MobileViT-v2.</em>",Me,A,nt='This model was contributed by <a href="https://huggingface.co/shehan97" rel="nofollow">shehan97</a>. The original code can be found <a href="https://github.com/Amshaker/SwiftFormer" rel="nofollow">here</a>.',Ce,X,Fe,C,Y,Pe,ne,st=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a>. It is used to instantiate an
SwiftFormer model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SwiftFormer
<a href="https://huggingface.co/MBZUAI/swiftformer-xs" rel="nofollow">MBZUAI/swiftformer-xs</a> architecture.`,We,se,at=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,He,W,xe,Q,Se,T,D,Ve,ae,it="The bare Swiftformer Model outputting raw hidden-states without any specific head on top.",ke,ie,lt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ze,le,mt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Re,I,O,Ne,me,dt='The <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> forward method, overrides the <code>__call__</code> special method.',Le,H,Be,V,Ie,K,Ue,M,ee,Ge,de,ct="The Swiftformer Model with an image classification head on top e.g. for ImageNet.",Ee,ce,ft=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qe,fe,pt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ae,U,te,Xe,pe,ht='The <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Ye,k,Qe,z,je,oe,Ze,he,Je;return N=new ge({props:{title:"SwiftFormer",local:"swiftformer",headingTag:"h1"}}),L=new ge({props:{title:"Overview",local:"overview",headingTag:"h2"}}),X=new ge({props:{title:"SwiftFormerConfig",local:"transformers.SwiftFormerConfig",headingTag:"h2"}}),Y=new ue({props:{name:"class transformers.SwiftFormerConfig",anchor:"transformers.SwiftFormerConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"depths",val:" = [3, 3, 6, 4]"},{name:"embed_dims",val:" = [48, 56, 112, 220]"},{name:"mlp_ratio",val:" = 4"},{name:"downsamples",val:" = [True, True, True, True]"},{name:"hidden_act",val:" = 'gelu'"},{name:"down_patch_size",val:" = 3"},{name:"down_stride",val:" = 2"},{name:"down_pad",val:" = 1"},{name:"drop_path_rate",val:" = 0.0"},{name:"drop_mlp_rate",val:" = 0.0"},{name:"drop_conv_encoder_rate",val:" = 0.0"},{name:"use_layer_scale",val:" = True"},{name:"layer_scale_init_value",val:" = 1e-05"},{name:"batch_norm_eps",val:" = 1e-05"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SwiftFormerConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image`,name:"image_size"},{anchor:"transformers.SwiftFormerConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels`,name:"num_channels"},{anchor:"transformers.SwiftFormerConfig.depths",description:`<strong>depths</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 6, 4]</code>) &#x2014;
Depth of each stage`,name:"depths"},{anchor:"transformers.SwiftFormerConfig.embed_dims",description:`<strong>embed_dims</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[48, 56, 112, 220]</code>) &#x2014;
The embedding dimension at each stage`,name:"embed_dims"},{anchor:"transformers.SwiftFormerConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of size of the hidden dimensionality of an MLP to the dimensionality of its input.`,name:"mlp_ratio"},{anchor:"transformers.SwiftFormerConfig.downsamples",description:`<strong>downsamples</strong> (<code>list[bool]</code>, <em>optional</em>, defaults to <code>[True, True, True, True]</code>) &#x2014;
Whether or not to downsample inputs between two stages.`,name:"downsamples"},{anchor:"transformers.SwiftFormerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (string). <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.SwiftFormerConfig.down_patch_size",description:`<strong>down_patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The size of patches in downsampling layers.`,name:"down_patch_size"},{anchor:"transformers.SwiftFormerConfig.down_stride",description:`<strong>down_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The stride of convolution kernels in downsampling layers.`,name:"down_stride"},{anchor:"transformers.SwiftFormerConfig.down_pad",description:`<strong>down_pad</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding in downsampling layers.`,name:"down_pad"},{anchor:"transformers.SwiftFormerConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Rate at which to increase dropout probability in DropPath.`,name:"drop_path_rate"},{anchor:"transformers.SwiftFormerConfig.drop_mlp_rate",description:`<strong>drop_mlp_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Dropout rate for the MLP component of SwiftFormer.`,name:"drop_mlp_rate"},{anchor:"transformers.SwiftFormerConfig.drop_conv_encoder_rate",description:`<strong>drop_conv_encoder_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Dropout rate for the ConvEncoder component of SwiftFormer.`,name:"drop_conv_encoder_rate"},{anchor:"transformers.SwiftFormerConfig.use_layer_scale",description:`<strong>use_layer_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to scale outputs from token mixers.`,name:"use_layer_scale"},{anchor:"transformers.SwiftFormerConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
Factor by which outputs from token mixers are scaled.`,name:"layer_scale_init_value"},{anchor:"transformers.SwiftFormerConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swiftformer/configuration_swiftformer.py#L30"}}),W=new De({props:{anchor:"transformers.SwiftFormerConfig.example",$$slots:{default:[$t]},$$scope:{ctx:S}}}),Q=new ge({props:{title:"SwiftFormerModel",local:"transformers.SwiftFormerModel",headingTag:"h2"}}),D=new ue({props:{name:"class transformers.SwiftFormerModel",anchor:"transformers.SwiftFormerModel",parameters:[{name:"config",val:": SwiftFormerConfig"}],parametersDescription:[{anchor:"transformers.SwiftFormerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swiftformer/modeling_swiftformer.py#L417"}}),O=new ue({props:{name:"forward",anchor:"transformers.SwiftFormerModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SwiftFormerModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SwiftFormerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SwiftFormerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swiftformer/modeling_swiftformer.py#L428",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig"
>SwiftFormerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new ut({props:{$$slots:{default:[Tt]},$$scope:{ctx:S}}}),V=new De({props:{anchor:"transformers.SwiftFormerModel.forward.example",$$slots:{default:[Mt]},$$scope:{ctx:S}}}),K=new ge({props:{title:"SwiftFormerForImageClassification",local:"transformers.SwiftFormerForImageClassification",headingTag:"h2"}}),ee=new ue({props:{name:"class transformers.SwiftFormerForImageClassification",anchor:"transformers.SwiftFormerForImageClassification",parameters:[{name:"config",val:": SwiftFormerConfig"}],parametersDescription:[{anchor:"transformers.SwiftFormerForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swiftformer/modeling_swiftformer.py#L460"}}),te=new ue({props:{name:"forward",anchor:"transformers.SwiftFormerForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SwiftFormerForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SwiftFormerForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.SwiftFormerForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SwiftFormerForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swiftformer/modeling_swiftformer.py#L477",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig"
>SwiftFormerConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),k=new ut({props:{$$slots:{default:[Ct]},$$scope:{ctx:S}}}),z=new De({props:{anchor:"transformers.SwiftFormerForImageClassification.forward.example",$$slots:{default:[Ft]},$$scope:{ctx:S}}}),oe=new yt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/swiftformer.md"}}),{c(){o=c("meta"),u=s(),l=c("p"),m=s(),p=c("p"),p.innerHTML=r,$=s(),g(N.$$.fragment),_e=s(),P=c("div"),P.innerHTML=Ke,be=s(),g(L.$$.fragment),ve=s(),B=c("p"),B.innerHTML=et,ye=s(),G=c("p"),G.textContent=tt,$e=s(),E=c("p"),E.textContent=ot,Te=s(),q=c("p"),q.innerHTML=rt,Me=s(),A=c("p"),A.innerHTML=nt,Ce=s(),g(X.$$.fragment),Fe=s(),C=c("div"),g(Y.$$.fragment),Pe=s(),ne=c("p"),ne.innerHTML=st,We=s(),se=c("p"),se.innerHTML=at,He=s(),g(W.$$.fragment),xe=s(),g(Q.$$.fragment),Se=s(),T=c("div"),g(D.$$.fragment),Ve=s(),ae=c("p"),ae.textContent=it,ke=s(),ie=c("p"),ie.innerHTML=lt,ze=s(),le=c("p"),le.innerHTML=mt,Re=s(),I=c("div"),g(O.$$.fragment),Ne=s(),me=c("p"),me.innerHTML=dt,Le=s(),g(H.$$.fragment),Be=s(),g(V.$$.fragment),Ie=s(),g(K.$$.fragment),Ue=s(),M=c("div"),g(ee.$$.fragment),Ge=s(),de=c("p"),de.textContent=ct,Ee=s(),ce=c("p"),ce.innerHTML=ft,qe=s(),fe=c("p"),fe.innerHTML=pt,Ae=s(),U=c("div"),g(te.$$.fragment),Xe=s(),pe=c("p"),pe.innerHTML=ht,Ye=s(),g(k.$$.fragment),Qe=s(),g(z.$$.fragment),je=s(),g(oe.$$.fragment),Ze=s(),he=c("p"),this.h()},l(e){const t=vt("svelte-u9bgzb",document.head);o=f(t,"META",{name:!0,content:!0}),t.forEach(n),u=a(e),l=f(e,"P",{}),re(l).forEach(n),m=a(e),p=f(e,"P",{"data-svelte-h":!0}),h(p)!=="svelte-u3w482"&&(p.innerHTML=r),$=a(e),w(N.$$.fragment,e),_e=a(e),P=f(e,"DIV",{class:!0,"data-svelte-h":!0}),h(P)!=="svelte-13t8s2t"&&(P.innerHTML=Ke),be=a(e),w(L.$$.fragment,e),ve=a(e),B=f(e,"P",{"data-svelte-h":!0}),h(B)!=="svelte-74edhd"&&(B.innerHTML=et),ye=a(e),G=f(e,"P",{"data-svelte-h":!0}),h(G)!=="svelte-q1qv2z"&&(G.textContent=tt),$e=a(e),E=f(e,"P",{"data-svelte-h":!0}),h(E)!=="svelte-vfdo9a"&&(E.textContent=ot),Te=a(e),q=f(e,"P",{"data-svelte-h":!0}),h(q)!=="svelte-1hgv92f"&&(q.innerHTML=rt),Me=a(e),A=f(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-1d60ttx"&&(A.innerHTML=nt),Ce=a(e),w(X.$$.fragment,e),Fe=a(e),C=f(e,"DIV",{class:!0});var j=re(C);w(Y.$$.fragment,j),Pe=a(j),ne=f(j,"P",{"data-svelte-h":!0}),h(ne)!=="svelte-156dbt"&&(ne.innerHTML=st),We=a(j),se=f(j,"P",{"data-svelte-h":!0}),h(se)!=="svelte-1ek1ss9"&&(se.innerHTML=at),He=a(j),w(W.$$.fragment,j),j.forEach(n),xe=a(e),w(Q.$$.fragment,e),Se=a(e),T=f(e,"DIV",{class:!0});var F=re(T);w(D.$$.fragment,F),Ve=a(F),ae=f(F,"P",{"data-svelte-h":!0}),h(ae)!=="svelte-kbt2bk"&&(ae.textContent=it),ke=a(F),ie=f(F,"P",{"data-svelte-h":!0}),h(ie)!=="svelte-q52n56"&&(ie.innerHTML=lt),ze=a(F),le=f(F,"P",{"data-svelte-h":!0}),h(le)!=="svelte-hswkmf"&&(le.innerHTML=mt),Re=a(F),I=f(F,"DIV",{class:!0});var Z=re(I);w(O.$$.fragment,Z),Ne=a(Z),me=f(Z,"P",{"data-svelte-h":!0}),h(me)!=="svelte-asg9gf"&&(me.innerHTML=dt),Le=a(Z),w(H.$$.fragment,Z),Be=a(Z),w(V.$$.fragment,Z),Z.forEach(n),F.forEach(n),Ie=a(e),w(K.$$.fragment,e),Ue=a(e),M=f(e,"DIV",{class:!0});var x=re(M);w(ee.$$.fragment,x),Ge=a(x),de=f(x,"P",{"data-svelte-h":!0}),h(de)!=="svelte-elx10h"&&(de.textContent=ct),Ee=a(x),ce=f(x,"P",{"data-svelte-h":!0}),h(ce)!=="svelte-q52n56"&&(ce.innerHTML=ft),qe=a(x),fe=f(x,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-hswkmf"&&(fe.innerHTML=pt),Ae=a(x),U=f(x,"DIV",{class:!0});var J=re(U);w(te.$$.fragment,J),Xe=a(J),pe=f(J,"P",{"data-svelte-h":!0}),h(pe)!=="svelte-1ai6ud"&&(pe.innerHTML=ht),Ye=a(J),w(k.$$.fragment,J),Qe=a(J),w(z.$$.fragment,J),J.forEach(n),x.forEach(n),je=a(e),w(oe.$$.fragment,e),Ze=a(e),he=f(e,"P",{}),re(he).forEach(n),this.h()},h(){R(o,"name","hf:doc:metadata"),R(o,"content",St),R(P,"class","flex flex-wrap space-x-1"),R(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){d(document.head,o),i(e,u,t),i(e,l,t),i(e,m,t),i(e,p,t),i(e,$,t),_(N,e,t),i(e,_e,t),i(e,P,t),i(e,be,t),_(L,e,t),i(e,ve,t),i(e,B,t),i(e,ye,t),i(e,G,t),i(e,$e,t),i(e,E,t),i(e,Te,t),i(e,q,t),i(e,Me,t),i(e,A,t),i(e,Ce,t),_(X,e,t),i(e,Fe,t),i(e,C,t),_(Y,C,null),d(C,Pe),d(C,ne),d(C,We),d(C,se),d(C,He),_(W,C,null),i(e,xe,t),_(Q,e,t),i(e,Se,t),i(e,T,t),_(D,T,null),d(T,Ve),d(T,ae),d(T,ke),d(T,ie),d(T,ze),d(T,le),d(T,Re),d(T,I),_(O,I,null),d(I,Ne),d(I,me),d(I,Le),_(H,I,null),d(I,Be),_(V,I,null),i(e,Ie,t),_(K,e,t),i(e,Ue,t),i(e,M,t),_(ee,M,null),d(M,Ge),d(M,de),d(M,Ee),d(M,ce),d(M,qe),d(M,fe),d(M,Ae),d(M,U),_(te,U,null),d(U,Xe),d(U,pe),d(U,Ye),_(k,U,null),d(U,Qe),_(z,U,null),i(e,je,t),_(oe,e,t),i(e,Ze,t),i(e,he,t),Je=!0},p(e,[t]){const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),W.$set(j);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),H.$set(F);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),V.$set(Z);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),k.$set(x);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),z.$set(J)},i(e){Je||(b(N.$$.fragment,e),b(L.$$.fragment,e),b(X.$$.fragment,e),b(Y.$$.fragment,e),b(W.$$.fragment,e),b(Q.$$.fragment,e),b(D.$$.fragment,e),b(O.$$.fragment,e),b(H.$$.fragment,e),b(V.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(k.$$.fragment,e),b(z.$$.fragment,e),b(oe.$$.fragment,e),Je=!0)},o(e){v(N.$$.fragment,e),v(L.$$.fragment,e),v(X.$$.fragment,e),v(Y.$$.fragment,e),v(W.$$.fragment,e),v(Q.$$.fragment,e),v(D.$$.fragment,e),v(O.$$.fragment,e),v(H.$$.fragment,e),v(V.$$.fragment,e),v(K.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(k.$$.fragment,e),v(z.$$.fragment,e),v(oe.$$.fragment,e),Je=!1},d(e){e&&(n(u),n(l),n(m),n(p),n($),n(_e),n(P),n(be),n(ve),n(B),n(ye),n(G),n($e),n(E),n(Te),n(q),n(Me),n(A),n(Ce),n(Fe),n(C),n(xe),n(Se),n(T),n(Ie),n(Ue),n(M),n(je),n(Ze),n(he)),n(o),y(N,e),y(L,e),y(X,e),y(Y),y(W),y(Q,e),y(D),y(O),y(H),y(V),y(K,e),y(ee),y(te),y(k),y(z),y(oe,e)}}}const St='{"title":"SwiftFormer","local":"swiftformer","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"SwiftFormerConfig","local":"transformers.SwiftFormerConfig","sections":[],"depth":2},{"title":"SwiftFormerModel","local":"transformers.SwiftFormerModel","sections":[],"depth":2},{"title":"SwiftFormerForImageClassification","local":"transformers.SwiftFormerForImageClassification","sections":[],"depth":2}],"depth":1}';function It(S){return wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vt extends _t{constructor(o){super(),bt(this,o,It,xt,gt,{})}}export{Vt as component};
