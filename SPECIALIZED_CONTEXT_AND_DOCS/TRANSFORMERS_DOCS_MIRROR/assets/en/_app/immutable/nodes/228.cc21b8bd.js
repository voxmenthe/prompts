import{s as et,o as tt,n as se}from"../chunks/scheduler.18a86fab.js";import{S as st,i as nt,g as p,s as d,r as u,A as ot,h as f,f as o,c as m,j as ee,x as w,u as h,k as te,l as at,y as M,a as i,v as g,d as _,t as b,w as y}from"../chunks/index.98837b22.js";import{T as ze}from"../chunks/Tip.77304350.js";import{D as de}from"../chunks/Docstring.a1ef7999.js";import{C as He}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Oe}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ne,E as rt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as lt,a as Ke}from"../chunks/HfOption.6641485e.js";function it($){let t,c=`This model was contributed by <a href="https://github.com/VladOS95-cyber" rel="nofollow">VladOS95-cyber</a>.
Click on the HGNet V2 models in the right sidebar for more examples of how to apply HGNet V2 to different computer vision tasks.`;return{c(){t=p("p"),t.innerHTML=c},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-1yfcqdj"&&(t.innerHTML=c)},m(s,l){i(s,t,l)},p:se,d(s){s&&o(t)}}}function ct($){let t,c;return t=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJpbWFnZS1jbGFzc2lmaWNhdGlvbiUyMiUyQyUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEJTIydXN0Yy1jb21tdW5pdHklMkZoZ25ldC12MiUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZSUzRDAlMEEpJTBBcGlwZWxpbmUoJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;image-classification&quot;</span>,
    model=<span class="hljs-string">&quot;ustc-community/hgnet-v2&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,l){g(t,s,l),c=!0},p:se,i(s){c||(_(t.$$.fragment,s),c=!0)},o(s){b(t.$$.fragment,s),c=!1},d(s){y(t,s)}}}function dt($){let t,c;return t=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBIR05ldFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUyQyUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFtb2RlbCUyMCUzRCUyMEhHTmV0VjJGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJ1c3RjLWNvbW11bml0eSUyRmhnbmV0LXYyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIydXN0Yy1jb21tdW5pdHklMkZoZ25ldC12MiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBcHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heChkaW0lM0QtMSkuaXRlbSgpJTBBJTBBY2xhc3NfbGFiZWxzJTIwJTNEJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTBBcHJlZGljdGVkX2NsYXNzX2xhYmVsJTIwJTNEJTIwY2xhc3NfbGFiZWxzJTVCcHJlZGljdGVkX2NsYXNzX2lkJTVEJTBBcHJpbnQoZiUyMlRoZSUyMHByZWRpY3RlZCUyMGNsYXNzJTIwbGFiZWwlMjBpcyUzQSUyMCU3QnByZWRpY3RlZF9jbGFzc19sYWJlbCU3RCUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> HGNetV2ForImageClassification, AutoImageProcessor
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

model = HGNetV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;ustc-community/hgnet-v2&quot;</span>)
processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;ustc-community/hgnet-v2&quot;</span>)

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.no_grad():
    logits = model(**inputs).logits
predicted_class_id = logits.argmax(dim=-<span class="hljs-number">1</span>).item()

class_labels = model.config.id2label
predicted_class_label = class_labels[predicted_class_id]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted class label is: <span class="hljs-subst">{predicted_class_label}</span>&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,l){g(t,s,l),c=!0},p:se,i(s){c||(_(t.$$.fragment,s),c=!0)},o(s){b(t.$$.fragment,s),c=!1},d(s){y(t,s)}}}function mt($){let t,c,s,l;return t=new Ke({props:{id:"usage",option:"Pipeline",$$slots:{default:[ct]},$$scope:{ctx:$}}}),s=new Ke({props:{id:"usage",option:"AutoModel",$$slots:{default:[dt]},$$scope:{ctx:$}}}),{c(){u(t.$$.fragment),c=d(),u(s.$$.fragment)},l(r){h(t.$$.fragment,r),c=m(r),h(s.$$.fragment,r)},m(r,a){g(t,r,a),i(r,c,a),g(s,r,a),l=!0},p(r,a){const v={};a&2&&(v.$$scope={dirty:a,ctx:r}),t.$set(v);const j={};a&2&&(j.$$scope={dirty:a,ctx:r}),s.$set(j)},i(r){l||(_(t.$$.fragment,r),_(s.$$.fragment,r),l=!0)},o(r){b(t.$$.fragment,r),b(s.$$.fragment,r),l=!1},d(r){r&&o(c),y(t,r),y(s,r)}}}function pt($){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=c},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(s,l){i(s,t,l)},p:se,d(s){s&&o(t)}}}function ft($){let t,c="Examples:",s,l,r;return l=new He({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEhHTmV0VjJDb25maWclMkMlMjBIR05ldFYyQmFja2JvbmUlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWNvbmZpZyUyMCUzRCUyMEhHTmV0VjJDb25maWcoKSUwQW1vZGVsJTIwJTNEJTIwSEdOZXRWMkJhY2tib25lKGNvbmZpZyklMEElMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjB0b3JjaC5yYW5kbigxJTJDJTIwMyUyQyUyMDIyNCUyQyUyMDIyNCklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyklMEElMEFmZWF0dXJlX21hcHMlMjAlM0QlMjBvdXRwdXRzLmZlYXR1cmVfbWFwcyUwQWxpc3QoZmVhdHVyZV9tYXBzJTVCLTElNUQuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> HGNetV2Config, HGNetV2Backbone
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>config = HGNetV2Config()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = HGNetV2Backbone(config)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(pixel_values)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_maps = outputs.feature_maps
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(feature_maps[-<span class="hljs-number">1</span>].shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">2048</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=p("p"),t.textContent=c,s=d(),u(l.$$.fragment)},l(a){t=f(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=c),s=m(a),h(l.$$.fragment,a)},m(a,v){i(a,t,v),i(a,s,v),g(l,a,v),r=!0},p:se,i(a){r||(_(l.$$.fragment,a),r=!0)},o(a){b(l.$$.fragment,a),r=!1},d(a){a&&(o(t),o(s)),y(l,a)}}}function ut($){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=c},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(s,l){i(s,t,l)},p:se,d(s){s&&o(t)}}}function ht($){let t,c="Examples:",s,l,r;return l=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBIR05ldFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUyQyUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFtb2RlbCUyMCUzRCUyMEhHTmV0VjJGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJ1c3RjLWNvbW11bml0eSUyRmhnbmV0LXYyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIydXN0Yy1jb21tdW5pdHklMkZoZ25ldC12MiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBb3V0cHV0cy5sb2dpdHMuc2hhcGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> HGNetV2ForImageClassification, AutoImageProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = HGNetV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;ustc-community/hgnet-v2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;ustc-community/hgnet-v2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs.logits.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])`,wrap:!1}}),{c(){t=p("p"),t.textContent=c,s=d(),u(l.$$.fragment)},l(a){t=f(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=c),s=m(a),h(l.$$.fragment,a)},m(a,v){i(a,t,v),i(a,s,v),g(l,a,v),r=!0},p:se,i(a){r||(_(l.$$.fragment,a),r=!0)},o(a){b(l.$$.fragment,a),r=!1},d(a){a&&(o(t),o(s)),y(l,a)}}}function gt($){let t,c,s,l,r,a="<em>This model was released on 2024-07-01 and added to Hugging Face Transformers on 2025-04-29.</em>",v,j,Ee='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',me,W,pe,z,Xe='<a href="https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/zh_CN/models/ImageNet1k/PP-HGNetV2.md" rel="nofollow">HGNetV2</a> is a next-generation convolutional neural network (CNN) backbone built for optimal accuracy-latency tradeoff on NVIDIA GPUs. Building on the original<a href="https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/en/models/PP-HGNet_en.md" rel="nofollow">HGNet</a>, HGNetV2 delivers high accuracy at fast inference speeds and performs strongly on tasks like image classification, object detection, and segmentation, making it a practical choice for GPU-based computer vision applications.',fe,E,Re='You can find all the original HGNet V2 models under the <a href="https://huggingface.co/ustc-community/models?search=hgnet" rel="nofollow">USTC</a> organization.',ue,F,he,X,Pe='The example below demonstrates how to classify an image with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',ge,x,_e,R,be,G,P,je,ne,Qe=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone">HGNetV2Backbone</a>. It is used to instantiate a HGNet-V2
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of D-FINE-X-COCO B4 ”<a href="https://huggingface.co/ustc-community/dfine_x_coco%22" rel="nofollow">ustc-community/dfine_x_coco”</a>.
Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ye,Q,Me,U,L,Ve,N,q,ke,oe,Le='The <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone">HGNetV2Backbone</a> forward method, overrides the <code>__call__</code> special method.',Ge,J,Ue,I,$e,S,we,C,Y,Fe,ae,qe=`HGNetV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,xe,re,Se=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Je,le,Ye=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ie,H,A,Ze,ie,Ae='The <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification">HGNetV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Be,Z,We,B,ve,D,Ce,ce,Te;return W=new Ne({props:{title:"HGNet-V2",local:"hgnet-v2",headingTag:"h1"}}),F=new ze({props:{warning:!1,$$slots:{default:[it]},$$scope:{ctx:$}}}),x=new lt({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[mt]},$$scope:{ctx:$}}}),R=new Ne({props:{title:"HGNetV2Config",local:"transformers.HGNetV2Config",headingTag:"h2"}}),P=new de({props:{name:"class transformers.HGNetV2Config",anchor:"transformers.HGNetV2Config",parameters:[{name:"num_channels",val:" = 3"},{name:"embedding_size",val:" = 64"},{name:"depths",val:" = [3, 4, 6, 3]"},{name:"hidden_sizes",val:" = [256, 512, 1024, 2048]"},{name:"hidden_act",val:" = 'relu'"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"stem_channels",val:" = [3, 32, 48]"},{name:"stage_in_channels",val:" = [48, 128, 512, 1024]"},{name:"stage_mid_channels",val:" = [48, 96, 192, 384]"},{name:"stage_out_channels",val:" = [128, 512, 1024, 2048]"},{name:"stage_num_blocks",val:" = [1, 1, 3, 1]"},{name:"stage_downsample",val:" = [False, True, True, True]"},{name:"stage_light_block",val:" = [False, False, True, True]"},{name:"stage_kernel_size",val:" = [3, 3, 5, 5]"},{name:"stage_numb_of_layers",val:" = [6, 6, 6, 6]"},{name:"use_learnable_affine_block",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.HGNetV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.HGNetV2Config.embedding_size",description:`<strong>embedding_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality (hidden size) for the embedding layer.`,name:"embedding_size"},{anchor:"transformers.HGNetV2Config.depths",description:`<strong>depths</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 4, 6, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.HGNetV2Config.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[256, 512, 1024, 2048]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.HGNetV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code>
are supported.`,name:"hidden_act"},{anchor:"transformers.HGNetV2Config.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.HGNetV2Config.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"},{anchor:"transformers.HGNetV2Config.stem_channels",description:`<strong>stem_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 32, 48]</code>) &#x2014;
Channel dimensions for the stem layers:<ul>
<li>First number (3) is input image channels</li>
<li>Second number (32) is intermediate stem channels</li>
<li>Third number (48) is output stem channels</li>
</ul>`,name:"stem_channels"},{anchor:"transformers.HGNetV2Config.stage_in_channels",description:`<strong>stage_in_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[48, 128, 512, 1024]</code>) &#x2014;
Input channel dimensions for each stage of the backbone.
This defines how many channels the input to each stage will have.`,name:"stage_in_channels"},{anchor:"transformers.HGNetV2Config.stage_mid_channels",description:`<strong>stage_mid_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[48, 96, 192, 384]</code>) &#x2014;
Mid-channel dimensions for each stage of the backbone.
This defines the number of channels used in the intermediate layers of each stage.`,name:"stage_mid_channels"},{anchor:"transformers.HGNetV2Config.stage_out_channels",description:`<strong>stage_out_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[128, 512, 1024, 2048]</code>) &#x2014;
Output channel dimensions for each stage of the backbone.
This defines how many channels the output of each stage will have.`,name:"stage_out_channels"},{anchor:"transformers.HGNetV2Config.stage_num_blocks",description:`<strong>stage_num_blocks</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 1, 3, 1]</code>) &#x2014;
Number of blocks to be used in each stage of the backbone.
This controls the depth of each stage by specifying how many convolutional blocks to stack.`,name:"stage_num_blocks"},{anchor:"transformers.HGNetV2Config.stage_downsample",description:`<strong>stage_downsample</strong> (<code>list[bool]</code>, <em>optional</em>, defaults to <code>[False, True, True, True]</code>) &#x2014;
Indicates whether to downsample the feature maps at each stage.
If <code>True</code>, the spatial dimensions of the feature maps will be reduced.`,name:"stage_downsample"},{anchor:"transformers.HGNetV2Config.stage_light_block",description:`<strong>stage_light_block</strong> (<code>list[bool]</code>, <em>optional</em>, defaults to <code>[False, False, True, True]</code>) &#x2014;
Indicates whether to use light blocks in each stage.
Light blocks are a variant of convolutional blocks that may have fewer parameters.`,name:"stage_light_block"},{anchor:"transformers.HGNetV2Config.stage_kernel_size",description:`<strong>stage_kernel_size</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 5, 5]</code>) &#x2014;
Kernel sizes for the convolutional layers in each stage.`,name:"stage_kernel_size"},{anchor:"transformers.HGNetV2Config.stage_numb_of_layers",description:`<strong>stage_numb_of_layers</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[6, 6, 6, 6]</code>) &#x2014;
Number of layers to be used in each block of the stage.`,name:"stage_numb_of_layers"},{anchor:"transformers.HGNetV2Config.use_learnable_affine_block",description:`<strong>use_learnable_affine_block</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use Learnable Affine Blocks (LAB) in the network.
LAB adds learnable scale and bias parameters after certain operations.`,name:"use_learnable_affine_block"},{anchor:"transformers.HGNetV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/hgnet_v2/configuration_hgnet_v2.py#L29"}}),Q=new Ne({props:{title:"HGNetV2Backbone",local:"transformers.HGNetV2Backbone",headingTag:"h2"}}),L=new de({props:{name:"class transformers.HGNetV2Backbone",anchor:"transformers.HGNetV2Backbone",parameters:[{name:"config",val:": HGNetV2Config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/hgnet_v2/modeling_hgnet_v2.py#L334"}}),q=new de({props:{name:"forward",anchor:"transformers.HGNetV2Backbone.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.HGNetV2Backbone.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.HGNetV2Backbone.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.HGNetV2Backbone.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/hgnet_v2/modeling_hgnet_v2.py#L348",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BackboneOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config"
>HGNetV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>feature_maps</strong> (<code>tuple(torch.FloatTensor)</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Feature maps of the stages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code> or <code>(batch_size, num_channels, height, width)</code>,
depending on the backbone.</p>
<p>Hidden-states of the model at the output of each stage plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Only applicable if the backbone uses attention.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BackboneOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),J=new ze({props:{$$slots:{default:[pt]},$$scope:{ctx:$}}}),I=new Oe({props:{anchor:"transformers.HGNetV2Backbone.forward.example",$$slots:{default:[ft]},$$scope:{ctx:$}}}),S=new Ne({props:{title:"HGNetV2ForImageClassification",local:"transformers.HGNetV2ForImageClassification",headingTag:"h2"}}),Y=new de({props:{name:"class transformers.HGNetV2ForImageClassification",anchor:"transformers.HGNetV2ForImageClassification",parameters:[{name:"config",val:": HGNetV2Config"}],parametersDescription:[{anchor:"transformers.HGNetV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config">HGNetV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/hgnet_v2/modeling_hgnet_v2.py#L406"}}),A=new de({props:{name:"forward",anchor:"transformers.HGNetV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.HGNetV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.HGNetV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.HGNetV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.HGNetV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/hgnet_v2/modeling_hgnet_v2.py#L422",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config"
>HGNetV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new ze({props:{$$slots:{default:[ut]},$$scope:{ctx:$}}}),B=new Oe({props:{anchor:"transformers.HGNetV2ForImageClassification.forward.example",$$slots:{default:[ht]},$$scope:{ctx:$}}}),D=new rt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/hgnet_v2.md"}}),{c(){t=p("meta"),c=d(),s=p("p"),l=d(),r=p("p"),r.innerHTML=a,v=d(),j=p("div"),j.innerHTML=Ee,me=d(),u(W.$$.fragment),pe=d(),z=p("p"),z.innerHTML=Xe,fe=d(),E=p("p"),E.innerHTML=Re,ue=d(),u(F.$$.fragment),he=d(),X=p("p"),X.innerHTML=Pe,ge=d(),u(x.$$.fragment),_e=d(),u(R.$$.fragment),be=d(),G=p("div"),u(P.$$.fragment),je=d(),ne=p("p"),ne.innerHTML=Qe,ye=d(),u(Q.$$.fragment),Me=d(),U=p("div"),u(L.$$.fragment),Ve=d(),N=p("div"),u(q.$$.fragment),ke=d(),oe=p("p"),oe.innerHTML=Le,Ge=d(),u(J.$$.fragment),Ue=d(),u(I.$$.fragment),$e=d(),u(S.$$.fragment),we=d(),C=p("div"),u(Y.$$.fragment),Fe=d(),ae=p("p"),ae.textContent=qe,xe=d(),re=p("p"),re.innerHTML=Se,Je=d(),le=p("p"),le.innerHTML=Ye,Ie=d(),H=p("div"),u(A.$$.fragment),Ze=d(),ie=p("p"),ie.innerHTML=Ae,Be=d(),u(Z.$$.fragment),We=d(),u(B.$$.fragment),ve=d(),u(D.$$.fragment),Ce=d(),ce=p("p"),this.h()},l(e){const n=ot("svelte-u9bgzb",document.head);t=f(n,"META",{name:!0,content:!0}),n.forEach(o),c=m(e),s=f(e,"P",{}),ee(s).forEach(o),l=m(e),r=f(e,"P",{"data-svelte-h":!0}),w(r)!=="svelte-87otfk"&&(r.innerHTML=a),v=m(e),j=f(e,"DIV",{style:!0,"data-svelte-h":!0}),w(j)!=="svelte-wa5t4p"&&(j.innerHTML=Ee),me=m(e),h(W.$$.fragment,e),pe=m(e),z=f(e,"P",{"data-svelte-h":!0}),w(z)!=="svelte-1nszwti"&&(z.innerHTML=Xe),fe=m(e),E=f(e,"P",{"data-svelte-h":!0}),w(E)!=="svelte-ldjs2v"&&(E.innerHTML=Re),ue=m(e),h(F.$$.fragment,e),he=m(e),X=f(e,"P",{"data-svelte-h":!0}),w(X)!=="svelte-7bwa3a"&&(X.innerHTML=Pe),ge=m(e),h(x.$$.fragment,e),_e=m(e),h(R.$$.fragment,e),be=m(e),G=f(e,"DIV",{class:!0});var O=ee(G);h(P.$$.fragment,O),je=m(O),ne=f(O,"P",{"data-svelte-h":!0}),w(ne)!=="svelte-nlm9ik"&&(ne.innerHTML=Qe),O.forEach(o),ye=m(e),h(Q.$$.fragment,e),Me=m(e),U=f(e,"DIV",{class:!0});var K=ee(U);h(L.$$.fragment,K),Ve=m(K),N=f(K,"DIV",{class:!0});var V=ee(N);h(q.$$.fragment,V),ke=m(V),oe=f(V,"P",{"data-svelte-h":!0}),w(oe)!=="svelte-lx89oy"&&(oe.innerHTML=Le),Ge=m(V),h(J.$$.fragment,V),Ue=m(V),h(I.$$.fragment,V),V.forEach(o),K.forEach(o),$e=m(e),h(S.$$.fragment,e),we=m(e),C=f(e,"DIV",{class:!0});var T=ee(C);h(Y.$$.fragment,T),Fe=m(T),ae=f(T,"P",{"data-svelte-h":!0}),w(ae)!=="svelte-5cfdvs"&&(ae.textContent=qe),xe=m(T),re=f(T,"P",{"data-svelte-h":!0}),w(re)!=="svelte-q52n56"&&(re.innerHTML=Se),Je=m(T),le=f(T,"P",{"data-svelte-h":!0}),w(le)!=="svelte-hswkmf"&&(le.innerHTML=Ye),Ie=m(T),H=f(T,"DIV",{class:!0});var k=ee(H);h(A.$$.fragment,k),Ze=m(k),ie=f(k,"P",{"data-svelte-h":!0}),w(ie)!=="svelte-3s07he"&&(ie.innerHTML=Ae),Be=m(k),h(Z.$$.fragment,k),We=m(k),h(B.$$.fragment,k),k.forEach(o),T.forEach(o),ve=m(e),h(D.$$.fragment,e),Ce=m(e),ce=f(e,"P",{}),ee(ce).forEach(o),this.h()},h(){te(t,"name","hf:doc:metadata"),te(t,"content",_t),at(j,"float","right"),te(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),te(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),te(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),te(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),te(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){M(document.head,t),i(e,c,n),i(e,s,n),i(e,l,n),i(e,r,n),i(e,v,n),i(e,j,n),i(e,me,n),g(W,e,n),i(e,pe,n),i(e,z,n),i(e,fe,n),i(e,E,n),i(e,ue,n),g(F,e,n),i(e,he,n),i(e,X,n),i(e,ge,n),g(x,e,n),i(e,_e,n),g(R,e,n),i(e,be,n),i(e,G,n),g(P,G,null),M(G,je),M(G,ne),i(e,ye,n),g(Q,e,n),i(e,Me,n),i(e,U,n),g(L,U,null),M(U,Ve),M(U,N),g(q,N,null),M(N,ke),M(N,oe),M(N,Ge),g(J,N,null),M(N,Ue),g(I,N,null),i(e,$e,n),g(S,e,n),i(e,we,n),i(e,C,n),g(Y,C,null),M(C,Fe),M(C,ae),M(C,xe),M(C,re),M(C,Je),M(C,le),M(C,Ie),M(C,H),g(A,H,null),M(H,Ze),M(H,ie),M(H,Be),g(Z,H,null),M(H,We),g(B,H,null),i(e,ve,n),g(D,e,n),i(e,Ce,n),i(e,ce,n),Te=!0},p(e,[n]){const O={};n&2&&(O.$$scope={dirty:n,ctx:e}),F.$set(O);const K={};n&2&&(K.$$scope={dirty:n,ctx:e}),x.$set(K);const V={};n&2&&(V.$$scope={dirty:n,ctx:e}),J.$set(V);const T={};n&2&&(T.$$scope={dirty:n,ctx:e}),I.$set(T);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),Z.$set(k);const De={};n&2&&(De.$$scope={dirty:n,ctx:e}),B.$set(De)},i(e){Te||(_(W.$$.fragment,e),_(F.$$.fragment,e),_(x.$$.fragment,e),_(R.$$.fragment,e),_(P.$$.fragment,e),_(Q.$$.fragment,e),_(L.$$.fragment,e),_(q.$$.fragment,e),_(J.$$.fragment,e),_(I.$$.fragment,e),_(S.$$.fragment,e),_(Y.$$.fragment,e),_(A.$$.fragment,e),_(Z.$$.fragment,e),_(B.$$.fragment,e),_(D.$$.fragment,e),Te=!0)},o(e){b(W.$$.fragment,e),b(F.$$.fragment,e),b(x.$$.fragment,e),b(R.$$.fragment,e),b(P.$$.fragment,e),b(Q.$$.fragment,e),b(L.$$.fragment,e),b(q.$$.fragment,e),b(J.$$.fragment,e),b(I.$$.fragment,e),b(S.$$.fragment,e),b(Y.$$.fragment,e),b(A.$$.fragment,e),b(Z.$$.fragment,e),b(B.$$.fragment,e),b(D.$$.fragment,e),Te=!1},d(e){e&&(o(c),o(s),o(l),o(r),o(v),o(j),o(me),o(pe),o(z),o(fe),o(E),o(ue),o(he),o(X),o(ge),o(_e),o(be),o(G),o(ye),o(Me),o(U),o($e),o(we),o(C),o(ve),o(Ce),o(ce)),o(t),y(W,e),y(F,e),y(x,e),y(R,e),y(P),y(Q,e),y(L),y(q),y(J),y(I),y(S,e),y(Y),y(A),y(Z),y(B),y(D,e)}}}const _t='{"title":"HGNet-V2","local":"hgnet-v2","sections":[{"title":"HGNetV2Config","local":"transformers.HGNetV2Config","sections":[],"depth":2},{"title":"HGNetV2Backbone","local":"transformers.HGNetV2Backbone","sections":[],"depth":2},{"title":"HGNetV2ForImageClassification","local":"transformers.HGNetV2ForImageClassification","sections":[],"depth":2}],"depth":1}';function bt($){return tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ht extends st{constructor(t){super(),nt(this,t,bt,gt,et,{})}}export{Ht as component};
