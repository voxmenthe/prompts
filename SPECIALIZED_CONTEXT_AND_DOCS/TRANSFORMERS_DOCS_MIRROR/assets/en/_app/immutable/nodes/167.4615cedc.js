import{s as Bt,o as Xt,n as je}from"../chunks/scheduler.18a86fab.js";import{S as Nt,i as Vt,g as f,s as a,r as u,A as Gt,h as g,f as s,c as r,j as B,x as w,u as _,k as P,l as Ht,y as i,a as d,v as y,d as T,t as b,w as M}from"../chunks/index.98837b22.js";import{T as Ft}from"../chunks/Tip.77304350.js";import{D as ce}from"../chunks/Docstring.a1ef7999.js";import{C as Re}from"../chunks/CodeBlock.8d0c2e8a.js";import{F as qt,M as Pt}from"../chunks/Markdown.ae01904b.js";import{E as It}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as He,E as Yt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Qt,a as Wt}from"../chunks/HfOption.6641485e.js";function St($){let t,h='This model was contributed by <a href="https://huggingface.co/stevenbucaille" rel="nofollow">stevenbucaille</a>.',o,l,p="Click on the EfficientLoFTR models in the right sidebar for more examples of how to apply EfficientLoFTR to different computer vision tasks.";return{c(){t=f("p"),t.innerHTML=h,o=a(),l=f("p"),l.textContent=p},l(n){t=g(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-1ir9jnx"&&(t.innerHTML=h),o=r(n),l=g(n,"P",{"data-svelte-h":!0}),w(l)!=="svelte-1gmq71y"&&(l.textContent=p)},m(n,v){d(n,t,v),d(n,o,v),d(n,l,v)},p:je,d(n){n&&(s(t),s(o),s(l))}}}function At($){let t,h;return t=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBa2V5cG9pbnRfbWF0Y2hlciUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJrZXlwb2ludC1tYXRjaGluZyUyMiUyQyUyMG1vZGVsJTNEJTIyemp1LWNvbW11bml0eSUyRmVmZmljaWVudGxvZnRyJTIyKSUwQSUwQXVybF8wJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF85ODE2OTg4OF8zMzQ3NzEwODUyLmpwZyUyMiUwQXVybF8xJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF8yNjc1NzAyN182NzE3MDg0MDYxLmpwZyUyMiUwQSUwQXJlc3VsdHMlMjAlM0QlMjBrZXlwb2ludF9tYXRjaGVyKCU1QnVybF8wJTJDJTIwdXJsXzElNUQlMkMlMjB0aHJlc2hvbGQlM0QwLjkpJTBBcHJpbnQocmVzdWx0cyU1QjAlNUQpJTBBJTIzJTIwJTdCJ2tleXBvaW50X2ltYWdlXzAnJTNBJTIwJTdCJ3gnJTNBJTIwLi4uJTJDJTIwJ3knJTNBJTIwLi4uJTdEJTJDJTIwJ2tleXBvaW50X2ltYWdlXzEnJTNBJTIwJTdCJ3gnJTNBJTIwLi4uJTJDJTIwJ3knJTNBJTIwLi4uJTdEJTJDJTIwJ3Njb3JlJyUzQSUyMC4uLiU3RA==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

keypoint_matcher = pipeline(task=<span class="hljs-string">&quot;keypoint-matching&quot;</span>, model=<span class="hljs-string">&quot;zju-community/efficientloftr&quot;</span>)

url_0 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
url_1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>

results = keypoint_matcher([url_0, url_1], threshold=<span class="hljs-number">0.9</span>)
<span class="hljs-built_in">print</span>(results[<span class="hljs-number">0</span>])
<span class="hljs-comment"># {&#x27;keypoint_image_0&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;keypoint_image_1&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;score&#x27;: ...}</span>`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){_(t.$$.fragment,o)},m(o,l){y(t,o,l),h=!0},p:je,i(o){h||(T(t.$$.fragment,o),h=!0)},o(o){b(t.$$.fragment,o),h=!1},d(o){M(t,o)}}}function Dt($){let t,h;return t=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvcktleXBvaW50TWF0Y2hpbmclMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsX2ltYWdlMSUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGcmVmcyUyRmhlYWRzJTJGbWFzdGVyJTJGYXNzZXRzJTJGcGhvdG90b3VyaXNtX3NhbXBsZV9pbWFnZXMlMkZ1bml0ZWRfc3RhdGVzX2NhcGl0b2xfOTgxNjk4ODhfMzM0NzcxMDg1Mi5qcGclMjIlMEFpbWFnZTElMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmxfaW1hZ2UxJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXVybF9pbWFnZTIlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRnJhdy5naXRodWJ1c2VyY29udGVudC5jb20lMkZtYWdpY2xlYXAlMkZTdXBlckdsdWVQcmV0cmFpbmVkTmV0d29yayUyRnJlZnMlMkZoZWFkcyUyRm1hc3RlciUyRmFzc2V0cyUyRnBob3RvdG91cmlzbV9zYW1wbGVfaW1hZ2VzJTJGdW5pdGVkX3N0YXRlc19jYXBpdG9sXzI2NzU3MDI3XzY3MTcwODQwNjEuanBnJTIyJTBBaW1hZ2UyJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsX2ltYWdlMiUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZXMlMjAlM0QlMjAlNUJpbWFnZTElMkMlMjBpbWFnZTIlNUQlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMnpqdS1jb21tdW5pdHklMkZlZmZpY2llbnRsb2Z0ciUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvcktleXBvaW50TWF0Y2hpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMnpqdS1jb21tdW5pdHklMkZlZmZpY2llbnRsb2Z0ciUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2guaW5mZXJlbmNlX21vZGUoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjBQb3N0LXByb2Nlc3MlMjB0byUyMGdldCUyMGtleXBvaW50cyUyMGFuZCUyMG1hdGNoZXMlMEFpbWFnZV9zaXplcyUyMCUzRCUyMCU1QiU1QihpbWFnZS5oZWlnaHQlMkMlMjBpbWFnZS53aWR0aCklMjBmb3IlMjBpbWFnZSUyMGluJTIwaW1hZ2VzJTVEJTVEJTBBcHJvY2Vzc2VkX291dHB1dHMlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX2tleXBvaW50X21hdGNoaW5nKG91dHB1dHMlMkMlMjBpbWFnZV9zaXplcyUyQyUyMHRocmVzaG9sZCUzRDAuMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForKeypointMatching
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url_image1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image1, stream=<span class="hljs-literal">True</span>).raw)
url_image2 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>
image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image2, stream=<span class="hljs-literal">True</span>).raw)

images = [image1, image2]

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;zju-community/efficientloftr&quot;</span>)
model = AutoModelForKeypointMatching.from_pretrained(<span class="hljs-string">&quot;zju-community/efficientloftr&quot;</span>)

inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Post-process to get keypoints and matches</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){_(t.$$.fragment,o)},m(o,l){y(t,o,l),h=!0},p:je,i(o){h||(T(t.$$.fragment,o),h=!0)},o(o){b(t.$$.fragment,o),h=!1},d(o){M(t,o)}}}function Kt($){let t,h,o,l;return t=new Wt({props:{id:"usage",option:"Pipeline",$$slots:{default:[At]},$$scope:{ctx:$}}}),o=new Wt({props:{id:"usage",option:"AutoModel",$$slots:{default:[Dt]},$$scope:{ctx:$}}}),{c(){u(t.$$.fragment),h=a(),u(o.$$.fragment)},l(p){_(t.$$.fragment,p),h=r(p),_(o.$$.fragment,p)},m(p,n){y(t,p,n),d(p,h,n),y(o,p,n),l=!0},p(p,n){const v={};n&2&&(v.$$scope={dirty:n,ctx:p}),t.$set(v);const X={};n&2&&(X.$$scope={dirty:n,ctx:p}),o.$set(X)},i(p){l||(T(t.$$.fragment,p),T(o.$$.fragment,p),l=!0)},o(p){b(t.$$.fragment,p),b(o.$$.fragment,p),l=!1},d(p){p&&s(h),M(t,p),M(o,p)}}}function Ot($){let t,h="Examples:",o,l,p;return l=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEVmZmljaWVudExvRlRSQ29uZmlnJTJDJTIwRWZmaWNpZW50TG9GVFJGb3JLZXlwb2ludE1hdGNoaW5nJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEVmZmljaWVudExvRlRSJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBFZmZpY2llbnRMb0ZUUkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMEVmZmljaWVudExvRlRSJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRWZmaWNpZW50TG9GVFJGb3JLZXlwb2ludE1hdGNoaW5nKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EfficientLoFTRConfig, EfficientLoFTRForKeypointMatching

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a EfficientLoFTR configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = EfficientLoFTRConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the EfficientLoFTR configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EfficientLoFTRForKeypointMatching(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=f("p"),t.textContent=h,o=a(),u(l.$$.fragment)},l(n){t=g(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=h),o=r(n),_(l.$$.fragment,n)},m(n,v){d(n,t,v),d(n,o,v),y(l,n,v),p=!0},p:je,i(n){p||(T(l.$$.fragment,n),p=!0)},o(n){b(l.$$.fragment,n),p=!1},d(n){n&&(s(t),s(o)),M(l,n)}}}function eo($){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=f("p"),t.innerHTML=h},l(o){t=g(o,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(o,l){d(o,t,l)},p:je,d(o){o&&s(t)}}}function to($){let t,h="Examples:",o,l,p;return l=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZtYWdpY2xlYXAlMkZTdXBlckdsdWVQcmV0cmFpbmVkTmV0d29yayUyRmJsb2IlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRmxvbmRvbl9icmlkZ2VfNzg5MTY2NzVfNDU2ODE0MTI4OC5qcGclM0ZyYXclM0R0cnVlJTIyJTBBaW1hZ2UxJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGYmxvYiUyRm1hc3RlciUyRmFzc2V0cyUyRnBob3RvdG91cmlzbV9zYW1wbGVfaW1hZ2VzJTJGbG9uZG9uX2JyaWRnZV8xOTQ4MTc5N18yMjk1ODkyNDIxLmpwZyUzRnJhdyUzRHRydWUlMjIlMEFpbWFnZTIlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2VzJTIwJTNEJTIwJTVCaW1hZ2UxJTJDJTIwaW1hZ2UyJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50X2xvZnRyJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50X2xvZnRyJTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>images = [image1, image2]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;zju-community/efficient_loftr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;zju-community/efficient_loftr&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">&gt;&gt;&gt; </span>    inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>    outputs = model(**inputs)`,wrap:!1}}),{c(){t=f("p"),t.textContent=h,o=a(),u(l.$$.fragment)},l(n){t=g(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=h),o=r(n),_(l.$$.fragment,n)},m(n,v){d(n,t,v),d(n,o,v),y(l,n,v),p=!0},p:je,i(n){p||(T(l.$$.fragment,n),p=!0)},o(n){b(l.$$.fragment,n),p=!1},d(n){n&&(s(t),s(o)),M(l,n)}}}function oo($){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=f("p"),t.innerHTML=h},l(o){t=g(o,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(o,l){d(o,t,l)},p:je,d(o){o&&s(t)}}}function no($){let t,h="Examples:",o,l,p;return l=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZtYWdpY2xlYXAlMkZTdXBlckdsdWVQcmV0cmFpbmVkTmV0d29yayUyRmJsb2IlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRmxvbmRvbl9icmlkZ2VfNzg5MTY2NzVfNDU2ODE0MTI4OC5qcGclM0ZyYXclM0R0cnVlJTIyJTBBaW1hZ2UxJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGYmxvYiUyRm1hc3RlciUyRmFzc2V0cyUyRnBob3RvdG91cmlzbV9zYW1wbGVfaW1hZ2VzJTJGbG9uZG9uX2JyaWRnZV8xOTQ4MTc5N18yMjk1ODkyNDIxLmpwZyUzRnJhdyUzRHRydWUlMjIlMEFpbWFnZTIlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2VzJTIwJTNEJTIwJTVCaW1hZ2UxJTJDJTIwaW1hZ2UyJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50X2xvZnRyJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50X2xvZnRyJTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>images = [image1, image2]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;zju-community/efficient_loftr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;zju-community/efficient_loftr&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">&gt;&gt;&gt; </span>    inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>    outputs = model(**inputs)`,wrap:!1}}),{c(){t=f("p"),t.textContent=h,o=a(),u(l.$$.fragment)},l(n){t=g(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=h),o=r(n),_(l.$$.fragment,n)},m(n,v){d(n,t,v),d(n,o,v),y(l,n,v),p=!0},p:je,i(n){p||(T(l.$$.fragment,n),p=!0)},o(n){b(l.$$.fragment,n),p=!1},d(n){n&&(s(t),s(o)),M(l,n)}}}function so($){let t,h,o,l,p,n,v="EfficientLoFTR model taking images as inputs and outputting the features of the images.",X,me,Fe=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,S,A,oe=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qe,U,x,ge,N,ot='The <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel">EfficientLoFTRModel</a> forward method, overrides the <code>__call__</code> special method.',he,Z,ue,z,de,j,ne="<li>forward</li>",D,se,_e,R,K,ae,pe,Pe="EfficientLoFTR model taking images as inputs and outputting the matching of them.",O,H,nt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ie,V,Ye=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ee,I,re,Ue,G,Qe='The <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRForKeypointMatching">EfficientLoFTRForKeypointMatching</a> forward method, overrides the <code>__call__</code> special method.',ie,te,ye,k,Ze,Y,le="<li>forward</li>",fe;return t=new He({props:{title:"EfficientLoFTRModel",local:"transformers.EfficientLoFTRModel",headingTag:"h2"}}),l=new ce({props:{name:"class transformers.EfficientLoFTRModel",anchor:"transformers.EfficientLoFTRModel",parameters:[{name:"config",val:": EfficientLoFTRConfig"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig">EfficientLoFTRConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/modeling_efficientloftr.py#L672"}}),x=new ce({props:{name:"forward",anchor:"transformers.EfficientLoFTRModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessor">EfficientLoFTRImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">EfficientLoFTRImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessor">EfficientLoFTRImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.EfficientLoFTRModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/modeling_efficientloftr.py#L683",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BackboneOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig"
>EfficientLoFTRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>feature_maps</strong> (<code>tuple(torch.FloatTensor)</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Feature maps of the stages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code> or <code>(batch_size, num_channels, height, width)</code>,
depending on the backbone.</p>
<p>Hidden-states of the model at the output of each stage plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Only applicable if the backbone uses attention.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BackboneOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new Ft({props:{$$slots:{default:[eo]},$$scope:{ctx:$}}}),z=new It({props:{anchor:"transformers.EfficientLoFTRModel.forward.example",$$slots:{default:[to]},$$scope:{ctx:$}}}),se=new He({props:{title:"EfficientLoFTRForKeypointMatching",local:"transformers.EfficientLoFTRForKeypointMatching",headingTag:"h2"}}),K=new ce({props:{name:"class transformers.EfficientLoFTRForKeypointMatching",anchor:"transformers.EfficientLoFTRForKeypointMatching",parameters:[{name:"config",val:": EfficientLoFTRConfig"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRForKeypointMatching.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig">EfficientLoFTRConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/modeling_efficientloftr.py#L890"}}),re=new ce({props:{name:"forward",anchor:"transformers.EfficientLoFTRForKeypointMatching.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRForKeypointMatching.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.EfficientLoFTRForKeypointMatching.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/modeling_efficientloftr.py#L1251",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.efficientloftr.modeling_efficientloftr.KeypointMatchingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>None</code>) and inputs.</p>
<ul>
<li><strong>matches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Index of keypoint matched in the other image.</li>
<li><strong>matching_scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Scores of predicted matches.</li>
<li><strong>keypoints</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, 2)</code>) — Absolute (x, y) coordinates of predicted keypoints in a given image.</li>
<li><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, 2, num_channels, num_keypoints)</code>, returned when <code>output_hidden_states=True</code> is passed or when
<code>config.output_hidden_states=True</code>)</li>
<li><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, 2, num_heads, num_keypoints, num_keypoints)</code>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.efficientloftr.modeling_efficientloftr.KeypointMatchingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),te=new Ft({props:{$$slots:{default:[oo]},$$scope:{ctx:$}}}),k=new It({props:{anchor:"transformers.EfficientLoFTRForKeypointMatching.forward.example",$$slots:{default:[no]},$$scope:{ctx:$}}}),{c(){u(t.$$.fragment),h=a(),o=f("div"),u(l.$$.fragment),p=a(),n=f("p"),n.textContent=v,X=a(),me=f("p"),me.innerHTML=Fe,S=a(),A=f("p"),A.innerHTML=oe,qe=a(),U=f("div"),u(x.$$.fragment),ge=a(),N=f("p"),N.innerHTML=ot,he=a(),u(Z.$$.fragment),ue=a(),u(z.$$.fragment),de=a(),j=f("ul"),j.innerHTML=ne,D=a(),u(se.$$.fragment),_e=a(),R=f("div"),u(K.$$.fragment),ae=a(),pe=f("p"),pe.textContent=Pe,O=a(),H=f("p"),H.innerHTML=nt,Ie=a(),V=f("p"),V.innerHTML=Ye,ee=a(),I=f("div"),u(re.$$.fragment),Ue=a(),G=f("p"),G.innerHTML=Qe,ie=a(),u(te.$$.fragment),ye=a(),u(k.$$.fragment),Ze=a(),Y=f("ul"),Y.innerHTML=le,this.h()},l(c){_(t.$$.fragment,c),h=r(c),o=g(c,"DIV",{class:!0});var J=B(o);_(l.$$.fragment,J),p=r(J),n=g(J,"P",{"data-svelte-h":!0}),w(n)!=="svelte-1xwfxdq"&&(n.textContent=v),X=r(J),me=g(J,"P",{"data-svelte-h":!0}),w(me)!=="svelte-q52n56"&&(me.innerHTML=Fe),S=r(J),A=g(J,"P",{"data-svelte-h":!0}),w(A)!=="svelte-hswkmf"&&(A.innerHTML=oe),qe=r(J),U=g(J,"DIV",{class:!0});var C=B(U);_(x.$$.fragment,C),ge=r(C),N=g(C,"P",{"data-svelte-h":!0}),w(N)!=="svelte-1oqbeix"&&(N.innerHTML=ot),he=r(C),_(Z.$$.fragment,C),ue=r(C),_(z.$$.fragment,C),C.forEach(s),J.forEach(s),de=r(c),j=g(c,"UL",{"data-svelte-h":!0}),w(j)!=="svelte-n3ow4o"&&(j.innerHTML=ne),D=r(c),_(se.$$.fragment,c),_e=r(c),R=g(c,"DIV",{class:!0});var F=B(R);_(K.$$.fragment,F),ae=r(F),pe=g(F,"P",{"data-svelte-h":!0}),w(pe)!=="svelte-jckufx"&&(pe.textContent=Pe),O=r(F),H=g(F,"P",{"data-svelte-h":!0}),w(H)!=="svelte-q52n56"&&(H.innerHTML=nt),Ie=r(F),V=g(F,"P",{"data-svelte-h":!0}),w(V)!=="svelte-hswkmf"&&(V.innerHTML=Ye),ee=r(F),I=g(F,"DIV",{class:!0});var L=B(I);_(re.$$.fragment,L),Ue=r(L),G=g(L,"P",{"data-svelte-h":!0}),w(G)!=="svelte-5wznjp"&&(G.innerHTML=Qe),ie=r(L),_(te.$$.fragment,L),ye=r(L),_(k.$$.fragment,L),L.forEach(s),F.forEach(s),Ze=r(c),Y=g(c,"UL",{"data-svelte-h":!0}),w(Y)!=="svelte-n3ow4o"&&(Y.innerHTML=le),this.h()},h(){P(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,J){y(t,c,J),d(c,h,J),d(c,o,J),y(l,o,null),i(o,p),i(o,n),i(o,X),i(o,me),i(o,S),i(o,A),i(o,qe),i(o,U),y(x,U,null),i(U,ge),i(U,N),i(U,he),y(Z,U,null),i(U,ue),y(z,U,null),d(c,de,J),d(c,j,J),d(c,D,J),y(se,c,J),d(c,_e,J),d(c,R,J),y(K,R,null),i(R,ae),i(R,pe),i(R,O),i(R,H),i(R,Ie),i(R,V),i(R,ee),i(R,I),y(re,I,null),i(I,Ue),i(I,G),i(I,ie),y(te,I,null),i(I,ye),y(k,I,null),d(c,Ze,J),d(c,Y,J),fe=!0},p(c,J){const C={};J&2&&(C.$$scope={dirty:J,ctx:c}),Z.$set(C);const F={};J&2&&(F.$$scope={dirty:J,ctx:c}),z.$set(F);const L={};J&2&&(L.$$scope={dirty:J,ctx:c}),te.$set(L);const W={};J&2&&(W.$$scope={dirty:J,ctx:c}),k.$set(W)},i(c){fe||(T(t.$$.fragment,c),T(l.$$.fragment,c),T(x.$$.fragment,c),T(Z.$$.fragment,c),T(z.$$.fragment,c),T(se.$$.fragment,c),T(K.$$.fragment,c),T(re.$$.fragment,c),T(te.$$.fragment,c),T(k.$$.fragment,c),fe=!0)},o(c){b(t.$$.fragment,c),b(l.$$.fragment,c),b(x.$$.fragment,c),b(Z.$$.fragment,c),b(z.$$.fragment,c),b(se.$$.fragment,c),b(K.$$.fragment,c),b(re.$$.fragment,c),b(te.$$.fragment,c),b(k.$$.fragment,c),fe=!1},d(c){c&&(s(h),s(o),s(de),s(j),s(D),s(_e),s(R),s(Ze),s(Y)),M(t,c),M(l),M(x),M(Z),M(z),M(se,c),M(K),M(re),M(te),M(k)}}}function ao($){let t,h;return t=new Pt({props:{$$slots:{default:[so]},$$scope:{ctx:$}}}),{c(){u(t.$$.fragment)},l(o){_(t.$$.fragment,o)},m(o,l){y(t,o,l),h=!0},p(o,l){const p={};l&2&&(p.$$scope={dirty:l,ctx:o}),t.$set(p)},i(o){h||(T(t.$$.fragment,o),h=!0)},o(o){b(t.$$.fragment,o),h=!1},d(o){M(t,o)}}}function ro($){let t,h,o,l,p,n="<em>This model was released on 2024-03-07 and added to Hugging Face Transformers on 2025-07-22.</em>",v,X,me='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Fe,S,A,oe,qe='<a href="https://huggingface.co/papers/2403.04765" rel="nofollow">EfficientLoFTR</a> is an efficient detector-free local feature matching method that produces semi-dense matches across images with sparse-like speed. It builds upon the original <a href="https://huggingface.co/papers/2104.00680" rel="nofollow">LoFTR</a> architecture but introduces significant improvements for both efficiency and accuracy. The key innovation is an aggregated attention mechanism with adaptive token selection that makes the model ~2.5× faster than LoFTR while achieving higher accuracy. EfficientLoFTR can even surpass state-of-the-art efficient sparse matching pipelines like <a href="./superpoint">SuperPoint</a> + <a href="./lightglue">LightGlue</a> in terms of speed, making it suitable for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction.',U,x,ge,N,ot='The example below demonstrates how to match keypoints between two images with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',he,Z,ue,z,de,j,ne,D,se="EfficientLoFTR is designed for efficiency while maintaining high accuracy. It uses an aggregated attention mechanism with adaptive token selection to reduce computational overhead compared to the original LoFTR.",_e,R,K,ae,pe="<p>The model produces semi-dense matches, offering a good balance between the density of matches and computational efficiency. It excels in handling large viewpoint changes and texture-poor scenarios.</p>",Pe,O,H,nt='For better visualization and analysis, use the <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessor.post_process_keypoint_matching">post_process_keypoint_matching()</a> method to get matches in a more readable format.',Ie,V,Ye,ee,I,re="Visualize the matches between the images using the built-in plotting functionality.",Ue,G,Qe,ie,te="<p>EfficientLoFTR uses a novel two-stage correlation layer that achieves accurate subpixel correspondences, improving upon the original LoFTR’s fine correlation module.</p>",ye,k,Ze='<img src="https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/2nJZQlFToCYp_iLurvcZ4.png"/>',Y,le,fe,c,J='<li>Refer to the <a href="https://github.com/zju3dv/EfficientLoFTR" rel="nofollow">original EfficientLoFTR repository</a> for more examples and implementation details.</li> <li><a href="https://zju3dv.github.io/efficientloftr/" rel="nofollow">EfficientLoFTR project page</a> with interactive demos and additional information.</li>',C,F,L,W,ke,ut,Se,Ut=`This is the configuration class to store the configuration of a <code>EffientLoFTRFromKeypointMatching</code>.
It is used to instantiate a EfficientLoFTR model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the
EfficientLoFTR <a href="https://huggingface.co/zju-community/efficientloftr" rel="nofollow">zju-community/efficientloftr</a> architecture.`,_t,Ae,Zt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,yt,Te,at,Ee,rt,E,xe,Tt,De,kt="Constructs a EfficientLoFTR image processor.",bt,be,ze,Mt,Ke,Et=`Converts the raw output of <code>KeypointMatchingOutput</code> into lists of keypoints, scores and descriptors
with coordinates absolute to the original image sizes.`,wt,Me,Ce,Jt,Oe,xt="Preprocess an image or batch of images.",vt,we,Le,$t,et,zt="Resize an image.",Rt,Je,We,jt,tt,Ct="Plots the image pairs side by side with the detected keypoints as well as the matching between them.",it,Be,Lt="<li>preprocess</li> <li>post_process_keypoint_matching</li> <li>visualize_keypoint_matching</li>",lt,ve,ct,Xe,mt,st,dt;return S=new He({props:{title:"EfficientLoFTR",local:"efficientloftr",headingTag:"h1"}}),x=new Ft({props:{warning:!1,$$slots:{default:[St]},$$scope:{ctx:$}}}),Z=new Qt({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[Kt]},$$scope:{ctx:$}}}),z=new He({props:{title:"Notes",local:"notes",headingTag:"h2"}}),R=new Re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvcktleXBvaW50TWF0Y2hpbmclMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50bG9mdHIlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JLZXlwb2ludE1hdGNoaW5nLmZyb21fcHJldHJhaW5lZCglMjJ6anUtY29tbXVuaXR5JTJGZWZmaWNpZW50bG9mdHIlMjIpJTBBJTBBJTIzJTIwRWZmaWNpZW50TG9GVFIlMjByZXF1aXJlcyUyMHBhaXJzJTIwb2YlMjBpbWFnZXMlMEFpbWFnZXMlMjAlM0QlMjAlNUJpbWFnZTElMkMlMjBpbWFnZTIlNUQlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2guaW5mZXJlbmNlX21vZGUoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjBFeHRyYWN0JTIwbWF0Y2hpbmclMjBpbmZvcm1hdGlvbiUwQWtleXBvaW50cyUyMCUzRCUyMG91dHB1dHMua2V5cG9pbnRzJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwS2V5cG9pbnRzJTIwaW4lMjBib3RoJTIwaW1hZ2VzJTBBbWF0Y2hlcyUyMCUzRCUyMG91dHB1dHMubWF0Y2hlcyUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMyUyME1hdGNoaW5nJTIwaW5kaWNlcyUyMCUwQW1hdGNoaW5nX3Njb3JlcyUyMCUzRCUyMG91dHB1dHMubWF0Y2hpbmdfc2NvcmVzJTIwJTIwJTIzJTIwQ29uZmlkZW5jZSUyMHNjb3Jlcw==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForKeypointMatching
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;zju-community/efficientloftr&quot;</span>)
model = AutoModelForKeypointMatching.from_pretrained(<span class="hljs-string">&quot;zju-community/efficientloftr&quot;</span>)

<span class="hljs-comment"># EfficientLoFTR requires pairs of images</span>
images = [image1, image2]
inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Extract matching information</span>
keypoints = outputs.keypoints        <span class="hljs-comment"># Keypoints in both images</span>
matches = outputs.matches            <span class="hljs-comment"># Matching indices </span>
matching_scores = outputs.matching_scores  <span class="hljs-comment"># Confidence scores</span>`,wrap:!1}}),V=new Re({props:{code:"JTIzJTIwUHJvY2VzcyUyMG91dHB1dHMlMjBmb3IlMjB2aXN1YWxpemF0aW9uJTBBaW1hZ2Vfc2l6ZXMlMjAlM0QlMjAlNUIlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTIwZm9yJTIwaW1hZ2UlMjBpbiUyMGltYWdlcyU1RCU1RCUwQXByb2Nlc3NlZF9vdXRwdXRzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19rZXlwb2ludF9tYXRjaGluZyhvdXRwdXRzJTJDJTIwaW1hZ2Vfc2l6ZXMlMkMlMjB0aHJlc2hvbGQlM0QwLjIpJTBBJTBBZm9yJTIwaSUyQyUyMG91dHB1dCUyMGluJTIwZW51bWVyYXRlKHByb2Nlc3NlZF9vdXRwdXRzKSUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGYlMjJGb3IlMjB0aGUlMjBpbWFnZSUyMHBhaXIlMjAlN0JpJTdEJTIyKSUwQSUyMCUyMCUyMCUyMGZvciUyMGtleXBvaW50MCUyQyUyMGtleXBvaW50MSUyQyUyMG1hdGNoaW5nX3Njb3JlJTIwaW4lMjB6aXAoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3V0cHV0JTVCJTIya2V5cG9pbnRzMCUyMiU1RCUyQyUyMG91dHB1dCU1QiUyMmtleXBvaW50czElMjIlNUQlMkMlMjBvdXRwdXQlNUIlMjJtYXRjaGluZ19zY29yZXMlMjIlNUQlMEElMjAlMjAlMjAlMjApJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMktleXBvaW50JTIwYXQlMjAlN0JrZXlwb2ludDAubnVtcHkoKSU3RCUyMG1hdGNoZXMlMjB3aXRoJTIwa2V5cG9pbnQlMjBhdCUyMCU3QmtleXBvaW50MS5udW1weSgpJTdEJTIwd2l0aCUyMHNjb3JlJTIwJTdCbWF0Y2hpbmdfc2NvcmUlN0QlMjIp",highlighted:`<span class="hljs-comment"># Process outputs for visualization</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)

<span class="hljs-keyword">for</span> i, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(processed_outputs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;For the image pair <span class="hljs-subst">{i}</span>&quot;</span>)
    <span class="hljs-keyword">for</span> keypoint0, keypoint1, matching_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(
            output[<span class="hljs-string">&quot;keypoints0&quot;</span>], output[<span class="hljs-string">&quot;keypoints1&quot;</span>], output[<span class="hljs-string">&quot;matching_scores&quot;</span>]
    ):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keypoint at <span class="hljs-subst">{keypoint0.numpy()}</span> matches with keypoint at <span class="hljs-subst">{keypoint1.numpy()}</span> with score <span class="hljs-subst">{matching_score}</span>&quot;</span>)`,wrap:!1}}),G=new Re({props:{code:"JTIzJTIwRWFzeSUyMHZpc3VhbGl6YXRpb24lMjB1c2luZyUyMHRoZSUyMGJ1aWx0LWluJTIwcGxvdHRpbmclMjBtZXRob2QlMEF2aXN1YWxpemVkX2ltYWdlcyUyMCUzRCUyMHByb2Nlc3Nvci52aXN1YWxpemVfa2V5cG9pbnRfbWF0Y2hpbmcoaW1hZ2VzJTJDJTIwcHJvY2Vzc2VkX291dHB1dHMp",highlighted:`<span class="hljs-comment"># Easy visualization using the built-in plotting method</span>
visualized_images = processor.visualize_keypoint_matching(images, processed_outputs)`,wrap:!1}}),le=new He({props:{title:"Resources",local:"resources",headingTag:"h2"}}),F=new He({props:{title:"EfficientLoFTRConfig",local:"transformers.EfficientLoFTRConfig",headingTag:"h2"}}),ke=new ce({props:{name:"class transformers.EfficientLoFTRConfig",anchor:"transformers.EfficientLoFTRConfig",parameters:[{name:"stage_num_blocks",val:": typing.Optional[list[int]] = None"},{name:"out_features",val:": typing.Optional[list[int]] = None"},{name:"stage_stride",val:": typing.Optional[list[int]] = None"},{name:"hidden_size",val:": int = 256"},{name:"activation_function",val:": str = 'relu'"},{name:"q_aggregation_kernel_size",val:": int = 4"},{name:"kv_aggregation_kernel_size",val:": int = 4"},{name:"q_aggregation_stride",val:": int = 4"},{name:"kv_aggregation_stride",val:": int = 4"},{name:"num_attention_layers",val:": int = 4"},{name:"num_attention_heads",val:": int = 8"},{name:"attention_dropout",val:": float = 0.0"},{name:"attention_bias",val:": bool = False"},{name:"mlp_activation_function",val:": str = 'leaky_relu'"},{name:"coarse_matching_skip_softmax",val:": bool = False"},{name:"coarse_matching_threshold",val:": float = 0.2"},{name:"coarse_matching_temperature",val:": float = 0.1"},{name:"coarse_matching_border_removal",val:": int = 2"},{name:"fine_kernel_size",val:": int = 8"},{name:"batch_norm_eps",val:": float = 1e-05"},{name:"rope_theta",val:": float = 10000.0"},{name:"partial_rotary_factor",val:": float = 4.0"},{name:"rope_scaling",val:": typing.Optional[dict] = None"},{name:"fine_matching_slice_dim",val:": int = 8"},{name:"fine_matching_regress_temperature",val:": float = 10.0"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientLoFTRConfig.stage_num_blocks",description:`<strong>stage_num_blocks</strong> (<code>List</code>, <em>optional</em>, defaults to [1, 2, 4, 14]) &#x2014;
The number of blocks in each stages`,name:"stage_num_blocks"},{anchor:"transformers.EfficientLoFTRConfig.out_features",description:`<strong>out_features</strong> (<code>List</code>, <em>optional</em>, defaults to [64, 64, 128, 256]) &#x2014;
The number of channels in each stage`,name:"out_features"},{anchor:"transformers.EfficientLoFTRConfig.stage_stride",description:`<strong>stage_stride</strong> (<code>List</code>, <em>optional</em>, defaults to [2, 1, 2, 2]) &#x2014;
The stride used in each stage`,name:"stage_stride"},{anchor:"transformers.EfficientLoFTRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimension of the descriptors.`,name:"hidden_size"},{anchor:"transformers.EfficientLoFTRConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The activation function used in the backbone`,name:"activation_function"},{anchor:"transformers.EfficientLoFTRConfig.q_aggregation_kernel_size",description:`<strong>q_aggregation_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The kernel size of the aggregation of query states in the fusion network`,name:"q_aggregation_kernel_size"},{anchor:"transformers.EfficientLoFTRConfig.kv_aggregation_kernel_size",description:`<strong>kv_aggregation_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The kernel size of the aggregation of key and value states in the fusion network`,name:"kv_aggregation_kernel_size"},{anchor:"transformers.EfficientLoFTRConfig.q_aggregation_stride",description:`<strong>q_aggregation_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The stride of the aggregation of query states in the fusion network`,name:"q_aggregation_stride"},{anchor:"transformers.EfficientLoFTRConfig.kv_aggregation_stride",description:`<strong>kv_aggregation_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The stride of the aggregation of key and value states in the fusion network`,name:"kv_aggregation_stride"},{anchor:"transformers.EfficientLoFTRConfig.num_attention_layers",description:`<strong>num_attention_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention layers in the LocalFeatureTransformer`,name:"num_attention_layers"},{anchor:"transformers.EfficientLoFTRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The number of heads in the GNN layers.`,name:"num_attention_heads"},{anchor:"transformers.EfficientLoFTRConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.EfficientLoFTRConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a bias in the query, key, value and output projection layers during attention.`,name:"attention_bias"},{anchor:"transformers.EfficientLoFTRConfig.mlp_activation_function",description:`<strong>mlp_activation_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;leaky_relu&quot;</code>) &#x2014;
Activation function used in the attention mlp layer.`,name:"mlp_activation_function"},{anchor:"transformers.EfficientLoFTRConfig.coarse_matching_skip_softmax",description:`<strong>coarse_matching_skip_softmax</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to skip softmax or not at the coarse matching step.`,name:"coarse_matching_skip_softmax"},{anchor:"transformers.EfficientLoFTRConfig.coarse_matching_threshold",description:`<strong>coarse_matching_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The threshold for the minimum score required for a match.`,name:"coarse_matching_threshold"},{anchor:"transformers.EfficientLoFTRConfig.coarse_matching_temperature",description:`<strong>coarse_matching_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The temperature to apply to the coarse similarity matrix`,name:"coarse_matching_temperature"},{anchor:"transformers.EfficientLoFTRConfig.coarse_matching_border_removal",description:`<strong>coarse_matching_border_removal</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The size of the border to remove during coarse matching`,name:"coarse_matching_border_removal"},{anchor:"transformers.EfficientLoFTRConfig.fine_kernel_size",description:`<strong>fine_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Kernel size used for the fine feature matching`,name:"fine_kernel_size"},{anchor:"transformers.EfficientLoFTRConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"},{anchor:"transformers.EfficientLoFTRConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.EfficientLoFTRConfig.partial_rotary_factor",description:`<strong>partial_rotary_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Dim factor for the RoPE embeddings, in EfficientLoFTR, frequencies should be generated for
the whole hidden_size, so this factor is used to compensate.`,name:"partial_rotary_factor"},{anchor:"transformers.EfficientLoFTRConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
and you expect the model to work on longer <code>max_position_embeddings</code>, we recommend you to update this value
accordingly.
Expected contents:
<code>rope_type</code> (<code>str</code>):
The sub-variant of RoPE to use. Can be one of [&#x2018;default&#x2019;, &#x2018;linear&#x2019;, &#x2018;dynamic&#x2019;, &#x2018;yarn&#x2019;, &#x2018;longrope&#x2019;,
&#x2018;llama3&#x2019;, &#x2018;2d&#x2019;], with &#x2018;default&#x2019; being the original RoPE implementation.
<code>dim</code> (<code>int</code>): The dimension of the RoPE embeddings.`,name:"rope_scaling"},{anchor:"transformers.EfficientLoFTRConfig.fine_matching_slice_dim",description:`<strong>fine_matching_slice_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The size of the slice used to divide the fine features for the first and second fine matching stages.`,name:"fine_matching_slice_dim"},{anchor:"transformers.EfficientLoFTRConfig.fine_matching_regress_temperature",description:`<strong>fine_matching_regress_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 10.0) &#x2014;
The temperature to apply to the fine similarity matrix`,name:"fine_matching_regress_temperature"},{anchor:"transformers.EfficientLoFTRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/configuration_efficientloftr.py#L20"}}),Te=new It({props:{anchor:"transformers.EfficientLoFTRConfig.example",$$slots:{default:[Ot]},$$scope:{ctx:$}}}),Ee=new He({props:{title:"EfficientLoFTRImageProcessor",local:"transformers.EfficientLoFTRImageProcessor",headingTag:"h2"}}),xe=new ce({props:{name:"class transformers.EfficientLoFTRImageProcessor",anchor:"transformers.EfficientLoFTRImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": float = 0.00392156862745098"},{name:"do_grayscale",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientLoFTRImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden
by <code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.EfficientLoFTRImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 480, &quot;width&quot;: 640}</code>):
Resolution of the output image after <code>resize</code> is applied. Only has an effect if <code>do_resize</code> is set to
<code>True</code>. Can be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.EfficientLoFTRImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.EfficientLoFTRImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.EfficientLoFTRImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.EfficientLoFTRImageProcessor.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to grayscale. Can be overridden by <code>do_grayscale</code> in the <code>preprocess</code> method.`,name:"do_grayscale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/image_processing_efficientloftr.py#L135"}}),ze=new ce({props:{name:"post_process_keypoint_matching",anchor:"transformers.EfficientLoFTRImageProcessor.post_process_keypoint_matching",parameters:[{name:"outputs",val:": KeypointMatchingOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]]"},{name:"threshold",val:": float = 0.0"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRImageProcessor.post_process_keypoint_matching.outputs",description:`<strong>outputs</strong> (<code>KeypointMatchingOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.EfficientLoFTRImageProcessor.post_process_keypoint_matching.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>List[Tuple[Tuple[int, int]]]</code>, <em>optional</em>) &#x2014;
Tensor of shape <code>(batch_size, 2, 2)</code> or list of tuples of tuples (<code>Tuple[int, int]</code>) containing the
target size <code>(height, width)</code> of each image in the batch. This must be the original image size (before
any processing).`,name:"target_sizes"},{anchor:"transformers.EfficientLoFTRImageProcessor.post_process_keypoint_matching.threshold",description:`<strong>threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold to filter out the matches with low scores.`,name:"threshold"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/image_processing_efficientloftr.py#L340",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints in the first and second image
of the pair, the matching scores and the matching indices.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[Dict]</code></p>
`}}),Ce=new ce({props:{name:"preprocess",anchor:"transformers.EfficientLoFTRImageProcessor.preprocess",parameters:[{name:"images",val:""},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_grayscale",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with
pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set
<code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output image after <code>resize</code> has been applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image
is resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the
image will be matched to <code>int(size[&quot;shortest_edge&quot;]/ crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of <code>PILImageResampling</code>, filters. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_grayscale</code>) &#x2014;
Whether to convert the image to grayscale.`,name:"do_grayscale"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.EfficientLoFTRImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/image_processing_efficientloftr.py#L222"}}),Le=new ce({props:{name:"resize",anchor:"transformers.EfficientLoFTRImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientLoFTRImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.EfficientLoFTRImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary of the form <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code>, specifying the size of the output image.`,name:"size"},{anchor:"transformers.EfficientLoFTRImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If not provided, it will be inferred from the input
image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.EfficientLoFTRImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/image_processing_efficientloftr.py#L182"}}),We=new ce({props:{name:"visualize_keypoint_matching",anchor:"transformers.EfficientLoFTRImageProcessor.visualize_keypoint_matching",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"keypoint_matching_output",val:": list"}],parametersDescription:[{anchor:"transformers.EfficientLoFTRImageProcessor.visualize_keypoint_matching.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to plot. Same as <code>EfficientLoFTRImageProcessor.preprocess</code>. Expects either a list of 2
images or a list of list of 2 images list with pixel values ranging from 0 to 255.`,name:"images"},{anchor:"transformers.EfficientLoFTRImageProcessor.visualize_keypoint_matching.keypoint_matching_output",description:`<strong>keypoint_matching_output</strong> (List[Dict[str, torch.Tensor]]]) &#x2014;
A post processed keypoint matching output`,name:"keypoint_matching_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientloftr/image_processing_efficientloftr.py#L399",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of PIL images, each containing the image pairs side by side with the detected
keypoints as well as the matching between them.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[PIL.Image.Image]</code></p>
`}}),ve=new qt({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[ao]},$$scope:{ctx:$}}}),Xe=new Yt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/efficientloftr.md"}}),{c(){t=f("meta"),h=a(),o=f("p"),l=a(),p=f("p"),p.innerHTML=n,v=a(),X=f("div"),X.innerHTML=me,Fe=a(),u(S.$$.fragment),A=a(),oe=f("p"),oe.innerHTML=qe,U=a(),u(x.$$.fragment),ge=a(),N=f("p"),N.innerHTML=ot,he=a(),u(Z.$$.fragment),ue=a(),u(z.$$.fragment),de=a(),j=f("ul"),ne=f("li"),D=f("p"),D.textContent=se,_e=a(),u(R.$$.fragment),K=a(),ae=f("li"),ae.innerHTML=pe,Pe=a(),O=f("li"),H=f("p"),H.innerHTML=nt,Ie=a(),u(V.$$.fragment),Ye=a(),ee=f("li"),I=f("p"),I.textContent=re,Ue=a(),u(G.$$.fragment),Qe=a(),ie=f("li"),ie.innerHTML=te,ye=a(),k=f("div"),k.innerHTML=Ze,Y=a(),u(le.$$.fragment),fe=a(),c=f("ul"),c.innerHTML=J,C=a(),u(F.$$.fragment),L=a(),W=f("div"),u(ke.$$.fragment),ut=a(),Se=f("p"),Se.innerHTML=Ut,_t=a(),Ae=f("p"),Ae.innerHTML=Zt,yt=a(),u(Te.$$.fragment),at=a(),u(Ee.$$.fragment),rt=a(),E=f("div"),u(xe.$$.fragment),Tt=a(),De=f("p"),De.textContent=kt,bt=a(),be=f("div"),u(ze.$$.fragment),Mt=a(),Ke=f("p"),Ke.innerHTML=Et,wt=a(),Me=f("div"),u(Ce.$$.fragment),Jt=a(),Oe=f("p"),Oe.textContent=xt,vt=a(),we=f("div"),u(Le.$$.fragment),$t=a(),et=f("p"),et.textContent=zt,Rt=a(),Je=f("div"),u(We.$$.fragment),jt=a(),tt=f("p"),tt.textContent=Ct,it=a(),Be=f("ul"),Be.innerHTML=Lt,lt=a(),u(ve.$$.fragment),ct=a(),u(Xe.$$.fragment),mt=a(),st=f("p"),this.h()},l(e){const m=Gt("svelte-u9bgzb",document.head);t=g(m,"META",{name:!0,content:!0}),m.forEach(s),h=r(e),o=g(e,"P",{}),B(o).forEach(s),l=r(e),p=g(e,"P",{"data-svelte-h":!0}),w(p)!=="svelte-mg18v4"&&(p.innerHTML=n),v=r(e),X=g(e,"DIV",{style:!0,"data-svelte-h":!0}),w(X)!=="svelte-wa5t4p"&&(X.innerHTML=me),Fe=r(e),_(S.$$.fragment,e),A=r(e),oe=g(e,"P",{"data-svelte-h":!0}),w(oe)!=="svelte-1j4x8za"&&(oe.innerHTML=qe),U=r(e),_(x.$$.fragment,e),ge=r(e),N=g(e,"P",{"data-svelte-h":!0}),w(N)!=="svelte-jt8707"&&(N.innerHTML=ot),he=r(e),_(Z.$$.fragment,e),ue=r(e),_(z.$$.fragment,e),de=r(e),j=g(e,"UL",{});var q=B(j);ne=g(q,"LI",{});var Ne=B(ne);D=g(Ne,"P",{"data-svelte-h":!0}),w(D)!=="svelte-uabfov"&&(D.textContent=se),_e=r(Ne),_(R.$$.fragment,Ne),Ne.forEach(s),K=r(q),ae=g(q,"LI",{"data-svelte-h":!0}),w(ae)!=="svelte-1nb1wj0"&&(ae.innerHTML=pe),Pe=r(q),O=g(q,"LI",{});var Ve=B(O);H=g(Ve,"P",{"data-svelte-h":!0}),w(H)!=="svelte-5e91ts"&&(H.innerHTML=nt),Ie=r(Ve),_(V.$$.fragment,Ve),Ve.forEach(s),Ye=r(q),ee=g(q,"LI",{});var Ge=B(ee);I=g(Ge,"P",{"data-svelte-h":!0}),w(I)!=="svelte-155ueni"&&(I.textContent=re),Ue=r(Ge),_(G.$$.fragment,Ge),Ge.forEach(s),Qe=r(q),ie=g(q,"LI",{"data-svelte-h":!0}),w(ie)!=="svelte-1sn02u6"&&(ie.innerHTML=te),q.forEach(s),ye=r(e),k=g(e,"DIV",{class:!0,"data-svelte-h":!0}),w(k)!=="svelte-1aycqk7"&&(k.innerHTML=Ze),Y=r(e),_(le.$$.fragment,e),fe=r(e),c=g(e,"UL",{"data-svelte-h":!0}),w(c)!=="svelte-66k600"&&(c.innerHTML=J),C=r(e),_(F.$$.fragment,e),L=r(e),W=g(e,"DIV",{class:!0});var $e=B(W);_(ke.$$.fragment,$e),ut=r($e),Se=g($e,"P",{"data-svelte-h":!0}),w(Se)!=="svelte-147a415"&&(Se.innerHTML=Ut),_t=r($e),Ae=g($e,"P",{"data-svelte-h":!0}),w(Ae)!=="svelte-1ek1ss9"&&(Ae.innerHTML=Zt),yt=r($e),_(Te.$$.fragment,$e),$e.forEach(s),at=r(e),_(Ee.$$.fragment,e),rt=r(e),E=g(e,"DIV",{class:!0});var Q=B(E);_(xe.$$.fragment,Q),Tt=r(Q),De=g(Q,"P",{"data-svelte-h":!0}),w(De)!=="svelte-14a1npe"&&(De.textContent=kt),bt=r(Q),be=g(Q,"DIV",{class:!0});var pt=B(be);_(ze.$$.fragment,pt),Mt=r(pt),Ke=g(pt,"P",{"data-svelte-h":!0}),w(Ke)!=="svelte-y19agd"&&(Ke.innerHTML=Et),pt.forEach(s),wt=r(Q),Me=g(Q,"DIV",{class:!0});var ft=B(Me);_(Ce.$$.fragment,ft),Jt=r(ft),Oe=g(ft,"P",{"data-svelte-h":!0}),w(Oe)!=="svelte-1x3yxsa"&&(Oe.textContent=xt),ft.forEach(s),vt=r(Q),we=g(Q,"DIV",{class:!0});var gt=B(we);_(Le.$$.fragment,gt),$t=r(gt),et=g(gt,"P",{"data-svelte-h":!0}),w(et)!=="svelte-1eb2h1k"&&(et.textContent=zt),gt.forEach(s),Rt=r(Q),Je=g(Q,"DIV",{class:!0});var ht=B(Je);_(We.$$.fragment,ht),jt=r(ht),tt=g(ht,"P",{"data-svelte-h":!0}),w(tt)!=="svelte-5x4wxx"&&(tt.textContent=Ct),ht.forEach(s),Q.forEach(s),it=r(e),Be=g(e,"UL",{"data-svelte-h":!0}),w(Be)!=="svelte-74hxmd"&&(Be.innerHTML=Lt),lt=r(e),_(ve.$$.fragment,e),ct=r(e),_(Xe.$$.fragment,e),mt=r(e),st=g(e,"P",{}),B(st).forEach(s),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",io),Ht(X,"float","right"),P(k,"class","flex justify-center"),P(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,m){i(document.head,t),d(e,h,m),d(e,o,m),d(e,l,m),d(e,p,m),d(e,v,m),d(e,X,m),d(e,Fe,m),y(S,e,m),d(e,A,m),d(e,oe,m),d(e,U,m),y(x,e,m),d(e,ge,m),d(e,N,m),d(e,he,m),y(Z,e,m),d(e,ue,m),y(z,e,m),d(e,de,m),d(e,j,m),i(j,ne),i(ne,D),i(ne,_e),y(R,ne,null),i(j,K),i(j,ae),i(j,Pe),i(j,O),i(O,H),i(O,Ie),y(V,O,null),i(j,Ye),i(j,ee),i(ee,I),i(ee,Ue),y(G,ee,null),i(j,Qe),i(j,ie),d(e,ye,m),d(e,k,m),d(e,Y,m),y(le,e,m),d(e,fe,m),d(e,c,m),d(e,C,m),y(F,e,m),d(e,L,m),d(e,W,m),y(ke,W,null),i(W,ut),i(W,Se),i(W,_t),i(W,Ae),i(W,yt),y(Te,W,null),d(e,at,m),y(Ee,e,m),d(e,rt,m),d(e,E,m),y(xe,E,null),i(E,Tt),i(E,De),i(E,bt),i(E,be),y(ze,be,null),i(be,Mt),i(be,Ke),i(E,wt),i(E,Me),y(Ce,Me,null),i(Me,Jt),i(Me,Oe),i(E,vt),i(E,we),y(Le,we,null),i(we,$t),i(we,et),i(E,Rt),i(E,Je),y(We,Je,null),i(Je,jt),i(Je,tt),d(e,it,m),d(e,Be,m),d(e,lt,m),y(ve,e,m),d(e,ct,m),y(Xe,e,m),d(e,mt,m),d(e,st,m),dt=!0},p(e,[m]){const q={};m&2&&(q.$$scope={dirty:m,ctx:e}),x.$set(q);const Ne={};m&2&&(Ne.$$scope={dirty:m,ctx:e}),Z.$set(Ne);const Ve={};m&2&&(Ve.$$scope={dirty:m,ctx:e}),Te.$set(Ve);const Ge={};m&2&&(Ge.$$scope={dirty:m,ctx:e}),ve.$set(Ge)},i(e){dt||(T(S.$$.fragment,e),T(x.$$.fragment,e),T(Z.$$.fragment,e),T(z.$$.fragment,e),T(R.$$.fragment,e),T(V.$$.fragment,e),T(G.$$.fragment,e),T(le.$$.fragment,e),T(F.$$.fragment,e),T(ke.$$.fragment,e),T(Te.$$.fragment,e),T(Ee.$$.fragment,e),T(xe.$$.fragment,e),T(ze.$$.fragment,e),T(Ce.$$.fragment,e),T(Le.$$.fragment,e),T(We.$$.fragment,e),T(ve.$$.fragment,e),T(Xe.$$.fragment,e),dt=!0)},o(e){b(S.$$.fragment,e),b(x.$$.fragment,e),b(Z.$$.fragment,e),b(z.$$.fragment,e),b(R.$$.fragment,e),b(V.$$.fragment,e),b(G.$$.fragment,e),b(le.$$.fragment,e),b(F.$$.fragment,e),b(ke.$$.fragment,e),b(Te.$$.fragment,e),b(Ee.$$.fragment,e),b(xe.$$.fragment,e),b(ze.$$.fragment,e),b(Ce.$$.fragment,e),b(Le.$$.fragment,e),b(We.$$.fragment,e),b(ve.$$.fragment,e),b(Xe.$$.fragment,e),dt=!1},d(e){e&&(s(h),s(o),s(l),s(p),s(v),s(X),s(Fe),s(A),s(oe),s(U),s(ge),s(N),s(he),s(ue),s(de),s(j),s(ye),s(k),s(Y),s(fe),s(c),s(C),s(L),s(W),s(at),s(rt),s(E),s(it),s(Be),s(lt),s(ct),s(mt),s(st)),s(t),M(S,e),M(x,e),M(Z,e),M(z,e),M(R),M(V),M(G),M(le,e),M(F,e),M(ke),M(Te),M(Ee,e),M(xe),M(ze),M(Ce),M(Le),M(We),M(ve,e),M(Xe,e)}}}const io='{"title":"EfficientLoFTR","local":"efficientloftr","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"EfficientLoFTRConfig","local":"transformers.EfficientLoFTRConfig","sections":[],"depth":2},{"title":"EfficientLoFTRImageProcessor","local":"transformers.EfficientLoFTRImageProcessor","sections":[],"depth":2},{"title":"EfficientLoFTRModel","local":"transformers.EfficientLoFTRModel","sections":[],"depth":2},{"title":"EfficientLoFTRForKeypointMatching","local":"transformers.EfficientLoFTRForKeypointMatching","sections":[],"depth":2}],"depth":1}';function lo($){return Xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class To extends Nt{constructor(t){super(),Vt(this,t,lo,ro,Bt,{})}}export{To as component};
