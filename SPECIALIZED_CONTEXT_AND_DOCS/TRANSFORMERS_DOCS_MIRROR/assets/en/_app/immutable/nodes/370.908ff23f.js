import{s as Dt,o as Et,n as Ft}from"../chunks/scheduler.18a86fab.js";import{S as Ht,i as It,g as i,s as r,r as p,A as St,h as a,f as n,c as s,j as z,x as c,u as f,k as $,y as t,a as d,v as h,d as u,t as _,w as g}from"../chunks/index.98837b22.js";import{T as At}from"../chunks/Tip.77304350.js";import{D as P}from"../chunks/Docstring.a1ef7999.js";import{H as xe,E as jt}from"../chunks/getInferenceSnippets.06c2775f.js";function Nt(ye){let m,S="This model is in maintenance mode only, so we won’t accept any new PRs changing its code.",y,T,B=`If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.
You can do so by running the following command: <code>pip install -U transformers==4.30.0</code>.`;return{c(){m=i("p"),m.textContent=S,y=r(),T=i("p"),T.innerHTML=B},l(w){m=a(w,"P",{"data-svelte-h":!0}),c(m)!=="svelte-lwu440"&&(m.textContent=S),y=s(w),T=a(w,"P",{"data-svelte-h":!0}),c(T)!=="svelte-4042uy"&&(T.innerHTML=B)},m(w,D){d(w,m,D),d(w,y,D),d(w,T,D)},p:Ft,d(w){w&&(n(m),n(y),n(T))}}}function Wt(ye){let m,S,y,T,B,w="<em>This model was released on 2020-06-12 and added to Hugging Face Transformers on 2023-06-20.</em>",D,A,Be,E,ft='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Re,F,qe,j,Ce,N,ht=`The <a href="https://huggingface.co/yjernite/retribert-base-uncased/tree/main" rel="nofollow">RetriBERT</a> model was proposed in the blog post <a href="https://yjernite.github.io/lfqa.html" rel="nofollow">Explain Anything Like I’m Five: A Model for Open Domain Long Form
Question Answering</a>. RetriBERT is a small model that uses either a single or
pair of BERT encoders with lower-dimension projection for dense semantic indexing of text.`,Le,W,ut=`This model was contributed by <a href="https://huggingface.co/yjernite" rel="nofollow">yjernite</a>. Code to train and use the model can be
found <a href="https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation" rel="nofollow">here</a>.`,Me,O,Pe,x,U,Ue,re,_t=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a>. It is used to instantiate a
RetriBertModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the RetriBERT
<a href="https://huggingface.co/yjernite/retribert-base-uncased" rel="nofollow">yjernite/retribert-base-uncased</a> architecture.`,Ve,se,gt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,De,V,Ee,l,K,Ke,ie,kt="Constructs a RetriBERT tokenizer.",Ge,ae,bt=`<a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertTokenizer">RetriBertTokenizer</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation splitting
and wordpiece.`,Je,de,vt=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`,Xe,R,G,Qe,ce,Tt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,Ye,le,wt="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",Ze,H,J,et,me,$t="Converts a sequence of tokens (string) in a single string.",tt,I,X,nt,pe,xt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Fe,Q,He,k,Y,ot,fe,zt="Construct a “fast” RetriBERT tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",rt,he,yt=`<a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertTokenizerFast">RetriBertTokenizerFast</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,st,ue,Bt=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,it,q,Z,at,_e,Rt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,dt,ge,qt="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",Ie,ee,Se,b,te,ct,ke,Ct="Bert Based model to embed queries or document for document retrieval.",lt,be,Lt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mt,ve,Mt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pt,Te,ne,Ae,oe,je,ze,Ne;return A=new xe({props:{title:"RetriBERT",local:"retribert",headingTag:"h1"}}),F=new At({props:{warning:!0,$$slots:{default:[Nt]},$$scope:{ctx:ye}}}),j=new xe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),O=new xe({props:{title:"RetriBertConfig",local:"transformers.RetriBertConfig",headingTag:"h2"}}),U=new P({props:{name:"class transformers.RetriBertConfig",anchor:"transformers.RetriBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 8"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"share_encoders",val:" = True"},{name:"projection_dim",val:" = 128"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the RetriBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a>`,name:"vocab_size"},{anchor:"transformers.RetriBertConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RetriBertConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RetriBertConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RetriBertConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RetriBertConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RetriBertConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RetriBertConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RetriBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RetriBertConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.RetriBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RetriBertConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RetriBertConfig.share_encoders",description:`<strong>share_encoders</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use the same Bert-type encoder for the queries and document`,name:"share_encoders"},{anchor:"transformers.RetriBertConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Final dimension of the query and document representation after projection`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/configuration_retribert.py#L24"}}),V=new xe({props:{title:"RetriBertTokenizer",local:"transformers.RetriBertTokenizer",headingTag:"h2"}}),K=new P({props:{name:"class transformers.RetriBertTokenizer",anchor:"transformers.RetriBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RetriBertTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RetriBertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizer.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L51"}}),G=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L183",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),J=new P({props:{name:"convert_tokens_to_string",anchor:"transformers.RetriBertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L178"}}),X=new P({props:{name:"get_special_tokens_mask",anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L208",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Q=new xe({props:{title:"RetriBertTokenizerFast",local:"transformers.RetriBertTokenizerFast",headingTag:"h2"}}),Y=new P({props:{name:"class transformers.RetriBertTokenizerFast",anchor:"transformers.RetriBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RetriBertTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RetriBertTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L32"}}),Z=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L121",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ee=new xe({props:{title:"RetriBertModel",local:"transformers.RetriBertModel",headingTag:"h2"}}),te=new P({props:{name:"class transformers.RetriBertModel",anchor:"transformers.RetriBertModel",parameters:[{name:"config",val:": RetriBertConfig"}],parametersDescription:[{anchor:"transformers.RetriBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/modeling_retribert.py#L82"}}),ne=new P({props:{name:"forward",anchor:"transformers.RetriBertModel.forward",parameters:[{name:"input_ids_query",val:": LongTensor"},{name:"attention_mask_query",val:": typing.Optional[torch.FloatTensor]"},{name:"input_ids_doc",val:": LongTensor"},{name:"attention_mask_doc",val:": typing.Optional[torch.FloatTensor]"},{name:"checkpoint_batch_size",val:": int = -1"}],parametersDescription:[{anchor:"transformers.RetriBertModel.forward.input_ids_query",description:`<strong>input_ids_query</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the queries in a batch.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_query"},{anchor:"transformers.RetriBertModel.forward.attention_mask_query",description:`<strong>attention_mask_query</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask_query"},{anchor:"transformers.RetriBertModel.forward.input_ids_doc",description:`<strong>input_ids_doc</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the documents in a batch.`,name:"input_ids_doc"},{anchor:"transformers.RetriBertModel.forward.attention_mask_doc",description:`<strong>attention_mask_doc</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on documents padding token indices.`,name:"attention_mask_doc"},{anchor:"transformers.RetriBertModel.forward.checkpoint_batch_size",description:`<strong>checkpoint_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>-1</code>) &#x2014;
If greater than 0, uses gradient checkpointing to only compute sequence representation on
<code>checkpoint_batch_size</code> examples at a time on the GPU. All query representations are still compared to
all document representations in the batch.`,name:"checkpoint_batch_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/retribert/modeling_retribert.py#L170",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The bidirectional cross-entropy loss obtained while trying to match each query to its
corresponding document and each document to its corresponding query in the batch</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>\`torch.FloatTensor“</p>
`}}),oe=new jt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/retribert.md"}}),{c(){m=i("meta"),S=r(),y=i("p"),T=r(),B=i("p"),B.innerHTML=w,D=r(),p(A.$$.fragment),Be=r(),E=i("div"),E.innerHTML=ft,Re=r(),p(F.$$.fragment),qe=r(),p(j.$$.fragment),Ce=r(),N=i("p"),N.innerHTML=ht,Le=r(),W=i("p"),W.innerHTML=ut,Me=r(),p(O.$$.fragment),Pe=r(),x=i("div"),p(U.$$.fragment),Ue=r(),re=i("p"),re.innerHTML=_t,Ve=r(),se=i("p"),se.innerHTML=gt,De=r(),p(V.$$.fragment),Ee=r(),l=i("div"),p(K.$$.fragment),Ke=r(),ie=i("p"),ie.textContent=kt,Ge=r(),ae=i("p"),ae.innerHTML=bt,Je=r(),de=i("p"),de.innerHTML=vt,Xe=r(),R=i("div"),p(G.$$.fragment),Qe=r(),ce=i("p"),ce.textContent=Tt,Ye=r(),le=i("ul"),le.innerHTML=wt,Ze=r(),H=i("div"),p(J.$$.fragment),et=r(),me=i("p"),me.textContent=$t,tt=r(),I=i("div"),p(X.$$.fragment),nt=r(),pe=i("p"),pe.innerHTML=xt,Fe=r(),p(Q.$$.fragment),He=r(),k=i("div"),p(Y.$$.fragment),ot=r(),fe=i("p"),fe.innerHTML=zt,rt=r(),he=i("p"),he.innerHTML=yt,st=r(),ue=i("p"),ue.innerHTML=Bt,it=r(),q=i("div"),p(Z.$$.fragment),at=r(),_e=i("p"),_e.textContent=Rt,dt=r(),ge=i("ul"),ge.innerHTML=qt,Ie=r(),p(ee.$$.fragment),Se=r(),b=i("div"),p(te.$$.fragment),ct=r(),ke=i("p"),ke.textContent=Ct,lt=r(),be=i("p"),be.innerHTML=Lt,mt=r(),ve=i("p"),ve.innerHTML=Mt,pt=r(),Te=i("div"),p(ne.$$.fragment),Ae=r(),p(oe.$$.fragment),je=r(),ze=i("p"),this.h()},l(e){const o=St("svelte-u9bgzb",document.head);m=a(o,"META",{name:!0,content:!0}),o.forEach(n),S=s(e),y=a(e,"P",{}),z(y).forEach(n),T=s(e),B=a(e,"P",{"data-svelte-h":!0}),c(B)!=="svelte-1rh8i1k"&&(B.innerHTML=w),D=s(e),f(A.$$.fragment,e),Be=s(e),E=a(e,"DIV",{class:!0,"data-svelte-h":!0}),c(E)!=="svelte-13t8s2t"&&(E.innerHTML=ft),Re=s(e),f(F.$$.fragment,e),qe=s(e),f(j.$$.fragment,e),Ce=s(e),N=a(e,"P",{"data-svelte-h":!0}),c(N)!=="svelte-1pbomin"&&(N.innerHTML=ht),Le=s(e),W=a(e,"P",{"data-svelte-h":!0}),c(W)!=="svelte-1hctczn"&&(W.innerHTML=ut),Me=s(e),f(O.$$.fragment,e),Pe=s(e),x=a(e,"DIV",{class:!0});var M=z(x);f(U.$$.fragment,M),Ue=s(M),re=a(M,"P",{"data-svelte-h":!0}),c(re)!=="svelte-uvxg7b"&&(re.innerHTML=_t),Ve=s(M),se=a(M,"P",{"data-svelte-h":!0}),c(se)!=="svelte-1ek1ss9"&&(se.innerHTML=gt),M.forEach(n),De=s(e),f(V.$$.fragment,e),Ee=s(e),l=a(e,"DIV",{class:!0});var v=z(l);f(K.$$.fragment,v),Ke=s(v),ie=a(v,"P",{"data-svelte-h":!0}),c(ie)!=="svelte-otns3l"&&(ie.textContent=kt),Ge=s(v),ae=a(v,"P",{"data-svelte-h":!0}),c(ae)!=="svelte-3mjclc"&&(ae.innerHTML=bt),Je=s(v),de=a(v,"P",{"data-svelte-h":!0}),c(de)!=="svelte-meqz30"&&(de.innerHTML=vt),Xe=s(v),R=a(v,"DIV",{class:!0});var we=z(R);f(G.$$.fragment,we),Qe=s(we),ce=a(we,"P",{"data-svelte-h":!0}),c(ce)!=="svelte-t7qurq"&&(ce.textContent=Tt),Ye=s(we),le=a(we,"UL",{"data-svelte-h":!0}),c(le)!=="svelte-xi6653"&&(le.innerHTML=wt),we.forEach(n),Ze=s(v),H=a(v,"DIV",{class:!0});var We=z(H);f(J.$$.fragment,We),et=s(We),me=a(We,"P",{"data-svelte-h":!0}),c(me)!=="svelte-b3k2yi"&&(me.textContent=$t),We.forEach(n),tt=s(v),I=a(v,"DIV",{class:!0});var Oe=z(I);f(X.$$.fragment,Oe),nt=s(Oe),pe=a(Oe,"P",{"data-svelte-h":!0}),c(pe)!=="svelte-1f4f5kp"&&(pe.innerHTML=xt),Oe.forEach(n),v.forEach(n),Fe=s(e),f(Q.$$.fragment,e),He=s(e),k=a(e,"DIV",{class:!0});var C=z(k);f(Y.$$.fragment,C),ot=s(C),fe=a(C,"P",{"data-svelte-h":!0}),c(fe)!=="svelte-1g6s8hh"&&(fe.innerHTML=zt),rt=s(C),he=a(C,"P",{"data-svelte-h":!0}),c(he)!=="svelte-1po7qkw"&&(he.innerHTML=yt),st=s(C),ue=a(C,"P",{"data-svelte-h":!0}),c(ue)!=="svelte-gxzj9w"&&(ue.innerHTML=Bt),it=s(C),q=a(C,"DIV",{class:!0});var $e=z(q);f(Z.$$.fragment,$e),at=s($e),_e=a($e,"P",{"data-svelte-h":!0}),c(_e)!=="svelte-t7qurq"&&(_e.textContent=Rt),dt=s($e),ge=a($e,"UL",{"data-svelte-h":!0}),c(ge)!=="svelte-xi6653"&&(ge.innerHTML=qt),$e.forEach(n),C.forEach(n),Ie=s(e),f(ee.$$.fragment,e),Se=s(e),b=a(e,"DIV",{class:!0});var L=z(b);f(te.$$.fragment,L),ct=s(L),ke=a(L,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-1c8tt6f"&&(ke.textContent=Ct),lt=s(L),be=a(L,"P",{"data-svelte-h":!0}),c(be)!=="svelte-q52n56"&&(be.innerHTML=Lt),mt=s(L),ve=a(L,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-hswkmf"&&(ve.innerHTML=Mt),pt=s(L),Te=a(L,"DIV",{class:!0});var Pt=z(Te);f(ne.$$.fragment,Pt),Pt.forEach(n),L.forEach(n),Ae=s(e),f(oe.$$.fragment,e),je=s(e),ze=a(e,"P",{}),z(ze).forEach(n),this.h()},h(){$(m,"name","hf:doc:metadata"),$(m,"content",Ot),$(E,"class","flex flex-wrap space-x-1"),$(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){t(document.head,m),d(e,S,o),d(e,y,o),d(e,T,o),d(e,B,o),d(e,D,o),h(A,e,o),d(e,Be,o),d(e,E,o),d(e,Re,o),h(F,e,o),d(e,qe,o),h(j,e,o),d(e,Ce,o),d(e,N,o),d(e,Le,o),d(e,W,o),d(e,Me,o),h(O,e,o),d(e,Pe,o),d(e,x,o),h(U,x,null),t(x,Ue),t(x,re),t(x,Ve),t(x,se),d(e,De,o),h(V,e,o),d(e,Ee,o),d(e,l,o),h(K,l,null),t(l,Ke),t(l,ie),t(l,Ge),t(l,ae),t(l,Je),t(l,de),t(l,Xe),t(l,R),h(G,R,null),t(R,Qe),t(R,ce),t(R,Ye),t(R,le),t(l,Ze),t(l,H),h(J,H,null),t(H,et),t(H,me),t(l,tt),t(l,I),h(X,I,null),t(I,nt),t(I,pe),d(e,Fe,o),h(Q,e,o),d(e,He,o),d(e,k,o),h(Y,k,null),t(k,ot),t(k,fe),t(k,rt),t(k,he),t(k,st),t(k,ue),t(k,it),t(k,q),h(Z,q,null),t(q,at),t(q,_e),t(q,dt),t(q,ge),d(e,Ie,o),h(ee,e,o),d(e,Se,o),d(e,b,o),h(te,b,null),t(b,ct),t(b,ke),t(b,lt),t(b,be),t(b,mt),t(b,ve),t(b,pt),t(b,Te),h(ne,Te,null),d(e,Ae,o),h(oe,e,o),d(e,je,o),d(e,ze,o),Ne=!0},p(e,[o]){const M={};o&2&&(M.$$scope={dirty:o,ctx:e}),F.$set(M)},i(e){Ne||(u(A.$$.fragment,e),u(F.$$.fragment,e),u(j.$$.fragment,e),u(O.$$.fragment,e),u(U.$$.fragment,e),u(V.$$.fragment,e),u(K.$$.fragment,e),u(G.$$.fragment,e),u(J.$$.fragment,e),u(X.$$.fragment,e),u(Q.$$.fragment,e),u(Y.$$.fragment,e),u(Z.$$.fragment,e),u(ee.$$.fragment,e),u(te.$$.fragment,e),u(ne.$$.fragment,e),u(oe.$$.fragment,e),Ne=!0)},o(e){_(A.$$.fragment,e),_(F.$$.fragment,e),_(j.$$.fragment,e),_(O.$$.fragment,e),_(U.$$.fragment,e),_(V.$$.fragment,e),_(K.$$.fragment,e),_(G.$$.fragment,e),_(J.$$.fragment,e),_(X.$$.fragment,e),_(Q.$$.fragment,e),_(Y.$$.fragment,e),_(Z.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),Ne=!1},d(e){e&&(n(S),n(y),n(T),n(B),n(D),n(Be),n(E),n(Re),n(qe),n(Ce),n(N),n(Le),n(W),n(Me),n(Pe),n(x),n(De),n(Ee),n(l),n(Fe),n(He),n(k),n(Ie),n(Se),n(b),n(Ae),n(je),n(ze)),n(m),g(A,e),g(F,e),g(j,e),g(O,e),g(U),g(V,e),g(K),g(G),g(J),g(X),g(Q,e),g(Y),g(Z),g(ee,e),g(te),g(ne),g(oe,e)}}}const Ot='{"title":"RetriBERT","local":"retribert","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"RetriBertConfig","local":"transformers.RetriBertConfig","sections":[],"depth":2},{"title":"RetriBertTokenizer","local":"transformers.RetriBertTokenizer","sections":[],"depth":2},{"title":"RetriBertTokenizerFast","local":"transformers.RetriBertTokenizerFast","sections":[],"depth":2},{"title":"RetriBertModel","local":"transformers.RetriBertModel","sections":[],"depth":2}],"depth":1}';function Ut(ye){return Et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qt extends Ht{constructor(m){super(),It(this,m,Ut,Wt,Dt,{})}}export{Qt as component};
