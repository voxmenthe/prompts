import{s as Ys,z as Qs,o as Ss,n as H}from"../chunks/scheduler.18a86fab.js";import{S as As,i as Ds,g as p,s as a,r as g,A as Ks,h as m,f as o,c as r,j as F,x as u,u as f,k as G,y as d,a as i,v as _,d as y,t as M,w as b}from"../chunks/index.98837b22.js";import{T as gs}from"../chunks/Tip.77304350.js";import{D as L}from"../chunks/Docstring.a1ef7999.js";import{C as Fe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ve}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as q,E as Os}from"../chunks/getInferenceSnippets.06c2775f.js";function en(v){let s,T="Example:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdpdFZpc2lvbkNvbmZpZyUyQyUyMEdpdFZpc2lvbk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdpdFZpc2lvbkNvbmZpZyUyMHdpdGglMjBtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBHaXRWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBHaXRWaXNpb25Nb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwbWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEdpdFZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GitVisionConfig, GitVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GitVisionConfig with microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GitVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GitVisionModel (with random weights) from the microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-11lpom8"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function tn(v){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=p("p"),s.innerHTML=T},l(c){s=m(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(c,l){i(c,s,l)},p:H,d(c){c&&o(s)}}}function sn(v){let s,T="Examples:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEdpdFZpc2lvbk1vZGVsJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBHaXRWaXNpb25Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, GitVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitVisionModel.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function nn(v){let s,T="Examples:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdpdENvbmZpZyUyQyUyMEdpdE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdJVCUyMG1pY3Jvc29mdCUyRmdpdC1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEdpdENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwR2l0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GitConfig, GitModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GIT microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GitConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function on(v){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=p("p"),s.innerHTML=T},l(c){s=m(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(c,l){i(c,s,l)},p:H,d(c){c&&o(s)}}}function an(v){let s,T="Examples:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWwlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmdpdC1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBdGV4dCUyMCUzRCUyMCUyMnRoaXMlMjBpcyUyMGFuJTIwaW1hZ2UlMjBvZiUyMHR3byUyMGNhdHMlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjB0ZXh0JTNEdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;this is an image of two cats&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function rn(v){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=p("p"),s.innerHTML=T},l(c){s=m(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(c,l){i(c,s,l)},p:H,d(c){c&&o(s)}}}function ln(v){let s,T="Image captioning example:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UtY29jbyUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZS1jb2NvJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUocGl4ZWxfdmFsdWVzJTNEcGl4ZWxfdmFsdWVzJTJDJTIwbWF4X2xlbmd0aCUzRDUwKSUwQWdlbmVyYXRlZF9jYXB0aW9uJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQXByaW50KGdlbmVyYXRlZF9jYXB0aW9uKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-coco&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-coco&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, max_length=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_caption)
two cats sleeping on a pink blanket <span class="hljs-built_in">next</span> to remotes.`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-jwd2q3"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function dn(v){let s,T="Visual question answering (VQA) example:",c,l,h;return l=new Fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQWZyb20lMjBodWdnaW5nZmFjZV9odWIlMjBpbXBvcnQlMjBoZl9odWJfZG93bmxvYWQlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZS10ZXh0dnFhJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmdpdC1iYXNlLXRleHR2cWElMjIpJTBBJTBBZmlsZV9wYXRoJTIwJTNEJTIwaGZfaHViX2Rvd25sb2FkKHJlcG9faWQlM0QlMjJuaWVsc3IlMkZ0ZXh0dnFhLXNhbXBsZSUyMiUyQyUyMGZpbGVuYW1lJTNEJTIyYnVzLnBuZyUyMiUyQyUyMHJlcG9fdHlwZSUzRCUyMmRhdGFzZXQlMjIpJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKGZpbGVfcGF0aCkuY29udmVydCglMjJSR0IlMjIpJTBBJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVzJTBBJTBBcXVlc3Rpb24lMjAlM0QlMjAlMjJ3aGF0JTIwZG9lcyUyMHRoZSUyMGZyb250JTIwb2YlMjB0aGUlMjBidXMlMjBzYXklMjBhdCUyMHRoZSUyMHRvcCUzRiUyMiUwQSUwQWlucHV0X2lkcyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcXVlc3Rpb24lMkMlMjBhZGRfc3BlY2lhbF90b2tlbnMlM0RGYWxzZSkuaW5wdXRfaWRzJTBBaW5wdXRfaWRzJTIwJTNEJTIwJTVCcHJvY2Vzc29yLnRva2VuaXplci5jbHNfdG9rZW5faWQlNUQlMjAlMkIlMjBpbnB1dF9pZHMlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoaW5wdXRfaWRzKS51bnNxdWVlemUoMCklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUocGl4ZWxfdmFsdWVzJTNEcGl4ZWxfdmFsdWVzJTJDJTIwaW5wdXRfaWRzJTNEaW5wdXRfaWRzJTJDJTIwbWF4X2xlbmd0aCUzRDUwKSUwQXByaW50KHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-textvqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-textvqa&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(repo_id=<span class="hljs-string">&quot;nielsr/textvqa-sample&quot;</span>, filename=<span class="hljs-string">&quot;bus.png&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(file_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;what does the front of the bus say at the top?&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = processor(text=question, add_special_tokens=<span class="hljs-literal">False</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = [processor.tokenizer.cls_token_id] + input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor(input_ids).unsqueeze(<span class="hljs-number">0</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>))
[<span class="hljs-string">&#x27;what does the front of the bus say at the top? special&#x27;</span>]`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-1yp8bv1"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function cn(v){let s,T="Video captioning example:",c,l,h;return l=new Fe({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> av
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-vatex&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-vatex&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set seed for reproducibility</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>np.random.seed(<span class="hljs-number">45</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_video_pyav</span>(<span class="hljs-params">container, indices</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;&#x27;&#x27;
<span class="hljs-meta">... </span>    Decode the video with PyAV decoder.
<span class="hljs-meta">... </span>    Args:
<span class="hljs-meta">... </span>        container (\`av.container.input.InputContainer\`): PyAV container.
<span class="hljs-meta">... </span>        indices (\`list[int]\`): List of frame indices to decode.
<span class="hljs-meta">... </span>    Returns:
<span class="hljs-meta">... </span>        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
<span class="hljs-meta">... </span>    &#x27;&#x27;&#x27;</span>
<span class="hljs-meta">... </span>    frames = []
<span class="hljs-meta">... </span>    container.seek(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>    start_index = indices[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    end_index = indices[-<span class="hljs-number">1</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, frame <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(container.decode(video=<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt; end_index:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt;= start_index <span class="hljs-keyword">and</span> i <span class="hljs-keyword">in</span> indices:
<span class="hljs-meta">... </span>            frames.append(frame)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> np.stack([x.to_ndarray(<span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;rgb24&quot;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> frames])


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_frame_indices</span>(<span class="hljs-params">clip_len, frame_sample_rate, seg_len</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;&#x27;&#x27;
<span class="hljs-meta">... </span>    Sample a given number of frame indices from the video.
<span class="hljs-meta">... </span>    Args:
<span class="hljs-meta">... </span>        clip_len (\`int\`): Total number of frames to sample.
<span class="hljs-meta">... </span>        frame_sample_rate (\`int\`): Sample every n-th frame.
<span class="hljs-meta">... </span>        seg_len (\`int\`): Maximum allowed index of sample&#x27;s last frame.
<span class="hljs-meta">... </span>    Returns:
<span class="hljs-meta">... </span>        indices (\`list[int]\`): List of sampled frame indices
<span class="hljs-meta">... </span>    &#x27;&#x27;&#x27;</span>
<span class="hljs-meta">... </span>    converted_len = <span class="hljs-built_in">int</span>(clip_len * frame_sample_rate)
<span class="hljs-meta">... </span>    end_idx = np.random.randint(converted_len, seg_len)
<span class="hljs-meta">... </span>    start_idx = end_idx - converted_len
<span class="hljs-meta">... </span>    indices = np.linspace(start_idx, end_idx, num=clip_len)
<span class="hljs-meta">... </span>    indices = np.clip(indices, start_idx, end_idx - <span class="hljs-number">1</span>).astype(np.int64)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> indices


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load video</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(
<span class="hljs-meta">... </span>    repo_id=<span class="hljs-string">&quot;nielsr/video-demo&quot;</span>, filename=<span class="hljs-string">&quot;eating_spaghetti.mp4&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>container = av.<span class="hljs-built_in">open</span>(file_path)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample frames</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_frames = model.config.num_image_with_embedding
<span class="hljs-meta">&gt;&gt;&gt; </span>indices = sample_frame_indices(
<span class="hljs-meta">... </span>    clip_len=num_frames, frame_sample_rate=<span class="hljs-number">4</span>, seg_len=container.streams.video[<span class="hljs-number">0</span>].frames
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>frames = read_video_pyav(container, indices)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=<span class="hljs-built_in">list</span>(frames), return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, max_length=<span class="hljs-number">50</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Generated caption:&quot;</span>, processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>))
Generated caption: [<span class="hljs-string">&#x27;a woman is sitting at a table and she is talking about the food she is holding.&#x27;</span>]`,wrap:!1}}),{c(){s=p("p"),s.textContent=T,c=a(),g(l.$$.fragment)},l(t){s=m(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-sx5and"&&(s.textContent=T),c=r(t),f(l.$$.fragment,t)},m(t,w){i(t,s,w),i(t,c,w),_(l,t,w),h=!0},p:H,i(t){h||(y(l.$$.fragment,t),h=!0)},o(t){M(l.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(l,t)}}}function pn(v){let s,T,c,l,h,t="<em>This model was released on 2022-05-27 and added to Hugging Face Transformers on 2023-01-03.</em>",w,re,rt,Y,fs='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',it,ie,lt,le,_s=`The GIT model was proposed in <a href="https://huggingface.co/papers/2205.14100" rel="nofollow">GIT: A Generative Image-to-text Transformer for Vision and Language</a> by
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer
that leverages <a href="clip">CLIP</a>â€™s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on
image captioning and visual question answering benchmarks.`,dt,de,ys="The abstract from the paper is the following:",ct,ce,Ms="<em>In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks.</em>",pt,Q,bs,mt,pe,Ts='GIT architecture. Taken from the <a href="https://huggingface.co/papers/2205.14100" target="_blank">original paper</a>.',ht,me,ws=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/microsoft/GenerativeImage2Text" rel="nofollow">here</a>.`,ut,he,gt,ue,vs="<li>GIT is implemented in a very similar way to GPT-2, the only difference being that the model is also conditioned on <code>pixel_values</code>.</li>",ft,ge,_t,fe,Js="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with GIT.",yt,_e,js='<li>Demo notebooks regarding inference + fine-tuning GIT on custom data can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GIT" rel="nofollow">here</a>.</li> <li>See also: <a href="../tasks/language_modeling">Causal language modeling task guide</a></li>',Mt,ye,Gs=`If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.
The resource should ideally demonstrate something new instead of duplicating an existing resource.`,bt,Me,Tt,k,be,Bt,Ne,Cs=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitVisionModel">GitVisionModel</a>. It is used to instantiate a GIT
vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the vision encoder of the GIT
<a href="https://huggingface.co/microsoft/git-base" rel="nofollow">microsoft/git-base</a> architecture.`,zt,Re,Is=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Vt,S,wt,Te,vt,C,we,Ft,Xe,Us="The vision model from CLIP, used in GIT, without any head or projection on top.",Nt,Pe,ks=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Rt,Ee,$s=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Xt,z,ve,Pt,Le,Zs='The <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitVisionModel">GitVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Et,A,Lt,D,Jt,Je,jt,$,je,qt,qe,xs=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a>. It is used to instantiate a GIT model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the GIT
<a href="https://huggingface.co/microsoft/git-base" rel="nofollow">microsoft/git-base</a> architecture.`,Ht,He,Ws=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Yt,K,Gt,Ge,Ct,Z,Ce,Qt,Ye,Bs="Constructs a GIT processor which wraps a CLIP image processor and a BERT tokenizer into a single processor.",St,Qe,zs=`<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>. See the
<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor.__call__"><strong>call</strong>()</a> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,At,O,Ie,Dt,Se,Vs=`Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the <code>text</code>
and <code>kwargs</code> arguments to BertTokenizerFastâ€™s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a> if <code>text</code> is not <code>None</code> to encode
the text. To prepare the image(s), this method forwards the <code>images</code> and <code>kwrags</code> arguments to
CLIPImageProcessorâ€™s <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__"><strong>call</strong>()</a> if <code>images</code> is not <code>None</code>. Please refer to the docstring
of the above two methods for more information.`,It,Ue,Ut,I,ke,Kt,Ae,Fs="The bare GIT Model transformer consisting of a CLIP image encoder and text decoder outputting raw hidden-states",Ot,De,Ns=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,es,Ke,Rs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ts,V,$e,ss,Oe,Xs='The <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a> forward method, overrides the <code>__call__</code> special method.',ns,ee,os,te,kt,Ze,$t,U,xe,as,et,Ps="GIT Model with a <code>language modeling</code> head on top for autoregressive language modeling.",rs,tt,Es=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,is,st,Ls=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ls,J,We,ds,nt,qs='The <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',cs,se,ps,ot,Hs="Examples:",ms,ne,hs,oe,us,ae,Zt,Be,xt,at,Wt;return re=new q({props:{title:"GIT",local:"git",headingTag:"h1"}}),ie=new q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),he=new q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),ge=new q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Me=new q({props:{title:"GitVisionConfig",local:"transformers.GitVisionConfig",headingTag:"h2"}}),be=new L({props:{name:"class transformers.GitVisionConfig",anchor:"transformers.GitVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GitVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GitVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GitVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GitVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.GitVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.GitVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.GitVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GitVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GitVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/configuration_git.py#L24"}}),S=new Ve({props:{anchor:"transformers.GitVisionConfig.example",$$slots:{default:[en]},$$scope:{ctx:v}}}),Te=new q({props:{title:"GitVisionModel",local:"transformers.GitVisionModel",headingTag:"h2"}}),we=new L({props:{name:"class transformers.GitVisionModel",anchor:"transformers.GitVisionModel",parameters:[{name:"config",val:": GitVisionConfig"}],parametersDescription:[{anchor:"transformers.GitVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitVisionConfig">GitVisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L885"}}),ve=new L({props:{name:"forward",anchor:"transformers.GitVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.GitVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.GitVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitVisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.GitVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L899",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig"
>GitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),A=new gs({props:{$$slots:{default:[tn]},$$scope:{ctx:v}}}),D=new Ve({props:{anchor:"transformers.GitVisionModel.forward.example",$$slots:{default:[sn]},$$scope:{ctx:v}}}),Je=new q({props:{title:"GitConfig",local:"transformers.GitConfig",headingTag:"h2"}}),je=new L({props:{name:"class transformers.GitConfig",anchor:"transformers.GitConfig",parameters:[{name:"vision_config",val:" = None"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 1024"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"bos_token_id",val:" = 101"},{name:"eos_token_id",val:" = 102"},{name:"num_image_with_embedding",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitVisionConfig">GitVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.GitConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the GIT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a>.`,name:"vocab_size"},{anchor:"transformers.GitConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GitConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GitConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GitConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GitConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.GitConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.GitConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.GitConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GitConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GitConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GitConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.GitConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.GitConfig.num_image_with_embedding",description:`<strong>num_image_with_embedding</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of temporal embeddings to add, in case the model is used for video captioning/VQA.`,name:"num_image_with_embedding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/configuration_git.py#L105"}}),K=new Ve({props:{anchor:"transformers.GitConfig.example",$$slots:{default:[nn]},$$scope:{ctx:v}}}),Ge=new q({props:{title:"GitProcessor",local:"transformers.GitProcessor",headingTag:"h2"}}),Ce=new L({props:{name:"class transformers.GitProcessor",anchor:"transformers.GitProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.GitProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.GitProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/processing_git.py#L35"}}),Ie=new L({props:{name:"__call__",anchor:"transformers.GitProcessor.__call__",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"text",val:": typing.Union[str, list[str], list[list[str]], NoneType] = None"},{name:"audio",val:" = None"},{name:"videos",val:" = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.git.processing_git.GitProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.GitProcessor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>list[PIL.Image.Image]</code>, <code>list[np.ndarray]</code>, <code>list[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. Both channels-first and channels-last formats are supported.`,name:"images"},{anchor:"transformers.GitProcessor.__call__.text",description:`<strong>text</strong> (<code>TextInput</code>, <code>PreTokenizedInput</code>, <code>list[TextInput]</code>, <code>list[PreTokenizedInput]</code>, <em>optional</em>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.GitProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/processing_git.py#L57",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>input_ids</strong> â€” List of token ids to be fed to a model. Returned when <code>text</code> is not <code>None</code>.</li>
<li><strong>attention_mask</strong> â€” List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>â€œattention_maskâ€</em> is in <code>self.model_input_names</code> and if <code>text</code> is not
<code>None</code>).</li>
<li><strong>pixel_values</strong> â€” Pixel values to be fed to a model. Returned when <code>images</code> is not <code>None</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ue=new q({props:{title:"GitModel",local:"transformers.GitModel",headingTag:"h2"}}),ke=new L({props:{name:"class transformers.GitModel",anchor:"transformers.GitModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L956"}}),$e=new L({props:{name:"forward",anchor:"transformers.GitModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Union[list[torch.FloatTensor], transformers.cache_utils.Cache, NoneType] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.GitModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GitModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GitModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GitModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.GitModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GitModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GitModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Union[list[torch.FloatTensor], ~cache_utils.Cache, NoneType]</code>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GitModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GitModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.GitModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L1046",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig"
>GitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ee=new gs({props:{$$slots:{default:[on]},$$scope:{ctx:v}}}),te=new Ve({props:{anchor:"transformers.GitModel.forward.example",$$slots:{default:[an]},$$scope:{ctx:v}}}),Ze=new q({props:{title:"GitForCausalLM",local:"transformers.GitForCausalLM",headingTag:"h2"}}),xe=new L({props:{name:"class transformers.GitForCausalLM",anchor:"transformers.GitForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GitForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L1218"}}),We=new L({props:{name:"forward",anchor:"transformers.GitForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Union[transformers.cache_utils.Cache, list[torch.Tensor], NoneType] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GitForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GitForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GitForCausalLM.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.GitForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GitForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GitForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.GitForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Union[~cache_utils.Cache, list[torch.Tensor], NoneType]</code>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GitForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GitForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitForCausalLM.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.GitForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/git/modeling_git.py#L1236",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig"
>GitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new gs({props:{$$slots:{default:[rn]},$$scope:{ctx:v}}}),ne=new Ve({props:{anchor:"transformers.GitForCausalLM.forward.example",$$slots:{default:[ln]},$$scope:{ctx:v}}}),oe=new Ve({props:{anchor:"transformers.GitForCausalLM.forward.example-2",$$slots:{default:[dn]},$$scope:{ctx:v}}}),ae=new Ve({props:{anchor:"transformers.GitForCausalLM.forward.example-3",$$slots:{default:[cn]},$$scope:{ctx:v}}}),Be=new Os({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/git.md"}}),{c(){s=p("meta"),T=a(),c=p("p"),l=a(),h=p("p"),h.innerHTML=t,w=a(),g(re.$$.fragment),rt=a(),Y=p("div"),Y.innerHTML=fs,it=a(),g(ie.$$.fragment),lt=a(),le=p("p"),le.innerHTML=_s,dt=a(),de=p("p"),de.textContent=ys,ct=a(),ce=p("p"),ce.innerHTML=Ms,pt=a(),Q=p("img"),mt=a(),pe=p("small"),pe.innerHTML=Ts,ht=a(),me=p("p"),me.innerHTML=ws,ut=a(),g(he.$$.fragment),gt=a(),ue=p("ul"),ue.innerHTML=vs,ft=a(),g(ge.$$.fragment),_t=a(),fe=p("p"),fe.textContent=Js,yt=a(),_e=p("ul"),_e.innerHTML=js,Mt=a(),ye=p("p"),ye.textContent=Gs,bt=a(),g(Me.$$.fragment),Tt=a(),k=p("div"),g(be.$$.fragment),Bt=a(),Ne=p("p"),Ne.innerHTML=Cs,zt=a(),Re=p("p"),Re.innerHTML=Is,Vt=a(),g(S.$$.fragment),wt=a(),g(Te.$$.fragment),vt=a(),C=p("div"),g(we.$$.fragment),Ft=a(),Xe=p("p"),Xe.textContent=Us,Nt=a(),Pe=p("p"),Pe.innerHTML=ks,Rt=a(),Ee=p("p"),Ee.innerHTML=$s,Xt=a(),z=p("div"),g(ve.$$.fragment),Pt=a(),Le=p("p"),Le.innerHTML=Zs,Et=a(),g(A.$$.fragment),Lt=a(),g(D.$$.fragment),Jt=a(),g(Je.$$.fragment),jt=a(),$=p("div"),g(je.$$.fragment),qt=a(),qe=p("p"),qe.innerHTML=xs,Ht=a(),He=p("p"),He.innerHTML=Ws,Yt=a(),g(K.$$.fragment),Gt=a(),g(Ge.$$.fragment),Ct=a(),Z=p("div"),g(Ce.$$.fragment),Qt=a(),Ye=p("p"),Ye.textContent=Bs,St=a(),Qe=p("p"),Qe.innerHTML=zs,At=a(),O=p("div"),g(Ie.$$.fragment),Dt=a(),Se=p("p"),Se.innerHTML=Vs,It=a(),g(Ue.$$.fragment),Ut=a(),I=p("div"),g(ke.$$.fragment),Kt=a(),Ae=p("p"),Ae.textContent=Fs,Ot=a(),De=p("p"),De.innerHTML=Ns,es=a(),Ke=p("p"),Ke.innerHTML=Rs,ts=a(),V=p("div"),g($e.$$.fragment),ss=a(),Oe=p("p"),Oe.innerHTML=Xs,ns=a(),g(ee.$$.fragment),os=a(),g(te.$$.fragment),kt=a(),g(Ze.$$.fragment),$t=a(),U=p("div"),g(xe.$$.fragment),as=a(),et=p("p"),et.innerHTML=Ps,rs=a(),tt=p("p"),tt.innerHTML=Es,is=a(),st=p("p"),st.innerHTML=Ls,ls=a(),J=p("div"),g(We.$$.fragment),ds=a(),nt=p("p"),nt.innerHTML=qs,cs=a(),g(se.$$.fragment),ps=a(),ot=p("p"),ot.textContent=Hs,ms=a(),g(ne.$$.fragment),hs=a(),g(oe.$$.fragment),us=a(),g(ae.$$.fragment),Zt=a(),g(Be.$$.fragment),xt=a(),at=p("p"),this.h()},l(e){const n=Ks("svelte-u9bgzb",document.head);s=m(n,"META",{name:!0,content:!0}),n.forEach(o),T=r(e),c=m(e,"P",{}),F(c).forEach(o),l=r(e),h=m(e,"P",{"data-svelte-h":!0}),u(h)!=="svelte-2dmr2d"&&(h.innerHTML=t),w=r(e),f(re.$$.fragment,e),rt=r(e),Y=m(e,"DIV",{class:!0,"data-svelte-h":!0}),u(Y)!=="svelte-13t8s2t"&&(Y.innerHTML=fs),it=r(e),f(ie.$$.fragment,e),lt=r(e),le=m(e,"P",{"data-svelte-h":!0}),u(le)!=="svelte-1ah9myq"&&(le.innerHTML=_s),dt=r(e),de=m(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-vfdo9a"&&(de.textContent=ys),ct=r(e),ce=m(e,"P",{"data-svelte-h":!0}),u(ce)!=="svelte-1vsq93p"&&(ce.innerHTML=Ms),pt=r(e),Q=m(e,"IMG",{src:!0,alt:!0,width:!0}),mt=r(e),pe=m(e,"SMALL",{"data-svelte-h":!0}),u(pe)!=="svelte-1ynk3vl"&&(pe.innerHTML=Ts),ht=r(e),me=m(e,"P",{"data-svelte-h":!0}),u(me)!=="svelte-euav7t"&&(me.innerHTML=ws),ut=r(e),f(he.$$.fragment,e),gt=r(e),ue=m(e,"UL",{"data-svelte-h":!0}),u(ue)!=="svelte-d8zlsq"&&(ue.innerHTML=vs),ft=r(e),f(ge.$$.fragment,e),_t=r(e),fe=m(e,"P",{"data-svelte-h":!0}),u(fe)!=="svelte-15k8xp3"&&(fe.textContent=Js),yt=r(e),_e=m(e,"UL",{"data-svelte-h":!0}),u(_e)!=="svelte-tex1qj"&&(_e.innerHTML=js),Mt=r(e),ye=m(e,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-1rggs50"&&(ye.textContent=Gs),bt=r(e),f(Me.$$.fragment,e),Tt=r(e),k=m(e,"DIV",{class:!0});var N=F(k);f(be.$$.fragment,N),Bt=r(N),Ne=m(N,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-1jb00ek"&&(Ne.innerHTML=Cs),zt=r(N),Re=m(N,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-1ek1ss9"&&(Re.innerHTML=Is),Vt=r(N),f(S.$$.fragment,N),N.forEach(o),wt=r(e),f(Te.$$.fragment,e),vt=r(e),C=m(e,"DIV",{class:!0});var x=F(C);f(we.$$.fragment,x),Ft=r(x),Xe=m(x,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-183x34s"&&(Xe.textContent=Us),Nt=r(x),Pe=m(x,"P",{"data-svelte-h":!0}),u(Pe)!=="svelte-q52n56"&&(Pe.innerHTML=ks),Rt=r(x),Ee=m(x,"P",{"data-svelte-h":!0}),u(Ee)!=="svelte-hswkmf"&&(Ee.innerHTML=$s),Xt=r(x),z=m(x,"DIV",{class:!0});var R=F(z);f(ve.$$.fragment,R),Pt=r(R),Le=m(R,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-1v38rx5"&&(Le.innerHTML=Zs),Et=r(R),f(A.$$.fragment,R),Lt=r(R),f(D.$$.fragment,R),R.forEach(o),x.forEach(o),Jt=r(e),f(Je.$$.fragment,e),jt=r(e),$=m(e,"DIV",{class:!0});var X=F($);f(je.$$.fragment,X),qt=r(X),qe=m(X,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-nfn3jf"&&(qe.innerHTML=xs),Ht=r(X),He=m(X,"P",{"data-svelte-h":!0}),u(He)!=="svelte-1ek1ss9"&&(He.innerHTML=Ws),Yt=r(X),f(K.$$.fragment,X),X.forEach(o),Gt=r(e),f(Ge.$$.fragment,e),Ct=r(e),Z=m(e,"DIV",{class:!0});var P=F(Z);f(Ce.$$.fragment,P),Qt=r(P),Ye=m(P,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-ihjj5e"&&(Ye.textContent=Bs),St=r(P),Qe=m(P,"P",{"data-svelte-h":!0}),u(Qe)!=="svelte-dt813o"&&(Qe.innerHTML=zs),At=r(P),O=m(P,"DIV",{class:!0});var ze=F(O);f(Ie.$$.fragment,ze),Dt=r(ze),Se=m(ze,"P",{"data-svelte-h":!0}),u(Se)!=="svelte-ne60ul"&&(Se.innerHTML=Vs),ze.forEach(o),P.forEach(o),It=r(e),f(Ue.$$.fragment,e),Ut=r(e),I=m(e,"DIV",{class:!0});var W=F(I);f(ke.$$.fragment,W),Kt=r(W),Ae=m(W,"P",{"data-svelte-h":!0}),u(Ae)!=="svelte-g1773j"&&(Ae.textContent=Fs),Ot=r(W),De=m(W,"P",{"data-svelte-h":!0}),u(De)!=="svelte-q52n56"&&(De.innerHTML=Ns),es=r(W),Ke=m(W,"P",{"data-svelte-h":!0}),u(Ke)!=="svelte-hswkmf"&&(Ke.innerHTML=Rs),ts=r(W),V=m(W,"DIV",{class:!0});var E=F(V);f($e.$$.fragment,E),ss=r(E),Oe=m(E,"P",{"data-svelte-h":!0}),u(Oe)!=="svelte-un5mdp"&&(Oe.innerHTML=Xs),ns=r(E),f(ee.$$.fragment,E),os=r(E),f(te.$$.fragment,E),E.forEach(o),W.forEach(o),kt=r(e),f(Ze.$$.fragment,e),$t=r(e),U=m(e,"DIV",{class:!0});var B=F(U);f(xe.$$.fragment,B),as=r(B),et=m(B,"P",{"data-svelte-h":!0}),u(et)!=="svelte-n4hc9w"&&(et.innerHTML=Ps),rs=r(B),tt=m(B,"P",{"data-svelte-h":!0}),u(tt)!=="svelte-q52n56"&&(tt.innerHTML=Es),is=r(B),st=m(B,"P",{"data-svelte-h":!0}),u(st)!=="svelte-hswkmf"&&(st.innerHTML=Ls),ls=r(B),J=m(B,"DIV",{class:!0});var j=F(J);f(We.$$.fragment,j),ds=r(j),nt=m(j,"P",{"data-svelte-h":!0}),u(nt)!=="svelte-1413a1t"&&(nt.innerHTML=qs),cs=r(j),f(se.$$.fragment,j),ps=r(j),ot=m(j,"P",{"data-svelte-h":!0}),u(ot)!=="svelte-kvfsh7"&&(ot.textContent=Hs),ms=r(j),f(ne.$$.fragment,j),hs=r(j),f(oe.$$.fragment,j),us=r(j),f(ae.$$.fragment,j),j.forEach(o),B.forEach(o),Zt=r(e),f(Be.$$.fragment,e),xt=r(e),at=m(e,"P",{}),F(at).forEach(o),this.h()},h(){G(s,"name","hf:doc:metadata"),G(s,"content",mn),G(Y,"class","flex flex-wrap space-x-1"),Qs(Q.src,bs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/git_architecture.jpg")||G(Q,"src",bs),G(Q,"alt","drawing"),G(Q,"width","600"),G(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){d(document.head,s),i(e,T,n),i(e,c,n),i(e,l,n),i(e,h,n),i(e,w,n),_(re,e,n),i(e,rt,n),i(e,Y,n),i(e,it,n),_(ie,e,n),i(e,lt,n),i(e,le,n),i(e,dt,n),i(e,de,n),i(e,ct,n),i(e,ce,n),i(e,pt,n),i(e,Q,n),i(e,mt,n),i(e,pe,n),i(e,ht,n),i(e,me,n),i(e,ut,n),_(he,e,n),i(e,gt,n),i(e,ue,n),i(e,ft,n),_(ge,e,n),i(e,_t,n),i(e,fe,n),i(e,yt,n),i(e,_e,n),i(e,Mt,n),i(e,ye,n),i(e,bt,n),_(Me,e,n),i(e,Tt,n),i(e,k,n),_(be,k,null),d(k,Bt),d(k,Ne),d(k,zt),d(k,Re),d(k,Vt),_(S,k,null),i(e,wt,n),_(Te,e,n),i(e,vt,n),i(e,C,n),_(we,C,null),d(C,Ft),d(C,Xe),d(C,Nt),d(C,Pe),d(C,Rt),d(C,Ee),d(C,Xt),d(C,z),_(ve,z,null),d(z,Pt),d(z,Le),d(z,Et),_(A,z,null),d(z,Lt),_(D,z,null),i(e,Jt,n),_(Je,e,n),i(e,jt,n),i(e,$,n),_(je,$,null),d($,qt),d($,qe),d($,Ht),d($,He),d($,Yt),_(K,$,null),i(e,Gt,n),_(Ge,e,n),i(e,Ct,n),i(e,Z,n),_(Ce,Z,null),d(Z,Qt),d(Z,Ye),d(Z,St),d(Z,Qe),d(Z,At),d(Z,O),_(Ie,O,null),d(O,Dt),d(O,Se),i(e,It,n),_(Ue,e,n),i(e,Ut,n),i(e,I,n),_(ke,I,null),d(I,Kt),d(I,Ae),d(I,Ot),d(I,De),d(I,es),d(I,Ke),d(I,ts),d(I,V),_($e,V,null),d(V,ss),d(V,Oe),d(V,ns),_(ee,V,null),d(V,os),_(te,V,null),i(e,kt,n),_(Ze,e,n),i(e,$t,n),i(e,U,n),_(xe,U,null),d(U,as),d(U,et),d(U,rs),d(U,tt),d(U,is),d(U,st),d(U,ls),d(U,J),_(We,J,null),d(J,ds),d(J,nt),d(J,cs),_(se,J,null),d(J,ps),d(J,ot),d(J,ms),_(ne,J,null),d(J,hs),_(oe,J,null),d(J,us),_(ae,J,null),i(e,Zt,n),_(Be,e,n),i(e,xt,n),i(e,at,n),Wt=!0},p(e,[n]){const N={};n&2&&(N.$$scope={dirty:n,ctx:e}),S.$set(N);const x={};n&2&&(x.$$scope={dirty:n,ctx:e}),A.$set(x);const R={};n&2&&(R.$$scope={dirty:n,ctx:e}),D.$set(R);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),K.$set(X);const P={};n&2&&(P.$$scope={dirty:n,ctx:e}),ee.$set(P);const ze={};n&2&&(ze.$$scope={dirty:n,ctx:e}),te.$set(ze);const W={};n&2&&(W.$$scope={dirty:n,ctx:e}),se.$set(W);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),ne.$set(E);const B={};n&2&&(B.$$scope={dirty:n,ctx:e}),oe.$set(B);const j={};n&2&&(j.$$scope={dirty:n,ctx:e}),ae.$set(j)},i(e){Wt||(y(re.$$.fragment,e),y(ie.$$.fragment,e),y(he.$$.fragment,e),y(ge.$$.fragment,e),y(Me.$$.fragment,e),y(be.$$.fragment,e),y(S.$$.fragment,e),y(Te.$$.fragment,e),y(we.$$.fragment,e),y(ve.$$.fragment,e),y(A.$$.fragment,e),y(D.$$.fragment,e),y(Je.$$.fragment,e),y(je.$$.fragment,e),y(K.$$.fragment,e),y(Ge.$$.fragment,e),y(Ce.$$.fragment,e),y(Ie.$$.fragment,e),y(Ue.$$.fragment,e),y(ke.$$.fragment,e),y($e.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(Ze.$$.fragment,e),y(xe.$$.fragment,e),y(We.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(oe.$$.fragment,e),y(ae.$$.fragment,e),y(Be.$$.fragment,e),Wt=!0)},o(e){M(re.$$.fragment,e),M(ie.$$.fragment,e),M(he.$$.fragment,e),M(ge.$$.fragment,e),M(Me.$$.fragment,e),M(be.$$.fragment,e),M(S.$$.fragment,e),M(Te.$$.fragment,e),M(we.$$.fragment,e),M(ve.$$.fragment,e),M(A.$$.fragment,e),M(D.$$.fragment,e),M(Je.$$.fragment,e),M(je.$$.fragment,e),M(K.$$.fragment,e),M(Ge.$$.fragment,e),M(Ce.$$.fragment,e),M(Ie.$$.fragment,e),M(Ue.$$.fragment,e),M(ke.$$.fragment,e),M($e.$$.fragment,e),M(ee.$$.fragment,e),M(te.$$.fragment,e),M(Ze.$$.fragment,e),M(xe.$$.fragment,e),M(We.$$.fragment,e),M(se.$$.fragment,e),M(ne.$$.fragment,e),M(oe.$$.fragment,e),M(ae.$$.fragment,e),M(Be.$$.fragment,e),Wt=!1},d(e){e&&(o(T),o(c),o(l),o(h),o(w),o(rt),o(Y),o(it),o(lt),o(le),o(dt),o(de),o(ct),o(ce),o(pt),o(Q),o(mt),o(pe),o(ht),o(me),o(ut),o(gt),o(ue),o(ft),o(_t),o(fe),o(yt),o(_e),o(Mt),o(ye),o(bt),o(Tt),o(k),o(wt),o(vt),o(C),o(Jt),o(jt),o($),o(Gt),o(Ct),o(Z),o(It),o(Ut),o(I),o(kt),o($t),o(U),o(Zt),o(xt),o(at)),o(s),b(re,e),b(ie,e),b(he,e),b(ge,e),b(Me,e),b(be),b(S),b(Te,e),b(we),b(ve),b(A),b(D),b(Je,e),b(je),b(K),b(Ge,e),b(Ce),b(Ie),b(Ue,e),b(ke),b($e),b(ee),b(te),b(Ze,e),b(xe),b(We),b(se),b(ne),b(oe),b(ae),b(Be,e)}}}const mn='{"title":"GIT","local":"git","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"GitVisionConfig","local":"transformers.GitVisionConfig","sections":[],"depth":2},{"title":"GitVisionModel","local":"transformers.GitVisionModel","sections":[],"depth":2},{"title":"GitConfig","local":"transformers.GitConfig","sections":[],"depth":2},{"title":"GitProcessor","local":"transformers.GitProcessor","sections":[],"depth":2},{"title":"GitModel","local":"transformers.GitModel","sections":[],"depth":2},{"title":"GitForCausalLM","local":"transformers.GitForCausalLM","sections":[],"depth":2}],"depth":1}';function hn(v){return Ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tn extends As{constructor(s){super(),Ds(this,s,hn,pn,Ys,{})}}export{Tn as component};
