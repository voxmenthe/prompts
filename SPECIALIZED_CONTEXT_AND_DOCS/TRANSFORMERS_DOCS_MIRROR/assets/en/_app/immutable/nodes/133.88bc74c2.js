import{s as Qt,z as At,o as Ht,n as Ue}from"../chunks/scheduler.18a86fab.js";import{S as St,i as Ot,g as i,s,r as u,A as Lt,h as c,f as o,c as a,j as ge,x as h,u as f,k as R,y as m,a as n,v as g,d as b,t as _,w as y}from"../chunks/index.98837b22.js";import{T as Gt}from"../chunks/Tip.77304350.js";import{D as Ce}from"../chunks/Docstring.a1ef7999.js";import{C as q}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Tt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as xe,E as Pt}from"../chunks/getInferenceSnippets.06c2775f.js";function Kt(k){let r,M="Examples:",d,p,w;return p=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhYkRldHJDb25maWclMkMlMjBEYWJEZXRyTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwREFCLURFVFIlMjBJREVBLVJlc2VhcmNoJTJGZGFiX2RldHItYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBEYWJEZXRyQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMElERUEtUmVzZWFyY2glMkZkYWJfZGV0ci1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBEYWJEZXRyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DabDetrConfig, DabDetrModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DAB-DETR IDEA-Research/dab_detr-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DabDetrConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the IDEA-Research/dab_detr-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DabDetrModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=i("p"),r.textContent=M,d=s(),u(p.$$.fragment)},l(l){r=c(l,"P",{"data-svelte-h":!0}),h(r)!=="svelte-kvfsh7"&&(r.textContent=M),d=a(l),f(p.$$.fragment,l)},m(l,T){n(l,r,T),n(l,d,T),g(p,l,T),w=!0},p:Ue,i(l){w||(b(p.$$.fragment,l),w=!0)},o(l){_(p.$$.fragment,l),w=!1},d(l){l&&(o(r),o(d)),y(p,l)}}}function eo(k){let r,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=M},l(d){r=c(d,"P",{"data-svelte-h":!0}),h(r)!=="svelte-fincs2"&&(r.innerHTML=M)},m(d,p){n(d,r,p)},p:Ue,d(d){d&&o(r)}}}function to(k){let r,M="Examples:",d,p,w;return p=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIySURFQS1SZXNlYXJjaCUyRmRhYl9kZXRyLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMklERUEtUmVzZWFyY2glMkZkYWJfZGV0ci1iYXNlJTIyKSUwQSUwQSUyMyUyMHByZXBhcmUlMjBpbWFnZSUyMGZvciUyMHRoZSUyMG1vZGVsJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBmb3J3YXJkJTIwcGFzcyUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjB0aGUlMjBsYXN0JTIwaGlkZGVuJTIwc3RhdGVzJTIwYXJlJTIwdGhlJTIwZmluYWwlMjBxdWVyeSUyMGVtYmVkZGluZ3MlMjBvZiUyMHRoZSUyMFRyYW5zZm9ybWVyJTIwZGVjb2RlciUwQSUyMyUyMHRoZXNlJTIwYXJlJTIwb2YlMjBzaGFwZSUyMChiYXRjaF9zaXplJTJDJTIwbnVtX3F1ZXJpZXMlMkMlMjBoaWRkZW5fc2l6ZSklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab_detr-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab_detr-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the last hidden states are the final query embeddings of the Transformer decoder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># these are of shape (batch_size, num_queries, hidden_size)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">300</span>, <span class="hljs-number">256</span>]`,wrap:!1}}),{c(){r=i("p"),r.textContent=M,d=s(),u(p.$$.fragment)},l(l){r=c(l,"P",{"data-svelte-h":!0}),h(r)!=="svelte-kvfsh7"&&(r.textContent=M),d=a(l),f(p.$$.fragment,l)},m(l,T){n(l,r,T),n(l,d,T),g(p,l,T),w=!0},p:Ue,i(l){w||(b(p.$$.fragment,l),w=!0)},o(l){_(p.$$.fragment,l),w=!1},d(l){l&&(o(r),o(d)),y(p,l)}}}function oo(k){let r,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=M},l(d){r=c(d,"P",{"data-svelte-h":!0}),h(r)!=="svelte-fincs2"&&(r.innerHTML=M)},m(d,p){n(d,r,p)},p:Ue,d(d){d&&o(r)}}}function no(k){let r,M="Examples:",d,p,w;return p=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIySURFQS1SZXNlYXJjaCUyRmRhYi1kZXRyLXJlc25ldC01MCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIySURFQS1SZXNlYXJjaCUyRmRhYi1kZXRyLXJlc25ldC01MCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwY29udmVydCUyMG91dHB1dHMlMjAoYm91bmRpbmclMjBib3hlcyUyMGFuZCUyMGNsYXNzJTIwbG9naXRzKSUyMHRvJTIwUGFzY2FsJTIwVk9DJTIwZm9ybWF0JTIwKHhtaW4lMkMlMjB5bWluJTJDJTIweG1heCUyQyUyMHltYXgpJTBBdGFyZ2V0X3NpemVzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QihpbWFnZS5oZWlnaHQlMkMlMjBpbWFnZS53aWR0aCklNUQpJTBBcmVzdWx0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3Nvci5wb3N0X3Byb2Nlc3Nfb2JqZWN0X2RldGVjdGlvbihvdXRwdXRzJTJDJTIwdGhyZXNob2xkJTNEMC41JTJDJTIwdGFyZ2V0X3NpemVzJTNEdGFyZ2V0X3NpemVzKSU1QjAlNUQlMEFmb3IlMjBzY29yZSUyQyUyMGxhYmVsJTJDJTIwYm94JTIwaW4lMjB6aXAocmVzdWx0cyU1QiUyMnNjb3JlcyUyMiU1RCUyQyUyMHJlc3VsdHMlNUIlMjJsYWJlbHMlMjIlNUQlMkMlMjByZXN1bHRzJTVCJTIyYm94ZXMlMjIlNUQpJTNBJTBBJTIwJTIwJTIwJTIwYm94JTIwJTNEJTIwJTVCcm91bmQoaSUyQyUyMDIpJTIwZm9yJTIwaSUyMGluJTIwYm94LnRvbGlzdCgpJTVEJTBBJTIwJTIwJTIwJTIwcHJpbnQoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZiUyMkRldGVjdGVkJTIwJTdCbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCbGFiZWwuaXRlbSgpJTVEJTdEJTIwd2l0aCUyMGNvbmZpZGVuY2UlMjAlMjIlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBmJTIyJTdCcm91bmQoc2NvcmUuaXRlbSgpJTJDJTIwMyklN0QlMjBhdCUyMGxvY2F0aW9uJTIwJTdCYm94JTdEJTIyJTBBJTIwJTIwJTIwJTIwKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForObjectDetection
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab-detr-resnet-50&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab-detr-resnet-50&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">&gt;&gt;&gt; </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.tensor([(image.height, image.width)])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = image_processor.post_process_object_detection(outputs, threshold=<span class="hljs-number">0.5</span>, target_sizes=target_sizes)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> score, label, box <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(results[<span class="hljs-string">&quot;scores&quot;</span>], results[<span class="hljs-string">&quot;labels&quot;</span>], results[<span class="hljs-string">&quot;boxes&quot;</span>]):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(
<span class="hljs-meta">... </span>        <span class="hljs-string">f&quot;Detected <span class="hljs-subst">{model.config.id2label[label.item()]}</span> with confidence &quot;</span>
<span class="hljs-meta">... </span>        <span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>
<span class="hljs-meta">... </span>    )
Detected remote <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.833</span> at location [<span class="hljs-number">38.31</span>, <span class="hljs-number">72.1</span>, <span class="hljs-number">177.63</span>, <span class="hljs-number">118.45</span>]
Detected cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.831</span> at location [<span class="hljs-number">9.2</span>, <span class="hljs-number">51.38</span>, <span class="hljs-number">321.13</span>, <span class="hljs-number">469.0</span>]
Detected cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.804</span> at location [<span class="hljs-number">340.3</span>, <span class="hljs-number">16.85</span>, <span class="hljs-number">642.93</span>, <span class="hljs-number">370.95</span>]
Detected remote <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.683</span> at location [<span class="hljs-number">334.48</span>, <span class="hljs-number">73.49</span>, <span class="hljs-number">366.37</span>, <span class="hljs-number">190.01</span>]
Detected couch <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.535</span> at location [<span class="hljs-number">0.52</span>, <span class="hljs-number">1.19</span>, <span class="hljs-number">640.35</span>, <span class="hljs-number">475.1</span>]`,wrap:!1}}),{c(){r=i("p"),r.textContent=M,d=s(),u(p.$$.fragment)},l(l){r=c(l,"P",{"data-svelte-h":!0}),h(r)!=="svelte-kvfsh7"&&(r.textContent=M),d=a(l),f(p.$$.fragment,l)},m(l,T){n(l,r,T),n(l,d,T),g(p,l,T),w=!0},p:Ue,i(l){w||(b(p.$$.fragment,l),w=!0)},o(l){_(p.$$.fragment,l),w=!1},d(l){l&&(o(r),o(d)),y(p,l)}}}function so(k){let r,M,d,p,w,l="<em>This model was released on 2022-01-28 and added to Hugging Face Transformers on 2025-02-04.</em>",T,V,$e,F,jt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Ie,Y,Ze,G,Dt=`The DAB-DETR model was proposed in <a href="https://huggingface.co/papers/2201.12329" rel="nofollow">DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR</a> by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.
DAB-DETR is an enhanced variant of Conditional DETR. It utilizes dynamically updated anchor boxes to provide both a reference query point (x, y) and a reference anchor size (w, h), improving cross-attention computation. This new approach achieves 45.7% AP when trained for 50 epochs with a single ResNet-50 model as the backbone.`,Re,z,vt,Fe,Q,Jt="The abstract from the paper is the following:",ze,A,xt=`<em>We present in this paper a novel query formulation using dynamic anchor boxes
for DETR (DEtection TRansformer) and offer a deeper understanding of the role
of queries in DETR. This new formulation directly uses box coordinates as queries
in Transformer decoders and dynamically updates them layer-by-layer. Using box
coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR,
but also allows us to modulate the positional attention map using the box width
and height information. Such a design makes it clear that queries in DETR can be
implemented as performing soft ROI pooling layer-by-layer in a cascade manner.
As a result, it leads to the best performance on MS-COCO benchmark among
the DETR-like detection models under the same setting, e.g., AP 45.7% using
ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive
experiments to confirm our analysis and verify the effectiveness of our methods.</em>`,Ee,H,kt=`This model was contributed by <a href="https://huggingface.co/davidhajdu" rel="nofollow">davidhajdu</a>.
The original code can be found <a href="https://github.com/IDEA-Research/DAB-DETR" rel="nofollow">here</a>.`,Ne,S,We,O,Ct="Use the code below to get started with the model.",Be,L,Xe,P,Ut="This should output",qe,K,Ve,ee,$t="There are three other ways to instantiate a DAB-DETR model (depending on what you prefer):",Ye,te,It="Option 1: Instantiate DAB-DETR with pre-trained weights for entire model",Ge,oe,Qe,ne,Zt="Option 2: Instantiate DAB-DETR with randomly initialized weights for Transformer, but pre-trained weights for backbone",Ae,se,He,ae,Rt="Option 3: Instantiate DAB-DETR with randomly initialized weights for backbone + Transformer",Se,re,Oe,le,Le,v,ie,at,be,Ft=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrModel">DabDetrModel</a>. It is used to instantiate
a DAB-DETR model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the DAB-DETR
<a href="https://huggingface.co/IDEA-Research/dab_detr-base" rel="nofollow">IDEA-Research/dab_detr-base</a> architecture.`,rt,_e,zt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,lt,E,Pe,ce,Ke,j,de,it,ye,Et=`The bare DAB-DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw
hidden-states, intermediate hidden states, reference points, output coordinates without any specific head on top.`,ct,we,Nt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,dt,Me,Wt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pt,C,pe,mt,Te,Bt='The <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrModel">DabDetrModel</a> forward method, overrides the <code>__call__</code> special method.',ht,N,ut,W,et,me,tt,D,he,ft,je,Xt=`DAB_DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on
top, for tasks such as COCO detection.`,gt,De,qt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,bt,ve,Vt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,_t,U,ue,yt,Je,Yt='The <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection">DabDetrForObjectDetection</a> forward method, overrides the <code>__call__</code> special method.',wt,B,Mt,X,ot,fe,nt,ke,st;return V=new xe({props:{title:"DAB-DETR",local:"dab-detr",headingTag:"h1"}}),Y=new xe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),S=new xe({props:{title:"How to Get Started with the Model",local:"how-to-get-started-with-the-model",headingTag:"h2"}}),L=new q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMkMlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMEElMEF1cmwlMjAlM0QlMjAnaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyclMjAlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMklERUEtUmVzZWFyY2glMkZkYWItZGV0ci1yZXNuZXQtNTAlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMklERUEtUmVzZWFyY2glMkZkYWItZGV0ci1yZXNuZXQtNTAlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQXJlc3VsdHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IucG9zdF9wcm9jZXNzX29iamVjdF9kZXRlY3Rpb24ob3V0cHV0cyUyQyUyMHRhcmdldF9zaXplcyUzRHRvcmNoLnRlbnNvciglNUJpbWFnZS5zaXplJTVCJTNBJTNBLTElNUQlNUQpJTJDJTIwdGhyZXNob2xkJTNEMC4zKSUwQSUwQWZvciUyMHJlc3VsdCUyMGluJTIwcmVzdWx0cyUzQSUwQSUyMCUyMCUyMCUyMGZvciUyMHNjb3JlJTJDJTIwbGFiZWxfaWQlMkMlMjBib3glMjBpbiUyMHppcChyZXN1bHQlNUIlMjJzY29yZXMlMjIlNUQlMkMlMjByZXN1bHQlNUIlMjJsYWJlbHMlMjIlNUQlMkMlMjByZXN1bHQlNUIlMjJib3hlcyUyMiU1RCklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBzY29yZSUyQyUyMGxhYmVsJTIwJTNEJTIwc2NvcmUuaXRlbSgpJTJDJTIwbGFiZWxfaWQuaXRlbSgpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYm94JTIwJTNEJTIwJTVCcm91bmQoaSUyQyUyMDIpJTIwZm9yJTIwaSUyMGluJTIwYm94LnRvbGlzdCgpJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMiU3Qm1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QmxhYmVsJTVEJTdEJTNBJTIwJTdCc2NvcmUlM0EuMmYlN0QlMjAlN0Jib3glN0QlMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForObjectDetection, AutoImageProcessor

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span> 
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab-detr-resnet-50&quot;</span>)
model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab-detr-resnet-50&quot;</span>)

inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-<span class="hljs-number">1</span>]]), threshold=<span class="hljs-number">0.3</span>)

<span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
    <span class="hljs-keyword">for</span> score, label_id, box <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(result[<span class="hljs-string">&quot;scores&quot;</span>], result[<span class="hljs-string">&quot;labels&quot;</span>], result[<span class="hljs-string">&quot;boxes&quot;</span>]):
        score, label = score.item(), label_id.item()
        box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{model.config.id2label[label]}</span>: <span class="hljs-subst">{score:<span class="hljs-number">.2</span>f}</span> <span class="hljs-subst">{box}</span>&quot;</span>)`,wrap:!1}}),K=new q({props:{code:"Y2F0JTNBJTIwMC44NyUyMCU1QjE0LjclMkMlMjA0OS4zOSUyQyUyMDMyMC41MiUyQyUyMDQ2OS4yOCU1RCUwQXJlbW90ZSUzQSUyMDAuODYlMjAlNUI0MS4wOCUyQyUyMDcyLjM3JTJDJTIwMTczLjM5JTJDJTIwMTE3LjIlNUQlMEFjYXQlM0ElMjAwLjg2JTIwJTVCMzQ0LjQ1JTJDJTIwMTkuNDMlMkMlMjA2MzkuODUlMkMlMjAzNjcuODYlNUQlMEFyZW1vdGUlM0ElMjAwLjYxJTIwJTVCMzM0LjI3JTJDJTIwNzUuOTMlMkMlMjAzNjcuOTIlMkMlMjAxODguODElNUQlMEFjb3VjaCUzQSUyMDAuNTklMjAlNUItMC4wNCUyQyUyMDEuMzQlMkMlMjA2MzkuOSUyQyUyMDQ3Ny4wOSU1RA==",highlighted:`<span class="hljs-attribute">cat</span>: <span class="hljs-number">0</span>.<span class="hljs-number">87</span><span class="hljs-meta"> [14.7, 49.39, 320.52, 469.28]</span>
<span class="hljs-attribute">remote</span>: <span class="hljs-number">0</span>.<span class="hljs-number">86</span><span class="hljs-meta"> [41.08, 72.37, 173.39, 117.2]</span>
<span class="hljs-attribute">cat</span>: <span class="hljs-number">0</span>.<span class="hljs-number">86</span><span class="hljs-meta"> [344.45, 19.43, 639.85, 367.86]</span>
<span class="hljs-attribute">remote</span>: <span class="hljs-number">0</span>.<span class="hljs-number">61</span><span class="hljs-meta"> [334.27, 75.93, 367.92, 188.81]</span>
<span class="hljs-attribute">couch</span>: <span class="hljs-number">0</span>.<span class="hljs-number">59</span><span class="hljs-meta"> [-0.04, 1.34, 639.9, 477.09]</span>`,wrap:!1}}),oe=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhYkRldHJGb3JPYmplY3REZXRlY3Rpb24lMEElMEFtb2RlbCUyMCUzRCUyMERhYkRldHJGb3JPYmplY3REZXRlY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMklERUEtUmVzZWFyY2glMkZkYWItZGV0ci1yZXNuZXQtNTAlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DabDetrForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = DabDetrForObjectDetection.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/dab-detr-resnet-50&quot;</span>)`,wrap:!1}}),se=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhYkRldHJDb25maWclMkMlMjBEYWJEZXRyRm9yT2JqZWN0RGV0ZWN0aW9uJTBBJTBBY29uZmlnJTIwJTNEJTIwRGFiRGV0ckNvbmZpZygpJTBBbW9kZWwlMjAlM0QlMjBEYWJEZXRyRm9yT2JqZWN0RGV0ZWN0aW9uKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DabDetrConfig, DabDetrForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DabDetrConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DabDetrForObjectDetection(config)`,wrap:!1}}),re=new q({props:{code:"Y29uZmlnJTIwJTNEJTIwRGFiRGV0ckNvbmZpZyh1c2VfcHJldHJhaW5lZF9iYWNrYm9uZSUzREZhbHNlKSUwQW1vZGVsJTIwJTNEJTIwRGFiRGV0ckZvck9iamVjdERldGVjdGlvbihjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>config = DabDetrConfig(use_pretrained_backbone=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DabDetrForObjectDetection(config)`,wrap:!1}}),le=new xe({props:{title:"DabDetrConfig",local:"transformers.DabDetrConfig",headingTag:"h2"}}),ie=new Ce({props:{name:"class transformers.DabDetrConfig",anchor:"transformers.DabDetrConfig",parameters:[{name:"use_timm_backbone",val:" = True"},{name:"backbone_config",val:" = None"},{name:"backbone",val:" = 'resnet50'"},{name:"use_pretrained_backbone",val:" = True"},{name:"backbone_kwargs",val:" = None"},{name:"num_queries",val:" = 300"},{name:"encoder_layers",val:" = 6"},{name:"encoder_ffn_dim",val:" = 2048"},{name:"encoder_attention_heads",val:" = 8"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 8"},{name:"is_encoder_decoder",val:" = True"},{name:"activation_function",val:" = 'prelu'"},{name:"hidden_size",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"init_xavier_std",val:" = 1.0"},{name:"auxiliary_loss",val:" = False"},{name:"dilation",val:" = False"},{name:"class_cost",val:" = 2"},{name:"bbox_cost",val:" = 5"},{name:"giou_cost",val:" = 2"},{name:"cls_loss_coefficient",val:" = 2"},{name:"bbox_loss_coefficient",val:" = 5"},{name:"giou_loss_coefficient",val:" = 2"},{name:"focal_alpha",val:" = 0.25"},{name:"temperature_height",val:" = 20"},{name:"temperature_width",val:" = 20"},{name:"query_dim",val:" = 4"},{name:"random_refpoints_xy",val:" = False"},{name:"keep_query_pos",val:" = False"},{name:"num_patterns",val:" = 0"},{name:"normalize_before",val:" = False"},{name:"sine_position_embedding_scale",val:" = None"},{name:"initializer_bias_prior_prob",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DabDetrConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use the <code>timm</code> library for the backbone. If set to <code>False</code>, will use the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a>
API.`,name:"use_timm_backbone"},{anchor:"transformers.DabDetrConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>) &#x2014;
The configuration of the backbone model. Only used in case <code>use_timm_backbone</code> is set to <code>False</code> in which
case it will default to <code>ResNetConfig()</code>.`,name:"backbone_config"},{anchor:"transformers.DabDetrConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;resnet50&quot;</code>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.DabDetrConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.DabDetrConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.DabDetrConfig.num_queries",description:`<strong>num_queries</strong> (<code>int</code>, <em>optional</em>, defaults to 300) &#x2014;
Number of object queries, i.e. detection slots. This is the maximal number of objects
<a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrModel">DabDetrModel</a> can detect in a single image. For COCO, we recommend 100 queries.`,name:"num_queries"},{anchor:"transformers.DabDetrConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.DabDetrConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in encoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.DabDetrConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.DabDetrConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.DabDetrConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.DabDetrConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.DabDetrConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Indicates whether the transformer model architecture is an encoder-decoder or not.`,name:"is_encoder_decoder"},{anchor:"transformers.DabDetrConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;prelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.DabDetrConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
This parameter is a general dimension parameter, defining dimensions for components such as the encoder layer and projection parameters in the decoder layer, among others.`,name:"hidden_size"},{anchor:"transformers.DabDetrConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DabDetrConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DabDetrConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.DabDetrConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.DabDetrConfig.init_xavier_std",description:`<strong>init_xavier_std</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The scaling factor used for the Xavier initialization gain in the HM Attention map module.`,name:"init_xavier_std"},{anchor:"transformers.DabDetrConfig.auxiliary_loss",description:`<strong>auxiliary_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether auxiliary decoding losses (loss at each decoder layer) are to be used.`,name:"auxiliary_loss"},{anchor:"transformers.DabDetrConfig.dilation",description:`<strong>dilation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to replace stride with dilation in the last convolutional block (DC5). Only supported when <code>use_timm_backbone</code> = <code>True</code>.`,name:"dilation"},{anchor:"transformers.DabDetrConfig.class_cost",description:`<strong>class_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 2) &#x2014;
Relative weight of the classification error in the Hungarian matching cost.`,name:"class_cost"},{anchor:"transformers.DabDetrConfig.bbox_cost",description:`<strong>bbox_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 5) &#x2014;
Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.`,name:"bbox_cost"},{anchor:"transformers.DabDetrConfig.giou_cost",description:`<strong>giou_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 2) &#x2014;
Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.`,name:"giou_cost"},{anchor:"transformers.DabDetrConfig.cls_loss_coefficient",description:`<strong>cls_loss_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2) &#x2014;
Relative weight of the classification loss in the object detection loss function.`,name:"cls_loss_coefficient"},{anchor:"transformers.DabDetrConfig.bbox_loss_coefficient",description:`<strong>bbox_loss_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 5) &#x2014;
Relative weight of the L1 bounding box loss in the object detection loss.`,name:"bbox_loss_coefficient"},{anchor:"transformers.DabDetrConfig.giou_loss_coefficient",description:`<strong>giou_loss_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2) &#x2014;
Relative weight of the generalized IoU loss in the object detection loss.`,name:"giou_loss_coefficient"},{anchor:"transformers.DabDetrConfig.focal_alpha",description:`<strong>focal_alpha</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Alpha parameter in the focal loss.`,name:"focal_alpha"},{anchor:"transformers.DabDetrConfig.temperature_height",description:`<strong>temperature_height</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
Temperature parameter to tune the flatness of positional attention (HEIGHT)`,name:"temperature_height"},{anchor:"transformers.DabDetrConfig.temperature_width",description:`<strong>temperature_width</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
Temperature parameter to tune the flatness of positional attention (WIDTH)`,name:"temperature_width"},{anchor:"transformers.DabDetrConfig.query_dim",description:`<strong>query_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Query dimension parameter represents the size of the output vector.`,name:"query_dim"},{anchor:"transformers.DabDetrConfig.random_refpoints_xy",description:`<strong>random_refpoints_xy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to fix the x and y coordinates of the anchor boxes with random initialization.`,name:"random_refpoints_xy"},{anchor:"transformers.DabDetrConfig.keep_query_pos",description:`<strong>keep_query_pos</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to concatenate the projected positional embedding from the object query into the original query (key) in every decoder layer.`,name:"keep_query_pos"},{anchor:"transformers.DabDetrConfig.num_patterns",description:`<strong>num_patterns</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Number of pattern embeddings.`,name:"num_patterns"},{anchor:"transformers.DabDetrConfig.normalize_before",description:`<strong>normalize_before</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether we use a normalization layer in the Encoder or not.`,name:"normalize_before"},{anchor:"transformers.DabDetrConfig.sine_position_embedding_scale",description:`<strong>sine_position_embedding_scale</strong> (<code>float</code>, <em>optional</em>, defaults to &#x2018;None&#x2019;) &#x2014;
Scaling factor applied to the normalized positional encodings.`,name:"sine_position_embedding_scale"},{anchor:"transformers.DabDetrConfig.initializer_bias_prior_prob",description:`<strong>initializer_bias_prior_prob</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The prior probability used by the bias initializer to initialize biases for <code>enc_score_head</code> and <code>class_embed</code>.
If <code>None</code>, <code>prior_prob</code> computed as <code>prior_prob = 1 / (num_labels + 1)</code> while initializing model weights.`,name:"initializer_bias_prior_prob"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dab_detr/configuration_dab_detr.py#L26"}}),E=new Tt({props:{anchor:"transformers.DabDetrConfig.example",$$slots:{default:[Kt]},$$scope:{ctx:k}}}),ce=new xe({props:{title:"DabDetrModel",local:"transformers.DabDetrModel",headingTag:"h2"}}),de=new Ce({props:{name:"class transformers.DabDetrModel",anchor:"transformers.DabDetrModel",parameters:[{name:"config",val:": DabDetrConfig"}],parametersDescription:[{anchor:"transformers.DabDetrModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig">DabDetrConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dab_detr/modeling_dab_detr.py#L1159"}}),pe=new Ce({props:{name:"forward",anchor:"transformers.DabDetrModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"pixel_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DabDetrModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.DabDetrModel.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"pixel_mask"},{anchor:"transformers.DabDetrModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries)</code>, <em>optional</em>) &#x2014;
Not used by default. Can be used to mask object queries.`,name:"decoder_attention_mask"},{anchor:"transformers.DabDetrModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.DabDetrModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you
can choose to directly pass a flattened representation of an image.`,name:"inputs_embeds"},{anchor:"transformers.DabDetrModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an
embedded representation.`,name:"decoder_inputs_embeds"},{anchor:"transformers.DabDetrModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DabDetrModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DabDetrModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dab_detr/modeling_dab_detr.py#L1214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dab_detr.modeling_dab_detr.DabDetrModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig"
>DabDetrConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>~cache_utils.EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>intermediate_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(config.decoder_layers, batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, returned when <code>config.auxiliary_loss=True</code>) — Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a
layernorm.</p>
</li>
<li>
<p><strong>reference_points</strong> (<code>torch.FloatTensor</code> of shape <code>(config.decoder_layers, batch_size, num_queries, 2 (anchor points))</code>) — Reference points (reference points of each layer of the decoder).</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dab_detr.modeling_dab_detr.DabDetrModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),N=new Gt({props:{$$slots:{default:[eo]},$$scope:{ctx:k}}}),W=new Tt({props:{anchor:"transformers.DabDetrModel.forward.example",$$slots:{default:[to]},$$scope:{ctx:k}}}),me=new xe({props:{title:"DabDetrForObjectDetection",local:"transformers.DabDetrForObjectDetection",headingTag:"h2"}}),he=new Ce({props:{name:"class transformers.DabDetrForObjectDetection",anchor:"transformers.DabDetrForObjectDetection",parameters:[{name:"config",val:": DabDetrConfig"}],parametersDescription:[{anchor:"transformers.DabDetrForObjectDetection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig">DabDetrConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dab_detr/modeling_dab_detr.py#L1431"}}),ue=new Ce({props:{name:"forward",anchor:"transformers.DabDetrForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"pixel_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[list[dict]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DabDetrForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.DabDetrForObjectDetection.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"pixel_mask"},{anchor:"transformers.DabDetrForObjectDetection.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries)</code>, <em>optional</em>) &#x2014;
Not used by default. Can be used to mask object queries.`,name:"decoder_attention_mask"},{anchor:"transformers.DabDetrForObjectDetection.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.DabDetrForObjectDetection.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you
can choose to directly pass a flattened representation of an image.`,name:"inputs_embeds"},{anchor:"transformers.DabDetrForObjectDetection.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an
embedded representation.`,name:"decoder_inputs_embeds"},{anchor:"transformers.DabDetrForObjectDetection.forward.labels",description:`<strong>labels</strong> (<code>list[Dict]</code> of len <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the
following 2 keys: &#x2018;class_labels&#x2019; and &#x2018;boxes&#x2019; (the class labels and bounding boxes of an image in the batch
respectively). The class labels themselves should be a <code>torch.LongTensor</code> of len <code>(number of bounding boxes in the image,)</code> and the boxes a <code>torch.FloatTensor</code> of shape <code>(number of bounding boxes in the image, 4)</code>.`,name:"labels"},{anchor:"transformers.DabDetrForObjectDetection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DabDetrForObjectDetection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DabDetrForObjectDetection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dab_detr/modeling_dab_detr.py#L1468",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dab_detr.modeling_dab_detr.DabDetrObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig"
>DabDetrConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) — Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</p>
</li>
<li>
<p><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) — A dictionary containing the individual losses. Useful for logging.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries, num_classes + 1)</code>) — Classification logits (including no-object) for all queries.</p>
</li>
<li>
<p><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_queries, 4)</code>) — Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>~DabDetrImageProcessor.post_process_object_detection</code> to retrieve the
unnormalized bounding boxes.</p>
</li>
<li>
<p><strong>auxiliary_outputs</strong> (<code>list[Dict]</code>, <em>optional</em>) — Optional, only returned when auxiliary losses are activated (i.e. <code>config.auxiliary_loss</code> is set to <code>True</code>)
and labels are provided. It is a list of dictionaries containing the two above keys (<code>logits</code> and
<code>pred_boxes</code>) for each decoder layer.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dab_detr.modeling_dab_detr.DabDetrObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new Gt({props:{$$slots:{default:[oo]},$$scope:{ctx:k}}}),X=new Tt({props:{anchor:"transformers.DabDetrForObjectDetection.forward.example",$$slots:{default:[no]},$$scope:{ctx:k}}}),fe=new Pt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dab-detr.md"}}),{c(){r=i("meta"),M=s(),d=i("p"),p=s(),w=i("p"),w.innerHTML=l,T=s(),u(V.$$.fragment),$e=s(),F=i("div"),F.innerHTML=jt,Ie=s(),u(Y.$$.fragment),Ze=s(),G=i("p"),G.innerHTML=Dt,Re=s(),z=i("img"),Fe=s(),Q=i("p"),Q.textContent=Jt,ze=s(),A=i("p"),A.innerHTML=xt,Ee=s(),H=i("p"),H.innerHTML=kt,Ne=s(),u(S.$$.fragment),We=s(),O=i("p"),O.textContent=Ct,Be=s(),u(L.$$.fragment),Xe=s(),P=i("p"),P.textContent=Ut,qe=s(),u(K.$$.fragment),Ve=s(),ee=i("p"),ee.textContent=$t,Ye=s(),te=i("p"),te.textContent=It,Ge=s(),u(oe.$$.fragment),Qe=s(),ne=i("p"),ne.textContent=Zt,Ae=s(),u(se.$$.fragment),He=s(),ae=i("p"),ae.textContent=Rt,Se=s(),u(re.$$.fragment),Oe=s(),u(le.$$.fragment),Le=s(),v=i("div"),u(ie.$$.fragment),at=s(),be=i("p"),be.innerHTML=Ft,rt=s(),_e=i("p"),_e.innerHTML=zt,lt=s(),u(E.$$.fragment),Pe=s(),u(ce.$$.fragment),Ke=s(),j=i("div"),u(de.$$.fragment),it=s(),ye=i("p"),ye.textContent=Et,ct=s(),we=i("p"),we.innerHTML=Nt,dt=s(),Me=i("p"),Me.innerHTML=Wt,pt=s(),C=i("div"),u(pe.$$.fragment),mt=s(),Te=i("p"),Te.innerHTML=Bt,ht=s(),u(N.$$.fragment),ut=s(),u(W.$$.fragment),et=s(),u(me.$$.fragment),tt=s(),D=i("div"),u(he.$$.fragment),ft=s(),je=i("p"),je.textContent=Xt,gt=s(),De=i("p"),De.innerHTML=qt,bt=s(),ve=i("p"),ve.innerHTML=Vt,_t=s(),U=i("div"),u(ue.$$.fragment),yt=s(),Je=i("p"),Je.innerHTML=Yt,wt=s(),u(B.$$.fragment),Mt=s(),u(X.$$.fragment),ot=s(),u(fe.$$.fragment),nt=s(),ke=i("p"),this.h()},l(e){const t=Lt("svelte-u9bgzb",document.head);r=c(t,"META",{name:!0,content:!0}),t.forEach(o),M=a(e),d=c(e,"P",{}),ge(d).forEach(o),p=a(e),w=c(e,"P",{"data-svelte-h":!0}),h(w)!=="svelte-rmqt2k"&&(w.innerHTML=l),T=a(e),f(V.$$.fragment,e),$e=a(e),F=c(e,"DIV",{class:!0,"data-svelte-h":!0}),h(F)!=="svelte-13t8s2t"&&(F.innerHTML=jt),Ie=a(e),f(Y.$$.fragment,e),Ze=a(e),G=c(e,"P",{"data-svelte-h":!0}),h(G)!=="svelte-1a1lcm7"&&(G.innerHTML=Dt),Re=a(e),z=c(e,"IMG",{src:!0,alt:!0,width:!0}),Fe=a(e),Q=c(e,"P",{"data-svelte-h":!0}),h(Q)!=="svelte-vfdo9a"&&(Q.textContent=Jt),ze=a(e),A=c(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-ct3tl8"&&(A.innerHTML=xt),Ee=a(e),H=c(e,"P",{"data-svelte-h":!0}),h(H)!=="svelte-yd6iz"&&(H.innerHTML=kt),Ne=a(e),f(S.$$.fragment,e),We=a(e),O=c(e,"P",{"data-svelte-h":!0}),h(O)!=="svelte-15ft1fk"&&(O.textContent=Ct),Be=a(e),f(L.$$.fragment,e),Xe=a(e),P=c(e,"P",{"data-svelte-h":!0}),h(P)!=="svelte-192r0ao"&&(P.textContent=Ut),qe=a(e),f(K.$$.fragment,e),Ve=a(e),ee=c(e,"P",{"data-svelte-h":!0}),h(ee)!=="svelte-1bp0kxx"&&(ee.textContent=$t),Ye=a(e),te=c(e,"P",{"data-svelte-h":!0}),h(te)!=="svelte-1m4ojtg"&&(te.textContent=It),Ge=a(e),f(oe.$$.fragment,e),Qe=a(e),ne=c(e,"P",{"data-svelte-h":!0}),h(ne)!=="svelte-fa3upe"&&(ne.textContent=Zt),Ae=a(e),f(se.$$.fragment,e),He=a(e),ae=c(e,"P",{"data-svelte-h":!0}),h(ae)!=="svelte-1m4gpgg"&&(ae.textContent=Rt),Se=a(e),f(re.$$.fragment,e),Oe=a(e),f(le.$$.fragment,e),Le=a(e),v=c(e,"DIV",{class:!0});var $=ge(v);f(ie.$$.fragment,$),at=a($),be=c($,"P",{"data-svelte-h":!0}),h(be)!=="svelte-24tmcs"&&(be.innerHTML=Ft),rt=a($),_e=c($,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-1ek1ss9"&&(_e.innerHTML=zt),lt=a($),f(E.$$.fragment,$),$.forEach(o),Pe=a(e),f(ce.$$.fragment,e),Ke=a(e),j=c(e,"DIV",{class:!0});var J=ge(j);f(de.$$.fragment,J),it=a(J),ye=c(J,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-19gfwdg"&&(ye.textContent=Et),ct=a(J),we=c(J,"P",{"data-svelte-h":!0}),h(we)!=="svelte-q52n56"&&(we.innerHTML=Nt),dt=a(J),Me=c(J,"P",{"data-svelte-h":!0}),h(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Wt),pt=a(J),C=c(J,"DIV",{class:!0});var I=ge(C);f(pe.$$.fragment,I),mt=a(I),Te=c(I,"P",{"data-svelte-h":!0}),h(Te)!=="svelte-dnom5e"&&(Te.innerHTML=Bt),ht=a(I),f(N.$$.fragment,I),ut=a(I),f(W.$$.fragment,I),I.forEach(o),J.forEach(o),et=a(e),f(me.$$.fragment,e),tt=a(e),D=c(e,"DIV",{class:!0});var x=ge(D);f(he.$$.fragment,x),ft=a(x),je=c(x,"P",{"data-svelte-h":!0}),h(je)!=="svelte-13cvp2k"&&(je.textContent=Xt),gt=a(x),De=c(x,"P",{"data-svelte-h":!0}),h(De)!=="svelte-q52n56"&&(De.innerHTML=qt),bt=a(x),ve=c(x,"P",{"data-svelte-h":!0}),h(ve)!=="svelte-hswkmf"&&(ve.innerHTML=Vt),_t=a(x),U=c(x,"DIV",{class:!0});var Z=ge(U);f(ue.$$.fragment,Z),yt=a(Z),Je=c(Z,"P",{"data-svelte-h":!0}),h(Je)!=="svelte-1w07qo"&&(Je.innerHTML=Yt),wt=a(Z),f(B.$$.fragment,Z),Mt=a(Z),f(X.$$.fragment,Z),Z.forEach(o),x.forEach(o),ot=a(e),f(fe.$$.fragment,e),nt=a(e),ke=c(e,"P",{}),ge(ke).forEach(o),this.h()},h(){R(r,"name","hf:doc:metadata"),R(r,"content",ao),R(F,"class","flex flex-wrap space-x-1"),At(z.src,vt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dab_detr_convergence_plot.png")||R(z,"src",vt),R(z,"alt","drawing"),R(z,"width","600"),R(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){m(document.head,r),n(e,M,t),n(e,d,t),n(e,p,t),n(e,w,t),n(e,T,t),g(V,e,t),n(e,$e,t),n(e,F,t),n(e,Ie,t),g(Y,e,t),n(e,Ze,t),n(e,G,t),n(e,Re,t),n(e,z,t),n(e,Fe,t),n(e,Q,t),n(e,ze,t),n(e,A,t),n(e,Ee,t),n(e,H,t),n(e,Ne,t),g(S,e,t),n(e,We,t),n(e,O,t),n(e,Be,t),g(L,e,t),n(e,Xe,t),n(e,P,t),n(e,qe,t),g(K,e,t),n(e,Ve,t),n(e,ee,t),n(e,Ye,t),n(e,te,t),n(e,Ge,t),g(oe,e,t),n(e,Qe,t),n(e,ne,t),n(e,Ae,t),g(se,e,t),n(e,He,t),n(e,ae,t),n(e,Se,t),g(re,e,t),n(e,Oe,t),g(le,e,t),n(e,Le,t),n(e,v,t),g(ie,v,null),m(v,at),m(v,be),m(v,rt),m(v,_e),m(v,lt),g(E,v,null),n(e,Pe,t),g(ce,e,t),n(e,Ke,t),n(e,j,t),g(de,j,null),m(j,it),m(j,ye),m(j,ct),m(j,we),m(j,dt),m(j,Me),m(j,pt),m(j,C),g(pe,C,null),m(C,mt),m(C,Te),m(C,ht),g(N,C,null),m(C,ut),g(W,C,null),n(e,et,t),g(me,e,t),n(e,tt,t),n(e,D,t),g(he,D,null),m(D,ft),m(D,je),m(D,gt),m(D,De),m(D,bt),m(D,ve),m(D,_t),m(D,U),g(ue,U,null),m(U,yt),m(U,Je),m(U,wt),g(B,U,null),m(U,Mt),g(X,U,null),n(e,ot,t),g(fe,e,t),n(e,nt,t),n(e,ke,t),st=!0},p(e,[t]){const $={};t&2&&($.$$scope={dirty:t,ctx:e}),E.$set($);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),N.$set(J);const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),W.$set(I);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),B.$set(x);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),X.$set(Z)},i(e){st||(b(V.$$.fragment,e),b(Y.$$.fragment,e),b(S.$$.fragment,e),b(L.$$.fragment,e),b(K.$$.fragment,e),b(oe.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(le.$$.fragment,e),b(ie.$$.fragment,e),b(E.$$.fragment,e),b(ce.$$.fragment,e),b(de.$$.fragment,e),b(pe.$$.fragment,e),b(N.$$.fragment,e),b(W.$$.fragment,e),b(me.$$.fragment,e),b(he.$$.fragment,e),b(ue.$$.fragment,e),b(B.$$.fragment,e),b(X.$$.fragment,e),b(fe.$$.fragment,e),st=!0)},o(e){_(V.$$.fragment,e),_(Y.$$.fragment,e),_(S.$$.fragment,e),_(L.$$.fragment,e),_(K.$$.fragment,e),_(oe.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(ie.$$.fragment,e),_(E.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(pe.$$.fragment,e),_(N.$$.fragment,e),_(W.$$.fragment,e),_(me.$$.fragment,e),_(he.$$.fragment,e),_(ue.$$.fragment,e),_(B.$$.fragment,e),_(X.$$.fragment,e),_(fe.$$.fragment,e),st=!1},d(e){e&&(o(M),o(d),o(p),o(w),o(T),o($e),o(F),o(Ie),o(Ze),o(G),o(Re),o(z),o(Fe),o(Q),o(ze),o(A),o(Ee),o(H),o(Ne),o(We),o(O),o(Be),o(Xe),o(P),o(qe),o(Ve),o(ee),o(Ye),o(te),o(Ge),o(Qe),o(ne),o(Ae),o(He),o(ae),o(Se),o(Oe),o(Le),o(v),o(Pe),o(Ke),o(j),o(et),o(tt),o(D),o(ot),o(nt),o(ke)),o(r),y(V,e),y(Y,e),y(S,e),y(L,e),y(K,e),y(oe,e),y(se,e),y(re,e),y(le,e),y(ie),y(E),y(ce,e),y(de),y(pe),y(N),y(W),y(me,e),y(he),y(ue),y(B),y(X),y(fe,e)}}}const ao='{"title":"DAB-DETR","local":"dab-detr","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"How to Get Started with the Model","local":"how-to-get-started-with-the-model","sections":[],"depth":2},{"title":"DabDetrConfig","local":"transformers.DabDetrConfig","sections":[],"depth":2},{"title":"DabDetrModel","local":"transformers.DabDetrModel","sections":[],"depth":2},{"title":"DabDetrForObjectDetection","local":"transformers.DabDetrForObjectDetection","sections":[],"depth":2}],"depth":1}';function ro(k){return Ht(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fo extends St{constructor(r){super(),Ot(this,r,ro,so,Qt,{})}}export{fo as component};
