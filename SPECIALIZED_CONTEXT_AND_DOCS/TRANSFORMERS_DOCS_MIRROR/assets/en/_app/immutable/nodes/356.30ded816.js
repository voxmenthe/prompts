import{s as zi,o as Ai,n as te}from"../chunks/scheduler.18a86fab.js";import{S as Ni,i as Ri,g as i,s as t,r as p,A as Xi,h as l,f as a,c as o,j as v,x as m,u as h,k as b,y as s,a as r,v as u,d as g,t as f,w as _}from"../chunks/index.98837b22.js";import{T as yo}from"../chunks/Tip.77304350.js";import{D as U}from"../chunks/Docstring.a1ef7999.js";import{C as $}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as wo}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as k,E as Ei}from"../chunks/getInferenceSnippets.06c2775f.js";function Hi(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBRd2VuMl81T21uaVRoaW5rZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBRd2VuMl81T21uaVRhbGtlckNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMFF3ZW4yXzVPbW5pVG9rZW4yV2F2Q29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwUXdlbjJfNU9tbmlGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMEElMjAlMjAlMjAlMjBRd2VuMl81T21uaUNvbmZpZyUyQyUwQSklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBzdWItbW9kdWxlcyUyMGNvbmZpZ3VyYXRpb25zLiUwQXRoaW5rZXJfY29uZmlnJTIwJTNEJTIwUXdlbjJfNU9tbmlUaGlua2VyQ29uZmlnKCklMEF0YWxrZXJfY29uZmlnJTIwJTNEJTIwUXdlbjJfNU9tbmlUYWxrZXJDb25maWcoKSUwQXRva2VuMndhdl9jb25maWclMjAlM0QlMjBRd2VuMl81T21uaVRva2VuMldhdkNvbmZpZygpJTBBJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZHVsZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBRd2VuMl81T21uaUNvbmZpZy5mcm9tX3N1Yl9tb2RlbF9jb25maWdzKCUwQSUyMCUyMCUyMCUyMHRoaW5rZXJfY29uZmlnJTJDJTIwdGFsa2VyX2NvbmZpZyUyQyUyMHRva2VuMndhdl9jb25maWclMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUwQW1vZGVsJTIwJTNEJTIwUXdlbjJfNU9tbmlGb3JDb25kaXRpb25hbEdlbmVyYXRpb24oY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Qwen2_5OmniThinkerConfig,
<span class="hljs-meta">... </span>    Qwen2_5OmniTalkerConfig,
<span class="hljs-meta">... </span>    Qwen2_5OmniToken2WavConfig,
<span class="hljs-meta">... </span>    Qwen2_5OmniForConditionalGeneration,
<span class="hljs-meta">... </span>    Qwen2_5OmniConfig,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing sub-modules configurations.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>thinker_config = Qwen2_5OmniThinkerConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>talker_config = Qwen2_5OmniTalkerConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>token2wav_config = Qwen2_5OmniToken2WavConfig()


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Qwen2_5OmniConfig.from_sub_model_configs(
<span class="hljs-meta">... </span>    thinker_config, talker_config, token2wav_config
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2_5OmniForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function Yi(j){let d,T=`Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=T},l(M){d=l(M,"P",{"data-svelte-h":!0}),m(d)!=="svelte-rqqap8"&&(d.innerHTML=T)},m(M,w){r(M,d,w)},p:te,d(M){M&&a(d)}}}function Li(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yXzVPbW5pVGhpbmtlckZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUyQyUyMFF3ZW4yXzVPbW5pVGhpbmtlckNvbmZpZyUyQyUyMFF3ZW4yXzVPbW5pQXVkaW9FbmNvZGVyQ29uZmlnJTJDJTIwUXdlbjJfNU9tbmlWaXNpb25FbmNvZGVyQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFF3ZW4yXzVPbW5pQXVkaW9FbmNvZGVyJTIwY29uZmlnJTBBYXVkaW9fY29uZmlnJTIwJTNEJTIwUXdlbjJfNU9tbmlBdWRpb0VuY29kZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBRd2VuMl81T21uaVZpc2lvbkVuY29kZXIlMjBjb25maWclMEF2aXNpb25fY29uZmlnJTIwJTNEJTIwUXdlbjJfNU9tbmlWaXNpb25FbmNvZGVyQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwUXdlbjJfNU9tbmlUZXh0Q29uZmlnJTIwY29uZmlnJTBBdGV4dF9jb25maWclMjAlM0QlMjBRd2VuMl81T21uaVRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBRd2VuMi41T21uaVRoaW5rZXIlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFF3ZW4yXzVPbW5pVGhpbmtlckNvbmZpZyhhdWRpb19jb25maWclMkMlMjB2aXNpb25fY29uZmlnJTJDJTIwdGV4dF9jb25maWcpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMFF3ZW4tT21uaSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwUXdlbjJfNU9tbmlUaGlua2VyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2_5OmniAudioEncoder config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_config = Qwen2_5OmniAudioEncoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2_5OmniVisionEncoder config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = Qwen2_5OmniVisionEncoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2_5OmniTextConfig config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = Qwen2_5OmniTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2.5OmniThinker configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Qwen2_5OmniThinkerConfig(audio_config, vision_config, text_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the Qwen-Omni style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2_5OmniThinkerForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function Si(j){let d,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=T},l(M){d=l(M,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=T)},m(M,w){r(M,d,w)},p:te,d(M){M&&a(d)}}}function Pi(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMGlvJTIwaW1wb3J0JTIwQnl0ZXNJTyUwQWZyb20lMjB1cmxsaWIucmVxdWVzdCUyMGltcG9ydCUyMHVybG9wZW4lMEFpbXBvcnQlMjBsaWJyb3NhJTBBZnJvbSUyMHF3ZW5fdmxfdXRpbHMlMjBpbXBvcnQlMjBwcm9jZXNzX3Zpc2lvbl9pbmZvJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yXzVPbW5pUHJvY2Vzc29yJTJDJTIwUXdlbjJfNU9tbmlUaGlua2VyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBdGhpbmtlciUyMCUzRCUyMFF3ZW4yXzVPbW5pVGhpbmtlckZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyUXdlbiUyRlF3ZW4yLjUtT21uaS03QiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBRd2VuMl81T21uaVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyUXdlbiUyRlF3ZW4yLjUtT21uaS03QiUyMiklMEElMEFjb252ZXJzYXRpb25zJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTdCJ3JvbGUnJTNBJTIwJ3N5c3RlbSclMkMlMjAnY29udGVudCclM0ElMjAnWW91JTIwYXJlJTIwYSUyMGhlbHBmdWwlMjB2b2ljZSUyMGNoYXQlMjBib3QlMkMlMjBhbmQlMjBwbGVhc2UlMjByZXNwb25kJTIwdG8lMjBtZSUyMGluJTIwYSUyMGNhc3VhbCUyMGNvbnZlcnNhdGlvbiUyMG1hbm5lciUyMHVzaW5nJTIwcmFuZG9tJTIwdm9pY2UuJyU3RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU3QiUyMnJvbGUlMjIlM0ElMjAlMjJ1c2VyJTIyJTJDJTIwJTIyY29udGVudCUyMiUzQSUyMCU1QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU3QiUyMnR5cGUlMjIlM0ElMjAlMjJpbWFnZSUyMiUyQyUyMCUyMmltYWdlX3VybCUyMiUzQSUyMCUyMmh0dHBzJTNBJTJGJTJGd3d3LmlsYW5rZWxtYW4ub3JnJTJGc3RvcHNpZ25zJTJGYXVzdHJhbGlhLmpwZyUyMiU3RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU3QiUyMnR5cGUlMjIlM0ElMjAlMjJhdWRpbyUyMiUyQyUyMCUyMmF1ZGlvX3VybCUyMiUzQSUyMCUyMmh0dHBzJTNBJTJGJTJGcWlhbndlbi1yZXMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tJTJGUXdlbjItQXVkaW8lMkZhdWRpbyUyRmdsYXNzLWJyZWFraW5nLTE1MTI1Ni5tcDMlMjIlN0QlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUQlN0QlMkMlMEElNUQlMEElMEF0ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmFwcGx5X2NoYXRfdGVtcGxhdGUoY29udmVyc2F0aW9uJTJDJTIwYWRkX2dlbmVyYXRpb25fcHJvbXB0JTNEVHJ1ZSUyQyUyMHRva2VuaXplJTNERmFsc2UpJTBBYXVkaW9zJTIwJTNEJTIwJTVCJTIwbGlicm9zYS5sb2FkKEJ5dGVzSU8odXJsb3BlbiglMjBjb252ZXJzYXRpb25zJTVCMSU1RCU1Qidjb250ZW50JyU1RCU1QjElNUQlNUInYXVkaW9fdXJsJyU1RCUyMCkucmVhZCgpKSUyQyUyMHNyJTNEc2VsZi5wcm9jZXNzb3IuZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSklMjAlNUQlMEFpbWFnZXMlMkMlMjB2aWRlb3MlMjAlM0QlMjBwcm9jZXNzX3Zpc2lvbl9pbmZvKGNvbnZlcnNhdGlvbnMpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0R0ZXh0JTJDJTIwYXVkaW9zJTNEYXVkaW9zJTJDJTIwaW1hZ2VzJTNEaW1hZ2VzJTJDJTIwdmlkZW9zJTNEdmlkZW9zJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMHBhZGRpbmclM0RUcnVlKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBaW5wdXRzJTVCJ3VzZV9hdWRpb19pbl92aWRlbyclNUQlMjAlM0QlMjAlNjBUcnVlJTYwJTIwb3IlMjAlNjBGYWxzZSU2MCUwQWdlbmVyYXRpb24lMjAlM0QlMjB0aGlua2VyLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMDQ4KSUwQWdlbmVyYXRlX2lkcyUyMCUzRCUyMGdlbmVyYXRpb24lNUIlM0ElMkMlMjBpbnB1dHMuaW5wdXRfaWRzLnNpemUoMSklM0ElNUQlMEElMEFyZXNwb25zZSUyMCUzRCUyMHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUlMkMlMjBjbGVhbl91cF90b2tlbml6YXRpb25fc3BhY2VzJTNERmFsc2UpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> librosa
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> qwen_vl_utils <span class="hljs-keyword">import</span> process_vision_info
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>thinker = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Qwen2_5OmniProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>conversations = [
<span class="hljs-meta">&gt;&gt;&gt; </span>        {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;You are a helpful voice chat bot, and please respond to me in a casual conversation manner using random voice.&#x27;</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>        {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
<span class="hljs-meta">&gt;&gt;&gt; </span>            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image_url&quot;</span>: <span class="hljs-string">&quot;https://www.ilankelman.org/stopsigns/australia.jpg&quot;</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>        ]},
<span class="hljs-meta">&gt;&gt;&gt; </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>text = processor.apply_chat_template(conversation, add_generation_prompt=<span class="hljs-literal">True</span>, tokenize=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audios = [ librosa.load(BytesIO(urlopen( conversations[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;content&#x27;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;audio_url&#x27;</span>] ).read()), sr=self.processor.feature_extractor.sampling_rate) ]
<span class="hljs-meta">&gt;&gt;&gt; </span>images, videos = process_vision_info(conversations)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&#x27;use_audio_in_video&#x27;</span>] = \`<span class="hljs-literal">True</span>\` <span class="hljs-keyword">or</span> \`<span class="hljs-literal">False</span>\`
<span class="hljs-meta">&gt;&gt;&gt; </span>generation = thinker.generate(**inputs, max_new_tokens=<span class="hljs-number">2048</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = generation[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

<span class="hljs-meta">&gt;&gt;&gt; </span>response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function Di(j){let d,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=T},l(M){d=l(M,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=T)},m(M,w){r(M,d,w)},p:te,d(M){M&&a(d)}}}function Ki(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yXzVPbW5pVGFsa2VyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTJDJTIwUXdlbjJfNU9tbmlUaGlua2VyQ29uZmlnJTJDJTIwUXdlbjJfNU9tbmlBdWRpb0VuY29kZXJDb25maWclMkMlMjBRd2VuMl81T21uaVZpc2lvbkVuY29kZXJDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwUXdlbjJfNU9tbmlBdWRpb0VuY29kZXIlMjBjb25maWclMEFhdWRpb19jb25maWclMjAlM0QlMjBRd2VuMl81T21uaUF1ZGlvRW5jb2RlckNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFF3ZW4yJTIwY29uZmlnJTBBdGV4dF9jb25maWclMjAlM0QlMjBRd2VuMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFF3ZW4yXzVPbW5pJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBRd2VuMl81T21uaVRoaW5rZXJDb25maWcoYXVkaW9fY29uZmlnJTJDJTIwdGV4dF9jb25maWcpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMHF3ZW4yLWF1ZGlvJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBRd2VuMl81T21uaVRhbGtlckZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbihjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniTalkerForConditionalGeneration, Qwen2_5OmniThinkerConfig, Qwen2_5OmniAudioEncoderConfig, Qwen2_5OmniVisionEncoderConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2_5OmniAudioEncoder config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_config = Qwen2_5OmniAudioEncoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2 config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = Qwen2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2_5Omni configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Qwen2_5OmniThinkerConfig(audio_config, text_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the qwen2-audio style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2_5OmniTalkerForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function el(j){let d,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=T},l(M){d=l(M,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=T)},m(M,w){r(M,d,w)},p:te,d(M){M&&a(d)}}}function nl(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMGlvJTIwaW1wb3J0JTIwQnl0ZXNJTyUwQWZyb20lMjB1cmxsaWIucmVxdWVzdCUyMGltcG9ydCUyMHVybG9wZW4lMEFpbXBvcnQlMjBsaWJyb3NhJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBRd2VuMl81T21uaVRhbGtlckZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwUXdlbjJfNU9tbmlUYWxrZXJGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMlF3ZW4lMkZRd2VuMi1BdWRpby03QiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJRd2VuJTJGUXdlbjItQXVkaW8tN0IlMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyJTNDJTdDYXVkaW9fYm9zJTdDJTNFJTNDJTdDQVVESU8lN0MlM0UlM0MlN0NhdWRpb19lb3MlN0MlM0VHZW5lcmF0ZSUyMHRoZSUyMGNhcHRpb24lMjBpbiUyMEVuZ2xpc2glM0ElMjIlMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRnFpYW53ZW4tcmVzLm9zcy1jbi1iZWlqaW5nLmFsaXl1bmNzLmNvbSUyRlF3ZW4yLUF1ZGlvJTJGYXVkaW8lMkZnbGFzcy1icmVha2luZy0xNTEyNTYubXAzJTIyJTBBYXVkaW8lMkMlMjBfJTIwJTNEJTIwbGlicm9zYS5sb2FkKEJ5dGVzSU8odXJsb3Blbih1cmwpLnJlYWQoKSklMkMlMjBzciUzRHNlbGYucHJvY2Vzc29yLmZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0Rwcm9tcHQlMkMlMjBhdWRpb3MlM0RhdWRpbyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwR2VuZXJhdGUlMEFnZW5lcmF0ZV9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEFwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> librosa
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Qwen2_5OmniTalkerForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2_5OmniTalkerForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;Generate the caption in English:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio, _ = librosa.load(BytesIO(urlopen(url).read()), sr=self.processor.feature_extractor.sampling_rate)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=prompt, audios=audio, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Generate the caption in English: Glass is breaking.&quot;</span>`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function tl(j){let d,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=T},l(M){d=l(M,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=T)},m(M,w){r(M,d,w)},p:te,d(M){M&&a(d)}}}function ol(j){let d,T="Example:",M,w,y;return w=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yXzVPbW5pVG9rZW4yV2F2TW9kZWwlMkMlMjBEaVRfQXJncyUyQyUyMEJpZ1ZHQU5fQXJncyUwQSUwQSUyMyUyMEluaXRpYWxpemUlMjBEaVQlMjBjb25maWd1cmF0aW9uJTBBZGl0X2NvbmZpZyUyMCUzRCUyMERpVF9BcmdzKCUwQSUyMCUyMCUyMCUyMGRpbSUzRDEwMjQlMkMlMEElMjAlMjAlMjAlMjBkZXB0aCUzRDIyJTJDJTBBJTIwJTIwJTIwJTIwaGVhZHMlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMGZmX211bHQlM0QyJTBBKSUwQSUwQSUyMyUyMEluaXRpYWxpemUlMjBCaWdWR0FOJTIwY29uZmlndXJhdGlvbiUwQWJpZ3ZnYW5fY29uZmlnJTIwJTNEJTIwQmlnVkdBTl9BcmdzKCUwQSUyMCUyMCUyMCUyMG1lbF9kaW0lM0Q4MCUyQyUwQSUyMCUyMCUyMCUyMHVwc2FtcGxlX3JhdGVzJTNEJTVCNSUyQzMlMkMyJTJDMiUyQzIlMkMyJTVEJTBBKSUwQSUwQSUyMyUyMEluaXRpYWxpemUlMjBtYWluJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZyUyMCUzRCUyMFF3ZW4yXzVPbW5pVG9rZW4yV2F2Q29uZmlnKGRpdF9jb25maWclMkMlMjBiaWd2Z2FuX2NvbmZpZyklMEElMEElMjMlMjBJbml0aWFsaXplJTIwbW9kZWwlMjB3aXRoJTIwY29uZmlnJTBBbW9kZWwlMjAlM0QlMjBRd2VuMl81T21uaVRva2VuMldhdihjb25maWcpJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniToken2WavModel, DiT_Args, BigVGAN_Args

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize DiT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dit_config = DiT_Args(
<span class="hljs-meta">... </span>    dim=<span class="hljs-number">1024</span>,
<span class="hljs-meta">... </span>    depth=<span class="hljs-number">22</span>,
<span class="hljs-meta">... </span>    heads=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    ff_mult=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize BigVGAN configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bigvgan_config = BigVGAN_Args(
<span class="hljs-meta">... </span>    mel_dim=<span class="hljs-number">80</span>,
<span class="hljs-meta">... </span>    upsample_rates=[<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize main configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = Qwen2_5OmniToken2WavConfig(dit_config, bigvgan_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize model with config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2_5OmniToken2Wav(config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=T,M=t(),p(w.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=T),M=o(c),h(w.$$.fragment,c)},m(c,J){r(c,d,J),r(c,M,J),u(w,c,J),y=!0},p:te,i(c){y||(g(w.$$.fragment,c),y=!0)},o(c){f(w.$$.fragment,c),y=!1},d(c){c&&(a(d),a(M)),_(w,c)}}}function sl(j){let d,T,M,w,y,c="<em>This model was released on 2025-03-26 and added to Hugging Face Transformers on 2025-04-14.</em>",J,Ge,bo,pe,Tr='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',vo,Ze,Jo,qe,yr='The <a href="https://qwenlm.github.io/blog/qwen2.5-omni/" rel="nofollow">Qwen2.5-Omni</a> model is a unified multiple modalities model proposed in <a href="https://huggingface.co/papers/2503.20215" rel="nofollow">Qwen2.5-Omni Technical Report</a> from Qwen team, Alibaba Group.',Uo,Oe,br="The abstract from the technical report is the following:",ko,Fe,vr="<em>We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.</em>",jo,Ve,Co,ze,Jr='<li>Use <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration">Qwen2_5OmniForConditionalGeneration</a> to generate audio and text output. To generate only one output type, use <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerForConditionalGeneration">Qwen2_5OmniThinkerForConditionalGeneration</a> for text-only and <code>Qwen2_5OmniTalkersForConditionalGeneration</code> for audio-only outputs.</li> <li>Audio generation with <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration">Qwen2_5OmniForConditionalGeneration</a> supports only single batch size at the moment.</li> <li>In case out out-of-memory errors hwen working with video input, decrease <code>processor.max_pixels</code>. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds <code>processor.max_pixels</code>.</li> <li>The processor has its own <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.apply_chat_template">apply_chat_template()</a> method to convert chat messages to model inputs.</li>',Qo,Ae,Io,Ne,Ur='<code>Qwen2.5-Omni</code> can be found on the <a href="https://huggingface.co/Qwen" rel="nofollow">Huggingface Hub</a>.',xo,Re,$o,Xe,kr="The model can accept text, images, audio and videos as input. Here’s an example code for inference.",Bo,Ee,Wo,He,Go,Ye,jr="To generate only text output and save compute by not loading the audio generation model, we can use <code>Qwen2_5OmniThinkerForConditionalGeneration</code> model.",Zo,Le,qo,Se,Oo,Pe,Cr="The model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when using <code>Qwen2_5OmniThinkerForConditionalGeneration</code> model. Here is an example.",Fo,De,Vo,Ke,zo,en,Ao,nn,Qr="The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs.",No,tn,Ro,on,Xo,sn,Ir="If users need audio output, the system prompt must be set as “You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.”, otherwise the audio output may not work as expected.",Eo,an,Ho,rn,Yo,ln,xr="The model supports both text and audio outputs, if users do not need audio outputs, they can set <code>enable_audio_output</code> in the <code>from_pretrained</code> function. This option will save about <code>~2GB</code> of GPU memory but the <code>return_audio</code> option for <code>generate</code> function will only allow to be set at <code>False</code>.",Lo,dn,So,cn,$r="In order to obtain a flexible experience, we recommend that users set <code>enable_audio_output</code> at <code>True</code> when initializing the model through <code>from_pretrained</code> function, and then decide whether to return audio when <code>generate</code> function is called. When <code>return_audio</code> is set to <code>False</code>, the model will only return text outputs to get text responses faster.",Po,mn,Do,pn,Ko,hn,Br="Qwen2.5-Omni supports the ability to change the voice of the output audio. Users can use the <code>spk</code> parameter of <code>generate</code> function to specify the voice type. The <code>&quot;Qwen/Qwen2.5-Omni-7B&quot;</code> checkpoint support two voice types: <code>Chelsie</code> and <code>Ethan</code>, while <code>Chelsie</code> is a female voice and <code>Ethan</code> is a male voice. By default, if <code>spk</code> is not specified, the default voice type is <code>Chelsie</code>.",es,un,ns,gn,ts,fn,os,_n,Wr="First, make sure to install the latest version of Flash Attention 2:",ss,Mn,as,wn,Gr='Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">flash attention repository</a>. FlashAttention-2 can only be used when a model is loaded in <code>torch.float16</code> or <code>torch.bfloat16</code>.',rs,Tn,Zr="To load and run a model using FlashAttention-2, add <code>attn_implementation=&quot;flash_attention_2&quot;</code> when loading the model:",is,yn,ls,bn,ds,I,vn,Xs,pt,qr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration">Qwen2_5OmniForConditionalGeneration</a>. It is used to instantiate a Qwen2.5Omni
model according to the specified sub-models configurations, defining the model architecture.`,Es,ht,Or=`Instantiating a configuration with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" rel="nofollow">Qwen/Qwen2.5-Omni-7B</a> architecture.`,Hs,ut,Fr=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ys,he,Ls,ue,Jn,Ss,gt,Vr=`Returns the config that is meant to be used with text IO. On most models, it is the original config instance
itself. On specific composite models, it is under a set of valid names.`,cs,Un,ms,K,kn,Ps,ft,zr=`Constructs a Qwen2.5Omni processor.
<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor">Qwen2_5OmniProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor">Qwen2VLImageProcessor</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a>, and <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Ds,V,jn,Ks,_t,Ar="Splits token index list into chunks based on token value ranges.",ea,Mt,Nr=`Given a list of token indices, returns a list of (start, end) index tuples representing
slices of the list where the token values fall within successive ranges of <code>t_ntoken_per_chunk</code>.`,na,wt,Rr="For example, if <code>t_ntoken_per_chunk</code> is 1000, the function will create chunks such that:",ta,Tt,Xr="<li>the first chunk contains token values &lt; 1000,</li> <li>the second chunk contains values &gt;= 1000 and &lt; 2000, and so on.</li>",ps,Cn,hs,x,Qn,oa,yt,Er="The full Qwen2.5Omni model, a multimodal model composed of 3 sub-models:",sa,bt,Hr=`<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerForConditionalGeneration">Qwen2_5OmniThinkerForConditionalGeneration</a>:
a causal auto-regressive transformer takes text, audio, image, video as input and predict text tokens.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniTalkerForConditionalGeneration">Qwen2_5OmniTalkerForConditionalGeneration</a>:
a causal auto-regressive transformer takes thinker hidden states and response as input and predict speech tokens.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniToken2WavModel">Qwen2_5OmniToken2WavModel</a>:
a DiT model take speech tokens as input and predict mel spectrogram and a BigVGAN vocoder take mel spectrogram as input and predict waveform.</li>`,aa,vt,Yr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ra,Jt,Lr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ia,S,In,la,Ut,Sr="Define the computation performed at every call.",da,kt,Pr="Should be overridden by all subclasses.",ca,ge,us,xn,gs,ee,$n,ma,z,Bn,pa,jt,Dr="Splits token index list into chunks based on token value ranges.",ha,Ct,Kr=`Given a list of token indices, returns a list of (start, end) index tuples representing
slices of the list where the token values fall within successive ranges of <code>t_ntoken_per_chunk</code>.`,ua,Qt,ei="For example, if <code>t_ntoken_per_chunk</code> is 1000, the function will create chunks such that:",ga,It,ni="<li>the first chunk contains token values &lt; 1000,</li> <li>the second chunk contains values &gt;= 1000 and &lt; 2000, and so on.</li>",fa,A,Wn,_a,xt,ti="Calculate the 3D rope index based on image and video’s temporal, height and width in LLM.",Ma,$t,oi=`Explanation:
Each embedding sequence contains vision embedding and text embedding or just contains text embedding.`,wa,Bt,si=`For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.
Examples:
input_ids: [T T T T T], here T is for text.
temporal position_ids: [0, 1, 2, 3, 4]
height position_ids: [0, 1, 2, 3, 4]
width position_ids: [0, 1, 2, 3, 4]`,Ta,Wt,ai=`For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
and 1D rotary position embedding for text part.
Examples:
Temporal (Time): 3 patches, representing different segments of the video in time.
Height: 2 patches, dividing each frame vertically.
Width: 2 patches, dividing each frame horizontally.
We also have some important parameters:
fps (Frames Per Second): The video’s frame rate, set to 1. This means one frame is processed each second.
tokens_per_second: This is a crucial parameter. It dictates how many “time-steps” or “temporal tokens” are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
temporal_patch_size: The number of frames that compose one temporal patch. Here, it’s 2 frames.
interval: The step size for the temporal position IDs, calculated as tokens_per_second <em>temporal_patch_size / fps. In this case, 25</em> 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
text temporal position_ids: [101, 102, 103, 104, 105]
text height position_ids: [101, 102, 103, 104, 105]
text width position_ids: [101, 102, 103, 104, 105]
Here we calculate the text start position_ids as the max vision position_ids plus 1.`,fs,Gn,_s,B,Zn,ya,Gt,ri=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerForConditionalGeneration">Qwen2_5OmniThinkerForConditionalGeneration</a>. It is used to instantiate an
Qwen2.5-Omni-Thinker model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Qwen2.5-Omni-Thinker.`,ba,Zt,ii='e.g. <a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" rel="nofollow">Qwen/Qwen2.5-Omni-7B</a>',va,qt,li=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ja,fe,Ms,qn,ws,C,On,Ua,Ot,di="The Qwen2.5OmniThinker model which consists of a audio backbone and a language model.",ka,Ft,ci=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ja,Vt,mi=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ca,P,Fn,Qa,zt,pi='The <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerForConditionalGeneration">Qwen2_5OmniThinkerForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Ia,_e,xa,Me,$a,we,Vn,Ba,At,hi="Encodes audios into continuous embeddings that can be forwarded to the language model.",Wa,Te,zn,Ga,Nt,ui="Encodes images into continuous embeddings that can be forwarded to the language model.",Za,ye,An,qa,Rt,gi=`Obtains multimodal placeholder mask from <code>input_ids</code> or <code>inputs_embeds</code>, and checks that the placeholder token count is
equal to the length of multimodal features. If the lengths are different, an error is raised.`,Oa,be,Nn,Fa,Xt,fi="Encodes videos into continuous embeddings that can be forwarded to the language model.",Ts,Rn,ys,W,Xn,Va,Et,_i="The bare Qwen2 5 Omni Text Model outputting raw hidden-states without any specific head on to.",za,Ht,Mi=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Aa,Yt,wi=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Na,oe,En,Ra,Lt,Ti='The <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerTextModel">Qwen2_5OmniThinkerTextModel</a> forward method, overrides the <code>__call__</code> special method.',Xa,ve,bs,Hn,vs,G,Yn,Ea,St,yi=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniTalkerForConditionalGeneration">Qwen2_5OmniTalkerForConditionalGeneration</a>. It is used to instantiate an
Qwen2.5-Omni-Talker model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Qwen2.5-Omni-Thinker.`,Ha,Pt,bi='e.g. <a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B" rel="nofollow">Qwen/Qwen2.5-Omni-7B</a>',Ya,Dt,vi=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,La,Je,Js,Ln,Us,de,Sn,Sa,D,Pn,Pa,Kt,Ji='The <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniTalkerForConditionalGeneration">Qwen2_5OmniTalkerForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Da,Ue,Ka,ke,ks,Dn,js,Z,Kn,er,eo,Ui="The bare Qwen2 5 Omni Model outputting raw hidden-states without any specific head on top.",nr,no,ki=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,tr,to,ji=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,or,se,et,sr,oo,Ci='The <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniTalkerModel">Qwen2_5OmniTalkerModel</a> forward method, overrides the <code>__call__</code> special method.',ar,je,Cs,nt,Qs,N,tt,rr,so,Qi=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniToken2WavModel">Qwen2_5OmniToken2WavModel</a>.
It is used to instantiate the Qwen2.5-Omni-Token2Wav model which combines a Diffusion Transformer (DiT) for mel-spectrogram generation with a BigVGAN model for waveform synthesis. The configuration contains sub-configurations for both components.`,ir,ao,Ii=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,lr,Ce,Is,ot,xs,q,st,dr,ro,xi="The full Qwen2.5Omni Token2Wav model. Consists a DiT model take speech tokens as input and predict mel spectrogram and a BigVGAN vocoder take mel spectrogram as input and predict waveform.",cr,io,$i=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mr,lo,Bi=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pr,Qe,at,hr,co,Wi="Generates a waveform from input code and conditioning parameters.",$s,rt,Bs,R,it,ur,mo,Gi="The full Qwen2.5Omni Token2WavDiT model. Which take speech tokens as input and predict mel spectrogram.",gr,po,Zi=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fr,ho,qi=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ws,lt,Gs,X,dt,_r,uo,Oi="The full Qwen2.5Omni Token2WavBigVGAN model. Which take mel spectrogram as input and predict waveform.",Mr,go,Fi=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,wr,fo,Vi=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Zs,ct,qs,To,Os;return Ge=new k({props:{title:"Qwen2.5-Omni",local:"qwen25-omni",headingTag:"h1"}}),Ze=new k({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ve=new k({props:{title:"Notes",local:"notes",headingTag:"h2"}}),Ae=new k({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),Re=new k({props:{title:"Single Media inference",local:"single-media-inference",headingTag:"h3"}}),Ee=new $({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)
processor = Qwen2_5OmniProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>)

conversations = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <span class="hljs-string">&quot;video&quot;</span>: <span class="hljs-string">&quot;/path/to/video.mp4&quot;</span>},
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What cant you hear and see in this video?&quot;</span>},
        ],
    },
]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=<span class="hljs-literal">True</span>,
    add_generation_prompt=<span class="hljs-literal">True</span>,
    tokenize=<span class="hljs-literal">True</span>,
    return_dict=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
    video_fps=<span class="hljs-number">1</span>,

    <span class="hljs-comment"># kwargs to be passed to \`Qwen2-5-OmniProcessor\`</span>
    padding=<span class="hljs-literal">True</span>,
    use_audio_in_video=<span class="hljs-literal">True</span>,
).to(model.device)

<span class="hljs-comment"># Generation params for audio or text can be different and have to be prefixed with \`thinker_\` or \`talker_\`</span>
text_ids, audio = model.generate(**inputs, use_audio_in_video=<span class="hljs-literal">True</span>, thinker_do_sample=<span class="hljs-literal">False</span>, talker_do_sample=<span class="hljs-literal">True</span>)
text = processor.batch_decode(text_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)

sf.write(
    <span class="hljs-string">&quot;output.wav&quot;</span>,
    audio.reshape(-<span class="hljs-number">1</span>).detach().cpu().numpy(),
    samplerate=<span class="hljs-number">24000</span>,
)
<span class="hljs-built_in">print</span>(text)`,wrap:!1}}),He=new k({props:{title:"Text-only generation",local:"text-only-generation",headingTag:"h3"}}),Le=new $({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
)
processor = Qwen2_5OmniProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>)

conversations = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <span class="hljs-string">&quot;video&quot;</span>: <span class="hljs-string">&quot;/path/to/video.mp4&quot;</span>},
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What cant you hear and see in this video?&quot;</span>},
        ],
    },
]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=<span class="hljs-literal">True</span>,
    add_generation_prompt=<span class="hljs-literal">True</span>,
    tokenize=<span class="hljs-literal">True</span>,
    return_dict=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
    video_fps=<span class="hljs-number">1</span>,

    <span class="hljs-comment"># kwargs to be passed to \`Qwen2-5-OmniProcessor\`</span>
    padding=<span class="hljs-literal">True</span>,
    use_audio_in_video=<span class="hljs-literal">True</span>,
).to(model.device)


text_ids = model.generate(**inputs, use_audio_in_video=<span class="hljs-literal">True</span>)
text = processor.batch_decode(text_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)

sf.write(
    <span class="hljs-string">&quot;output.wav&quot;</span>,
    audio.reshape(-<span class="hljs-number">1</span>).detach().cpu().numpy(),
    samplerate=<span class="hljs-number">24000</span>,
)
<span class="hljs-built_in">print</span>(text)`,wrap:!1}}),Se=new k({props:{title:"Batch Mixed Media Inference",local:"batch-mixed-media-inference",headingTag:"h3"}}),De=new $({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)
processor = Qwen2_5OmniProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>)

<span class="hljs-comment"># Conversation with video only</span>
conversation1 = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/path/to/video.mp4&quot;</span>},
        ]
    }
]

<span class="hljs-comment"># Conversation with audio only</span>
conversation2 = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/path/to/audio.wav&quot;</span>},
        ]
    }
]

<span class="hljs-comment"># Conversation with pure text</span>
conversation3 = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [{<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;who are you?&quot;</span>}],
    }
]


<span class="hljs-comment"># Conversation with mixed media</span>
conversation4 = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>}
        ],
    },
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/path/to/image.jpg&quot;</span>},
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/path/to/video.mp4&quot;</span>},
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;path&quot;</span>: <span class="hljs-string">&quot;/path/to/audio.wav&quot;</span>},
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What are the elements can you see and hear in these medias?&quot;</span>},
        ],
    }
]

conversations = [conversation1, conversation2, conversation3, conversation4]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=<span class="hljs-literal">True</span>,
    add_generation_prompt=<span class="hljs-literal">True</span>,
    tokenize=<span class="hljs-literal">True</span>,
    return_dict=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
    video_fps=<span class="hljs-number">1</span>,

    <span class="hljs-comment"># kwargs to be passed to \`Qwen2-5-OmniProcessor\`</span>
    padding=<span class="hljs-literal">True</span>,
    use_audio_in_video=<span class="hljs-literal">True</span>,
).to(model.thinker.device)

text_ids = model.generate(**inputs, use_audio_in_video=<span class="hljs-literal">True</span>)
text = processor.batch_decode(text_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)

<span class="hljs-built_in">print</span>(text)`,wrap:!1}}),Ke=new k({props:{title:"Usage Tips",local:"usage-tips",headingTag:"h3"}}),en=new k({props:{title:"Image Resolution trade-off",local:"image-resolution-trade-off",headingTag:"h4"}}),tn=new $({props:{code:"bWluX3BpeGVscyUyMCUzRCUyMDEyOCoyOCoyOCUwQW1heF9waXhlbHMlMjAlM0QlMjA3NjgqMjgqMjglMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJRd2VuJTJGUXdlbjIuNS1PbW5pLTdCJTIyJTJDJTIwbWluX3BpeGVscyUzRG1pbl9waXhlbHMlMkMlMjBtYXhfcGl4ZWxzJTNEbWF4X3BpeGVscyk=",highlighted:`min_pixels = <span class="hljs-number">128</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span>
max_pixels = <span class="hljs-number">768</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span>
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>, min_pixels=min_pixels, max_pixels=max_pixels)`,wrap:!1}}),on=new k({props:{title:"Prompt for audio output",local:"prompt-for-audio-output",headingTag:"h4"}}),an=new $({props:{code:"JTdCJTBBJTIwJTIwJTIwJTIwJTIycm9sZSUyMiUzQSUyMCUyMnN5c3RlbSUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMmNvbnRlbnQlMjIlM0ElMjAlMjJZb3UlMjBhcmUlMjBRd2VuJTJDJTIwYSUyMHZpcnR1YWwlMjBodW1hbiUyMGRldmVsb3BlZCUyMGJ5JTIwdGhlJTIwUXdlbiUyMFRlYW0lMkMlMjBBbGliYWJhJTIwR3JvdXAlMkMlMjBjYXBhYmxlJTIwb2YlMjBwZXJjZWl2aW5nJTIwYXVkaXRvcnklMjBhbmQlMjB2aXN1YWwlMjBpbnB1dHMlMkMlMjBhcyUyMHdlbGwlMjBhcyUyMGdlbmVyYXRpbmclMjB0ZXh0JTIwYW5kJTIwc3BlZWNoLiUyMiUyQyUwQSU3RA==",highlighted:`{
    <span class="hljs-comment">&quot;role&quot;</span>: <span class="hljs-comment">&quot;system&quot;</span>,
    <span class="hljs-comment">&quot;content&quot;</span>: <span class="hljs-comment">&quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;</span>,
}`,wrap:!1}}),rn=new k({props:{title:"Use audio output or not",local:"use-audio-output-or-not",headingTag:"h4"}}),dn=new $({props:{code:"bW9kZWwlMjAlM0QlMjBRd2VuMl81T21uaUZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyUXdlbiUyRlF3ZW4yLjUtT21uaS03QiUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZW5hYmxlX2F1ZGlvX291dHB1dCUzREZhbHNlJTJDJTBBKQ==",highlighted:`model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    enable_audio_output=<span class="hljs-literal">False</span>,
)`,wrap:!1}}),mn=new $({props:{code:"",highlighted:`model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    enable_audio_output=<span class="hljs-literal">True</span>,
)
...
text_ids = model.generate(**inputs, return_audio=<span class="hljs-literal">False</span>)`,wrap:!1}}),pn=new k({props:{title:"Change voice type of output audio",local:"change-voice-type-of-output-audio",headingTag:"h4"}}),un=new $({props:{code:"dGV4dF9pZHMlMkMlMjBhdWRpbyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc3BrJTNEJTIyQ2hlbHNpZSUyMik=",highlighted:'text_ids, audio = model.generate(**inputs, spk=<span class="hljs-string">&quot;Chelsie&quot;</span>)',wrap:!1}}),gn=new $({props:{code:"dGV4dF9pZHMlMkMlMjBhdWRpbyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc3BrJTNEJTIyRXRoYW4lMjIp",highlighted:'text_ids, audio = model.generate(**inputs, spk=<span class="hljs-string">&quot;Ethan&quot;</span>)',wrap:!1}}),fn=new k({props:{title:"Flash-Attention 2 to speed up generation",local:"flash-attention-2-to-speed-up-generation",headingTag:"h4"}}),Mn=new $({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),yn=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yXzVPbW5pRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBRd2VuMl81T21uaUZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyUXdlbiUyRlF3ZW4yLjUtT21uaS03QiUyMiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJmbGFzaF9hdHRlbnRpb25fMiUyMiUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2_5OmniForConditionalGeneration

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    <span class="hljs-string">&quot;Qwen/Qwen2.5-Omni-7B&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    dtype=torch.bfloat16,
    attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
)`,wrap:!1}}),bn=new k({props:{title:"Qwen2_5OmniConfig",local:"transformers.Qwen2_5OmniConfig",headingTag:"h2"}}),vn=new U({props:{name:"class transformers.Qwen2_5OmniConfig",anchor:"transformers.Qwen2_5OmniConfig",parameters:[{name:"thinker_config",val:" = None"},{name:"talker_config",val:" = None"},{name:"token2wav_config",val:" = None"},{name:"enable_audio_output",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniConfig.thinker_config",description:"<strong>thinker_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014; Configuration of the underlying thinker sub-model.",name:"thinker_config"},{anchor:"transformers.Qwen2_5OmniConfig.talker_config",description:"<strong>talker_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014; Configuration of the underlying talker sub-model.",name:"talker_config"},{anchor:"transformers.Qwen2_5OmniConfig.token2wav_config",description:"<strong>token2wav_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014; Configuration of the underlying codec sub-model.",name:"token2wav_config"},{anchor:"transformers.Qwen2_5OmniConfig.enable_audio_output",description:"<strong>enable_audio_output</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014; Whether enable audio output and load talker and token2wav module.",name:"enable_audio_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py#L995"}}),he=new wo({props:{anchor:"transformers.Qwen2_5OmniConfig.example",$$slots:{default:[Hi]},$$scope:{ctx:j}}}),Jn=new U({props:{name:"get_text_config",anchor:"transformers.Qwen2_5OmniConfig.get_text_config",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniConfig.get_text_config.decoder",description:`<strong>decoder</strong> (<code>Optional[bool]</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If set to <code>True</code>, then only search for decoder config names.`,name:"decoder"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py#L1076"}}),Un=new k({props:{title:"Qwen2_5OmniProcessor",local:"transformers.Qwen2_5OmniProcessor",headingTag:"h2"}}),kn=new U({props:{name:"class transformers.Qwen2_5OmniProcessor",anchor:"transformers.Qwen2_5OmniProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"video_processor",val:" = None"},{name:"feature_extractor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"chat_template",val:" = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor">Qwen2VLImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor.`,name:"image_processor"},{anchor:"transformers.Qwen2_5OmniProcessor.video_processor",description:`<strong>video_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a>, <em>optional</em>) &#x2014;
The video processor.`,name:"video_processor"},{anchor:"transformers.Qwen2_5OmniProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a>, <em>optional</em>) &#x2014;
The audio feature extractor.`,name:"feature_extractor"},{anchor:"transformers.Qwen2_5OmniProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a>, <em>optional</em>) &#x2014;
The text tokenizer.`,name:"tokenizer"},{anchor:"transformers.Qwen2_5OmniProcessor.chat_template",description:`<strong>chat_template</strong> (<code>Optional[str]</code>, <em>optional</em>) &#x2014;
The Jinja template to use for formatting the conversation. If not provided, the default chat template is used.`,name:"chat_template"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py#L76"}}),jn=new U({props:{name:"get_chunked_index",anchor:"transformers.Qwen2_5OmniProcessor.get_chunked_index",parameters:[{name:"token_indices",val:": ndarray"},{name:"tokens_per_chunk",val:": int"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniProcessor.get_chunked_index.token_indices",description:"<strong>token_indices</strong> (<code>np.ndarray</code>) &#x2014; A monotonically increasing list of token index values.",name:"token_indices"},{anchor:"transformers.Qwen2_5OmniProcessor.get_chunked_index.t_ntoken_per_chunk",description:"<strong>t_ntoken_per_chunk</strong> (<code>int</code>) &#x2014; Number of tokens per chunk (used as the chunk size threshold).",name:"t_ntoken_per_chunk"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py#L288",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of tuples, each representing the start (inclusive)
and end (exclusive) indices of a chunk in <code>token_indices</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[tuple[int, int]]</code></p>
`}}),Cn=new k({props:{title:"Qwen2_5OmniForConditionalGeneration",local:"transformers.Qwen2_5OmniForConditionalGeneration",headingTag:"h2"}}),Qn=new U({props:{name:"class transformers.Qwen2_5OmniForConditionalGeneration",anchor:"transformers.Qwen2_5OmniForConditionalGeneration",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniForConditionalGeneration">Qwen2_5OmniForConditionalGeneration</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L3696"}}),In=new U({props:{name:"_forward_unimplemented",anchor:"transformers.Qwen2_5OmniForConditionalGeneration.forward",parameters:[{name:"*input",val:": typing.Any"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/torch/nn/modules/module.py#L388"}}),ge=new yo({props:{$$slots:{default:[Yi]},$$scope:{ctx:j}}}),xn=new k({props:{title:"Qwen2_5OmniPreTrainedModelForConditionalGeneration",local:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration",headingTag:"h2"}}),$n=new U({props:{name:"class transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration",anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration",parameters:[{name:"config",val:": PretrainedConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L76"}}),Bn=new U({props:{name:"get_chunked_index",anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_chunked_index",parameters:[{name:"token_indices",val:": Tensor"},{name:"tokens_per_chunk",val:": int"},{name:"remove_index",val:": int"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_chunked_index.token_indices",description:`<strong>token_indices</strong> (<code>torch.Tensor</code> of shape <code>(seq_len, )</code>) &#x2014; A monotonically increasing list of
token index values.`,name:"token_indices"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_chunked_index.t_ntoken_per_chunk",description:"<strong>t_ntoken_per_chunk</strong> (<code>int</code>) &#x2014; Number of tokens per chunk (used as the chunk size threshold).",name:"t_ntoken_per_chunk"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_chunked_index.remove_index",description:"<strong>remove_index</strong> (<code>int</code>) An index id to subtract from <code>token_indices</code> before chunking &#x2014;",name:"remove_index"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L152",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of tuples, each representing the start (inclusive)
and end (exclusive) indices of a chunk in <code>token_indices</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[tuple[int, int]]</code></p>
`}}),Wn=new U({props:{name:"get_rope_index",anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"video_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"use_audio_in_video",val:": bool = False"},{name:"audio_seqlens",val:": typing.Optional[torch.LongTensor] = None"},{name:"second_per_grids",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.`,name:"input_ids"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.image_grid_thw",description:`<strong>image_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_images, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each image in LLM.`,name:"image_grid_thw"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.video_grid_thw",description:`<strong>video_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each video in LLM.`,name:"video_grid_thw"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.use_audio_in_video",description:`<strong>use_audio_in_video</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, use the audio in video.`,name:"use_audio_in_video"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.audio_seqlens",description:`<strong>audio_seqlens</strong> (<code>torch.LongTensor</code> of shape <code>(num_audios)</code>, <em>optional</em>) &#x2014;
The length of feature shape of each audio in LLM.`,name:"audio_seqlens"},{anchor:"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration.get_rope_index.second_per_grids",description:`<strong>second_per_grids</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos)</code>, <em>optional</em>) &#x2014;
The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.`,name:"second_per_grids"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L189",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>position_ids (<code>torch.LongTensor</code> of shape <code>(3, batch_size, sequence_length)</code>)
mrope_position_deltas (<code>torch.Tensor</code> of shape <code>(batch_size)</code>)</p>
`}}),Gn=new k({props:{title:"Qwen2_5OmniThinkerConfig",local:"transformers.Qwen2_5OmniThinkerConfig",headingTag:"h2"}}),Zn=new U({props:{name:"class transformers.Qwen2_5OmniThinkerConfig",anchor:"transformers.Qwen2_5OmniThinkerConfig",parameters:[{name:"audio_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"text_config",val:" = None"},{name:"audio_token_index",val:" = 151646"},{name:"image_token_index",val:" = 151655"},{name:"video_token_index",val:" = 151656"},{name:"position_id_per_seconds",val:" = 25"},{name:"seconds_per_chunk",val:" = 2"},{name:"audio_start_token_id",val:" = 151647"},{name:"audio_end_token_id",val:" = 151648"},{name:"user_token_id",val:" = 872"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerConfig.audio_config",description:`<strong>audio_config</strong> (<code>dict</code>,  <em>optional</em>) &#x2014;
The config dictionary of the audio backbone.`,name:"audio_config"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
The config dictionary of the vision backbone.`,name:"vision_config"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
The config dictionary of the text backbone.`,name:"text_config"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.audio_token_index",description:`<strong>audio_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151646) &#x2014;
The audio token index to encode the audio prompt.`,name:"audio_token_index"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.image_token_index",description:`<strong>image_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151655) &#x2014;
The image token index to encode the image prompt.`,name:"image_token_index"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.video_token_index",description:`<strong>video_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151656) &#x2014;
The video token index to encode the video prompt.`,name:"video_token_index"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.position_id_per_seconds",description:`<strong>position_id_per_seconds</strong> (<code>int</code>, <em>optional</em>, defaults to 25) &#x2014;
The increment of position id per second.`,name:"position_id_per_seconds"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.seconds_per_chunk",description:`<strong>seconds_per_chunk</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The duration in seconds of the chunk of audio and video data.`,name:"seconds_per_chunk"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.audio_start_token_id",description:`<strong>audio_start_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151647) &#x2014;
The audio start token index to encode the audio prompt.`,name:"audio_start_token_id"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.audio_end_token_id",description:`<strong>audio_end_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151648) &#x2014;
The audio end token index to encode the audio prompt.`,name:"audio_end_token_id"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.user_token_id",description:"<strong>user_token_id</strong> (`int, <em>optional</em>, defaults to 872) &#x2014;\nThe user token index to encode the user token.",name:"user_token_id"},{anchor:"transformers.Qwen2_5OmniThinkerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py#L413"}}),fe=new wo({props:{anchor:"transformers.Qwen2_5OmniThinkerConfig.example",$$slots:{default:[Li]},$$scope:{ctx:j}}}),qn=new k({props:{title:"Qwen2_5OmniThinkerForConditionalGeneration",local:"transformers.Qwen2_5OmniThinkerForConditionalGeneration",headingTag:"h2"}}),On=new U({props:{name:"class transformers.Qwen2_5OmniThinkerForConditionalGeneration",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration",parameters:[{name:"config",val:": Qwen2_5OmniThinkerConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniThinkerConfig">Qwen2_5OmniThinkerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1663"}}),Fn=new U({props:{name:"forward",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_features",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pixel_values_videos",val:": typing.Optional[torch.FloatTensor] = None"},{name:"image_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"video_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"feature_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"audio_feature_lengths",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"rope_deltas",val:": typing.Optional[torch.LongTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"use_audio_in_video",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"video_second_per_grid",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.input_features",description:`<strong>input_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, feature_dim)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input audio features. Audio features can be obtained using
<code>feature_extractor_class</code>. See <code>feature_extractor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor">Qwen2_5OmniProcessor</a> uses
<code>feature_extractor_class</code> for processing audios).`,name:"input_features"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor">Qwen2_5OmniProcessor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.pixel_values_videos",description:`<strong>pixel_values_videos</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_frames, num_channels, frame_size, frame_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input video. Pixel values for videos can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a>. See <code>Qwen2VLVideoProcessor.__call__()</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor">Qwen2_5OmniProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a> for processing videos).`,name:"pixel_values_videos"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.image_grid_thw",description:`<strong>image_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_images, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each image in LLM.`,name:"image_grid_thw"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.video_grid_thw",description:`<strong>video_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each video in LLM.`,name:"video_grid_thw"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.feature_attention_mask",description:`<strong>feature_attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, feature_sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding feature indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"feature_attention_mask"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.audio_feature_lengths",description:`<strong>audio_feature_lengths</strong> (<code>torch.LongTensor</code> of shape <code>(num_audios)</code>, <em>optional</em>) &#x2014;
The length of feature shape of each audio in LLM.`,name:"audio_feature_lengths"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.rope_deltas",description:`<strong>rope_deltas</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>) &#x2014;
The rope index difference between sequence length and multimodal rope.`,name:"rope_deltas"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.use_audio_in_video",description:`<strong>use_audio_in_video</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not use audio track in video, should same as the parameter in <code>process_audio_info</code>.`,name:"use_audio_in_video"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.video_second_per_grid",description:`<strong>video_second_per_grid</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos)</code>, <em>optional</em>) &#x2014;
Number of seconds per grid for each video, used for temporal feature mapping.`,name:"video_second_per_grid"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1804",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.qwen2_5_omni.modeling_qwen2_5_omni.Qwen2_5OmniThinkerCausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig"
>Qwen2_5OmniConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>, <em>optional</em>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>rope_deltas</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>) — The rope index difference between sequence length and multimodal rope.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.qwen2_5_omni.modeling_qwen2_5_omni.Qwen2_5OmniThinkerCausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_e=new yo({props:{$$slots:{default:[Si]},$$scope:{ctx:j}}}),Me=new wo({props:{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.forward.example",$$slots:{default:[Pi]},$$scope:{ctx:j}}}),Vn=new U({props:{name:"get_audio_features",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_audio_features",parameters:[{name:"input_features",val:": FloatTensor"},{name:"feature_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"audio_feature_lengths",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_audio_features.input_features",description:`<strong>input_features</strong> (<code>torch.FloatTensor</code>) &#x2014;
The tensors corresponding to the input audios.`,name:"input_features"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_audio_features.feature_attention_mask",description:`<strong>feature_attention_mask</strong> (<code>torch.LongTensor</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding feature indices. Mask values selected in <code>[0, 1]</code>:`,name:"feature_attention_mask"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_audio_features.audio_feature_lengths",description:`<strong>audio_feature_lengths</strong> (<code>torch.LongTensor</code> of shape <code>(num_audios)</code>, <em>optional</em>) &#x2014;
The length of feature shape of each audio in LLM.`,name:"audio_feature_lengths"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1717"}}),zn=new U({props:{name:"get_image_features",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_image_features",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"image_grid_thw",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images.`,name:"pixel_values"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_image_features.image_grid_thw",description:`<strong>image_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_images, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each image in LLM.`,name:"image_grid_thw"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1703"}}),An=new U({props:{name:"get_placeholder_mask",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_placeholder_mask",parameters:[{name:"input_ids",val:": LongTensor"},{name:"inputs_embeds",val:": FloatTensor"},{name:"image_features",val:": FloatTensor = None"},{name:"video_features",val:": FloatTensor = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1756"}}),Nn=new U({props:{name:"get_video_features",anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_video_features",parameters:[{name:"pixel_values_videos",val:": FloatTensor"},{name:"video_grid_thw",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_video_features.pixel_values_videos",description:`<strong>pixel_values_videos</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input videos.`,name:"pixel_values_videos"},{anchor:"transformers.Qwen2_5OmniThinkerForConditionalGeneration.get_video_features.video_grid_thw",description:`<strong>video_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each video in LLM.`,name:"video_grid_thw"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1687"}}),Rn=new k({props:{title:"Qwen2_5OmniThinkerTextModel",local:"transformers.Qwen2_5OmniThinkerTextModel",headingTag:"h2"}}),Xn=new U({props:{name:"class transformers.Qwen2_5OmniThinkerTextModel",anchor:"transformers.Qwen2_5OmniThinkerTextModel",parameters:[{name:"config",val:": Qwen2_5OmniTextConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerTextModel.config",description:`<strong>config</strong> (<code>Qwen2_5OmniTextConfig</code>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1501"}}),En=new U({props:{name:"forward",anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs]"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Qwen2_5OmniThinkerTextModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L1523",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig"
>Qwen2_5OmniConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new yo({props:{$$slots:{default:[Di]},$$scope:{ctx:j}}}),Hn=new k({props:{title:"Qwen2_5OmniTalkerConfig",local:"transformers.Qwen2_5OmniTalkerConfig",headingTag:"h2"}}),Yn=new U({props:{name:"class transformers.Qwen2_5OmniTalkerConfig",anchor:"transformers.Qwen2_5OmniTalkerConfig",parameters:[{name:"audio_token_index",val:" = 151646"},{name:"image_token_index",val:" = 151655"},{name:"video_token_index",val:" = 151656"},{name:"vocab_size",val:" = 8448"},{name:"tts_text_start_token_id",val:" = 151860"},{name:"tts_text_end_token_id",val:" = 151861"},{name:"tts_text_pad_token_id",val:" = 151859"},{name:"tts_codec_start_token_id",val:" = 8293"},{name:"tts_codec_end_token_id",val:" = 8294"},{name:"tts_codec_pad_token_id",val:" = 8292"},{name:"tts_codec_mask_token_id",val:" = 8296"},{name:"vision_start_token_id",val:" = 151652"},{name:"vision_end_token_id",val:" = 151653"},{name:"embedding_size",val:" = 3584"},{name:"hidden_size",val:" = 3584"},{name:"intermediate_size",val:" = 18944"},{name:"num_hidden_layers",val:" = 28"},{name:"num_attention_heads",val:" = 28"},{name:"num_key_value_heads",val:" = 4"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 32768"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"head_dim",val:" = 128"},{name:"use_cache",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 1000000.0"},{name:"use_sliding_window",val:" = False"},{name:"sliding_window",val:" = 32768"},{name:"max_window_layers",val:" = 28"},{name:"attention_dropout",val:" = 0.0"},{name:"rope_scaling",val:" = None"},{name:"position_id_per_seconds",val:" = 25"},{name:"seconds_per_chunk",val:" = 2"},{name:"audio_start_token_id",val:" = 151647"},{name:"audio_end_token_id",val:" = 151648"},{name:"initializer_range",val:" = 0.02"},{name:"spatial_merge_size",val:" = 2"},{name:"layer_types",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniTalkerConfig.audio_token_index",description:`<strong>audio_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151646) &#x2014;
The audio token index to encode the audio prompt.`,name:"audio_token_index"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.image_token_index",description:`<strong>image_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151655) &#x2014;
The image token index to encode the image prompt.`,name:"image_token_index"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.video_token_index",description:`<strong>video_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151656) &#x2014;
The video token index to encode the video prompt.`,name:"video_token_index"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8448) &#x2014;
Vocabulary size of the QwenOmni model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLModel">Qwen2VLModel</a>`,name:"vocab_size"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_text_start_token_id",description:`<strong>tts_text_start_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151860) &#x2014;
The tts text start token index to encode the start of tts text.`,name:"tts_text_start_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_text_end_token_id",description:`<strong>tts_text_end_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151861) &#x2014;
The tts text end token index to encode the end of tts text.`,name:"tts_text_end_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_text_pad_token_id",description:`<strong>tts_text_pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151859) &#x2014;
The tts text pad token index to encode the pad of tts text.`,name:"tts_text_pad_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_codec_start_token_id",description:`<strong>tts_codec_start_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 8293) &#x2014;
The tts codec start token index to encode the start of tts codec.`,name:"tts_codec_start_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_codec_end_token_id",description:`<strong>tts_codec_end_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 8294) &#x2014;
The tts codec end token index to encode the end of tts codec.`,name:"tts_codec_end_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_codec_pad_token_id",description:`<strong>tts_codec_pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 8292) &#x2014;
The tts codec pad token index to encode the pad of tts codec.`,name:"tts_codec_pad_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tts_codec_mask_token_id",description:`<strong>tts_codec_mask_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 8296) &#x2014;
The tts codec mask token index to encode the mask of tts codec.`,name:"tts_codec_mask_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.vision_start_token_id",description:`<strong>vision_start_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151652) &#x2014;
The tts vision start token index to encode the start of vision.`,name:"vision_start_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.vision_end_token_id",description:`<strong>vision_end_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151653) &#x2014;
The tts vision end token index to encode the end of vision.`,name:"vision_end_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.embedding_size",description:`<strong>embedding_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3584) &#x2014;
Dimension of the embedding representations.`,name:"embedding_size"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3584) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 18944) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 28) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 28) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA) otherwise GQA is used. When
converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
by meanpooling all the original heads within that group. For more details, check out <a href="https://huggingface.co/papers/2305.13245" rel="nofollow">this
paper</a>. If it is not specified, will default to <code>32</code>.`,name:"num_key_value_heads"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 32768) &#x2014;
The maximum sequence length that this model might ever be used with.`,name:"max_position_embeddings"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.head_dim",description:`<strong>head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The dimension of each attention head.`,name:"head_dim"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model&#x2019;s input and output word embeddings should be tied.`,name:"tie_word_embeddings"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 1000000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.use_sliding_window",description:`<strong>use_sliding_window</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sliding window attention.`,name:"use_sliding_window"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.sliding_window",description:`<strong>sliding_window</strong> (<code>int</code>, <em>optional</em>, defaults to 32768) &#x2014;
Sliding window attention (SWA) window size. If not specified, will default to <code>4096</code>.`,name:"sliding_window"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.max_window_layers",description:`<strong>max_window_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 28) &#x2014;
The number of layers using full attention. The first <code>max_window_layers</code> layers will use full attention, while any
additional layer afterwards will use SWA (Sliding Window Attention).`,name:"max_window_layers"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
and you expect the model to work on longer <code>max_position_embeddings</code>, we recommend you to update this value
accordingly.
Expected contents:
<code>rope_type</code> (<code>str</code>):
The sub-variant of RoPE to use. Can be one of [&#x2018;default&#x2019;, &#x2018;linear&#x2019;, &#x2018;dynamic&#x2019;, &#x2018;yarn&#x2019;, &#x2018;longrope&#x2019;,
&#x2018;llama3&#x2019;], with &#x2018;default&#x2019; being the original RoPE implementation.
<code>factor</code> (<code>float</code>, <em>optional</em>):
Used with all rope types except &#x2018;default&#x2019;. The scaling factor to apply to the RoPE embeddings. In
most scaling types, a <code>factor</code> of x will enable the model to handle sequences of length x <em>
original maximum pre-trained length.
<code>original_max_position_embeddings</code> (<code>int</code>, </em>optional<em>):
Used with &#x2018;dynamic&#x2019;, &#x2018;longrope&#x2019; and &#x2018;llama3&#x2019;. The original max position embeddings used during
pretraining.
<code>attention_factor</code> (<code>float</code>, </em>optional<em>):
Used with &#x2018;yarn&#x2019; and &#x2018;longrope&#x2019;. The scaling factor to be applied on the attention
computation. If unspecified, it defaults to value recommended by the implementation, using the
<code>factor</code> field to infer the suggested value.
<code>beta_fast</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;yarn&#x2019;. Parameter to set the boundary for extrapolation (only) in the linear
ramp function. If unspecified, it defaults to 32.
<code>beta_slow</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;yarn&#x2019;. Parameter to set the boundary for interpolation (only) in the linear
ramp function. If unspecified, it defaults to 1.
<code>short_factor</code> (<code>list[float]</code>, </em>optional<em>):
Only used with &#x2018;longrope&#x2019;. The scaling factor to be applied to short contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>long_factor</code> (<code>list[float]</code>, </em>optional<em>):
Only used with &#x2018;longrope&#x2019;. The scaling factor to be applied to long contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>low_freq_factor</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;llama3&#x2019;. Scaling factor applied to low frequency components of the RoPE
<code>high_freq_factor</code> (<code>float</code>, </em>optional*):
Only used with &#x2018;llama3&#x2019;. Scaling factor applied to high frequency components of the RoPE`,name:"rope_scaling"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.position_id_per_seconds",description:`<strong>position_id_per_seconds</strong> (<code>int</code>, <em>optional</em>, defaults to 25) &#x2014;
The increment of position id per second.`,name:"position_id_per_seconds"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.seconds_per_chunk",description:`<strong>seconds_per_chunk</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The duration in seconds of the chunk of audio and video data.`,name:"seconds_per_chunk"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.audio_start_token_id",description:`<strong>audio_start_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151647) &#x2014;
The audio start token index to encode the audio prompt.`,name:"audio_start_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.audio_end_token_id",description:`<strong>audio_end_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 151648) &#x2014;
The audio end token index to encode the audio prompt.`,name:"audio_end_token_id"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.spatial_merge_size",description:`<strong>spatial_merge_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The size used for merging spatial dimensions.`,name:"spatial_merge_size"},{anchor:"transformers.Qwen2_5OmniTalkerConfig.layer_types",description:`<strong>layer_types</strong> (<code>list</code>, <em>optional</em>) &#x2014;
Attention pattern for each layer.`,name:"layer_types"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py#L533"}}),Je=new wo({props:{anchor:"transformers.Qwen2_5OmniTalkerConfig.example",$$slots:{default:[Ki]},$$scope:{ctx:j}}}),Ln=new k({props:{title:"Qwen2_5OmniTalkerForConditionalGeneration",local:"transformers.Qwen2_5OmniTalkerForConditionalGeneration",headingTag:"h2"}}),Sn=new U({props:{name:"class transformers.Qwen2_5OmniTalkerForConditionalGeneration",anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration",parameters:[{name:"config",val:": Qwen2_5OmniTalkerConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L2233"}}),Pn=new U({props:{name:"forward",anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"thinker_reply_part",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"rope_deltas",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_text_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"video_grid_thw",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_audio_in_video",val:": typing.Optional[bool] = None"},{name:"audio_feature_lengths",val:": typing.Optional[torch.LongTensor] = None"},{name:"video_second_per_grid",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.thinker_reply_part",description:`<strong>thinker_reply_part</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Hidden states from the thinker model&#x2019;s output that represent the text reply part to be processed.`,name:"thinker_reply_part"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.rope_deltas",description:`<strong>rope_deltas</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>) &#x2014;
The rope index difference between sequence length and multimodal rope.`,name:"rope_deltas"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.input_text_ids",description:`<strong>input_text_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Input token IDs for text-only content, used for position calculation in multimodal contexts.`,name:"input_text_ids"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.image_grid_thw",description:`<strong>image_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_images, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each image in LLM.`,name:"image_grid_thw"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.video_grid_thw",description:`<strong>video_grid_thw</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos, 3)</code>, <em>optional</em>) &#x2014;
The temporal, height and width of feature shape of each video in LLM.`,name:"video_grid_thw"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.use_audio_in_video",description:`<strong>use_audio_in_video</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not use audio track in video, should same as the parameter in <code>process_audio_info</code>.`,name:"use_audio_in_video"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.audio_feature_lengths",description:`<strong>audio_feature_lengths</strong> (<code>torch.LongTensor</code> of shape <code>(num_audios)</code>, <em>optional</em>) &#x2014;
The length of feature shape of each audio in LLM.`,name:"audio_feature_lengths"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.video_second_per_grid",description:`<strong>video_second_per_grid</strong> (<code>torch.LongTensor</code> of shape <code>(num_videos)</code>, <em>optional</em>) &#x2014;
Number of seconds per grid for each video, used for temporal feature mapping.`,name:"video_second_per_grid"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L2266",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.qwen2_5_omni.modeling_qwen2_5_omni.Qwen2_5OmniTalkerCausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig"
>Qwen2_5OmniConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>rope_deltas</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>) — The rope index difference between sequence length and multimodal rope.</p>
</li>
<li>
<p><strong>thinker_reply_part</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Hidden states from the thinker model that are used as input for the talker model. These represent the encoded
response that the talker model will use to generate speech tokens.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.qwen2_5_omni.modeling_qwen2_5_omni.Qwen2_5OmniTalkerCausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new yo({props:{$$slots:{default:[el]},$$scope:{ctx:j}}}),ke=new wo({props:{anchor:"transformers.Qwen2_5OmniTalkerForConditionalGeneration.forward.example",$$slots:{default:[nl]},$$scope:{ctx:j}}}),Dn=new k({props:{title:"Qwen2_5OmniTalkerModel",local:"transformers.Qwen2_5OmniTalkerModel",headingTag:"h2"}}),Kn=new U({props:{name:"class transformers.Qwen2_5OmniTalkerModel",anchor:"transformers.Qwen2_5OmniTalkerModel",parameters:[{name:"config",val:": Qwen2_5OmniTalkerConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniTalkerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniTalkerConfig">Qwen2_5OmniTalkerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L2077"}}),et=new U({props:{name:"forward",anchor:"transformers.Qwen2_5OmniTalkerModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs]"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Qwen2_5OmniTalkerModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L2098",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig"
>Qwen2_5OmniConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),je=new yo({props:{$$slots:{default:[tl]},$$scope:{ctx:j}}}),nt=new k({props:{title:"Qwen2_5OmniToken2WavConfig",local:"transformers.Qwen2_5OmniToken2WavConfig",headingTag:"h2"}}),tt=new U({props:{name:"class transformers.Qwen2_5OmniToken2WavConfig",anchor:"transformers.Qwen2_5OmniToken2WavConfig",parameters:[{name:"dit_config",val:" = None"},{name:"bigvgan_config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniToken2WavConfig.dit_config",description:`<strong>dit_config</strong> (<code>DiT_Args</code>, <em>optional</em>) &#x2014;
Configuration class for the Diffusion Transformer (DiT) module responsible for generating mel-spectrograms.`,name:"dit_config"},{anchor:"transformers.Qwen2_5OmniToken2WavConfig.bigvgan_config",description:`<strong>bigvgan_config</strong> (<code>BigVGAN_Args</code>, <em>optional</em>) &#x2014;
Configuration class for the BigVGAN module responsible for converting mel-spectrograms to waveforms.`,name:"bigvgan_config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py#L936"}}),Ce=new wo({props:{anchor:"transformers.Qwen2_5OmniToken2WavConfig.example",$$slots:{default:[ol]},$$scope:{ctx:j}}}),ot=new k({props:{title:"Qwen2_5OmniToken2WavModel",local:"transformers.Qwen2_5OmniToken2WavModel",headingTag:"h2"}}),st=new U({props:{name:"class transformers.Qwen2_5OmniToken2WavModel",anchor:"transformers.Qwen2_5OmniToken2WavModel",parameters:[{name:"config",val:": Qwen2_5OmniToken2WavConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniToken2WavModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniToken2WavConfig">Qwen2_5OmniToken2WavConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L3628"}}),at=new U({props:{name:"forward",anchor:"transformers.Qwen2_5OmniToken2WavModel.forward",parameters:[{name:"code",val:""},{name:"conditioning",val:""},{name:"reference_mel",val:""},{name:"num_steps",val:" = 10"},{name:"guidance_scale",val:" = 0.5"},{name:"sway_coefficient",val:" = -1.0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L3654"}}),rt=new k({props:{title:"Qwen2_5OmniToken2WavDiTModel",local:"transformers.Qwen2_5OmniToken2WavDiTModel",headingTag:"h2"}}),it=new U({props:{name:"class transformers.Qwen2_5OmniToken2WavDiTModel",anchor:"transformers.Qwen2_5OmniToken2WavDiTModel",parameters:[{name:"config",val:": Qwen2_5OmniDiTConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniToken2WavDiTModel.config",description:`<strong>config</strong> (<code>Qwen2_5OmniDiTConfig</code>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L3473"}}),lt=new k({props:{title:"Qwen2_5OmniToken2WavBigVGANModel",local:"transformers.Qwen2_5OmniToken2WavBigVGANModel",headingTag:"h2"}}),dt=new U({props:{name:"class transformers.Qwen2_5OmniToken2WavBigVGANModel",anchor:"transformers.Qwen2_5OmniToken2WavBigVGANModel",parameters:[{name:"config",val:": Qwen2_5OmniBigVGANConfig"}],parametersDescription:[{anchor:"transformers.Qwen2_5OmniToken2WavBigVGANModel.config",description:`<strong>config</strong> (<code>Qwen2_5OmniBigVGANConfig</code>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py#L3338"}}),ct=new Ei({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md"}}),{c(){d=i("meta"),T=t(),M=i("p"),w=t(),y=i("p"),y.innerHTML=c,J=t(),p(Ge.$$.fragment),bo=t(),pe=i("div"),pe.innerHTML=Tr,vo=t(),p(Ze.$$.fragment),Jo=t(),qe=i("p"),qe.innerHTML=yr,Uo=t(),Oe=i("p"),Oe.textContent=br,ko=t(),Fe=i("p"),Fe.innerHTML=vr,jo=t(),p(Ve.$$.fragment),Co=t(),ze=i("ul"),ze.innerHTML=Jr,Qo=t(),p(Ae.$$.fragment),Io=t(),Ne=i("p"),Ne.innerHTML=Ur,xo=t(),p(Re.$$.fragment),$o=t(),Xe=i("p"),Xe.textContent=kr,Bo=t(),p(Ee.$$.fragment),Wo=t(),p(He.$$.fragment),Go=t(),Ye=i("p"),Ye.innerHTML=jr,Zo=t(),p(Le.$$.fragment),qo=t(),p(Se.$$.fragment),Oo=t(),Pe=i("p"),Pe.innerHTML=Cr,Fo=t(),p(De.$$.fragment),Vo=t(),p(Ke.$$.fragment),zo=t(),p(en.$$.fragment),Ao=t(),nn=i("p"),nn.textContent=Qr,No=t(),p(tn.$$.fragment),Ro=t(),p(on.$$.fragment),Xo=t(),sn=i("p"),sn.textContent=Ir,Eo=t(),p(an.$$.fragment),Ho=t(),p(rn.$$.fragment),Yo=t(),ln=i("p"),ln.innerHTML=xr,Lo=t(),p(dn.$$.fragment),So=t(),cn=i("p"),cn.innerHTML=$r,Po=t(),p(mn.$$.fragment),Do=t(),p(pn.$$.fragment),Ko=t(),hn=i("p"),hn.innerHTML=Br,es=t(),p(un.$$.fragment),ns=t(),p(gn.$$.fragment),ts=t(),p(fn.$$.fragment),os=t(),_n=i("p"),_n.textContent=Wr,ss=t(),p(Mn.$$.fragment),as=t(),wn=i("p"),wn.innerHTML=Gr,rs=t(),Tn=i("p"),Tn.innerHTML=Zr,is=t(),p(yn.$$.fragment),ls=t(),p(bn.$$.fragment),ds=t(),I=i("div"),p(vn.$$.fragment),Xs=t(),pt=i("p"),pt.innerHTML=qr,Es=t(),ht=i("p"),ht.innerHTML=Or,Hs=t(),ut=i("p"),ut.innerHTML=Fr,Ys=t(),p(he.$$.fragment),Ls=t(),ue=i("div"),p(Jn.$$.fragment),Ss=t(),gt=i("p"),gt.textContent=Vr,cs=t(),p(Un.$$.fragment),ms=t(),K=i("div"),p(kn.$$.fragment),Ps=t(),ft=i("p"),ft.innerHTML=zr,Ds=t(),V=i("div"),p(jn.$$.fragment),Ks=t(),_t=i("p"),_t.textContent=Ar,ea=t(),Mt=i("p"),Mt.innerHTML=Nr,na=t(),wt=i("p"),wt.innerHTML=Rr,ta=t(),Tt=i("ul"),Tt.innerHTML=Xr,ps=t(),p(Cn.$$.fragment),hs=t(),x=i("div"),p(Qn.$$.fragment),oa=t(),yt=i("p"),yt.textContent=Er,sa=t(),bt=i("ul"),bt.innerHTML=Hr,aa=t(),vt=i("p"),vt.innerHTML=Yr,ra=t(),Jt=i("p"),Jt.innerHTML=Lr,ia=t(),S=i("div"),p(In.$$.fragment),la=t(),Ut=i("p"),Ut.textContent=Sr,da=t(),kt=i("p"),kt.textContent=Pr,ca=t(),p(ge.$$.fragment),us=t(),p(xn.$$.fragment),gs=t(),ee=i("div"),p($n.$$.fragment),ma=t(),z=i("div"),p(Bn.$$.fragment),pa=t(),jt=i("p"),jt.textContent=Dr,ha=t(),Ct=i("p"),Ct.innerHTML=Kr,ua=t(),Qt=i("p"),Qt.innerHTML=ei,ga=t(),It=i("ul"),It.innerHTML=ni,fa=t(),A=i("div"),p(Wn.$$.fragment),_a=t(),xt=i("p"),xt.textContent=ti,Ma=t(),$t=i("p"),$t.textContent=oi,wa=t(),Bt=i("p"),Bt.textContent=si,Ta=t(),Wt=i("p"),Wt.innerHTML=ai,fs=t(),p(Gn.$$.fragment),_s=t(),B=i("div"),p(Zn.$$.fragment),ya=t(),Gt=i("p"),Gt.innerHTML=ri,ba=t(),Zt=i("p"),Zt.innerHTML=ii,va=t(),qt=i("p"),qt.innerHTML=li,Ja=t(),p(fe.$$.fragment),Ms=t(),p(qn.$$.fragment),ws=t(),C=i("div"),p(On.$$.fragment),Ua=t(),Ot=i("p"),Ot.textContent=di,ka=t(),Ft=i("p"),Ft.innerHTML=ci,ja=t(),Vt=i("p"),Vt.innerHTML=mi,Ca=t(),P=i("div"),p(Fn.$$.fragment),Qa=t(),zt=i("p"),zt.innerHTML=pi,Ia=t(),p(_e.$$.fragment),xa=t(),p(Me.$$.fragment),$a=t(),we=i("div"),p(Vn.$$.fragment),Ba=t(),At=i("p"),At.textContent=hi,Wa=t(),Te=i("div"),p(zn.$$.fragment),Ga=t(),Nt=i("p"),Nt.textContent=ui,Za=t(),ye=i("div"),p(An.$$.fragment),qa=t(),Rt=i("p"),Rt.innerHTML=gi,Oa=t(),be=i("div"),p(Nn.$$.fragment),Fa=t(),Xt=i("p"),Xt.textContent=fi,Ts=t(),p(Rn.$$.fragment),ys=t(),W=i("div"),p(Xn.$$.fragment),Va=t(),Et=i("p"),Et.textContent=_i,za=t(),Ht=i("p"),Ht.innerHTML=Mi,Aa=t(),Yt=i("p"),Yt.innerHTML=wi,Na=t(),oe=i("div"),p(En.$$.fragment),Ra=t(),Lt=i("p"),Lt.innerHTML=Ti,Xa=t(),p(ve.$$.fragment),bs=t(),p(Hn.$$.fragment),vs=t(),G=i("div"),p(Yn.$$.fragment),Ea=t(),St=i("p"),St.innerHTML=yi,Ha=t(),Pt=i("p"),Pt.innerHTML=bi,Ya=t(),Dt=i("p"),Dt.innerHTML=vi,La=t(),p(Je.$$.fragment),Js=t(),p(Ln.$$.fragment),Us=t(),de=i("div"),p(Sn.$$.fragment),Sa=t(),D=i("div"),p(Pn.$$.fragment),Pa=t(),Kt=i("p"),Kt.innerHTML=Ji,Da=t(),p(Ue.$$.fragment),Ka=t(),p(ke.$$.fragment),ks=t(),p(Dn.$$.fragment),js=t(),Z=i("div"),p(Kn.$$.fragment),er=t(),eo=i("p"),eo.textContent=Ui,nr=t(),no=i("p"),no.innerHTML=ki,tr=t(),to=i("p"),to.innerHTML=ji,or=t(),se=i("div"),p(et.$$.fragment),sr=t(),oo=i("p"),oo.innerHTML=Ci,ar=t(),p(je.$$.fragment),Cs=t(),p(nt.$$.fragment),Qs=t(),N=i("div"),p(tt.$$.fragment),rr=t(),so=i("p"),so.innerHTML=Qi,ir=t(),ao=i("p"),ao.innerHTML=Ii,lr=t(),p(Ce.$$.fragment),Is=t(),p(ot.$$.fragment),xs=t(),q=i("div"),p(st.$$.fragment),dr=t(),ro=i("p"),ro.textContent=xi,cr=t(),io=i("p"),io.innerHTML=$i,mr=t(),lo=i("p"),lo.innerHTML=Bi,pr=t(),Qe=i("div"),p(at.$$.fragment),hr=t(),co=i("p"),co.textContent=Wi,$s=t(),p(rt.$$.fragment),Bs=t(),R=i("div"),p(it.$$.fragment),ur=t(),mo=i("p"),mo.textContent=Gi,gr=t(),po=i("p"),po.innerHTML=Zi,fr=t(),ho=i("p"),ho.innerHTML=qi,Ws=t(),p(lt.$$.fragment),Gs=t(),X=i("div"),p(dt.$$.fragment),_r=t(),uo=i("p"),uo.textContent=Oi,Mr=t(),go=i("p"),go.innerHTML=Fi,wr=t(),fo=i("p"),fo.innerHTML=Vi,Zs=t(),p(ct.$$.fragment),qs=t(),To=i("p"),this.h()},l(e){const n=Xi("svelte-u9bgzb",document.head);d=l(n,"META",{name:!0,content:!0}),n.forEach(a),T=o(e),M=l(e,"P",{}),v(M).forEach(a),w=o(e),y=l(e,"P",{"data-svelte-h":!0}),m(y)!=="svelte-19fy1aw"&&(y.innerHTML=c),J=o(e),h(Ge.$$.fragment,e),bo=o(e),pe=l(e,"DIV",{class:!0,"data-svelte-h":!0}),m(pe)!=="svelte-b95w5j"&&(pe.innerHTML=Tr),vo=o(e),h(Ze.$$.fragment,e),Jo=o(e),qe=l(e,"P",{"data-svelte-h":!0}),m(qe)!=="svelte-9zvnh"&&(qe.innerHTML=yr),Uo=o(e),Oe=l(e,"P",{"data-svelte-h":!0}),m(Oe)!=="svelte-6s4hx9"&&(Oe.textContent=br),ko=o(e),Fe=l(e,"P",{"data-svelte-h":!0}),m(Fe)!=="svelte-78riwv"&&(Fe.innerHTML=vr),jo=o(e),h(Ve.$$.fragment,e),Co=o(e),ze=l(e,"UL",{"data-svelte-h":!0}),m(ze)!=="svelte-gw7xhw"&&(ze.innerHTML=Jr),Qo=o(e),h(Ae.$$.fragment,e),Io=o(e),Ne=l(e,"P",{"data-svelte-h":!0}),m(Ne)!=="svelte-k446kp"&&(Ne.innerHTML=Ur),xo=o(e),h(Re.$$.fragment,e),$o=o(e),Xe=l(e,"P",{"data-svelte-h":!0}),m(Xe)!=="svelte-1a1s84g"&&(Xe.textContent=kr),Bo=o(e),h(Ee.$$.fragment,e),Wo=o(e),h(He.$$.fragment,e),Go=o(e),Ye=l(e,"P",{"data-svelte-h":!0}),m(Ye)!=="svelte-h1kv84"&&(Ye.innerHTML=jr),Zo=o(e),h(Le.$$.fragment,e),qo=o(e),h(Se.$$.fragment,e),Oo=o(e),Pe=l(e,"P",{"data-svelte-h":!0}),m(Pe)!=="svelte-cs0ob"&&(Pe.innerHTML=Cr),Fo=o(e),h(De.$$.fragment,e),Vo=o(e),h(Ke.$$.fragment,e),zo=o(e),h(en.$$.fragment,e),Ao=o(e),nn=l(e,"P",{"data-svelte-h":!0}),m(nn)!=="svelte-1rlewe6"&&(nn.textContent=Qr),No=o(e),h(tn.$$.fragment,e),Ro=o(e),h(on.$$.fragment,e),Xo=o(e),sn=l(e,"P",{"data-svelte-h":!0}),m(sn)!=="svelte-1yx8gz8"&&(sn.textContent=Ir),Eo=o(e),h(an.$$.fragment,e),Ho=o(e),h(rn.$$.fragment,e),Yo=o(e),ln=l(e,"P",{"data-svelte-h":!0}),m(ln)!=="svelte-17cyilo"&&(ln.innerHTML=xr),Lo=o(e),h(dn.$$.fragment,e),So=o(e),cn=l(e,"P",{"data-svelte-h":!0}),m(cn)!=="svelte-1bqejqv"&&(cn.innerHTML=$r),Po=o(e),h(mn.$$.fragment,e),Do=o(e),h(pn.$$.fragment,e),Ko=o(e),hn=l(e,"P",{"data-svelte-h":!0}),m(hn)!=="svelte-910kc7"&&(hn.innerHTML=Br),es=o(e),h(un.$$.fragment,e),ns=o(e),h(gn.$$.fragment,e),ts=o(e),h(fn.$$.fragment,e),os=o(e),_n=l(e,"P",{"data-svelte-h":!0}),m(_n)!=="svelte-13kjllm"&&(_n.textContent=Wr),ss=o(e),h(Mn.$$.fragment,e),as=o(e),wn=l(e,"P",{"data-svelte-h":!0}),m(wn)!=="svelte-1l8vrw1"&&(wn.innerHTML=Gr),rs=o(e),Tn=l(e,"P",{"data-svelte-h":!0}),m(Tn)!=="svelte-1lmbzwv"&&(Tn.innerHTML=Zr),is=o(e),h(yn.$$.fragment,e),ls=o(e),h(bn.$$.fragment,e),ds=o(e),I=l(e,"DIV",{class:!0});var O=v(I);h(vn.$$.fragment,O),Xs=o(O),pt=l(O,"P",{"data-svelte-h":!0}),m(pt)!=="svelte-1npm1ld"&&(pt.innerHTML=qr),Es=o(O),ht=l(O,"P",{"data-svelte-h":!0}),m(ht)!=="svelte-w281dc"&&(ht.innerHTML=Or),Hs=o(O),ut=l(O,"P",{"data-svelte-h":!0}),m(ut)!=="svelte-1ek1ss9"&&(ut.innerHTML=Fr),Ys=o(O),h(he.$$.fragment,O),Ls=o(O),ue=l(O,"DIV",{class:!0});var mt=v(ue);h(Jn.$$.fragment,mt),Ss=o(mt),gt=l(mt,"P",{"data-svelte-h":!0}),m(gt)!=="svelte-1jiloh6"&&(gt.textContent=Vr),mt.forEach(a),O.forEach(a),cs=o(e),h(Un.$$.fragment,e),ms=o(e),K=l(e,"DIV",{class:!0});var ce=v(K);h(kn.$$.fragment,ce),Ps=o(ce),ft=l(ce,"P",{"data-svelte-h":!0}),m(ft)!=="svelte-kamr2n"&&(ft.innerHTML=zr),Ds=o(ce),V=l(ce,"DIV",{class:!0});var E=v(V);h(jn.$$.fragment,E),Ks=o(E),_t=l(E,"P",{"data-svelte-h":!0}),m(_t)!=="svelte-c90czw"&&(_t.textContent=Ar),ea=o(E),Mt=l(E,"P",{"data-svelte-h":!0}),m(Mt)!=="svelte-1dpdnic"&&(Mt.innerHTML=Nr),na=o(E),wt=l(E,"P",{"data-svelte-h":!0}),m(wt)!=="svelte-wn1u3n"&&(wt.innerHTML=Rr),ta=o(E),Tt=l(E,"UL",{"data-svelte-h":!0}),m(Tt)!=="svelte-1mmdaxn"&&(Tt.innerHTML=Xr),E.forEach(a),ce.forEach(a),ps=o(e),h(Cn.$$.fragment,e),hs=o(e),x=l(e,"DIV",{class:!0});var F=v(x);h(Qn.$$.fragment,F),oa=o(F),yt=l(F,"P",{"data-svelte-h":!0}),m(yt)!=="svelte-hr418h"&&(yt.textContent=Er),sa=o(F),bt=l(F,"UL",{"data-svelte-h":!0}),m(bt)!=="svelte-znis1i"&&(bt.innerHTML=Hr),aa=o(F),vt=l(F,"P",{"data-svelte-h":!0}),m(vt)!=="svelte-q52n56"&&(vt.innerHTML=Yr),ra=o(F),Jt=l(F,"P",{"data-svelte-h":!0}),m(Jt)!=="svelte-hswkmf"&&(Jt.innerHTML=Lr),ia=o(F),S=l(F,"DIV",{class:!0});var ne=v(S);h(In.$$.fragment,ne),la=o(ne),Ut=l(ne,"P",{"data-svelte-h":!0}),m(Ut)!=="svelte-1q5ym45"&&(Ut.textContent=Sr),da=o(ne),kt=l(ne,"P",{"data-svelte-h":!0}),m(kt)!=="svelte-w8wo9i"&&(kt.textContent=Pr),ca=o(ne),h(ge.$$.fragment,ne),ne.forEach(a),F.forEach(a),us=o(e),h(xn.$$.fragment,e),gs=o(e),ee=l(e,"DIV",{class:!0});var me=v(ee);h($n.$$.fragment,me),ma=o(me),z=l(me,"DIV",{class:!0});var H=v(z);h(Bn.$$.fragment,H),pa=o(H),jt=l(H,"P",{"data-svelte-h":!0}),m(jt)!=="svelte-c90czw"&&(jt.textContent=Dr),ha=o(H),Ct=l(H,"P",{"data-svelte-h":!0}),m(Ct)!=="svelte-1dpdnic"&&(Ct.innerHTML=Kr),ua=o(H),Qt=l(H,"P",{"data-svelte-h":!0}),m(Qt)!=="svelte-wn1u3n"&&(Qt.innerHTML=ei),ga=o(H),It=l(H,"UL",{"data-svelte-h":!0}),m(It)!=="svelte-1mmdaxn"&&(It.innerHTML=ni),H.forEach(a),fa=o(me),A=l(me,"DIV",{class:!0});var Y=v(A);h(Wn.$$.fragment,Y),_a=o(Y),xt=l(Y,"P",{"data-svelte-h":!0}),m(xt)!=="svelte-178sk3p"&&(xt.textContent=ti),Ma=o(Y),$t=l(Y,"P",{"data-svelte-h":!0}),m($t)!=="svelte-10gp2od"&&($t.textContent=oi),wa=o(Y),Bt=l(Y,"P",{"data-svelte-h":!0}),m(Bt)!=="svelte-425t1k"&&(Bt.textContent=si),Ta=o(Y),Wt=l(Y,"P",{"data-svelte-h":!0}),m(Wt)!=="svelte-1yqcr51"&&(Wt.innerHTML=ai),Y.forEach(a),me.forEach(a),fs=o(e),h(Gn.$$.fragment,e),_s=o(e),B=l(e,"DIV",{class:!0});var L=v(B);h(Zn.$$.fragment,L),ya=o(L),Gt=l(L,"P",{"data-svelte-h":!0}),m(Gt)!=="svelte-178gubg"&&(Gt.innerHTML=ri),ba=o(L),Zt=l(L,"P",{"data-svelte-h":!0}),m(Zt)!=="svelte-160q1n1"&&(Zt.innerHTML=ii),va=o(L),qt=l(L,"P",{"data-svelte-h":!0}),m(qt)!=="svelte-1ek1ss9"&&(qt.innerHTML=li),Ja=o(L),h(fe.$$.fragment,L),L.forEach(a),Ms=o(e),h(qn.$$.fragment,e),ws=o(e),C=l(e,"DIV",{class:!0});var Q=v(C);h(On.$$.fragment,Q),Ua=o(Q),Ot=l(Q,"P",{"data-svelte-h":!0}),m(Ot)!=="svelte-64gikx"&&(Ot.textContent=di),ka=o(Q),Ft=l(Q,"P",{"data-svelte-h":!0}),m(Ft)!=="svelte-q52n56"&&(Ft.innerHTML=ci),ja=o(Q),Vt=l(Q,"P",{"data-svelte-h":!0}),m(Vt)!=="svelte-hswkmf"&&(Vt.innerHTML=mi),Ca=o(Q),P=l(Q,"DIV",{class:!0});var Ie=v(P);h(Fn.$$.fragment,Ie),Qa=o(Ie),zt=l(Ie,"P",{"data-svelte-h":!0}),m(zt)!=="svelte-192ypu6"&&(zt.innerHTML=pi),Ia=o(Ie),h(_e.$$.fragment,Ie),xa=o(Ie),h(Me.$$.fragment,Ie),Ie.forEach(a),$a=o(Q),we=l(Q,"DIV",{class:!0});var Fs=v(we);h(Vn.$$.fragment,Fs),Ba=o(Fs),At=l(Fs,"P",{"data-svelte-h":!0}),m(At)!=="svelte-1x5epig"&&(At.textContent=hi),Fs.forEach(a),Wa=o(Q),Te=l(Q,"DIV",{class:!0});var Vs=v(Te);h(zn.$$.fragment,Vs),Ga=o(Vs),Nt=l(Vs,"P",{"data-svelte-h":!0}),m(Nt)!=="svelte-16izj0z"&&(Nt.textContent=ui),Vs.forEach(a),Za=o(Q),ye=l(Q,"DIV",{class:!0});var zs=v(ye);h(An.$$.fragment,zs),qa=o(zs),Rt=l(zs,"P",{"data-svelte-h":!0}),m(Rt)!=="svelte-3ue1dv"&&(Rt.innerHTML=gi),zs.forEach(a),Oa=o(Q),be=l(Q,"DIV",{class:!0});var As=v(be);h(Nn.$$.fragment,As),Fa=o(As),Xt=l(As,"P",{"data-svelte-h":!0}),m(Xt)!=="svelte-114cymd"&&(Xt.textContent=fi),As.forEach(a),Q.forEach(a),Ts=o(e),h(Rn.$$.fragment,e),ys=o(e),W=l(e,"DIV",{class:!0});var ae=v(W);h(Xn.$$.fragment,ae),Va=o(ae),Et=l(ae,"P",{"data-svelte-h":!0}),m(Et)!=="svelte-c8yhui"&&(Et.textContent=_i),za=o(ae),Ht=l(ae,"P",{"data-svelte-h":!0}),m(Ht)!=="svelte-q52n56"&&(Ht.innerHTML=Mi),Aa=o(ae),Yt=l(ae,"P",{"data-svelte-h":!0}),m(Yt)!=="svelte-hswkmf"&&(Yt.innerHTML=wi),Na=o(ae),oe=l(ae,"DIV",{class:!0});var _o=v(oe);h(En.$$.fragment,_o),Ra=o(_o),Lt=l(_o,"P",{"data-svelte-h":!0}),m(Lt)!=="svelte-54w522"&&(Lt.innerHTML=Ti),Xa=o(_o),h(ve.$$.fragment,_o),_o.forEach(a),ae.forEach(a),bs=o(e),h(Hn.$$.fragment,e),vs=o(e),G=l(e,"DIV",{class:!0});var re=v(G);h(Yn.$$.fragment,re),Ea=o(re),St=l(re,"P",{"data-svelte-h":!0}),m(St)!=="svelte-x7m4r4"&&(St.innerHTML=yi),Ha=o(re),Pt=l(re,"P",{"data-svelte-h":!0}),m(Pt)!=="svelte-160q1n1"&&(Pt.innerHTML=bi),Ya=o(re),Dt=l(re,"P",{"data-svelte-h":!0}),m(Dt)!=="svelte-1ek1ss9"&&(Dt.innerHTML=vi),La=o(re),h(Je.$$.fragment,re),re.forEach(a),Js=o(e),h(Ln.$$.fragment,e),Us=o(e),de=l(e,"DIV",{class:!0});var Ns=v(de);h(Sn.$$.fragment,Ns),Sa=o(Ns),D=l(Ns,"DIV",{class:!0});var xe=v(D);h(Pn.$$.fragment,xe),Pa=o(xe),Kt=l(xe,"P",{"data-svelte-h":!0}),m(Kt)!=="svelte-v2wmqm"&&(Kt.innerHTML=Ji),Da=o(xe),h(Ue.$$.fragment,xe),Ka=o(xe),h(ke.$$.fragment,xe),xe.forEach(a),Ns.forEach(a),ks=o(e),h(Dn.$$.fragment,e),js=o(e),Z=l(e,"DIV",{class:!0});var ie=v(Z);h(Kn.$$.fragment,ie),er=o(ie),eo=l(ie,"P",{"data-svelte-h":!0}),m(eo)!=="svelte-ltm00h"&&(eo.textContent=Ui),nr=o(ie),no=l(ie,"P",{"data-svelte-h":!0}),m(no)!=="svelte-q52n56"&&(no.innerHTML=ki),tr=o(ie),to=l(ie,"P",{"data-svelte-h":!0}),m(to)!=="svelte-hswkmf"&&(to.innerHTML=ji),or=o(ie),se=l(ie,"DIV",{class:!0});var Mo=v(se);h(et.$$.fragment,Mo),sr=o(Mo),oo=l(Mo,"P",{"data-svelte-h":!0}),m(oo)!=="svelte-1sq4soa"&&(oo.innerHTML=Ci),ar=o(Mo),h(je.$$.fragment,Mo),Mo.forEach(a),ie.forEach(a),Cs=o(e),h(nt.$$.fragment,e),Qs=o(e),N=l(e,"DIV",{class:!0});var $e=v(N);h(tt.$$.fragment,$e),rr=o($e),so=l($e,"P",{"data-svelte-h":!0}),m(so)!=="svelte-3lhn8q"&&(so.innerHTML=Qi),ir=o($e),ao=l($e,"P",{"data-svelte-h":!0}),m(ao)!=="svelte-1ek1ss9"&&(ao.innerHTML=Ii),lr=o($e),h(Ce.$$.fragment,$e),$e.forEach(a),Is=o(e),h(ot.$$.fragment,e),xs=o(e),q=l(e,"DIV",{class:!0});var le=v(q);h(st.$$.fragment,le),dr=o(le),ro=l(le,"P",{"data-svelte-h":!0}),m(ro)!=="svelte-1p6tkdl"&&(ro.textContent=xi),cr=o(le),io=l(le,"P",{"data-svelte-h":!0}),m(io)!=="svelte-q52n56"&&(io.innerHTML=$i),mr=o(le),lo=l(le,"P",{"data-svelte-h":!0}),m(lo)!=="svelte-hswkmf"&&(lo.innerHTML=Bi),pr=o(le),Qe=l(le,"DIV",{class:!0});var Rs=v(Qe);h(at.$$.fragment,Rs),hr=o(Rs),co=l(Rs,"P",{"data-svelte-h":!0}),m(co)!=="svelte-1ir5413"&&(co.textContent=Wi),Rs.forEach(a),le.forEach(a),$s=o(e),h(rt.$$.fragment,e),Bs=o(e),R=l(e,"DIV",{class:!0});var Be=v(R);h(it.$$.fragment,Be),ur=o(Be),mo=l(Be,"P",{"data-svelte-h":!0}),m(mo)!=="svelte-1rgawov"&&(mo.textContent=Gi),gr=o(Be),po=l(Be,"P",{"data-svelte-h":!0}),m(po)!=="svelte-q52n56"&&(po.innerHTML=Zi),fr=o(Be),ho=l(Be,"P",{"data-svelte-h":!0}),m(ho)!=="svelte-hswkmf"&&(ho.innerHTML=qi),Be.forEach(a),Ws=o(e),h(lt.$$.fragment,e),Gs=o(e),X=l(e,"DIV",{class:!0});var We=v(X);h(dt.$$.fragment,We),_r=o(We),uo=l(We,"P",{"data-svelte-h":!0}),m(uo)!=="svelte-cinuax"&&(uo.textContent=Oi),Mr=o(We),go=l(We,"P",{"data-svelte-h":!0}),m(go)!=="svelte-q52n56"&&(go.innerHTML=Fi),wr=o(We),fo=l(We,"P",{"data-svelte-h":!0}),m(fo)!=="svelte-hswkmf"&&(fo.innerHTML=Vi),We.forEach(a),Zs=o(e),h(ct.$$.fragment,e),qs=o(e),To=l(e,"P",{}),v(To).forEach(a),this.h()},h(){b(d,"name","hf:doc:metadata"),b(d,"content",al),b(pe,"class","flex flex-wrap space-x-1"),b(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){s(document.head,d),r(e,T,n),r(e,M,n),r(e,w,n),r(e,y,n),r(e,J,n),u(Ge,e,n),r(e,bo,n),r(e,pe,n),r(e,vo,n),u(Ze,e,n),r(e,Jo,n),r(e,qe,n),r(e,Uo,n),r(e,Oe,n),r(e,ko,n),r(e,Fe,n),r(e,jo,n),u(Ve,e,n),r(e,Co,n),r(e,ze,n),r(e,Qo,n),u(Ae,e,n),r(e,Io,n),r(e,Ne,n),r(e,xo,n),u(Re,e,n),r(e,$o,n),r(e,Xe,n),r(e,Bo,n),u(Ee,e,n),r(e,Wo,n),u(He,e,n),r(e,Go,n),r(e,Ye,n),r(e,Zo,n),u(Le,e,n),r(e,qo,n),u(Se,e,n),r(e,Oo,n),r(e,Pe,n),r(e,Fo,n),u(De,e,n),r(e,Vo,n),u(Ke,e,n),r(e,zo,n),u(en,e,n),r(e,Ao,n),r(e,nn,n),r(e,No,n),u(tn,e,n),r(e,Ro,n),u(on,e,n),r(e,Xo,n),r(e,sn,n),r(e,Eo,n),u(an,e,n),r(e,Ho,n),u(rn,e,n),r(e,Yo,n),r(e,ln,n),r(e,Lo,n),u(dn,e,n),r(e,So,n),r(e,cn,n),r(e,Po,n),u(mn,e,n),r(e,Do,n),u(pn,e,n),r(e,Ko,n),r(e,hn,n),r(e,es,n),u(un,e,n),r(e,ns,n),u(gn,e,n),r(e,ts,n),u(fn,e,n),r(e,os,n),r(e,_n,n),r(e,ss,n),u(Mn,e,n),r(e,as,n),r(e,wn,n),r(e,rs,n),r(e,Tn,n),r(e,is,n),u(yn,e,n),r(e,ls,n),u(bn,e,n),r(e,ds,n),r(e,I,n),u(vn,I,null),s(I,Xs),s(I,pt),s(I,Es),s(I,ht),s(I,Hs),s(I,ut),s(I,Ys),u(he,I,null),s(I,Ls),s(I,ue),u(Jn,ue,null),s(ue,Ss),s(ue,gt),r(e,cs,n),u(Un,e,n),r(e,ms,n),r(e,K,n),u(kn,K,null),s(K,Ps),s(K,ft),s(K,Ds),s(K,V),u(jn,V,null),s(V,Ks),s(V,_t),s(V,ea),s(V,Mt),s(V,na),s(V,wt),s(V,ta),s(V,Tt),r(e,ps,n),u(Cn,e,n),r(e,hs,n),r(e,x,n),u(Qn,x,null),s(x,oa),s(x,yt),s(x,sa),s(x,bt),s(x,aa),s(x,vt),s(x,ra),s(x,Jt),s(x,ia),s(x,S),u(In,S,null),s(S,la),s(S,Ut),s(S,da),s(S,kt),s(S,ca),u(ge,S,null),r(e,us,n),u(xn,e,n),r(e,gs,n),r(e,ee,n),u($n,ee,null),s(ee,ma),s(ee,z),u(Bn,z,null),s(z,pa),s(z,jt),s(z,ha),s(z,Ct),s(z,ua),s(z,Qt),s(z,ga),s(z,It),s(ee,fa),s(ee,A),u(Wn,A,null),s(A,_a),s(A,xt),s(A,Ma),s(A,$t),s(A,wa),s(A,Bt),s(A,Ta),s(A,Wt),r(e,fs,n),u(Gn,e,n),r(e,_s,n),r(e,B,n),u(Zn,B,null),s(B,ya),s(B,Gt),s(B,ba),s(B,Zt),s(B,va),s(B,qt),s(B,Ja),u(fe,B,null),r(e,Ms,n),u(qn,e,n),r(e,ws,n),r(e,C,n),u(On,C,null),s(C,Ua),s(C,Ot),s(C,ka),s(C,Ft),s(C,ja),s(C,Vt),s(C,Ca),s(C,P),u(Fn,P,null),s(P,Qa),s(P,zt),s(P,Ia),u(_e,P,null),s(P,xa),u(Me,P,null),s(C,$a),s(C,we),u(Vn,we,null),s(we,Ba),s(we,At),s(C,Wa),s(C,Te),u(zn,Te,null),s(Te,Ga),s(Te,Nt),s(C,Za),s(C,ye),u(An,ye,null),s(ye,qa),s(ye,Rt),s(C,Oa),s(C,be),u(Nn,be,null),s(be,Fa),s(be,Xt),r(e,Ts,n),u(Rn,e,n),r(e,ys,n),r(e,W,n),u(Xn,W,null),s(W,Va),s(W,Et),s(W,za),s(W,Ht),s(W,Aa),s(W,Yt),s(W,Na),s(W,oe),u(En,oe,null),s(oe,Ra),s(oe,Lt),s(oe,Xa),u(ve,oe,null),r(e,bs,n),u(Hn,e,n),r(e,vs,n),r(e,G,n),u(Yn,G,null),s(G,Ea),s(G,St),s(G,Ha),s(G,Pt),s(G,Ya),s(G,Dt),s(G,La),u(Je,G,null),r(e,Js,n),u(Ln,e,n),r(e,Us,n),r(e,de,n),u(Sn,de,null),s(de,Sa),s(de,D),u(Pn,D,null),s(D,Pa),s(D,Kt),s(D,Da),u(Ue,D,null),s(D,Ka),u(ke,D,null),r(e,ks,n),u(Dn,e,n),r(e,js,n),r(e,Z,n),u(Kn,Z,null),s(Z,er),s(Z,eo),s(Z,nr),s(Z,no),s(Z,tr),s(Z,to),s(Z,or),s(Z,se),u(et,se,null),s(se,sr),s(se,oo),s(se,ar),u(je,se,null),r(e,Cs,n),u(nt,e,n),r(e,Qs,n),r(e,N,n),u(tt,N,null),s(N,rr),s(N,so),s(N,ir),s(N,ao),s(N,lr),u(Ce,N,null),r(e,Is,n),u(ot,e,n),r(e,xs,n),r(e,q,n),u(st,q,null),s(q,dr),s(q,ro),s(q,cr),s(q,io),s(q,mr),s(q,lo),s(q,pr),s(q,Qe),u(at,Qe,null),s(Qe,hr),s(Qe,co),r(e,$s,n),u(rt,e,n),r(e,Bs,n),r(e,R,n),u(it,R,null),s(R,ur),s(R,mo),s(R,gr),s(R,po),s(R,fr),s(R,ho),r(e,Ws,n),u(lt,e,n),r(e,Gs,n),r(e,X,n),u(dt,X,null),s(X,_r),s(X,uo),s(X,Mr),s(X,go),s(X,wr),s(X,fo),r(e,Zs,n),u(ct,e,n),r(e,qs,n),r(e,To,n),Os=!0},p(e,[n]){const O={};n&2&&(O.$$scope={dirty:n,ctx:e}),he.$set(O);const mt={};n&2&&(mt.$$scope={dirty:n,ctx:e}),ge.$set(mt);const ce={};n&2&&(ce.$$scope={dirty:n,ctx:e}),fe.$set(ce);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),_e.$set(E);const F={};n&2&&(F.$$scope={dirty:n,ctx:e}),Me.$set(F);const ne={};n&2&&(ne.$$scope={dirty:n,ctx:e}),ve.$set(ne);const me={};n&2&&(me.$$scope={dirty:n,ctx:e}),Je.$set(me);const H={};n&2&&(H.$$scope={dirty:n,ctx:e}),Ue.$set(H);const Y={};n&2&&(Y.$$scope={dirty:n,ctx:e}),ke.$set(Y);const L={};n&2&&(L.$$scope={dirty:n,ctx:e}),je.$set(L);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),Ce.$set(Q)},i(e){Os||(g(Ge.$$.fragment,e),g(Ze.$$.fragment,e),g(Ve.$$.fragment,e),g(Ae.$$.fragment,e),g(Re.$$.fragment,e),g(Ee.$$.fragment,e),g(He.$$.fragment,e),g(Le.$$.fragment,e),g(Se.$$.fragment,e),g(De.$$.fragment,e),g(Ke.$$.fragment,e),g(en.$$.fragment,e),g(tn.$$.fragment,e),g(on.$$.fragment,e),g(an.$$.fragment,e),g(rn.$$.fragment,e),g(dn.$$.fragment,e),g(mn.$$.fragment,e),g(pn.$$.fragment,e),g(un.$$.fragment,e),g(gn.$$.fragment,e),g(fn.$$.fragment,e),g(Mn.$$.fragment,e),g(yn.$$.fragment,e),g(bn.$$.fragment,e),g(vn.$$.fragment,e),g(he.$$.fragment,e),g(Jn.$$.fragment,e),g(Un.$$.fragment,e),g(kn.$$.fragment,e),g(jn.$$.fragment,e),g(Cn.$$.fragment,e),g(Qn.$$.fragment,e),g(In.$$.fragment,e),g(ge.$$.fragment,e),g(xn.$$.fragment,e),g($n.$$.fragment,e),g(Bn.$$.fragment,e),g(Wn.$$.fragment,e),g(Gn.$$.fragment,e),g(Zn.$$.fragment,e),g(fe.$$.fragment,e),g(qn.$$.fragment,e),g(On.$$.fragment,e),g(Fn.$$.fragment,e),g(_e.$$.fragment,e),g(Me.$$.fragment,e),g(Vn.$$.fragment,e),g(zn.$$.fragment,e),g(An.$$.fragment,e),g(Nn.$$.fragment,e),g(Rn.$$.fragment,e),g(Xn.$$.fragment,e),g(En.$$.fragment,e),g(ve.$$.fragment,e),g(Hn.$$.fragment,e),g(Yn.$$.fragment,e),g(Je.$$.fragment,e),g(Ln.$$.fragment,e),g(Sn.$$.fragment,e),g(Pn.$$.fragment,e),g(Ue.$$.fragment,e),g(ke.$$.fragment,e),g(Dn.$$.fragment,e),g(Kn.$$.fragment,e),g(et.$$.fragment,e),g(je.$$.fragment,e),g(nt.$$.fragment,e),g(tt.$$.fragment,e),g(Ce.$$.fragment,e),g(ot.$$.fragment,e),g(st.$$.fragment,e),g(at.$$.fragment,e),g(rt.$$.fragment,e),g(it.$$.fragment,e),g(lt.$$.fragment,e),g(dt.$$.fragment,e),g(ct.$$.fragment,e),Os=!0)},o(e){f(Ge.$$.fragment,e),f(Ze.$$.fragment,e),f(Ve.$$.fragment,e),f(Ae.$$.fragment,e),f(Re.$$.fragment,e),f(Ee.$$.fragment,e),f(He.$$.fragment,e),f(Le.$$.fragment,e),f(Se.$$.fragment,e),f(De.$$.fragment,e),f(Ke.$$.fragment,e),f(en.$$.fragment,e),f(tn.$$.fragment,e),f(on.$$.fragment,e),f(an.$$.fragment,e),f(rn.$$.fragment,e),f(dn.$$.fragment,e),f(mn.$$.fragment,e),f(pn.$$.fragment,e),f(un.$$.fragment,e),f(gn.$$.fragment,e),f(fn.$$.fragment,e),f(Mn.$$.fragment,e),f(yn.$$.fragment,e),f(bn.$$.fragment,e),f(vn.$$.fragment,e),f(he.$$.fragment,e),f(Jn.$$.fragment,e),f(Un.$$.fragment,e),f(kn.$$.fragment,e),f(jn.$$.fragment,e),f(Cn.$$.fragment,e),f(Qn.$$.fragment,e),f(In.$$.fragment,e),f(ge.$$.fragment,e),f(xn.$$.fragment,e),f($n.$$.fragment,e),f(Bn.$$.fragment,e),f(Wn.$$.fragment,e),f(Gn.$$.fragment,e),f(Zn.$$.fragment,e),f(fe.$$.fragment,e),f(qn.$$.fragment,e),f(On.$$.fragment,e),f(Fn.$$.fragment,e),f(_e.$$.fragment,e),f(Me.$$.fragment,e),f(Vn.$$.fragment,e),f(zn.$$.fragment,e),f(An.$$.fragment,e),f(Nn.$$.fragment,e),f(Rn.$$.fragment,e),f(Xn.$$.fragment,e),f(En.$$.fragment,e),f(ve.$$.fragment,e),f(Hn.$$.fragment,e),f(Yn.$$.fragment,e),f(Je.$$.fragment,e),f(Ln.$$.fragment,e),f(Sn.$$.fragment,e),f(Pn.$$.fragment,e),f(Ue.$$.fragment,e),f(ke.$$.fragment,e),f(Dn.$$.fragment,e),f(Kn.$$.fragment,e),f(et.$$.fragment,e),f(je.$$.fragment,e),f(nt.$$.fragment,e),f(tt.$$.fragment,e),f(Ce.$$.fragment,e),f(ot.$$.fragment,e),f(st.$$.fragment,e),f(at.$$.fragment,e),f(rt.$$.fragment,e),f(it.$$.fragment,e),f(lt.$$.fragment,e),f(dt.$$.fragment,e),f(ct.$$.fragment,e),Os=!1},d(e){e&&(a(T),a(M),a(w),a(y),a(J),a(bo),a(pe),a(vo),a(Jo),a(qe),a(Uo),a(Oe),a(ko),a(Fe),a(jo),a(Co),a(ze),a(Qo),a(Io),a(Ne),a(xo),a($o),a(Xe),a(Bo),a(Wo),a(Go),a(Ye),a(Zo),a(qo),a(Oo),a(Pe),a(Fo),a(Vo),a(zo),a(Ao),a(nn),a(No),a(Ro),a(Xo),a(sn),a(Eo),a(Ho),a(Yo),a(ln),a(Lo),a(So),a(cn),a(Po),a(Do),a(Ko),a(hn),a(es),a(ns),a(ts),a(os),a(_n),a(ss),a(as),a(wn),a(rs),a(Tn),a(is),a(ls),a(ds),a(I),a(cs),a(ms),a(K),a(ps),a(hs),a(x),a(us),a(gs),a(ee),a(fs),a(_s),a(B),a(Ms),a(ws),a(C),a(Ts),a(ys),a(W),a(bs),a(vs),a(G),a(Js),a(Us),a(de),a(ks),a(js),a(Z),a(Cs),a(Qs),a(N),a(Is),a(xs),a(q),a($s),a(Bs),a(R),a(Ws),a(Gs),a(X),a(Zs),a(qs),a(To)),a(d),_(Ge,e),_(Ze,e),_(Ve,e),_(Ae,e),_(Re,e),_(Ee,e),_(He,e),_(Le,e),_(Se,e),_(De,e),_(Ke,e),_(en,e),_(tn,e),_(on,e),_(an,e),_(rn,e),_(dn,e),_(mn,e),_(pn,e),_(un,e),_(gn,e),_(fn,e),_(Mn,e),_(yn,e),_(bn,e),_(vn),_(he),_(Jn),_(Un,e),_(kn),_(jn),_(Cn,e),_(Qn),_(In),_(ge),_(xn,e),_($n),_(Bn),_(Wn),_(Gn,e),_(Zn),_(fe),_(qn,e),_(On),_(Fn),_(_e),_(Me),_(Vn),_(zn),_(An),_(Nn),_(Rn,e),_(Xn),_(En),_(ve),_(Hn,e),_(Yn),_(Je),_(Ln,e),_(Sn),_(Pn),_(Ue),_(ke),_(Dn,e),_(Kn),_(et),_(je),_(nt,e),_(tt),_(Ce),_(ot,e),_(st),_(at),_(rt,e),_(it),_(lt,e),_(dt),_(ct,e)}}}const al='{"title":"Qwen2.5-Omni","local":"qwen25-omni","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[{"title":"Single Media inference","local":"single-media-inference","sections":[],"depth":3},{"title":"Text-only generation","local":"text-only-generation","sections":[],"depth":3},{"title":"Batch Mixed Media Inference","local":"batch-mixed-media-inference","sections":[],"depth":3},{"title":"Usage Tips","local":"usage-tips","sections":[{"title":"Image Resolution trade-off","local":"image-resolution-trade-off","sections":[],"depth":4},{"title":"Prompt for audio output","local":"prompt-for-audio-output","sections":[],"depth":4},{"title":"Use audio output or not","local":"use-audio-output-or-not","sections":[],"depth":4},{"title":"Change voice type of output audio","local":"change-voice-type-of-output-audio","sections":[],"depth":4},{"title":"Flash-Attention 2 to speed up generation","local":"flash-attention-2-to-speed-up-generation","sections":[],"depth":4}],"depth":3}],"depth":2},{"title":"Qwen2_5OmniConfig","local":"transformers.Qwen2_5OmniConfig","sections":[],"depth":2},{"title":"Qwen2_5OmniProcessor","local":"transformers.Qwen2_5OmniProcessor","sections":[],"depth":2},{"title":"Qwen2_5OmniForConditionalGeneration","local":"transformers.Qwen2_5OmniForConditionalGeneration","sections":[],"depth":2},{"title":"Qwen2_5OmniPreTrainedModelForConditionalGeneration","local":"transformers.Qwen2_5OmniPreTrainedModelForConditionalGeneration","sections":[],"depth":2},{"title":"Qwen2_5OmniThinkerConfig","local":"transformers.Qwen2_5OmniThinkerConfig","sections":[],"depth":2},{"title":"Qwen2_5OmniThinkerForConditionalGeneration","local":"transformers.Qwen2_5OmniThinkerForConditionalGeneration","sections":[],"depth":2},{"title":"Qwen2_5OmniThinkerTextModel","local":"transformers.Qwen2_5OmniThinkerTextModel","sections":[],"depth":2},{"title":"Qwen2_5OmniTalkerConfig","local":"transformers.Qwen2_5OmniTalkerConfig","sections":[],"depth":2},{"title":"Qwen2_5OmniTalkerForConditionalGeneration","local":"transformers.Qwen2_5OmniTalkerForConditionalGeneration","sections":[],"depth":2},{"title":"Qwen2_5OmniTalkerModel","local":"transformers.Qwen2_5OmniTalkerModel","sections":[],"depth":2},{"title":"Qwen2_5OmniToken2WavConfig","local":"transformers.Qwen2_5OmniToken2WavConfig","sections":[],"depth":2},{"title":"Qwen2_5OmniToken2WavModel","local":"transformers.Qwen2_5OmniToken2WavModel","sections":[],"depth":2},{"title":"Qwen2_5OmniToken2WavDiTModel","local":"transformers.Qwen2_5OmniToken2WavDiTModel","sections":[],"depth":2},{"title":"Qwen2_5OmniToken2WavBigVGANModel","local":"transformers.Qwen2_5OmniToken2WavBigVGANModel","sections":[],"depth":2}],"depth":1}';function rl(j){return Ai(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ul extends Ni{constructor(d){super(),Ri(this,d,rl,sl,zi,{})}}export{ul as component};
