import{s as Ut,o as bt,n as _n}from"../chunks/scheduler.18a86fab.js";import{S as It,i as _t,g as r,s as o,r as u,A as Ct,h as i,f as t,c as a,j as q,x as p,u as m,k as G,y as c,a as s,v as h,d as M,t as y,w as J}from"../chunks/index.98837b22.js";import{T as vt}from"../chunks/Tip.77304350.js";import{D as Ie}from"../chunks/Docstring.a1ef7999.js";import{C as _e}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Sn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as W,E as Qt}from"../chunks/getInferenceSnippets.06c2775f.js";function Zt(A){let d,g="Example:",T,w,f;return w=new _e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yQXVkaW9Gb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMjBRd2VuMkF1ZGlvQ29uZmlnJTJDJTIwUXdlbjJBdWRpb0VuY29kZXJDb25maWclMkMlMjBRd2VuMkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBRd2VuMkF1ZGlvRW5jb2RlciUyMGNvbmZpZyUwQWF1ZGlvX2NvbmZpZyUyMCUzRCUyMFF3ZW4yQXVkaW9FbmNvZGVyQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwUXdlbjIlMjBjb25maWclMEF0ZXh0X2NvbmZpZyUyMCUzRCUyMFF3ZW4yQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwUXdlbjJBdWRpbyUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwUXdlbjJBdWRpb0NvbmZpZyhhdWRpb19jb25maWclMkMlMjB0ZXh0X2NvbmZpZyklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwcXdlbjItYXVkaW8lMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFF3ZW4yQXVkaW9Gb3JDb25kaXRpb25hbEdlbmVyYXRpb24oY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2AudioForConditionalGeneration, Qwen2AudioConfig, Qwen2AudioEncoderConfig, Qwen2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2AudioEncoder config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_config = Qwen2AudioEncoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2 config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = Qwen2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2Audio configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Qwen2AudioConfig(audio_config, text_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the qwen2-audio style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2AudioForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=r("p"),d.textContent=g,T=o(),u(w.$$.fragment)},l(l){d=i(l,"P",{"data-svelte-h":!0}),p(d)!=="svelte-11lpom8"&&(d.textContent=g),T=a(l),m(w.$$.fragment,l)},m(l,j){s(l,d,j),s(l,T,j),h(w,l,j),f=!0},p:_n,i(l){f||(M(w.$$.fragment,l),f=!0)},o(l){y(w.$$.fragment,l),f=!1},d(l){l&&(t(d),t(T)),J(w,l)}}}function Wt(A){let d,g="Example:",T,w,f;return w=new _e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFF3ZW4yQXVkaW9FbmNvZGVyQ29uZmlnJTJDJTIwUXdlbjJBdWRpb0VuY29kZXIlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwUXdlbjJBdWRpb0VuY29kZXJDb25maWclMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwUXdlbjJBdWRpb0VuY29kZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBRd2VuMkF1ZGlvRW5jb2RlciUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMEFtb2RlbCUyMCUzRCUyMFF3ZW4yQXVkaW9FbmNvZGVyKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2AudioEncoderConfig, Qwen2AudioEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2AudioEncoderConfig</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Qwen2AudioEncoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Qwen2AudioEncoder (with random weights)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2AudioEncoder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=r("p"),d.textContent=g,T=o(),u(w.$$.fragment)},l(l){d=i(l,"P",{"data-svelte-h":!0}),p(d)!=="svelte-11lpom8"&&(d.textContent=g),T=a(l),m(w.$$.fragment,l)},m(l,j){s(l,d,j),s(l,T,j),h(w,l,j),f=!0},p:_n,i(l){f||(M(w.$$.fragment,l),f=!0)},o(l){y(w.$$.fragment,l),f=!1},d(l){l&&(t(d),t(T)),J(w,l)}}}function kt(A){let d,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=r("p"),d.innerHTML=g},l(T){d=i(T,"P",{"data-svelte-h":!0}),p(d)!=="svelte-fincs2"&&(d.innerHTML=g)},m(T,w){s(T,d,w)},p:_n,d(T){T&&t(d)}}}function At(A){let d,g="Example:",T,w,f;return w=new _e({props:{code:"ZnJvbSUyMGlvJTIwaW1wb3J0JTIwQnl0ZXNJTyUwQWZyb20lMjB1cmxsaWIucmVxdWVzdCUyMGltcG9ydCUyMHVybG9wZW4lMEFpbXBvcnQlMjBsaWJyb3NhJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBRd2VuMkF1ZGlvRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBRd2VuMkF1ZGlvRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJRd2VuJTJGUXdlbjItQXVkaW8tN0IlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyUXdlbiUyRlF3ZW4yLUF1ZGlvLTdCJTIyKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMiUzQyU3Q2F1ZGlvX2JvcyU3QyUzRSUzQyU3Q0FVRElPJTdDJTNFJTNDJTdDYXVkaW9fZW9zJTdDJTNFR2VuZXJhdGUlMjB0aGUlMjBjYXB0aW9uJTIwaW4lMjBFbmdsaXNoJTNBJTIyJTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZxaWFud2VuLXJlcy5vc3MtY24tYmVpamluZy5hbGl5dW5jcy5jb20lMkZRd2VuMi1BdWRpbyUyRmF1ZGlvJTJGZ2xhc3MtYnJlYWtpbmctMTUxMjU2Lm1wMyUyMiUwQWF1ZGlvJTJDJTIwXyUyMCUzRCUyMGxpYnJvc2EubG9hZChCeXRlc0lPKHVybG9wZW4odXJsKS5yZWFkKCkpJTJDJTIwc3IlM0RzZWxmLnByb2Nlc3Nvci5mZWF0dXJlX2V4dHJhY3Rvci5zYW1wbGluZ19yYXRlKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcHJvbXB0JTJDJTIwYXVkaW9zJTNEYXVkaW8lMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbGVuZ3RoJTNEMzApJTBBcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZV9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSUyQyUyMGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMlM0RGYWxzZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> librosa
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Qwen2AudioForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Qwen2AudioForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;Generate the caption in English:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio, _ = librosa.load(BytesIO(urlopen(url).read()), sr=self.processor.feature_extractor.sampling_rate)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=prompt, audios=audio, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Generate the caption in English: Glass is breaking.&quot;</span>`,wrap:!1}}),{c(){d=r("p"),d.textContent=g,T=o(),u(w.$$.fragment)},l(l){d=i(l,"P",{"data-svelte-h":!0}),p(d)!=="svelte-11lpom8"&&(d.textContent=g),T=a(l),m(w.$$.fragment,l)},m(l,j){s(l,d,j),s(l,T,j),h(w,l,j),f=!0},p:_n,i(l){f||(M(w.$$.fragment,l),f=!0)},o(l){y(w.$$.fragment,l),f=!1},d(l){l&&(t(d),t(T)),J(w,l)}}}function Bt(A){let d,g,T,w,f,l="<em>This model was released on 2024-07-15 and added to Hugging Face Transformers on 2024-08-08.</em>",j,F,Fe,V,Ln='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',$e,$,Ye,Y,Dn="The Qwen2-Audio is the new model series of large audio-language models from the Qwen team. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. We introduce two distinct audio interaction modes:",He,H,Pn="<li>voice chat: users can freely engage in voice interactions with Qwen2-Audio without text input</li> <li>audio analysis: users could provide audio and text instructions for analysis during the interaction</li>",Se,S,Kn='It was proposed in <a href="https://huggingface.co/papers/2407.10759" rel="nofollow">Qwen2-Audio Technical Report</a> by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.',Le,L,On="The abstract from the paper is the following:",De,D,et="<em>We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. In contrast to complex hierarchical tags, we have simplified the pre-training process by utilizing natural language prompts for different data and tasks, and have further expanded the data volume. We have boosted the instruction-following capability of Qwen2-Audio and implemented two distinct audio interaction modes for voice chat and audio analysis. In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input. In the audio analysis mode, users could provide audio and text instructions for analysis during the interaction. Note that we do not use any system prompts to switch between voice chat and audio analysis modes. Qwen2-Audio is capable of intelligently comprehending the content within audio and following voice commands to respond appropriately. For instance, in an audio segment that simultaneously contains sounds, multi-speaker conversations, and a voice command, Qwen2-Audio can directly understand the command and provide an interpretation and response to the audio. Additionally, DPO has optimized the model’s performance in terms of factuality and adherence to desired behavior. According to the evaluation results from AIR-Bench, Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests focused on audio-centric instruction-following capabilities. Qwen2-Audio is open-sourced with the aim of fostering the advancement of the multi-modal language community.</em>",Pe,P,Ke,K,nt='<code>Qwen2-Audio-7B</code> and <code>Qwen2-Audio-7B-Instruct</code> can be found on the <a href="https://huggingface.co/Qwen" rel="nofollow">Huggingface Hub</a>',Oe,O,tt=`<p>[!NOTE]
The <code>head_mask</code> argument is ignored when using all attention implementation other than “eager”. If you have a <code>head_mask</code> and want it to have effect, load the model with <code>XXXModel.from_pretrained(model_id, attn_implementation=&quot;eager&quot;)</code></p>`,en,ee,nn,ne,tn,te,st="In the following, we demonstrate how to use <code>Qwen2-Audio-7B-Instruct</code> for the inference, supporting both voice chat and audio analysis modes. Note that we have used the ChatML format for dialog, in this demo we show how to leverage <code>apply_chat_template</code> for this purpose.",sn,se,on,oe,ot="In the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input:",an,ae,ln,le,rn,re,at="In the audio analysis, users could provide both audio and text instructions for analysis:",dn,ie,cn,de,pn,ce,lt="We also support batch inference:",un,pe,mn,ue,hn,U,me,Cn,Ce,rt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a>. It is used to instantiate an
Qwen2-Audio model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Qwen2-Audio.`,vn,ve,it='e.g. <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B" rel="nofollow">Qwen/Qwen2-Audio-7B</a>',Qn,Qe,dt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Zn,N,Mn,he,yn,b,Me,Wn,Ze,ct=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder">Qwen2AudioEncoder</a>. It is used to instantiate a
Qwen2-Audio audio encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the audio encoder of the Qwen2-Audio
architecture.`,kn,We,pt='e.g. <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B" rel="nofollow">Qwen/Qwen2-Audio-7B</a>',An,ke,ut=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Bn,R,Jn,ye,wn,k,Je,Gn,Ae,mt="Constructs a Qwen2Audio processor which wraps a Qwen2Audio feature extractor and a Qwen2Audio tokenizer into a single processor.",Xn,Be,ht=`<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioProcessor">Qwen2AudioProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Tn,we,fn,I,Te,qn,Ge,Mt="The audio model from Qwen2Audio without any head or projection on top.",Vn,Xe,yt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Nn,qe,Jt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Rn,Ve,fe,gn,ge,jn,_,je,xn,Ne,wt="The QWEN2AUDIO model which consists of a audio backbone and a language model.",En,Re,Tt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,zn,xe,ft=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Fn,Z,Ue,$n,Ee,gt='The <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Yn,x,Hn,E,Un,be,bn,ze,In;return F=new W({props:{title:"Qwen2Audio",local:"qwen2audio",headingTag:"h1"}}),$=new W({props:{title:"Overview",local:"overview",headingTag:"h2"}}),P=new W({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),ee=new W({props:{title:"Inference",local:"inference",headingTag:"h3"}}),ne=new _e({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Qwen2AudioForConditionalGeneration

model = Qwen2AudioForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)

prompt = <span class="hljs-string">&quot;&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;Generate the caption in English:&quot;</span>
url = <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3&quot;</span>
audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)
inputs = processor(text=prompt, audios=audio, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">256</span>)
generate_ids = generate_ids[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]

<span class="hljs-comment"># We can also omit the audio_bos and audio_eos tokens</span>
prompt = <span class="hljs-string">&quot;&lt;|AUDIO|&gt;Generate the caption in English:&quot;</span>
inputs = processor(text=prompt, audios=audio, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">256</span>)
generate_ids = generate_ids[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),se=new W({props:{title:"Voice Chat Inference",local:"voice-chat-inference",headingTag:"h3"}}),ae=new _e({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>)
model = Qwen2AudioForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

conversation = [
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav&quot;</span>},
    ]},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Yes, the speaker is female and in her twenties.&quot;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav&quot;</span>},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=<span class="hljs-literal">True</span>, tokenize=<span class="hljs-literal">False</span>)
audios = []
<span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> conversation:
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(message[<span class="hljs-string">&quot;content&quot;</span>], <span class="hljs-built_in">list</span>):
        <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> message[<span class="hljs-string">&quot;content&quot;</span>]:
            <span class="hljs-keyword">if</span> ele[<span class="hljs-string">&quot;type&quot;</span>] == <span class="hljs-string">&quot;audio&quot;</span>:
                audios.append(librosa.load(
                    BytesIO(urlopen(ele[<span class="hljs-string">&#x27;audio_url&#x27;</span>]).read()),
                    sr=processor.feature_extractor.sampling_rate)[<span class="hljs-number">0</span>]
                )

inputs = processor(text=text, audios=audios, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
inputs.input_ids = inputs.input_ids.to(model.device)

generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">256</span>)
generate_ids = generate_ids[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),le=new W({props:{title:"Audio Analysis Inference",local:"audio-analysis-inference",headingTag:"h3"}}),ie=new _e({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>)
model = Qwen2AudioForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

conversation = [
    {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;You are a helpful assistant.&#x27;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span>},
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What&#x27;s that sound?&quot;</span>},
    ]},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;It is the sound of glass shattering.&quot;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What can you do when you hear that?&quot;</span>},
    ]},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.&quot;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac&quot;</span>},
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What does the person say?&quot;</span>},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=<span class="hljs-literal">True</span>, tokenize=<span class="hljs-literal">False</span>)
audios = []
<span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> conversation:
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(message[<span class="hljs-string">&quot;content&quot;</span>], <span class="hljs-built_in">list</span>):
        <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> message[<span class="hljs-string">&quot;content&quot;</span>]:
            <span class="hljs-keyword">if</span> ele[<span class="hljs-string">&quot;type&quot;</span>] == <span class="hljs-string">&quot;audio&quot;</span>:
                audios.append(
                    librosa.load(
                        BytesIO(urlopen(ele[<span class="hljs-string">&#x27;audio_url&#x27;</span>]).read()),
                        sr=processor.feature_extractor.sampling_rate)[<span class="hljs-number">0</span>]
                )

inputs = processor(text=text, audios=audios, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
inputs.input_ids = inputs.input_ids.to(model.device)

generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">256</span>)
generate_ids = generate_ids[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),de=new W({props:{title:"Batch Inference",local:"batch-inference",headingTag:"h3"}}),pe=new _e({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>)
model = Qwen2AudioForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

conversation1 = [
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span>},
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What&#x27;s that sound?&quot;</span>},
    ]},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;It is the sound of glass shattering.&quot;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav&quot;</span>},
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What can you hear?&quot;</span>},
    ]}
]

conversation2 = [
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;audio&quot;</span>, <span class="hljs-string">&quot;audio_url&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac&quot;</span>},
        {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What does the person say?&quot;</span>},
    ]},
]

conversations = [conversation1, conversation2]

text = [processor.apply_chat_template(conversation, add_generation_prompt=<span class="hljs-literal">True</span>, tokenize=<span class="hljs-literal">False</span>) <span class="hljs-keyword">for</span> conversation <span class="hljs-keyword">in</span> conversations]

audios = []
<span class="hljs-keyword">for</span> conversation <span class="hljs-keyword">in</span> conversations:
    <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> conversation:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(message[<span class="hljs-string">&quot;content&quot;</span>], <span class="hljs-built_in">list</span>):
            <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> message[<span class="hljs-string">&quot;content&quot;</span>]:
                <span class="hljs-keyword">if</span> ele[<span class="hljs-string">&quot;type&quot;</span>] == <span class="hljs-string">&quot;audio&quot;</span>:
                    audios.append(
                        librosa.load(
                            BytesIO(urlopen(ele[<span class="hljs-string">&#x27;audio_url&#x27;</span>]).read()),
                            sr=processor.feature_extractor.sampling_rate)[<span class="hljs-number">0</span>]
                    )

inputs = processor(text=text, audios=audios, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>] = inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>].to(model.device)
inputs.input_ids = inputs.input_ids.to(model.device)

generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">256</span>)
generate_ids = generate_ids[:, inputs.input_ids.size(<span class="hljs-number">1</span>):]

response = processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)`,wrap:!1}}),ue=new W({props:{title:"Qwen2AudioConfig",local:"transformers.Qwen2AudioConfig",headingTag:"h2"}}),me=new Ie({props:{name:"class transformers.Qwen2AudioConfig",anchor:"transformers.Qwen2AudioConfig",parameters:[{name:"audio_config",val:" = None"},{name:"text_config",val:" = None"},{name:"audio_token_index",val:" = 151646"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2AudioConfig.audio_config",description:`<strong>audio_config</strong> (<code>Union[AutoConfig, dict]</code>,  <em>optional</em>, defaults to <code>CLIPVisionConfig</code>) &#x2014;
The config object or dictionary of the audio backbone.`,name:"audio_config"},{anchor:"transformers.Qwen2AudioConfig.text_config",description:`<strong>text_config</strong> (<code>Union[AutoConfig, dict]</code>, <em>optional</em>, defaults to <code>LlamaConfig</code>) &#x2014;
The config object or dictionary of the text backbone.`,name:"text_config"},{anchor:"transformers.Qwen2AudioConfig.audio_token_index",description:`<strong>audio_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 151646) &#x2014;
The image token index to encode the image prompt.`,name:"audio_token_index"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/configuration_qwen2_audio.py#L119"}}),N=new Sn({props:{anchor:"transformers.Qwen2AudioConfig.example",$$slots:{default:[Zt]},$$scope:{ctx:A}}}),he=new W({props:{title:"Qwen2AudioEncoderConfig",local:"transformers.Qwen2AudioEncoderConfig",headingTag:"h2"}}),Me=new Ie({props:{name:"class transformers.Qwen2AudioEncoderConfig",anchor:"transformers.Qwen2AudioEncoderConfig",parameters:[{name:"num_mel_bins",val:" = 128"},{name:"encoder_layers",val:" = 32"},{name:"encoder_attention_heads",val:" = 20"},{name:"encoder_ffn_dim",val:" = 5120"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"d_model",val:" = 1280"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_function",val:" = 'gelu'"},{name:"activation_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"max_source_positions",val:" = 1500"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Qwen2AudioEncoderConfig.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of mel features used per input features. Should correspond to the value used in the
<code>Qwen2AudioProcessor</code> class.`,name:"num_mel_bins"},{anchor:"transformers.Qwen2AudioEncoderConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.Qwen2AudioEncoderConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.Qwen2AudioEncoderConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 5120) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in encoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.Qwen2AudioEncoderConfig.encoder_layerdrop",description:`<strong>encoder_layerdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The LayerDrop probability for the encoder. See the [LayerDrop paper](see <a href="https://huggingface.co/papers/1909.11556" rel="nofollow">https://huggingface.co/papers/1909.11556</a>)
for more details.`,name:"encoder_layerdrop"},{anchor:"transformers.Qwen2AudioEncoderConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
Dimensionality of the layers.`,name:"d_model"},{anchor:"transformers.Qwen2AudioEncoderConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.Qwen2AudioEncoderConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Qwen2AudioEncoderConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.Qwen2AudioEncoderConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Qwen2AudioEncoderConfig.scale_embedding",description:`<strong>scale_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Scale embeddings by diving by sqrt(d_model).`,name:"scale_embedding"},{anchor:"transformers.Qwen2AudioEncoderConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Qwen2AudioEncoderConfig.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1500) &#x2014;
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.`,name:"max_source_positions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/configuration_qwen2_audio.py#L24"}}),R=new Sn({props:{anchor:"transformers.Qwen2AudioEncoderConfig.example",$$slots:{default:[Wt]},$$scope:{ctx:A}}}),ye=new W({props:{title:"Qwen2AudioProcessor",local:"transformers.Qwen2AudioProcessor",headingTag:"h2"}}),Je=new Ie({props:{name:"class transformers.Qwen2AudioProcessor",anchor:"transformers.Qwen2AudioProcessor",parameters:[{name:"feature_extractor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"chat_template",val:" = None"},{name:"audio_token",val:" = '<|AUDIO|>'"},{name:"audio_bos_token",val:" = '<|audio_bos|>'"},{name:"audio_eos_token",val:" = '<|audio_eos|>'"}],parametersDescription:[{anchor:"transformers.Qwen2AudioProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a>, <em>optional</em>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.Qwen2AudioProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"},{anchor:"transformers.Qwen2AudioProcessor.chat_template",description:`<strong>chat_template</strong> (<code>Optional[str]</code>, <em>optional</em>) &#x2014;
The Jinja template to use for formatting the conversation. If not provided, the default chat template
is used.`,name:"chat_template"},{anchor:"transformers.Qwen2AudioProcessor.audio_token",description:`<strong>audio_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|AUDIO|&gt;&quot;</code>) &#x2014;
The token to use for audio tokens.`,name:"audio_token"},{anchor:"transformers.Qwen2AudioProcessor.audio_bos_token",description:`<strong>audio_bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|audio_bos|&gt;&quot;</code>) &#x2014;
The token to use for audio bos tokens.`,name:"audio_bos_token"},{anchor:"transformers.Qwen2AudioProcessor.audio_eos_token",description:`<strong>audio_eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|audio_eos|&gt;&quot;</code>) &#x2014;
The token to use for audio eos tokens.`,name:"audio_eos_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/processing_qwen2_audio.py#L37"}}),we=new W({props:{title:"Qwen2AudioEncoder",local:"transformers.Qwen2AudioEncoder",headingTag:"h2"}}),Te=new Ie({props:{name:"class transformers.Qwen2AudioEncoder",anchor:"transformers.Qwen2AudioEncoder",parameters:[{name:"config",val:": Qwen2AudioEncoderConfig"}],parametersDescription:[{anchor:"transformers.Qwen2AudioEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig">Qwen2AudioEncoderConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py#L295"}}),fe=new Ie({props:{name:"forward",anchor:"transformers.Qwen2AudioEncoder.forward",parameters:[{name:"input_features",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.Qwen2AudioEncoder.forward.attention_mask",description:"<strong>attention_mask</strong> (<code>torch.Tensor</code>)<code>, *optional*) -- Qwen2Audio does not support masking of the </code>input_features`, this argument is preserved for compatibility,\nbut it is not used. By default the silence in the input log mel spectrogram are ignored.",name:"attention_mask"},{anchor:"transformers.Qwen2AudioEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Qwen2AudioEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2AudioEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2AudioEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py#L346"}}),ge=new W({props:{title:"Qwen2AudioForConditionalGeneration",local:"transformers.Qwen2AudioForConditionalGeneration",headingTag:"h2"}}),je=new Ie({props:{name:"class transformers.Qwen2AudioForConditionalGeneration",anchor:"transformers.Qwen2AudioForConditionalGeneration",parameters:[{name:"config",val:": Qwen2AudioConfig"}],parametersDescription:[{anchor:"transformers.Qwen2AudioForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig">Qwen2AudioConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py#L474"}}),Ue=new Ie({props:{name:"forward",anchor:"transformers.Qwen2AudioForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_features",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"feature_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.input_features",description:`<strong>input_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, feature_dim)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input audio features. Audio features can be obtained using
<code>feature_extractor_class</code>. See <code>feature_extractor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioProcessor">Qwen2AudioProcessor</a> uses
<code>feature_extractor_class</code> for processing audios).`,name:"input_features"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.feature_attention_mask",description:`<strong>feature_attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, feature_sequence_length)</code>) &#x2014;
Mask to avoid performing attention on padding feature indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"feature_attention_mask"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/qwen2_audio/modeling_qwen2_audio.py#L714",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.qwen2_audio.modeling_qwen2_audio.Qwen2AudioCausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig"
>Qwen2AudioConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are
two sets of pre-computed hidden-states: key and values states in the self-attention blocks.
The <code>past_key_values</code> are returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>.
It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those
that don’t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of
all <code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>attention_mask</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) — Attentions mask, used to update attention mask and position_ids.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.qwen2_audio.modeling_qwen2_audio.Qwen2AudioCausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),x=new vt({props:{$$slots:{default:[kt]},$$scope:{ctx:A}}}),E=new Sn({props:{anchor:"transformers.Qwen2AudioForConditionalGeneration.forward.example",$$slots:{default:[At]},$$scope:{ctx:A}}}),be=new Qt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_audio.md"}}),{c(){d=r("meta"),g=o(),T=r("p"),w=o(),f=r("p"),f.innerHTML=l,j=o(),u(F.$$.fragment),Fe=o(),V=r("div"),V.innerHTML=Ln,$e=o(),u($.$$.fragment),Ye=o(),Y=r("p"),Y.textContent=Dn,He=o(),H=r("ul"),H.innerHTML=Pn,Se=o(),S=r("p"),S.innerHTML=Kn,Le=o(),L=r("p"),L.textContent=On,De=o(),D=r("p"),D.innerHTML=et,Pe=o(),u(P.$$.fragment),Ke=o(),K=r("p"),K.innerHTML=nt,Oe=o(),O=r("blockquote"),O.innerHTML=tt,en=o(),u(ee.$$.fragment),nn=o(),u(ne.$$.fragment),tn=o(),te=r("p"),te.innerHTML=st,sn=o(),u(se.$$.fragment),on=o(),oe=r("p"),oe.textContent=ot,an=o(),u(ae.$$.fragment),ln=o(),u(le.$$.fragment),rn=o(),re=r("p"),re.textContent=at,dn=o(),u(ie.$$.fragment),cn=o(),u(de.$$.fragment),pn=o(),ce=r("p"),ce.textContent=lt,un=o(),u(pe.$$.fragment),mn=o(),u(ue.$$.fragment),hn=o(),U=r("div"),u(me.$$.fragment),Cn=o(),Ce=r("p"),Ce.innerHTML=rt,vn=o(),ve=r("p"),ve.innerHTML=it,Qn=o(),Qe=r("p"),Qe.innerHTML=dt,Zn=o(),u(N.$$.fragment),Mn=o(),u(he.$$.fragment),yn=o(),b=r("div"),u(Me.$$.fragment),Wn=o(),Ze=r("p"),Ze.innerHTML=ct,kn=o(),We=r("p"),We.innerHTML=pt,An=o(),ke=r("p"),ke.innerHTML=ut,Bn=o(),u(R.$$.fragment),Jn=o(),u(ye.$$.fragment),wn=o(),k=r("div"),u(Je.$$.fragment),Gn=o(),Ae=r("p"),Ae.textContent=mt,Xn=o(),Be=r("p"),Be.innerHTML=ht,Tn=o(),u(we.$$.fragment),fn=o(),I=r("div"),u(Te.$$.fragment),qn=o(),Ge=r("p"),Ge.textContent=Mt,Vn=o(),Xe=r("p"),Xe.innerHTML=yt,Nn=o(),qe=r("p"),qe.innerHTML=Jt,Rn=o(),Ve=r("div"),u(fe.$$.fragment),gn=o(),u(ge.$$.fragment),jn=o(),_=r("div"),u(je.$$.fragment),xn=o(),Ne=r("p"),Ne.textContent=wt,En=o(),Re=r("p"),Re.innerHTML=Tt,zn=o(),xe=r("p"),xe.innerHTML=ft,Fn=o(),Z=r("div"),u(Ue.$$.fragment),$n=o(),Ee=r("p"),Ee.innerHTML=gt,Yn=o(),u(x.$$.fragment),Hn=o(),u(E.$$.fragment),Un=o(),u(be.$$.fragment),bn=o(),ze=r("p"),this.h()},l(e){const n=Ct("svelte-u9bgzb",document.head);d=i(n,"META",{name:!0,content:!0}),n.forEach(t),g=a(e),T=i(e,"P",{}),q(T).forEach(t),w=a(e),f=i(e,"P",{"data-svelte-h":!0}),p(f)!=="svelte-1b1sqyn"&&(f.innerHTML=l),j=a(e),m(F.$$.fragment,e),Fe=a(e),V=i(e,"DIV",{class:!0,"data-svelte-h":!0}),p(V)!=="svelte-b95w5j"&&(V.innerHTML=Ln),$e=a(e),m($.$$.fragment,e),Ye=a(e),Y=i(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-1r54lck"&&(Y.textContent=Dn),He=a(e),H=i(e,"UL",{"data-svelte-h":!0}),p(H)!=="svelte-p799jx"&&(H.innerHTML=Pn),Se=a(e),S=i(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-1m9mk0x"&&(S.innerHTML=Kn),Le=a(e),L=i(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-vfdo9a"&&(L.textContent=On),De=a(e),D=i(e,"P",{"data-svelte-h":!0}),p(D)!=="svelte-1453f3k"&&(D.innerHTML=et),Pe=a(e),m(P.$$.fragment,e),Ke=a(e),K=i(e,"P",{"data-svelte-h":!0}),p(K)!=="svelte-12upnh6"&&(K.innerHTML=nt),Oe=a(e),O=i(e,"BLOCKQUOTE",{"data-svelte-h":!0}),p(O)!=="svelte-1fwzni2"&&(O.innerHTML=tt),en=a(e),m(ee.$$.fragment,e),nn=a(e),m(ne.$$.fragment,e),tn=a(e),te=i(e,"P",{"data-svelte-h":!0}),p(te)!=="svelte-ubq47c"&&(te.innerHTML=st),sn=a(e),m(se.$$.fragment,e),on=a(e),oe=i(e,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-hygfg8"&&(oe.textContent=ot),an=a(e),m(ae.$$.fragment,e),ln=a(e),m(le.$$.fragment,e),rn=a(e),re=i(e,"P",{"data-svelte-h":!0}),p(re)!=="svelte-i9t6g7"&&(re.textContent=at),dn=a(e),m(ie.$$.fragment,e),cn=a(e),m(de.$$.fragment,e),pn=a(e),ce=i(e,"P",{"data-svelte-h":!0}),p(ce)!=="svelte-1smugch"&&(ce.textContent=lt),un=a(e),m(pe.$$.fragment,e),mn=a(e),m(ue.$$.fragment,e),hn=a(e),U=i(e,"DIV",{class:!0});var C=q(U);m(me.$$.fragment,C),Cn=a(C),Ce=i(C,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-jn8en1"&&(Ce.innerHTML=rt),vn=a(C),ve=i(C,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-v3fb75"&&(ve.innerHTML=it),Qn=a(C),Qe=i(C,"P",{"data-svelte-h":!0}),p(Qe)!=="svelte-1ek1ss9"&&(Qe.innerHTML=dt),Zn=a(C),m(N.$$.fragment,C),C.forEach(t),Mn=a(e),m(he.$$.fragment,e),yn=a(e),b=i(e,"DIV",{class:!0});var v=q(b);m(Me.$$.fragment,v),Wn=a(v),Ze=i(v,"P",{"data-svelte-h":!0}),p(Ze)!=="svelte-ho6d2f"&&(Ze.innerHTML=ct),kn=a(v),We=i(v,"P",{"data-svelte-h":!0}),p(We)!=="svelte-v3fb75"&&(We.innerHTML=pt),An=a(v),ke=i(v,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-1ek1ss9"&&(ke.innerHTML=ut),Bn=a(v),m(R.$$.fragment,v),v.forEach(t),Jn=a(e),m(ye.$$.fragment,e),wn=a(e),k=i(e,"DIV",{class:!0});var X=q(k);m(Je.$$.fragment,X),Gn=a(X),Ae=i(X,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-1i2lr9"&&(Ae.textContent=mt),Xn=a(X),Be=i(X,"P",{"data-svelte-h":!0}),p(Be)!=="svelte-mg87et"&&(Be.innerHTML=ht),X.forEach(t),Tn=a(e),m(we.$$.fragment,e),fn=a(e),I=i(e,"DIV",{class:!0});var Q=q(I);m(Te.$$.fragment,Q),qn=a(Q),Ge=i(Q,"P",{"data-svelte-h":!0}),p(Ge)!=="svelte-1mpy8wd"&&(Ge.textContent=Mt),Vn=a(Q),Xe=i(Q,"P",{"data-svelte-h":!0}),p(Xe)!=="svelte-q52n56"&&(Xe.innerHTML=yt),Nn=a(Q),qe=i(Q,"P",{"data-svelte-h":!0}),p(qe)!=="svelte-hswkmf"&&(qe.innerHTML=Jt),Rn=a(Q),Ve=i(Q,"DIV",{class:!0});var jt=q(Ve);m(fe.$$.fragment,jt),jt.forEach(t),Q.forEach(t),gn=a(e),m(ge.$$.fragment,e),jn=a(e),_=i(e,"DIV",{class:!0});var B=q(_);m(je.$$.fragment,B),xn=a(B),Ne=i(B,"P",{"data-svelte-h":!0}),p(Ne)!=="svelte-1cqaci6"&&(Ne.textContent=wt),En=a(B),Re=i(B,"P",{"data-svelte-h":!0}),p(Re)!=="svelte-q52n56"&&(Re.innerHTML=Tt),zn=a(B),xe=i(B,"P",{"data-svelte-h":!0}),p(xe)!=="svelte-hswkmf"&&(xe.innerHTML=ft),Fn=a(B),Z=i(B,"DIV",{class:!0});var z=q(Z);m(Ue.$$.fragment,z),$n=a(z),Ee=i(z,"P",{"data-svelte-h":!0}),p(Ee)!=="svelte-176qw3x"&&(Ee.innerHTML=gt),Yn=a(z),m(x.$$.fragment,z),Hn=a(z),m(E.$$.fragment,z),z.forEach(t),B.forEach(t),Un=a(e),m(be.$$.fragment,e),bn=a(e),ze=i(e,"P",{}),q(ze).forEach(t),this.h()},h(){G(d,"name","hf:doc:metadata"),G(d,"content",Gt),G(V,"class","flex flex-wrap space-x-1"),G(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){c(document.head,d),s(e,g,n),s(e,T,n),s(e,w,n),s(e,f,n),s(e,j,n),h(F,e,n),s(e,Fe,n),s(e,V,n),s(e,$e,n),h($,e,n),s(e,Ye,n),s(e,Y,n),s(e,He,n),s(e,H,n),s(e,Se,n),s(e,S,n),s(e,Le,n),s(e,L,n),s(e,De,n),s(e,D,n),s(e,Pe,n),h(P,e,n),s(e,Ke,n),s(e,K,n),s(e,Oe,n),s(e,O,n),s(e,en,n),h(ee,e,n),s(e,nn,n),h(ne,e,n),s(e,tn,n),s(e,te,n),s(e,sn,n),h(se,e,n),s(e,on,n),s(e,oe,n),s(e,an,n),h(ae,e,n),s(e,ln,n),h(le,e,n),s(e,rn,n),s(e,re,n),s(e,dn,n),h(ie,e,n),s(e,cn,n),h(de,e,n),s(e,pn,n),s(e,ce,n),s(e,un,n),h(pe,e,n),s(e,mn,n),h(ue,e,n),s(e,hn,n),s(e,U,n),h(me,U,null),c(U,Cn),c(U,Ce),c(U,vn),c(U,ve),c(U,Qn),c(U,Qe),c(U,Zn),h(N,U,null),s(e,Mn,n),h(he,e,n),s(e,yn,n),s(e,b,n),h(Me,b,null),c(b,Wn),c(b,Ze),c(b,kn),c(b,We),c(b,An),c(b,ke),c(b,Bn),h(R,b,null),s(e,Jn,n),h(ye,e,n),s(e,wn,n),s(e,k,n),h(Je,k,null),c(k,Gn),c(k,Ae),c(k,Xn),c(k,Be),s(e,Tn,n),h(we,e,n),s(e,fn,n),s(e,I,n),h(Te,I,null),c(I,qn),c(I,Ge),c(I,Vn),c(I,Xe),c(I,Nn),c(I,qe),c(I,Rn),c(I,Ve),h(fe,Ve,null),s(e,gn,n),h(ge,e,n),s(e,jn,n),s(e,_,n),h(je,_,null),c(_,xn),c(_,Ne),c(_,En),c(_,Re),c(_,zn),c(_,xe),c(_,Fn),c(_,Z),h(Ue,Z,null),c(Z,$n),c(Z,Ee),c(Z,Yn),h(x,Z,null),c(Z,Hn),h(E,Z,null),s(e,Un,n),h(be,e,n),s(e,bn,n),s(e,ze,n),In=!0},p(e,[n]){const C={};n&2&&(C.$$scope={dirty:n,ctx:e}),N.$set(C);const v={};n&2&&(v.$$scope={dirty:n,ctx:e}),R.$set(v);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),x.$set(X);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),E.$set(Q)},i(e){In||(M(F.$$.fragment,e),M($.$$.fragment,e),M(P.$$.fragment,e),M(ee.$$.fragment,e),M(ne.$$.fragment,e),M(se.$$.fragment,e),M(ae.$$.fragment,e),M(le.$$.fragment,e),M(ie.$$.fragment,e),M(de.$$.fragment,e),M(pe.$$.fragment,e),M(ue.$$.fragment,e),M(me.$$.fragment,e),M(N.$$.fragment,e),M(he.$$.fragment,e),M(Me.$$.fragment,e),M(R.$$.fragment,e),M(ye.$$.fragment,e),M(Je.$$.fragment,e),M(we.$$.fragment,e),M(Te.$$.fragment,e),M(fe.$$.fragment,e),M(ge.$$.fragment,e),M(je.$$.fragment,e),M(Ue.$$.fragment,e),M(x.$$.fragment,e),M(E.$$.fragment,e),M(be.$$.fragment,e),In=!0)},o(e){y(F.$$.fragment,e),y($.$$.fragment,e),y(P.$$.fragment,e),y(ee.$$.fragment,e),y(ne.$$.fragment,e),y(se.$$.fragment,e),y(ae.$$.fragment,e),y(le.$$.fragment,e),y(ie.$$.fragment,e),y(de.$$.fragment,e),y(pe.$$.fragment,e),y(ue.$$.fragment,e),y(me.$$.fragment,e),y(N.$$.fragment,e),y(he.$$.fragment,e),y(Me.$$.fragment,e),y(R.$$.fragment,e),y(ye.$$.fragment,e),y(Je.$$.fragment,e),y(we.$$.fragment,e),y(Te.$$.fragment,e),y(fe.$$.fragment,e),y(ge.$$.fragment,e),y(je.$$.fragment,e),y(Ue.$$.fragment,e),y(x.$$.fragment,e),y(E.$$.fragment,e),y(be.$$.fragment,e),In=!1},d(e){e&&(t(g),t(T),t(w),t(f),t(j),t(Fe),t(V),t($e),t(Ye),t(Y),t(He),t(H),t(Se),t(S),t(Le),t(L),t(De),t(D),t(Pe),t(Ke),t(K),t(Oe),t(O),t(en),t(nn),t(tn),t(te),t(sn),t(on),t(oe),t(an),t(ln),t(rn),t(re),t(dn),t(cn),t(pn),t(ce),t(un),t(mn),t(hn),t(U),t(Mn),t(yn),t(b),t(Jn),t(wn),t(k),t(Tn),t(fn),t(I),t(gn),t(jn),t(_),t(Un),t(bn),t(ze)),t(d),J(F,e),J($,e),J(P,e),J(ee,e),J(ne,e),J(se,e),J(ae,e),J(le,e),J(ie,e),J(de,e),J(pe,e),J(ue,e),J(me),J(N),J(he,e),J(Me),J(R),J(ye,e),J(Je),J(we,e),J(Te),J(fe),J(ge,e),J(je),J(Ue),J(x),J(E),J(be,e)}}}const Gt='{"title":"Qwen2Audio","local":"qwen2audio","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[{"title":"Inference","local":"inference","sections":[],"depth":3},{"title":"Voice Chat Inference","local":"voice-chat-inference","sections":[],"depth":3},{"title":"Audio Analysis Inference","local":"audio-analysis-inference","sections":[],"depth":3},{"title":"Batch Inference","local":"batch-inference","sections":[],"depth":3}],"depth":2},{"title":"Qwen2AudioConfig","local":"transformers.Qwen2AudioConfig","sections":[],"depth":2},{"title":"Qwen2AudioEncoderConfig","local":"transformers.Qwen2AudioEncoderConfig","sections":[],"depth":2},{"title":"Qwen2AudioProcessor","local":"transformers.Qwen2AudioProcessor","sections":[],"depth":2},{"title":"Qwen2AudioEncoder","local":"transformers.Qwen2AudioEncoder","sections":[],"depth":2},{"title":"Qwen2AudioForConditionalGeneration","local":"transformers.Qwen2AudioForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function Xt(A){return bt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ft extends It{constructor(d){super(),_t(this,d,Xt,Bt,Ut,{})}}export{Ft as component};
