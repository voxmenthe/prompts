import{s as pr,o as mr,n as me}from"../chunks/scheduler.18a86fab.js";import{S as ur,i as fr,g as d,s as o,r as f,A as hr,h as l,f as s,c as r,j as x,x as p,u as h,k as $,y as n,a as c,v as g,d as _,t as v,w as b}from"../chunks/index.98837b22.js";import{T as bo}from"../chunks/Tip.77304350.js";import{D}from"../chunks/Docstring.a1ef7999.js";import{C as Ut}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Lt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as j,E as gr}from"../chunks/getInferenceSnippets.06c2775f.js";function _r(k){let i,R="Example:",u,m,T;return m=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUkNvbmZpZyUyQyUyMERQUkNvbnRleHRFbmNvZGVyJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMERQUiUyMGZhY2Vib29rJTJGZHByLWN0eF9lbmNvZGVyLXNpbmdsZS1ucS1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMERQUkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBmYWNlYm9vayUyRmRwci1jdHhfZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRFBSQ29udGV4dEVuY29kZXIoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRConfig, DPRContextEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DPR facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DPRConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=d("p"),i.textContent=R,u=o(),f(m.$$.fragment)},l(a){i=l(a,"P",{"data-svelte-h":!0}),p(i)!=="svelte-11lpom8"&&(i.textContent=R),u=r(a),h(m.$$.fragment,a)},m(a,y){c(a,i,y),c(a,u,y),g(m,a,y),T=!0},p:me,i(a){T||(_(m.$$.fragment,a),T=!0)},o(a){v(m.$$.fragment,a),T=!1},d(a){a&&(s(i),s(u)),b(m,a)}}}function vr(k){let i,R="with the format:",u,m,T;return m=new Ut({props:{code:"JTVCQ0xTJTVEJTIwJTNDcXVlc3Rpb24lMjB0b2tlbiUyMGlkcyUzRSUyMCU1QlNFUCU1RCUyMCUzQ3RpdGxlcyUyMGlkcyUzRSUyMCU1QlNFUCU1RCUyMCUzQ3RleHRzJTIwaWRzJTNF",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>',wrap:!1}}),{c(){i=d("p"),i.textContent=R,u=o(),f(m.$$.fragment)},l(a){i=l(a,"P",{"data-svelte-h":!0}),p(i)!=="svelte-1kqkfm0"&&(i.textContent=R),u=r(a),h(m.$$.fragment,a)},m(a,y){c(a,i,y),c(a,u,y),g(m,a,y),T=!0},p:me,i(a){T||(_(m.$$.fragment,a),T=!0)},o(a){v(m.$$.fragment,a),T=!1},d(a){a&&(s(i),s(u)),b(m,a)}}}function br(k){let i,R=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=R},l(u){i=l(u,"P",{"data-svelte-h":!0}),p(i)!=="svelte-fincs2"&&(i.innerHTML=R)},m(u,m){c(u,i,m)},p:me,d(u){u&&s(i)}}}function Tr(k){let i,R="Examples:",u,m,T;return m=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUkNvbnRleHRFbmNvZGVyJTJDJTIwRFBSQ29udGV4dEVuY29kZXJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBEUFJDb250ZXh0RW5jb2RlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItY3R4X2VuY29kZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBEUFJDb250ZXh0RW5jb2Rlci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItY3R4X2VuY29kZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwaXMlMjBteSUyMGRvZyUyMGN1dGUlMjAlM0YlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQWVtYmVkZGluZ3MlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMpLnBvb2xlcl9vdXRwdXQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){i=d("p"),i.textContent=R,u=o(),f(m.$$.fragment)},l(a){i=l(a,"P",{"data-svelte-h":!0}),p(i)!=="svelte-kvfsh7"&&(i.textContent=R),u=r(a),h(m.$$.fragment,a)},m(a,y){c(a,i,y),c(a,u,y),g(m,a,y),T=!0},p:me,i(a){T||(_(m.$$.fragment,a),T=!0)},o(a){v(m.$$.fragment,a),T=!1},d(a){a&&(s(i),s(u)),b(m,a)}}}function Rr(k){let i,R=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=R},l(u){i=l(u,"P",{"data-svelte-h":!0}),p(i)!=="svelte-fincs2"&&(i.innerHTML=R)},m(u,m){c(u,i,m)},p:me,d(u){u&&s(i)}}}function yr(k){let i,R="Examples:",u,m,T;return m=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUlF1ZXN0aW9uRW5jb2RlciUyQyUyMERQUlF1ZXN0aW9uRW5jb2RlclRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMERQUlF1ZXN0aW9uRW5jb2RlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcXVlc3Rpb25fZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMERQUlF1ZXN0aW9uRW5jb2Rlci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcXVlc3Rpb25fZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMiklMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBpcyUyMG15JTIwZG9nJTIwY3V0ZSUyMCUzRiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBZW1iZWRkaW5ncyUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcykucG9vbGVyX291dHB1dA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){i=d("p"),i.textContent=R,u=o(),f(m.$$.fragment)},l(a){i=l(a,"P",{"data-svelte-h":!0}),p(i)!=="svelte-kvfsh7"&&(i.textContent=R),u=r(a),h(m.$$.fragment,a)},m(a,y){c(a,i,y),c(a,u,y),g(m,a,y),T=!0},p:me,i(a){T||(_(m.$$.fragment,a),T=!0)},o(a){v(m.$$.fragment,a),T=!1},d(a){a&&(s(i),s(u)),b(m,a)}}}function $r(k){let i,R=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=R},l(u){i=l(u,"P",{"data-svelte-h":!0}),p(i)!=="svelte-fincs2"&&(i.innerHTML=R)},m(u,m){c(u,i,m)},p:me,d(u){u&&s(i)}}}function kr(k){let i,R="Examples:",u,m,T;return m=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUlJlYWRlciUyQyUyMERQUlJlYWRlclRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMERQUlJlYWRlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcmVhZGVyLXNpbmdsZS1ucS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwRFBSUmVhZGVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRwci1yZWFkZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwcXVlc3Rpb25zJTNEJTVCJTIyV2hhdCUyMGlzJTIwbG92ZSUyMCUzRiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRpdGxlcyUzRCU1QiUyMkhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwdGV4dHMlM0QlNUIlMjInV2hhdCUyMElzJTIwTG92ZSclMjBpcyUyMGElMjBzb25nJTIwcmVjb3JkZWQlMjBieSUyMHRoZSUyMGFydGlzdCUyMEhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGVkX2lucHV0cyklMEFzdGFydF9sb2dpdHMlMjAlM0QlMjBvdXRwdXRzLnN0YXJ0X2xvZ2l0cyUwQWVuZF9sb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmVuZF9sb2dpdHMlMEFyZWxldmFuY2VfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5yZWxldmFuY2VfbG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`,wrap:!1}}),{c(){i=d("p"),i.textContent=R,u=o(),f(m.$$.fragment)},l(a){i=l(a,"P",{"data-svelte-h":!0}),p(i)!=="svelte-kvfsh7"&&(i.textContent=R),u=r(a),h(m.$$.fragment,a)},m(a,y){c(a,i,y),c(a,u,y),g(m,a,y),T=!0},p:me,i(a){T||(_(m.$$.fragment,a),T=!0)},o(a){v(m.$$.fragment,a),T=!1},d(a){a&&(s(i),s(u)),b(m,a)}}}function xr(k){let i,R,u,m,T,a="<em>This model was released on 2020-04-10 and added to Hugging Face Transformers on 2020-11-16.</em>",y,ue,Ht,ee,To='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',It,fe,Jt,he,Ro=`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&amp;A research. It was
introduced in <a href="https://huggingface.co/papers/2004.04906" rel="nofollow">Dense Passage Retrieval for Open-Domain Question Answering</a> by
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`,Qt,ge,yo="The abstract from the paper is the following:",Zt,_e,$o=`<em>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.</em>`,Wt,ve,ko='This model was contributed by <a href="https://huggingface.co/lhoestq" rel="nofollow">lhoestq</a>. The original code can be found <a href="https://github.com/facebookresearch/DPR" rel="nofollow">here</a>.',Nt,be,Bt,Te,xo="<li><p>DPR consists in three models:</p> <ul><li>Question encoder: encode questions as vectors</li> <li>Context encoder: encode contexts as vectors</li> <li>Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score (high if the inferred span actually answers the question).</li></ul></li>",St,Re,Vt,M,ye,yn,Ge,Po='<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> is the configuration class to store the configuration of a <em>DPRModel</em>.',$n,Ae,wo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRContextEncoder">DPRContextEncoder</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>, or a
<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReader">DPRReader</a>. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
<a href="https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base" rel="nofollow">facebook/dpr-ctx_encoder-single-nq-base</a>
architecture.`,kn,Ye,Do='This class is a subclass of <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a>. Please check the superclass for the documentation of all kwargs.',xn,te,Ot,$e,Xt,L,ke,Pn,Ke,Mo="Construct a DPRContextEncoder tokenizer.",wn,et,zo=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer">DPRContextEncoderTokenizer</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,Dn,tt,Co='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',Gt,xe,At,U,Pe,Mn,nt,qo="Construct a “fast” DPRContextEncoder tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",zn,ot,Eo=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast">DPRContextEncoderTokenizerFast</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece.`,Cn,rt,Fo='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',Yt,we,Kt,H,De,qn,st,jo="Constructs a DPRQuestionEncoder tokenizer.",En,at,Lo=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer">DPRQuestionEncoderTokenizer</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,Fn,it,Uo='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',en,Me,tn,I,ze,jn,dt,Ho="Constructs a “fast” DPRQuestionEncoder tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",Ln,lt,Io=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast">DPRQuestionEncoderTokenizerFast</a> is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece.`,Un,ct,Jo='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',nn,Ce,on,P,qe,Hn,pt,Qo="Construct a DPRReader tokenizer.",In,mt,Zo=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a> is almost identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> model.`,Jn,ut,Wo='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',Qn,ft,No=`Return a dictionary with the token ids of the input strings and other information to give to <code>.decode_best_spans</code>.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting <code>input_ids</code> is a matrix of size <code>(n_passages, sequence_length)</code>`,Zn,ne,rn,Ee,sn,w,Fe,Wn,ht,Bo="Constructs a “fast” DPRReader tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",Nn,gt,So=`<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReaderTokenizerFast">DPRReaderTokenizerFast</a> is almost identical to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> model.`,Bn,_t,Vo='Refer to superclass <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',Sn,vt,Oo=`Return a dictionary with the token ids of the input strings and other information to give to <code>.decode_best_spans</code>.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting <code>input_ids</code> is a matrix of size <code>(n_passages, sequence_length)</code>
with the format:`,Vn,bt,Xo="[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",an,je,dn,A,Le,On,Tt,Go='Class for outputs of <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',ln,Y,Ue,Xn,Rt,Ao='Class for outputs of <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',cn,K,He,Gn,yt,Yo='Class for outputs of <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',pn,Ie,mn,z,Je,An,$t,Ko="The bare DPRContextEncoder transformer outputting pooler outputs as context representations.",Yn,kt,er=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Kn,xt,tr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,eo,Q,Qe,to,Pt,nr='The <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRContextEncoder">DPRContextEncoder</a> forward method, overrides the <code>__call__</code> special method.',no,oe,oo,re,un,Ze,fn,C,We,ro,wt,or="The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations.",so,Dt,rr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ao,Mt,sr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,io,Z,Ne,lo,zt,ar='The <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> forward method, overrides the <code>__call__</code> special method.',co,se,po,ae,hn,Be,gn,q,Se,mo,Ct,ir="The bare DPRReader transformer outputting span predictions.",uo,qt,dr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fo,Et,lr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ho,W,Ve,go,Ft,cr='The <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> forward method, overrides the <code>__call__</code> special method.',_o,ie,vo,de,_n,Oe,vn,jt,bn;return ue=new j({props:{title:"DPR",local:"dpr",headingTag:"h1"}}),fe=new j({props:{title:"Overview",local:"overview",headingTag:"h2"}}),be=new j({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Re=new j({props:{title:"DPRConfig",local:"transformers.DPRConfig",headingTag:"h2"}}),ye=new D({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/configuration_dpr.py#L24"}}),te=new Lt({props:{anchor:"transformers.DPRConfig.example",$$slots:{default:[_r]},$$scope:{ctx:k}}}),$e=new j({props:{title:"DPRContextEncoderTokenizer",local:"transformers.DPRContextEncoderTokenizer",headingTag:"h2"}}),ke=new D({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr.py#L30"}}),xe=new j({props:{title:"DPRContextEncoderTokenizerFast",local:"transformers.DPRContextEncoderTokenizerFast",headingTag:"h2"}}),Pe=new D({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L31"}}),we=new j({props:{title:"DPRQuestionEncoderTokenizer",local:"transformers.DPRQuestionEncoderTokenizer",headingTag:"h2"}}),De=new D({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr.py#L43"}}),Me=new j({props:{title:"DPRQuestionEncoderTokenizerFast",local:"transformers.DPRQuestionEncoderTokenizerFast",headingTag:"h2"}}),ze=new D({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L45"}}),Ce=new j({props:{title:"DPRReaderTokenizer",local:"transformers.DPRReaderTokenizer",headingTag:"h2"}}),qe=new D({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr.py#L306",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>dict[str, list[list[int]]]</code></p>
`}}),ne=new Lt({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[vr]},$$scope:{ctx:k}}}),Ee=new j({props:{title:"DPRReaderTokenizerFast",local:"transformers.DPRReaderTokenizerFast",headingTag:"h2"}}),Fe=new D({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>list[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L304",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>dict[str, list[list[int]]]</code></p>
`}}),je=new j({props:{title:"DPR specific outputs",local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",headingTag:"h2"}}),Le=new D({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L48"}}),Ue=new D({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L67"}}),He=new D({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"relevance_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L86"}}),Ie=new j({props:{title:"DPRContextEncoder",local:"transformers.DPRContextEncoder",headingTag:"h2"}}),Je=new D({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L286"}}),Qe=new D({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L294",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new bo({props:{$$slots:{default:[br]},$$scope:{ctx:k}}}),re=new Lt({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[Tr]},$$scope:{ctx:k}}}),Ze=new j({props:{title:"DPRQuestionEncoder",local:"transformers.DPRQuestionEncoder",headingTag:"h2"}}),We=new D({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L391"}}),Ne=new D({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L399",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new bo({props:{$$slots:{default:[Rr]},$$scope:{ctx:k}}}),ae=new Lt({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[yr]},$$scope:{ctx:k}}}),Be=new j({props:{title:"DPRReader",local:"transformers.DPRReader",headingTag:"h2"}}),Se=new D({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L497"}}),Ve=new D({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>tuple[torch.LongTensor]</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dpr/modeling_dpr.py#L505",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) — Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new bo({props:{$$slots:{default:[$r]},$$scope:{ctx:k}}}),de=new Lt({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[kr]},$$scope:{ctx:k}}}),Oe=new gr({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dpr.md"}}),{c(){i=d("meta"),R=o(),u=d("p"),m=o(),T=d("p"),T.innerHTML=a,y=o(),f(ue.$$.fragment),Ht=o(),ee=d("div"),ee.innerHTML=To,It=o(),f(fe.$$.fragment),Jt=o(),he=d("p"),he.innerHTML=Ro,Qt=o(),ge=d("p"),ge.textContent=yo,Zt=o(),_e=d("p"),_e.innerHTML=$o,Wt=o(),ve=d("p"),ve.innerHTML=ko,Nt=o(),f(be.$$.fragment),Bt=o(),Te=d("ul"),Te.innerHTML=xo,St=o(),f(Re.$$.fragment),Vt=o(),M=d("div"),f(ye.$$.fragment),yn=o(),Ge=d("p"),Ge.innerHTML=Po,$n=o(),Ae=d("p"),Ae.innerHTML=wo,kn=o(),Ye=d("p"),Ye.innerHTML=Do,xn=o(),f(te.$$.fragment),Ot=o(),f($e.$$.fragment),Xt=o(),L=d("div"),f(ke.$$.fragment),Pn=o(),Ke=d("p"),Ke.textContent=Mo,wn=o(),et=d("p"),et.innerHTML=zo,Dn=o(),tt=d("p"),tt.innerHTML=Co,Gt=o(),f(xe.$$.fragment),At=o(),U=d("div"),f(Pe.$$.fragment),Mn=o(),nt=d("p"),nt.innerHTML=qo,zn=o(),ot=d("p"),ot.innerHTML=Eo,Cn=o(),rt=d("p"),rt.innerHTML=Fo,Yt=o(),f(we.$$.fragment),Kt=o(),H=d("div"),f(De.$$.fragment),qn=o(),st=d("p"),st.textContent=jo,En=o(),at=d("p"),at.innerHTML=Lo,Fn=o(),it=d("p"),it.innerHTML=Uo,en=o(),f(Me.$$.fragment),tn=o(),I=d("div"),f(ze.$$.fragment),jn=o(),dt=d("p"),dt.innerHTML=Ho,Ln=o(),lt=d("p"),lt.innerHTML=Io,Un=o(),ct=d("p"),ct.innerHTML=Jo,nn=o(),f(Ce.$$.fragment),on=o(),P=d("div"),f(qe.$$.fragment),Hn=o(),pt=d("p"),pt.textContent=Qo,In=o(),mt=d("p"),mt.innerHTML=Zo,Jn=o(),ut=d("p"),ut.innerHTML=Wo,Qn=o(),ft=d("p"),ft.innerHTML=No,Zn=o(),f(ne.$$.fragment),rn=o(),f(Ee.$$.fragment),sn=o(),w=d("div"),f(Fe.$$.fragment),Wn=o(),ht=d("p"),ht.innerHTML=Bo,Nn=o(),gt=d("p"),gt.innerHTML=So,Bn=o(),_t=d("p"),_t.innerHTML=Vo,Sn=o(),vt=d("p"),vt.innerHTML=Oo,Vn=o(),bt=d("p"),bt.textContent=Xo,an=o(),f(je.$$.fragment),dn=o(),A=d("div"),f(Le.$$.fragment),On=o(),Tt=d("p"),Tt.innerHTML=Go,ln=o(),Y=d("div"),f(Ue.$$.fragment),Xn=o(),Rt=d("p"),Rt.innerHTML=Ao,cn=o(),K=d("div"),f(He.$$.fragment),Gn=o(),yt=d("p"),yt.innerHTML=Yo,pn=o(),f(Ie.$$.fragment),mn=o(),z=d("div"),f(Je.$$.fragment),An=o(),$t=d("p"),$t.textContent=Ko,Yn=o(),kt=d("p"),kt.innerHTML=er,Kn=o(),xt=d("p"),xt.innerHTML=tr,eo=o(),Q=d("div"),f(Qe.$$.fragment),to=o(),Pt=d("p"),Pt.innerHTML=nr,no=o(),f(oe.$$.fragment),oo=o(),f(re.$$.fragment),un=o(),f(Ze.$$.fragment),fn=o(),C=d("div"),f(We.$$.fragment),ro=o(),wt=d("p"),wt.textContent=or,so=o(),Dt=d("p"),Dt.innerHTML=rr,ao=o(),Mt=d("p"),Mt.innerHTML=sr,io=o(),Z=d("div"),f(Ne.$$.fragment),lo=o(),zt=d("p"),zt.innerHTML=ar,co=o(),f(se.$$.fragment),po=o(),f(ae.$$.fragment),hn=o(),f(Be.$$.fragment),gn=o(),q=d("div"),f(Se.$$.fragment),mo=o(),Ct=d("p"),Ct.textContent=ir,uo=o(),qt=d("p"),qt.innerHTML=dr,fo=o(),Et=d("p"),Et.innerHTML=lr,ho=o(),W=d("div"),f(Ve.$$.fragment),go=o(),Ft=d("p"),Ft.innerHTML=cr,_o=o(),f(ie.$$.fragment),vo=o(),f(de.$$.fragment),_n=o(),f(Oe.$$.fragment),vn=o(),jt=d("p"),this.h()},l(e){const t=hr("svelte-u9bgzb",document.head);i=l(t,"META",{name:!0,content:!0}),t.forEach(s),R=r(e),u=l(e,"P",{}),x(u).forEach(s),m=r(e),T=l(e,"P",{"data-svelte-h":!0}),p(T)!=="svelte-uxq1t0"&&(T.innerHTML=a),y=r(e),h(ue.$$.fragment,e),Ht=r(e),ee=l(e,"DIV",{class:!0,"data-svelte-h":!0}),p(ee)!=="svelte-1yc98sx"&&(ee.innerHTML=To),It=r(e),h(fe.$$.fragment,e),Jt=r(e),he=l(e,"P",{"data-svelte-h":!0}),p(he)!=="svelte-nzw2up"&&(he.innerHTML=Ro),Qt=r(e),ge=l(e,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-vfdo9a"&&(ge.textContent=yo),Zt=r(e),_e=l(e,"P",{"data-svelte-h":!0}),p(_e)!=="svelte-1xle5de"&&(_e.innerHTML=$o),Wt=r(e),ve=l(e,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-21k9xm"&&(ve.innerHTML=ko),Nt=r(e),h(be.$$.fragment,e),Bt=r(e),Te=l(e,"UL",{"data-svelte-h":!0}),p(Te)!=="svelte-zqyul3"&&(Te.innerHTML=xo),St=r(e),h(Re.$$.fragment,e),Vt=r(e),M=l(e,"DIV",{class:!0});var J=x(M);h(ye.$$.fragment,J),yn=r(J),Ge=l(J,"P",{"data-svelte-h":!0}),p(Ge)!=="svelte-17djys9"&&(Ge.innerHTML=Po),$n=r(J),Ae=l(J,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-13e1jye"&&(Ae.innerHTML=wo),kn=r(J),Ye=l(J,"P",{"data-svelte-h":!0}),p(Ye)!=="svelte-1nva65h"&&(Ye.innerHTML=Do),xn=r(J),h(te.$$.fragment,J),J.forEach(s),Ot=r(e),h($e.$$.fragment,e),Xt=r(e),L=l(e,"DIV",{class:!0});var N=x(L);h(ke.$$.fragment,N),Pn=r(N),Ke=l(N,"P",{"data-svelte-h":!0}),p(Ke)!=="svelte-1dv8gby"&&(Ke.textContent=Mo),wn=r(N),et=l(N,"P",{"data-svelte-h":!0}),p(et)!=="svelte-q2iyhp"&&(et.innerHTML=zo),Dn=r(N),tt=l(N,"P",{"data-svelte-h":!0}),p(tt)!=="svelte-5rtawo"&&(tt.innerHTML=Co),N.forEach(s),Gt=r(e),h(xe.$$.fragment,e),At=r(e),U=l(e,"DIV",{class:!0});var B=x(U);h(Pe.$$.fragment,B),Mn=r(B),nt=l(B,"P",{"data-svelte-h":!0}),p(nt)!=="svelte-1flugr"&&(nt.innerHTML=qo),zn=r(B),ot=l(B,"P",{"data-svelte-h":!0}),p(ot)!=="svelte-6tif69"&&(ot.innerHTML=Eo),Cn=r(B),rt=l(B,"P",{"data-svelte-h":!0}),p(rt)!=="svelte-qq50wc"&&(rt.innerHTML=Fo),B.forEach(s),Yt=r(e),h(we.$$.fragment,e),Kt=r(e),H=l(e,"DIV",{class:!0});var S=x(H);h(De.$$.fragment,S),qn=r(S),st=l(S,"P",{"data-svelte-h":!0}),p(st)!=="svelte-1fcy66e"&&(st.textContent=jo),En=r(S),at=l(S,"P",{"data-svelte-h":!0}),p(at)!=="svelte-1eko6t1"&&(at.innerHTML=Lo),Fn=r(S),it=l(S,"P",{"data-svelte-h":!0}),p(it)!=="svelte-5rtawo"&&(it.innerHTML=Uo),S.forEach(s),en=r(e),h(Me.$$.fragment,e),tn=r(e),I=l(e,"DIV",{class:!0});var V=x(I);h(ze.$$.fragment,V),jn=r(V),dt=l(V,"P",{"data-svelte-h":!0}),p(dt)!=="svelte-137cq53"&&(dt.innerHTML=Ho),Ln=r(V),lt=l(V,"P",{"data-svelte-h":!0}),p(lt)!=="svelte-1r3wm6h"&&(lt.innerHTML=Io),Un=r(V),ct=l(V,"P",{"data-svelte-h":!0}),p(ct)!=="svelte-qq50wc"&&(ct.innerHTML=Jo),V.forEach(s),nn=r(e),h(Ce.$$.fragment,e),on=r(e),P=l(e,"DIV",{class:!0});var E=x(P);h(qe.$$.fragment,E),Hn=r(E),pt=l(E,"P",{"data-svelte-h":!0}),p(pt)!=="svelte-7on9jw"&&(pt.textContent=Qo),In=r(E),mt=l(E,"P",{"data-svelte-h":!0}),p(mt)!=="svelte-12d9mkq"&&(mt.innerHTML=Zo),Jn=r(E),ut=l(E,"P",{"data-svelte-h":!0}),p(ut)!=="svelte-5rtawo"&&(ut.innerHTML=Wo),Qn=r(E),ft=l(E,"P",{"data-svelte-h":!0}),p(ft)!=="svelte-q0w208"&&(ft.innerHTML=No),Zn=r(E),h(ne.$$.fragment,E),E.forEach(s),rn=r(e),h(Ee.$$.fragment,e),sn=r(e),w=l(e,"DIV",{class:!0});var F=x(w);h(Fe.$$.fragment,F),Wn=r(F),ht=l(F,"P",{"data-svelte-h":!0}),p(ht)!=="svelte-els1hy"&&(ht.innerHTML=Bo),Nn=r(F),gt=l(F,"P",{"data-svelte-h":!0}),p(gt)!=="svelte-im5kru"&&(gt.innerHTML=So),Bn=r(F),_t=l(F,"P",{"data-svelte-h":!0}),p(_t)!=="svelte-qq50wc"&&(_t.innerHTML=Vo),Sn=r(F),vt=l(F,"P",{"data-svelte-h":!0}),p(vt)!=="svelte-1ov4u9q"&&(vt.innerHTML=Oo),Vn=r(F),bt=l(F,"P",{"data-svelte-h":!0}),p(bt)!=="svelte-kc4w4c"&&(bt.textContent=Xo),F.forEach(s),an=r(e),h(je.$$.fragment,e),dn=r(e),A=l(e,"DIV",{class:!0});var Xe=x(A);h(Le.$$.fragment,Xe),On=r(Xe),Tt=l(Xe,"P",{"data-svelte-h":!0}),p(Tt)!=="svelte-12433jj"&&(Tt.innerHTML=Go),Xe.forEach(s),ln=r(e),Y=l(e,"DIV",{class:!0});var Tn=x(Y);h(Ue.$$.fragment,Tn),Xn=r(Tn),Rt=l(Tn,"P",{"data-svelte-h":!0}),p(Rt)!=="svelte-12433jj"&&(Rt.innerHTML=Ao),Tn.forEach(s),cn=r(e),K=l(e,"DIV",{class:!0});var Rn=x(K);h(He.$$.fragment,Rn),Gn=r(Rn),yt=l(Rn,"P",{"data-svelte-h":!0}),p(yt)!=="svelte-12433jj"&&(yt.innerHTML=Yo),Rn.forEach(s),pn=r(e),h(Ie.$$.fragment,e),mn=r(e),z=l(e,"DIV",{class:!0});var O=x(z);h(Je.$$.fragment,O),An=r(O),$t=l(O,"P",{"data-svelte-h":!0}),p($t)!=="svelte-18o6tt0"&&($t.textContent=Ko),Yn=r(O),kt=l(O,"P",{"data-svelte-h":!0}),p(kt)!=="svelte-q52n56"&&(kt.innerHTML=er),Kn=r(O),xt=l(O,"P",{"data-svelte-h":!0}),p(xt)!=="svelte-hswkmf"&&(xt.innerHTML=tr),eo=r(O),Q=l(O,"DIV",{class:!0});var le=x(Q);h(Qe.$$.fragment,le),to=r(le),Pt=l(le,"P",{"data-svelte-h":!0}),p(Pt)!=="svelte-yt7cdx"&&(Pt.innerHTML=nr),no=r(le),h(oe.$$.fragment,le),oo=r(le),h(re.$$.fragment,le),le.forEach(s),O.forEach(s),un=r(e),h(Ze.$$.fragment,e),fn=r(e),C=l(e,"DIV",{class:!0});var X=x(C);h(We.$$.fragment,X),ro=r(X),wt=l(X,"P",{"data-svelte-h":!0}),p(wt)!=="svelte-1trrfka"&&(wt.textContent=or),so=r(X),Dt=l(X,"P",{"data-svelte-h":!0}),p(Dt)!=="svelte-q52n56"&&(Dt.innerHTML=rr),ao=r(X),Mt=l(X,"P",{"data-svelte-h":!0}),p(Mt)!=="svelte-hswkmf"&&(Mt.innerHTML=sr),io=r(X),Z=l(X,"DIV",{class:!0});var ce=x(Z);h(Ne.$$.fragment,ce),lo=r(ce),zt=l(ce,"P",{"data-svelte-h":!0}),p(zt)!=="svelte-vbha0h"&&(zt.innerHTML=ar),co=r(ce),h(se.$$.fragment,ce),po=r(ce),h(ae.$$.fragment,ce),ce.forEach(s),X.forEach(s),hn=r(e),h(Be.$$.fragment,e),gn=r(e),q=l(e,"DIV",{class:!0});var G=x(q);h(Se.$$.fragment,G),mo=r(G),Ct=l(G,"P",{"data-svelte-h":!0}),p(Ct)!=="svelte-sj9qc8"&&(Ct.textContent=ir),uo=r(G),qt=l(G,"P",{"data-svelte-h":!0}),p(qt)!=="svelte-q52n56"&&(qt.innerHTML=dr),fo=r(G),Et=l(G,"P",{"data-svelte-h":!0}),p(Et)!=="svelte-hswkmf"&&(Et.innerHTML=lr),ho=r(G),W=l(G,"DIV",{class:!0});var pe=x(W);h(Ve.$$.fragment,pe),go=r(pe),Ft=l(pe,"P",{"data-svelte-h":!0}),p(Ft)!=="svelte-1c81jn5"&&(Ft.innerHTML=cr),_o=r(pe),h(ie.$$.fragment,pe),vo=r(pe),h(de.$$.fragment,pe),pe.forEach(s),G.forEach(s),_n=r(e),h(Oe.$$.fragment,e),vn=r(e),jt=l(e,"P",{}),x(jt).forEach(s),this.h()},h(){$(i,"name","hf:doc:metadata"),$(i,"content",Pr),$(ee,"class","flex flex-wrap space-x-1"),$(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){n(document.head,i),c(e,R,t),c(e,u,t),c(e,m,t),c(e,T,t),c(e,y,t),g(ue,e,t),c(e,Ht,t),c(e,ee,t),c(e,It,t),g(fe,e,t),c(e,Jt,t),c(e,he,t),c(e,Qt,t),c(e,ge,t),c(e,Zt,t),c(e,_e,t),c(e,Wt,t),c(e,ve,t),c(e,Nt,t),g(be,e,t),c(e,Bt,t),c(e,Te,t),c(e,St,t),g(Re,e,t),c(e,Vt,t),c(e,M,t),g(ye,M,null),n(M,yn),n(M,Ge),n(M,$n),n(M,Ae),n(M,kn),n(M,Ye),n(M,xn),g(te,M,null),c(e,Ot,t),g($e,e,t),c(e,Xt,t),c(e,L,t),g(ke,L,null),n(L,Pn),n(L,Ke),n(L,wn),n(L,et),n(L,Dn),n(L,tt),c(e,Gt,t),g(xe,e,t),c(e,At,t),c(e,U,t),g(Pe,U,null),n(U,Mn),n(U,nt),n(U,zn),n(U,ot),n(U,Cn),n(U,rt),c(e,Yt,t),g(we,e,t),c(e,Kt,t),c(e,H,t),g(De,H,null),n(H,qn),n(H,st),n(H,En),n(H,at),n(H,Fn),n(H,it),c(e,en,t),g(Me,e,t),c(e,tn,t),c(e,I,t),g(ze,I,null),n(I,jn),n(I,dt),n(I,Ln),n(I,lt),n(I,Un),n(I,ct),c(e,nn,t),g(Ce,e,t),c(e,on,t),c(e,P,t),g(qe,P,null),n(P,Hn),n(P,pt),n(P,In),n(P,mt),n(P,Jn),n(P,ut),n(P,Qn),n(P,ft),n(P,Zn),g(ne,P,null),c(e,rn,t),g(Ee,e,t),c(e,sn,t),c(e,w,t),g(Fe,w,null),n(w,Wn),n(w,ht),n(w,Nn),n(w,gt),n(w,Bn),n(w,_t),n(w,Sn),n(w,vt),n(w,Vn),n(w,bt),c(e,an,t),g(je,e,t),c(e,dn,t),c(e,A,t),g(Le,A,null),n(A,On),n(A,Tt),c(e,ln,t),c(e,Y,t),g(Ue,Y,null),n(Y,Xn),n(Y,Rt),c(e,cn,t),c(e,K,t),g(He,K,null),n(K,Gn),n(K,yt),c(e,pn,t),g(Ie,e,t),c(e,mn,t),c(e,z,t),g(Je,z,null),n(z,An),n(z,$t),n(z,Yn),n(z,kt),n(z,Kn),n(z,xt),n(z,eo),n(z,Q),g(Qe,Q,null),n(Q,to),n(Q,Pt),n(Q,no),g(oe,Q,null),n(Q,oo),g(re,Q,null),c(e,un,t),g(Ze,e,t),c(e,fn,t),c(e,C,t),g(We,C,null),n(C,ro),n(C,wt),n(C,so),n(C,Dt),n(C,ao),n(C,Mt),n(C,io),n(C,Z),g(Ne,Z,null),n(Z,lo),n(Z,zt),n(Z,co),g(se,Z,null),n(Z,po),g(ae,Z,null),c(e,hn,t),g(Be,e,t),c(e,gn,t),c(e,q,t),g(Se,q,null),n(q,mo),n(q,Ct),n(q,uo),n(q,qt),n(q,fo),n(q,Et),n(q,ho),n(q,W),g(Ve,W,null),n(W,go),n(W,Ft),n(W,_o),g(ie,W,null),n(W,vo),g(de,W,null),c(e,_n,t),g(Oe,e,t),c(e,vn,t),c(e,jt,t),bn=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),te.$set(J);const N={};t&2&&(N.$$scope={dirty:t,ctx:e}),ne.$set(N);const B={};t&2&&(B.$$scope={dirty:t,ctx:e}),oe.$set(B);const S={};t&2&&(S.$$scope={dirty:t,ctx:e}),re.$set(S);const V={};t&2&&(V.$$scope={dirty:t,ctx:e}),se.$set(V);const E={};t&2&&(E.$$scope={dirty:t,ctx:e}),ae.$set(E);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),ie.$set(F);const Xe={};t&2&&(Xe.$$scope={dirty:t,ctx:e}),de.$set(Xe)},i(e){bn||(_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(be.$$.fragment,e),_(Re.$$.fragment,e),_(ye.$$.fragment,e),_(te.$$.fragment,e),_($e.$$.fragment,e),_(ke.$$.fragment,e),_(xe.$$.fragment,e),_(Pe.$$.fragment,e),_(we.$$.fragment,e),_(De.$$.fragment,e),_(Me.$$.fragment,e),_(ze.$$.fragment,e),_(Ce.$$.fragment,e),_(qe.$$.fragment,e),_(ne.$$.fragment,e),_(Ee.$$.fragment,e),_(Fe.$$.fragment,e),_(je.$$.fragment,e),_(Le.$$.fragment,e),_(Ue.$$.fragment,e),_(He.$$.fragment,e),_(Ie.$$.fragment,e),_(Je.$$.fragment,e),_(Qe.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(Ze.$$.fragment,e),_(We.$$.fragment,e),_(Ne.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(Be.$$.fragment,e),_(Se.$$.fragment,e),_(Ve.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(Oe.$$.fragment,e),bn=!0)},o(e){v(ue.$$.fragment,e),v(fe.$$.fragment,e),v(be.$$.fragment,e),v(Re.$$.fragment,e),v(ye.$$.fragment,e),v(te.$$.fragment,e),v($e.$$.fragment,e),v(ke.$$.fragment,e),v(xe.$$.fragment,e),v(Pe.$$.fragment,e),v(we.$$.fragment,e),v(De.$$.fragment,e),v(Me.$$.fragment,e),v(ze.$$.fragment,e),v(Ce.$$.fragment,e),v(qe.$$.fragment,e),v(ne.$$.fragment,e),v(Ee.$$.fragment,e),v(Fe.$$.fragment,e),v(je.$$.fragment,e),v(Le.$$.fragment,e),v(Ue.$$.fragment,e),v(He.$$.fragment,e),v(Ie.$$.fragment,e),v(Je.$$.fragment,e),v(Qe.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(Ze.$$.fragment,e),v(We.$$.fragment,e),v(Ne.$$.fragment,e),v(se.$$.fragment,e),v(ae.$$.fragment,e),v(Be.$$.fragment,e),v(Se.$$.fragment,e),v(Ve.$$.fragment,e),v(ie.$$.fragment,e),v(de.$$.fragment,e),v(Oe.$$.fragment,e),bn=!1},d(e){e&&(s(R),s(u),s(m),s(T),s(y),s(Ht),s(ee),s(It),s(Jt),s(he),s(Qt),s(ge),s(Zt),s(_e),s(Wt),s(ve),s(Nt),s(Bt),s(Te),s(St),s(Vt),s(M),s(Ot),s(Xt),s(L),s(Gt),s(At),s(U),s(Yt),s(Kt),s(H),s(en),s(tn),s(I),s(nn),s(on),s(P),s(rn),s(sn),s(w),s(an),s(dn),s(A),s(ln),s(Y),s(cn),s(K),s(pn),s(mn),s(z),s(un),s(fn),s(C),s(hn),s(gn),s(q),s(_n),s(vn),s(jt)),s(i),b(ue,e),b(fe,e),b(be,e),b(Re,e),b(ye),b(te),b($e,e),b(ke),b(xe,e),b(Pe),b(we,e),b(De),b(Me,e),b(ze),b(Ce,e),b(qe),b(ne),b(Ee,e),b(Fe),b(je,e),b(Le),b(Ue),b(He),b(Ie,e),b(Je),b(Qe),b(oe),b(re),b(Ze,e),b(We),b(Ne),b(se),b(ae),b(Be,e),b(Se),b(Ve),b(ie),b(de),b(Oe,e)}}}const Pr='{"title":"DPR","local":"dpr","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"DPRConfig","local":"transformers.DPRConfig","sections":[],"depth":2},{"title":"DPRContextEncoderTokenizer","local":"transformers.DPRContextEncoderTokenizer","sections":[],"depth":2},{"title":"DPRContextEncoderTokenizerFast","local":"transformers.DPRContextEncoderTokenizerFast","sections":[],"depth":2},{"title":"DPRQuestionEncoderTokenizer","local":"transformers.DPRQuestionEncoderTokenizer","sections":[],"depth":2},{"title":"DPRQuestionEncoderTokenizerFast","local":"transformers.DPRQuestionEncoderTokenizerFast","sections":[],"depth":2},{"title":"DPRReaderTokenizer","local":"transformers.DPRReaderTokenizer","sections":[],"depth":2},{"title":"DPRReaderTokenizerFast","local":"transformers.DPRReaderTokenizerFast","sections":[],"depth":2},{"title":"DPR specific outputs","local":"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput","sections":[],"depth":2},{"title":"DPRContextEncoder","local":"transformers.DPRContextEncoder","sections":[],"depth":2},{"title":"DPRQuestionEncoder","local":"transformers.DPRQuestionEncoder","sections":[],"depth":2},{"title":"DPRReader","local":"transformers.DPRReader","sections":[],"depth":2}],"depth":1}';function wr(k){return mr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jr extends ur{constructor(i){super(),fr(this,i,wr,xr,pr,{})}}export{jr as component};
