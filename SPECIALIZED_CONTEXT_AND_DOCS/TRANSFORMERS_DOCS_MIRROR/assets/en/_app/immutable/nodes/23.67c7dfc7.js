import{s as cn,o as un,n as H}from"../chunks/scheduler.18a86fab.js";import{S as fn,i as Tn,g as y,s as a,r as T,A as hn,h as c,f as t,c as r,j as pn,u as h,x as u,k as mn,y as dn,a as n,v as d,d as b,t as J,w as k}from"../chunks/index.98837b22.js";import{T as ss}from"../chunks/Tip.77304350.js";import{Y as Mn}from"../chunks/Youtube.14fb207c.js";import{C as g}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as B,E as bn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as yn,a as ts}from"../chunks/HfOption.6641485e.js";function Jn(z){let l,j='Learn about the most popular tokenization algorithms on the <a href="./tokenizer_summary">Summary of the tokenizers</a> doc.';return{c(){l=y("p"),l.innerHTML=j},l(i){l=c(i,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1w1scvh"&&(l.innerHTML=j)},m(i,p){n(i,l,p)},p:H,d(i){i&&t(l)}}}function kn(z){let l,j='The <a href="./model_doc/auto">AutoClass</a> API is a fast and easy way to load a tokenizer without needing to know whether a Python or Rust-based implementation is available. By default, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> tries to load a fast tokenizer if it‚Äôs available, otherwise, it loads the Python implementation.',i,p,M='Use <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a> to load a tokenizer.',o,m,w,U,I='Load your own tokenizer by passing its vocabulary file to <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained">from_pretrained()</a>.',C,Z,x;return m=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZnZW1tYS0yLTJiJTIyKSUwQXRva2VuaXplciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnkuJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElN0InaW5wdXRfaWRzJyUzQSUyMHRlbnNvciglNUIlNUIlMjAlMjAlMjAlMjAlMjAyJTJDJTIwJTIwJTIwMTczNCUyQyUyMCUyMCUyMCUyMDcwOCUyQyUyMCUyMCUyMDE1MDglMkMlMjAlMjAlMjA0OTE1JTJDJTIwJTIwJTIwJTIwNTc3JTJDJTIwJTIwJTIwMTUwMCUyQyUyMCUyMCUyMCUyMDY5MiUyQyUyMCUyMCUyMCUyMDU3MyUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDE1NjgwOCUyQyUyMDEyODE0OSUyQyUyMCUyMCUyMDk1ODElMkMlMjAyMzUyNjUlNUQlNUQpJTJDJTBBJTIwJ2F0dGVudGlvbl9tYXNrJyUzQSUyMHRlbnNvciglNUIlNUIxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTVEKSUwQSU3RA==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)
tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[     <span class="hljs-number">2</span>,   <span class="hljs-number">1734</span>,    <span class="hljs-number">708</span>,   <span class="hljs-number">1508</span>,   <span class="hljs-number">4915</span>,    <span class="hljs-number">577</span>,   <span class="hljs-number">1500</span>,    <span class="hljs-number">692</span>,    <span class="hljs-number">573</span>,
         <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>,   <span class="hljs-number">9581</span>, <span class="hljs-number">235265</span>]]),
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
}`,wrap:!1}}),Z=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjIuJTJGbW9kZWxfZGlyZWN0b3J5JTJGbXlfdm9jYWJfZmlsZS50eHQlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./model_directory/my_vocab_file.txt&quot;</span>)`,wrap:!1}}),{c(){l=y("p"),l.innerHTML=j,i=a(),p=y("p"),p.innerHTML=M,o=a(),T(m.$$.fragment),w=a(),U=y("p"),U.innerHTML=I,C=a(),T(Z.$$.fragment)},l($){l=c($,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1sxqesa"&&(l.innerHTML=j),i=r($),p=c($,"P",{"data-svelte-h":!0}),u(p)!=="svelte-1m5xik6"&&(p.innerHTML=M),o=r($),h(m.$$.fragment,$),w=r($),U=c($,"P",{"data-svelte-h":!0}),u(U)!=="svelte-9yho0e"&&(U.innerHTML=I),C=r($),h(Z.$$.fragment,$)},m($,v){n($,l,v),n($,i,v),n($,p,v),n($,o,v),d(m,$,v),n($,w,v),n($,U,v),n($,C,v),d(Z,$,v),x=!0},p:H,i($){x||(b(m.$$.fragment,$),b(Z.$$.fragment,$),x=!0)},o($){J(m.$$.fragment,$),J(Z.$$.fragment,$),x=!1},d($){$&&(t(l),t(i),t(p),t(o),t(w),t(U),t(C)),k(m,$),k(Z,$)}}}function jn(z){let l,j="Refer to a models API documentation to check whether a fast tokenizer is supported.";return{c(){l=y("p"),l.textContent=j},l(i){l=c(i,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1ea3kbc"&&(l.textContent=j)},m(i,p){n(i,l,p)},p:H,d(i){i&&t(l)}}}function Un(z){let l,j="Each pretrained model is associated with a tokenizer and the specific vocabulary it was trained on. A tokenizer can be loaded directly from the model-specific class.",i,p,M,o,m,w,U="To load a fast tokenizer, use the fast implementation class.",I,C,Z,x,$="Load your own tokenizer by passing its vocabulary file to the <code>vocab_file</code> parameter.",v,Q,V;return p=new ss({props:{warning:!1,$$slots:{default:[jn]},$$scope:{ctx:z}}}),o=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdlbW1hVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwR2VtbWFUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmdlbW1hLTItMmIlMjIpJTBBdG9rZW5pemVyKCUyMldlJTIwYXJlJTIwdmVyeSUyMGhhcHB5JTIwdG8lMjBzaG93JTIweW91JTIwdGhlJTIwJUYwJTlGJUE0JTk3JTIwVHJhbnNmb3JtZXJzJTIwbGlicmFyeS4lMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GemmaTokenizer

tokenizer = GemmaTokenizer.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)
tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),C=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdlbW1hVG9rZW5pemVyRmFzdCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEdlbW1hVG9rZW5pemVyRmFzdC5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMi0yYiUyMiklMEF0b2tlbml6ZXIoJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GemmaTokenizerFast

tokenizer = GemmaTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)
tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),Q=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdlbW1hVG9rZW5pemVyRmFzdCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEdlbW1hVG9rZW5pemVyRmFzdCh2b2NhYl9maWxlJTNEJTIybXlfdm9jYWJfZmlsZS50eHQlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GemmaTokenizerFast

tokenizer = GemmaTokenizerFast(vocab_file=<span class="hljs-string">&quot;my_vocab_file.txt&quot;</span>)`,wrap:!1}}),{c(){l=y("p"),l.textContent=j,i=a(),T(p.$$.fragment),M=a(),T(o.$$.fragment),m=a(),w=y("p"),w.textContent=U,I=a(),T(C.$$.fragment),Z=a(),x=y("p"),x.innerHTML=$,v=a(),T(Q.$$.fragment)},l(f){l=c(f,"P",{"data-svelte-h":!0}),u(l)!=="svelte-whttf7"&&(l.textContent=j),i=r(f),h(p.$$.fragment,f),M=r(f),h(o.$$.fragment,f),m=r(f),w=c(f,"P",{"data-svelte-h":!0}),u(w)!=="svelte-ibk39z"&&(w.textContent=U),I=r(f),h(C.$$.fragment,f),Z=r(f),x=c(f,"P",{"data-svelte-h":!0}),u(x)!=="svelte-a07nyw"&&(x.innerHTML=$),v=r(f),h(Q.$$.fragment,f)},m(f,_){n(f,l,_),n(f,i,_),d(p,f,_),n(f,M,_),d(o,f,_),n(f,m,_),n(f,w,_),n(f,I,_),d(C,f,_),n(f,Z,_),n(f,x,_),n(f,v,_),d(Q,f,_),V=!0},p(f,_){const W={};_&2&&(W.$$scope={dirty:_,ctx:f}),p.$set(W)},i(f){V||(b(p.$$.fragment,f),b(o.$$.fragment,f),b(C.$$.fragment,f),b(Q.$$.fragment,f),V=!0)},o(f){J(p.$$.fragment,f),J(o.$$.fragment,f),J(C.$$.fragment,f),J(Q.$$.fragment,f),V=!1},d(f){f&&(t(l),t(i),t(M),t(m),t(w),t(I),t(Z),t(x),t(v)),k(p,f),k(o,f),k(C,f),k(Q,f)}}}function $n(z){let l,j,i,p;return l=new ts({props:{id:"tokenizer-classes",option:"AutoTokenizer",$$slots:{default:[kn]},$$scope:{ctx:z}}}),i=new ts({props:{id:"tokenizer-classes",option:"model-specific tokenizer",$$slots:{default:[Un]},$$scope:{ctx:z}}}),{c(){T(l.$$.fragment),j=a(),T(i.$$.fragment)},l(M){h(l.$$.fragment,M),j=r(M),h(i.$$.fragment,M)},m(M,o){d(l,M,o),n(M,j,o),d(i,M,o),p=!0},p(M,o){const m={};o&2&&(m.$$scope={dirty:o,ctx:M}),l.$set(m);const w={};o&2&&(w.$$scope={dirty:o,ctx:M}),i.$set(w)},i(M){p||(b(l.$$.fragment,M),b(i.$$.fragment,M),p=!0)},o(M){J(l.$$.fragment,M),J(i.$$.fragment,M),p=!1},d(M){M&&t(j),k(l,M),k(i,M)}}}function wn(z){let l,j='In the first step, a string of text is split into tokens by the <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.tokenize">tokenize()</a> function. How the text is split depends on the tokenization algorithm.',i,p,M,o,m='Gemma uses a <a href="./tokenizer_summary#sentencepiece">SentencePiece</a> tokenizer which replaces spaces with an underscore <code>_</code>.',w;return p=new g({props:{code:"dG9rZW5zJTIwJTNEJTIwdG9rZW5pemVyLnRva2VuaXplKCUyMldlJTIwYXJlJTIwdmVyeSUyMGhhcHB5JTIwdG8lMjBzaG93JTIweW91JTIwdGhlJTIwJUYwJTlGJUE0JTk3JTIwVHJhbnNmb3JtZXJzJTIwbGlicmFyeSUyMiklMEFwcmludCh0b2tlbnMpJTBBJTVCJ1dlJyUyQyUyMCclRTIlOTYlODFhcmUnJTJDJTIwJyVFMiU5NiU4MXZlcnknJTJDJTIwJyVFMiU5NiU4MWhhcHB5JyUyQyUyMCclRTIlOTYlODF0byclMkMlMjAnJUUyJTk2JTgxc2hvdyclMkMlMjAnJUUyJTk2JTgxeW91JyUyQyUyMCclRTIlOTYlODF0aGUnJTJDJTIwJyVFMiU5NiU4MSVGMCU5RiVBNCU5NyclMkMlMjAnJUUyJTk2JTgxVHJhbnNmb3JtZXJzJyUyQyUyMCclRTIlOTYlODFsaWJyYXJ5JyU1RA==",highlighted:`tokens = tokenizer.tokenize(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library&quot;</span>)
<span class="hljs-built_in">print</span>(tokens)
[<span class="hljs-string">&#x27;We&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅare&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅvery&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅhappy&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅto&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅshow&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅyou&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅthe&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅü§ó&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅTransformers&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅlibrary&#x27;</span>]`,wrap:!1}}),{c(){l=y("p"),l.innerHTML=j,i=a(),T(p.$$.fragment),M=a(),o=y("p"),o.innerHTML=m},l(U){l=c(U,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1bk55nd"&&(l.innerHTML=j),i=r(U),h(p.$$.fragment,U),M=r(U),o=c(U,"P",{"data-svelte-h":!0}),u(o)!=="svelte-mcscva"&&(o.innerHTML=m)},m(U,I){n(U,l,I),n(U,i,I),d(p,U,I),n(U,M,I),n(U,o,I),w=!0},p:H,i(U){w||(b(p.$$.fragment,U),w=!0)},o(U){J(p.$$.fragment,U),w=!1},d(U){U&&(t(l),t(i),t(M),t(o)),k(p,U)}}}function zn(z){let l,j='In the second step, the tokens are converted into ids with <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids">convert_tokens_to_ids()</a>.',i,p,M;return p=new g({props:{code:"aWRzJTIwJTNEJTIwdG9rZW5pemVyLmNvbnZlcnRfdG9rZW5zX3RvX2lkcyh0b2tlbnMpJTBBcHJpbnQoaWRzKSUwQSU1QjE3MzQlMkMlMjA3MDglMkMlMjAxNTA4JTJDJTIwNDkxNSUyQyUyMDU3NyUyQyUyMDE1MDAlMkMlMjA2OTIlMkMlMjA1NzMlMkMlMjAxNTY4MDglMkMlMjAxMjgxNDklMkMlMjA5NTgxJTVE",highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)
<span class="hljs-built_in">print</span>(ids)
[<span class="hljs-number">1734</span>, <span class="hljs-number">708</span>, <span class="hljs-number">1508</span>, <span class="hljs-number">4915</span>, <span class="hljs-number">577</span>, <span class="hljs-number">1500</span>, <span class="hljs-number">692</span>, <span class="hljs-number">573</span>, <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>, <span class="hljs-number">9581</span>]`,wrap:!1}}),{c(){l=y("p"),l.innerHTML=j,i=a(),T(p.$$.fragment)},l(o){l=c(o,"P",{"data-svelte-h":!0}),u(l)!=="svelte-s8jf2h"&&(l.innerHTML=j),i=r(o),h(p.$$.fragment,o)},m(o,m){n(o,l,m),n(o,i,m),d(p,o,m),M=!0},p:H,i(o){M||(b(p.$$.fragment,o),M=!0)},o(o){J(p.$$.fragment,o),M=!1},d(o){o&&(t(l),t(i)),k(p,o)}}}function In(z){let l,j='Lastly, the model prediction typically generates numerical outputs which are converted back to text with <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode">decode()</a>.',i,p,M;return p=new g({props:{code:"ZGVjb2RlZF9zdHJpbmclMjAlM0QlMjB0b2tlbml6ZXIuZGVjb2RlKGlkcyklMEFwcmludChkZWNvZGVkX3N0cmluZyklMEEnV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5Jw==",highlighted:`decoded_string = tokenizer.decode(ids)
<span class="hljs-built_in">print</span>(decoded_string)
<span class="hljs-string">&#x27;We are very happy to show you the ü§ó Transformers library&#x27;</span>`,wrap:!1}}),{c(){l=y("p"),l.innerHTML=j,i=a(),T(p.$$.fragment)},l(o){l=c(o,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1r3kdvt"&&(l.innerHTML=j),i=r(o),h(p.$$.fragment,o)},m(o,m){n(o,l,m),n(o,i,m),d(p,o,m),M=!0},p:H,i(o){M||(b(p.$$.fragment,o),M=!0)},o(o){J(p.$$.fragment,o),M=!1},d(o){o&&(t(l),t(i)),k(p,o)}}}function gn(z){let l,j,i,p,M,o;return l=new ts({props:{id:"steps",option:"1. tokenize",$$slots:{default:[wn]},$$scope:{ctx:z}}}),i=new ts({props:{id:"steps",option:"2. convert tokens to ids",$$slots:{default:[zn]},$$scope:{ctx:z}}}),M=new ts({props:{id:"steps",option:"3. decode ids to text",$$slots:{default:[In]},$$scope:{ctx:z}}}),{c(){T(l.$$.fragment),j=a(),T(i.$$.fragment),p=a(),T(M.$$.fragment)},l(m){h(l.$$.fragment,m),j=r(m),h(i.$$.fragment,m),p=r(m),h(M.$$.fragment,m)},m(m,w){d(l,m,w),n(m,j,w),d(i,m,w),n(m,p,w),d(M,m,w),o=!0},p(m,w){const U={};w&2&&(U.$$scope={dirty:w,ctx:m}),l.$set(U);const I={};w&2&&(I.$$scope={dirty:w,ctx:m}),i.$set(I);const C={};w&2&&(C.$$scope={dirty:w,ctx:m}),M.$set(C)},i(m){o||(b(l.$$.fragment,m),b(i.$$.fragment,m),b(M.$$.fragment,m),o=!0)},o(m){J(l.$$.fragment,m),J(i.$$.fragment,m),J(M.$$.fragment,m),o=!1},d(m){m&&(t(j),t(p)),k(l,m),k(i,m),k(M,m)}}}function vn(z){let l,j='Visualize how different tokenizers work in the <a href="https://xenova-the-tokenizer-playground.static.hf.space" rel="nofollow">Tokenizer Playground</a>.';return{c(){l=y("p"),l.innerHTML=j},l(i){l=c(i,"P",{"data-svelte-h":!0}),u(l)!=="svelte-orqdo9"&&(l.innerHTML=j)},m(i,p){n(i,l,p)},p:H,d(i){i&&t(l)}}}function _n(z){let l,j='Learn about additional padding strategies in the <a href="./pad_truncation">Padding and truncation</a> guide.';return{c(){l=y("p"),l.innerHTML=j},l(i){l=c(i,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1aj38k"&&(l.innerHTML=j)},m(i,p){n(i,l,p)},p:H,d(i){i&&t(l)}}}function Cn(z){let l,j='Learn about additional truncation strategies in the <a href="./pad_truncation">Padding and truncation</a> guide.';return{c(){l=y("p"),l.innerHTML=j},l(i){l=c(i,"P",{"data-svelte-h":!0}),u(l)!=="svelte-1g4p536"&&(l.innerHTML=j)},m(i,p){n(i,l,p)},p:H,d(i){i&&t(l)}}}function Zn(z){let l,j,i,p,M,o,m,w="Tokenizers convert text into an array of numbers known as tensors, the inputs to a text model. There are several tokenizer algorithms, but they all share the same purpose. Split text into smaller words or subwords (tokens) according to some rules, and convert them into numbers (input ids). A Transformers tokenizer also returns an attention mask to indicate which tokens should be attended to.",U,I,C,Z,x='Call <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a> to load a tokenizer and its configuration from the Hugging Face <a href="https://hf.co" rel="nofollow">Hub</a> or a local directory. The pretrained tokenizer is saved in a <a href="https://huggingface.co/google/gemma-2-2b/blob/main/tokenizer.model" rel="nofollow">tokenizer.model</a> file with all its associated vocabulary files.',$,v,Q="Pass a string of text to the tokenizer to return the input ids and attention mask, and set the framework tensor type to return with the <code>return_tensors</code> parameter.",V,f,_,W,Jt="Whichever tokenizer you use, make sure the tokenizer vocabulary is the same as the pretrained models tokenizer vocabulary. This is especially important if you‚Äôre using a custom tokenizer with a different vocabulary from the pretrained models tokenizer.",ns,D,kt="This guide provides a brief overview of the tokenizer classes and how to preprocess text with it.",ls,X,as,F,jt='All tokenizers inherit from a <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase">PreTrainedTokenizerBase</a> class that provides common methods for all tokenizers like <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode">batch_decode()</a>. There are two main tokenizer classes that build on top of the base class.',rs,L,Ut='<li><a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> is a Python implementation, for example <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a>.</li> <li><a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> is a fast Rust-based implementation from the <a href="https://hf.co/docs/tokenizers/index" rel="nofollow">Tokenizers</a> library, for example <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a>.</li>',is,S,$t='There are two ways you can load a tokenizer, with <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> or a model-specific tokenizer.',os,N,ps,P,ms,q,wt="In addition to text tokens, multimodal tokenizers also holds tokens from other modalities as a part of its attributes for easy access.",Ms,Y,zt='To add these special tokens to a tokenizer, pass them as a dictionary to the <code>extra_special_tokens</code> parameter in <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained">from_pretrained()</a>. The example below adds the <code>image_token</code> to a vision-language model.',ys,O,It="Save the tokenizer so you can reuse it with direct access to the <code>image_token</code>, <code>boi_token</code>, and <code>eoi_token</code>.",cs,K,us,ee,fs,se,Ts,te,gt='<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> or <em>fast tokenizers</em> are Rust-based tokenizers from the <a href="https://hf.co/docs/tokenizers" rel="nofollow">Tokenizers</a> library. It is significantly faster at batched tokenization and provides additional alignment methods compared to the Python-based tokenizers.',hs,ne,vt='<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> automatically loads a fast tokenizer if it‚Äôs supported. Otherwise, you need to explicitly load the fast tokenizer.',ds,le,_t="This section will show you how to train a fast tokenizer and reuse it in Transformers.",bs,ae,Ct="To train a Byte-Pair Encoding (BPE) tokenizer, create a <code>Tokenizer</code> and <code>BpeTrainer</code> class and define the unknown token and special tokens.",Js,re,ks,ie,Zt="Split the tokens on <code>Whitespace</code> to create tokens that don‚Äôt overlap with each other.",js,oe,Us,pe,xt="Call <code>train</code> on the text files and trainer to start training.",$s,me,ws,Me,Bt="Use <code>save</code> to save the tokenizers configuration and vocabulary to a JSON file.",zs,ye,Is,ce,Qt='Now you can load and reuse the tokenizer object in Transformers by passing it to the <code>tokenizer_object</code> parameter in <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>.',gs,ue,vs,fe,Wt='To load a saved tokenizer from its JSON file, pass the file path to the <code>tokenizer_file</code> parameter in <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>.',_s,Te,Cs,he,Zs,de,Vt='<a href="https://github.com/openai/tiktoken" rel="nofollow">tiktoken</a> is a <a href="./tokenizer_summary#byte-pair-encoding-bpe">byte-pair encoding (BPE)</a> tokenizer by OpenAI. It includes several tokenization schemes or encodings for how text should be tokenized.',xs,be,Ht='There are currently two models trained and released with tiktoken, GPT2 and Llama3. Transformers supports models with a <a href="https://hf.co/meta-llama/Meta-Llama-3-8B/blob/main/original/tokenizer.model" rel="nofollow">tokenizer.model</a> tiktoken file. The tiktoken file is automatically converted into Transformers Rust-based <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>.',Bs,Je,Nt='Add the <code>subfolder</code> parameter to <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> to specify where the <code>tokenizer.model</code> tiktoken file is located.',Qs,ke,Ws,je,Vs,Ue,At='The tiktoken <code>tokenizer.model</code> file contains no information about additional tokens or pattern strings. If these are important, convert the tokenizer to <code>tokenizer.json</code> (the appropriate format for <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>).',Hs,$e,Gt='Generate the tiktoken <code>tokenizer.model</code> file with the <a href="https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63" rel="nofollow">tiktoken.get_encoding</a> function, and convert it to <code>tokenizer.json</code> with <a href="https://github.com/huggingface/transformers/blob/99e0ab6ed888136ea4877c6d8ab03690a1478363/src/transformers/integrations/tiktoken.py#L8" rel="nofollow">convert_tiktoken_to_fast</a>.',Ns,we,As,ze,Rt='The resulting <code>tokenizer.json</code> file is saved to the specified directory and loaded with <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a>.',Gs,Ie,Rs,ge,Es,ve,Ds,_e,Et="A Transformers model expects the input to be a PyTorch or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the <code>return_tensors</code> parameter.",Xs,Ce,Fs,Ze,Dt="The tokenization process of converting text into input ids is completed in two steps.",Ls,A,Ss,G,Ps,xe,qs,Be,Xt="Special tokens provide the model with some additional information about the text.",Ys,Qe,Ft='For example, if you compare the tokens obtained from passing text directly to the tokenizer and from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_tokens_to_ids">convert_tokens_to_ids()</a>, you‚Äôll notice some additional tokens are added.',Os,We,Ks,Ve,Lt='When you <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode">decode()</a> the ids, you‚Äôll see <code>&lt;bos&gt;</code> at the beginning of the string. This is used to indicate the beginning of a sentence to the model.',et,He,st,Ne,St="Not all models need special tokens, but if they do, a tokenizer automatically adds them.",tt,Ae,nt,Ge,Pt="It is faster and more efficient to preprocess <em>batches</em> of text instead of a single sentence at a time. Fast tokenizers are especially good at parallelizing tokenization.",lt,Re,qt="Pass a list of string text to the tokenizer.",at,Ee,rt,De,it,R,ot,Xe,Yt="In the output above, the <code>input_ids</code> have different lengths. This is an issue because Transformers expects them to have the same lengths so it can pack them into a batch. Sequences with uneven lengths can‚Äôt be batched.",pt,Fe,Ot="Padding adds a special <em>padding token</em> to ensure all sequences have the same length. Set <code>padding=True</code> to pad the sequences to the longest sequence length in the batch.",mt,Le,Mt,Se,Kt="The tokenizer added the special padding token <code>0</code> to the left side (<em>left padding</em>) because Gemma and LLMs in general are not trained to continue generation from a padding token.",yt,Pe,ct,E,ut,qe,en="Models are only able to process sequences up to a certain length. If you try to process a sequence longer than a model can handle, it crashes.",ft,Ye,sn="Truncation removes tokens from a sequence to ensure it doesn‚Äôt exceed the maximum length. Set <code>truncation=True</code> to truncate a sequence to the maximum length accepted by the model. You can also set the maximum length yourself with the <code>max_length</code> parameter.",Tt,Oe,ht,Ke,dt,es,bt;return M=new B({props:{title:"Tokenizers",local:"tokenizers",headingTag:"h1"}}),I=new ss({props:{warning:!1,$$slots:{default:[Jn]},$$scope:{ctx:z}}}),f=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZnZW1tYS0yLTJiJTIyKSUwQXRva2VuaXplciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnklMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSU3QidpbnB1dF9pZHMnJTNBJTIwdGVuc29yKCU1QiU1QiUyMCUyMCUyMCUyMCUyMDIlMkMlMjAlMjAlMjAxNzM0JTJDJTIwJTIwJTIwJTIwNzA4JTJDJTIwJTIwJTIwMTUwOCUyQyUyMCUyMCUyMDQ5MTUlMkMlMjAlMjAlMjAlMjA1NzclMkMlMjAlMjAlMjAxNTAwJTJDJTIwJTIwJTIwJTIwNjkyJTJDJTIwJTIwJTIwJTIwNTczJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwMTU2ODA4JTJDJTIwMTI4MTQ5JTJDJTIwJTIwJTIwOTU4MSUyQyUyMDIzNTI2NSU1RCU1RCklMkMlMEElMjAnYXR0ZW50aW9uX21hc2snJTNBJTIwdGVuc29yKCU1QiU1QjElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElNUQlNUQpJTBBJTdE",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)
tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[     <span class="hljs-number">2</span>,   <span class="hljs-number">1734</span>,    <span class="hljs-number">708</span>,   <span class="hljs-number">1508</span>,   <span class="hljs-number">4915</span>,    <span class="hljs-number">577</span>,   <span class="hljs-number">1500</span>,    <span class="hljs-number">692</span>,    <span class="hljs-number">573</span>,
         <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>,   <span class="hljs-number">9581</span>, <span class="hljs-number">235265</span>]]),
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
}`,wrap:!1}}),X=new B({props:{title:"Tokenizer classes",local:"tokenizer-classes",headingTag:"h2"}}),N=new yn({props:{id:"tokenizer-classes",options:["AutoTokenizer","model-specific tokenizer"],$$slots:{default:[$n]},$$scope:{ctx:z}}}),P=new B({props:{title:"Multimodal tokenizers",local:"multimodal-tokenizers",headingTag:"h2"}}),K=new g({props:{code:"dmlzaW9uX3Rva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmxsYXZhLWhmJTJGbGxhdmEtMS41LTdiLWhmJTIyJTJDJTBBJTIwJTIwJTIwJTIwZXh0cmFfc3BlY2lhbF90b2tlbnMlM0QlN0IlMjJpbWFnZV90b2tlbiUyMiUzQSUyMCUyMiUzQ2ltYWdlJTNFJTIyJTJDJTIwJTIyYm9pX3Rva2VuJTIyJTNBJTIwJTIyJTNDaW1hZ2Vfc3RhcnQlM0UlMjIlMkMlMjAlMjJlb2lfdG9rZW4lMjIlM0ElMjAlMjIlM0NpbWFnZV9lbmQlM0UlMjIlN0QlMEEpJTBBcHJpbnQodmlzaW9uX3Rva2VuaXplci5pbWFnZV90b2tlbiUyQyUyMHZpc2lvbl90b2tlbml6ZXIuaW1hZ2VfdG9rZW5faWQpJTBBKCUyMiUzQ2ltYWdlJTNFJTIyJTJDJTIwMzIwMDApJTBBJTBBdmlzaW9uX3Rva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIyLiUyRnBhdGglMkZ0byUyRnRva2VuaXplciUyMik=",highlighted:`vision_tokenizer = AutoTokenizer.from_pretrained(
    <span class="hljs-string">&quot;llava-hf/llava-1.5-7b-hf&quot;</span>,
    extra_special_tokens={<span class="hljs-string">&quot;image_token&quot;</span>: <span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;boi_token&quot;</span>: <span class="hljs-string">&quot;&lt;image_start&gt;&quot;</span>, <span class="hljs-string">&quot;eoi_token&quot;</span>: <span class="hljs-string">&quot;&lt;image_end&gt;&quot;</span>}
)
<span class="hljs-built_in">print</span>(vision_tokenizer.image_token, vision_tokenizer.image_token_id)
(<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-number">32000</span>)

vision_tokenizer.save_pretrained(<span class="hljs-string">&quot;./path/to/tokenizer&quot;</span>)`,wrap:!1}}),ee=new B({props:{title:"Fast tokenizers",local:"fast-tokenizers",headingTag:"h2"}}),se=new Mn({props:{id:"3umI3tm27Vw"}}),re=new g({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKQ==",highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer

tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])`,wrap:!1}}),oe=new g({props:{code:"ZnJvbSUyMHRva2VuaXplcnMucHJlX3Rva2VuaXplcnMlMjBpbXBvcnQlMjBXaGl0ZXNwYWNlJTBBJTBBdG9rZW5pemVyLnByZV90b2tlbml6ZXIlMjAlM0QlMjBXaGl0ZXNwYWNlKCk=",highlighted:`<span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

tokenizer.pre_tokenizer = Whitespace()`,wrap:!1}}),me=new g({props:{code:"ZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`files = [...]
tokenizer.train(files, trainer)`,wrap:!1}}),ye=new g({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),ue=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),Te=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),he=new B({props:{title:"tiktoken",local:"tiktoken",headingTag:"h2"}}),ke=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTWV0YS1MbGFtYS0zLThCLUluc3RydWN0JTIyJTJDJTIwc3ViZm9sZGVyJTNEJTIyb3JpZ2luYWwlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>, subfolder=<span class="hljs-string">&quot;original&quot;</span>)`,wrap:!1}}),je=new B({props:{title:"Create a tiktoken tokenizer",local:"create-a-tiktoken-tokenizer",headingTag:"h3"}}),we=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMudGlrdG9rZW4lMjBpbXBvcnQlMjBjb252ZXJ0X3Rpa3Rva2VuX3RvX2Zhc3QlMEFmcm9tJTIwdGlrdG9rZW4lMjBpbXBvcnQlMjBnZXRfZW5jb2RpbmclMEElMEElMjMlMjBMb2FkJTIweW91ciUyMGN1c3RvbSUyMGVuY29kaW5nJTIwb3IlMjB0aGUlMjBvbmUlMjBwcm92aWRlZCUyMGJ5JTIwT3BlbkFJJTBBZW5jb2RpbmclMjAlM0QlMjBnZXRfZW5jb2RpbmcoJTIyZ3B0MiUyMiklMEFjb252ZXJ0X3Rpa3Rva2VuX3RvX2Zhc3QoZW5jb2RpbmclMkMlMjAlMjJjb25maWclMkZzYXZlJTJGZGlyJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers.integrations.tiktoken <span class="hljs-keyword">import</span> convert_tiktoken_to_fast
<span class="hljs-keyword">from</span> tiktoken <span class="hljs-keyword">import</span> get_encoding

<span class="hljs-comment"># Load your custom encoding or the one provided by OpenAI</span>
encoding = get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)
convert_tiktoken_to_fast(encoding, <span class="hljs-string">&quot;config/save/dir&quot;</span>)`,wrap:!1}}),Ie=new g({props:{code:"dG9rZW5pemVyJTIwJTNEJTIwUHJlVHJhaW5lZFRva2VuaXplckZhc3QuZnJvbV9wcmV0cmFpbmVkKCUyMmNvbmZpZyUyRnNhdmUlMkZkaXIlMjIp",highlighted:'tokenizer = PreTrainedTokenizerFast.from_pretrained(<span class="hljs-string">&quot;config/save/dir&quot;</span>)',wrap:!1}}),ge=new B({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),ve=new Mn({props:{id:"Yffk5aydLzg"}}),Ce=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZnZW1tYS0yLTJiJTIyKSUwQXRva2VuaXplciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnkuJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElN0InaW5wdXRfaWRzJyUzQSUyMHRlbnNvciglNUIlNUIlMjAlMjAlMjAlMjAlMjAyJTJDJTIwJTIwJTIwMTczNCUyQyUyMCUyMCUyMCUyMDcwOCUyQyUyMCUyMCUyMDE1MDglMkMlMjAlMjAlMjA0OTE1JTJDJTIwJTIwJTIwJTIwNTc3JTJDJTIwJTIwJTIwMTUwMCUyQyUyMCUyMCUyMCUyMDY5MiUyQyUyMCUyMCUyMCUyMDU3MyUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDE1NjgwOCUyQyUyMDEyODE0OSUyQyUyMCUyMCUyMDk1ODElMkMlMjAyMzUyNjUlNUQlNUQpJTJDJTBBJTIwJ2F0dGVudGlvbl9tYXNrJyUzQSUyMHRlbnNvciglNUIlNUIxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTVEKSUwQSU3RA==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)
tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[     <span class="hljs-number">2</span>,   <span class="hljs-number">1734</span>,    <span class="hljs-number">708</span>,   <span class="hljs-number">1508</span>,   <span class="hljs-number">4915</span>,    <span class="hljs-number">577</span>,   <span class="hljs-number">1500</span>,    <span class="hljs-number">692</span>,    <span class="hljs-number">573</span>,
         <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>,   <span class="hljs-number">9581</span>, <span class="hljs-number">235265</span>]]),
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
}`,wrap:!1}}),A=new yn({props:{id:"steps",options:["1. tokenize","2. convert tokens to ids","3. decode ids to text"],$$slots:{default:[gn]},$$scope:{ctx:z}}}),G=new ss({props:{warning:!1,$$slots:{default:[vn]},$$scope:{ctx:z}}}),xe=new B({props:{title:"Special tokens",local:"special-tokens",headingTag:"h3"}}),We=new g({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMldlJTIwYXJlJTIwdmVyeSUyMGhhcHB5JTIwdG8lMjBzaG93JTIweW91JTIwdGhlJTIwJUYwJTlGJUE0JTk3JTIwVHJhbnNmb3JtZXJzJTIwbGlicmFyeS4lMjIpJTBBJTVCMiUyQyUyMDE3MzQlMkMlMjA3MDglMkMlMjAxNTA4JTJDJTIwNDkxNSUyQyUyMDU3NyUyQyUyMDE1MDAlMkMlMjA2OTIlMkMlMjA1NzMlMkMlMjAxNTY4MDglMkMlMjAxMjgxNDklMkMlMjA5NTgxJTVEJTBBdG9rZW5pemVyLmNvbnZlcnRfdG9rZW5zX3RvX2lkcyh0b2tlbnMpJTBBJTVCMTczNCUyQyUyMDcwOCUyQyUyMDE1MDglMkMlMjA0OTE1JTJDJTIwNTc3JTJDJTIwMTUwMCUyQyUyMDY5MiUyQyUyMDU3MyUyQyUyMDE1NjgwOCUyQyUyMDEyODE0OSUyQyUyMDk1ODElNUQ=",highlighted:`model_inputs = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>)
[<span class="hljs-number">2</span>, <span class="hljs-number">1734</span>, <span class="hljs-number">708</span>, <span class="hljs-number">1508</span>, <span class="hljs-number">4915</span>, <span class="hljs-number">577</span>, <span class="hljs-number">1500</span>, <span class="hljs-number">692</span>, <span class="hljs-number">573</span>, <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>, <span class="hljs-number">9581</span>]
tokenizer.convert_tokens_to_ids(tokens)
[<span class="hljs-number">1734</span>, <span class="hljs-number">708</span>, <span class="hljs-number">1508</span>, <span class="hljs-number">4915</span>, <span class="hljs-number">577</span>, <span class="hljs-number">1500</span>, <span class="hljs-number">692</span>, <span class="hljs-number">573</span>, <span class="hljs-number">156808</span>, <span class="hljs-number">128149</span>, <span class="hljs-number">9581</span>]`,wrap:!1}}),He=new g({props:{code:"cHJpbnQodG9rZW5pemVyLmRlY29kZShtb2RlbF9pbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQpKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUoaWRzKSklMEEnJTNDYm9zJTNFV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiclMEEnV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5Jw==",highlighted:`<span class="hljs-built_in">print</span>(tokenizer.decode(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))
<span class="hljs-built_in">print</span>(tokenizer.decode(ids))
<span class="hljs-string">&#x27;&lt;bos&gt;We are very happy to show you the ü§ó Transformers library.&#x27;</span>
<span class="hljs-string">&#x27;We are very happy to show you the ü§ó Transformers library&#x27;</span>`,wrap:!1}}),Ae=new B({props:{title:"Batch tokenization",local:"batch-tokenization",headingTag:"h3"}}),Ee=new g({props:{code:"YmF0Y2hfc2VudGVuY2VzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIyQnV0JTIwd2hhdCUyMGFib3V0JTIwc2Vjb25kJTIwYnJlYWtmYXN0JTNGJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIyRG9uJ3QlMjB0aGluayUyMGhlJTIwa25vd3MlMjBhYm91dCUyMHNlY29uZCUyMGJyZWFrZmFzdCUyQyUyMFBpcC4lMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJXaGF0JTIwYWJvdXQlMjBlbGV2ZW5zaWVzJTNGJTIyJTJDJTBBJTVEJTBBZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoYmF0Y2hfc2VudGVuY2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFwcmludChlbmNvZGVkX2lucHV0cyklMEElN0IlMEElMjAnaW5wdXRfaWRzJyUzQSUwQSUyMCUyMCUyMCUyMCU1QiU1QjIlMkMlMjAxODYwJTJDJTIwMTIxMiUyQyUyMDExMDUlMkMlMjAyMjU3JTJDJTIwMTQ0NTclMkMlMjAyMzUzMzYlNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlNUIyJTJDJTIwNDQ1NCUyQyUyMDIzNTMwMyUyQyUyMDIzNTI1MSUyQyUyMDE3NDIlMkMlMjA2OTMlMkMlMjA5MjQyJTJDJTIwMTEwNSUyQyUyMDIyNTclMkMlMjAxNDQ1NyUyQyUyMDIzNTI2OSUyQyUyMDQ4NzgyJTJDJTIwMjM1MjY1JTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTVCMiUyQyUyMDE4NDElMkMlMjAxMTA1JTJDJTIwMjk3NTQlMkMlMjAzNzQ1MyUyQyUyMDIzNTMzNiU1RCU1RCUyQyUwQSUyMCdhdHRlbnRpb25fbWFzayclM0ElMjAlNUIlNUIxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSU1RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU1QjElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSU1RCU1RCUwQSU3RA==",highlighted:`batch_sentences = [
    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
]
encoded_inputs = tokenizer(batch_sentences, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(encoded_inputs)
{
 <span class="hljs-string">&#x27;input_ids&#x27;</span>:
    [[<span class="hljs-number">2</span>, <span class="hljs-number">1860</span>, <span class="hljs-number">1212</span>, <span class="hljs-number">1105</span>, <span class="hljs-number">2257</span>, <span class="hljs-number">14457</span>, <span class="hljs-number">235336</span>],
     [<span class="hljs-number">2</span>, <span class="hljs-number">4454</span>, <span class="hljs-number">235303</span>, <span class="hljs-number">235251</span>, <span class="hljs-number">1742</span>, <span class="hljs-number">693</span>, <span class="hljs-number">9242</span>, <span class="hljs-number">1105</span>, <span class="hljs-number">2257</span>, <span class="hljs-number">14457</span>, <span class="hljs-number">235269</span>, <span class="hljs-number">48782</span>, <span class="hljs-number">235265</span>],
     [<span class="hljs-number">2</span>, <span class="hljs-number">1841</span>, <span class="hljs-number">1105</span>, <span class="hljs-number">29754</span>, <span class="hljs-number">37453</span>, <span class="hljs-number">235336</span>]],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]
}`,wrap:!1}}),De=new B({props:{title:"Padding",local:"padding",headingTag:"h3"}}),R=new ss({props:{warning:!1,$$slots:{default:[_n]},$$scope:{ctx:z}}}),Le=new g({props:{code:"ZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoYmF0Y2hfc2VudGVuY2VzJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXByaW50KGVuY29kZWRfaW5wdXRzKQ==",highlighted:`encoded_inputs = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(encoded_inputs)`,wrap:!1}}),Pe=new B({props:{title:"Truncation",local:"truncation",headingTag:"h3"}}),E=new ss({props:{warning:!1,$$slots:{default:[Cn]},$$scope:{ctx:z}}}),Oe=new g({props:{code:"ZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoYmF0Y2hfc2VudGVuY2VzJTJDJTIwbWF4X2xlbmd0aCUzRDglMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBcHJpbnQoZW5jb2RlZF9pbnB1dHMp",highlighted:`encoded_inputs = tokenizer(batch_sentences, max_length=<span class="hljs-number">8</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(encoded_inputs)`,wrap:!1}}),Ke=new bn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md"}}),{c(){l=y("meta"),j=a(),i=y("p"),p=a(),T(M.$$.fragment),o=a(),m=y("p"),m.textContent=w,U=a(),T(I.$$.fragment),C=a(),Z=y("p"),Z.innerHTML=x,$=a(),v=y("p"),v.innerHTML=Q,V=a(),T(f.$$.fragment),_=a(),W=y("p"),W.textContent=Jt,ns=a(),D=y("p"),D.textContent=kt,ls=a(),T(X.$$.fragment),as=a(),F=y("p"),F.innerHTML=jt,rs=a(),L=y("ul"),L.innerHTML=Ut,is=a(),S=y("p"),S.innerHTML=$t,os=a(),T(N.$$.fragment),ps=a(),T(P.$$.fragment),ms=a(),q=y("p"),q.textContent=wt,Ms=a(),Y=y("p"),Y.innerHTML=zt,ys=a(),O=y("p"),O.innerHTML=It,cs=a(),T(K.$$.fragment),us=a(),T(ee.$$.fragment),fs=a(),T(se.$$.fragment),Ts=a(),te=y("p"),te.innerHTML=gt,hs=a(),ne=y("p"),ne.innerHTML=vt,ds=a(),le=y("p"),le.textContent=_t,bs=a(),ae=y("p"),ae.innerHTML=Ct,Js=a(),T(re.$$.fragment),ks=a(),ie=y("p"),ie.innerHTML=Zt,js=a(),T(oe.$$.fragment),Us=a(),pe=y("p"),pe.innerHTML=xt,$s=a(),T(me.$$.fragment),ws=a(),Me=y("p"),Me.innerHTML=Bt,zs=a(),T(ye.$$.fragment),Is=a(),ce=y("p"),ce.innerHTML=Qt,gs=a(),T(ue.$$.fragment),vs=a(),fe=y("p"),fe.innerHTML=Wt,_s=a(),T(Te.$$.fragment),Cs=a(),T(he.$$.fragment),Zs=a(),de=y("p"),de.innerHTML=Vt,xs=a(),be=y("p"),be.innerHTML=Ht,Bs=a(),Je=y("p"),Je.innerHTML=Nt,Qs=a(),T(ke.$$.fragment),Ws=a(),T(je.$$.fragment),Vs=a(),Ue=y("p"),Ue.innerHTML=At,Hs=a(),$e=y("p"),$e.innerHTML=Gt,Ns=a(),T(we.$$.fragment),As=a(),ze=y("p"),ze.innerHTML=Rt,Gs=a(),T(Ie.$$.fragment),Rs=a(),T(ge.$$.fragment),Es=a(),T(ve.$$.fragment),Ds=a(),_e=y("p"),_e.innerHTML=Et,Xs=a(),T(Ce.$$.fragment),Fs=a(),Ze=y("p"),Ze.textContent=Dt,Ls=a(),T(A.$$.fragment),Ss=a(),T(G.$$.fragment),Ps=a(),T(xe.$$.fragment),qs=a(),Be=y("p"),Be.textContent=Xt,Ys=a(),Qe=y("p"),Qe.innerHTML=Ft,Os=a(),T(We.$$.fragment),Ks=a(),Ve=y("p"),Ve.innerHTML=Lt,et=a(),T(He.$$.fragment),st=a(),Ne=y("p"),Ne.textContent=St,tt=a(),T(Ae.$$.fragment),nt=a(),Ge=y("p"),Ge.innerHTML=Pt,lt=a(),Re=y("p"),Re.textContent=qt,at=a(),T(Ee.$$.fragment),rt=a(),T(De.$$.fragment),it=a(),T(R.$$.fragment),ot=a(),Xe=y("p"),Xe.innerHTML=Yt,pt=a(),Fe=y("p"),Fe.innerHTML=Ot,mt=a(),T(Le.$$.fragment),Mt=a(),Se=y("p"),Se.innerHTML=Kt,yt=a(),T(Pe.$$.fragment),ct=a(),T(E.$$.fragment),ut=a(),qe=y("p"),qe.textContent=en,ft=a(),Ye=y("p"),Ye.innerHTML=sn,Tt=a(),T(Oe.$$.fragment),ht=a(),T(Ke.$$.fragment),dt=a(),es=y("p"),this.h()},l(e){const s=hn("svelte-u9bgzb",document.head);l=c(s,"META",{name:!0,content:!0}),s.forEach(t),j=r(e),i=c(e,"P",{}),pn(i).forEach(t),p=r(e),h(M.$$.fragment,e),o=r(e),m=c(e,"P",{"data-svelte-h":!0}),u(m)!=="svelte-11r544d"&&(m.textContent=w),U=r(e),h(I.$$.fragment,e),C=r(e),Z=c(e,"P",{"data-svelte-h":!0}),u(Z)!=="svelte-gwc1vy"&&(Z.innerHTML=x),$=r(e),v=c(e,"P",{"data-svelte-h":!0}),u(v)!=="svelte-1v4y8i9"&&(v.innerHTML=Q),V=r(e),h(f.$$.fragment,e),_=r(e),W=c(e,"P",{"data-svelte-h":!0}),u(W)!=="svelte-fmq7bu"&&(W.textContent=Jt),ns=r(e),D=c(e,"P",{"data-svelte-h":!0}),u(D)!=="svelte-74r2x9"&&(D.textContent=kt),ls=r(e),h(X.$$.fragment,e),as=r(e),F=c(e,"P",{"data-svelte-h":!0}),u(F)!=="svelte-1xjopa2"&&(F.innerHTML=jt),rs=r(e),L=c(e,"UL",{"data-svelte-h":!0}),u(L)!=="svelte-1kudymi"&&(L.innerHTML=Ut),is=r(e),S=c(e,"P",{"data-svelte-h":!0}),u(S)!=="svelte-19tg0ci"&&(S.innerHTML=$t),os=r(e),h(N.$$.fragment,e),ps=r(e),h(P.$$.fragment,e),ms=r(e),q=c(e,"P",{"data-svelte-h":!0}),u(q)!=="svelte-1wtso2"&&(q.textContent=wt),Ms=r(e),Y=c(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-1n553nj"&&(Y.innerHTML=zt),ys=r(e),O=c(e,"P",{"data-svelte-h":!0}),u(O)!=="svelte-1xu9dqq"&&(O.innerHTML=It),cs=r(e),h(K.$$.fragment,e),us=r(e),h(ee.$$.fragment,e),fs=r(e),h(se.$$.fragment,e),Ts=r(e),te=c(e,"P",{"data-svelte-h":!0}),u(te)!=="svelte-pa5zp6"&&(te.innerHTML=gt),hs=r(e),ne=c(e,"P",{"data-svelte-h":!0}),u(ne)!=="svelte-j0ydyp"&&(ne.innerHTML=vt),ds=r(e),le=c(e,"P",{"data-svelte-h":!0}),u(le)!=="svelte-1v32tzh"&&(le.textContent=_t),bs=r(e),ae=c(e,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-2c7hlw"&&(ae.innerHTML=Ct),Js=r(e),h(re.$$.fragment,e),ks=r(e),ie=c(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-vf8ng4"&&(ie.innerHTML=Zt),js=r(e),h(oe.$$.fragment,e),Us=r(e),pe=c(e,"P",{"data-svelte-h":!0}),u(pe)!=="svelte-8ejm9k"&&(pe.innerHTML=xt),$s=r(e),h(me.$$.fragment,e),ws=r(e),Me=c(e,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-10cbx6z"&&(Me.innerHTML=Bt),zs=r(e),h(ye.$$.fragment,e),Is=r(e),ce=c(e,"P",{"data-svelte-h":!0}),u(ce)!=="svelte-pmk80d"&&(ce.innerHTML=Qt),gs=r(e),h(ue.$$.fragment,e),vs=r(e),fe=c(e,"P",{"data-svelte-h":!0}),u(fe)!=="svelte-z8gt56"&&(fe.innerHTML=Wt),_s=r(e),h(Te.$$.fragment,e),Cs=r(e),h(he.$$.fragment,e),Zs=r(e),de=c(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-10w7qr0"&&(de.innerHTML=Vt),xs=r(e),be=c(e,"P",{"data-svelte-h":!0}),u(be)!=="svelte-1v7tegn"&&(be.innerHTML=Ht),Bs=r(e),Je=c(e,"P",{"data-svelte-h":!0}),u(Je)!=="svelte-1pwyct1"&&(Je.innerHTML=Nt),Qs=r(e),h(ke.$$.fragment,e),Ws=r(e),h(je.$$.fragment,e),Vs=r(e),Ue=c(e,"P",{"data-svelte-h":!0}),u(Ue)!=="svelte-ba67fl"&&(Ue.innerHTML=At),Hs=r(e),$e=c(e,"P",{"data-svelte-h":!0}),u($e)!=="svelte-t11i5b"&&($e.innerHTML=Gt),Ns=r(e),h(we.$$.fragment,e),As=r(e),ze=c(e,"P",{"data-svelte-h":!0}),u(ze)!=="svelte-la1oxa"&&(ze.innerHTML=Rt),Gs=r(e),h(Ie.$$.fragment,e),Rs=r(e),h(ge.$$.fragment,e),Es=r(e),h(ve.$$.fragment,e),Ds=r(e),_e=c(e,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-1bfoj6p"&&(_e.innerHTML=Et),Xs=r(e),h(Ce.$$.fragment,e),Fs=r(e),Ze=c(e,"P",{"data-svelte-h":!0}),u(Ze)!=="svelte-1bpp0kx"&&(Ze.textContent=Dt),Ls=r(e),h(A.$$.fragment,e),Ss=r(e),h(G.$$.fragment,e),Ps=r(e),h(xe.$$.fragment,e),qs=r(e),Be=c(e,"P",{"data-svelte-h":!0}),u(Be)!=="svelte-81nneu"&&(Be.textContent=Xt),Ys=r(e),Qe=c(e,"P",{"data-svelte-h":!0}),u(Qe)!=="svelte-kv2uds"&&(Qe.innerHTML=Ft),Os=r(e),h(We.$$.fragment,e),Ks=r(e),Ve=c(e,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-16if7zq"&&(Ve.innerHTML=Lt),et=r(e),h(He.$$.fragment,e),st=r(e),Ne=c(e,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-10r0jkh"&&(Ne.textContent=St),tt=r(e),h(Ae.$$.fragment,e),nt=r(e),Ge=c(e,"P",{"data-svelte-h":!0}),u(Ge)!=="svelte-1pwp3dv"&&(Ge.innerHTML=Pt),lt=r(e),Re=c(e,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-1502w4"&&(Re.textContent=qt),at=r(e),h(Ee.$$.fragment,e),rt=r(e),h(De.$$.fragment,e),it=r(e),h(R.$$.fragment,e),ot=r(e),Xe=c(e,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-13g0g3k"&&(Xe.innerHTML=Yt),pt=r(e),Fe=c(e,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-sor0ra"&&(Fe.innerHTML=Ot),mt=r(e),h(Le.$$.fragment,e),Mt=r(e),Se=c(e,"P",{"data-svelte-h":!0}),u(Se)!=="svelte-19p5uh3"&&(Se.innerHTML=Kt),yt=r(e),h(Pe.$$.fragment,e),ct=r(e),h(E.$$.fragment,e),ut=r(e),qe=c(e,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-pvywvo"&&(qe.textContent=en),ft=r(e),Ye=c(e,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-x3l9nl"&&(Ye.innerHTML=sn),Tt=r(e),h(Oe.$$.fragment,e),ht=r(e),h(Ke.$$.fragment,e),dt=r(e),es=c(e,"P",{}),pn(es).forEach(t),this.h()},h(){mn(l,"name","hf:doc:metadata"),mn(l,"content",xn)},m(e,s){dn(document.head,l),n(e,j,s),n(e,i,s),n(e,p,s),d(M,e,s),n(e,o,s),n(e,m,s),n(e,U,s),d(I,e,s),n(e,C,s),n(e,Z,s),n(e,$,s),n(e,v,s),n(e,V,s),d(f,e,s),n(e,_,s),n(e,W,s),n(e,ns,s),n(e,D,s),n(e,ls,s),d(X,e,s),n(e,as,s),n(e,F,s),n(e,rs,s),n(e,L,s),n(e,is,s),n(e,S,s),n(e,os,s),d(N,e,s),n(e,ps,s),d(P,e,s),n(e,ms,s),n(e,q,s),n(e,Ms,s),n(e,Y,s),n(e,ys,s),n(e,O,s),n(e,cs,s),d(K,e,s),n(e,us,s),d(ee,e,s),n(e,fs,s),d(se,e,s),n(e,Ts,s),n(e,te,s),n(e,hs,s),n(e,ne,s),n(e,ds,s),n(e,le,s),n(e,bs,s),n(e,ae,s),n(e,Js,s),d(re,e,s),n(e,ks,s),n(e,ie,s),n(e,js,s),d(oe,e,s),n(e,Us,s),n(e,pe,s),n(e,$s,s),d(me,e,s),n(e,ws,s),n(e,Me,s),n(e,zs,s),d(ye,e,s),n(e,Is,s),n(e,ce,s),n(e,gs,s),d(ue,e,s),n(e,vs,s),n(e,fe,s),n(e,_s,s),d(Te,e,s),n(e,Cs,s),d(he,e,s),n(e,Zs,s),n(e,de,s),n(e,xs,s),n(e,be,s),n(e,Bs,s),n(e,Je,s),n(e,Qs,s),d(ke,e,s),n(e,Ws,s),d(je,e,s),n(e,Vs,s),n(e,Ue,s),n(e,Hs,s),n(e,$e,s),n(e,Ns,s),d(we,e,s),n(e,As,s),n(e,ze,s),n(e,Gs,s),d(Ie,e,s),n(e,Rs,s),d(ge,e,s),n(e,Es,s),d(ve,e,s),n(e,Ds,s),n(e,_e,s),n(e,Xs,s),d(Ce,e,s),n(e,Fs,s),n(e,Ze,s),n(e,Ls,s),d(A,e,s),n(e,Ss,s),d(G,e,s),n(e,Ps,s),d(xe,e,s),n(e,qs,s),n(e,Be,s),n(e,Ys,s),n(e,Qe,s),n(e,Os,s),d(We,e,s),n(e,Ks,s),n(e,Ve,s),n(e,et,s),d(He,e,s),n(e,st,s),n(e,Ne,s),n(e,tt,s),d(Ae,e,s),n(e,nt,s),n(e,Ge,s),n(e,lt,s),n(e,Re,s),n(e,at,s),d(Ee,e,s),n(e,rt,s),d(De,e,s),n(e,it,s),d(R,e,s),n(e,ot,s),n(e,Xe,s),n(e,pt,s),n(e,Fe,s),n(e,mt,s),d(Le,e,s),n(e,Mt,s),n(e,Se,s),n(e,yt,s),d(Pe,e,s),n(e,ct,s),d(E,e,s),n(e,ut,s),n(e,qe,s),n(e,ft,s),n(e,Ye,s),n(e,Tt,s),d(Oe,e,s),n(e,ht,s),d(Ke,e,s),n(e,dt,s),n(e,es,s),bt=!0},p(e,[s]){const tn={};s&2&&(tn.$$scope={dirty:s,ctx:e}),I.$set(tn);const nn={};s&2&&(nn.$$scope={dirty:s,ctx:e}),N.$set(nn);const ln={};s&2&&(ln.$$scope={dirty:s,ctx:e}),A.$set(ln);const an={};s&2&&(an.$$scope={dirty:s,ctx:e}),G.$set(an);const rn={};s&2&&(rn.$$scope={dirty:s,ctx:e}),R.$set(rn);const on={};s&2&&(on.$$scope={dirty:s,ctx:e}),E.$set(on)},i(e){bt||(b(M.$$.fragment,e),b(I.$$.fragment,e),b(f.$$.fragment,e),b(X.$$.fragment,e),b(N.$$.fragment,e),b(P.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(oe.$$.fragment,e),b(me.$$.fragment,e),b(ye.$$.fragment,e),b(ue.$$.fragment,e),b(Te.$$.fragment,e),b(he.$$.fragment,e),b(ke.$$.fragment,e),b(je.$$.fragment,e),b(we.$$.fragment,e),b(Ie.$$.fragment,e),b(ge.$$.fragment,e),b(ve.$$.fragment,e),b(Ce.$$.fragment,e),b(A.$$.fragment,e),b(G.$$.fragment,e),b(xe.$$.fragment,e),b(We.$$.fragment,e),b(He.$$.fragment,e),b(Ae.$$.fragment,e),b(Ee.$$.fragment,e),b(De.$$.fragment,e),b(R.$$.fragment,e),b(Le.$$.fragment,e),b(Pe.$$.fragment,e),b(E.$$.fragment,e),b(Oe.$$.fragment,e),b(Ke.$$.fragment,e),bt=!0)},o(e){J(M.$$.fragment,e),J(I.$$.fragment,e),J(f.$$.fragment,e),J(X.$$.fragment,e),J(N.$$.fragment,e),J(P.$$.fragment,e),J(K.$$.fragment,e),J(ee.$$.fragment,e),J(se.$$.fragment,e),J(re.$$.fragment,e),J(oe.$$.fragment,e),J(me.$$.fragment,e),J(ye.$$.fragment,e),J(ue.$$.fragment,e),J(Te.$$.fragment,e),J(he.$$.fragment,e),J(ke.$$.fragment,e),J(je.$$.fragment,e),J(we.$$.fragment,e),J(Ie.$$.fragment,e),J(ge.$$.fragment,e),J(ve.$$.fragment,e),J(Ce.$$.fragment,e),J(A.$$.fragment,e),J(G.$$.fragment,e),J(xe.$$.fragment,e),J(We.$$.fragment,e),J(He.$$.fragment,e),J(Ae.$$.fragment,e),J(Ee.$$.fragment,e),J(De.$$.fragment,e),J(R.$$.fragment,e),J(Le.$$.fragment,e),J(Pe.$$.fragment,e),J(E.$$.fragment,e),J(Oe.$$.fragment,e),J(Ke.$$.fragment,e),bt=!1},d(e){e&&(t(j),t(i),t(p),t(o),t(m),t(U),t(C),t(Z),t($),t(v),t(V),t(_),t(W),t(ns),t(D),t(ls),t(as),t(F),t(rs),t(L),t(is),t(S),t(os),t(ps),t(ms),t(q),t(Ms),t(Y),t(ys),t(O),t(cs),t(us),t(fs),t(Ts),t(te),t(hs),t(ne),t(ds),t(le),t(bs),t(ae),t(Js),t(ks),t(ie),t(js),t(Us),t(pe),t($s),t(ws),t(Me),t(zs),t(Is),t(ce),t(gs),t(vs),t(fe),t(_s),t(Cs),t(Zs),t(de),t(xs),t(be),t(Bs),t(Je),t(Qs),t(Ws),t(Vs),t(Ue),t(Hs),t($e),t(Ns),t(As),t(ze),t(Gs),t(Rs),t(Es),t(Ds),t(_e),t(Xs),t(Fs),t(Ze),t(Ls),t(Ss),t(Ps),t(qs),t(Be),t(Ys),t(Qe),t(Os),t(Ks),t(Ve),t(et),t(st),t(Ne),t(tt),t(nt),t(Ge),t(lt),t(Re),t(at),t(rt),t(it),t(ot),t(Xe),t(pt),t(Fe),t(mt),t(Mt),t(Se),t(yt),t(ct),t(ut),t(qe),t(ft),t(Ye),t(Tt),t(ht),t(dt),t(es)),t(l),k(M,e),k(I,e),k(f,e),k(X,e),k(N,e),k(P,e),k(K,e),k(ee,e),k(se,e),k(re,e),k(oe,e),k(me,e),k(ye,e),k(ue,e),k(Te,e),k(he,e),k(ke,e),k(je,e),k(we,e),k(Ie,e),k(ge,e),k(ve,e),k(Ce,e),k(A,e),k(G,e),k(xe,e),k(We,e),k(He,e),k(Ae,e),k(Ee,e),k(De,e),k(R,e),k(Le,e),k(Pe,e),k(E,e),k(Oe,e),k(Ke,e)}}}const xn='{"title":"Tokenizers","local":"tokenizers","sections":[{"title":"Tokenizer classes","local":"tokenizer-classes","sections":[],"depth":2},{"title":"Multimodal tokenizers","local":"multimodal-tokenizers","sections":[],"depth":2},{"title":"Fast tokenizers","local":"fast-tokenizers","sections":[],"depth":2},{"title":"tiktoken","local":"tiktoken","sections":[{"title":"Create a tiktoken tokenizer","local":"create-a-tiktoken-tokenizer","sections":[],"depth":3}],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[{"title":"Special tokens","local":"special-tokens","sections":[],"depth":3},{"title":"Batch tokenization","local":"batch-tokenization","sections":[],"depth":3},{"title":"Padding","local":"padding","sections":[],"depth":3},{"title":"Truncation","local":"truncation","sections":[],"depth":3}],"depth":2}],"depth":1}';function Bn(z){return un(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rn extends fn{constructor(l){super(),Tn(this,l,Bn,Zn,cn,{})}}export{Rn as component};
