import{s as xt,n as $t,o as wt}from"../chunks/scheduler.18a86fab.js";import{S as Mt,i as zt,g as a,s as o,r as p,A as qt,h as i,f as n,c as s,j as y,x as m,u as f,k as T,y as t,a as d,v as _,d as g,t as u,w as h}from"../chunks/index.98837b22.js";import{D as $}from"../chunks/Docstring.a1ef7999.js";import{H as He,E as Dt}from"../chunks/getInferenceSnippets.06c2775f.js";function Lt(ot){let w,ge,fe,ue,C,st="<em>This model was released on 2024-03-15 and added to Hugging Face Transformers on 2024-10-06.</em>",he,E,ke,I,ve,P,rt=`The myt5 model was proposed in <a href="https://huggingface.co/papers/2403.10691" rel="nofollow">MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling</a> by Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.
MyT5 (<strong>My</strong>te <strong>T5</strong>) is a multilingual language model based on T5 architecture.
The model uses a <strong>m</strong>orphologically-driven <strong>byte</strong> (<strong>MYTE</strong>) representation described in our paper.
<strong>MYTE</strong> uses codepoints corresponding to morphemes in contrast to characters used in UTF-8 encoding.
As a pre-requisite, we used unsupervised morphological segmentation (<a href="https://aclanthology.org/E14-2006.pdf" rel="nofollow">Morfessor</a>) to obtain morpheme inventories for 99 languages.
However, the morphological segmentation step is not needed when using the pre-defined morpheme inventory from the hub (see: <a href="https://huggingface.co/Tomlim/myt5-base" rel="nofollow">Tomli/myt5-base</a>).`,ye,H,at="The abstract from the paper is the following:",Te,A,it="<em>A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the worldâ€™s writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.</em>",be,O,dt=`This model was contributed by <a href="https://huggingface.co/Tomlim" rel="nofollow">Tomasz Limisiewicz</a>.
The original code can be found <a href="https://github.com/tomlimi/MYTE" rel="nofollow">here</a>.`,xe,V,$e,l,B,Ae,J,lt="Construct a MyT5 tokenizer.",Oe,K,ct=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Ve,b,N,Be,Q,mt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`,Ne,ee,pt="<li>single sequence: <code>X &lt;/s&gt;</code></li> <li>pair of sequences: <code>A &lt;/s&gt; B &lt;/s&gt;</code></li>",je,M,j,Fe,te,ft=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Ue,z,F,Ye,ne,_t=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. MyT5 does not
make use of token type ids, therefore a list of zeros is returned.`,We,oe,U,we,Y,Me,c,W,Ge,se,gt="Construct a MyT5 tokenizer.",Re,re,ut=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Se,x,G,Xe,ae,ht=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`,Ze,ie,kt="<li>single sequence: <code>X &lt;/s&gt;</code></li> <li>pair of sequences: <code>A &lt;/s&gt; B &lt;/s&gt;</code></li>",Je,q,R,Ke,de,vt="Converts a sequence of tokens (string) in a single string.",Qe,D,S,et,le,yt=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. MyT5 does not
make use of token type ids, therefore a list of zeros is returned.`,tt,L,X,nt,ce,Tt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,ze,Z,qe,_e,De;return E=new He({props:{title:"myt5",local:"myt5",headingTag:"h1"}}),I=new He({props:{title:"Overview",local:"overview",headingTag:"h2"}}),V=new He({props:{title:"MyT5Tokenizer",local:"transformers.MyT5Tokenizer",headingTag:"h2"}}),B=new $({props:{name:"class transformers.MyT5Tokenizer",anchor:"transformers.MyT5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 125"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.vocab_file",description:"<strong>vocab_file</strong> (<code>str</code>) &#x2014; The file containing the byte rewriting rules.",name:"vocab_file"},{anchor:"transformers.MyT5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.MyT5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.MyT5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.MyT5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 125) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in ByT5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.MyT5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L135"}}),N=new $({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L284",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),j=new $({props:{name:"get_special_tokens_mask",anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L222",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),F=new $({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L261",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),U=new $({props:{name:"save_vocabulary",anchor:"transformers.MyT5Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L368"}}),Y=new He({props:{title:"MyT5Tokenizer",local:"transformers.MyT5Tokenizer",headingTag:"h2"}}),W=new $({props:{name:"class transformers.MyT5Tokenizer",anchor:"transformers.MyT5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 125"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.vocab_file",description:"<strong>vocab_file</strong> (<code>str</code>) &#x2014; The file containing the byte rewriting rules.",name:"vocab_file"},{anchor:"transformers.MyT5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.MyT5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.MyT5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.MyT5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 125) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in ByT5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.MyT5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L135"}}),G=new $({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L284",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),R=new $({props:{name:"convert_tokens_to_string",anchor:"transformers.MyT5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L345"}}),S=new $({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L261",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),X=new $({props:{name:"get_special_tokens_mask",anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.MyT5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/myt5/tokenization_myt5.py#L222",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Z=new Dt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/myt5.md"}}),{c(){w=a("meta"),ge=o(),fe=a("p"),ue=o(),C=a("p"),C.innerHTML=st,he=o(),p(E.$$.fragment),ke=o(),p(I.$$.fragment),ve=o(),P=a("p"),P.innerHTML=rt,ye=o(),H=a("p"),H.textContent=at,Te=o(),A=a("p"),A.innerHTML=it,be=o(),O=a("p"),O.innerHTML=dt,xe=o(),p(V.$$.fragment),$e=o(),l=a("div"),p(B.$$.fragment),Ae=o(),J=a("p"),J.textContent=lt,Oe=o(),K=a("p"),K.innerHTML=ct,Ve=o(),b=a("div"),p(N.$$.fragment),Be=o(),Q=a("p"),Q.textContent=mt,Ne=o(),ee=a("ul"),ee.innerHTML=pt,je=o(),M=a("div"),p(j.$$.fragment),Fe=o(),te=a("p"),te.innerHTML=ft,Ue=o(),z=a("div"),p(F.$$.fragment),Ye=o(),ne=a("p"),ne.textContent=_t,We=o(),oe=a("div"),p(U.$$.fragment),we=o(),p(Y.$$.fragment),Me=o(),c=a("div"),p(W.$$.fragment),Ge=o(),se=a("p"),se.textContent=gt,Re=o(),re=a("p"),re.innerHTML=ut,Se=o(),x=a("div"),p(G.$$.fragment),Xe=o(),ae=a("p"),ae.textContent=ht,Ze=o(),ie=a("ul"),ie.innerHTML=kt,Je=o(),q=a("div"),p(R.$$.fragment),Ke=o(),de=a("p"),de.textContent=vt,Qe=o(),D=a("div"),p(S.$$.fragment),et=o(),le=a("p"),le.textContent=yt,tt=o(),L=a("div"),p(X.$$.fragment),nt=o(),ce=a("p"),ce.innerHTML=Tt,ze=o(),p(Z.$$.fragment),qe=o(),_e=a("p"),this.h()},l(e){const r=qt("svelte-u9bgzb",document.head);w=i(r,"META",{name:!0,content:!0}),r.forEach(n),ge=s(e),fe=i(e,"P",{}),y(fe).forEach(n),ue=s(e),C=i(e,"P",{"data-svelte-h":!0}),m(C)!=="svelte-nly2hc"&&(C.innerHTML=st),he=s(e),f(E.$$.fragment,e),ke=s(e),f(I.$$.fragment,e),ve=s(e),P=i(e,"P",{"data-svelte-h":!0}),m(P)!=="svelte-pn60ee"&&(P.innerHTML=rt),ye=s(e),H=i(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-vfdo9a"&&(H.textContent=at),Te=s(e),A=i(e,"P",{"data-svelte-h":!0}),m(A)!=="svelte-5zkjjh"&&(A.innerHTML=it),be=s(e),O=i(e,"P",{"data-svelte-h":!0}),m(O)!=="svelte-1rzej88"&&(O.innerHTML=dt),xe=s(e),f(V.$$.fragment,e),$e=s(e),l=i(e,"DIV",{class:!0});var k=y(l);f(B.$$.fragment,k),Ae=s(k),J=i(k,"P",{"data-svelte-h":!0}),m(J)!=="svelte-1sq3vog"&&(J.textContent=lt),Oe=s(k),K=i(k,"P",{"data-svelte-h":!0}),m(K)!=="svelte-ntrhio"&&(K.innerHTML=ct),Ve=s(k),b=i(k,"DIV",{class:!0});var me=y(b);f(N.$$.fragment,me),Be=s(me),Q=i(me,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-1wjq39d"&&(Q.textContent=mt),Ne=s(me),ee=i(me,"UL",{"data-svelte-h":!0}),m(ee)!=="svelte-8gh3n2"&&(ee.innerHTML=pt),me.forEach(n),je=s(k),M=i(k,"DIV",{class:!0});var Le=y(M);f(j.$$.fragment,Le),Fe=s(Le),te=i(Le,"P",{"data-svelte-h":!0}),m(te)!=="svelte-1f4f5kp"&&(te.innerHTML=ft),Le.forEach(n),Ue=s(k),z=i(k,"DIV",{class:!0});var Ce=y(z);f(F.$$.fragment,Ce),Ye=s(Ce),ne=i(Ce,"P",{"data-svelte-h":!0}),m(ne)!=="svelte-1uh0qbi"&&(ne.textContent=_t),Ce.forEach(n),We=s(k),oe=i(k,"DIV",{class:!0});var bt=y(oe);f(U.$$.fragment,bt),bt.forEach(n),k.forEach(n),we=s(e),f(Y.$$.fragment,e),Me=s(e),c=i(e,"DIV",{class:!0});var v=y(c);f(W.$$.fragment,v),Ge=s(v),se=i(v,"P",{"data-svelte-h":!0}),m(se)!=="svelte-1sq3vog"&&(se.textContent=gt),Re=s(v),re=i(v,"P",{"data-svelte-h":!0}),m(re)!=="svelte-ntrhio"&&(re.innerHTML=ut),Se=s(v),x=i(v,"DIV",{class:!0});var pe=y(x);f(G.$$.fragment,pe),Xe=s(pe),ae=i(pe,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-1wjq39d"&&(ae.textContent=ht),Ze=s(pe),ie=i(pe,"UL",{"data-svelte-h":!0}),m(ie)!=="svelte-8gh3n2"&&(ie.innerHTML=kt),pe.forEach(n),Je=s(v),q=i(v,"DIV",{class:!0});var Ee=y(q);f(R.$$.fragment,Ee),Ke=s(Ee),de=i(Ee,"P",{"data-svelte-h":!0}),m(de)!=="svelte-b3k2yi"&&(de.textContent=vt),Ee.forEach(n),Qe=s(v),D=i(v,"DIV",{class:!0});var Ie=y(D);f(S.$$.fragment,Ie),et=s(Ie),le=i(Ie,"P",{"data-svelte-h":!0}),m(le)!=="svelte-1uh0qbi"&&(le.textContent=yt),Ie.forEach(n),tt=s(v),L=i(v,"DIV",{class:!0});var Pe=y(L);f(X.$$.fragment,Pe),nt=s(Pe),ce=i(Pe,"P",{"data-svelte-h":!0}),m(ce)!=="svelte-1f4f5kp"&&(ce.innerHTML=Tt),Pe.forEach(n),v.forEach(n),ze=s(e),f(Z.$$.fragment,e),qe=s(e),_e=i(e,"P",{}),y(_e).forEach(n),this.h()},h(){T(w,"name","hf:doc:metadata"),T(w,"content",Ct),T(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,r){t(document.head,w),d(e,ge,r),d(e,fe,r),d(e,ue,r),d(e,C,r),d(e,he,r),_(E,e,r),d(e,ke,r),_(I,e,r),d(e,ve,r),d(e,P,r),d(e,ye,r),d(e,H,r),d(e,Te,r),d(e,A,r),d(e,be,r),d(e,O,r),d(e,xe,r),_(V,e,r),d(e,$e,r),d(e,l,r),_(B,l,null),t(l,Ae),t(l,J),t(l,Oe),t(l,K),t(l,Ve),t(l,b),_(N,b,null),t(b,Be),t(b,Q),t(b,Ne),t(b,ee),t(l,je),t(l,M),_(j,M,null),t(M,Fe),t(M,te),t(l,Ue),t(l,z),_(F,z,null),t(z,Ye),t(z,ne),t(l,We),t(l,oe),_(U,oe,null),d(e,we,r),_(Y,e,r),d(e,Me,r),d(e,c,r),_(W,c,null),t(c,Ge),t(c,se),t(c,Re),t(c,re),t(c,Se),t(c,x),_(G,x,null),t(x,Xe),t(x,ae),t(x,Ze),t(x,ie),t(c,Je),t(c,q),_(R,q,null),t(q,Ke),t(q,de),t(c,Qe),t(c,D),_(S,D,null),t(D,et),t(D,le),t(c,tt),t(c,L),_(X,L,null),t(L,nt),t(L,ce),d(e,ze,r),_(Z,e,r),d(e,qe,r),d(e,_e,r),De=!0},p:$t,i(e){De||(g(E.$$.fragment,e),g(I.$$.fragment,e),g(V.$$.fragment,e),g(B.$$.fragment,e),g(N.$$.fragment,e),g(j.$$.fragment,e),g(F.$$.fragment,e),g(U.$$.fragment,e),g(Y.$$.fragment,e),g(W.$$.fragment,e),g(G.$$.fragment,e),g(R.$$.fragment,e),g(S.$$.fragment,e),g(X.$$.fragment,e),g(Z.$$.fragment,e),De=!0)},o(e){u(E.$$.fragment,e),u(I.$$.fragment,e),u(V.$$.fragment,e),u(B.$$.fragment,e),u(N.$$.fragment,e),u(j.$$.fragment,e),u(F.$$.fragment,e),u(U.$$.fragment,e),u(Y.$$.fragment,e),u(W.$$.fragment,e),u(G.$$.fragment,e),u(R.$$.fragment,e),u(S.$$.fragment,e),u(X.$$.fragment,e),u(Z.$$.fragment,e),De=!1},d(e){e&&(n(ge),n(fe),n(ue),n(C),n(he),n(ke),n(ve),n(P),n(ye),n(H),n(Te),n(A),n(be),n(O),n(xe),n($e),n(l),n(we),n(Me),n(c),n(ze),n(qe),n(_e)),n(w),h(E,e),h(I,e),h(V,e),h(B),h(N),h(j),h(F),h(U),h(Y,e),h(W),h(G),h(R),h(S),h(X),h(Z,e)}}}const Ct='{"title":"myt5","local":"myt5","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"MyT5Tokenizer","local":"transformers.MyT5Tokenizer","sections":[],"depth":2},{"title":"MyT5Tokenizer","local":"transformers.MyT5Tokenizer","sections":[],"depth":2}],"depth":1}';function Et(ot){return wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ot extends Mt{constructor(w){super(),zt(this,w,Et,Lt,xt,{})}}export{Ot as component};
