import{s as Nt,z as kt,o as zt,n as ye}from"../chunks/scheduler.18a86fab.js";import{S as Ht,i as Pt,g as c,s as r,r as h,A as Gt,h as m,f as n,c as l,j as fe,x as p,u as _,k as W,y as g,a,v as b,d as v,t as $,w}from"../chunks/index.98837b22.js";import{T as mt}from"../chunks/Tip.77304350.js";import{D as Te}from"../chunks/Docstring.a1ef7999.js";import{C as ft}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as pt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as At}from"../chunks/PipelineTag.7749150e.js";import{H as we,E as Bt}from"../chunks/getInferenceSnippets.06c2775f.js";function Lt(M){let s,u="This model is in maintenance mode only, we donâ€™t accept any new PRs changing its code.",d,i,f=`If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.
You can do so by running the following command: <code>pip install -U transformers==4.30.0</code>.`;return{c(){s=c("p"),s.textContent=u,d=r(),i=c("p"),i.innerHTML=f},l(o){s=m(o,"P",{"data-svelte-h":!0}),p(s)!=="svelte-1dwyvn5"&&(s.textContent=u),d=l(o),i=m(o,"P",{"data-svelte-h":!0}),p(i)!=="svelte-4042uy"&&(i.innerHTML=f)},m(o,y){a(o,s,y),a(o,d,y),a(o,i,y)},p:ye,d(o){o&&(n(s),n(d),n(i))}}}function Rt(M){let s,u="Example:",d,i,f;return i=new ft({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZhbk1vZGVsJTJDJTIwVmFuQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZBTiUyMHZhbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFZhbkNvbmZpZygpJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMHZhbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBWYW5Nb2RlbChjb25maWd1cmF0aW9uKSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VanModel, VanConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VAN van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VanConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=c("p"),s.textContent=u,d=r(),h(i.$$.fragment)},l(o){s=m(o,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=u),d=l(o),_(i.$$.fragment,o)},m(o,y){a(o,s,y),a(o,d,y),b(i,o,y),f=!0},p:ye,i(o){f||(v(i.$$.fragment,o),f=!0)},o(o){$(i.$$.fragment,o),f=!1},d(o){o&&(n(s),n(d)),w(i,o)}}}function Et(M){let s,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=c("p"),s.innerHTML=u},l(d){s=m(d,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=u)},m(d,i){a(d,s,i)},p:ye,d(d){d&&n(s)}}}function Xt(M){let s,u="Example:",d,i,f;return i=new ft({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZhbk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJWaXN1YWwtQXR0ZW50aW9uLU5ldHdvcmslMkZ2YW4tYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMFZhbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJWaXN1YWwtQXR0ZW50aW9uLU5ldHdvcmslMkZ2YW4tYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBbGFzdF9oaWRkZW5fc3RhdGVzJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQWxpc3QobGFzdF9oaWRkZW5fc3RhdGVzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, VanModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){s=c("p"),s.textContent=u,d=r(),h(i.$$.fragment)},l(o){s=m(o,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=u),d=l(o),_(i.$$.fragment,o)},m(o,y){a(o,s,y),a(o,d,y),b(i,o,y),f=!0},p:ye,i(o){f||(v(i.$$.fragment,o),f=!0)},o(o){$(i.$$.fragment,o),f=!1},d(o){o&&(n(s),n(d)),w(i,o)}}}function Yt(M){let s,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=c("p"),s.innerHTML=u},l(d){s=m(d,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=u)},m(d,i){a(d,s,i)},p:ye,d(d){d&&n(s)}}}function Qt(M){let s,u="Example:",d,i,f;return i=new ft({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZhbkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMlZpc3VhbC1BdHRlbnRpb24tTmV0d29yayUyRnZhbi1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwVmFuRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyVmlzdWFsLUF0dGVudGlvbi1OZXR3b3JrJTJGdmFuLWJhc2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, VanForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanForImageClassification.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){s=c("p"),s.textContent=u,d=r(),h(i.$$.fragment)},l(o){s=m(o,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=u),d=l(o),_(i.$$.fragment,o)},m(o,y){a(o,s,y),a(o,d,y),b(i,o,y),f=!0},p:ye,i(o){f||(v(i.$$.fragment,o),f=!0)},o(o){$(i.$$.fragment,o),f=!1},d(o){o&&(n(s),n(d)),w(i,o)}}}function qt(M){let s,u,d,i,f,o="<em>This model was released on 2022-02-20 and added to Hugging Face Transformers on 2023-06-20.</em>",y,B,Ce,N,gt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Ve,k,xe,L,je,R,ut='The VAN model was proposed in <a href="https://huggingface.co/papers/2202.09741" rel="nofollow">Visual Attention Network</a> by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.',Ie,E,ht="This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations.",Je,X,_t="The abstract from the paper is the following:",Fe,Y,bt='<em>While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at <a href="https://github.com/Visual-Attention-Network/VAN-Classification" rel="nofollow">this https URL</a>.</em>',Ze,Q,vt="Tips:",We,q,$t="<li>VAN does not have an embedding layer, thus the <code>hidden_states</code> will have a length equal to the number of stages.</li>",Ue,S,wt='The figure below illustrates the architecture of a Visual Attention Layer. Taken from the <a href="https://huggingface.co/papers/2202.09741" rel="nofollow">original paper</a>.',Ne,D,yt,ke,O,Mt='This model was contributed by <a href="https://huggingface.co/Francesco" rel="nofollow">Francesco</a>. The original code can be found <a href="https://github.com/Visual-Attention-Network/VAN-Classification" rel="nofollow">here</a>.',ze,K,He,ee,Tt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with VAN.",Pe,te,Ge,ne,Ct='<li><a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',Ae,ae,Vt="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Be,se,Le,T,oe,De,ge,xt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanModel">VanModel</a>. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN
<a href="https://huggingface.co/Visual-Attention-Network/van-base" rel="nofollow">Visual-Attention-Network/van-base</a> architecture.`,Oe,ue,jt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ke,z,Re,re,Ee,j,le,et,he,It=`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,tt,V,ie,nt,_e,Jt='The <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanModel">VanModel</a> forward method, overrides the <code>__call__</code> special method.',at,H,st,P,Xe,de,Ye,C,ce,ot,be,Ft=`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,rt,ve,Zt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,lt,x,me,it,$e,Wt='The <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',dt,G,ct,A,Qe,pe,qe,Me,Se;return B=new we({props:{title:"VAN",local:"van",headingTag:"h1"}}),k=new mt({props:{warning:!0,$$slots:{default:[Lt]},$$scope:{ctx:M}}}),L=new we({props:{title:"Overview",local:"overview",headingTag:"h2"}}),K=new we({props:{title:"Resources",local:"resources",headingTag:"h2"}}),te=new At({props:{pipeline:"image-classification"}}),se=new we({props:{title:"VanConfig",local:"transformers.VanConfig",headingTag:"h2"}}),oe=new Te({props:{name:"class transformers.VanConfig",anchor:"transformers.VanConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3, 3]"},{name:"strides",val:" = [4, 2, 2, 2]"},{name:"hidden_sizes",val:" = [64, 128, 320, 512]"},{name:"depths",val:" = [3, 3, 12, 3]"},{name:"mlp_ratios",val:" = [8, 8, 4, 4]"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"layer_scale_init_value",val:" = 0.01"},{name:"drop_path_rate",val:" = 0.0"},{name:"dropout_rate",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VanConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VanConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VanConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3, 3]</code>) &#x2014;
Patch size to use in each stage&#x2019;s embedding layer.`,name:"patch_sizes"},{anchor:"transformers.VanConfig.strides",description:`<strong>strides</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2, 2]</code>) &#x2014;
Stride size to use in each stage&#x2019;s embedding layer to downsample the input.`,name:"strides"},{anchor:"transformers.VanConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[64, 128, 320, 512]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.VanConfig.depths",description:`<strong>depths</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 12, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.VanConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4, 4]</code>) &#x2014;
The expansion ratio for mlp layer at each stage.`,name:"mlp_ratios"},{anchor:"transformers.VanConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each layer. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VanConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VanConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VanConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.01) &#x2014;
The initial value for layer scaling.`,name:"layer_scale_init_value"},{anchor:"transformers.VanConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.VanConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for dropout.`,name:"dropout_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/van/configuration_van.py#L24"}}),z=new pt({props:{anchor:"transformers.VanConfig.example",$$slots:{default:[Rt]},$$scope:{ctx:M}}}),re=new we({props:{title:"VanModel",local:"transformers.VanModel",headingTag:"h2"}}),le=new Te({props:{name:"class transformers.VanModel",anchor:"transformers.VanModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/van/modeling_van.py#L416"}}),ie=new Te({props:{name:"forward",anchor:"transformers.VanModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor]"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VanModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.VanModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/van/modeling_van.py#L426",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new mt({props:{$$slots:{default:[Et]},$$scope:{ctx:M}}}),P=new pt({props:{anchor:"transformers.VanModel.forward.example",$$slots:{default:[Xt]},$$scope:{ctx:M}}}),de=new we({props:{title:"VanForImageClassification",local:"transformers.VanForImageClassification",headingTag:"h2"}}),ce=new Te({props:{name:"class transformers.VanForImageClassification",anchor:"transformers.VanForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/van/modeling_van.py#L471"}}),me=new Te({props:{name:"forward",anchor:"transformers.VanForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VanForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.VanForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.VanForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/van/modeling_van.py#L483",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new mt({props:{$$slots:{default:[Yt]},$$scope:{ctx:M}}}),A=new pt({props:{anchor:"transformers.VanForImageClassification.forward.example",$$slots:{default:[Qt]},$$scope:{ctx:M}}}),pe=new Bt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/van.md"}}),{c(){s=c("meta"),u=r(),d=c("p"),i=r(),f=c("p"),f.innerHTML=o,y=r(),h(B.$$.fragment),Ce=r(),N=c("div"),N.innerHTML=gt,Ve=r(),h(k.$$.fragment),xe=r(),h(L.$$.fragment),je=r(),R=c("p"),R.innerHTML=ut,Ie=r(),E=c("p"),E.textContent=ht,Je=r(),X=c("p"),X.textContent=_t,Fe=r(),Y=c("p"),Y.innerHTML=bt,Ze=r(),Q=c("p"),Q.textContent=vt,We=r(),q=c("ul"),q.innerHTML=$t,Ue=r(),S=c("p"),S.innerHTML=wt,Ne=r(),D=c("img"),ke=r(),O=c("p"),O.innerHTML=Mt,ze=r(),h(K.$$.fragment),He=r(),ee=c("p"),ee.textContent=Tt,Pe=r(),h(te.$$.fragment),Ge=r(),ne=c("ul"),ne.innerHTML=Ct,Ae=r(),ae=c("p"),ae.textContent=Vt,Be=r(),h(se.$$.fragment),Le=r(),T=c("div"),h(oe.$$.fragment),De=r(),ge=c("p"),ge.innerHTML=xt,Oe=r(),ue=c("p"),ue.innerHTML=jt,Ke=r(),h(z.$$.fragment),Re=r(),h(re.$$.fragment),Ee=r(),j=c("div"),h(le.$$.fragment),et=r(),he=c("p"),he.innerHTML=It,tt=r(),V=c("div"),h(ie.$$.fragment),nt=r(),_e=c("p"),_e.innerHTML=Jt,at=r(),h(H.$$.fragment),st=r(),h(P.$$.fragment),Xe=r(),h(de.$$.fragment),Ye=r(),C=c("div"),h(ce.$$.fragment),ot=r(),be=c("p"),be.textContent=Ft,rt=r(),ve=c("p"),ve.innerHTML=Zt,lt=r(),x=c("div"),h(me.$$.fragment),it=r(),$e=c("p"),$e.innerHTML=Wt,dt=r(),h(G.$$.fragment),ct=r(),h(A.$$.fragment),Qe=r(),h(pe.$$.fragment),qe=r(),Me=c("p"),this.h()},l(e){const t=Gt("svelte-u9bgzb",document.head);s=m(t,"META",{name:!0,content:!0}),t.forEach(n),u=l(e),d=m(e,"P",{}),fe(d).forEach(n),i=l(e),f=m(e,"P",{"data-svelte-h":!0}),p(f)!=="svelte-11lcomf"&&(f.innerHTML=o),y=l(e),_(B.$$.fragment,e),Ce=l(e),N=m(e,"DIV",{class:!0,"data-svelte-h":!0}),p(N)!=="svelte-13t8s2t"&&(N.innerHTML=gt),Ve=l(e),_(k.$$.fragment,e),xe=l(e),_(L.$$.fragment,e),je=l(e),R=m(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-nzm2ys"&&(R.innerHTML=ut),Ie=l(e),E=m(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-1k14mfv"&&(E.textContent=ht),Je=l(e),X=m(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-vfdo9a"&&(X.textContent=_t),Fe=l(e),Y=m(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-eh9hlc"&&(Y.innerHTML=bt),Ze=l(e),Q=m(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-axv494"&&(Q.textContent=vt),We=l(e),q=m(e,"UL",{"data-svelte-h":!0}),p(q)!=="svelte-159tyma"&&(q.innerHTML=$t),Ue=l(e),S=m(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-146b4x2"&&(S.innerHTML=wt),Ne=l(e),D=m(e,"IMG",{width:!0,src:!0}),ke=l(e),O=m(e,"P",{"data-svelte-h":!0}),p(O)!=="svelte-10esb2x"&&(O.innerHTML=Mt),ze=l(e),_(K.$$.fragment,e),He=l(e),ee=m(e,"P",{"data-svelte-h":!0}),p(ee)!=="svelte-1gzppm8"&&(ee.textContent=Tt),Pe=l(e),_(te.$$.fragment,e),Ge=l(e),ne=m(e,"UL",{"data-svelte-h":!0}),p(ne)!=="svelte-h8guki"&&(ne.innerHTML=Ct),Ae=l(e),ae=m(e,"P",{"data-svelte-h":!0}),p(ae)!=="svelte-1xesile"&&(ae.textContent=Vt),Be=l(e),_(se.$$.fragment,e),Le=l(e),T=m(e,"DIV",{class:!0});var I=fe(T);_(oe.$$.fragment,I),De=l(I),ge=m(I,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-itu1g0"&&(ge.innerHTML=xt),Oe=l(I),ue=m(I,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-1ek1ss9"&&(ue.innerHTML=jt),Ke=l(I),_(z.$$.fragment,I),I.forEach(n),Re=l(e),_(re.$$.fragment,e),Ee=l(e),j=m(e,"DIV",{class:!0});var U=fe(j);_(le.$$.fragment,U),et=l(U),he=m(U,"P",{"data-svelte-h":!0}),p(he)!=="svelte-16mr62j"&&(he.innerHTML=It),tt=l(U),V=m(U,"DIV",{class:!0});var J=fe(V);_(ie.$$.fragment,J),nt=l(J),_e=m(J,"P",{"data-svelte-h":!0}),p(_e)!=="svelte-12wzxz0"&&(_e.innerHTML=Jt),at=l(J),_(H.$$.fragment,J),st=l(J),_(P.$$.fragment,J),J.forEach(n),U.forEach(n),Xe=l(e),_(de.$$.fragment,e),Ye=l(e),C=m(e,"DIV",{class:!0});var F=fe(C);_(ce.$$.fragment,F),ot=l(F),be=m(F,"P",{"data-svelte-h":!0}),p(be)!=="svelte-lxjyb1"&&(be.textContent=Ft),rt=l(F),ve=m(F,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1gjh92c"&&(ve.innerHTML=Zt),lt=l(F),x=m(F,"DIV",{class:!0});var Z=fe(x);_(me.$$.fragment,Z),it=l(Z),$e=m(Z,"P",{"data-svelte-h":!0}),p($e)!=="svelte-1gb242c"&&($e.innerHTML=Wt),dt=l(Z),_(G.$$.fragment,Z),ct=l(Z),_(A.$$.fragment,Z),Z.forEach(n),F.forEach(n),Qe=l(e),_(pe.$$.fragment,e),qe=l(e),Me=m(e,"P",{}),fe(Me).forEach(n),this.h()},h(){W(s,"name","hf:doc:metadata"),W(s,"content",St),W(N,"class","flex flex-wrap space-x-1"),W(D,"width","600"),kt(D.src,yt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png")||W(D,"src",yt),W(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){g(document.head,s),a(e,u,t),a(e,d,t),a(e,i,t),a(e,f,t),a(e,y,t),b(B,e,t),a(e,Ce,t),a(e,N,t),a(e,Ve,t),b(k,e,t),a(e,xe,t),b(L,e,t),a(e,je,t),a(e,R,t),a(e,Ie,t),a(e,E,t),a(e,Je,t),a(e,X,t),a(e,Fe,t),a(e,Y,t),a(e,Ze,t),a(e,Q,t),a(e,We,t),a(e,q,t),a(e,Ue,t),a(e,S,t),a(e,Ne,t),a(e,D,t),a(e,ke,t),a(e,O,t),a(e,ze,t),b(K,e,t),a(e,He,t),a(e,ee,t),a(e,Pe,t),b(te,e,t),a(e,Ge,t),a(e,ne,t),a(e,Ae,t),a(e,ae,t),a(e,Be,t),b(se,e,t),a(e,Le,t),a(e,T,t),b(oe,T,null),g(T,De),g(T,ge),g(T,Oe),g(T,ue),g(T,Ke),b(z,T,null),a(e,Re,t),b(re,e,t),a(e,Ee,t),a(e,j,t),b(le,j,null),g(j,et),g(j,he),g(j,tt),g(j,V),b(ie,V,null),g(V,nt),g(V,_e),g(V,at),b(H,V,null),g(V,st),b(P,V,null),a(e,Xe,t),b(de,e,t),a(e,Ye,t),a(e,C,t),b(ce,C,null),g(C,ot),g(C,be),g(C,rt),g(C,ve),g(C,lt),g(C,x),b(me,x,null),g(x,it),g(x,$e),g(x,dt),b(G,x,null),g(x,ct),b(A,x,null),a(e,Qe,t),b(pe,e,t),a(e,qe,t),a(e,Me,t),Se=!0},p(e,[t]){const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),k.$set(I);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),z.$set(U);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),H.$set(J);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),P.$set(F);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),G.$set(Z);const Ut={};t&2&&(Ut.$$scope={dirty:t,ctx:e}),A.$set(Ut)},i(e){Se||(v(B.$$.fragment,e),v(k.$$.fragment,e),v(L.$$.fragment,e),v(K.$$.fragment,e),v(te.$$.fragment,e),v(se.$$.fragment,e),v(oe.$$.fragment,e),v(z.$$.fragment,e),v(re.$$.fragment,e),v(le.$$.fragment,e),v(ie.$$.fragment,e),v(H.$$.fragment,e),v(P.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(me.$$.fragment,e),v(G.$$.fragment,e),v(A.$$.fragment,e),v(pe.$$.fragment,e),Se=!0)},o(e){$(B.$$.fragment,e),$(k.$$.fragment,e),$(L.$$.fragment,e),$(K.$$.fragment,e),$(te.$$.fragment,e),$(se.$$.fragment,e),$(oe.$$.fragment,e),$(z.$$.fragment,e),$(re.$$.fragment,e),$(le.$$.fragment,e),$(ie.$$.fragment,e),$(H.$$.fragment,e),$(P.$$.fragment,e),$(de.$$.fragment,e),$(ce.$$.fragment,e),$(me.$$.fragment,e),$(G.$$.fragment,e),$(A.$$.fragment,e),$(pe.$$.fragment,e),Se=!1},d(e){e&&(n(u),n(d),n(i),n(f),n(y),n(Ce),n(N),n(Ve),n(xe),n(je),n(R),n(Ie),n(E),n(Je),n(X),n(Fe),n(Y),n(Ze),n(Q),n(We),n(q),n(Ue),n(S),n(Ne),n(D),n(ke),n(O),n(ze),n(He),n(ee),n(Pe),n(Ge),n(ne),n(Ae),n(ae),n(Be),n(Le),n(T),n(Re),n(Ee),n(j),n(Xe),n(Ye),n(C),n(Qe),n(qe),n(Me)),n(s),w(B,e),w(k,e),w(L,e),w(K,e),w(te,e),w(se,e),w(oe),w(z),w(re,e),w(le),w(ie),w(H),w(P),w(de,e),w(ce),w(me),w(G),w(A),w(pe,e)}}}const St='{"title":"VAN","local":"van","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"VanConfig","local":"transformers.VanConfig","sections":[],"depth":2},{"title":"VanModel","local":"transformers.VanModel","sections":[],"depth":2},{"title":"VanForImageClassification","local":"transformers.VanForImageClassification","sections":[],"depth":2}],"depth":1}';function Dt(M){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rn extends Ht{constructor(s){super(),Pt(this,s,Dt,qt,Nt,{})}}export{rn as component};
