import{s as Yt,o as Qt,n as be}from"../chunks/scheduler.18a86fab.js";import{S as At,i as qt,g as m,s as a,r as g,A as Ot,h as p,f as s,c as r,j as B,x as f,u,k as P,y as c,a as i,v as b,d as _,t as T,w as M}from"../chunks/index.98837b22.js";import{T as yt}from"../chunks/Tip.77304350.js";import{D as ge}from"../chunks/Docstring.a1ef7999.js";import{C as et}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ke}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ue,E as Dt}from"../chunks/getInferenceSnippets.06c2775f.js";function Kt($){let t,v="Example:",l,d,h;return d=new et({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1vYmlsZVZpVFYyQ29uZmlnJTJDJTIwTW9iaWxlVmlUVjJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2JpbGV2aXR2Mi1zbWFsbCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBNb2JpbGVWaVRWMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMG1vYmlsZXZpdHYyLXNtYWxsJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MobileViTV2Config, MobileViTV2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a mobilevitv2-small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MobileViTV2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the mobilevitv2-small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=v,l=a(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=v),l=r(o),u(d.$$.fragment,o)},m(o,V){i(o,t,V),i(o,l,V),b(d,o,V),h=!0},p:be,i(o){h||(_(d.$$.fragment,o),h=!0)},o(o){T(d.$$.fragment,o),h=!1},d(o){o&&(s(t),s(l)),M(d,o)}}}function eo($){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=v},l(l){t=p(l,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(l,d){i(l,t,d)},p:be,d(l){l&&s(t)}}}function to($){let t,v="Example:",l,d,h;return d=new et({props:{code:"",highlighted:"",wrap:!1}}),{c(){t=m("p"),t.textContent=v,l=a(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=v),l=r(o),u(d.$$.fragment,o)},m(o,V){i(o,t,V),i(o,l,V),b(d,o,V),h=!0},p:be,i(o){h||(_(d.$$.fragment,o),h=!0)},o(o){T(d.$$.fragment,o),h=!1},d(o){o&&(s(t),s(l)),M(d,o)}}}function oo($){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=v},l(l){t=p(l,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(l,d){i(l,t,d)},p:be,d(l){l&&s(t)}}}function no($){let t,v="Example:",l,d,h;return d=new et({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME1vYmlsZVZpVFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZtb2JpbGV2aXR2Mi0xLjAlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmFwcGxlJTJGbW9iaWxldml0djItMS4wJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MobileViTV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){t=m("p"),t.textContent=v,l=a(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=v),l=r(o),u(d.$$.fragment,o)},m(o,V){i(o,t,V),i(o,l,V),b(d,o,V),h=!0},p:be,i(o){h||(_(d.$$.fragment,o),h=!0)},o(o){T(d.$$.fragment,o),h=!1},d(o){o&&(s(t),s(l)),M(d,o)}}}function so($){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=v},l(l){t=p(l,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(l,d){i(l,t,d)},p:be,d(l){l&&s(t)}}}function ao($){let t,v="Examples:",l,d,h;return d=new et({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMkMlMjBNb2JpbGVWaVRWMkZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZtb2JpbGV2aXR2Mi0xLjAtaW1hZ2VuZXQxay0yNTYlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMkZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJhcHBsZSUyRm1vYmlsZXZpdHYyLTEuMC1pbWFnZW5ldDFrLTI1NiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwbG9naXRzJTIwYXJlJTIwb2YlMjBzaGFwZSUyMChiYXRjaF9zaXplJTJDJTIwbnVtX2xhYmVscyUyQyUyMGhlaWdodCUyQyUyMHdpZHRoKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MobileViTV2ForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2ForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># logits are of shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=m("p"),t.textContent=v,l=a(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=v),l=r(o),u(d.$$.fragment,o)},m(o,V){i(o,t,V),i(o,l,V),b(d,o,V),h=!0},p:be,i(o){h||(_(d.$$.fragment,o),h=!0)},o(o){T(d.$$.fragment,o),h=!1},d(o){o&&(s(t),s(l)),M(d,o)}}}function ro($){let t,v,l,d,h,o="<em>This model was released on 2022-06-06 and added to Hugging Face Transformers on 2023-06-02.</em>",V,Q,ke,N,Ct='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Ue,A,Ze,q,xt='The MobileViTV2 model was proposed in <a href="https://huggingface.co/papers/2206.02680" rel="nofollow">Separable Self-attention for Mobile Vision Transformers</a> by Sachin Mehta and Mohammad Rastegari.',Se,O,jt="MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.",ze,D,It="The abstract from the paper is the following:",Pe,K,Jt="<em>Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires O(k2) time complexity with respect to the number of tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTV2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTV2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running 3.2× faster on a mobile device.</em>",Be,ee,Wt=`This model was contributed by <a href="https://huggingface.co/shehan97" rel="nofollow">shehan97</a>.
The original code can be found <a href="https://github.com/apple/ml-cvnets" rel="nofollow">here</a>.`,Ne,te,He,oe,Ft='<li>MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map.</li> <li>One can use <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).</li> <li>The available image classification checkpoints are pre-trained on <a href="https://huggingface.co/datasets/imagenet-1k" rel="nofollow">ImageNet-1k</a> (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).</li> <li>The segmentation model uses a <a href="https://huggingface.co/papers/1706.05587" rel="nofollow">DeepLabV3</a> head. The available semantic segmentation checkpoints are pre-trained on <a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="nofollow">PASCAL VOC</a>.</li>',Ge,ne,Le,x,se,tt,_e,kt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a>. It is used to instantiate a
MobileViTV2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the MobileViTV2
<a href="https://huggingface.co/apple/mobilevitv2-1.0" rel="nofollow">apple/mobilevitv2-1.0</a> architecture.`,ot,Te,Ut=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,nt,H,Re,ae,Xe,w,re,st,Me,Zt="The bare Mobilevitv2 Model outputting raw hidden-states without any specific head on top.",at,ve,St=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,rt,Ve,zt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,it,W,ie,lt,$e,Pt='The <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> forward method, overrides the <code>__call__</code> special method.',dt,G,ct,L,Ee,le,Ye,y,de,mt,we,Bt=`MobileViTV2 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,pt,ye,Nt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ft,Ce,Ht=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ht,F,ce,gt,xe,Gt='The <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',ut,R,bt,X,Qe,me,Ae,C,pe,_t,je,Lt="MobileViTV2 model with a semantic segmentation head on top, e.g. for Pascal VOC.",Tt,Ie,Rt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Mt,Je,Xt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,vt,k,fe,Vt,We,Et='The <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',$t,E,wt,Y,qe,he,Oe,Fe,De;return Q=new ue({props:{title:"MobileViTV2",local:"mobilevitv2",headingTag:"h1"}}),A=new ue({props:{title:"Overview",local:"overview",headingTag:"h2"}}),te=new ue({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),ne=new ue({props:{title:"MobileViTV2Config",local:"transformers.MobileViTV2Config",headingTag:"h2"}}),se=new ge({props:{name:"class transformers.MobileViTV2Config",anchor:"transformers.MobileViTV2Config",parameters:[{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 256"},{name:"patch_size",val:" = 2"},{name:"expand_ratio",val:" = 2.0"},{name:"hidden_act",val:" = 'swish'"},{name:"conv_kernel_size",val:" = 3"},{name:"output_stride",val:" = 32"},{name:"classifier_dropout_prob",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"aspp_out_channels",val:" = 512"},{name:"atrous_rates",val:" = [6, 12, 18]"},{name:"aspp_dropout_prob",val:" = 0.1"},{name:"semantic_loss_ignore_index",val:" = 255"},{name:"n_attn_blocks",val:" = [2, 4, 3]"},{name:"base_attn_unit_dims",val:" = [128, 192, 256]"},{name:"width_multiplier",val:" = 1.0"},{name:"ffn_multiplier",val:" = 2"},{name:"attn_dropout",val:" = 0.0"},{name:"ffn_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MobileViTV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.MobileViTV2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.MobileViTV2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.MobileViTV2Config.expand_ratio",description:`<strong>expand_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Expansion factor for the MobileNetv2 layers.`,name:"expand_ratio"},{anchor:"transformers.MobileViTV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;swish&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the Transformer encoder and convolution layers.`,name:"hidden_act"},{anchor:"transformers.MobileViTV2Config.conv_kernel_size",description:`<strong>conv_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The size of the convolutional kernel in the MobileViTV2 layer.`,name:"conv_kernel_size"},{anchor:"transformers.MobileViTV2Config.output_stride",description:`<strong>output_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The ratio of the spatial resolution of the output to the resolution of the input image.`,name:"output_stride"},{anchor:"transformers.MobileViTV2Config.classifier_dropout_prob",description:`<strong>classifier_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for attached classifiers.`,name:"classifier_dropout_prob"},{anchor:"transformers.MobileViTV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MobileViTV2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MobileViTV2Config.aspp_out_channels",description:`<strong>aspp_out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Number of output channels used in the ASPP layer for semantic segmentation.`,name:"aspp_out_channels"},{anchor:"transformers.MobileViTV2Config.atrous_rates",description:`<strong>atrous_rates</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[6, 12, 18]</code>) &#x2014;
Dilation (atrous) factors used in the ASPP layer for semantic segmentation.`,name:"atrous_rates"},{anchor:"transformers.MobileViTV2Config.aspp_dropout_prob",description:`<strong>aspp_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the ASPP layer for semantic segmentation.`,name:"aspp_dropout_prob"},{anchor:"transformers.MobileViTV2Config.semantic_loss_ignore_index",description:`<strong>semantic_loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function of the semantic segmentation model.`,name:"semantic_loss_ignore_index"},{anchor:"transformers.MobileViTV2Config.n_attn_blocks",description:`<strong>n_attn_blocks</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 4, 3]</code>) &#x2014;
The number of attention blocks in each MobileViTV2Layer`,name:"n_attn_blocks"},{anchor:"transformers.MobileViTV2Config.base_attn_unit_dims",description:`<strong>base_attn_unit_dims</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[128, 192, 256]</code>) &#x2014;
The base multiplier for dimensions of attention blocks in each MobileViTV2Layer`,name:"base_attn_unit_dims"},{anchor:"transformers.MobileViTV2Config.width_multiplier",description:`<strong>width_multiplier</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The width multiplier for MobileViTV2.`,name:"width_multiplier"},{anchor:"transformers.MobileViTV2Config.ffn_multiplier",description:`<strong>ffn_multiplier</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The FFN multiplier for MobileViTV2.`,name:"ffn_multiplier"},{anchor:"transformers.MobileViTV2Config.attn_dropout",description:`<strong>attn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout in the attention layer.`,name:"attn_dropout"},{anchor:"transformers.MobileViTV2Config.ffn_dropout",description:`<strong>ffn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout between FFN layers.`,name:"ffn_dropout"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/configuration_mobilevitv2.py#L30"}}),H=new Ke({props:{anchor:"transformers.MobileViTV2Config.example",$$slots:{default:[Kt]},$$scope:{ctx:$}}}),ae=new ue({props:{title:"MobileViTV2Model",local:"transformers.MobileViTV2Model",headingTag:"h2"}}),re=new ge({props:{name:"class transformers.MobileViTV2Model",anchor:"transformers.MobileViTV2Model",parameters:[{name:"config",val:": MobileViTV2Config"},{name:"expand_output",val:": bool = True"}],parametersDescription:[{anchor:"transformers.MobileViTV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.MobileViTV2Model.expand_output",description:`<strong>expand_output</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to expand the output of the model. If <code>True</code>, the model will output pooled features in addition to
hidden states. If <code>False</code>, only the hidden states will be returned.`,name:"expand_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L593"}}),ie=new ge({props:{name:"forward",anchor:"transformers.MobileViTV2Model.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MobileViTV2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new yt({props:{$$slots:{default:[eo]},$$scope:{ctx:$}}}),L=new Ke({props:{anchor:"transformers.MobileViTV2Model.forward.example",$$slots:{default:[to]},$$scope:{ctx:$}}}),le=new ue({props:{title:"MobileViTV2ForImageClassification",local:"transformers.MobileViTV2ForImageClassification",headingTag:"h2"}}),de=new ge({props:{name:"class transformers.MobileViTV2ForImageClassification",anchor:"transformers.MobileViTV2ForImageClassification",parameters:[{name:"config",val:": MobileViTV2Config"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L681"}}),ce=new ge({props:{name:"forward",anchor:"transformers.MobileViTV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss). If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L699",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new yt({props:{$$slots:{default:[oo]},$$scope:{ctx:$}}}),X=new Ke({props:{anchor:"transformers.MobileViTV2ForImageClassification.forward.example",$$slots:{default:[no]},$$scope:{ctx:$}}}),me=new ue({props:{title:"MobileViTV2ForSemanticSegmentation",local:"transformers.MobileViTV2ForSemanticSegmentation",headingTag:"h2"}}),pe=new ge({props:{name:"class transformers.MobileViTV2ForSemanticSegmentation",anchor:"transformers.MobileViTV2ForSemanticSegmentation",parameters:[{name:"config",val:": MobileViTV2Config"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L874"}}),fe=new ge({props:{name:"forward",anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L885",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) — Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new yt({props:{$$slots:{default:[so]},$$scope:{ctx:$}}}),Y=new Ke({props:{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.example",$$slots:{default:[ao]},$$scope:{ctx:$}}}),he=new Dt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mobilevitv2.md"}}),{c(){t=m("meta"),v=a(),l=m("p"),d=a(),h=m("p"),h.innerHTML=o,V=a(),g(Q.$$.fragment),ke=a(),N=m("div"),N.innerHTML=Ct,Ue=a(),g(A.$$.fragment),Ze=a(),q=m("p"),q.innerHTML=xt,Se=a(),O=m("p"),O.textContent=jt,ze=a(),D=m("p"),D.textContent=It,Pe=a(),K=m("p"),K.innerHTML=Jt,Be=a(),ee=m("p"),ee.innerHTML=Wt,Ne=a(),g(te.$$.fragment),He=a(),oe=m("ul"),oe.innerHTML=Ft,Ge=a(),g(ne.$$.fragment),Le=a(),x=m("div"),g(se.$$.fragment),tt=a(),_e=m("p"),_e.innerHTML=kt,ot=a(),Te=m("p"),Te.innerHTML=Ut,nt=a(),g(H.$$.fragment),Re=a(),g(ae.$$.fragment),Xe=a(),w=m("div"),g(re.$$.fragment),st=a(),Me=m("p"),Me.textContent=Zt,at=a(),ve=m("p"),ve.innerHTML=St,rt=a(),Ve=m("p"),Ve.innerHTML=zt,it=a(),W=m("div"),g(ie.$$.fragment),lt=a(),$e=m("p"),$e.innerHTML=Pt,dt=a(),g(G.$$.fragment),ct=a(),g(L.$$.fragment),Ee=a(),g(le.$$.fragment),Ye=a(),y=m("div"),g(de.$$.fragment),mt=a(),we=m("p"),we.textContent=Bt,pt=a(),ye=m("p"),ye.innerHTML=Nt,ft=a(),Ce=m("p"),Ce.innerHTML=Ht,ht=a(),F=m("div"),g(ce.$$.fragment),gt=a(),xe=m("p"),xe.innerHTML=Gt,ut=a(),g(R.$$.fragment),bt=a(),g(X.$$.fragment),Qe=a(),g(me.$$.fragment),Ae=a(),C=m("div"),g(pe.$$.fragment),_t=a(),je=m("p"),je.textContent=Lt,Tt=a(),Ie=m("p"),Ie.innerHTML=Rt,Mt=a(),Je=m("p"),Je.innerHTML=Xt,vt=a(),k=m("div"),g(fe.$$.fragment),Vt=a(),We=m("p"),We.innerHTML=Et,$t=a(),g(E.$$.fragment),wt=a(),g(Y.$$.fragment),qe=a(),g(he.$$.fragment),Oe=a(),Fe=m("p"),this.h()},l(e){const n=Ot("svelte-u9bgzb",document.head);t=p(n,"META",{name:!0,content:!0}),n.forEach(s),v=r(e),l=p(e,"P",{}),B(l).forEach(s),d=r(e),h=p(e,"P",{"data-svelte-h":!0}),f(h)!=="svelte-20amqn"&&(h.innerHTML=o),V=r(e),u(Q.$$.fragment,e),ke=r(e),N=p(e,"DIV",{class:!0,"data-svelte-h":!0}),f(N)!=="svelte-13t8s2t"&&(N.innerHTML=Ct),Ue=r(e),u(A.$$.fragment,e),Ze=r(e),q=p(e,"P",{"data-svelte-h":!0}),f(q)!=="svelte-kiddb8"&&(q.innerHTML=xt),Se=r(e),O=p(e,"P",{"data-svelte-h":!0}),f(O)!=="svelte-1eurxj7"&&(O.textContent=jt),ze=r(e),D=p(e,"P",{"data-svelte-h":!0}),f(D)!=="svelte-vfdo9a"&&(D.textContent=It),Pe=r(e),K=p(e,"P",{"data-svelte-h":!0}),f(K)!=="svelte-1xe01bc"&&(K.innerHTML=Jt),Be=r(e),ee=p(e,"P",{"data-svelte-h":!0}),f(ee)!=="svelte-rz3bb6"&&(ee.innerHTML=Wt),Ne=r(e),u(te.$$.fragment,e),He=r(e),oe=p(e,"UL",{"data-svelte-h":!0}),f(oe)!=="svelte-1p0j62i"&&(oe.innerHTML=Ft),Ge=r(e),u(ne.$$.fragment,e),Le=r(e),x=p(e,"DIV",{class:!0});var U=B(x);u(se.$$.fragment,U),tt=r(U),_e=p(U,"P",{"data-svelte-h":!0}),f(_e)!=="svelte-1a1p0bs"&&(_e.innerHTML=kt),ot=r(U),Te=p(U,"P",{"data-svelte-h":!0}),f(Te)!=="svelte-1ek1ss9"&&(Te.innerHTML=Ut),nt=r(U),u(H.$$.fragment,U),U.forEach(s),Re=r(e),u(ae.$$.fragment,e),Xe=r(e),w=p(e,"DIV",{class:!0});var j=B(w);u(re.$$.fragment,j),st=r(j),Me=p(j,"P",{"data-svelte-h":!0}),f(Me)!=="svelte-m8mqp1"&&(Me.textContent=Zt),at=r(j),ve=p(j,"P",{"data-svelte-h":!0}),f(ve)!=="svelte-q52n56"&&(ve.innerHTML=St),rt=r(j),Ve=p(j,"P",{"data-svelte-h":!0}),f(Ve)!=="svelte-hswkmf"&&(Ve.innerHTML=zt),it=r(j),W=p(j,"DIV",{class:!0});var Z=B(W);u(ie.$$.fragment,Z),lt=r(Z),$e=p(Z,"P",{"data-svelte-h":!0}),f($e)!=="svelte-1m8q8u0"&&($e.innerHTML=Pt),dt=r(Z),u(G.$$.fragment,Z),ct=r(Z),u(L.$$.fragment,Z),Z.forEach(s),j.forEach(s),Ee=r(e),u(le.$$.fragment,e),Ye=r(e),y=p(e,"DIV",{class:!0});var I=B(y);u(de.$$.fragment,I),mt=r(I),we=p(I,"P",{"data-svelte-h":!0}),f(we)!=="svelte-1ojd07h"&&(we.textContent=Bt),pt=r(I),ye=p(I,"P",{"data-svelte-h":!0}),f(ye)!=="svelte-q52n56"&&(ye.innerHTML=Nt),ft=r(I),Ce=p(I,"P",{"data-svelte-h":!0}),f(Ce)!=="svelte-hswkmf"&&(Ce.innerHTML=Ht),ht=r(I),F=p(I,"DIV",{class:!0});var S=B(F);u(ce.$$.fragment,S),gt=r(S),xe=p(S,"P",{"data-svelte-h":!0}),f(xe)!=="svelte-lve52g"&&(xe.innerHTML=Gt),ut=r(S),u(R.$$.fragment,S),bt=r(S),u(X.$$.fragment,S),S.forEach(s),I.forEach(s),Qe=r(e),u(me.$$.fragment,e),Ae=r(e),C=p(e,"DIV",{class:!0});var J=B(C);u(pe.$$.fragment,J),_t=r(J),je=p(J,"P",{"data-svelte-h":!0}),f(je)!=="svelte-w9c1ym"&&(je.textContent=Lt),Tt=r(J),Ie=p(J,"P",{"data-svelte-h":!0}),f(Ie)!=="svelte-q52n56"&&(Ie.innerHTML=Rt),Mt=r(J),Je=p(J,"P",{"data-svelte-h":!0}),f(Je)!=="svelte-hswkmf"&&(Je.innerHTML=Xt),vt=r(J),k=p(J,"DIV",{class:!0});var z=B(k);u(fe.$$.fragment,z),Vt=r(z),We=p(z,"P",{"data-svelte-h":!0}),f(We)!=="svelte-myqs2g"&&(We.innerHTML=Et),$t=r(z),u(E.$$.fragment,z),wt=r(z),u(Y.$$.fragment,z),z.forEach(s),J.forEach(s),qe=r(e),u(he.$$.fragment,e),Oe=r(e),Fe=p(e,"P",{}),B(Fe).forEach(s),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",io),P(N,"class","flex flex-wrap space-x-1"),P(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){c(document.head,t),i(e,v,n),i(e,l,n),i(e,d,n),i(e,h,n),i(e,V,n),b(Q,e,n),i(e,ke,n),i(e,N,n),i(e,Ue,n),b(A,e,n),i(e,Ze,n),i(e,q,n),i(e,Se,n),i(e,O,n),i(e,ze,n),i(e,D,n),i(e,Pe,n),i(e,K,n),i(e,Be,n),i(e,ee,n),i(e,Ne,n),b(te,e,n),i(e,He,n),i(e,oe,n),i(e,Ge,n),b(ne,e,n),i(e,Le,n),i(e,x,n),b(se,x,null),c(x,tt),c(x,_e),c(x,ot),c(x,Te),c(x,nt),b(H,x,null),i(e,Re,n),b(ae,e,n),i(e,Xe,n),i(e,w,n),b(re,w,null),c(w,st),c(w,Me),c(w,at),c(w,ve),c(w,rt),c(w,Ve),c(w,it),c(w,W),b(ie,W,null),c(W,lt),c(W,$e),c(W,dt),b(G,W,null),c(W,ct),b(L,W,null),i(e,Ee,n),b(le,e,n),i(e,Ye,n),i(e,y,n),b(de,y,null),c(y,mt),c(y,we),c(y,pt),c(y,ye),c(y,ft),c(y,Ce),c(y,ht),c(y,F),b(ce,F,null),c(F,gt),c(F,xe),c(F,ut),b(R,F,null),c(F,bt),b(X,F,null),i(e,Qe,n),b(me,e,n),i(e,Ae,n),i(e,C,n),b(pe,C,null),c(C,_t),c(C,je),c(C,Tt),c(C,Ie),c(C,Mt),c(C,Je),c(C,vt),c(C,k),b(fe,k,null),c(k,Vt),c(k,We),c(k,$t),b(E,k,null),c(k,wt),b(Y,k,null),i(e,qe,n),b(he,e,n),i(e,Oe,n),i(e,Fe,n),De=!0},p(e,[n]){const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),H.$set(U);const j={};n&2&&(j.$$scope={dirty:n,ctx:e}),G.$set(j);const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),L.$set(Z);const I={};n&2&&(I.$$scope={dirty:n,ctx:e}),R.$set(I);const S={};n&2&&(S.$$scope={dirty:n,ctx:e}),X.$set(S);const J={};n&2&&(J.$$scope={dirty:n,ctx:e}),E.$set(J);const z={};n&2&&(z.$$scope={dirty:n,ctx:e}),Y.$set(z)},i(e){De||(_(Q.$$.fragment,e),_(A.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(H.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(G.$$.fragment,e),_(L.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(R.$$.fragment,e),_(X.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(fe.$$.fragment,e),_(E.$$.fragment,e),_(Y.$$.fragment,e),_(he.$$.fragment,e),De=!0)},o(e){T(Q.$$.fragment,e),T(A.$$.fragment,e),T(te.$$.fragment,e),T(ne.$$.fragment,e),T(se.$$.fragment,e),T(H.$$.fragment,e),T(ae.$$.fragment,e),T(re.$$.fragment,e),T(ie.$$.fragment,e),T(G.$$.fragment,e),T(L.$$.fragment,e),T(le.$$.fragment,e),T(de.$$.fragment,e),T(ce.$$.fragment,e),T(R.$$.fragment,e),T(X.$$.fragment,e),T(me.$$.fragment,e),T(pe.$$.fragment,e),T(fe.$$.fragment,e),T(E.$$.fragment,e),T(Y.$$.fragment,e),T(he.$$.fragment,e),De=!1},d(e){e&&(s(v),s(l),s(d),s(h),s(V),s(ke),s(N),s(Ue),s(Ze),s(q),s(Se),s(O),s(ze),s(D),s(Pe),s(K),s(Be),s(ee),s(Ne),s(He),s(oe),s(Ge),s(Le),s(x),s(Re),s(Xe),s(w),s(Ee),s(Ye),s(y),s(Qe),s(Ae),s(C),s(qe),s(Oe),s(Fe)),s(t),M(Q,e),M(A,e),M(te,e),M(ne,e),M(se),M(H),M(ae,e),M(re),M(ie),M(G),M(L),M(le,e),M(de),M(ce),M(R),M(X),M(me,e),M(pe),M(fe),M(E),M(Y),M(he,e)}}}const io='{"title":"MobileViTV2","local":"mobilevitv2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"MobileViTV2Config","local":"transformers.MobileViTV2Config","sections":[],"depth":2},{"title":"MobileViTV2Model","local":"transformers.MobileViTV2Model","sections":[],"depth":2},{"title":"MobileViTV2ForImageClassification","local":"transformers.MobileViTV2ForImageClassification","sections":[],"depth":2},{"title":"MobileViTV2ForSemanticSegmentation","local":"transformers.MobileViTV2ForSemanticSegmentation","sections":[],"depth":2}],"depth":1}';function lo($){return Qt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bo extends At{constructor(t){super(),qt(this,t,lo,ro,Yt,{})}}export{bo as component};
