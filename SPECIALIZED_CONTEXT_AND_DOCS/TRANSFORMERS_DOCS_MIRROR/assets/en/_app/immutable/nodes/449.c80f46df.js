import{s as Et,o as Ut,n as Ne}from"../chunks/scheduler.18a86fab.js";import{S as Zt,i as kt,g as i,s as a,r as f,A as Ct,h as d,f as o,c as r,j as R,x as h,u,k as P,l as $t,y as c,a as s,v as g,d as _,t as y,w as J}from"../chunks/index.98837b22.js";import{T as jt}from"../chunks/Tip.77304350.js";import{D as Je}from"../chunks/Docstring.a1ef7999.js";import{C as He}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Vt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Me,E as Pt}from"../chunks/getInferenceSnippets.06c2775f.js";function Wt(k){let n,b="Example:",p,m,M;return m=new He({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZKRVBBMkNvbmZpZyUyQyUyMFZKRVBBMk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZKRVBBMiUyMHZqZXBhMi12aXRsLWZwYzY0LTI1NiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBWSkVQQTJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwdmplcGEyLXZpdGwtZnBjNjQtMjU2JTIwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBWSkVQQTJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VJEPA2Config, VJEPA2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VJEPA2 vjepa2-vitl-fpc64-256 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VJEPA2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the vjepa2-vitl-fpc64-256  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VJEPA2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=i("p"),n.textContent=b,p=a(),f(m.$$.fragment)},l(l){n=d(l,"P",{"data-svelte-h":!0}),h(n)!=="svelte-11lpom8"&&(n.textContent=b),p=r(l),u(m.$$.fragment,l)},m(l,E){s(l,n,E),s(l,p,E),g(m,l,E),M=!0},p:Ne,i(l){M||(_(m.$$.fragment,l),M=!0)},o(l){y(m.$$.fragment,l),M=!1},d(l){l&&(o(n),o(p)),J(m,l)}}}function At(k){let n,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=i("p"),n.innerHTML=b},l(p){n=d(p,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=b)},m(p,m){s(p,n,m)},p:Ne,d(p){p&&o(n)}}}function It(k){let n,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=i("p"),n.innerHTML=b},l(p){n=d(p,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=b)},m(p,m){s(p,n,m)},p:Ne,d(p){p&&o(n)}}}function xt(k){let n,b="Examples:",p,m,M;return m=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjBudW1weSUyMGFzJTIwbnAlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1ZpZGVvUHJvY2Vzc29yJTJDJTIwVkpFUEEyRm9yVmlkZW9DbGFzc2lmaWNhdGlvbiUwQSUwQWRldmljZSUyMCUzRCUyMCUyMmN1ZGElMjIlMEElMEF2aWRlb19wcm9jZXNzb3IlMjAlM0QlMjBBdXRvVmlkZW9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGdmplcGEyLXZpdGwtZnBjMTYtMjU2LXNzdjIlMjIpJTBBbW9kZWwlMjAlM0QlMjBWSkVQQTJGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZqZXBhMi12aXRsLWZwYzE2LTI1Ni1zc3YyJTIyKS50byhkZXZpY2UpJTBBJTBBdmlkZW8lMjAlM0QlMjBucC5vbmVzKCg2NCUyQyUyMDI1NiUyQyUyMDI1NiUyQyUyMDMpKSUyMCUyMCUyMyUyMDY0JTIwZnJhbWVzJTJDJTIwMjU2eDI1NiUyMFJHQiUwQWlucHV0cyUyMCUzRCUyMHZpZGVvX3Byb2Nlc3Nvcih2aWRlbyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSklMEElMEElMjMlMjBGb3IlMjBpbmZlcmVuY2UlMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRzJTBBJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKSUwQSUwQSUyMyUyMEZvciUyMHRyYWluaW5nJTBBbGFiZWxzJTIwJTNEJTIwdG9yY2gub25lcygxJTJDJTIwZHR5cGUlM0R0b3JjaC5sb25nJTJDJTIwZGV2aWNlJTNEZGV2aWNlKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcyUwQQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor, VJEPA2ForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>video_processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vjepa2-vitl-fpc16-256-ssv2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VJEPA2ForVideoClassification.from_pretrained(<span class="hljs-string">&quot;facebook/vjepa2-vitl-fpc16-256-ssv2&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>video = np.ones((<span class="hljs-number">64</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>))  <span class="hljs-comment"># 64 frames, 256x256 RGB</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = video_processor(video, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For inference</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.ones(<span class="hljs-number">1</span>, dtype=torch.long, device=device)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
`,wrap:!1}}),{c(){n=i("p"),n.textContent=b,p=a(),f(m.$$.fragment)},l(l){n=d(l,"P",{"data-svelte-h":!0}),h(n)!=="svelte-kvfsh7"&&(n.textContent=b),p=r(l),u(m.$$.fragment,l)},m(l,E){s(l,n,E),s(l,p,E),g(m,l,E),M=!0},p:Ne,i(l){M||(_(m.$$.fragment,l),M=!0)},o(l){y(m.$$.fragment,l),M=!1},d(l){l&&(o(n),o(p)),J(m,l)}}}function Bt(k){let n,b,p,m,M,l="<em>This model was released on 2025-06-11 and added to Hugging Face Transformers on 2025-06-11.</em>",E,W,lt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/></div>',ve,F,we,H,it='<a href="https://huggingface.co/papers/2506.09985" rel="nofollow">V-JEPA 2</a> (<a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/" rel="nofollow">blog post</a>) is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.',Te,A,dt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif" alt="drawing" width="600"/>',je,N,ct='You can find all original V-JEPA2 checkpoints under the <a href="https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6" rel="nofollow">V-JEPA 2</a> collection.',Ve,X,pt='This model was contributed by <a href="https://huggingface.co/koustuvs" rel="nofollow">koustuvs</a>, <a href="https://huggingface.co/yonigozlan" rel="nofollow">yonigozlan</a> and <a href="https://huggingface.co/qubvel-hf" rel="nofollow">qubvel</a>. The original code can be found <a href="https://github.com/facebookresearch/vjepa2" rel="nofollow">here</a>.',Ee,Y,Ue,Q,mt="The snippet below shows how to load the V-JEPA 2 model for feature extraction using the <code>AutoModel</code> class.",Ze,S,ke,L,ht="V-JEPA 2 can also be finetuned for video classification. In the following snippet, we show how use finetuned on Something-Something-V2 video classification model.",Ce,q,$e,D,Pe,T,K,Xe,de,ft=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Model">VJEPA2Model</a>. It is used to instantiate an
VJEPA2 model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VJEPA2
<a href="https://huggingface.co/facebook/vjepa2-vitl-fpc64-256" rel="nofollow">facebook/vjepa2-vitl-fpc64-256</a> architecture.`,Ye,ce,ut=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Qe,I,We,O,Ae,v,ee,Se,pe,gt="The bare Vjepa2 Model outputting raw hidden-states without any specific head on top.",Le,me,_t=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qe,he,yt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,De,C,te,Ke,fe,Jt='The <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Model">VJEPA2Model</a> forward method, overrides the <code>__call__</code> special method.',Oe,x,Ie,oe,xe,w,se,et,ue,Mt="V-JEPA 2 Model transformer with a video classification head on top (a linear layer on top of the attentive pooler).",tt,ge,bt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ot,_e,vt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,st,U,ne,nt,ye,wt='The <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification">VJEPA2ForVideoClassification</a> forward method, overrides the <code>__call__</code> special method.',at,B,rt,z,Be,ae,ze,re,le,Ge,ie,Re,be,Fe;return F=new Me({props:{title:"V-JEPA 2",local:"v-jepa-2",headingTag:"h1"}}),Y=new Me({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),S=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdG9yY2hjb2RlYy5kZWNvZGVycyUyMGltcG9ydCUyMFZpZGVvRGVjb2RlciUwQWltcG9ydCUyMG51bXB5JTIwYXMlMjBucCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ2amVwYTItdml0bC1mcGM2NC0yNTYlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmZhY2Vib29rJTJGdmplcGEyLXZpdGwtZnBjNjQtMjU2JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBhdHRuX2ltcGxlbWVudGF0aW9uJTNEJTIyc2RwYSUyMiUwQSklMEElMEF2aWRlb191cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZuYXRlcmF3JTJGa2luZXRpY3MtbWluaSUyRnJlc29sdmUlMkZtYWluJTJGdmFsJTJGYXJjaGVyeSUyRi1RejI1clhkTWpFXzAwMDAxNF8wMDAwMjQubXA0JTIyJTBBJTBBdnIlMjAlM0QlMjBWaWRlb0RlY29kZXIodmlkZW9fdXJsKSUwQWZyYW1lX2lkeCUyMCUzRCUyMG5wLmFyYW5nZSgwJTJDJTIwNjQpJTIwJTIzJTIwY2hvb3NpbmclMjBzb21lJTIwZnJhbWVzLiUyMGhlcmUlMkMlMjB5b3UlMjBjYW4lMjBkZWZpbmUlMjBtb3JlJTIwY29tcGxleCUyMHNhbXBsaW5nJTIwc3RyYXRlZ3klMEF2aWRlbyUyMCUzRCUyMHZyLmdldF9mcmFtZXNfYXQoaW5kaWNlcyUzRGZyYW1lX2lkeCkuZGF0YSUyMCUyMCUyMyUyMFQlMjB4JTIwQyUyMHglMjBIJTIweCUyMFclMEF2aWRlbyUyMCUzRCUyMHByb2Nlc3Nvcih2aWRlbyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKip2aWRlbyklMEElMEElMjMlMjBWLUpFUEElMjAyJTIwZW5jb2RlciUyMG91dHB1dHMlMkMlMjBzYW1lJTIwYXMlMjBjYWxsaW5nJTIwJTYwbW9kZWwuZ2V0X3Zpc2lvbl9mZWF0dXJlcygpJTYwJTBBZW5jb2Rlcl9vdXRwdXRzJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQSUwQSUyMyUyMFYtSkVQQSUyMDIlMjBwcmVkaWN0b3IlMjBvdXRwdXRzJTBBcHJlZGljdG9yX291dHB1dHMlMjAlM0QlMjBvdXRwdXRzLnByZWRpY3Rvcl9vdXRwdXQubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torchcodec.decoders <span class="hljs-keyword">import</span> VideoDecoder
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vjepa2-vitl-fpc64-256&quot;</span>)
model = AutoModel.from_pretrained(
    <span class="hljs-string">&quot;facebook/vjepa2-vitl-fpc64-256&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>
)

video_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/archery/-Qz25rXdMjE_000014_000024.mp4&quot;</span>

vr = VideoDecoder(video_url)
frame_idx = np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">64</span>) <span class="hljs-comment"># choosing some frames. here, you can define more complex sampling strategy</span>
video = vr.get_frames_at(indices=frame_idx).data  <span class="hljs-comment"># T x C x H x W</span>
video = processor(video, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
outputs = model(**video)

<span class="hljs-comment"># V-JEPA 2 encoder outputs, same as calling \`model.get_vision_features()\`</span>
encoder_outputs = outputs.last_hidden_state

<span class="hljs-comment"># V-JEPA 2 predictor outputs</span>
predictor_outputs = outputs.predictor_output.last_hidden_state`,wrap:!1}}),q=new He({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> torchcodec.decoders <span class="hljs-keyword">import</span> VideoDecoder
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor, AutoModelForVideoClassification, infer_device

device = infer_device()

<span class="hljs-comment"># Load model and video preprocessor</span>
hf_repo = <span class="hljs-string">&quot;facebook/vjepa2-vitl-fpc16-256-ssv2&quot;</span>

model = AutoModelForVideoClassification.from_pretrained(hf_repo).to(device)
processor = AutoVideoProcessor.from_pretrained(hf_repo)

<span class="hljs-comment"># To load a video, sample the number of frames according to the model.</span>
video_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4&quot;</span>
vr = VideoDecoder(video_url)
frame_idx = np.arange(<span class="hljs-number">0</span>, model.config.frames_per_clip, <span class="hljs-number">8</span>) <span class="hljs-comment"># you can define more complex sampling strategy</span>
video = vr.get_frames_at(indices=frame_idx).data  <span class="hljs-comment"># frames x channels x height x width</span>

<span class="hljs-comment"># Preprocess and run inference</span>
inputs = processor(video, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)
logits = outputs.logits

<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top 5 predicted class names:&quot;</span>)
top5_indices = logits.topk(<span class="hljs-number">5</span>).indices[<span class="hljs-number">0</span>]
top5_probs = torch.softmax(logits, dim=-<span class="hljs-number">1</span>).topk(<span class="hljs-number">5</span>).values[<span class="hljs-number">0</span>]
<span class="hljs-keyword">for</span> idx, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(top5_indices, top5_probs):
    text_label = model.config.id2label[idx.item()]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot; - <span class="hljs-subst">{text_label}</span>: <span class="hljs-subst">{prob:<span class="hljs-number">.2</span>f}</span>&quot;</span>)`,wrap:!1}}),D=new Me({props:{title:"VJEPA2Config",local:"transformers.VJEPA2Config",headingTag:"h2"}}),K=new Je({props:{name:"class transformers.VJEPA2Config",anchor:"transformers.VJEPA2Config",parameters:[{name:"patch_size",val:" = 16"},{name:"crop_size",val:" = 256"},{name:"frames_per_clip",val:" = 64"},{name:"tubelet_size",val:" = 2"},{name:"hidden_size",val:" = 1024"},{name:"in_chans",val:" = 3"},{name:"num_attention_heads",val:" = 16"},{name:"num_hidden_layers",val:" = 24"},{name:"drop_path_rate",val:" = 0.0"},{name:"mlp_ratio",val:" = 4.0"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"qkv_bias",val:" = True"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"attention_dropout",val:" = 0.0"},{name:"num_pooler_layers",val:" = 3"},{name:"pred_hidden_size",val:" = 384"},{name:"pred_num_attention_heads",val:" = 12"},{name:"pred_num_hidden_layers",val:" = 12"},{name:"pred_num_mask_tokens",val:" = 10"},{name:"pred_zero_init_mask_tokens",val:" = True"},{name:"pred_mlp_ratio",val:" = 4.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VJEPA2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.VJEPA2Config.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Input resolution of the model`,name:"crop_size"},{anchor:"transformers.VJEPA2Config.frames_per_clip",description:`<strong>frames_per_clip</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of frames the model has been pretrained with. Does not impact inference.`,name:"frames_per_clip"},{anchor:"transformers.VJEPA2Config.tubelet_size",description:`<strong>tubelet_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of temporal frames used for a single rastor, check paper for more information.`,name:"tubelet_size"},{anchor:"transformers.VJEPA2Config.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the encoder layers`,name:"hidden_size"},{anchor:"transformers.VJEPA2Config.in_chans",description:`<strong>in_chans</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels`,name:"in_chans"},{anchor:"transformers.VJEPA2Config.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Encoder`,name:"num_attention_heads"},{anchor:"transformers.VJEPA2Config.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
The number of hidden layers`,name:"num_hidden_layers"},{anchor:"transformers.VJEPA2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Stochastic depth rate per sample (when applied in the main path of residual layers).`,name:"drop_path_rate"},{anchor:"transformers.VJEPA2Config.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of the hidden size of the MLPs used in Encoder relative to the <code>hidden_size</code>.`,name:"mlp_ratio"},{anchor:"transformers.VJEPA2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VJEPA2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.VJEPA2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for attentions.
The dropout probability for all fully connected layers.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.VJEPA2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VJEPA2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VJEPA2Config.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for attentions.`,name:"attention_dropout"},{anchor:"transformers.VJEPA2Config.num_pooler_layers",description:`<strong>num_pooler_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of self-attention layers in the pooler.`,name:"num_pooler_layers"},{anchor:"transformers.VJEPA2Config.pred_hidden_size",description:`<strong>pred_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
Dimensionality of the predictor layers`,name:"pred_hidden_size"},{anchor:"transformers.VJEPA2Config.pred_num_attention_heads",description:`<strong>pred_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Predictor`,name:"pred_num_attention_heads"},{anchor:"transformers.VJEPA2Config.pred_num_hidden_layers",description:`<strong>pred_num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Predictor`,name:"pred_num_hidden_layers"},{anchor:"transformers.VJEPA2Config.pred_num_mask_tokens",description:`<strong>pred_num_mask_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Define the number of mask tokens to use in the Predictor`,name:"pred_num_mask_tokens"},{anchor:"transformers.VJEPA2Config.pred_zero_init_mask_tokens",description:`<strong>pred_zero_init_mask_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Initialize the mask tokens in the predictor with 0.`,name:"pred_zero_init_mask_tokens"},{anchor:"transformers.VJEPA2Config.pred_mlp_ratio",description:`<strong>pred_mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of the hidden size of the MLPs used in Predictor relative to the <code>pred_hidden_size</code>.`,name:"pred_mlp_ratio"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/configuration_vjepa2.py#L20"}}),I=new Vt({props:{anchor:"transformers.VJEPA2Config.example",$$slots:{default:[Wt]},$$scope:{ctx:k}}}),O=new Me({props:{title:"VJEPA2Model",local:"transformers.VJEPA2Model",headingTag:"h2"}}),ee=new Je({props:{name:"class transformers.VJEPA2Model",anchor:"transformers.VJEPA2Model",parameters:[{name:"config",val:": VJEPA2Config"}],parametersDescription:[{anchor:"transformers.VJEPA2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config">VJEPA2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/modeling_vjepa2.py#L1047"}}),te=new Je({props:{name:"forward",anchor:"transformers.VJEPA2Model.forward",parameters:[{name:"pixel_values_videos",val:": Tensor"},{name:"context_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"context_mask",val:": typing.Optional[list[torch.Tensor]] = None"},{name:"target_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"target_mask",val:": typing.Optional[list[torch.Tensor]] = None"},{name:"skip_predictor",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VJEPA2Model.forward.pixel_values_videos",description:`<strong>pixel_values_videos</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_frames, num_channels, frame_size, frame_size)</code>) &#x2014;
The tensors corresponding to the input video. Pixel values for videos can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor">VJEPA2VideoProcessor</a>. See <code>VJEPA2VideoProcessor.__call__()</code> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor">VJEPA2VideoProcessor</a> for processing videos).`,name:"pixel_values_videos"},{anchor:"transformers.VJEPA2Model.forward.context_head_mask",description:`<strong>context_head_mask</strong> (<code>torch.Tensor</code> with shape <code>[num_heads]</code> or <code>[num_hidden_layers x num_heads]</code>, <em>optional</em>) &#x2014;
The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard) for the context.`,name:"context_head_mask"},{anchor:"transformers.VJEPA2Model.forward.context_mask",description:`<strong>context_mask</strong> (<code>torch.Tensor</code> with shape <code>[batch_size, patch_size, 1]</code>, <em>optional</em>) &#x2014;
The mask position ids indicating which encoder output patches are going to be exposed to the predictor.
By default, this mask is created as torch.arange(N).unsqueeze(0).repeat(B,1), indicating full context
available to the predictor.`,name:"context_mask"},{anchor:"transformers.VJEPA2Model.forward.target_head_mask",description:`<strong>target_head_mask</strong> (<code>torch.Tensor</code> with shape <code>[num_heads]</code> or <code>[num_hidden_layers x num_heads]</code>, <em>optional</em>) &#x2014;
The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard) for the target.`,name:"target_head_mask"},{anchor:"transformers.VJEPA2Model.forward.target_mask",description:`<strong>target_mask</strong> (<code>torch.Tensor</code> with shape <code>[batch_size, patch_size, 1]</code>, <em>optional</em>) &#x2014;
The mask position ids indicating which encoder output patches are going to be used as a prediction target
for the predictor. By default, this mask is created as torch.arange(N).unsqueeze(0).repeat(B,1), indicating
that the predictor should predict all encoder patches.`,name:"target_mask"},{anchor:"transformers.VJEPA2Model.forward.skip_predictor",description:`<strong>skip_predictor</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
flag to skip the predictor forward, useful if you just need the encoder outputs`,name:"skip_predictor"},{anchor:"transformers.VJEPA2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VJEPA2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/modeling_vjepa2.py#L1061",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.vjepa2.modeling_vjepa2.VJEPA2WithMaskedInputModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config"
>VJEPA2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>&lt;class 'torch.FloatTensor'&gt;.last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>masked_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, returned when <code>context_mask</code> is provided which is applied on VJEPA2Encoder outputs) — The masked hidden state of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>predictor_output</strong> (<code>VJEPA2WithMaskedInputPredictorOutput</code>, <em>optional</em>) — The output from the Predictor module.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.vjepa2.modeling_vjepa2.VJEPA2WithMaskedInputModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),x=new jt({props:{$$slots:{default:[At]},$$scope:{ctx:k}}}),oe=new Me({props:{title:"VJEPA2ForVideoClassification",local:"transformers.VJEPA2ForVideoClassification",headingTag:"h2"}}),se=new Je({props:{name:"class transformers.VJEPA2ForVideoClassification",anchor:"transformers.VJEPA2ForVideoClassification",parameters:[{name:"config",val:": VJEPA2Config"}],parametersDescription:[{anchor:"transformers.VJEPA2ForVideoClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config">VJEPA2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/modeling_vjepa2.py#L1155"}}),ne=new Je({props:{name:"forward",anchor:"transformers.VJEPA2ForVideoClassification.forward",parameters:[{name:"pixel_values_videos",val:": Tensor"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VJEPA2ForVideoClassification.forward.pixel_values_videos",description:`<strong>pixel_values_videos</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_frames, num_channels, frame_size, frame_size)</code>) &#x2014;
The tensors corresponding to the input video. Pixel values for videos can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor">VJEPA2VideoProcessor</a>. See <code>VJEPA2VideoProcessor.__call__()</code> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor">VJEPA2VideoProcessor</a> for processing videos).`,name:"pixel_values_videos"},{anchor:"transformers.VJEPA2ForVideoClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.VJEPA2ForVideoClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VJEPA2ForVideoClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/modeling_vjepa2.py#L1169",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config"
>VJEPA2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new jt({props:{$$slots:{default:[It]},$$scope:{ctx:k}}}),z=new Vt({props:{anchor:"transformers.VJEPA2ForVideoClassification.forward.example",$$slots:{default:[xt]},$$scope:{ctx:k}}}),ae=new Me({props:{title:"VJEPA2VideoProcessor",local:"transformers.VJEPA2VideoProcessor",headingTag:"h2"}}),le=new Je({props:{name:"class transformers.VJEPA2VideoProcessor",anchor:"transformers.VJEPA2VideoProcessor",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.vjepa2.video_processing_vjepa2.VJEPA2VideoProcessorInitKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vjepa2/video_processing_vjepa2.py#L35"}}),ie=new Pt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vjepa2.md"}}),{c(){n=i("meta"),b=a(),p=i("p"),m=a(),M=i("p"),M.innerHTML=l,E=a(),W=i("div"),W.innerHTML=lt,ve=a(),f(F.$$.fragment),we=a(),H=i("p"),H.innerHTML=it,Te=a(),A=i("div"),A.innerHTML=dt,je=a(),N=i("p"),N.innerHTML=ct,Ve=a(),X=i("p"),X.innerHTML=pt,Ee=a(),f(Y.$$.fragment),Ue=a(),Q=i("p"),Q.innerHTML=mt,Ze=a(),f(S.$$.fragment),ke=a(),L=i("p"),L.textContent=ht,Ce=a(),f(q.$$.fragment),$e=a(),f(D.$$.fragment),Pe=a(),T=i("div"),f(K.$$.fragment),Xe=a(),de=i("p"),de.innerHTML=ft,Ye=a(),ce=i("p"),ce.innerHTML=ut,Qe=a(),f(I.$$.fragment),We=a(),f(O.$$.fragment),Ae=a(),v=i("div"),f(ee.$$.fragment),Se=a(),pe=i("p"),pe.textContent=gt,Le=a(),me=i("p"),me.innerHTML=_t,qe=a(),he=i("p"),he.innerHTML=yt,De=a(),C=i("div"),f(te.$$.fragment),Ke=a(),fe=i("p"),fe.innerHTML=Jt,Oe=a(),f(x.$$.fragment),Ie=a(),f(oe.$$.fragment),xe=a(),w=i("div"),f(se.$$.fragment),et=a(),ue=i("p"),ue.textContent=Mt,tt=a(),ge=i("p"),ge.innerHTML=bt,ot=a(),_e=i("p"),_e.innerHTML=vt,st=a(),U=i("div"),f(ne.$$.fragment),nt=a(),ye=i("p"),ye.innerHTML=wt,at=a(),f(B.$$.fragment),rt=a(),f(z.$$.fragment),Be=a(),f(ae.$$.fragment),ze=a(),re=i("div"),f(le.$$.fragment),Ge=a(),f(ie.$$.fragment),Re=a(),be=i("p"),this.h()},l(e){const t=Ct("svelte-u9bgzb",document.head);n=d(t,"META",{name:!0,content:!0}),t.forEach(o),b=r(e),p=d(e,"P",{}),R(p).forEach(o),m=r(e),M=d(e,"P",{"data-svelte-h":!0}),h(M)!=="svelte-svdcl4"&&(M.innerHTML=l),E=r(e),W=d(e,"DIV",{style:!0,"data-svelte-h":!0}),h(W)!=="svelte-1dwwnh7"&&(W.innerHTML=lt),ve=r(e),u(F.$$.fragment,e),we=r(e),H=d(e,"P",{"data-svelte-h":!0}),h(H)!=="svelte-1g3vt5d"&&(H.innerHTML=it),Te=r(e),A=d(e,"DIV",{class:!0,"data-svelte-h":!0}),h(A)!=="svelte-2m330p"&&(A.innerHTML=dt),je=r(e),N=d(e,"P",{"data-svelte-h":!0}),h(N)!=="svelte-1vhjbbe"&&(N.innerHTML=ct),Ve=r(e),X=d(e,"P",{"data-svelte-h":!0}),h(X)!=="svelte-1ag57ly"&&(X.innerHTML=pt),Ee=r(e),u(Y.$$.fragment,e),Ue=r(e),Q=d(e,"P",{"data-svelte-h":!0}),h(Q)!=="svelte-m7oq35"&&(Q.innerHTML=mt),Ze=r(e),u(S.$$.fragment,e),ke=r(e),L=d(e,"P",{"data-svelte-h":!0}),h(L)!=="svelte-iuaz2a"&&(L.textContent=ht),Ce=r(e),u(q.$$.fragment,e),$e=r(e),u(D.$$.fragment,e),Pe=r(e),T=d(e,"DIV",{class:!0});var Z=R(T);u(K.$$.fragment,Z),Xe=r(Z),de=d(Z,"P",{"data-svelte-h":!0}),h(de)!=="svelte-1efsqx7"&&(de.innerHTML=ft),Ye=r(Z),ce=d(Z,"P",{"data-svelte-h":!0}),h(ce)!=="svelte-1ek1ss9"&&(ce.innerHTML=ut),Qe=r(Z),u(I.$$.fragment,Z),Z.forEach(o),We=r(e),u(O.$$.fragment,e),Ae=r(e),v=d(e,"DIV",{class:!0});var j=R(v);u(ee.$$.fragment,j),Se=r(j),pe=d(j,"P",{"data-svelte-h":!0}),h(pe)!=="svelte-1yytrda"&&(pe.textContent=gt),Le=r(j),me=d(j,"P",{"data-svelte-h":!0}),h(me)!=="svelte-q52n56"&&(me.innerHTML=_t),qe=r(j),he=d(j,"P",{"data-svelte-h":!0}),h(he)!=="svelte-hswkmf"&&(he.innerHTML=yt),De=r(j),C=d(j,"DIV",{class:!0});var $=R(C);u(te.$$.fragment,$),Ke=r($),fe=d($,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-kg5yix"&&(fe.innerHTML=Jt),Oe=r($),u(x.$$.fragment,$),$.forEach(o),j.forEach(o),Ie=r(e),u(oe.$$.fragment,e),xe=r(e),w=d(e,"DIV",{class:!0});var V=R(w);u(se.$$.fragment,V),et=r(V),ue=d(V,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-d2w0r6"&&(ue.textContent=Mt),tt=r(V),ge=d(V,"P",{"data-svelte-h":!0}),h(ge)!=="svelte-q52n56"&&(ge.innerHTML=bt),ot=r(V),_e=d(V,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-hswkmf"&&(_e.innerHTML=vt),st=r(V),U=d(V,"DIV",{class:!0});var G=R(U);u(ne.$$.fragment,G),nt=r(G),ye=d(G,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-1ha8ctp"&&(ye.innerHTML=wt),at=r(G),u(B.$$.fragment,G),rt=r(G),u(z.$$.fragment,G),G.forEach(o),V.forEach(o),Be=r(e),u(ae.$$.fragment,e),ze=r(e),re=d(e,"DIV",{class:!0});var Tt=R(re);u(le.$$.fragment,Tt),Tt.forEach(o),Ge=r(e),u(ie.$$.fragment,e),Re=r(e),be=d(e,"P",{}),R(be).forEach(o),this.h()},h(){P(n,"name","hf:doc:metadata"),P(n,"content",zt),$t(W,"float","right"),P(A,"class","flex justify-center"),P(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,n),s(e,b,t),s(e,p,t),s(e,m,t),s(e,M,t),s(e,E,t),s(e,W,t),s(e,ve,t),g(F,e,t),s(e,we,t),s(e,H,t),s(e,Te,t),s(e,A,t),s(e,je,t),s(e,N,t),s(e,Ve,t),s(e,X,t),s(e,Ee,t),g(Y,e,t),s(e,Ue,t),s(e,Q,t),s(e,Ze,t),g(S,e,t),s(e,ke,t),s(e,L,t),s(e,Ce,t),g(q,e,t),s(e,$e,t),g(D,e,t),s(e,Pe,t),s(e,T,t),g(K,T,null),c(T,Xe),c(T,de),c(T,Ye),c(T,ce),c(T,Qe),g(I,T,null),s(e,We,t),g(O,e,t),s(e,Ae,t),s(e,v,t),g(ee,v,null),c(v,Se),c(v,pe),c(v,Le),c(v,me),c(v,qe),c(v,he),c(v,De),c(v,C),g(te,C,null),c(C,Ke),c(C,fe),c(C,Oe),g(x,C,null),s(e,Ie,t),g(oe,e,t),s(e,xe,t),s(e,w,t),g(se,w,null),c(w,et),c(w,ue),c(w,tt),c(w,ge),c(w,ot),c(w,_e),c(w,st),c(w,U),g(ne,U,null),c(U,nt),c(U,ye),c(U,at),g(B,U,null),c(U,rt),g(z,U,null),s(e,Be,t),g(ae,e,t),s(e,ze,t),s(e,re,t),g(le,re,null),s(e,Ge,t),g(ie,e,t),s(e,Re,t),s(e,be,t),Fe=!0},p(e,[t]){const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),I.$set(Z);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),x.$set(j);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),B.$set($);const V={};t&2&&(V.$$scope={dirty:t,ctx:e}),z.$set(V)},i(e){Fe||(_(F.$$.fragment,e),_(Y.$$.fragment,e),_(S.$$.fragment,e),_(q.$$.fragment,e),_(D.$$.fragment,e),_(K.$$.fragment,e),_(I.$$.fragment,e),_(O.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(x.$$.fragment,e),_(oe.$$.fragment,e),_(se.$$.fragment,e),_(ne.$$.fragment,e),_(B.$$.fragment,e),_(z.$$.fragment,e),_(ae.$$.fragment,e),_(le.$$.fragment,e),_(ie.$$.fragment,e),Fe=!0)},o(e){y(F.$$.fragment,e),y(Y.$$.fragment,e),y(S.$$.fragment,e),y(q.$$.fragment,e),y(D.$$.fragment,e),y(K.$$.fragment,e),y(I.$$.fragment,e),y(O.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(x.$$.fragment,e),y(oe.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(B.$$.fragment,e),y(z.$$.fragment,e),y(ae.$$.fragment,e),y(le.$$.fragment,e),y(ie.$$.fragment,e),Fe=!1},d(e){e&&(o(b),o(p),o(m),o(M),o(E),o(W),o(ve),o(we),o(H),o(Te),o(A),o(je),o(N),o(Ve),o(X),o(Ee),o(Ue),o(Q),o(Ze),o(ke),o(L),o(Ce),o($e),o(Pe),o(T),o(We),o(Ae),o(v),o(Ie),o(xe),o(w),o(Be),o(ze),o(re),o(Ge),o(Re),o(be)),o(n),J(F,e),J(Y,e),J(S,e),J(q,e),J(D,e),J(K),J(I),J(O,e),J(ee),J(te),J(x),J(oe,e),J(se),J(ne),J(B),J(z),J(ae,e),J(le),J(ie,e)}}}const zt='{"title":"V-JEPA 2","local":"v-jepa-2","sections":[{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"VJEPA2Config","local":"transformers.VJEPA2Config","sections":[],"depth":2},{"title":"VJEPA2Model","local":"transformers.VJEPA2Model","sections":[],"depth":2},{"title":"VJEPA2ForVideoClassification","local":"transformers.VJEPA2ForVideoClassification","sections":[],"depth":2},{"title":"VJEPA2VideoProcessor","local":"transformers.VJEPA2VideoProcessor","sections":[],"depth":2}],"depth":1}';function Gt(k){return Ut(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class St extends Zt{constructor(n){super(),kt(this,n,Gt,Bt,Et,{})}}export{St as component};
