import{s as Vt,o as Et,n as q}from"../chunks/scheduler.18a86fab.js";import{S as Dt,i as Ht,g as h,s as l,r as u,A as Qt,h as f,f as o,c as r,j as z,x as w,u as g,k as ge,l as qt,y as m,a as i,v as M,d as b,t as _,w as y}from"../chunks/index.98837b22.js";import{T as yt}from"../chunks/Tip.77304350.js";import{D as Ie}from"../chunks/Docstring.a1ef7999.js";import{C as Q}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as wt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ke,E as At}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Lt,a as Rt}from"../chunks/HfOption.6641485e.js";function St(v){let t,p="Click on the DINOv2 models in the right sidebar for more examples of how to apply DINOv2 to different vision tasks.";return{c(){t=h("p"),t.textContent=p},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-1npw7gp"&&(t.textContent=p)},m(s,d){i(s,t,d)},p:q,d(s){s&&o(t)}}}function Pt(v){let t,p;return t=new Q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlJTIwJTNEJTIwcGlwZWxpbmUoJTBBJTIwJTIwJTIwJTIwdGFzayUzRCUyMmltYWdlLWNsYXNzaWZpY2F0aW9uJTIyJTJDJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0QlMjJmYWNlYm9vayUyRmRpbm92Mi1zbWFsbC1pbWFnZW5ldDFrLTEtbGF5ZXIlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2UlM0QwJTBBKSUwQSUwQXBpcGUoJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGcGlwZWxpbmUtY2F0LWNob25rLmpwZWclMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipe = pipeline(
    task=<span class="hljs-string">&quot;image-classification&quot;</span>,
    model=<span class="hljs-string">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)

pipe(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(s){g(t.$$.fragment,s)},m(s,d){M(t,s,d),p=!0},p:q,i(s){p||(b(t.$$.fragment,s),p=!0)},o(s){_(t.$$.fragment,s),p=!1},d(s){y(t,s)}}}function Kt(v){let t,p;return t=new Q({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0ltYWdlUHJvY2Vzc29yJTJDJTIwQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGlub3YyLXNtYWxsLWltYWdlbmV0MWstMS1sYXllciUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmZhY2Vib29rJTJGZGlub3YyLXNtYWxsLWltYWdlbmV0MWstMS1sYXllciUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMnNkcGElMjIlMEEpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBcHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForImageClassification
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>)
model = AutoModelForImageClassification.from_pretrained(
    <span class="hljs-string">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>
)

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
logits = model(**inputs).logits
predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`,wrap:!1}}),{c(){u(t.$$.fragment)},l(s){g(t.$$.fragment,s)},m(s,d){M(t,s,d),p=!0},p:q,i(s){p||(b(t.$$.fragment,s),p=!0)},o(s){_(t.$$.fragment,s),p=!1},d(s){y(t,s)}}}function Ot(v){let t,p,s,d;return t=new Rt({props:{id:"usage",option:"Pipeline",$$slots:{default:[Pt]},$$scope:{ctx:v}}}),s=new Rt({props:{id:"usage",option:"AutoModel",$$slots:{default:[Kt]},$$scope:{ctx:v}}}),{c(){u(t.$$.fragment),p=l(),u(s.$$.fragment)},l(c){g(t.$$.fragment,c),p=r(c),g(s.$$.fragment,c)},m(c,a){M(t,c,a),i(c,p,a),M(s,c,a),d=!0},p(c,a){const T={};a&2&&(T.$$scope={dirty:a,ctx:c}),t.$set(T);const I={};a&2&&(I.$$scope={dirty:a,ctx:c}),s.$set(I)},i(c){d||(b(t.$$.fragment,c),b(s.$$.fragment,c),d=!0)},o(c){_(t.$$.fragment,c),_(s.$$.fragment,c),d=!1},d(c){c&&o(p),y(t,c),y(s,c)}}}function es(v){let t,p="Example:",s,d,c;return d=new Q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERpbm92MkNvbmZpZyUyQyUyMERpbm92Mk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMERpbm92MiUyMGRpbm92Mi1iYXNlLXBhdGNoMTYtMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMERpbm92MkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBkaW5vdjItYmFzZS1wYXRjaDE2LTIyNCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRGlub3YyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Dinov2Config, Dinov2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Dinov2 dinov2-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Dinov2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the dinov2-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Dinov2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=h("p"),t.textContent=p,s=l(),u(d.$$.fragment)},l(a){t=f(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-11lpom8"&&(t.textContent=p),s=r(a),g(d.$$.fragment,a)},m(a,T){i(a,t,T),i(a,s,T),M(d,a,T),c=!0},p:q,i(a){c||(b(d.$$.fragment,a),c=!0)},o(a){_(d.$$.fragment,a),c=!1},d(a){a&&(o(t),o(s)),y(d,a)}}}function ts(v){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=p},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(s,d){i(s,t,d)},p:q,d(s){s&&o(t)}}}function ss(v){let t,p="Example:",s,d,c;return d=new Q({props:{code:"",highlighted:"",wrap:!1}}),{c(){t=h("p"),t.textContent=p,s=l(),u(d.$$.fragment)},l(a){t=f(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-11lpom8"&&(t.textContent=p),s=r(a),g(d.$$.fragment,a)},m(a,T){i(a,t,T),i(a,s,T),M(d,a,T),c=!0},p:q,i(a){c||(b(d.$$.fragment,a),c=!0)},o(a){_(d.$$.fragment,a),c=!1},d(a){a&&(o(t),o(s)),y(d,a)}}}function ns(v){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=p},l(s){t=f(s,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(s,d){i(s,t,d)},p:q,d(s){s&&o(t)}}}function os(v){let t,p="Example:",s,d,c;return d=new Q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERpbm92MkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmRpbm92Mi1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwRGlub3YyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZGlub3YyLWJhc2UtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Dinov2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/dinov2-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Dinov2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;google/dinov2-base-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){t=h("p"),t.textContent=p,s=l(),u(d.$$.fragment)},l(a){t=f(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-11lpom8"&&(t.textContent=p),s=r(a),g(d.$$.fragment,a)},m(a,T){i(a,t,T),i(a,s,T),M(d,a,T),c=!0},p:q,i(a){c||(b(d.$$.fragment,a),c=!0)},o(a){_(d.$$.fragment,a),c=!1},d(a){a&&(o(t),o(s)),y(d,a)}}}function as(v){let t,p,s,d,c,a="<em>This model was released on 2023-04-14 and added to Hugging Face Transformers on 2023-07-18.</em>",T,I,vt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Be,A,ze,L,Tt='<a href="https://huggingface.co/papers/2304.07193" rel="nofollow">DINOv2</a> is a vision foundation model that uses <a href="./vit">ViT</a> as a feature extractor for multiple downstream tasks like image classification and depth estimation. It focuses on stabilizing and accelerating training through techniques like a faster memory-efficient attention, sequence packing, improved stochastic depth, Fully Sharded Data Parallel (FSDP), and model distillation.',Fe,S,Jt='You can find all the original DINOv2 checkpoints under the <a href="https://huggingface.co/collections/facebook/dinov2-6526c98554b3d2576e071ce3" rel="nofollow">Dinov2</a> collection.',Ge,x,xe,P,jt='The example below demonstrates how to obtain an image embedding with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',Xe,X,Ye,K,Ut='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',Ne,O,$t='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to int4.',Re,ee,Ve,te,Ee,Y,F,Me,Wt="The example below shows how to split the output tensor into:",Oe,be,Zt=`<li>one embedding for the whole image, commonly referred to as a <code>CLS</code> token,
useful for classification and retrieval</li> <li>a set of local embeddings, one for each <code>14x14</code> patch of the input image,
useful for dense tasks, such as semantic segmentation</li>`,et,se,tt,ne,_e,Ct=`Use <a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html" rel="nofollow">torch.jit.trace</a> to speedup inference.
However, it will produce some mismatched elements. The difference between the original and traced model is 1e-4.`,st,oe,De,ae,He,U,le,nt,ye,It=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a>. It is used to instantiate an
Dinov2 model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Dinov2
<a href="https://huggingface.co/google/dinov2-base-patch16-224" rel="nofollow">google/dinov2-base-patch16-224</a> architecture.`,ot,we,kt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,at,N,Qe,re,qe,J,ie,lt,ve,Bt="The bare Dinov2 Model outputting raw hidden-states without any specific head on top.",rt,Te,zt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,it,Je,Ft=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,dt,Z,de,ct,je,Gt='The <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> forward method, overrides the <code>__call__</code> special method.',mt,R,pt,V,Ae,ce,Le,j,me,ht,Ue,xt=`Dinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,ft,$e,Xt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ut,We,Yt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,gt,C,pe,Mt,Ze,Nt='The <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',bt,E,_t,D,Se,he,Pe,Ce,Ke;return A=new ke({props:{title:"DINOv2",local:"dinov2",headingTag:"h1"}}),x=new yt({props:{warning:!1,$$slots:{default:[St]},$$scope:{ctx:v}}}),X=new Lt({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[Ot]},$$scope:{ctx:v}}}),ee=new Q({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRvcmNoYW8lMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBUb3JjaEFvQ29uZmlnJTJDJTIwQXV0b0ltYWdlUHJvY2Vzc29yJTJDJTIwQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBdXJsJTIwJTNEJTIwJ2h0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGcnJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCgnZmFjZWJvb2slMkZkaW5vdjItZ2lhbnQtaW1hZ2VuZXQxay0xLWxheWVyJyklMEElMEFxdWFudF9jb25maWclMjAlM0QlMjBJbnQ0V2VpZ2h0T25seUNvbmZpZyhncm91cF9zaXplJTNEMTI4KSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBUb3JjaEFvQ29uZmlnKHF1YW50X3R5cGUlM0RxdWFudF9jb25maWcpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAnZmFjZWJvb2slMkZkaW5vdjItZ2lhbnQtaW1hZ2VuZXQxay0xLWxheWVyJyUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTBBKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRzJTBBcHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-comment"># pip install torchao</span>
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoImageProcessor, AutoModelForImageClassification
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-giant-imagenet1k-1-layer&#x27;</span>)

quant_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>)
quantization_config = TorchAoConfig(quant_type=quant_config)

model = AutoModelForImageClassification.from_pretrained(
    <span class="hljs-string">&#x27;facebook/dinov2-giant-imagenet1k-1-layer&#x27;</span>,
    dtype=torch.bfloat16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`,wrap:!1}}),te=new ke({props:{title:"Notes",local:"notes",headingTag:"h2"}}),se=new Q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJ2h0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGcnJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBcHJpbnQoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTIwJTIwJTIzJTIwJTVCNDgwJTJDJTIwNjQwJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCgnZmFjZWJvb2slMkZkaW5vdjItYmFzZScpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCdmYWNlYm9vayUyRmRpbm92Mi1iYXNlJyklMEFwYXRjaF9zaXplJTIwJTNEJTIwbW9kZWwuY29uZmlnLnBhdGNoX3NpemUlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXByaW50KGlucHV0cy5waXhlbF92YWx1ZXMuc2hhcGUpJTIwJTIwJTIzJTIwJTVCMSUyQyUyMDMlMkMlMjAyMjQlMkMlMjAyMjQlNUQlMEFiYXRjaF9zaXplJTJDJTIwcmdiJTJDJTIwaW1nX2hlaWdodCUyQyUyMGltZ193aWR0aCUyMCUzRCUyMGlucHV0cy5waXhlbF92YWx1ZXMuc2hhcGUlMEFudW1fcGF0Y2hlc19oZWlnaHQlMkMlMjBudW1fcGF0Y2hlc193aWR0aCUyMCUzRCUyMGltZ19oZWlnaHQlMjAlMkYlMkYlMjBwYXRjaF9zaXplJTJDJTIwaW1nX3dpZHRoJTIwJTJGJTJGJTIwcGF0Y2hfc2l6ZSUwQW51bV9wYXRjaGVzX2ZsYXQlMjAlM0QlMjBudW1fcGF0Y2hlc19oZWlnaHQlMjAqJTIwbnVtX3BhdGNoZXNfd2lkdGglMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGVzJTIwJTNEJTIwb3V0cHV0cyU1QjAlNUQlMEFwcmludChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUpJTIwJTIwJTIzJTIwJTVCMSUyQyUyMDElMjAlMkIlMjAyNTYlMkMlMjA3NjglNUQlMEFhc3NlcnQlMjBsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUlMjAlM0QlM0QlMjAoYmF0Y2hfc2l6ZSUyQyUyMDElMjAlMkIlMjBudW1fcGF0Y2hlc19mbGF0JTJDJTIwbW9kZWwuY29uZmlnLmhpZGRlbl9zaXplKSUwQSUwQWNsc190b2tlbiUyMCUzRCUyMGxhc3RfaGlkZGVuX3N0YXRlcyU1QiUzQSUyQyUyMDAlMkMlMjAlM0ElNUQlMEFwYXRjaF9mZWF0dXJlcyUyMCUzRCUyMGxhc3RfaGlkZGVuX3N0YXRlcyU1QiUzQSUyQyUyMDElM0ElMkMlMjAlM0ElNUQudW5mbGF0dGVuKDElMkMlMjAobnVtX3BhdGNoZXNfaGVpZ2h0JTJDJTIwbnVtX3BhdGNoZXNfd2lkdGgpKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-built_in">print</span>(image.height, image.width)  <span class="hljs-comment"># [480, 640]</span>

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)
patch_size = model.config.patch_size

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs.pixel_values.shape)  <span class="hljs-comment"># [1, 3, 224, 224]</span>
batch_size, rgb, img_height, img_width = inputs.pixel_values.shape
num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size
num_patches_flat = num_patches_height * num_patches_width

outputs = model(**inputs)
last_hidden_states = outputs[<span class="hljs-number">0</span>]
<span class="hljs-built_in">print</span>(last_hidden_states.shape)  <span class="hljs-comment"># [1, 1 + 256, 768]</span>
<span class="hljs-keyword">assert</span> last_hidden_states.shape == (batch_size, <span class="hljs-number">1</span> + num_patches_flat, model.config.hidden_size)

cls_token = last_hidden_states[:, <span class="hljs-number">0</span>, :]
patch_features = last_hidden_states[:, <span class="hljs-number">1</span>:, :].unflatten(<span class="hljs-number">1</span>, (num_patches_height, num_patches_width))`,wrap:!1}}),oe=new Q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0ltYWdlUHJvY2Vzc29yJTJDJTIwQXV0b01vZGVsJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAnaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyclMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCdmYWNlYm9vayUyRmRpbm92Mi1iYXNlJyklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJ2ZhY2Vib29rJTJGZGlub3YyLWJhc2UnKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMlNUIwJTVEJTBBJTBBJTIzJTIwV2UlMjBoYXZlJTIwdG8lMjBmb3JjZSUyMHJldHVybl9kaWN0JTNERmFsc2UlMjBmb3IlMjB0cmFjaW5nJTBBbW9kZWwuY29uZmlnLnJldHVybl9kaWN0JTIwJTNEJTIwRmFsc2UlMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwdHJhY2VkX21vZGVsJTIwJTNEJTIwdG9yY2guaml0LnRyYWNlKG1vZGVsJTJDJTIwJTVCaW5wdXRzLnBpeGVsX3ZhbHVlcyU1RCklMEElMjAlMjAlMjAlMjB0cmFjZWRfb3V0cHV0cyUyMCUzRCUyMHRyYWNlZF9tb2RlbChpbnB1dHMucGl4ZWxfdmFsdWVzKSUwQSUwQXByaW50KChsYXN0X2hpZGRlbl9zdGF0ZXMlMjAtJTIwdHJhY2VkX291dHB1dHMlNUIwJTVEKS5hYnMoKS5tYXgoKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)
last_hidden_states = outputs[<span class="hljs-number">0</span>]

<span class="hljs-comment"># We have to force return_dict=False for tracing</span>
model.config.return_dict = <span class="hljs-literal">False</span>

<span class="hljs-keyword">with</span> torch.no_grad():
    traced_model = torch.jit.trace(model, [inputs.pixel_values])
    traced_outputs = traced_model(inputs.pixel_values)

<span class="hljs-built_in">print</span>((last_hidden_states - traced_outputs[<span class="hljs-number">0</span>]).<span class="hljs-built_in">abs</span>().<span class="hljs-built_in">max</span>())`,wrap:!1}}),ae=new ke({props:{title:"Dinov2Config",local:"transformers.Dinov2Config",headingTag:"h2"}}),le=new Ie({props:{name:"class transformers.Dinov2Config",anchor:"transformers.Dinov2Config",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"mlp_ratio",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 14"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"layerscale_value",val:" = 1.0"},{name:"drop_path_rate",val:" = 0.0"},{name:"use_swiglu_ffn",val:" = False"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"apply_layernorm",val:" = True"},{name:"reshape_hidden_states",val:" = True"},{name:"use_mask_token",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Dinov2Config.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Dinov2Config.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Dinov2Config.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Dinov2Config.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of the hidden size of the MLPs relative to the <code>hidden_size</code>.`,name:"mlp_ratio"},{anchor:"transformers.Dinov2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Dinov2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.Dinov2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Dinov2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Dinov2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Dinov2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Dinov2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Dinov2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.Dinov2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.Dinov2Config.layerscale_value",description:`<strong>layerscale_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Initial value to use for layer scale.`,name:"layerscale_value"},{anchor:"transformers.Dinov2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Stochastic depth rate per sample (when applied in the main path of residual layers).`,name:"drop_path_rate"},{anchor:"transformers.Dinov2Config.use_swiglu_ffn",description:`<strong>use_swiglu_ffn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use the SwiGLU feedforward neural network.`,name:"use_swiglu_ffn"},{anchor:"transformers.Dinov2Config.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.Dinov2Config.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"},{anchor:"transformers.Dinov2Config.apply_layernorm",description:`<strong>apply_layernorm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply layer normalization to the feature maps in case the model is used as backbone.`,name:"apply_layernorm"},{anchor:"transformers.Dinov2Config.reshape_hidden_states",description:`<strong>reshape_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to reshape the feature maps to 4D tensors of shape <code>(batch_size, hidden_size, height, width)</code> in
case the model is used as backbone. If <code>False</code>, the feature maps will be 3D tensors of shape <code>(batch_size, seq_len, hidden_size)</code>.`,name:"reshape_hidden_states"},{anchor:"transformers.Dinov2Config.use_mask_token",description:`<strong>use_mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use mask_token in embeddings.`,name:"use_mask_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dinov2/configuration_dinov2.py#L31"}}),N=new wt({props:{anchor:"transformers.Dinov2Config.example",$$slots:{default:[es]},$$scope:{ctx:v}}}),re=new ke({props:{title:"Dinov2Model",local:"transformers.Dinov2Model",headingTag:"h2"}}),ie=new Ie({props:{name:"class transformers.Dinov2Model",anchor:"transformers.Dinov2Model",parameters:[{name:"config",val:": Dinov2Config"}],parametersDescription:[{anchor:"transformers.Dinov2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dinov2/modeling_dinov2.py#L481"}}),de=new Ie({props:{name:"forward",anchor:"transformers.Dinov2Model.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Dinov2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BitImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Dinov2Model.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0). Only relevant for
pre-training.`,name:"bool_masked_pos"},{anchor:"transformers.Dinov2Model.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Dinov2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dinov2/modeling_dinov2.py#L505",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config"
>Dinov2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new yt({props:{$$slots:{default:[ts]},$$scope:{ctx:v}}}),V=new wt({props:{anchor:"transformers.Dinov2Model.forward.example",$$slots:{default:[ss]},$$scope:{ctx:v}}}),ce=new ke({props:{title:"Dinov2ForImageClassification",local:"transformers.Dinov2ForImageClassification",headingTag:"h2"}}),me=new Ie({props:{name:"class transformers.Dinov2ForImageClassification",anchor:"transformers.Dinov2ForImageClassification",parameters:[{name:"config",val:": Dinov2Config"}],parametersDescription:[{anchor:"transformers.Dinov2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dinov2/modeling_dinov2.py#L555"}}),pe=new Ie({props:{name:"forward",anchor:"transformers.Dinov2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Dinov2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BitImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Dinov2ForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Dinov2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dinov2/modeling_dinov2.py#L570",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config"
>Dinov2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new yt({props:{$$slots:{default:[ns]},$$scope:{ctx:v}}}),D=new wt({props:{anchor:"transformers.Dinov2ForImageClassification.forward.example",$$slots:{default:[os]},$$scope:{ctx:v}}}),he=new At({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinov2.md"}}),{c(){t=h("meta"),p=l(),s=h("p"),d=l(),c=h("p"),c.innerHTML=a,T=l(),I=h("div"),I.innerHTML=vt,Be=l(),u(A.$$.fragment),ze=l(),L=h("p"),L.innerHTML=Tt,Fe=l(),S=h("p"),S.innerHTML=Jt,Ge=l(),u(x.$$.fragment),xe=l(),P=h("p"),P.innerHTML=jt,Xe=l(),u(X.$$.fragment),Ye=l(),K=h("p"),K.innerHTML=Ut,Ne=l(),O=h("p"),O.innerHTML=$t,Re=l(),u(ee.$$.fragment),Ve=l(),u(te.$$.fragment),Ee=l(),Y=h("ul"),F=h("li"),Me=h("p"),Me.textContent=Wt,Oe=l(),be=h("ul"),be.innerHTML=Zt,et=l(),u(se.$$.fragment),tt=l(),ne=h("li"),_e=h("p"),_e.innerHTML=Ct,st=l(),u(oe.$$.fragment),De=l(),u(ae.$$.fragment),He=l(),U=h("div"),u(le.$$.fragment),nt=l(),ye=h("p"),ye.innerHTML=It,ot=l(),we=h("p"),we.innerHTML=kt,at=l(),u(N.$$.fragment),Qe=l(),u(re.$$.fragment),qe=l(),J=h("div"),u(ie.$$.fragment),lt=l(),ve=h("p"),ve.textContent=Bt,rt=l(),Te=h("p"),Te.innerHTML=zt,it=l(),Je=h("p"),Je.innerHTML=Ft,dt=l(),Z=h("div"),u(de.$$.fragment),ct=l(),je=h("p"),je.innerHTML=Gt,mt=l(),u(R.$$.fragment),pt=l(),u(V.$$.fragment),Ae=l(),u(ce.$$.fragment),Le=l(),j=h("div"),u(me.$$.fragment),ht=l(),Ue=h("p"),Ue.textContent=xt,ft=l(),$e=h("p"),$e.innerHTML=Xt,ut=l(),We=h("p"),We.innerHTML=Yt,gt=l(),C=h("div"),u(pe.$$.fragment),Mt=l(),Ze=h("p"),Ze.innerHTML=Nt,bt=l(),u(E.$$.fragment),_t=l(),u(D.$$.fragment),Se=l(),u(he.$$.fragment),Pe=l(),Ce=h("p"),this.h()},l(e){const n=Qt("svelte-u9bgzb",document.head);t=f(n,"META",{name:!0,content:!0}),n.forEach(o),p=r(e),s=f(e,"P",{}),z(s).forEach(o),d=r(e),c=f(e,"P",{"data-svelte-h":!0}),w(c)!=="svelte-1rwzegn"&&(c.innerHTML=a),T=r(e),I=f(e,"DIV",{style:!0,"data-svelte-h":!0}),w(I)!=="svelte-2m0t7r"&&(I.innerHTML=vt),Be=r(e),g(A.$$.fragment,e),ze=r(e),L=f(e,"P",{"data-svelte-h":!0}),w(L)!=="svelte-i29kee"&&(L.innerHTML=Tt),Fe=r(e),S=f(e,"P",{"data-svelte-h":!0}),w(S)!=="svelte-1ua7res"&&(S.innerHTML=Jt),Ge=r(e),g(x.$$.fragment,e),xe=r(e),P=f(e,"P",{"data-svelte-h":!0}),w(P)!=="svelte-175cfci"&&(P.innerHTML=jt),Xe=r(e),g(X.$$.fragment,e),Ye=r(e),K=f(e,"P",{"data-svelte-h":!0}),w(K)!=="svelte-nf5ooi"&&(K.innerHTML=Ut),Ne=r(e),O=f(e,"P",{"data-svelte-h":!0}),w(O)!=="svelte-w36i1c"&&(O.innerHTML=$t),Re=r(e),g(ee.$$.fragment,e),Ve=r(e),g(te.$$.fragment,e),Ee=r(e),Y=f(e,"UL",{});var fe=z(Y);F=f(fe,"LI",{});var G=z(F);Me=f(G,"P",{"data-svelte-h":!0}),w(Me)!=="svelte-90ha2s"&&(Me.textContent=Wt),Oe=r(G),be=f(G,"UL",{"data-svelte-h":!0}),w(be)!=="svelte-is7q7m"&&(be.innerHTML=Zt),et=r(G),g(se.$$.fragment,G),G.forEach(o),tt=r(fe),ne=f(fe,"LI",{});var ue=z(ne);_e=f(ue,"P",{"data-svelte-h":!0}),w(_e)!=="svelte-1qoetgv"&&(_e.innerHTML=Ct),st=r(ue),g(oe.$$.fragment,ue),ue.forEach(o),fe.forEach(o),De=r(e),g(ae.$$.fragment,e),He=r(e),U=f(e,"DIV",{class:!0});var k=z(U);g(le.$$.fragment,k),nt=r(k),ye=f(k,"P",{"data-svelte-h":!0}),w(ye)!=="svelte-1h2g0r1"&&(ye.innerHTML=It),ot=r(k),we=f(k,"P",{"data-svelte-h":!0}),w(we)!=="svelte-1ek1ss9"&&(we.innerHTML=kt),at=r(k),g(N.$$.fragment,k),k.forEach(o),Qe=r(e),g(re.$$.fragment,e),qe=r(e),J=f(e,"DIV",{class:!0});var $=z(J);g(ie.$$.fragment,$),lt=r($),ve=f($,"P",{"data-svelte-h":!0}),w(ve)!=="svelte-1jiibb0"&&(ve.textContent=Bt),rt=r($),Te=f($,"P",{"data-svelte-h":!0}),w(Te)!=="svelte-q52n56"&&(Te.innerHTML=zt),it=r($),Je=f($,"P",{"data-svelte-h":!0}),w(Je)!=="svelte-hswkmf"&&(Je.innerHTML=Ft),dt=r($),Z=f($,"DIV",{class:!0});var B=z(Z);g(de.$$.fragment,B),ct=r(B),je=f(B,"P",{"data-svelte-h":!0}),w(je)!=="svelte-r0kxjj"&&(je.innerHTML=Gt),mt=r(B),g(R.$$.fragment,B),pt=r(B),g(V.$$.fragment,B),B.forEach(o),$.forEach(o),Ae=r(e),g(ce.$$.fragment,e),Le=r(e),j=f(e,"DIV",{class:!0});var W=z(j);g(me.$$.fragment,W),ht=r(W),Ue=f(W,"P",{"data-svelte-h":!0}),w(Ue)!=="svelte-1i57dtz"&&(Ue.textContent=xt),ft=r(W),$e=f(W,"P",{"data-svelte-h":!0}),w($e)!=="svelte-q52n56"&&($e.innerHTML=Xt),ut=r(W),We=f(W,"P",{"data-svelte-h":!0}),w(We)!=="svelte-hswkmf"&&(We.innerHTML=Yt),gt=r(W),C=f(W,"DIV",{class:!0});var H=z(C);g(pe.$$.fragment,H),Mt=r(H),Ze=f(H,"P",{"data-svelte-h":!0}),w(Ze)!=="svelte-yly4aj"&&(Ze.innerHTML=Nt),bt=r(H),g(E.$$.fragment,H),_t=r(H),g(D.$$.fragment,H),H.forEach(o),W.forEach(o),Se=r(e),g(he.$$.fragment,e),Pe=r(e),Ce=f(e,"P",{}),z(Ce).forEach(o),this.h()},h(){ge(t,"name","hf:doc:metadata"),ge(t,"content",ls),qt(I,"float","right"),ge(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ge(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ge(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ge(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ge(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){m(document.head,t),i(e,p,n),i(e,s,n),i(e,d,n),i(e,c,n),i(e,T,n),i(e,I,n),i(e,Be,n),M(A,e,n),i(e,ze,n),i(e,L,n),i(e,Fe,n),i(e,S,n),i(e,Ge,n),M(x,e,n),i(e,xe,n),i(e,P,n),i(e,Xe,n),M(X,e,n),i(e,Ye,n),i(e,K,n),i(e,Ne,n),i(e,O,n),i(e,Re,n),M(ee,e,n),i(e,Ve,n),M(te,e,n),i(e,Ee,n),i(e,Y,n),m(Y,F),m(F,Me),m(F,Oe),m(F,be),m(F,et),M(se,F,null),m(Y,tt),m(Y,ne),m(ne,_e),m(ne,st),M(oe,ne,null),i(e,De,n),M(ae,e,n),i(e,He,n),i(e,U,n),M(le,U,null),m(U,nt),m(U,ye),m(U,ot),m(U,we),m(U,at),M(N,U,null),i(e,Qe,n),M(re,e,n),i(e,qe,n),i(e,J,n),M(ie,J,null),m(J,lt),m(J,ve),m(J,rt),m(J,Te),m(J,it),m(J,Je),m(J,dt),m(J,Z),M(de,Z,null),m(Z,ct),m(Z,je),m(Z,mt),M(R,Z,null),m(Z,pt),M(V,Z,null),i(e,Ae,n),M(ce,e,n),i(e,Le,n),i(e,j,n),M(me,j,null),m(j,ht),m(j,Ue),m(j,ft),m(j,$e),m(j,ut),m(j,We),m(j,gt),m(j,C),M(pe,C,null),m(C,Mt),m(C,Ze),m(C,bt),M(E,C,null),m(C,_t),M(D,C,null),i(e,Se,n),M(he,e,n),i(e,Pe,n),i(e,Ce,n),Ke=!0},p(e,[n]){const fe={};n&2&&(fe.$$scope={dirty:n,ctx:e}),x.$set(fe);const G={};n&2&&(G.$$scope={dirty:n,ctx:e}),X.$set(G);const ue={};n&2&&(ue.$$scope={dirty:n,ctx:e}),N.$set(ue);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),R.$set(k);const $={};n&2&&($.$$scope={dirty:n,ctx:e}),V.$set($);const B={};n&2&&(B.$$scope={dirty:n,ctx:e}),E.$set(B);const W={};n&2&&(W.$$scope={dirty:n,ctx:e}),D.$set(W)},i(e){Ke||(b(A.$$.fragment,e),b(x.$$.fragment,e),b(X.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(se.$$.fragment,e),b(oe.$$.fragment,e),b(ae.$$.fragment,e),b(le.$$.fragment,e),b(N.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(de.$$.fragment,e),b(R.$$.fragment,e),b(V.$$.fragment,e),b(ce.$$.fragment,e),b(me.$$.fragment,e),b(pe.$$.fragment,e),b(E.$$.fragment,e),b(D.$$.fragment,e),b(he.$$.fragment,e),Ke=!0)},o(e){_(A.$$.fragment,e),_(x.$$.fragment,e),_(X.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(se.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(le.$$.fragment,e),_(N.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(R.$$.fragment,e),_(V.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(E.$$.fragment,e),_(D.$$.fragment,e),_(he.$$.fragment,e),Ke=!1},d(e){e&&(o(p),o(s),o(d),o(c),o(T),o(I),o(Be),o(ze),o(L),o(Fe),o(S),o(Ge),o(xe),o(P),o(Xe),o(Ye),o(K),o(Ne),o(O),o(Re),o(Ve),o(Ee),o(Y),o(De),o(He),o(U),o(Qe),o(qe),o(J),o(Ae),o(Le),o(j),o(Se),o(Pe),o(Ce)),o(t),y(A,e),y(x,e),y(X,e),y(ee,e),y(te,e),y(se),y(oe),y(ae,e),y(le),y(N),y(re,e),y(ie),y(de),y(R),y(V),y(ce,e),y(me),y(pe),y(E),y(D),y(he,e)}}}const ls='{"title":"DINOv2","local":"dinov2","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Dinov2Config","local":"transformers.Dinov2Config","sections":[],"depth":2},{"title":"Dinov2Model","local":"transformers.Dinov2Model","sections":[],"depth":2},{"title":"Dinov2ForImageClassification","local":"transformers.Dinov2ForImageClassification","sections":[],"depth":2}],"depth":1}';function rs(v){return Et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gs extends Dt{constructor(t){super(),Ht(this,t,rs,as,Vt,{})}}export{gs as component};
