import{s as Ke,o as Oe,n as ee}from"../chunks/scheduler.18a86fab.js";import{S as et,i as tt,g as b,s as i,r as f,A as at,h as g,f as l,c as p,j as ze,u as d,x as w,k as qe,y as st,a as n,v as c,d as u,t as $,w as h}from"../chunks/index.98837b22.js";import{T as lt}from"../chunks/Tip.77304350.js";import{C as _}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as nt}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as We,E as ot}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as De,a as Ue}from"../chunks/HfOption.6641485e.js";function rt(y){let s,r;return s=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwcGVmdA==",highlighted:"pip install -U peft",wrap:!1}}),{c(){f(s.$$.fragment)},l(a){d(s.$$.fragment,a)},m(a,m){c(s,a,m),r=!0},p:ee,i(a){r||(u(s.$$.fragment,a),r=!0)},o(a){$(s.$$.fragment,a),r=!1},d(a){h(s,a)}}}function it(y){let s,r;return s=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMGdpdCUyQmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmh1Z2dpbmdmYWNlJTJGcGVmdC5naXQ=",highlighted:"pip install git+https://github.com/huggingface/peft.git",wrap:!1}}),{c(){f(s.$$.fragment)},l(a){d(s.$$.fragment,a)},m(a,m){c(s,a,m),r=!0},p:ee,i(a){r||(u(s.$$.fragment,a),r=!0)},o(a){$(s.$$.fragment,a),r=!1},d(a){h(s,a)}}}function pt(y){let s,r,a,m;return s=new Ue({props:{id:"install",option:"pip",$$slots:{default:[rt]},$$scope:{ctx:y}}}),a=new Ue({props:{id:"install",option:"source",$$slots:{default:[it]},$$scope:{ctx:y}}}),{c(){f(s.$$.fragment),r=i(),f(a.$$.fragment)},l(o){d(s.$$.fragment,o),r=p(o),d(a.$$.fragment,o)},m(o,M){c(s,o,M),n(o,r,M),c(a,o,M),m=!0},p(o,M){const T={};M&2&&(T.$$scope={dirty:M,ctx:o}),s.$set(T);const j={};M&2&&(j.$$scope={dirty:M,ctx:o}),a.$set(j)},i(o){m||(u(s.$$.fragment,o),u(a.$$.fragment,o),m=!0)},o(o){$(s.$$.fragment,o),$(a.$$.fragment,o),m=!1},d(o){o&&l(r),h(s,o),h(a,o)}}}function mt(y){let s,r="PEFT currently supports the LoRA, IA3, and AdaLoRA methods for Transformers. To use another PEFT method, such as prompt learning or prompt tuning, use the PEFT library directly.";return{c(){s=b("p"),s.textContent=r},l(a){s=g(a,"P",{"data-svelte-h":!0}),w(s)!=="svelte-1junfzf"&&(s.textContent=r)},m(a,m){n(a,s,m)},p:ee,d(a){a&&l(s)}}}function ft(y){let s,r;return s=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIya2xjc3AlMkZnZW1tYTdiLWxvcmEtYWxwYWNhLTExLXYxJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;klcsp/gemma7b-lora-alpaca-11-v1&quot;</span>)`,wrap:!1}}),{c(){f(s.$$.fragment)},l(a){d(s.$$.fragment,a)},m(a,m){c(s,a,m),r=!0},p:ee,i(a){r||(u(s.$$.fragment,a),r=!0)},o(a){$(s.$$.fragment,a),r=!1},d(a){h(s,a)}}}function dt(y){let s,r;return s=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtN2IlMjIpJTBBbW9kZWwubG9hZF9hZGFwdGVyKCUyMmtsY3NwJTJGZ2VtbWE3Yi1sb3JhLWFscGFjYS0xMS12MSUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-7b&quot;</span>)
model.load_adapter(<span class="hljs-string">&quot;klcsp/gemma7b-lora-alpaca-11-v1&quot;</span>)`,wrap:!1}}),{c(){f(s.$$.fragment)},l(a){d(s.$$.fragment,a)},m(a,m){c(s,a,m),r=!0},p:ee,i(a){r||(u(s.$$.fragment,a),r=!0)},o(a){$(s.$$.fragment,a),r=!1},d(a){h(s,a)}}}function ct(y){let s,r,a,m;return s=new Ue({props:{id:"load",option:"from_pretrained",$$slots:{default:[ft]},$$scope:{ctx:y}}}),a=new Ue({props:{id:"load",option:"load_adapter",$$slots:{default:[dt]},$$scope:{ctx:y}}}),{c(){f(s.$$.fragment),r=i(),f(a.$$.fragment)},l(o){d(s.$$.fragment,o),r=p(o),d(a.$$.fragment,o)},m(o,M){c(s,o,M),n(o,r,M),c(a,o,M),m=!0},p(o,M){const T={};M&2&&(T.$$scope={dirty:M,ctx:o}),s.$set(T);const j={};M&2&&(j.$$scope={dirty:M,ctx:o}),a.$set(j)},i(o){m||(u(s.$$.fragment,o),u(a.$$.fragment,o),m=!0)},o(o){$(s.$$.fragment,o),$(a.$$.fragment,o),m=!1},d(o){o&&l(r),h(s,o),h(a,o)}}}function ut(y){let s,r,a,m,o,M,T,j,k,Ge='<a href="https://huggingface.co/docs/peft/index" rel="nofollow">PEFT</a>, a library of parameter-efficient fine-tuning methods, enables training and storing large models on consumer GPUs. These methods only fine-tune a small number of extra model parameters, also known as adapters, on top of the pretrained model. A significant amount of memory is saved because the GPU doesnâ€™t need to store the optimizer states and gradients for the pretrained base model. Adapters are very lightweight, making it convenient to share, store, and load them.',te,Z,Ce='This guide provides a short introduction to the PEFT library and how to use it for training with Transformers. For more details, refer to the PEFT <a href="https://huggingface.co/docs/peft/index" rel="nofollow">documentation</a>.',ae,W,Fe="Install PEFT with the command below.",se,v,le,J,ne,U,xe='<a href="https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora" rel="nofollow">Low-Rank Adaptation (LoRA)</a> is a very common PEFT method that decomposes the weight matrix into two smaller trainable matrices. Start by defining a <a href="https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig" rel="nofollow">LoraConfig</a> object with the parameters shown below.',oe,G,re,C,Le='Add <a href="https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig" rel="nofollow">LoraConfig</a> to the model with <a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.add_adapter">add_adapter()</a>. The model is now ready to be passed to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> for training.',ie,F,pe,x,Xe='To add an additional trainable adapter on top of a model with an existing adapter attached, specify the modules you want to train in <a href="https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig.modules_to_save" rel="nofollow">modules_to_save()</a>.',me,L,Re='For example, to train the <code>lm_head</code> module on top of a causal language model with a LoRA adapter attached, set <code>modules_to_save=[&quot;lm_head&quot;]</code>. Add the adapter to the model as shown below, and then pass it to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>.',fe,X,de,R,Ye='Save your adapter with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> to reuse it.',ce,Y,ue,A,Ae='To load an adapter with Transformers, the Hub repository or local directory must contain an <code>adapter_config.json</code> file and the adapter weights. Load the adapter with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> or with <a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.load_adapter">load_adapter()</a>.',$e,B,he,E,Ee='For very large models, it is helpful to load a quantized version of the model in 8 or 4-bit precision to save memory. Transformers supports quantization with its <a href="https://huggingface.co/docs/bitsandbytes/index" rel="nofollow">bitsandbytes</a> integration. Specify in <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a> whether you want to load a model in 8 or 4-bit precision.',Me,I,Ie="For multiple devices, add <code>device_map=&quot;auto&quot;</code> to automatically distribute the model across your hardware.",be,H,ge,Q,ye,V,He='<a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.add_adapter">add_adapter()</a> adds a new adapter to a model. To add a second adapter, the new adapter must be the same type as the first adapter. Use the <code>adapter_name</code> parameter to assign a name to the adapter.',we,N,Te,P,Qe='Once added, use <a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.set_adapter">set_adapter()</a> to force a model to use the specified adapter and disable the other adapters.',_e,S,je,z,ve,q,Ve='<a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.enable_adapters">enable_adapters()</a> is a broader function that enables <em>all</em> adapters attached to a model, and <a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.disable_adapters">disable_adapters()</a> disables <em>all</em> attached adapters.',Je,D,Be,K,ke,O,Ze;return o=new We({props:{title:"PEFT",local:"peft",headingTag:"h1"}}),T=new nt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/peft.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/peft.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/peft.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/peft.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/peft.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/peft.ipynb"}]}}),v=new De({props:{id:"install",options:["pip","source"],$$slots:{default:[pt]},$$scope:{ctx:y}}}),J=new lt({props:{warning:!1,$$slots:{default:[mt]},$$scope:{ctx:y}}}),G=new _({props:{code:"ZnJvbSUyMHBlZnQlMjBpbXBvcnQlMjBMb3JhQ29uZmlnJTJDJTIwVGFza1R5cGUlMkMlMjBnZXRfcGVmdF9tb2RlbCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMGNyZWF0ZSUyMExvUkElMjBjb25maWd1cmF0aW9uJTIwb2JqZWN0JTBBbG9yYV9jb25maWclMjAlM0QlMjBMb3JhQ29uZmlnKCUwQSUyMCUyMCUyMCUyMHRhc2tfdHlwZSUzRFRhc2tUeXBlLkNBVVNBTF9MTSUyQyUyMCUyMyUyMHR5cGUlMjBvZiUyMHRhc2slMjB0byUyMHRyYWluJTIwb24lMEElMjAlMjAlMjAlMjBpbmZlcmVuY2VfbW9kZSUzREZhbHNlJTJDJTIwJTIzJTIwc2V0JTIwdG8lMjBGYWxzZSUyMGZvciUyMHRyYWluaW5nJTBBJTIwJTIwJTIwJTIwciUzRDglMkMlMjAlMjMlMjBkaW1lbnNpb24lMjBvZiUyMHRoZSUyMHNtYWxsZXIlMjBtYXRyaWNlcyUwQSUyMCUyMCUyMCUyMGxvcmFfYWxwaGElM0QzMiUyQyUyMCUyMyUyMHNjYWxpbmclMjBmYWN0b3IlMEElMjAlMjAlMjAlMjBsb3JhX2Ryb3BvdXQlM0QwLjElMjAlMjMlMjBkcm9wb3V0JTIwb2YlMjBMb1JBJTIwbGF5ZXJzJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-comment"># create LoRA configuration object</span>
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, <span class="hljs-comment"># type of task to train on</span>
    inference_mode=<span class="hljs-literal">False</span>, <span class="hljs-comment"># set to False for training</span>
    r=<span class="hljs-number">8</span>, <span class="hljs-comment"># dimension of the smaller matrices</span>
    lora_alpha=<span class="hljs-number">32</span>, <span class="hljs-comment"># scaling factor</span>
    lora_dropout=<span class="hljs-number">0.1</span> <span class="hljs-comment"># dropout of LoRA layers</span>
)`,wrap:!1}}),F=new _({props:{code:"bW9kZWwuYWRkX2FkYXB0ZXIobG9yYV9jb25maWclMkMlMjBhZGFwdGVyX25hbWUlM0QlMjJsb3JhXzElMjIpJTBBdHJhaW5lciUyMCUzRCUyMFRyYWluZXIobW9kZWwlM0Rtb2RlbCUyQyUyMC4uLiklMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`model.add_adapter(lora_config, adapter_name=<span class="hljs-string">&quot;lora_1&quot;</span>)
trainer = Trainer(model=model, ...)
trainer.train()`,wrap:!1}}),X=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBZnJvbSUyMHBlZnQlMjBpbXBvcnQlMjBMb3JhQ29uZmlnJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMi0yYiUyMiklMEElMEFsb3JhX2NvbmZpZyUyMCUzRCUyMExvcmFDb25maWcoJTBBJTIwJTIwJTIwJTIwdGFyZ2V0X21vZHVsZXMlM0QlNUIlMjJxX3Byb2olMjIlMkMlMjAlMjJrX3Byb2olMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBtb2R1bGVzX3RvX3NhdmUlM0QlNUIlMjJsbV9oZWFkJTIyJTVEJTJDJTBBKSUwQSUwQW1vZGVsLmFkZF9hZGFwdGVyKGxvcmFfY29uZmlnKSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKG1vZGVsJTNEbW9kZWwlMkMlMjAuLi4pJTBBdHJhaW5lci50cmFpbigp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-2b&quot;</span>)

lora_config = LoraConfig(
    target_modules=[<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>],
    modules_to_save=[<span class="hljs-string">&quot;lm_head&quot;</span>],
)

model.add_adapter(lora_config)
trainer = Trainer(model=model, ...)
trainer.train()`,wrap:!1}}),Y=new We({props:{title:"Load adapter",local:"load-adapter",headingTag:"h2"}}),B=new De({props:{id:"load",options:["from_pretrained","load_adapter"],$$slots:{default:[ct]},$$scope:{ctx:y}}}),H=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIya2xjc3AlMkZnZW1tYTdiLWxvcmEtYWxwYWNhLTExLXYxJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzREJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzhiaXQlM0RUcnVlKSUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;klcsp/gemma7b-lora-alpaca-11-v1&quot;</span>,
    quantization_config=BitsAndBytesConfig(load_in_8bit=<span class="hljs-literal">True</span>),
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
)`,wrap:!1}}),Q=new We({props:{title:"Set adapter",local:"set-adapter",headingTag:"h2"}}),N=new _({props:{code:"bW9kZWwuYWRkX2FkYXB0ZXIobG9yYV9jb25maWclMkMlMjBhZGFwdGVyX25hbWUlM0QlMjJsb3JhXzIlMjIp",highlighted:'model.add_adapter(lora_config, adapter_name=<span class="hljs-string">&quot;lora_2&quot;</span>)',wrap:!1}}),S=new _({props:{code:"bW9kZWwuc2V0X2FkYXB0ZXIoJTIybG9yYV8yJTIyKQ==",highlighted:'model.set_adapter(<span class="hljs-string">&quot;lora_2&quot;</span>)',wrap:!1}}),z=new We({props:{title:"Enable and disable adapter",local:"enable-and-disable-adapter",headingTag:"h2"}}),D=new _({props:{code:"bW9kZWwuYWRkX2FkYXB0ZXIobG9yYV8xKSUwQW1vZGVsLmFkZF9hZGFwdGVyKGxvcmFfMiklMEFtb2RlbC5lbmFibGVfYWRhcHRlcnMoKSUwQSUwQSUyMyUyMGRpc2FibGUlMjBhbGwlMjBhZGFwdGVycyUwQW1vZGVsLmRpc2FibGVfYWRhcHRlcnMoKQ==",highlighted:`model.add_adapter(lora_1)
model.add_adapter(lora_2)
model.enable_adapters()

<span class="hljs-comment"># disable all adapters</span>
model.disable_adapters()`,wrap:!1}}),K=new ot({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md"}}),{c(){s=b("meta"),r=i(),a=b("p"),m=i(),f(o.$$.fragment),M=i(),f(T.$$.fragment),j=i(),k=b("p"),k.innerHTML=Ge,te=i(),Z=b("p"),Z.innerHTML=Ce,ae=i(),W=b("p"),W.textContent=Fe,se=i(),f(v.$$.fragment),le=i(),f(J.$$.fragment),ne=i(),U=b("p"),U.innerHTML=xe,oe=i(),f(G.$$.fragment),re=i(),C=b("p"),C.innerHTML=Le,ie=i(),f(F.$$.fragment),pe=i(),x=b("p"),x.innerHTML=Xe,me=i(),L=b("p"),L.innerHTML=Re,fe=i(),f(X.$$.fragment),de=i(),R=b("p"),R.innerHTML=Ye,ce=i(),f(Y.$$.fragment),ue=i(),A=b("p"),A.innerHTML=Ae,$e=i(),f(B.$$.fragment),he=i(),E=b("p"),E.innerHTML=Ee,Me=i(),I=b("p"),I.innerHTML=Ie,be=i(),f(H.$$.fragment),ge=i(),f(Q.$$.fragment),ye=i(),V=b("p"),V.innerHTML=He,we=i(),f(N.$$.fragment),Te=i(),P=b("p"),P.innerHTML=Qe,_e=i(),f(S.$$.fragment),je=i(),f(z.$$.fragment),ve=i(),q=b("p"),q.innerHTML=Ve,Je=i(),f(D.$$.fragment),Be=i(),f(K.$$.fragment),ke=i(),O=b("p"),this.h()},l(e){const t=at("svelte-u9bgzb",document.head);s=g(t,"META",{name:!0,content:!0}),t.forEach(l),r=p(e),a=g(e,"P",{}),ze(a).forEach(l),m=p(e),d(o.$$.fragment,e),M=p(e),d(T.$$.fragment,e),j=p(e),k=g(e,"P",{"data-svelte-h":!0}),w(k)!=="svelte-ogk6az"&&(k.innerHTML=Ge),te=p(e),Z=g(e,"P",{"data-svelte-h":!0}),w(Z)!=="svelte-1e9h156"&&(Z.innerHTML=Ce),ae=p(e),W=g(e,"P",{"data-svelte-h":!0}),w(W)!=="svelte-jje5xh"&&(W.textContent=Fe),se=p(e),d(v.$$.fragment,e),le=p(e),d(J.$$.fragment,e),ne=p(e),U=g(e,"P",{"data-svelte-h":!0}),w(U)!=="svelte-1dpe4wu"&&(U.innerHTML=xe),oe=p(e),d(G.$$.fragment,e),re=p(e),C=g(e,"P",{"data-svelte-h":!0}),w(C)!=="svelte-1qg2vx6"&&(C.innerHTML=Le),ie=p(e),d(F.$$.fragment,e),pe=p(e),x=g(e,"P",{"data-svelte-h":!0}),w(x)!=="svelte-w99izm"&&(x.innerHTML=Xe),me=p(e),L=g(e,"P",{"data-svelte-h":!0}),w(L)!=="svelte-156ucc2"&&(L.innerHTML=Re),fe=p(e),d(X.$$.fragment,e),de=p(e),R=g(e,"P",{"data-svelte-h":!0}),w(R)!=="svelte-1092zv3"&&(R.innerHTML=Ye),ce=p(e),d(Y.$$.fragment,e),ue=p(e),A=g(e,"P",{"data-svelte-h":!0}),w(A)!=="svelte-gf7k0v"&&(A.innerHTML=Ae),$e=p(e),d(B.$$.fragment,e),he=p(e),E=g(e,"P",{"data-svelte-h":!0}),w(E)!=="svelte-v17ie2"&&(E.innerHTML=Ee),Me=p(e),I=g(e,"P",{"data-svelte-h":!0}),w(I)!=="svelte-g510r2"&&(I.innerHTML=Ie),be=p(e),d(H.$$.fragment,e),ge=p(e),d(Q.$$.fragment,e),ye=p(e),V=g(e,"P",{"data-svelte-h":!0}),w(V)!=="svelte-wk1cgl"&&(V.innerHTML=He),we=p(e),d(N.$$.fragment,e),Te=p(e),P=g(e,"P",{"data-svelte-h":!0}),w(P)!=="svelte-gyrptz"&&(P.innerHTML=Qe),_e=p(e),d(S.$$.fragment,e),je=p(e),d(z.$$.fragment,e),ve=p(e),q=g(e,"P",{"data-svelte-h":!0}),w(q)!=="svelte-od9e2c"&&(q.innerHTML=Ve),Je=p(e),d(D.$$.fragment,e),Be=p(e),d(K.$$.fragment,e),ke=p(e),O=g(e,"P",{}),ze(O).forEach(l),this.h()},h(){qe(s,"name","hf:doc:metadata"),qe(s,"content",$t)},m(e,t){st(document.head,s),n(e,r,t),n(e,a,t),n(e,m,t),c(o,e,t),n(e,M,t),c(T,e,t),n(e,j,t),n(e,k,t),n(e,te,t),n(e,Z,t),n(e,ae,t),n(e,W,t),n(e,se,t),c(v,e,t),n(e,le,t),c(J,e,t),n(e,ne,t),n(e,U,t),n(e,oe,t),c(G,e,t),n(e,re,t),n(e,C,t),n(e,ie,t),c(F,e,t),n(e,pe,t),n(e,x,t),n(e,me,t),n(e,L,t),n(e,fe,t),c(X,e,t),n(e,de,t),n(e,R,t),n(e,ce,t),c(Y,e,t),n(e,ue,t),n(e,A,t),n(e,$e,t),c(B,e,t),n(e,he,t),n(e,E,t),n(e,Me,t),n(e,I,t),n(e,be,t),c(H,e,t),n(e,ge,t),c(Q,e,t),n(e,ye,t),n(e,V,t),n(e,we,t),c(N,e,t),n(e,Te,t),n(e,P,t),n(e,_e,t),c(S,e,t),n(e,je,t),c(z,e,t),n(e,ve,t),n(e,q,t),n(e,Je,t),c(D,e,t),n(e,Be,t),c(K,e,t),n(e,ke,t),n(e,O,t),Ze=!0},p(e,[t]){const Ne={};t&2&&(Ne.$$scope={dirty:t,ctx:e}),v.$set(Ne);const Pe={};t&2&&(Pe.$$scope={dirty:t,ctx:e}),J.$set(Pe);const Se={};t&2&&(Se.$$scope={dirty:t,ctx:e}),B.$set(Se)},i(e){Ze||(u(o.$$.fragment,e),u(T.$$.fragment,e),u(v.$$.fragment,e),u(J.$$.fragment,e),u(G.$$.fragment,e),u(F.$$.fragment,e),u(X.$$.fragment,e),u(Y.$$.fragment,e),u(B.$$.fragment,e),u(H.$$.fragment,e),u(Q.$$.fragment,e),u(N.$$.fragment,e),u(S.$$.fragment,e),u(z.$$.fragment,e),u(D.$$.fragment,e),u(K.$$.fragment,e),Ze=!0)},o(e){$(o.$$.fragment,e),$(T.$$.fragment,e),$(v.$$.fragment,e),$(J.$$.fragment,e),$(G.$$.fragment,e),$(F.$$.fragment,e),$(X.$$.fragment,e),$(Y.$$.fragment,e),$(B.$$.fragment,e),$(H.$$.fragment,e),$(Q.$$.fragment,e),$(N.$$.fragment,e),$(S.$$.fragment,e),$(z.$$.fragment,e),$(D.$$.fragment,e),$(K.$$.fragment,e),Ze=!1},d(e){e&&(l(r),l(a),l(m),l(M),l(j),l(k),l(te),l(Z),l(ae),l(W),l(se),l(le),l(ne),l(U),l(oe),l(re),l(C),l(ie),l(pe),l(x),l(me),l(L),l(fe),l(de),l(R),l(ce),l(ue),l(A),l($e),l(he),l(E),l(Me),l(I),l(be),l(ge),l(ye),l(V),l(we),l(Te),l(P),l(_e),l(je),l(ve),l(q),l(Je),l(Be),l(ke),l(O)),l(s),h(o,e),h(T,e),h(v,e),h(J,e),h(G,e),h(F,e),h(X,e),h(Y,e),h(B,e),h(H,e),h(Q,e),h(N,e),h(S,e),h(z,e),h(D,e),h(K,e)}}}const $t='{"title":"PEFT","local":"peft","sections":[{"title":"Load adapter","local":"load-adapter","sections":[],"depth":2},{"title":"Set adapter","local":"set-adapter","sections":[],"depth":2},{"title":"Enable and disable adapter","local":"enable-and-disable-adapter","sections":[],"depth":2}],"depth":1}';function ht(y){return Oe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jt extends et{constructor(s){super(),tt(this,s,ht,ut,Ke,{})}}export{jt as component};
