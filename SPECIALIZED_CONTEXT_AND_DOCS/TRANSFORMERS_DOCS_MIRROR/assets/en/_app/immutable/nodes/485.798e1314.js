import{s as Ue,o as xe,n as fe}from"../chunks/scheduler.18a86fab.js";import{S as je,i as Ce,g as i,s,r as c,A as ke,h as m,f as n,c as a,j as _e,u,x as f,k as Je,y as We,a as o,v as d,d as h,t as $,w as T}from"../chunks/index.98837b22.js";import{T as me}from"../chunks/Tip.77304350.js";import{C as pe}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ie,E as Be}from"../chunks/getInferenceSnippets.06c2775f.js";function Ze(b){let r,p='Optimum includes an <a href="https://hf.co/docs/optimum/intel/index" rel="nofollow">Intel</a> extension that provides additional optimizations such as quantization, pruning, and knowledge distillation for Intel CPUs. This extension also includes tools to convert models to <a href="https://hf.co/docs/optimum/intel/inference" rel="nofollow">OpenVINO</a>, a toolkit for optimizing and deploying models, for even faster inference.';return{c(){r=i("p"),r.innerHTML=p},l(l){r=m(l,"P",{"data-svelte-h":!0}),f(r)!=="svelte-11iqel"&&(r.innerHTML=p)},m(l,w){o(l,r,w)},p:fe,d(l){l&&n(r)}}}function Ie(b){let r,p='BetterTransformer isnâ€™t supported for all models. Check this <a href="https://hf.co/docs/optimum/bettertransformer/overview#supported-models" rel="nofollow">list</a> to see whether a model supports BetterTransformer.';return{c(){r=i("p"),r.innerHTML=p},l(l){r=m(l,"P",{"data-svelte-h":!0}),f(r)!=="svelte-5iltey"&&(r.innerHTML=p)},m(l,w){o(l,r,w)},p:fe,d(l){l&&n(r)}}}function Xe(b){let r,p='Refer to the <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html" rel="nofollow">Introduction to PyTorch TorchScript</a> tutorial for a gentle introduction to TorchScript.';return{c(){r=i("p"),r.innerHTML=p},l(l){r=m(l,"P",{"data-svelte-h":!0}),f(r)!=="svelte-9qd11f"&&(r.innerHTML=p)},m(l,w){o(l,r,w)},p:fe,d(l){l&&n(r)}}}function qe(b){let r,p,l,w,v,P,_,ce="CPUs are a viable and cost-effective inference option. With a few optimization methods, it is possible to achieve good performance with large models on CPUs. These methods include fusing kernels to reduce overhead and compiling your code to a faster intermediate format that can be deployed in production environments.",V,J,ue="This guide will show you a few ways to optimize inference on a CPU.",R,U,N,x,de='<a href="https://hf.co/docs/optimum/en/index" rel="nofollow">Optimum</a> is a Hugging Face library focused on optimizing model performance across various hardware. It supports <a href="https://onnxruntime.ai/docs/" rel="nofollow">ONNX Runtime</a> (ORT), a model accelerator, for a wide range of hardware and frameworks including CPUs.',Q,j,he='Optimum provides the <a href="https://huggingface.co/docs/optimum/v1.27.0/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel" rel="nofollow">ORTModel</a> class for loading ONNX models. For example, load the <a href="https://hf.co/optimum/roberta-base-squad2" rel="nofollow">optimum/roberta-base-squad2</a> checkpoint for question answering inference. This checkpoint contains a <a href="https://hf.co/optimum/roberta-base-squad2/blob/main/model.onnx" rel="nofollow">model.onnx</a> file.',Y,C,z,M,E,k,F,W,$e='<a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow">BetterTransformer</a> is a <em>fastpath</em> execution of specialized Transformers functions directly on the hardware level such as a CPU. There are two main components of the fastpath execution.',O,B,Te="<li>fusing multiple operations into a single kernel for faster and more efficient execution</li> <li>skipping unnecessary computation of padding tokens with nested tensors</li>",A,y,D,Z,we='BetterTransformer is available through Optimum with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer">to_bettertransformer()</a>.',K,I,ee,X,te,q,be='<a href="https://pytorch.org/docs/stable/jit.html" rel="nofollow">TorchScript</a> is an intermediate PyTorch model format that can be run in non-Python environments, like C++, where performance is critical. Train a PyTorch model and convert it to a TorchScript function or module with <a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html" rel="nofollow">torch.jit.trace</a>. This function optimizes the model with just-in-time (JIT) compilation, and compared to the default eager mode, JIT-compiled models offer better inference performance.',ne,g,oe,G,Me='On a CPU, enable <code>torch.jit.trace</code> with the <code>--jit_mode_eval</code> flag in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>.',re,S,le,H,se,L,ae;return v=new ie({props:{title:"CPU",local:"cpu",headingTag:"h1"}}),U=new ie({props:{title:"Optimum",local:"optimum",headingTag:"h2"}}),C=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLm9ubnhydW50aW1lJTIwaW1wb3J0JTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQW9ubnhfcWElMjAlM0QlMjBwaXBlbGluZSglMjJxdWVzdGlvbi1hbnN3ZXJpbmclMjIlMkMlMjBtb2RlbCUzRCUyMm9wdGltdW0lMkZyb2JlcnRhLWJhc2Utc3F1YWQyJTIyJTJDJTIwdG9rZW5pemVyJTNEJTIyZGVlcHNldCUyRnJvYmVydGEtYmFzZS1zcXVhZDIlMjIpJTBBJTBBcXVlc3Rpb24lMjAlM0QlMjAlMjJXaGF0J3MlMjBteSUyMG5hbWUlM0YlMjIlMEFjb250ZXh0JTIwJTNEJTIwJTIyTXklMjBuYW1lJTIwaXMlMjBQaGlsaXBwJTIwYW5kJTIwSSUyMGxpdmUlMjBpbiUyME51cmVtYmVyZy4lMjIlMEFwcmVkJTIwJTNEJTIwb25ueF9xYShxdWVzdGlvbiUyQyUyMGNvbnRleHQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering

onnx_qa = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>, tokenizer=<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)

question = <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>
context = <span class="hljs-string">&quot;My name is Philipp and I live in Nuremberg.&quot;</span>
pred = onnx_qa(question, context)`,wrap:!1}}),M=new me({props:{warning:!1,$$slots:{default:[Ze]},$$scope:{ctx:b}}}),k=new ie({props:{title:"BetterTransformer",local:"bettertransformer",headingTag:"h3"}}),y=new me({props:{warning:!0,$$slots:{default:[Ie]},$$scope:{ctx:b}}}),I=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnc2NpZW5jZSUyRmJsb29tJTIyKSUwQW1vZGVsJTIwJTNEJTIwbW9kZWwudG9fYmV0dGVydHJhbnNmb3JtZXIoKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom&quot;</span>)
model = model.to_bettertransformer()`,wrap:!1}}),X=new ie({props:{title:"TorchScript",local:"torchscript",headingTag:"h2"}}),g=new me({props:{warning:!1,$$slots:{default:[Xe]},$$scope:{ctx:b}}}),S=new pe({props:{code:"cHl0aG9uJTIwZXhhbXBsZXMlMkZweXRvcmNoJTJGcXVlc3Rpb24tYW5zd2VyaW5nJTJGcnVuX3FhLnB5JTIwJTVDJTBBLS1tb2RlbF9uYW1lX29yX3BhdGglMjBjc2Fycm9uJTJGYmVydC1iYXNlLXVuY2FzZWQtc3F1YWQtdjElMjAlNUMlMEEtLWRhdGFzZXRfbmFtZSUyMHNxdWFkJTIwJTVDJTBBLS1kb19ldmFsJTIwJTVDJTBBLS1tYXhfc2VxX2xlbmd0aCUyMDM4NCUyMCU1QyUwQS0tZG9jX3N0cmlkZSUyMDEyOCUyMCU1QyUwQS0tb3V0cHV0X2RpciUyMCUyRnRtcCUyRiUyMCU1QyUwQS0tbm9fY3VkYSUyMCU1QyUwQS0taml0X21vZGVfZXZhbA==",highlighted:`python examples/pytorch/question-answering/run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
--jit_mode_eval`,wrap:!1}}),H=new Be({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.md"}}),{c(){r=i("meta"),p=s(),l=i("p"),w=s(),c(v.$$.fragment),P=s(),_=i("p"),_.textContent=ce,V=s(),J=i("p"),J.textContent=ue,R=s(),c(U.$$.fragment),N=s(),x=i("p"),x.innerHTML=de,Q=s(),j=i("p"),j.innerHTML=he,Y=s(),c(C.$$.fragment),z=s(),c(M.$$.fragment),E=s(),c(k.$$.fragment),F=s(),W=i("p"),W.innerHTML=$e,O=s(),B=i("ul"),B.innerHTML=Te,A=s(),c(y.$$.fragment),D=s(),Z=i("p"),Z.innerHTML=we,K=s(),c(I.$$.fragment),ee=s(),c(X.$$.fragment),te=s(),q=i("p"),q.innerHTML=be,ne=s(),c(g.$$.fragment),oe=s(),G=i("p"),G.innerHTML=Me,re=s(),c(S.$$.fragment),le=s(),c(H.$$.fragment),se=s(),L=i("p"),this.h()},l(e){const t=ke("svelte-u9bgzb",document.head);r=m(t,"META",{name:!0,content:!0}),t.forEach(n),p=a(e),l=m(e,"P",{}),_e(l).forEach(n),w=a(e),u(v.$$.fragment,e),P=a(e),_=m(e,"P",{"data-svelte-h":!0}),f(_)!=="svelte-17hx76f"&&(_.textContent=ce),V=a(e),J=m(e,"P",{"data-svelte-h":!0}),f(J)!=="svelte-jkeb6c"&&(J.textContent=ue),R=a(e),u(U.$$.fragment,e),N=a(e),x=m(e,"P",{"data-svelte-h":!0}),f(x)!=="svelte-147ma2r"&&(x.innerHTML=de),Q=a(e),j=m(e,"P",{"data-svelte-h":!0}),f(j)!=="svelte-1cnmwme"&&(j.innerHTML=he),Y=a(e),u(C.$$.fragment,e),z=a(e),u(M.$$.fragment,e),E=a(e),u(k.$$.fragment,e),F=a(e),W=m(e,"P",{"data-svelte-h":!0}),f(W)!=="svelte-fgop2b"&&(W.innerHTML=$e),O=a(e),B=m(e,"UL",{"data-svelte-h":!0}),f(B)!=="svelte-1duvg1i"&&(B.innerHTML=Te),A=a(e),u(y.$$.fragment,e),D=a(e),Z=m(e,"P",{"data-svelte-h":!0}),f(Z)!=="svelte-1133588"&&(Z.innerHTML=we),K=a(e),u(I.$$.fragment,e),ee=a(e),u(X.$$.fragment,e),te=a(e),q=m(e,"P",{"data-svelte-h":!0}),f(q)!=="svelte-qhdjso"&&(q.innerHTML=be),ne=a(e),u(g.$$.fragment,e),oe=a(e),G=m(e,"P",{"data-svelte-h":!0}),f(G)!=="svelte-1jbuqin"&&(G.innerHTML=Me),re=a(e),u(S.$$.fragment,e),le=a(e),u(H.$$.fragment,e),se=a(e),L=m(e,"P",{}),_e(L).forEach(n),this.h()},h(){Je(r,"name","hf:doc:metadata"),Je(r,"content",Ge)},m(e,t){We(document.head,r),o(e,p,t),o(e,l,t),o(e,w,t),d(v,e,t),o(e,P,t),o(e,_,t),o(e,V,t),o(e,J,t),o(e,R,t),d(U,e,t),o(e,N,t),o(e,x,t),o(e,Q,t),o(e,j,t),o(e,Y,t),d(C,e,t),o(e,z,t),d(M,e,t),o(e,E,t),d(k,e,t),o(e,F,t),o(e,W,t),o(e,O,t),o(e,B,t),o(e,A,t),d(y,e,t),o(e,D,t),o(e,Z,t),o(e,K,t),d(I,e,t),o(e,ee,t),d(X,e,t),o(e,te,t),o(e,q,t),o(e,ne,t),d(g,e,t),o(e,oe,t),o(e,G,t),o(e,re,t),d(S,e,t),o(e,le,t),d(H,e,t),o(e,se,t),o(e,L,t),ae=!0},p(e,[t]){const ye={};t&2&&(ye.$$scope={dirty:t,ctx:e}),M.$set(ye);const ge={};t&2&&(ge.$$scope={dirty:t,ctx:e}),y.$set(ge);const ve={};t&2&&(ve.$$scope={dirty:t,ctx:e}),g.$set(ve)},i(e){ae||(h(v.$$.fragment,e),h(U.$$.fragment,e),h(C.$$.fragment,e),h(M.$$.fragment,e),h(k.$$.fragment,e),h(y.$$.fragment,e),h(I.$$.fragment,e),h(X.$$.fragment,e),h(g.$$.fragment,e),h(S.$$.fragment,e),h(H.$$.fragment,e),ae=!0)},o(e){$(v.$$.fragment,e),$(U.$$.fragment,e),$(C.$$.fragment,e),$(M.$$.fragment,e),$(k.$$.fragment,e),$(y.$$.fragment,e),$(I.$$.fragment,e),$(X.$$.fragment,e),$(g.$$.fragment,e),$(S.$$.fragment,e),$(H.$$.fragment,e),ae=!1},d(e){e&&(n(p),n(l),n(w),n(P),n(_),n(V),n(J),n(R),n(N),n(x),n(Q),n(j),n(Y),n(z),n(E),n(F),n(W),n(O),n(B),n(A),n(D),n(Z),n(K),n(ee),n(te),n(q),n(ne),n(oe),n(G),n(re),n(le),n(se),n(L)),n(r),T(v,e),T(U,e),T(C,e),T(M,e),T(k,e),T(y,e),T(I,e),T(X,e),T(g,e),T(S,e),T(H,e)}}}const Ge='{"title":"CPU","local":"cpu","sections":[{"title":"Optimum","local":"optimum","sections":[{"title":"BetterTransformer","local":"bettertransformer","sections":[],"depth":3}],"depth":2},{"title":"TorchScript","local":"torchscript","sections":[],"depth":2}],"depth":1}';function Se(b){return xe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ne extends je{constructor(r){super(),Ce(this,r,Se,qe,Ue,{})}}export{Ne as component};
