import{s as Bn,o as $n,n as z}from"../chunks/scheduler.18a86fab.js";import{S as jn,i as Cn,g as u,s as i,r as M,A as Un,h as f,f as a,c as l,j as N,x as k,u as _,k as V,l as xn,y as d,a as p,v as b,d as T,t as w,w as y}from"../chunks/index.98837b22.js";import{T as et}from"../chunks/Tip.77304350.js";import{D as ee}from"../chunks/Docstring.a1ef7999.js";import{C as we}from"../chunks/CodeBlock.8d0c2e8a.js";import{F as zn,M as Fn}from"../chunks/Markdown.ae01904b.js";import{E as tt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ke,E as Wn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Zn,a as an}from"../chunks/HfOption.6641485e.js";function qn(J){let e,c="Click on the ModernBERT models in the right sidebar for more examples of how to apply ModernBERT to different language tasks.";return{c(){e=u("p"),e.textContent=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-muylgb"&&(e.textContent=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function In(J){let e,c;return e=new we({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJmaWxsLW1hc2slMjIlMkMlMEElMjAlMjAlMjAlMjBtb2RlbCUzRCUyMmFuc3dlcmRvdGFpJTJGTW9kZXJuQkVSVC1iYXNlJTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlJTNEMCUwQSklMEFwaXBlbGluZSglMjJQbGFudHMlMjBjcmVhdGUlMjAlNUJNQVNLJTVEJTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMHBob3Rvc3ludGhlc2lzLiUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;fill-mask&quot;</span>,
    model=<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;Plants create [MASK] through a process known as photosynthesis.&quot;</span>)`,wrap:!1}}),{c(){M(e.$$.fragment)},l(t){_(e.$$.fragment,t)},m(t,s){b(e,t,s),c=!0},p:z,i(t){c||(T(e.$$.fragment,t),c=!0)},o(t){w(e.$$.fragment,t),c=!1},d(t){y(e,t)}}}function Rn(J){let e,c;return e=new we({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yTWFza2VkTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIlMkMlMEEpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJzZHBhJTIyJTBBKSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJQbGFudHMlMjBjcmVhdGUlMjAlNUJNQVNLJTVEJTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMHBob3Rvc3ludGhlc2lzLiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMEFtYXNrZWRfaW5kZXglMjAlM0QlMjB0b3JjaC53aGVyZShpbnB1dHMlNUInaW5wdXRfaWRzJyU1RCUyMCUzRCUzRCUyMHRva2VuaXplci5tYXNrX3Rva2VuX2lkKSU1QjElNUQlMEFwcmVkaWN0ZWRfdG9rZW5faWQlMjAlM0QlMjBwcmVkaWN0aW9ucyU1QjAlMkMlMjBtYXNrZWRfaW5kZXglNUQuYXJnbWF4KGRpbSUzRC0xKSUwQXByZWRpY3RlZF90b2tlbiUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUocHJlZGljdGVkX3Rva2VuX2lkKSUwQSUwQXByaW50KGYlMjJUaGUlMjBwcmVkaWN0ZWQlMjB0b2tlbiUyMGlzJTNBJTIwJTdCcHJlZGljdGVkX3Rva2VuJTdEJTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    <span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>,
)
model = AutoModelForMaskedLM.from_pretrained(
    <span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>
)
inputs = tokenizer(<span class="hljs-string">&quot;Plants create [MASK] through a process known as photosynthesis.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
predicted_token_id = predictions[<span class="hljs-number">0</span>, masked_index].argmax(dim=-<span class="hljs-number">1</span>)
predicted_token = tokenizer.decode(predicted_token_id)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted token is: <span class="hljs-subst">{predicted_token}</span>&quot;</span>)`,wrap:!1}}),{c(){M(e.$$.fragment)},l(t){_(e.$$.fragment,t)},m(t,s){b(e,t,s),c=!0},p:z,i(t){c||(T(e.$$.fragment,t),c=!0)},o(t){w(e.$$.fragment,t),c=!1},d(t){y(e,t)}}}function Gn(J){let e,c;return e=new we({props:{code:"ZWNobyUyMC1lJTIwJTIyUGxhbnRzJTIwY3JlYXRlJTIwJTVCTUFTSyU1RCUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjBwaG90b3N5bnRoZXNpcy4lMjIlMjAlN0MlMjB0cmFuc2Zvcm1lcnMlMjBydW4lMjAtLXRhc2slMjBmaWxsLW1hc2slMjAtLW1vZGVsJTIwYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjAtLWRldmljZSUyMDA=",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Plants create [MASK] through a process known as photosynthesis.&quot;</span> | transformers run --task fill-mask --model answerdotai/ModernBERT-base --device 0',wrap:!1}}),{c(){M(e.$$.fragment)},l(t){_(e.$$.fragment,t)},m(t,s){b(e,t,s),c=!0},p:z,i(t){c||(T(e.$$.fragment,t),c=!0)},o(t){w(e.$$.fragment,t),c=!1},d(t){y(e,t)}}}function Nn(J){let e,c,t,s,g,o;return e=new an({props:{id:"usage",option:"Pipeline",$$slots:{default:[In]},$$scope:{ctx:J}}}),t=new an({props:{id:"usage",option:"AutoModel",$$slots:{default:[Rn]},$$scope:{ctx:J}}}),g=new an({props:{id:"usage",option:"transformers CLI",$$slots:{default:[Gn]},$$scope:{ctx:J}}}),{c(){M(e.$$.fragment),c=i(),M(t.$$.fragment),s=i(),M(g.$$.fragment)},l(m){_(e.$$.fragment,m),c=l(m),_(t.$$.fragment,m),s=l(m),_(g.$$.fragment,m)},m(m,B){b(e,m,B),p(m,c,B),b(t,m,B),p(m,s,B),b(g,m,B),o=!0},p(m,B){const te={};B&2&&(te.$$scope={dirty:B,ctx:m}),e.$set(te);const ge={};B&2&&(ge.$$scope={dirty:B,ctx:m}),t.$set(ge);const X={};B&2&&(X.$$scope={dirty:B,ctx:m}),g.$set(X)},i(m){o||(T(e.$$.fragment,m),T(t.$$.fragment,m),T(g.$$.fragment,m),o=!0)},o(m){w(e.$$.fragment,m),w(t.$$.fragment,m),w(g.$$.fragment,m),o=!1},d(m){m&&(a(c),a(s)),y(e,m),y(t,m),y(g,m)}}}function Vn(J){let e,c="Examples:",t,s,g;return s=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1vZGVybkJlcnRNb2RlbCUyQyUyME1vZGVybkJlcnRDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTW9kZXJuQmVydCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBNb2Rlcm5CZXJ0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwbW9kZXJuYmVydC1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBNb2Rlcm5CZXJ0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ModernBertModel, ModernBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ModernBert style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ModernBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the modernbert-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-kvfsh7"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Xn(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function En(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function Hn(J){let e,c="Example:",t,s,g;return s=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNb2Rlcm5CZXJ0Rm9yTWFza2VkTE0lMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFuc3dlcmRvdGFpJTJGTW9kZXJuQkVSVC1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwTW9kZXJuQmVydEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJhbnN3ZXJkb3RhaSUyRk1vZGVybkJFUlQtYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhlJTIwY2FwaXRhbCUyMG9mJTIwRnJhbmNlJTIwaXMlMjAlM0NtYXNrJTNFLiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjByZXRyaWV2ZSUyMGluZGV4JTIwb2YlMjAlM0NtYXNrJTNFJTBBbWFza190b2tlbl9pbmRleCUyMCUzRCUyMChpbnB1dHMuaW5wdXRfaWRzJTIwJTNEJTNEJTIwdG9rZW5pemVyLm1hc2tfdG9rZW5faWQpJTVCMCU1RC5ub256ZXJvKGFzX3R1cGxlJTNEVHJ1ZSklNUIwJTVEJTBBJTBBcHJlZGljdGVkX3Rva2VuX2lkJTIwJTNEJTIwbG9naXRzJTVCMCUyQyUyMG1hc2tfdG9rZW5faW5kZXglNUQuYXJnbWF4KGF4aXMlM0QtMSklMEF0b2tlbml6ZXIuZGVjb2RlKHByZWRpY3RlZF90b2tlbl9pZCklMEElMEFsYWJlbHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhlJTIwY2FwaXRhbCUyMG9mJTIwRnJhbmNlJTIwaXMlMjBQYXJpcy4lMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQSUyMyUyMG1hc2slMjBsYWJlbHMlMjBvZiUyMG5vbi0lM0NtYXNrJTNFJTIwdG9rZW5zJTBBbGFiZWxzJTIwJTNEJTIwdG9yY2gud2hlcmUoaW5wdXRzLmlucHV0X2lkcyUyMCUzRCUzRCUyMHRva2VuaXplci5tYXNrX3Rva2VuX2lkJTJDJTIwbGFiZWxzJTJDJTIwLTEwMCklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjBsYWJlbHMlM0RsYWJlbHMpJTBBcm91bmQob3V0cHV0cy5sb3NzLml0ZW0oKSUyQyUyMDIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of &lt;mask&gt;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
...

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-&lt;mask&gt; tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-11lpom8"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Qn(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function Ln(J){let e,c="Example of single-label classification:",t,s,g;return s=new we({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyME1vZGVybkJlcnRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2Rlcm5CZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBcHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heCgpLml0ZW0oKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RCUwQSUwQSUyMyUyMFRvJTIwdHJhaW4lMjBhJTIwbW9kZWwlMjBvbiUyMCU2MG51bV9sYWJlbHMlNjAlMjBjbGFzc2VzJTJDJTIweW91JTIwY2FuJTIwcGFzcyUyMCU2MG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTYwJTIwdG8lMjAlNjAuZnJvbV9wcmV0cmFpbmVkKC4uLiklNjAlMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKG1vZGVsLmNvbmZpZy5pZDJsYWJlbCklMEFtb2RlbCUyMCUzRCUyME1vZGVybkJlcnRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJhbnN3ZXJkb3RhaSUyRk1vZGVybkJFUlQtYmFzZSUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzKSUwQSUwQWxhYmVscyUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIxJTVEKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
...

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-ykxpe4"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Sn(J){let e,c="Example of multi-label classification:",t,s,g;return s=new we({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyME1vZGVybkJlcnRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2Rlcm5CZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIlMkMlMjBwcm9ibGVtX3R5cGUlM0QlMjJtdWx0aV9sYWJlbF9jbGFzc2lmaWNhdGlvbiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJhbmdlKDAlMkMlMjBsb2dpdHMuc2hhcGUlNUItMSU1RCklNUJ0b3JjaC5zaWdtb2lkKGxvZ2l0cykuc3F1ZWV6ZShkaW0lM0QwKSUyMCUzRSUyMDAuNSU1RCUwQSUwQSUyMyUyMFRvJTIwdHJhaW4lMjBhJTIwbW9kZWwlMjBvbiUyMCU2MG51bV9sYWJlbHMlNjAlMjBjbGFzc2VzJTJDJTIweW91JTIwY2FuJTIwcGFzcyUyMCU2MG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTYwJTIwdG8lMjAlNjAuZnJvbV9wcmV0cmFpbmVkKC4uLiklNjAlMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKG1vZGVsLmNvbmZpZy5pZDJsYWJlbCklMEFtb2RlbCUyMCUzRCUyME1vZGVybkJlcnRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJhbnN3ZXJkb3RhaSUyRk1vZGVybkJFUlQtYmFzZSUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTJDJTIwcHJvYmxlbV90eXBlJTNEJTIybXVsdGlfbGFiZWxfY2xhc3NpZmljYXRpb24lMjIlMEEpJTBBJTBBbGFiZWxzJTIwJTNEJTIwdG9yY2guc3VtKCUwQSUyMCUyMCUyMCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwub25lX2hvdChwcmVkaWN0ZWRfY2xhc3NfaWRzJTVCTm9uZSUyQyUyMCUzQSU1RC5jbG9uZSgpJTJDJTIwbnVtX2NsYXNzZXMlM0RudW1fbGFiZWxzKSUyQyUyMGRpbSUzRDElMEEpLnRvKHRvcmNoLmZsb2F0KSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.arange(<span class="hljs-number">0</span>, logits.shape[-<span class="hljs-number">1</span>])[torch.sigmoid(logits).squeeze(dim=<span class="hljs-number">0</span>) &gt; <span class="hljs-number">0.5</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.<span class="hljs-built_in">sum</span>(
<span class="hljs-meta">... </span>    torch.nn.functional.one_hot(predicted_class_ids[<span class="hljs-literal">None</span>, :].clone(), num_classes=num_labels), dim=<span class="hljs-number">1</span>
<span class="hljs-meta">... </span>).to(torch.<span class="hljs-built_in">float</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-1l8e32d"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Yn(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function An(J){let e,c="Example:",t,s,g;return s=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNb2Rlcm5CZXJ0Rm9yVG9rZW5DbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2Rlcm5CZXJ0Rm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYW5zd2VyZG90YWklMkZNb2Rlcm5CRVJULWJhc2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUwQSUyMCUyMCUyMCUyMCUyMkh1Z2dpbmdGYWNlJTIwaXMlMjBhJTIwY29tcGFueSUyMGJhc2VkJTIwaW4lMjBQYXJpcyUyMGFuZCUyME5ldyUyMFlvcmslMjIlMkMlMjBhZGRfc3BlY2lhbF90b2tlbnMlM0RGYWxzZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcmVkaWN0ZWRfdG9rZW5fY2xhc3NfaWRzJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSklMEElMEElMjMlMjBOb3RlJTIwdGhhdCUyMHRva2VucyUyMGFyZSUyMGNsYXNzaWZpZWQlMjByYXRoZXIlMjB0aGVuJTIwaW5wdXQlMjB3b3JkcyUyMHdoaWNoJTIwbWVhbnMlMjB0aGF0JTBBJTIzJTIwdGhlcmUlMjBtaWdodCUyMGJlJTIwbW9yZSUyMHByZWRpY3RlZCUyMHRva2VuJTIwY2xhc3NlcyUyMHRoYW4lMjB3b3Jkcy4lMEElMjMlMjBNdWx0aXBsZSUyMHRva2VuJTIwY2xhc3NlcyUyMG1pZ2h0JTIwYWNjb3VudCUyMGZvciUyMHRoZSUyMHNhbWUlMjB3b3JkJTBBcHJlZGljdGVkX3Rva2Vuc19jbGFzc2VzJTIwJTNEJTIwJTVCbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCdC5pdGVtKCklNUQlMjBmb3IlMjB0JTIwaW4lMjBwcmVkaWN0ZWRfdG9rZW5fY2xhc3NfaWRzJTVCMCU1RCU1RCUwQXByZWRpY3RlZF90b2tlbnNfY2xhc3NlcyUwQSUwQWxhYmVscyUyMCUzRCUyMHByZWRpY3RlZF90b2tlbl9jbGFzc19pZHMlMEFsb3NzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjBsYWJlbHMlM0RsYWJlbHMpLmxvc3MlMEFyb3VuZChsb3NzLml0ZW0oKSUyQyUyMDIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
...

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-11lpom8"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Pn(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function On(J){let e,c="Example:",t,s,g;return s=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNb2Rlcm5CZXJ0Rm9yTXVsdGlwbGVDaG9pY2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFuc3dlcmRvdGFpJTJGTW9kZXJuQkVSVC1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwTW9kZXJuQmVydEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJhbnN3ZXJkb3RhaSUyRk1vZGVybkJFUlQtYmFzZSUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJJbiUyMEl0YWx5JTJDJTIwcGl6emElMjBzZXJ2ZWQlMjBpbiUyMGZvcm1hbCUyMHNldHRpbmdzJTJDJTIwc3VjaCUyMGFzJTIwYXQlMjBhJTIwcmVzdGF1cmFudCUyQyUyMGlzJTIwcHJlc2VudGVkJTIwdW5zbGljZWQuJTIyJTBBY2hvaWNlMCUyMCUzRCUyMCUyMkl0JTIwaXMlMjBlYXRlbiUyMHdpdGglMjBhJTIwZm9yayUyMGFuZCUyMGElMjBrbmlmZS4lMjIlMEFjaG9pY2UxJTIwJTNEJTIwJTIySXQlMjBpcyUyMGVhdGVuJTIwd2hpbGUlMjBoZWxkJTIwaW4lMjB0aGUlMjBoYW5kLiUyMiUwQWxhYmVscyUyMCUzRCUyMHRvcmNoLnRlbnNvcigwKS51bnNxdWVlemUoMCklMjAlMjAlMjMlMjBjaG9pY2UwJTIwaXMlMjBjb3JyZWN0JTIwKGFjY29yZGluZyUyMHRvJTIwV2lraXBlZGlhJTIwJTNCKSklMkMlMjBiYXRjaCUyMHNpemUlMjAxJTBBJTBBZW5jb2RpbmclMjAlM0QlMjB0b2tlbml6ZXIoJTVCcHJvbXB0JTJDJTIwcHJvbXB0JTVEJTJDJTIwJTVCY2hvaWNlMCUyQyUyMGNob2ljZTElNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqJTdCayUzQSUyMHYudW5zcXVlZXplKDApJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMGVuY29kaW5nLml0ZW1zKCklN0QlMkMlMjBsYWJlbHMlM0RsYWJlbHMpJTIwJTIwJTIzJTIwYmF0Y2glMjBzaXplJTIwaXMlMjAxJTBBJTBBJTIzJTIwdGhlJTIwbGluZWFyJTIwY2xhc3NpZmllciUyMHN0aWxsJTIwbmVlZHMlMjB0byUyMGJlJTIwdHJhaW5lZCUwQWxvc3MlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-11lpom8"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function Dn(J){let e,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=c},l(t){e=f(t,"P",{"data-svelte-h":!0}),k(e)!=="svelte-fincs2"&&(e.innerHTML=c)},m(t,s){p(t,e,s)},p:z,d(t){t&&a(e)}}}function Kn(J){let e,c="Example:",t,s,g;return s=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNb2Rlcm5CZXJ0Rm9yUXVlc3Rpb25BbnN3ZXJpbmclMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFuc3dlcmRvdGFpJTJGTW9kZXJuQkVSVC1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwTW9kZXJuQmVydEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJhbnN3ZXJkb3RhaSUyRk1vZGVybkJFUlQtYmFzZSUyMiklMEElMEFxdWVzdGlvbiUyQyUyMHRleHQlMjAlM0QlMjAlMjJXaG8lMjB3YXMlMjBKaW0lMjBIZW5zb24lM0YlMjIlMkMlMjAlMjJKaW0lMjBIZW5zb24lMjB3YXMlMjBhJTIwbmljZSUyMHB1cHBldCUyMiUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihxdWVzdGlvbiUyQyUyMHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBYW5zd2VyX3N0YXJ0X2luZGV4JTIwJTNEJTIwb3V0cHV0cy5zdGFydF9sb2dpdHMuYXJnbWF4KCklMEFhbnN3ZXJfZW5kX2luZGV4JTIwJTNEJTIwb3V0cHV0cy5lbmRfbG9naXRzLmFyZ21heCgpJTBBJTBBcHJlZGljdF9hbnN3ZXJfdG9rZW5zJTIwJTNEJTIwaW5wdXRzLmlucHV0X2lkcyU1QjAlMkMlMjBhbnN3ZXJfc3RhcnRfaW5kZXglMjAlM0ElMjBhbnN3ZXJfZW5kX2luZGV4JTIwJTJCJTIwMSU1RCUwQXRva2VuaXplci5kZWNvZGUocHJlZGljdF9hbnN3ZXJfdG9rZW5zJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJTBBJTIzJTIwdGFyZ2V0JTIwaXMlMjAlMjJuaWNlJTIwcHVwcGV0JTIyJTBBdGFyZ2V0X3N0YXJ0X2luZGV4JTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QjE0JTVEKSUwQXRhcmdldF9lbmRfaW5kZXglMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCMTUlNUQpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzJTJDJTIwc3RhcnRfcG9zaXRpb25zJTNEdGFyZ2V0X3N0YXJ0X2luZGV4JTJDJTIwZW5kX3Bvc2l0aW9ucyUzRHRhcmdldF9lbmRfaW5kZXgpJTBBbG9zcyUyMCUzRCUyMG91dHB1dHMubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, ModernBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ModernBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;answerdotai/ModernBERT-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
...

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){e=u("p"),e.textContent=c,t=i(),M(s.$$.fragment)},l(o){e=f(o,"P",{"data-svelte-h":!0}),k(e)!=="svelte-11lpom8"&&(e.textContent=c),t=l(o),_(s.$$.fragment,o)},m(o,m){p(o,e,m),p(o,t,m),b(s,o,m),g=!0},p:z,i(o){g||(T(s.$$.fragment,o),g=!0)},o(o){w(s.$$.fragment,o),g=!1},d(o){o&&(a(e),a(t)),y(s,o)}}}function eo(J){let e,c,t,s,g,o,m="The bare Modernbert Model outputting raw hidden-states without any specific head on top.",B,te,ge=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,X,le,Me=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,nt,W,Z,ot,de,ce='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertModel">ModernBertModel</a> forward method, overrides the <code>__call__</code> special method.',ve,F,Ie,ne,Q,j,q,Je,C,_e="The ModernBert Model with a decoder head on top that is used for masked language modeling.",Re,L,Tt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ge,S,wt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ne,$,oe,se,pe,be='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMaskedLM">ModernBertForMaskedLM</a> forward method, overrides the <code>__call__</code> special method.',Be,E,$e,r,v,U,Ve,x,Te,xt,st,ln="The ModernBert Model with a sequence classification head on top that performs pooling.",zt,rt,dn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ft,at,cn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Wt,H,Xe,Zt,it,pn='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification">ModernBertForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',qt,je,It,Ce,Rt,Ue,yt,Ee,kt,I,He,Gt,lt,mn="The ModernBert Model with a token classification head on top, e.g. for Named Entity Recognition (NER) tasks.",Nt,dt,hn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Vt,ct,un=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Xt,re,Qe,Et,pt,fn='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForTokenClassification">ModernBertForTokenClassification</a> forward method, overrides the <code>__call__</code> special method.',Ht,xe,Qt,ze,vt,Le,Jt,R,Se,Lt,mt,gn="The ModernBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.",St,ht,Mn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Yt,ut,_n=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,At,ae,Ye,Pt,ft,bn='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice">ModernBertForMultipleChoice</a> forward method, overrides the <code>__call__</code> special method.',Ot,Fe,Dt,We,Bt,Ae,$t,G,Pe,Kt,gt,Tn=`The Modernbert transformer with a span classification head on top for extractive question-answering tasks like
SQuAD (a linear layer on top of the hidden-states output to compute <code>span start logits</code> and <code>span end logits</code>).`,en,Mt,wn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,tn,_t,yn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,nn,ie,Oe,on,bt,kn='The <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering">ModernBertForQuestionAnswering</a> forward method, overrides the <code>__call__</code> special method.',sn,Ze,rn,qe,jt,De,Ct,Ke,vn='The ModernBert model can be fine-tuned using the HuggingFace Transformers library with its <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py" rel="nofollow">official script</a> for question-answering tasks.',Ut;return e=new ke({props:{title:"ModernBertModel",local:"transformers.ModernBertModel",headingTag:"h2"}}),s=new ee({props:{name:"class transformers.ModernBertModel",anchor:"transformers.ModernBertModel",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L764"}}),Z=new ee({props:{name:"forward",anchor:"transformers.ModernBertModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ModernBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertModel.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ModernBertModel.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertModel.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertModel.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertModel.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertModel.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L782",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)  Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),F=new et({props:{$$slots:{default:[Xn]},$$scope:{ctx:J}}}),ne=new ke({props:{title:"ModernBertForMaskedLM",local:"transformers.ModernBertForMaskedLM",headingTag:"h2"}}),q=new ee({props:{name:"class transformers.ModernBertForMaskedLM",anchor:"transformers.ModernBertForMaskedLM",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L954"}}),oe=new ee({props:{name:"forward",anchor:"transformers.ModernBertForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ModernBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertForMaskedLM.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertForMaskedLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ModernBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.ModernBertForMaskedLM.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertForMaskedLM.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertForMaskedLM.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertForMaskedLM.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertForMaskedLM.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L980",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>)  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new et({props:{$$slots:{default:[En]},$$scope:{ctx:J}}}),r=new tt({props:{anchor:"transformers.ModernBertForMaskedLM.forward.example",$$slots:{default:[Hn]},$$scope:{ctx:J}}}),U=new ke({props:{title:"ModernBertForSequenceClassification",local:"transformers.ModernBertForSequenceClassification",headingTag:"h2"}}),Te=new ee({props:{name:"class transformers.ModernBertForSequenceClassification",anchor:"transformers.ModernBertForSequenceClassification",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1098"}}),Xe=new ee({props:{name:"forward",anchor:"transformers.ModernBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ModernBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertForSequenceClassification.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ModernBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.ModernBertForSequenceClassification.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertForSequenceClassification.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertForSequenceClassification.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertForSequenceClassification.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertForSequenceClassification.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1112",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>)  Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),je=new et({props:{$$slots:{default:[Qn]},$$scope:{ctx:J}}}),Ce=new tt({props:{anchor:"transformers.ModernBertForSequenceClassification.forward.example",$$slots:{default:[Ln]},$$scope:{ctx:J}}}),Ue=new tt({props:{anchor:"transformers.ModernBertForSequenceClassification.forward.example-2",$$slots:{default:[Sn]},$$scope:{ctx:J}}}),Ee=new ke({props:{title:"ModernBertForTokenClassification",local:"transformers.ModernBertForTokenClassification",headingTag:"h2"}}),He=new ee({props:{name:"class transformers.ModernBertForTokenClassification",anchor:"transformers.ModernBertForTokenClassification",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1235"}}),Qe=new ee({props:{name:"forward",anchor:"transformers.ModernBertForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.ModernBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertForTokenClassification.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertForTokenClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ModernBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"},{anchor:"transformers.ModernBertForTokenClassification.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertForTokenClassification.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertForTokenClassification.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertForTokenClassification.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertForTokenClassification.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1248",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)   Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>)  Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),xe=new et({props:{$$slots:{default:[Yn]},$$scope:{ctx:J}}}),ze=new tt({props:{anchor:"transformers.ModernBertForTokenClassification.forward.example",$$slots:{default:[An]},$$scope:{ctx:J}}}),Le=new ke({props:{title:"ModernBertForMultipleChoice",local:"transformers.ModernBertForMultipleChoice",headingTag:"h2"}}),Se=new ee({props:{name:"class transformers.ModernBertForMultipleChoice",anchor:"transformers.ModernBertForMultipleChoice",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1422"}}),Ye=new ee({props:{name:"forward",anchor:"transformers.ModernBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ModernBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertForMultipleChoice.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertForMultipleChoice.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.ModernBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors.`,name:"labels"},{anchor:"transformers.ModernBertForMultipleChoice.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertForMultipleChoice.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertForMultipleChoice.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertForMultipleChoice.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertForMultipleChoice.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1435",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided)  Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>)  <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Fe=new et({props:{$$slots:{default:[Pn]},$$scope:{ctx:J}}}),We=new tt({props:{anchor:"transformers.ModernBertForMultipleChoice.forward.example",$$slots:{default:[On]},$$scope:{ctx:J}}}),Ae=new ke({props:{title:"ModernBertForQuestionAnswering",local:"transformers.ModernBertForQuestionAnswering",headingTag:"h2"}}),Pe=new ee({props:{name:"class transformers.ModernBertForQuestionAnswering",anchor:"transformers.ModernBertForQuestionAnswering",parameters:[{name:"config",val:": ModernBertConfig"}],parametersDescription:[{anchor:"transformers.ModernBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1326"}}),Oe=new ee({props:{name:"forward",anchor:"transformers.ModernBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"sliding_window_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"indices",val:": typing.Optional[torch.Tensor] = None"},{name:"cu_seqlens",val:": typing.Optional[torch.Tensor] = None"},{name:"max_seqlen",val:": typing.Optional[int] = None"},{name:"batch_size",val:": typing.Optional[int] = None"},{name:"seq_len",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ModernBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.sliding_window_mask",description:`<strong>sliding_window_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers
perform global attention, while the rest perform local attention. This mask is used to avoid attending to
far-away tokens in the local attention layers when not using Flash Attention.`,name:"sliding_window_mask"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.indices",description:`<strong>indices</strong> (<code>torch.Tensor</code> of shape <code>(total_unpadded_tokens,)</code>, <em>optional</em>) &#x2014;
Indices of the non-padding tokens in the input sequence. Used for unpadding the output.`,name:"indices"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.cu_seqlens",description:`<strong>cu_seqlens</strong> (<code>torch.Tensor</code> of shape <code>(batch + 1,)</code>, <em>optional</em>) &#x2014;
Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.`,name:"cu_seqlens"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.max_seqlen",description:`<strong>max_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.`,name:"max_seqlen"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Batch size of the input sequences. Used to pad the output tensors.`,name:"batch_size"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.seq_len",description:`<strong>seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sequence length of the input sequences including padding tokens. Used to pad the output tensors.`,name:"seq_len"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ModernBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/modeling_modernbert.py#L1338",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig"
>ModernBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>)  Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>)  Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new et({props:{$$slots:{default:[Dn]},$$scope:{ctx:J}}}),qe=new tt({props:{anchor:"transformers.ModernBertForQuestionAnswering.forward.example",$$slots:{default:[Kn]},$$scope:{ctx:J}}}),De=new ke({props:{title:"Usage tips",local:"usage-tips",headingTag:"h3"}}),{c(){M(e.$$.fragment),c=i(),t=u("div"),M(s.$$.fragment),g=i(),o=u("p"),o.textContent=m,B=i(),te=u("p"),te.innerHTML=ge,X=i(),le=u("p"),le.innerHTML=Me,nt=i(),W=u("div"),M(Z.$$.fragment),ot=i(),de=u("p"),de.innerHTML=ce,ve=i(),M(F.$$.fragment),Ie=i(),M(ne.$$.fragment),Q=i(),j=u("div"),M(q.$$.fragment),Je=i(),C=u("p"),C.textContent=_e,Re=i(),L=u("p"),L.innerHTML=Tt,Ge=i(),S=u("p"),S.innerHTML=wt,Ne=i(),$=u("div"),M(oe.$$.fragment),se=i(),pe=u("p"),pe.innerHTML=be,Be=i(),M(E.$$.fragment),$e=i(),M(r.$$.fragment),v=i(),M(U.$$.fragment),Ve=i(),x=u("div"),M(Te.$$.fragment),xt=i(),st=u("p"),st.textContent=ln,zt=i(),rt=u("p"),rt.innerHTML=dn,Ft=i(),at=u("p"),at.innerHTML=cn,Wt=i(),H=u("div"),M(Xe.$$.fragment),Zt=i(),it=u("p"),it.innerHTML=pn,qt=i(),M(je.$$.fragment),It=i(),M(Ce.$$.fragment),Rt=i(),M(Ue.$$.fragment),yt=i(),M(Ee.$$.fragment),kt=i(),I=u("div"),M(He.$$.fragment),Gt=i(),lt=u("p"),lt.textContent=mn,Nt=i(),dt=u("p"),dt.innerHTML=hn,Vt=i(),ct=u("p"),ct.innerHTML=un,Xt=i(),re=u("div"),M(Qe.$$.fragment),Et=i(),pt=u("p"),pt.innerHTML=fn,Ht=i(),M(xe.$$.fragment),Qt=i(),M(ze.$$.fragment),vt=i(),M(Le.$$.fragment),Jt=i(),R=u("div"),M(Se.$$.fragment),Lt=i(),mt=u("p"),mt.textContent=gn,St=i(),ht=u("p"),ht.innerHTML=Mn,Yt=i(),ut=u("p"),ut.innerHTML=_n,At=i(),ae=u("div"),M(Ye.$$.fragment),Pt=i(),ft=u("p"),ft.innerHTML=bn,Ot=i(),M(Fe.$$.fragment),Dt=i(),M(We.$$.fragment),Bt=i(),M(Ae.$$.fragment),$t=i(),G=u("div"),M(Pe.$$.fragment),Kt=i(),gt=u("p"),gt.innerHTML=Tn,en=i(),Mt=u("p"),Mt.innerHTML=wn,tn=i(),_t=u("p"),_t.innerHTML=yn,nn=i(),ie=u("div"),M(Oe.$$.fragment),on=i(),bt=u("p"),bt.innerHTML=kn,sn=i(),M(Ze.$$.fragment),rn=i(),M(qe.$$.fragment),jt=i(),M(De.$$.fragment),Ct=i(),Ke=u("p"),Ke.innerHTML=vn,this.h()},l(n){_(e.$$.fragment,n),c=l(n),t=f(n,"DIV",{class:!0});var h=N(t);_(s.$$.fragment,h),g=l(h),o=f(h,"P",{"data-svelte-h":!0}),k(o)!=="svelte-z6trfk"&&(o.textContent=m),B=l(h),te=f(h,"P",{"data-svelte-h":!0}),k(te)!=="svelte-q52n56"&&(te.innerHTML=ge),X=l(h),le=f(h,"P",{"data-svelte-h":!0}),k(le)!=="svelte-hswkmf"&&(le.innerHTML=Me),nt=l(h),W=f(h,"DIV",{class:!0});var ye=N(W);_(Z.$$.fragment,ye),ot=l(ye),de=f(ye,"P",{"data-svelte-h":!0}),k(de)!=="svelte-8cqvrz"&&(de.innerHTML=ce),ve=l(ye),_(F.$$.fragment,ye),ye.forEach(a),h.forEach(a),Ie=l(n),_(ne.$$.fragment,n),Q=l(n),j=f(n,"DIV",{class:!0});var Y=N(j);_(q.$$.fragment,Y),Je=l(Y),C=f(Y,"P",{"data-svelte-h":!0}),k(C)!=="svelte-169dnsk"&&(C.textContent=_e),Re=l(Y),L=f(Y,"P",{"data-svelte-h":!0}),k(L)!=="svelte-q52n56"&&(L.innerHTML=Tt),Ge=l(Y),S=f(Y,"P",{"data-svelte-h":!0}),k(S)!=="svelte-hswkmf"&&(S.innerHTML=wt),Ne=l(Y),$=f(Y,"DIV",{class:!0});var me=N($);_(oe.$$.fragment,me),se=l(me),pe=f(me,"P",{"data-svelte-h":!0}),k(pe)!=="svelte-171airf"&&(pe.innerHTML=be),Be=l(me),_(E.$$.fragment,me),$e=l(me),_(r.$$.fragment,me),me.forEach(a),Y.forEach(a),v=l(n),_(U.$$.fragment,n),Ve=l(n),x=f(n,"DIV",{class:!0});var A=N(x);_(Te.$$.fragment,A),xt=l(A),st=f(A,"P",{"data-svelte-h":!0}),k(st)!=="svelte-jutaud"&&(st.textContent=ln),zt=l(A),rt=f(A,"P",{"data-svelte-h":!0}),k(rt)!=="svelte-q52n56"&&(rt.innerHTML=dn),Ft=l(A),at=f(A,"P",{"data-svelte-h":!0}),k(at)!=="svelte-hswkmf"&&(at.innerHTML=cn),Wt=l(A),H=f(A,"DIV",{class:!0});var P=N(H);_(Xe.$$.fragment,P),Zt=l(P),it=f(P,"P",{"data-svelte-h":!0}),k(it)!=="svelte-6w6d6f"&&(it.innerHTML=pn),qt=l(P),_(je.$$.fragment,P),It=l(P),_(Ce.$$.fragment,P),Rt=l(P),_(Ue.$$.fragment,P),P.forEach(a),A.forEach(a),yt=l(n),_(Ee.$$.fragment,n),kt=l(n),I=f(n,"DIV",{class:!0});var O=N(I);_(He.$$.fragment,O),Gt=l(O),lt=f(O,"P",{"data-svelte-h":!0}),k(lt)!=="svelte-16c9jqy"&&(lt.textContent=mn),Nt=l(O),dt=f(O,"P",{"data-svelte-h":!0}),k(dt)!=="svelte-q52n56"&&(dt.innerHTML=hn),Vt=l(O),ct=f(O,"P",{"data-svelte-h":!0}),k(ct)!=="svelte-hswkmf"&&(ct.innerHTML=un),Xt=l(O),re=f(O,"DIV",{class:!0});var he=N(re);_(Qe.$$.fragment,he),Et=l(he),pt=f(he,"P",{"data-svelte-h":!0}),k(pt)!=="svelte-198mom7"&&(pt.innerHTML=fn),Ht=l(he),_(xe.$$.fragment,he),Qt=l(he),_(ze.$$.fragment,he),he.forEach(a),O.forEach(a),vt=l(n),_(Le.$$.fragment,n),Jt=l(n),R=f(n,"DIV",{class:!0});var D=N(R);_(Se.$$.fragment,D),Lt=l(D),mt=f(D,"P",{"data-svelte-h":!0}),k(mt)!=="svelte-1v4gtij"&&(mt.textContent=gn),St=l(D),ht=f(D,"P",{"data-svelte-h":!0}),k(ht)!=="svelte-q52n56"&&(ht.innerHTML=Mn),Yt=l(D),ut=f(D,"P",{"data-svelte-h":!0}),k(ut)!=="svelte-hswkmf"&&(ut.innerHTML=_n),At=l(D),ae=f(D,"DIV",{class:!0});var ue=N(ae);_(Ye.$$.fragment,ue),Pt=l(ue),ft=f(ue,"P",{"data-svelte-h":!0}),k(ft)!=="svelte-1o6i9wj"&&(ft.innerHTML=bn),Ot=l(ue),_(Fe.$$.fragment,ue),Dt=l(ue),_(We.$$.fragment,ue),ue.forEach(a),D.forEach(a),Bt=l(n),_(Ae.$$.fragment,n),$t=l(n),G=f(n,"DIV",{class:!0});var K=N(G);_(Pe.$$.fragment,K),Kt=l(K),gt=f(K,"P",{"data-svelte-h":!0}),k(gt)!=="svelte-1w7ozi1"&&(gt.innerHTML=Tn),en=l(K),Mt=f(K,"P",{"data-svelte-h":!0}),k(Mt)!=="svelte-q52n56"&&(Mt.innerHTML=wn),tn=l(K),_t=f(K,"P",{"data-svelte-h":!0}),k(_t)!=="svelte-hswkmf"&&(_t.innerHTML=yn),nn=l(K),ie=f(K,"DIV",{class:!0});var fe=N(ie);_(Oe.$$.fragment,fe),on=l(fe),bt=f(fe,"P",{"data-svelte-h":!0}),k(bt)!=="svelte-xw2qvl"&&(bt.innerHTML=kn),sn=l(fe),_(Ze.$$.fragment,fe),rn=l(fe),_(qe.$$.fragment,fe),fe.forEach(a),K.forEach(a),jt=l(n),_(De.$$.fragment,n),Ct=l(n),Ke=f(n,"P",{"data-svelte-h":!0}),k(Ke)!=="svelte-gcoc80"&&(Ke.innerHTML=vn),this.h()},h(){V(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(n,h){b(e,n,h),p(n,c,h),p(n,t,h),b(s,t,null),d(t,g),d(t,o),d(t,B),d(t,te),d(t,X),d(t,le),d(t,nt),d(t,W),b(Z,W,null),d(W,ot),d(W,de),d(W,ve),b(F,W,null),p(n,Ie,h),b(ne,n,h),p(n,Q,h),p(n,j,h),b(q,j,null),d(j,Je),d(j,C),d(j,Re),d(j,L),d(j,Ge),d(j,S),d(j,Ne),d(j,$),b(oe,$,null),d($,se),d($,pe),d($,Be),b(E,$,null),d($,$e),b(r,$,null),p(n,v,h),b(U,n,h),p(n,Ve,h),p(n,x,h),b(Te,x,null),d(x,xt),d(x,st),d(x,zt),d(x,rt),d(x,Ft),d(x,at),d(x,Wt),d(x,H),b(Xe,H,null),d(H,Zt),d(H,it),d(H,qt),b(je,H,null),d(H,It),b(Ce,H,null),d(H,Rt),b(Ue,H,null),p(n,yt,h),b(Ee,n,h),p(n,kt,h),p(n,I,h),b(He,I,null),d(I,Gt),d(I,lt),d(I,Nt),d(I,dt),d(I,Vt),d(I,ct),d(I,Xt),d(I,re),b(Qe,re,null),d(re,Et),d(re,pt),d(re,Ht),b(xe,re,null),d(re,Qt),b(ze,re,null),p(n,vt,h),b(Le,n,h),p(n,Jt,h),p(n,R,h),b(Se,R,null),d(R,Lt),d(R,mt),d(R,St),d(R,ht),d(R,Yt),d(R,ut),d(R,At),d(R,ae),b(Ye,ae,null),d(ae,Pt),d(ae,ft),d(ae,Ot),b(Fe,ae,null),d(ae,Dt),b(We,ae,null),p(n,Bt,h),b(Ae,n,h),p(n,$t,h),p(n,G,h),b(Pe,G,null),d(G,Kt),d(G,gt),d(G,en),d(G,Mt),d(G,tn),d(G,_t),d(G,nn),d(G,ie),b(Oe,ie,null),d(ie,on),d(ie,bt),d(ie,sn),b(Ze,ie,null),d(ie,rn),b(qe,ie,null),p(n,jt,h),b(De,n,h),p(n,Ct,h),p(n,Ke,h),Ut=!0},p(n,h){const ye={};h&2&&(ye.$$scope={dirty:h,ctx:n}),F.$set(ye);const Y={};h&2&&(Y.$$scope={dirty:h,ctx:n}),E.$set(Y);const me={};h&2&&(me.$$scope={dirty:h,ctx:n}),r.$set(me);const A={};h&2&&(A.$$scope={dirty:h,ctx:n}),je.$set(A);const P={};h&2&&(P.$$scope={dirty:h,ctx:n}),Ce.$set(P);const O={};h&2&&(O.$$scope={dirty:h,ctx:n}),Ue.$set(O);const he={};h&2&&(he.$$scope={dirty:h,ctx:n}),xe.$set(he);const D={};h&2&&(D.$$scope={dirty:h,ctx:n}),ze.$set(D);const ue={};h&2&&(ue.$$scope={dirty:h,ctx:n}),Fe.$set(ue);const K={};h&2&&(K.$$scope={dirty:h,ctx:n}),We.$set(K);const fe={};h&2&&(fe.$$scope={dirty:h,ctx:n}),Ze.$set(fe);const Jn={};h&2&&(Jn.$$scope={dirty:h,ctx:n}),qe.$set(Jn)},i(n){Ut||(T(e.$$.fragment,n),T(s.$$.fragment,n),T(Z.$$.fragment,n),T(F.$$.fragment,n),T(ne.$$.fragment,n),T(q.$$.fragment,n),T(oe.$$.fragment,n),T(E.$$.fragment,n),T(r.$$.fragment,n),T(U.$$.fragment,n),T(Te.$$.fragment,n),T(Xe.$$.fragment,n),T(je.$$.fragment,n),T(Ce.$$.fragment,n),T(Ue.$$.fragment,n),T(Ee.$$.fragment,n),T(He.$$.fragment,n),T(Qe.$$.fragment,n),T(xe.$$.fragment,n),T(ze.$$.fragment,n),T(Le.$$.fragment,n),T(Se.$$.fragment,n),T(Ye.$$.fragment,n),T(Fe.$$.fragment,n),T(We.$$.fragment,n),T(Ae.$$.fragment,n),T(Pe.$$.fragment,n),T(Oe.$$.fragment,n),T(Ze.$$.fragment,n),T(qe.$$.fragment,n),T(De.$$.fragment,n),Ut=!0)},o(n){w(e.$$.fragment,n),w(s.$$.fragment,n),w(Z.$$.fragment,n),w(F.$$.fragment,n),w(ne.$$.fragment,n),w(q.$$.fragment,n),w(oe.$$.fragment,n),w(E.$$.fragment,n),w(r.$$.fragment,n),w(U.$$.fragment,n),w(Te.$$.fragment,n),w(Xe.$$.fragment,n),w(je.$$.fragment,n),w(Ce.$$.fragment,n),w(Ue.$$.fragment,n),w(Ee.$$.fragment,n),w(He.$$.fragment,n),w(Qe.$$.fragment,n),w(xe.$$.fragment,n),w(ze.$$.fragment,n),w(Le.$$.fragment,n),w(Se.$$.fragment,n),w(Ye.$$.fragment,n),w(Fe.$$.fragment,n),w(We.$$.fragment,n),w(Ae.$$.fragment,n),w(Pe.$$.fragment,n),w(Oe.$$.fragment,n),w(Ze.$$.fragment,n),w(qe.$$.fragment,n),w(De.$$.fragment,n),Ut=!1},d(n){n&&(a(c),a(t),a(Ie),a(Q),a(j),a(v),a(Ve),a(x),a(yt),a(kt),a(I),a(vt),a(Jt),a(R),a(Bt),a($t),a(G),a(jt),a(Ct),a(Ke)),y(e,n),y(s),y(Z),y(F),y(ne,n),y(q),y(oe),y(E),y(r),y(U,n),y(Te),y(Xe),y(je),y(Ce),y(Ue),y(Ee,n),y(He),y(Qe),y(xe),y(ze),y(Le,n),y(Se),y(Ye),y(Fe),y(We),y(Ae,n),y(Pe),y(Oe),y(Ze),y(qe),y(De,n)}}}function to(J){let e,c;return e=new Fn({props:{$$slots:{default:[eo]},$$scope:{ctx:J}}}),{c(){M(e.$$.fragment)},l(t){_(e.$$.fragment,t)},m(t,s){b(e,t,s),c=!0},p(t,s){const g={};s&2&&(g.$$scope={dirty:s,ctx:t}),e.$set(g)},i(t){c||(T(e.$$.fragment,t),c=!0)},o(t){w(e.$$.fragment,t),c=!1},d(t){y(e,t)}}}function no(J){let e,c,t,s,g,o="<em>This model was released on 2024-12-18 and added to Hugging Face Transformers on 2024-12-19.</em>",m,B,te='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ge,X,le,Me,nt='<a href="https://huggingface.co/papers/2412.13663" rel="nofollow">ModernBERT</a> is a modernized version of <code>BERT</code> trained on 2T tokens. It brings many improvements to the original architecture such as rotary positional embeddings to support sequences of up to 8192 tokens, unpadding to avoid wasting compute on padding tokens, GeGLU layers, and alternating attention.',W,Z,ot='You can find all the original ModernBERT checkpoints under the <a href="https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb" rel="nofollow">ModernBERT</a> collection.',de,ce,ve,F,Ie='The example below demonstrates how to predict the <code>[MASK]</code> token with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',ne,Q,j,q,Je,C,_e,Re,L,Tt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertModel">ModernBertModel</a>. It is used to instantiate an ModernBert
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the ModernBERT-base.
e.g. <a href="https://huggingface.co/answerdotai/ModernBERT-base" rel="nofollow">answerdotai/ModernBERT-base</a>`,Ge,S,wt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ne,$,oe,se,pe,be,Be,E,$e;return X=new ke({props:{title:"ModernBERT",local:"modernbert",headingTag:"h1"}}),ce=new et({props:{warning:!1,$$slots:{default:[qn]},$$scope:{ctx:J}}}),Q=new Zn({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Nn]},$$scope:{ctx:J}}}),q=new ke({props:{title:"ModernBertConfig",local:"transformers.ModernBertConfig",headingTag:"h2"}}),_e=new ee({props:{name:"class transformers.ModernBertConfig",anchor:"transformers.ModernBertConfig",parameters:[{name:"vocab_size",val:" = 50368"},{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 1152"},{name:"num_hidden_layers",val:" = 22"},{name:"num_attention_heads",val:" = 12"},{name:"hidden_activation",val:" = 'gelu'"},{name:"max_position_embeddings",val:" = 8192"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_cutoff_factor",val:" = 2.0"},{name:"norm_eps",val:" = 1e-05"},{name:"norm_bias",val:" = False"},{name:"pad_token_id",val:" = 50283"},{name:"eos_token_id",val:" = 50282"},{name:"bos_token_id",val:" = 50281"},{name:"cls_token_id",val:" = 50281"},{name:"sep_token_id",val:" = 50282"},{name:"global_rope_theta",val:" = 160000.0"},{name:"attention_bias",val:" = False"},{name:"attention_dropout",val:" = 0.0"},{name:"global_attn_every_n_layers",val:" = 3"},{name:"local_attention",val:" = 128"},{name:"local_rope_theta",val:" = 10000.0"},{name:"embedding_dropout",val:" = 0.0"},{name:"mlp_bias",val:" = False"},{name:"mlp_dropout",val:" = 0.0"},{name:"decoder_bias",val:" = True"},{name:"classifier_pooling",val:": typing.Literal['cls', 'mean'] = 'cls'"},{name:"classifier_dropout",val:" = 0.0"},{name:"classifier_bias",val:" = False"},{name:"classifier_activation",val:" = 'gelu'"},{name:"deterministic_flash_attn",val:" = False"},{name:"sparse_prediction",val:" = False"},{name:"sparse_pred_ignore_index",val:" = -100"},{name:"reference_compile",val:" = None"},{name:"repad_logits_with_grad",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ModernBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50368) &#x2014;
Vocabulary size of the ModernBert model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertModel">ModernBertModel</a>`,name:"vocab_size"},{anchor:"transformers.ModernBertConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.ModernBertConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1152) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.ModernBertConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 22) &#x2014;
Number of hidden layers in the Transformer decoder.`,name:"num_hidden_layers"},{anchor:"transformers.ModernBertConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_attention_heads"},{anchor:"transformers.ModernBertConfig.hidden_activation",description:`<strong>hidden_activation</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder. Will default to <code>&quot;gelu&quot;</code>
if not specified.`,name:"hidden_activation"},{anchor:"transformers.ModernBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
The maximum sequence length that this model might ever be used with.`,name:"max_position_embeddings"},{anchor:"transformers.ModernBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ModernBertConfig.initializer_cutoff_factor",description:`<strong>initializer_cutoff_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
The cutoff factor for the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_cutoff_factor"},{anchor:"transformers.ModernBertConfig.norm_eps",description:`<strong>norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the rms normalization layers.`,name:"norm_eps"},{anchor:"transformers.ModernBertConfig.norm_bias",description:`<strong>norm_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use bias in the normalization layers.`,name:"norm_bias"},{anchor:"transformers.ModernBertConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 50283) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ModernBertConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 50282) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ModernBertConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 50281) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ModernBertConfig.cls_token_id",description:`<strong>cls_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 50281) &#x2014;
Classification token id.`,name:"cls_token_id"},{anchor:"transformers.ModernBertConfig.sep_token_id",description:`<strong>sep_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 50282) &#x2014;
Separation token id.`,name:"sep_token_id"},{anchor:"transformers.ModernBertConfig.global_rope_theta",description:`<strong>global_rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 160000.0) &#x2014;
The base period of the global RoPE embeddings.`,name:"global_rope_theta"},{anchor:"transformers.ModernBertConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a bias in the query, key, value and output projection layers during self-attention.`,name:"attention_bias"},{anchor:"transformers.ModernBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ModernBertConfig.global_attn_every_n_layers",description:`<strong>global_attn_every_n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of layers between global attention layers.`,name:"global_attn_every_n_layers"},{anchor:"transformers.ModernBertConfig.local_attention",description:`<strong>local_attention</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The window size for local attention.`,name:"local_attention"},{anchor:"transformers.ModernBertConfig.local_rope_theta",description:`<strong>local_rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the local RoPE embeddings.`,name:"local_rope_theta"},{anchor:"transformers.ModernBertConfig.embedding_dropout",description:`<strong>embedding_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the embeddings.`,name:"embedding_dropout"},{anchor:"transformers.ModernBertConfig.mlp_bias",description:`<strong>mlp_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use bias in the MLP layers.`,name:"mlp_bias"},{anchor:"transformers.ModernBertConfig.mlp_dropout",description:`<strong>mlp_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the MLP layers.`,name:"mlp_dropout"},{anchor:"transformers.ModernBertConfig.decoder_bias",description:`<strong>decoder_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use bias in the decoder layers.`,name:"decoder_bias"},{anchor:"transformers.ModernBertConfig.classifier_pooling",description:`<strong>classifier_pooling</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;cls&quot;</code>) &#x2014;
The pooling method for the classifier. Should be either <code>&quot;cls&quot;</code> or <code>&quot;mean&quot;</code>. In local attention layers, the
CLS token doesn&#x2019;t attend to all tokens on long sequences.`,name:"classifier_pooling"},{anchor:"transformers.ModernBertConfig.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the classifier.`,name:"classifier_dropout"},{anchor:"transformers.ModernBertConfig.classifier_bias",description:`<strong>classifier_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use bias in the classifier.`,name:"classifier_bias"},{anchor:"transformers.ModernBertConfig.classifier_activation",description:`<strong>classifier_activation</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The activation function for the classifier.`,name:"classifier_activation"},{anchor:"transformers.ModernBertConfig.deterministic_flash_attn",description:`<strong>deterministic_flash_attn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use deterministic flash attention. If <code>False</code>, inference will be faster but not deterministic.`,name:"deterministic_flash_attn"},{anchor:"transformers.ModernBertConfig.sparse_prediction",description:`<strong>sparse_prediction</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sparse prediction for the masked language model instead of returning the full dense logits.`,name:"sparse_prediction"},{anchor:"transformers.ModernBertConfig.sparse_pred_ignore_index",description:`<strong>sparse_pred_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The index to ignore for the sparse prediction.`,name:"sparse_pred_ignore_index"},{anchor:"transformers.ModernBertConfig.reference_compile",description:`<strong>reference_compile</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to compile the layers of the model which were compiled during pretraining. If <code>None</code>, then parts of
the model will be compiled if 1) <code>triton</code> is installed, 2) the model is not on MPS, 3) the model is not
shared between devices, and 4) the model is not resized after initialization. If <code>True</code>, then the model may
be faster in some scenarios.`,name:"reference_compile"},{anchor:"transformers.ModernBertConfig.repad_logits_with_grad",description:`<strong>repad_logits_with_grad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When True, ModernBertForMaskedLM keeps track of the logits&#x2019; gradient when repadding for output. This only
applies when using Flash Attention 2 with passed labels. Otherwise output logits always have a gradient.`,name:"repad_logits_with_grad"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/modernbert/configuration_modernbert.py#L27"}}),$=new tt({props:{anchor:"transformers.ModernBertConfig.example",$$slots:{default:[Vn]},$$scope:{ctx:J}}}),se=new zn({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[to]},$$scope:{ctx:J}}}),be=new Wn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/modernbert.md"}}),{c(){e=u("meta"),c=i(),t=u("p"),s=i(),g=u("p"),g.innerHTML=o,m=i(),B=u("div"),B.innerHTML=te,ge=i(),M(X.$$.fragment),le=i(),Me=u("p"),Me.innerHTML=nt,W=i(),Z=u("p"),Z.innerHTML=ot,de=i(),M(ce.$$.fragment),ve=i(),F=u("p"),F.innerHTML=Ie,ne=i(),M(Q.$$.fragment),j=i(),M(q.$$.fragment),Je=i(),C=u("div"),M(_e.$$.fragment),Re=i(),L=u("p"),L.innerHTML=Tt,Ge=i(),S=u("p"),S.innerHTML=wt,Ne=i(),M($.$$.fragment),oe=i(),M(se.$$.fragment),pe=i(),M(be.$$.fragment),Be=i(),E=u("p"),this.h()},l(r){const v=Un("svelte-u9bgzb",document.head);e=f(v,"META",{name:!0,content:!0}),v.forEach(a),c=l(r),t=f(r,"P",{}),N(t).forEach(a),s=l(r),g=f(r,"P",{"data-svelte-h":!0}),k(g)!=="svelte-axhp8z"&&(g.innerHTML=o),m=l(r),B=f(r,"DIV",{style:!0,"data-svelte-h":!0}),k(B)!=="svelte-1lhmk4n"&&(B.innerHTML=te),ge=l(r),_(X.$$.fragment,r),le=l(r),Me=f(r,"P",{"data-svelte-h":!0}),k(Me)!=="svelte-beijbx"&&(Me.innerHTML=nt),W=l(r),Z=f(r,"P",{"data-svelte-h":!0}),k(Z)!=="svelte-11smd8w"&&(Z.innerHTML=ot),de=l(r),_(ce.$$.fragment,r),ve=l(r),F=f(r,"P",{"data-svelte-h":!0}),k(F)!=="svelte-lqa8w5"&&(F.innerHTML=Ie),ne=l(r),_(Q.$$.fragment,r),j=l(r),_(q.$$.fragment,r),Je=l(r),C=f(r,"DIV",{class:!0});var U=N(C);_(_e.$$.fragment,U),Re=l(U),L=f(U,"P",{"data-svelte-h":!0}),k(L)!=="svelte-szjknc"&&(L.innerHTML=Tt),Ge=l(U),S=f(U,"P",{"data-svelte-h":!0}),k(S)!=="svelte-1ek1ss9"&&(S.innerHTML=wt),Ne=l(U),_($.$$.fragment,U),U.forEach(a),oe=l(r),_(se.$$.fragment,r),pe=l(r),_(be.$$.fragment,r),Be=l(r),E=f(r,"P",{}),N(E).forEach(a),this.h()},h(){V(e,"name","hf:doc:metadata"),V(e,"content",oo),xn(B,"float","right"),V(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,v){d(document.head,e),p(r,c,v),p(r,t,v),p(r,s,v),p(r,g,v),p(r,m,v),p(r,B,v),p(r,ge,v),b(X,r,v),p(r,le,v),p(r,Me,v),p(r,W,v),p(r,Z,v),p(r,de,v),b(ce,r,v),p(r,ve,v),p(r,F,v),p(r,ne,v),b(Q,r,v),p(r,j,v),b(q,r,v),p(r,Je,v),p(r,C,v),b(_e,C,null),d(C,Re),d(C,L),d(C,Ge),d(C,S),d(C,Ne),b($,C,null),p(r,oe,v),b(se,r,v),p(r,pe,v),b(be,r,v),p(r,Be,v),p(r,E,v),$e=!0},p(r,[v]){const U={};v&2&&(U.$$scope={dirty:v,ctx:r}),ce.$set(U);const Ve={};v&2&&(Ve.$$scope={dirty:v,ctx:r}),Q.$set(Ve);const x={};v&2&&(x.$$scope={dirty:v,ctx:r}),$.$set(x);const Te={};v&2&&(Te.$$scope={dirty:v,ctx:r}),se.$set(Te)},i(r){$e||(T(X.$$.fragment,r),T(ce.$$.fragment,r),T(Q.$$.fragment,r),T(q.$$.fragment,r),T(_e.$$.fragment,r),T($.$$.fragment,r),T(se.$$.fragment,r),T(be.$$.fragment,r),$e=!0)},o(r){w(X.$$.fragment,r),w(ce.$$.fragment,r),w(Q.$$.fragment,r),w(q.$$.fragment,r),w(_e.$$.fragment,r),w($.$$.fragment,r),w(se.$$.fragment,r),w(be.$$.fragment,r),$e=!1},d(r){r&&(a(c),a(t),a(s),a(g),a(m),a(B),a(ge),a(le),a(Me),a(W),a(Z),a(de),a(ve),a(F),a(ne),a(j),a(Je),a(C),a(oe),a(pe),a(Be),a(E)),a(e),y(X,r),y(ce,r),y(Q,r),y(q,r),y(_e),y($),y(se,r),y(be,r)}}}const oo='{"title":"ModernBERT","local":"modernbert","sections":[{"title":"ModernBertConfig","local":"transformers.ModernBertConfig","sections":[],"depth":2},{"title":"ModernBertModel","local":"transformers.ModernBertModel","sections":[],"depth":2},{"title":"ModernBertForMaskedLM","local":"transformers.ModernBertForMaskedLM","sections":[],"depth":2},{"title":"ModernBertForSequenceClassification","local":"transformers.ModernBertForSequenceClassification","sections":[],"depth":2},{"title":"ModernBertForTokenClassification","local":"transformers.ModernBertForTokenClassification","sections":[],"depth":2},{"title":"ModernBertForMultipleChoice","local":"transformers.ModernBertForMultipleChoice","sections":[],"depth":2},{"title":"ModernBertForQuestionAnswering","local":"transformers.ModernBertForQuestionAnswering","sections":[{"title":"Usage tips","local":"usage-tips","sections":[],"depth":3}],"depth":2}],"depth":1}';function so(J){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fo extends jn{constructor(e){super(),Cn(this,e,so,no,Bn,{})}}export{fo as component};
