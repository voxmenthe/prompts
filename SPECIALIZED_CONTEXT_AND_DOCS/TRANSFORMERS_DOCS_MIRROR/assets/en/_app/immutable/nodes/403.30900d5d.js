import{s as Po,z as Co,o as Io,n as Ye}from"../chunks/scheduler.18a86fab.js";import{S as Uo,i as zo,g as c,s,r as g,A as So,h as p,f as t,c as r,j as J,x as u,u as f,k as j,l as ko,y as i,a,v as h,d as y,t as _,w as b}from"../chunks/index.98837b22.js";import{T as Jo}from"../chunks/Tip.77304350.js";import{D as V}from"../chunks/Docstring.a1ef7999.js";import{C as Le}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as jo}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ie,E as xo}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Bo,a as Vo}from"../chunks/HfOption.6641485e.js";function Zo(P){let n,M='This model was contributed by <a href="https://huggingface.co/stevenbucaille" rel="nofollow">stevenbucaille</a>.',l,m,w="Click on the SuperPoint models in the right sidebar for more examples of how to apply SuperPoint to different computer vision tasks.";return{c(){n=c("p"),n.innerHTML=M,l=s(),m=c("p"),m.textContent=w},l(d){n=p(d,"P",{"data-svelte-h":!0}),u(n)!=="svelte-1ir9jnx"&&(n.innerHTML=M),l=r(d),m=p(d,"P",{"data-svelte-h":!0}),u(m)!=="svelte-jba62c"&&(m.textContent=w)},m(d,T){a(d,n,T),a(d,l,T),a(d,m,T)},p:Ye,d(d){d&&(t(n),t(l),t(m))}}}function Wo(P){let n,M;return n=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN1cGVyUG9pbnRGb3JLZXlwb2ludERldGVjdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtYWdpYy1sZWFwLWNvbW11bml0eSUyRnN1cGVycG9pbnQlMjIpJTBBbW9kZWwlMjAlM0QlMjBTdXBlclBvaW50Rm9yS2V5cG9pbnREZXRlY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1hZ2ljLWxlYXAtY29tbXVuaXR5JTJGc3VwZXJwb2ludCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwUG9zdC1wcm9jZXNzJTIwdG8lMjBnZXQlMjBrZXlwb2ludHMlMkMlMjBzY29yZXMlMkMlMjBhbmQlMjBkZXNjcmlwdG9ycyUwQWltYWdlX3NpemUlMjAlM0QlMjAoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTBBcHJvY2Vzc2VkX291dHB1dHMlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX2tleXBvaW50X2RldGVjdGlvbihvdXRwdXRzJTJDJTIwJTVCaW1hZ2Vfc2l6ZSU1RCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SuperPointForKeypointDetection
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)
model = SuperPointForKeypointDetection.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)

inputs = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="hljs-comment"># Post-process to get keypoints, scores, and descriptors</span>
image_size = (image.height, image.width)
processed_outputs = processor.post_process_keypoint_detection(outputs, [image_size])`,wrap:!1}}),{c(){g(n.$$.fragment)},l(l){f(n.$$.fragment,l)},m(l,m){h(n,l,m),M=!0},p:Ye,i(l){M||(y(n.$$.fragment,l),M=!0)},o(l){_(n.$$.fragment,l),M=!1},d(l){b(n,l)}}}function Ro(P){let n,M;return n=new Vo({props:{id:"usage",option:"AutoModel",$$slots:{default:[Wo]},$$scope:{ctx:P}}}),{c(){g(n.$$.fragment)},l(l){f(n.$$.fragment,l)},m(l,m){h(n,l,m),M=!0},p(l,m){const w={};m&2&&(w.$$scope={dirty:m,ctx:l}),n.$set(w)},i(l){M||(y(n.$$.fragment,l),M=!0)},o(l){_(n.$$.fragment,l),M=!1},d(l){b(n,l)}}}function Ho(P){let n,M="Example:",l,m,w;return m=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN1cGVyUG9pbnRDb25maWclMkMlMjBTdXBlclBvaW50Rm9yS2V5cG9pbnREZXRlY3Rpb24lMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwU3VwZXJQb2ludCUyMHN1cGVycG9pbnQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwU3VwZXJQb2ludENvbmZpZygpJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMHN1cGVycG9pbnQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFN1cGVyUG9pbnRGb3JLZXlwb2ludERldGVjdGlvbihjb25maWd1cmF0aW9uKSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SuperPointConfig, SuperPointForKeypointDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SuperPoint superpoint style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SuperPointConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the superpoint style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SuperPointForKeypointDetection(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=c("p"),n.textContent=M,l=s(),g(m.$$.fragment)},l(d){n=p(d,"P",{"data-svelte-h":!0}),u(n)!=="svelte-11lpom8"&&(n.textContent=M),l=r(d),f(m.$$.fragment,d)},m(d,T){a(d,n,T),a(d,l,T),h(m,d,T),w=!0},p:Ye,i(d){w||(y(m.$$.fragment,d),w=!0)},o(d){_(m.$$.fragment,d),w=!1},d(d){d&&(t(n),t(l)),b(m,d)}}}function Fo(P){let n,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=M},l(l){n=p(l,"P",{"data-svelte-h":!0}),u(n)!=="svelte-fincs2"&&(n.innerHTML=M)},m(l,m){a(l,n,m)},p:Ye,d(l){l&&t(n)}}}function Go(P){let n,M="Examples:",l,m,w;return m=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN1cGVyUG9pbnRGb3JLZXlwb2ludERldGVjdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtYWdpYy1sZWFwLWNvbW11bml0eSUyRnN1cGVycG9pbnQlMjIpJTBBbW9kZWwlMjAlM0QlMjBTdXBlclBvaW50Rm9yS2V5cG9pbnREZXRlY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1hZ2ljLWxlYXAtY29tbXVuaXR5JTJGc3VwZXJwb2ludCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SuperPointForKeypointDetection
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SuperPointForKeypointDetection.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),{c(){n=c("p"),n.textContent=M,l=s(),g(m.$$.fragment)},l(d){n=p(d,"P",{"data-svelte-h":!0}),u(n)!=="svelte-kvfsh7"&&(n.textContent=M),l=r(d),f(m.$$.fragment,d)},m(d,T){a(d,n,T),a(d,l,T),h(m,d,T),w=!0},p:Ye,i(d){w||(y(m.$$.fragment,d),w=!0)},o(d){_(m.$$.fragment,d),w=!1},d(d){d&&(t(n),t(l)),b(m,d)}}}function Xo(P){let n,M,l,m,w,d="<em>This model was released on 2017-12-20 and added to Hugging Face Transformers on 2024-03-19.</em>",T,Z,Kt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Qe,Q,Ae,A,Ot='<a href="https://huggingface.co/papers/1712.07629" rel="nofollow">SuperPoint</a> is the result of self-supervised training of a fully-convolutional network for interest point detection and description. The model is able to detect interest points that are repeatable under homographic transformations and provide a descriptor for each point. Usage on it’s own is limited, but it can be used as a feature extractor for other tasks such as homography estimation and image matching.',Ke,W,eo,Oe,K,to='You can find all the original SuperPoint checkpoints under the <a href="https://huggingface.co/magic-leap-community" rel="nofollow">Magic Leap Community</a> organization.',et,R,tt,O,oo='The example below demonstrates how to detect interest points in an image with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',ot,H,nt,ee,st,I,te,Ue,no="SuperPoint outputs a dynamic number of keypoints per image, which makes it suitable for tasks requiring variable-length feature representations.",jt,oe,Pt,ze,so="<p>The model provides both keypoint coordinates and their corresponding descriptors (256-dimensional vectors) in a single forward pass.</p>",Ct,ne,Se,ro="For batch processing with multiple images, you need to use the mask attribute to retrieve the respective information for each image. You can use the <code>post_process_keypoint_detection</code> from the <code>SuperPointImageProcessor</code> to retrieve the each image information.",It,se,Ut,re,ke,ao="You can then print the keypoints on the image of your choice to visualize the result:",zt,ae,rt,F,io='<img src="https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/ZtFmphEhx8tcbEQqOolyE.png"/>',at,ie,it,le,lo='<li>Refer to this <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb" rel="nofollow">notebook</a> for an inference and visualization example.</li>',lt,ce,ct,C,pe,St,xe,co=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointForKeypointDetection">SuperPointForKeypointDetection</a>. It is used to instantiate a
SuperPoint model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SuperPoint
<a href="https://huggingface.co/magic-leap-community/superpoint" rel="nofollow">magic-leap-community/superpoint</a> architecture.`,kt,Be,po=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,xt,G,pt,de,dt,v,me,Bt,Ve,mo="Constructs a SuperPoint image processor.",Vt,X,ue,Zt,Ze,uo=`Converts the raw output of <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointForKeypointDetection">SuperPointForKeypointDetection</a> into lists of keypoints, scores and descriptors
with coordinates absolute to the original image sizes.`,Wt,N,ge,Rt,We,go="Preprocess an image or batch of images.",Ht,D,fe,Ft,Re,fo="Resize an image.",mt,he,ho="<li>preprocess</li>",ut,ye,gt,z,_e,Gt,He,yo="Constructs a fast Superpoint image processor.",Xt,E,be,Nt,Fe,_o=`Converts the raw output of <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointForKeypointDetection">SuperPointForKeypointDetection</a> into lists of keypoints, scores and descriptors
with coordinates absolute to the original image sizes.`,ft,Me,bo="<li>preprocess</li> <li>post_process_keypoint_detection</li>",ht,we,yt,$,Te,Dt,Ge,Mo="SuperPoint model outputting keypoints and descriptors.",Et,Xe,wo=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Lt,Ne,To=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qt,U,ve,Yt,De,vo='The <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointForKeypointDetection">SuperPointForKeypointDetection</a> forward method, overrides the <code>__call__</code> special method.',Qt,L,At,q,_t,$e,$o="<li>forward</li>",bt,Je,Mt,qe,wt;return Q=new Ie({props:{title:"SuperPoint",local:"superpoint",headingTag:"h1"}}),R=new Jo({props:{warning:!1,$$slots:{default:[Zo]},$$scope:{ctx:P}}}),H=new Bo({props:{id:"usage",options:["AutoModel"],$$slots:{default:[Ro]},$$scope:{ctx:P}}}),ee=new Ie({props:{title:"Notes",local:"notes",headingTag:"h2"}}),oe=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN1cGVyUG9pbnRGb3JLZXlwb2ludERldGVjdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1hZ2ljLWxlYXAtY29tbXVuaXR5JTJGc3VwZXJwb2ludCUyMiklMEFtb2RlbCUyMCUzRCUyMFN1cGVyUG9pbnRGb3JLZXlwb2ludERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWFnaWMtbGVhcC1jb21tdW5pdHklMkZzdXBlcnBvaW50JTIyKSUwQXVybF9pbWFnZV8xJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlXzElMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmxfaW1hZ2VfMSUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEF1cmxfaW1hZ2VfMiUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdGVzdC1zdHVmZjIwMTclMkYwMDAwMDAwMDA1NjguanBnJTIyJTBBaW1hZ2VfMiUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybF9pbWFnZV8yJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWltYWdlcyUyMCUzRCUyMCU1QmltYWdlXzElMkMlMjBpbWFnZV8yJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTIzJTIwRXhhbXBsZSUyMG9mJTIwaGFuZGxpbmclMjBkeW5hbWljJTIwa2V5cG9pbnQlMjBvdXRwdXQlMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBa2V5cG9pbnRzJTIwJTNEJTIwb3V0cHV0cy5rZXlwb2ludHMlMjAlMjAlMjMlMjBTaGFwZSUyMHZhcmllcyUyMHBlciUyMGltYWdlJTBBc2NvcmVzJTIwJTNEJTIwb3V0cHV0cy5zY29yZXMlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjMlMjBDb25maWRlbmNlJTIwc2NvcmVzJTIwZm9yJTIwZWFjaCUyMGtleXBvaW50JTBBZGVzY3JpcHRvcnMlMjAlM0QlMjBvdXRwdXRzLmRlc2NyaXB0b3JzJTIwJTIwJTIzJTIwMjU2LWRpbWVuc2lvbmFsJTIwZGVzY3JpcHRvcnMlMEFtYXNrJTIwJTNEJTIwb3V0cHV0cy5tYXNrJTIwJTIzJTIwVmFsdWUlMjBvZiUyMDElMjBjb3JyZXNwb25kcyUyMHRvJTIwYSUyMGtleXBvaW50JTIwZGV0ZWN0aW9u",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SuperPointForKeypointDetection
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)
model = SuperPointForKeypointDetection.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superpoint&quot;</span>)
url_image_1 = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image_1 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image_1, stream=<span class="hljs-literal">True</span>).raw)
url_image_2 = <span class="hljs-string">&quot;http://images.cocodataset.org/test-stuff2017/000000000568.jpg&quot;</span>
image_2 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image_2, stream=<span class="hljs-literal">True</span>).raw)
images = [image_1, image_2]
inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-comment"># Example of handling dynamic keypoint output</span>
outputs = model(**inputs)
keypoints = outputs.keypoints  <span class="hljs-comment"># Shape varies per image</span>
scores = outputs.scores        <span class="hljs-comment"># Confidence scores for each keypoint</span>
descriptors = outputs.descriptors  <span class="hljs-comment"># 256-dimensional descriptors</span>
mask = outputs.mask <span class="hljs-comment"># Value of 1 corresponds to a keypoint detection</span>`,wrap:!1}}),se=new Le({props:{code:"JTIzJTIwQmF0Y2glMjBwcm9jZXNzaW5nJTIwZXhhbXBsZSUwQWltYWdlcyUyMCUzRCUyMCU1QmltYWdlMSUyQyUyMGltYWdlMiUyQyUyMGltYWdlMyU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFpbWFnZV9zaXplcyUyMCUzRCUyMCU1QihpbWcuaGVpZ2h0JTJDJTIwaW1nLndpZHRoKSUyMGZvciUyMGltZyUyMGluJTIwaW1hZ2VzJTVEJTBBcHJvY2Vzc2VkX291dHB1dHMlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX2tleXBvaW50X2RldGVjdGlvbihvdXRwdXRzJTJDJTIwaW1hZ2Vfc2l6ZXMp",highlighted:`<span class="hljs-comment"># Batch processing example</span>
images = [image1, image2, image3]
inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)
image_sizes = [(img.height, img.width) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> images]
processed_outputs = processor.post_process_keypoint_detection(outputs, image_sizes)`,wrap:!1}}),ae=new Le({props:{code:"aW1wb3J0JTIwbWF0cGxvdGxpYi5weXBsb3QlMjBhcyUyMHBsdCUwQXBsdC5heGlzKCUyMm9mZiUyMiklMEFwbHQuaW1zaG93KGltYWdlXzEpJTBBcGx0LnNjYXR0ZXIoJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyU1QjAlNUQlNUIlMjJrZXlwb2ludHMlMjIlNUQlNUIlM0ElMkMlMjAwJTVEJTJDJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyU1QjAlNUQlNUIlMjJrZXlwb2ludHMlMjIlNUQlNUIlM0ElMkMlMjAxJTVEJTJDJTBBJTIwJTIwJTIwJTIwYyUzRG91dHB1dHMlNUIwJTVEJTVCJTIyc2NvcmVzJTIyJTVEJTIwKiUyMDEwMCUyQyUwQSUyMCUyMCUyMCUyMHMlM0RvdXRwdXRzJTVCMCU1RCU1QiUyMnNjb3JlcyUyMiU1RCUyMColMjA1MCUyQyUwQSUyMCUyMCUyMCUyMGFscGhhJTNEMC44JTBBKSUwQXBsdC5zYXZlZmlnKGYlMjJvdXRwdXRfaW1hZ2UucG5nJTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.axis(<span class="hljs-string">&quot;off&quot;</span>)
plt.imshow(image_1)
plt.scatter(
    outputs[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;keypoints&quot;</span>][:, <span class="hljs-number">0</span>],
    outputs[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;keypoints&quot;</span>][:, <span class="hljs-number">1</span>],
    c=outputs[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;scores&quot;</span>] * <span class="hljs-number">100</span>,
    s=outputs[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;scores&quot;</span>] * <span class="hljs-number">50</span>,
    alpha=<span class="hljs-number">0.8</span>
)
plt.savefig(<span class="hljs-string">f&quot;output_image.png&quot;</span>)`,wrap:!1}}),ie=new Ie({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ce=new Ie({props:{title:"SuperPointConfig",local:"transformers.SuperPointConfig",headingTag:"h2"}}),pe=new V({props:{name:"class transformers.SuperPointConfig",anchor:"transformers.SuperPointConfig",parameters:[{name:"encoder_hidden_sizes",val:": list = [64, 64, 128, 128]"},{name:"decoder_hidden_size",val:": int = 256"},{name:"keypoint_decoder_dim",val:": int = 65"},{name:"descriptor_decoder_dim",val:": int = 256"},{name:"keypoint_threshold",val:": float = 0.005"},{name:"max_keypoints",val:": int = -1"},{name:"nms_radius",val:": int = 4"},{name:"border_removal_distance",val:": int = 4"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperPointConfig.encoder_hidden_sizes",description:`<strong>encoder_hidden_sizes</strong> (<code>List</code>, <em>optional</em>, defaults to <code>[64, 64, 128, 128]</code>) &#x2014;
The number of channels in each convolutional layer in the encoder.`,name:"encoder_hidden_sizes"},{anchor:"transformers.SuperPointConfig.decoder_hidden_size",description:"<strong>decoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014; The hidden size of the decoder.",name:"decoder_hidden_size"},{anchor:"transformers.SuperPointConfig.keypoint_decoder_dim",description:"<strong>keypoint_decoder_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 65) &#x2014; The output dimension of the keypoint decoder.",name:"keypoint_decoder_dim"},{anchor:"transformers.SuperPointConfig.descriptor_decoder_dim",description:"<strong>descriptor_decoder_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014; The output dimension of the descriptor decoder.",name:"descriptor_decoder_dim"},{anchor:"transformers.SuperPointConfig.keypoint_threshold",description:`<strong>keypoint_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.005) &#x2014;
The threshold to use for extracting keypoints.`,name:"keypoint_threshold"},{anchor:"transformers.SuperPointConfig.max_keypoints",description:`<strong>max_keypoints</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The maximum number of keypoints to extract. If <code>-1</code>, will extract all keypoints.`,name:"max_keypoints"},{anchor:"transformers.SuperPointConfig.nms_radius",description:`<strong>nms_radius</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The radius for non-maximum suppression.`,name:"nms_radius"},{anchor:"transformers.SuperPointConfig.border_removal_distance",description:`<strong>border_removal_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The distance from the border to remove keypoints.`,name:"border_removal_distance"},{anchor:"transformers.SuperPointConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/configuration_superpoint.py#L22"}}),G=new jo({props:{anchor:"transformers.SuperPointConfig.example",$$slots:{default:[Ho]},$$scope:{ctx:P}}}),de=new Ie({props:{title:"SuperPointImageProcessor",local:"transformers.SuperPointImageProcessor",headingTag:"h2"}}),me=new V({props:{name:"class transformers.SuperPointImageProcessor",anchor:"transformers.SuperPointImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": float = 0.00392156862745098"},{name:"do_grayscale",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperPointImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden
by <code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.SuperPointImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 480, &quot;width&quot;: 640}</code>):
Resolution of the output image after <code>resize</code> is applied. Only has an effect if <code>do_resize</code> is set to
<code>True</code>. Can be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.SuperPointImageProcessor.resample",description:`<strong>resample</strong> (<code>Resampling</code>, <em>optional</em>, defaults to <code>2</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.SuperPointImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.SuperPointImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.SuperPointImageProcessor.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to convert the image to grayscale. Can be overridden by <code>do_grayscale</code> in the <code>preprocess</code> method.`,name:"do_grayscale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint.py#L100"}}),ue=new V({props:{name:"post_process_keypoint_detection",anchor:"transformers.SuperPointImageProcessor.post_process_keypoint_detection",parameters:[{name:"outputs",val:": SuperPointKeypointDescriptionOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]]"}],parametersDescription:[{anchor:"transformers.SuperPointImageProcessor.post_process_keypoint_detection.outputs",description:`<strong>outputs</strong> (<code>SuperPointKeypointDescriptionOutput</code>) &#x2014;
Raw outputs of the model containing keypoints in a relative (x, y) format, with scores and descriptors.`,name:"outputs"},{anchor:"transformers.SuperPointImageProcessor.post_process_keypoint_detection.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>list[tuple[int, int]]</code>) &#x2014;
Tensor of shape <code>(batch_size, 2)</code> or list of tuples (<code>tuple[int, int]</code>) containing the target size
<code>(height, width)</code> of each image in the batch. This must be the original
image size (before any processing).`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint.py#L302",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints in absolute format according
to target_sizes, scores and descriptors for an image in the batch as predicted by the model.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[Dict]</code></p>
`}}),ge=new V({props:{name:"preprocess",anchor:"transformers.SuperPointImageProcessor.preprocess",parameters:[{name:"images",val:""},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_grayscale",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperPointImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.SuperPointImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.SuperPointImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output image after <code>resize</code> has been applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image
is resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the
image will be matched to <code>int(size[&quot;shortest_edge&quot;]/ crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.SuperPointImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.SuperPointImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.SuperPointImageProcessor.preprocess.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_grayscale</code>) &#x2014;
Whether to convert the image to grayscale.`,name:"do_grayscale"},{anchor:"transformers.SuperPointImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.SuperPointImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SuperPointImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint.py#L185"}}),fe=new V({props:{name:"resize",anchor:"transformers.SuperPointImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperPointImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.SuperPointImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary of the form <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code>, specifying the size of the output image.`,name:"size"},{anchor:"transformers.SuperPointImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If not provided, it will be inferred from the input
image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SuperPointImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint.py#L146"}}),ye=new Ie({props:{title:"SuperPointImageProcessorFast",local:"transformers.SuperPointImageProcessorFast",headingTag:"h2"}}),_e=new V({props:{name:"class transformers.SuperPointImageProcessorFast",anchor:"transformers.SuperPointImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.superpoint.image_processing_superpoint_fast.SuperPointFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint_fast.py#L92"}}),be=new V({props:{name:"post_process_keypoint_detection",anchor:"transformers.SuperPointImageProcessorFast.post_process_keypoint_detection",parameters:[{name:"outputs",val:": SuperPointKeypointDescriptionOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]]"}],parametersDescription:[{anchor:"transformers.SuperPointImageProcessorFast.post_process_keypoint_detection.outputs",description:`<strong>outputs</strong> (<code>SuperPointKeypointDescriptionOutput</code>) &#x2014;
Raw outputs of the model containing keypoints in a relative (x, y) format, with scores and descriptors.`,name:"outputs"},{anchor:"transformers.SuperPointImageProcessorFast.post_process_keypoint_detection.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>List[Tuple[int, int]]</code>) &#x2014;
Tensor of shape <code>(batch_size, 2)</code> or list of tuples (<code>Tuple[int, int]</code>) containing the target size
<code>(height, width)</code> of each image in the batch. This must be the original
image size (before any processing).`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/image_processing_superpoint_fast.py#L132",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints in absolute format according
to target_sizes, scores and descriptors for an image in the batch as predicted by the model.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[Dict]</code></p>
`}}),we=new Ie({props:{title:"SuperPointForKeypointDetection",local:"transformers.SuperPointForKeypointDetection",headingTag:"h2"}}),Te=new V({props:{name:"class transformers.SuperPointForKeypointDetection",anchor:"transformers.SuperPointForKeypointDetection",parameters:[{name:"config",val:": SuperPointConfig"}],parametersDescription:[{anchor:"transformers.SuperPointForKeypointDetection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointConfig">SuperPointConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/modeling_superpoint.py#L364"}}),ve=new V({props:{name:"forward",anchor:"transformers.SuperPointForKeypointDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SuperPointForKeypointDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointImageProcessor">SuperPointImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SuperPointImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointImageProcessor">SuperPointImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SuperPointForKeypointDetection.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.SuperPointForKeypointDetection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SuperPointForKeypointDetection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superpoint/modeling_superpoint.py#L385",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.superpoint.modeling_superpoint.SuperPointKeypointDescriptionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointConfig"
>SuperPointConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>) — Loss computed during training.</li>
<li><strong>keypoints</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, 2)</code>) — Relative (x, y) coordinates of predicted keypoints in a given image.</li>
<li><strong>scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints)</code>) — Scores of predicted keypoints.</li>
<li><strong>descriptors</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, descriptor_size)</code>) — Descriptors of predicted keypoints.</li>
<li><strong>mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_keypoints)</code>) — Mask indicating which values in keypoints, scores and descriptors are keypoint information.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or</li>
<li><strong>when</strong> <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.superpoint.modeling_superpoint.SuperPointKeypointDescriptionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),L=new Jo({props:{$$slots:{default:[Fo]},$$scope:{ctx:P}}}),q=new jo({props:{anchor:"transformers.SuperPointForKeypointDetection.forward.example",$$slots:{default:[Go]},$$scope:{ctx:P}}}),Je=new xo({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/superpoint.md"}}),{c(){n=c("meta"),M=s(),l=c("p"),m=s(),w=c("p"),w.innerHTML=d,T=s(),Z=c("div"),Z.innerHTML=Kt,Qe=s(),g(Q.$$.fragment),Ae=s(),A=c("p"),A.innerHTML=Ot,Ke=s(),W=c("img"),Oe=s(),K=c("p"),K.innerHTML=to,et=s(),g(R.$$.fragment),tt=s(),O=c("p"),O.innerHTML=oo,ot=s(),g(H.$$.fragment),nt=s(),g(ee.$$.fragment),st=s(),I=c("ul"),te=c("li"),Ue=c("p"),Ue.textContent=no,jt=s(),g(oe.$$.fragment),Pt=s(),ze=c("li"),ze.innerHTML=so,Ct=s(),ne=c("li"),Se=c("p"),Se.innerHTML=ro,It=s(),g(se.$$.fragment),Ut=s(),re=c("li"),ke=c("p"),ke.textContent=ao,zt=s(),g(ae.$$.fragment),rt=s(),F=c("div"),F.innerHTML=io,at=s(),g(ie.$$.fragment),it=s(),le=c("ul"),le.innerHTML=lo,lt=s(),g(ce.$$.fragment),ct=s(),C=c("div"),g(pe.$$.fragment),St=s(),xe=c("p"),xe.innerHTML=co,kt=s(),Be=c("p"),Be.innerHTML=po,xt=s(),g(G.$$.fragment),pt=s(),g(de.$$.fragment),dt=s(),v=c("div"),g(me.$$.fragment),Bt=s(),Ve=c("p"),Ve.textContent=mo,Vt=s(),X=c("div"),g(ue.$$.fragment),Zt=s(),Ze=c("p"),Ze.innerHTML=uo,Wt=s(),N=c("div"),g(ge.$$.fragment),Rt=s(),We=c("p"),We.textContent=go,Ht=s(),D=c("div"),g(fe.$$.fragment),Ft=s(),Re=c("p"),Re.textContent=fo,mt=s(),he=c("ul"),he.innerHTML=ho,ut=s(),g(ye.$$.fragment),gt=s(),z=c("div"),g(_e.$$.fragment),Gt=s(),He=c("p"),He.textContent=yo,Xt=s(),E=c("div"),g(be.$$.fragment),Nt=s(),Fe=c("p"),Fe.innerHTML=_o,ft=s(),Me=c("ul"),Me.innerHTML=bo,ht=s(),g(we.$$.fragment),yt=s(),$=c("div"),g(Te.$$.fragment),Dt=s(),Ge=c("p"),Ge.textContent=Mo,Et=s(),Xe=c("p"),Xe.innerHTML=wo,Lt=s(),Ne=c("p"),Ne.innerHTML=To,qt=s(),U=c("div"),g(ve.$$.fragment),Yt=s(),De=c("p"),De.innerHTML=vo,Qt=s(),g(L.$$.fragment),At=s(),g(q.$$.fragment),_t=s(),$e=c("ul"),$e.innerHTML=$o,bt=s(),g(Je.$$.fragment),Mt=s(),qe=c("p"),this.h()},l(e){const o=So("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(t),M=r(e),l=p(e,"P",{}),J(l).forEach(t),m=r(e),w=p(e,"P",{"data-svelte-h":!0}),u(w)!=="svelte-d1mzc"&&(w.innerHTML=d),T=r(e),Z=p(e,"DIV",{style:!0,"data-svelte-h":!0}),u(Z)!=="svelte-wa5t4p"&&(Z.innerHTML=Kt),Qe=r(e),f(Q.$$.fragment,e),Ae=r(e),A=p(e,"P",{"data-svelte-h":!0}),u(A)!=="svelte-1gvl3qc"&&(A.innerHTML=Ot),Ke=r(e),W=p(e,"IMG",{src:!0,alt:!0,width:!0}),Oe=r(e),K=p(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-15g1poy"&&(K.innerHTML=to),et=r(e),f(R.$$.fragment,e),tt=r(e),O=p(e,"P",{"data-svelte-h":!0}),u(O)!=="svelte-kl0to2"&&(O.innerHTML=oo),ot=r(e),f(H.$$.fragment,e),nt=r(e),f(ee.$$.fragment,e),st=r(e),I=p(e,"UL",{});var S=J(I);te=p(S,"LI",{});var je=J(te);Ue=p(je,"P",{"data-svelte-h":!0}),u(Ue)!=="svelte-aesrgh"&&(Ue.textContent=no),jt=r(je),f(oe.$$.fragment,je),je.forEach(t),Pt=r(S),ze=p(S,"LI",{"data-svelte-h":!0}),u(ze)!=="svelte-s4aeuq"&&(ze.innerHTML=so),Ct=r(S),ne=p(S,"LI",{});var Pe=J(ne);Se=p(Pe,"P",{"data-svelte-h":!0}),u(Se)!=="svelte-d27svf"&&(Se.innerHTML=ro),It=r(Pe),f(se.$$.fragment,Pe),Pe.forEach(t),Ut=r(S),re=p(S,"LI",{});var Ce=J(re);ke=p(Ce,"P",{"data-svelte-h":!0}),u(ke)!=="svelte-1ak0l5f"&&(ke.textContent=ao),zt=r(Ce),f(ae.$$.fragment,Ce),Ce.forEach(t),S.forEach(t),rt=r(e),F=p(e,"DIV",{class:!0,"data-svelte-h":!0}),u(F)!=="svelte-1nr7u09"&&(F.innerHTML=io),at=r(e),f(ie.$$.fragment,e),it=r(e),le=p(e,"UL",{"data-svelte-h":!0}),u(le)!=="svelte-117o6vf"&&(le.innerHTML=lo),lt=r(e),f(ce.$$.fragment,e),ct=r(e),C=p(e,"DIV",{class:!0});var k=J(C);f(pe.$$.fragment,k),St=r(k),xe=p(k,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-tak9qs"&&(xe.innerHTML=co),kt=r(k),Be=p(k,"P",{"data-svelte-h":!0}),u(Be)!=="svelte-1ek1ss9"&&(Be.innerHTML=po),xt=r(k),f(G.$$.fragment,k),k.forEach(t),pt=r(e),f(de.$$.fragment,e),dt=r(e),v=p(e,"DIV",{class:!0});var x=J(v);f(me.$$.fragment,x),Bt=r(x),Ve=p(x,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-bn94yj"&&(Ve.textContent=mo),Vt=r(x),X=p(x,"DIV",{class:!0});var Tt=J(X);f(ue.$$.fragment,Tt),Zt=r(Tt),Ze=p(Tt,"P",{"data-svelte-h":!0}),u(Ze)!=="svelte-sx1dpd"&&(Ze.innerHTML=uo),Tt.forEach(t),Wt=r(x),N=p(x,"DIV",{class:!0});var vt=J(N);f(ge.$$.fragment,vt),Rt=r(vt),We=p(vt,"P",{"data-svelte-h":!0}),u(We)!=="svelte-1x3yxsa"&&(We.textContent=go),vt.forEach(t),Ht=r(x),D=p(x,"DIV",{class:!0});var $t=J(D);f(fe.$$.fragment,$t),Ft=r($t),Re=p($t,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-1eb2h1k"&&(Re.textContent=fo),$t.forEach(t),x.forEach(t),mt=r(e),he=p(e,"UL",{"data-svelte-h":!0}),u(he)!=="svelte-j0c82d"&&(he.innerHTML=ho),ut=r(e),f(ye.$$.fragment,e),gt=r(e),z=p(e,"DIV",{class:!0});var Ee=J(z);f(_e.$$.fragment,Ee),Gt=r(Ee),He=p(Ee,"P",{"data-svelte-h":!0}),u(He)!=="svelte-u1bxcd"&&(He.textContent=yo),Xt=r(Ee),E=p(Ee,"DIV",{class:!0});var Jt=J(E);f(be.$$.fragment,Jt),Nt=r(Jt),Fe=p(Jt,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-sx1dpd"&&(Fe.innerHTML=_o),Jt.forEach(t),Ee.forEach(t),ft=r(e),Me=p(e,"UL",{"data-svelte-h":!0}),u(Me)!=="svelte-166rkny"&&(Me.innerHTML=bo),ht=r(e),f(we.$$.fragment,e),yt=r(e),$=p(e,"DIV",{class:!0});var B=J($);f(Te.$$.fragment,B),Dt=r(B),Ge=p(B,"P",{"data-svelte-h":!0}),u(Ge)!=="svelte-i59ede"&&(Ge.textContent=Mo),Et=r(B),Xe=p(B,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-q52n56"&&(Xe.innerHTML=wo),Lt=r(B),Ne=p(B,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-hswkmf"&&(Ne.innerHTML=To),qt=r(B),U=p(B,"DIV",{class:!0});var Y=J(U);f(ve.$$.fragment,Y),Yt=r(Y),De=p(Y,"P",{"data-svelte-h":!0}),u(De)!=="svelte-4jmiqc"&&(De.innerHTML=vo),Qt=r(Y),f(L.$$.fragment,Y),At=r(Y),f(q.$$.fragment,Y),Y.forEach(t),B.forEach(t),_t=r(e),$e=p(e,"UL",{"data-svelte-h":!0}),u($e)!=="svelte-n3ow4o"&&($e.innerHTML=$o),bt=r(e),f(Je.$$.fragment,e),Mt=r(e),qe=p(e,"P",{}),J(qe).forEach(t),this.h()},h(){j(n,"name","hf:doc:metadata"),j(n,"content",No),ko(Z,"float","right"),Co(W.src,eo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/superpoint_architecture.png")||j(W,"src",eo),j(W,"alt","drawing"),j(W,"width","500"),j(F,"class","flex justify-center"),j(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){i(document.head,n),a(e,M,o),a(e,l,o),a(e,m,o),a(e,w,o),a(e,T,o),a(e,Z,o),a(e,Qe,o),h(Q,e,o),a(e,Ae,o),a(e,A,o),a(e,Ke,o),a(e,W,o),a(e,Oe,o),a(e,K,o),a(e,et,o),h(R,e,o),a(e,tt,o),a(e,O,o),a(e,ot,o),h(H,e,o),a(e,nt,o),h(ee,e,o),a(e,st,o),a(e,I,o),i(I,te),i(te,Ue),i(te,jt),h(oe,te,null),i(I,Pt),i(I,ze),i(I,Ct),i(I,ne),i(ne,Se),i(ne,It),h(se,ne,null),i(I,Ut),i(I,re),i(re,ke),i(re,zt),h(ae,re,null),a(e,rt,o),a(e,F,o),a(e,at,o),h(ie,e,o),a(e,it,o),a(e,le,o),a(e,lt,o),h(ce,e,o),a(e,ct,o),a(e,C,o),h(pe,C,null),i(C,St),i(C,xe),i(C,kt),i(C,Be),i(C,xt),h(G,C,null),a(e,pt,o),h(de,e,o),a(e,dt,o),a(e,v,o),h(me,v,null),i(v,Bt),i(v,Ve),i(v,Vt),i(v,X),h(ue,X,null),i(X,Zt),i(X,Ze),i(v,Wt),i(v,N),h(ge,N,null),i(N,Rt),i(N,We),i(v,Ht),i(v,D),h(fe,D,null),i(D,Ft),i(D,Re),a(e,mt,o),a(e,he,o),a(e,ut,o),h(ye,e,o),a(e,gt,o),a(e,z,o),h(_e,z,null),i(z,Gt),i(z,He),i(z,Xt),i(z,E),h(be,E,null),i(E,Nt),i(E,Fe),a(e,ft,o),a(e,Me,o),a(e,ht,o),h(we,e,o),a(e,yt,o),a(e,$,o),h(Te,$,null),i($,Dt),i($,Ge),i($,Et),i($,Xe),i($,Lt),i($,Ne),i($,qt),i($,U),h(ve,U,null),i(U,Yt),i(U,De),i(U,Qt),h(L,U,null),i(U,At),h(q,U,null),a(e,_t,o),a(e,$e,o),a(e,bt,o),h(Je,e,o),a(e,Mt,o),a(e,qe,o),wt=!0},p(e,[o]){const S={};o&2&&(S.$$scope={dirty:o,ctx:e}),R.$set(S);const je={};o&2&&(je.$$scope={dirty:o,ctx:e}),H.$set(je);const Pe={};o&2&&(Pe.$$scope={dirty:o,ctx:e}),G.$set(Pe);const Ce={};o&2&&(Ce.$$scope={dirty:o,ctx:e}),L.$set(Ce);const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),q.$set(k)},i(e){wt||(y(Q.$$.fragment,e),y(R.$$.fragment,e),y(H.$$.fragment,e),y(ee.$$.fragment,e),y(oe.$$.fragment,e),y(se.$$.fragment,e),y(ae.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(pe.$$.fragment,e),y(G.$$.fragment,e),y(de.$$.fragment,e),y(me.$$.fragment,e),y(ue.$$.fragment,e),y(ge.$$.fragment,e),y(fe.$$.fragment,e),y(ye.$$.fragment,e),y(_e.$$.fragment,e),y(be.$$.fragment,e),y(we.$$.fragment,e),y(Te.$$.fragment,e),y(ve.$$.fragment,e),y(L.$$.fragment,e),y(q.$$.fragment,e),y(Je.$$.fragment,e),wt=!0)},o(e){_(Q.$$.fragment,e),_(R.$$.fragment,e),_(H.$$.fragment,e),_(ee.$$.fragment,e),_(oe.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(pe.$$.fragment,e),_(G.$$.fragment,e),_(de.$$.fragment,e),_(me.$$.fragment,e),_(ue.$$.fragment,e),_(ge.$$.fragment,e),_(fe.$$.fragment,e),_(ye.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(we.$$.fragment,e),_(Te.$$.fragment,e),_(ve.$$.fragment,e),_(L.$$.fragment,e),_(q.$$.fragment,e),_(Je.$$.fragment,e),wt=!1},d(e){e&&(t(M),t(l),t(m),t(w),t(T),t(Z),t(Qe),t(Ae),t(A),t(Ke),t(W),t(Oe),t(K),t(et),t(tt),t(O),t(ot),t(nt),t(st),t(I),t(rt),t(F),t(at),t(it),t(le),t(lt),t(ct),t(C),t(pt),t(dt),t(v),t(mt),t(he),t(ut),t(gt),t(z),t(ft),t(Me),t(ht),t(yt),t($),t(_t),t($e),t(bt),t(Mt),t(qe)),t(n),b(Q,e),b(R,e),b(H,e),b(ee,e),b(oe),b(se),b(ae),b(ie,e),b(ce,e),b(pe),b(G),b(de,e),b(me),b(ue),b(ge),b(fe),b(ye,e),b(_e),b(be),b(we,e),b(Te),b(ve),b(L),b(q),b(Je,e)}}}const No='{"title":"SuperPoint","local":"superpoint","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"SuperPointConfig","local":"transformers.SuperPointConfig","sections":[],"depth":2},{"title":"SuperPointImageProcessor","local":"transformers.SuperPointImageProcessor","sections":[],"depth":2},{"title":"SuperPointImageProcessorFast","local":"transformers.SuperPointImageProcessorFast","sections":[],"depth":2},{"title":"SuperPointForKeypointDetection","local":"transformers.SuperPointForKeypointDetection","sections":[],"depth":2}],"depth":1}';function Do(P){return Io(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class en extends Uo{constructor(n){super(),zo(this,n,Do,Xo,Po,{})}}export{en as component};
