import{s as Ae,o as Ye,n as Ze}from"../chunks/scheduler.18a86fab.js";import{S as Oe,i as Ke,g as r,s as l,r as u,A as et,h as d,f as n,c as i,j as oe,x as f,u as M,k as ne,y as p,a as s,v as y,d as _,t as b,w as C}from"../chunks/index.98837b22.js";import{T as tt}from"../chunks/Tip.77304350.js";import{D as Ie}from"../chunks/Docstring.a1ef7999.js";import{C as ze}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as qe}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ae,E as nt}from"../chunks/getInferenceSnippets.06c2775f.js";function st(j){let o,w="Example:",m,c,g;return c=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1MQ0RWaXNpb25Db25maWclMkMlMjBNTENEVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTUxDRFZpc2lvbkNvbmZpZyUyMHdpdGglMjBEZWVwR2xpbnQtQUklMkZtbGNkLXZpdC1iaWdHLXBhdGNoMTQtMzM2JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyME1MQ0RWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBNTENEVmlzaW9uTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMERlZXBHbGludC1BSSUyRm1sY2Qtdml0LWJpZ0ctcGF0Y2gxNC0zMzYlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME1MQ0RWaXNpb25Nb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MLCDVisionConfig, MLCDVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MLCDVisionConfig with DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MLCDVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MLCDVisionModel (with random weights) from the DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MLCDVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=r("p"),o.textContent=w,m=l(),u(c.$$.fragment)},l(a){o=d(a,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=w),m=i(a),M(c.$$.fragment,a)},m(a,$){s(a,o,$),s(a,m,$),y(c,a,$),g=!0},p:Ze,i(a){g||(_(c.$$.fragment,a),g=!0)},o(a){b(c.$$.fragment,a),g=!1},d(a){a&&(n(o),n(m)),C(c,a)}}}function ot(j){let o,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=r("p"),o.innerHTML=w},l(m){o=d(m,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=w)},m(m,c){s(m,o,c)},p:Ze,d(m){m&&n(o)}}}function at(j){let o,w="Example:",m,c,g;return c=new ze({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1MQ0RWaXNpb25Nb2RlbCUwQW1vZGVsJTIwJTNEJTIwTUxDRFZpc2lvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJEZWVwR2xpbnQtQUklMkZtbGNkLXZpdC1iaWdHLXBhdGNoMTQtNDQ4JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkRlZXBHbGludC1BSSUyRm1sY2Qtdml0LWJpZ0ctcGF0Y2gxNC00NDglMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEElMEFmZWF0dXJlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwcmludChmJTIyRXh0cmFjdGVkJTIwZmVhdHVyZXMlMjBzaGFwZSUzQSUyMCU3QmZlYXR1cmVzLnNoYXBlJTdEJTIyKSUwQXByaW50KGYlMjJOdW1iZXIlMjBvZiUyMGF0dGVudGlvbiUyMGxheWVycyUzQSUyMCU3QmxlbihvdXRwdXRzLmF0dGVudGlvbnMpJTdEJTIyKSUwQXByaW50KGYlMjJBdHRlbnRpb24lMjBzaGFwZSUzQSUyMCU3Qm91dHB1dHMuYXR0ZW50aW9ucyU1QjAlNUQuc2hhcGUlN0QlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MLCDVisionModel
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MLCDVisionModel.from_pretrained(<span class="hljs-string">&quot;DeepGlint-AI/mlcd-vit-bigG-patch14-448&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;DeepGlint-AI/mlcd-vit-bigG-patch14-448&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, output_attentions=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>features = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Extracted features shape: <span class="hljs-subst">{features.shape}</span>&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of attention layers: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(outputs.attentions)}</span>&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Attention shape: <span class="hljs-subst">{outputs.attentions[<span class="hljs-number">0</span>].shape}</span>&quot;</span>)`,wrap:!1}}),{c(){o=r("p"),o.textContent=w,m=l(),u(c.$$.fragment)},l(a){o=d(a,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=w),m=i(a),M(c.$$.fragment,a)},m(a,$){s(a,o,$),s(a,m,$),y(c,a,$),g=!0},p:Ze,i(a){g||(_(c.$$.fragment,a),g=!0)},o(a){b(c.$$.fragment,a),g=!1},d(a){a&&(n(o),n(m)),C(c,a)}}}function lt(j){let o,w,m,c,g,a="<em>This model was released on 2024-07-24 and added to Hugging Face Transformers on 2025-04-15.</em>",$,Z,le,V,We='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ie,z,re,W,Ne='The <a href="https://huggingface.co/papers/2407.17331" rel="nofollow">MLCD</a> models were released by the DeepGlint-AI team in <a href="https://github.com/deepglint/unicom" rel="nofollow">unicom</a>, which focuses on building foundational visual models for large multimodal language models using large-scale datasets such as LAION400M and COYO700M, and employs sample-to-cluster contrastive learning to optimize performance. MLCD models are primarily used for multimodal visual large language models, such as LLaVA.',de,N,He="ðŸ”¥<strong>MLCD-ViT-bigG</strong>ðŸ”¥ series is the state-of-the-art vision transformer model enhanced with 2D Rotary Position Embedding (RoPE2D), achieving superior performance on document understanding and visual question answering tasks. Developed by DeepGlint AI, this model demonstrates exceptional capabilities in processing complex visual-language interactions.",ce,H,Ge="Tips:",me,G,Be='<li><p>We adopted the official <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" rel="nofollow">LLaVA-NeXT</a> and the official training dataset <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data" rel="nofollow">LLaVA-NeXT-Data</a> for evaluating the foundational visual models.</p></li> <li><p>The language model is <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" rel="nofollow">Qwen2.5-7B</a>.</p></li>',pe,B,Qe="Result:",ge,Q,Ee='<thead><tr><th align="left">Vision Tower</th> <th align="center">RoPE2D</th> <th align="left">ChartQA</th> <th align="left">DocVQA</th> <th align="left">InfoVQA</th> <th align="left">OCRBench</th> <th align="left">MMMU</th></tr></thead> <tbody><tr><td align="left">CLIP (ViT-L-14-336px)</td> <td align="center">Ã—</td> <td align="left">66.52</td> <td align="left">75.21</td> <td align="left">38.88</td> <td align="left">525.00</td> <td align="left">44.20</td></tr> <tr><td align="left">SigLIP (ViT-SO400M-384px)</td> <td align="center">Ã—</td> <td align="left">69.28</td> <td align="left">76.71</td> <td align="left">41.38</td> <td align="left">554.00</td> <td align="left">46.78</td></tr> <tr><td align="left">DFN5B (ViT-H-14-378px)</td> <td align="center">Ã—</td> <td align="left">64.36</td> <td align="left">70.87</td> <td align="left">38.59</td> <td align="left">473.00</td> <td align="left"><strong>48.00</strong></td></tr> <tr><td align="left"><strong><a href="https://huggingface.co/DeepGlint-AI/mlcd-vit-large-patch14-336" rel="nofollow">MLCD (ViT-L-14-336px)</a></strong></td> <td align="center">Ã—</td> <td align="left">67.84</td> <td align="left">76.46</td> <td align="left">43.48</td> <td align="left">531.00</td> <td align="left">44.30</td></tr> <tr><td align="left"><strong><a href="https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336" rel="nofollow">MLCD (ViT-bigG-14-336px)</a></strong></td> <td align="center">âˆš</td> <td align="left">71.07</td> <td align="left">79.63</td> <td align="left">44.38</td> <td align="left">572.00</td> <td align="left">46.78</td></tr> <tr><td align="left"><strong><a href="https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-448" rel="nofollow">MLCD (ViT-bigG-14-448px)</a></strong></td> <td align="center">âˆš</td> <td align="left"><strong>73.80</strong></td> <td align="left"><strong>83.34</strong></td> <td align="left"><strong>46.59</strong></td> <td align="left"><strong>582.00</strong></td> <td align="left">46.00</td></tr></tbody>',fe,E,he,R,ue,X,Me,T,k,Te,A,Re=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionModel">MLCDVisionModel</a>. It is used to instantiate a MLCD
vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the vision encoder of the MLCD
<a href="https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336" rel="nofollow">DeepGlint-AI/mlcd-vit-bigG-patch14-336</a> architecture.`,ve,Y,Xe=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,$e,D,ye,F,_e,h,P,Ue,O,ke="The vision model from M_L_C_D without any head or projection on top.",Le,K,Fe=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Je,ee,Pe=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,je,U,S,Ve,te,Se='The <a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionModel">MLCDVisionModel</a> forward method, overrides the <code>__call__</code> special method.',De,x,xe,I,be,q,Ce,se,we;return Z=new ae({props:{title:"MLCD",local:"mlcd",headingTag:"h1"}}),z=new ae({props:{title:"Overview",local:"overview",headingTag:"h2"}}),E=new ae({props:{title:"Usage",local:"usage",headingTag:"h2"}}),R=new ze({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1MQ0RWaXNpb25Nb2RlbCUwQSUwQSUyMyUyMExvYWQlMjBtb2RlbCUyMGFuZCUyMHByb2Nlc3NvciUwQW1vZGVsJTIwJTNEJTIwTUxDRFZpc2lvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJEZWVwR2xpbnQtQUklMkZtbGNkLXZpdC1iaWdHLXBhdGNoMTQtNDQ4JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkRlZXBHbGludC1BSSUyRm1sY2Qtdml0LWJpZ0ctcGF0Y2gxNC00NDglMjIpJTBBJTBBJTIzJTIwUHJvY2VzcyUyMHNpbmdsZSUyMGltYWdlJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwR2VuZXJhdGUlMjBvdXRwdXRzJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjBHZXQlMjB2aXN1YWwlMjBmZWF0dXJlcyUwQWZlYXR1cmVzJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQSUwQXByaW50KGYlMjJFeHRyYWN0ZWQlMjBmZWF0dXJlcyUyMHNoYXBlJTNBJTIwJTdCZmVhdHVyZXMuc2hhcGUlN0QlMjIp",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MLCDVisionModel

<span class="hljs-comment"># Load model and processor</span>
model = MLCDVisionModel.from_pretrained(<span class="hljs-string">&quot;DeepGlint-AI/mlcd-vit-bigG-patch14-448&quot;</span>)
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;DeepGlint-AI/mlcd-vit-bigG-patch14-448&quot;</span>)

<span class="hljs-comment"># Process single image</span>
url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Generate outputs</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="hljs-comment"># Get visual features</span>
features = outputs.last_hidden_state

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Extracted features shape: <span class="hljs-subst">{features.shape}</span>&quot;</span>)`,wrap:!1}}),X=new ae({props:{title:"MLCDVisionConfig",local:"transformers.MLCDVisionConfig",headingTag:"h2"}}),k=new Ie({props:{name:"class transformers.MLCDVisionConfig",anchor:"transformers.MLCDVisionConfig",parameters:[{name:"hidden_size",val:" = 1664"},{name:"intermediate_size",val:" = 8192"},{name:"num_hidden_layers",val:" = 48"},{name:"num_attention_heads",val:" = 16"},{name:"num_key_value_groups",val:" = 1"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 336"},{name:"patch_size",val:" = 14"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MLCDVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1664) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.MLCDVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.MLCDVisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.MLCDVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 48) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MLCDVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MLCDVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.MLCDVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 336) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.MLCDVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.MLCDVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.MLCDVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MLCDVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.MLCDVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MLCDVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mlcd/configuration_mlcd.py#L25"}}),D=new qe({props:{anchor:"transformers.MLCDVisionConfig.example",$$slots:{default:[st]},$$scope:{ctx:j}}}),F=new ae({props:{title:"MLCDVisionModel",local:"transformers.MLCDVisionModel",headingTag:"h2"}}),P=new Ie({props:{name:"class transformers.MLCDVisionModel",anchor:"transformers.MLCDVisionModel",parameters:[{name:"config",val:": MLCDVisionConfig"}],parametersDescription:[{anchor:"transformers.MLCDVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionConfig">MLCDVisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mlcd/modeling_mlcd.py#L553"}}),S=new Ie({props:{name:"forward",anchor:"transformers.MLCDVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MLCDVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MLCDVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MLCDVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MLCDVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mlcd/modeling_mlcd.py#L567",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionConfig"
>MLCDVisionConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),x=new tt({props:{$$slots:{default:[ot]},$$scope:{ctx:j}}}),I=new qe({props:{anchor:"transformers.MLCDVisionModel.forward.example",$$slots:{default:[at]},$$scope:{ctx:j}}}),q=new nt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mlcd.md"}}),{c(){o=r("meta"),w=l(),m=r("p"),c=l(),g=r("p"),g.innerHTML=a,$=l(),u(Z.$$.fragment),le=l(),V=r("div"),V.innerHTML=We,ie=l(),u(z.$$.fragment),re=l(),W=r("p"),W.innerHTML=Ne,de=l(),N=r("p"),N.innerHTML=He,ce=l(),H=r("p"),H.textContent=Ge,me=l(),G=r("ul"),G.innerHTML=Be,pe=l(),B=r("p"),B.textContent=Qe,ge=l(),Q=r("table"),Q.innerHTML=Ee,fe=l(),u(E.$$.fragment),he=l(),u(R.$$.fragment),ue=l(),u(X.$$.fragment),Me=l(),T=r("div"),u(k.$$.fragment),Te=l(),A=r("p"),A.innerHTML=Re,ve=l(),Y=r("p"),Y.innerHTML=Xe,$e=l(),u(D.$$.fragment),ye=l(),u(F.$$.fragment),_e=l(),h=r("div"),u(P.$$.fragment),Ue=l(),O=r("p"),O.textContent=ke,Le=l(),K=r("p"),K.innerHTML=Fe,Je=l(),ee=r("p"),ee.innerHTML=Pe,je=l(),U=r("div"),u(S.$$.fragment),Ve=l(),te=r("p"),te.innerHTML=Se,De=l(),u(x.$$.fragment),xe=l(),u(I.$$.fragment),be=l(),u(q.$$.fragment),Ce=l(),se=r("p"),this.h()},l(e){const t=et("svelte-u9bgzb",document.head);o=d(t,"META",{name:!0,content:!0}),t.forEach(n),w=i(e),m=d(e,"P",{}),oe(m).forEach(n),c=i(e),g=d(e,"P",{"data-svelte-h":!0}),f(g)!=="svelte-1j5ppio"&&(g.innerHTML=a),$=i(e),M(Z.$$.fragment,e),le=i(e),V=d(e,"DIV",{class:!0,"data-svelte-h":!0}),f(V)!=="svelte-1yc98sx"&&(V.innerHTML=We),ie=i(e),M(z.$$.fragment,e),re=i(e),W=d(e,"P",{"data-svelte-h":!0}),f(W)!=="svelte-zugy44"&&(W.innerHTML=Ne),de=i(e),N=d(e,"P",{"data-svelte-h":!0}),f(N)!=="svelte-1qvm3aj"&&(N.innerHTML=He),ce=i(e),H=d(e,"P",{"data-svelte-h":!0}),f(H)!=="svelte-axv494"&&(H.textContent=Ge),me=i(e),G=d(e,"UL",{"data-svelte-h":!0}),f(G)!=="svelte-1o381bq"&&(G.innerHTML=Be),pe=i(e),B=d(e,"P",{"data-svelte-h":!0}),f(B)!=="svelte-1mj6tkd"&&(B.textContent=Qe),ge=i(e),Q=d(e,"TABLE",{"data-svelte-h":!0}),f(Q)!=="svelte-1lvh1b"&&(Q.innerHTML=Ee),fe=i(e),M(E.$$.fragment,e),he=i(e),M(R.$$.fragment,e),ue=i(e),M(X.$$.fragment,e),Me=i(e),T=d(e,"DIV",{class:!0});var L=oe(T);M(k.$$.fragment,L),Te=i(L),A=d(L,"P",{"data-svelte-h":!0}),f(A)!=="svelte-1t25egu"&&(A.innerHTML=Re),ve=i(L),Y=d(L,"P",{"data-svelte-h":!0}),f(Y)!=="svelte-1ek1ss9"&&(Y.innerHTML=Xe),$e=i(L),M(D.$$.fragment,L),L.forEach(n),ye=i(e),M(F.$$.fragment,e),_e=i(e),h=d(e,"DIV",{class:!0});var v=oe(h);M(P.$$.fragment,v),Ue=i(v),O=d(v,"P",{"data-svelte-h":!0}),f(O)!=="svelte-l2k1dv"&&(O.textContent=ke),Le=i(v),K=d(v,"P",{"data-svelte-h":!0}),f(K)!=="svelte-q52n56"&&(K.innerHTML=Fe),Je=i(v),ee=d(v,"P",{"data-svelte-h":!0}),f(ee)!=="svelte-hswkmf"&&(ee.innerHTML=Pe),je=i(v),U=d(v,"DIV",{class:!0});var J=oe(U);M(S.$$.fragment,J),Ve=i(J),te=d(J,"P",{"data-svelte-h":!0}),f(te)!=="svelte-xmvzb5"&&(te.innerHTML=Se),De=i(J),M(x.$$.fragment,J),xe=i(J),M(I.$$.fragment,J),J.forEach(n),v.forEach(n),be=i(e),M(q.$$.fragment,e),Ce=i(e),se=d(e,"P",{}),oe(se).forEach(n),this.h()},h(){ne(o,"name","hf:doc:metadata"),ne(o,"content",it),ne(V,"class","flex flex-wrap space-x-1"),ne(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ne(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ne(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,o),s(e,w,t),s(e,m,t),s(e,c,t),s(e,g,t),s(e,$,t),y(Z,e,t),s(e,le,t),s(e,V,t),s(e,ie,t),y(z,e,t),s(e,re,t),s(e,W,t),s(e,de,t),s(e,N,t),s(e,ce,t),s(e,H,t),s(e,me,t),s(e,G,t),s(e,pe,t),s(e,B,t),s(e,ge,t),s(e,Q,t),s(e,fe,t),y(E,e,t),s(e,he,t),y(R,e,t),s(e,ue,t),y(X,e,t),s(e,Me,t),s(e,T,t),y(k,T,null),p(T,Te),p(T,A),p(T,ve),p(T,Y),p(T,$e),y(D,T,null),s(e,ye,t),y(F,e,t),s(e,_e,t),s(e,h,t),y(P,h,null),p(h,Ue),p(h,O),p(h,Le),p(h,K),p(h,Je),p(h,ee),p(h,je),p(h,U),y(S,U,null),p(U,Ve),p(U,te),p(U,De),y(x,U,null),p(U,xe),y(I,U,null),s(e,be,t),y(q,e,t),s(e,Ce,t),s(e,se,t),we=!0},p(e,[t]){const L={};t&2&&(L.$$scope={dirty:t,ctx:e}),D.$set(L);const v={};t&2&&(v.$$scope={dirty:t,ctx:e}),x.$set(v);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),I.$set(J)},i(e){we||(_(Z.$$.fragment,e),_(z.$$.fragment,e),_(E.$$.fragment,e),_(R.$$.fragment,e),_(X.$$.fragment,e),_(k.$$.fragment,e),_(D.$$.fragment,e),_(F.$$.fragment,e),_(P.$$.fragment,e),_(S.$$.fragment,e),_(x.$$.fragment,e),_(I.$$.fragment,e),_(q.$$.fragment,e),we=!0)},o(e){b(Z.$$.fragment,e),b(z.$$.fragment,e),b(E.$$.fragment,e),b(R.$$.fragment,e),b(X.$$.fragment,e),b(k.$$.fragment,e),b(D.$$.fragment,e),b(F.$$.fragment,e),b(P.$$.fragment,e),b(S.$$.fragment,e),b(x.$$.fragment,e),b(I.$$.fragment,e),b(q.$$.fragment,e),we=!1},d(e){e&&(n(w),n(m),n(c),n(g),n($),n(le),n(V),n(ie),n(re),n(W),n(de),n(N),n(ce),n(H),n(me),n(G),n(pe),n(B),n(ge),n(Q),n(fe),n(he),n(ue),n(Me),n(T),n(ye),n(_e),n(h),n(be),n(Ce),n(se)),n(o),C(Z,e),C(z,e),C(E,e),C(R,e),C(X,e),C(k),C(D),C(F,e),C(P),C(S),C(x),C(I),C(q,e)}}}const it='{"title":"MLCD","local":"mlcd","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage","local":"usage","sections":[],"depth":2},{"title":"MLCDVisionConfig","local":"transformers.MLCDVisionConfig","sections":[],"depth":2},{"title":"MLCDVisionModel","local":"transformers.MLCDVisionModel","sections":[],"depth":2}],"depth":1}';function rt(j){return Ye(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ut extends Oe{constructor(o){super(),Ke(this,o,rt,lt,Ae,{})}}export{ut as component};
