import{s as ee,o as te,n as le}from"../chunks/scheduler.18a86fab.js";import{S as ae,i as ne,g as i,s,r as _,A as se,h as m,f as l,c as o,j as D,u as U,x as G,k as P,y as oe,a,v as F,d as j,t as Z,w as C}from"../chunks/index.98837b22.js";import{T as re}from"../chunks/Tip.77304350.js";import{C as O}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ie,E as me}from"../chunks/getInferenceSnippets.06c2775f.js";function fe(J){let n,c='Models that support GGUF include Llama, Mistral, Qwen2, Qwen2Moe, Phi3, Bloom, Falcon, StableLM, GPT2, Starcoder2, and <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/ggml.py" rel="nofollow">more</a>';return{c(){n=i("p"),n.innerHTML=c},l(r){n=m(r,"P",{"data-svelte-h":!0}),G(n)!=="svelte-6rmteu"&&(n.innerHTML=c)},m(r,$){a(r,n,$)},p:le,d(r){r&&l(n)}}}function pe(J){let n,c,r,$,d,L,u,Y='<a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md" rel="nofollow">GGUF</a> is a file format used to store models for inference with <a href="https://github.com/ggerganov/ggml" rel="nofollow">GGML</a>, a fast and lightweight inference framework written in C and C++. GGUF is a single-file format containing the model metadata and tensors.',k,f,X='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png"/>',H,h,q='The GGUF format also supports many quantized data types (refer to <a href="https://hf.co/docs/hub/en/gguf#quantization-types" rel="nofollow">quantization type table</a> for a complete list of supported quantization types) which saves a significant amount of memory, making inference with large models like Whisper and Llama feasible on local and edge devices.',E,g,S="Transformers supports loading models stored in the GGUF format for further training or finetuning. The GGUF checkpoint is <strong>dequantized to fp32</strong> where the full model weights are available and compatible with PyTorch.",V,p,W,M,A='Add the <code>gguf_file</code> parameter to <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> to specify the GGUF file to load.',x,b,R,y,N='Once youâ€™re done tinkering with the model, save and convert it back to the GGUF format with the <a href="https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py" rel="nofollow">convert-hf-to-gguf.py</a> script.',Q,v,B,w,z,T,I;return d=new ie({props:{title:"GGUF",local:"gguf",headingTag:"h1"}}),p=new re({props:{warning:!1,$$slots:{default:[fe]},$$scope:{ctx:J}}}),b=new O({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMGdndWYlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvVG9rZW5pemVyJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMlRoZUJsb2tlJTJGVGlueUxsYW1hLTEuMUItQ2hhdC12MS4wLUdHVUYlMjIlMEFmaWxlbmFtZSUyMCUzRCUyMCUyMnRpbnlsbGFtYS0xLjFiLWNoYXQtdjEuMC5RNl9LLmdndWYlMjIlMEElMEFkdHlwZSUyMCUzRCUyMHRvcmNoLmZsb2F0MzIlMjAlMjMlMjBjb3VsZCUyMGJlJTIwdG9yY2guZmxvYXQxNiUyMG9yJTIwdG9yY2guYmZsb2F0MTYlMjB0b28lMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMGdndWZfZmlsZSUzRGZpbGVuYW1lKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZ2d1Zl9maWxlJTNEZmlsZW5hbWUlMkMlMjBkdHlwZSUzRGR0eXBlKQ==",highlighted:`<span class="hljs-comment"># pip install gguf</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

model_id = <span class="hljs-string">&quot;TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF&quot;</span>
filename = <span class="hljs-string">&quot;tinyllama-1.1b-chat-v1.0.Q6_K.gguf&quot;</span>

dtype = torch.float32 <span class="hljs-comment"># could be torch.float16 or torch.bfloat16 too</span>
tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, dtype=dtype)`,wrap:!1}}),v=new O({props:{code:"dG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZCglMjJkaXJlY3RvcnklMjIpJTBBbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMmRpcmVjdG9yeSUyMiklMEElMEEhcHl0aG9uJTIwJTI0JTdCcGF0aF90b19sbGFtYV9jcHAlN0QlMkZjb252ZXJ0LWhmLXRvLWdndWYucHklMjAlMjQlN0JkaXJlY3RvcnklN0Q=",highlighted:`tokenizer.save_pretrained(<span class="hljs-string">&quot;directory&quot;</span>)
model.save_pretrained(<span class="hljs-string">&quot;directory&quot;</span>)

!python \${path_to_llama_cpp}/convert-hf-to-gguf.py \${directory}`,wrap:!1}}),w=new me({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/gguf.md"}}),{c(){n=i("meta"),c=s(),r=i("p"),$=s(),_(d.$$.fragment),L=s(),u=i("p"),u.innerHTML=Y,k=s(),f=i("div"),f.innerHTML=X,H=s(),h=i("p"),h.innerHTML=q,E=s(),g=i("p"),g.innerHTML=S,V=s(),_(p.$$.fragment),W=s(),M=i("p"),M.innerHTML=A,x=s(),_(b.$$.fragment),R=s(),y=i("p"),y.innerHTML=N,Q=s(),_(v.$$.fragment),B=s(),_(w.$$.fragment),z=s(),T=i("p"),this.h()},l(e){const t=se("svelte-u9bgzb",document.head);n=m(t,"META",{name:!0,content:!0}),t.forEach(l),c=o(e),r=m(e,"P",{}),D(r).forEach(l),$=o(e),U(d.$$.fragment,e),L=o(e),u=m(e,"P",{"data-svelte-h":!0}),G(u)!=="svelte-1j8wxcx"&&(u.innerHTML=Y),k=o(e),f=m(e,"DIV",{class:!0,"data-svelte-h":!0}),G(f)!=="svelte-1kvm71c"&&(f.innerHTML=X),H=o(e),h=m(e,"P",{"data-svelte-h":!0}),G(h)!=="svelte-ddcg59"&&(h.innerHTML=q),E=o(e),g=m(e,"P",{"data-svelte-h":!0}),G(g)!=="svelte-1xvohjx"&&(g.innerHTML=S),V=o(e),U(p.$$.fragment,e),W=o(e),M=m(e,"P",{"data-svelte-h":!0}),G(M)!=="svelte-68o9a4"&&(M.innerHTML=A),x=o(e),U(b.$$.fragment,e),R=o(e),y=m(e,"P",{"data-svelte-h":!0}),G(y)!=="svelte-hgylw2"&&(y.innerHTML=N),Q=o(e),U(v.$$.fragment,e),B=o(e),U(w.$$.fragment,e),z=o(e),T=m(e,"P",{}),D(T).forEach(l),this.h()},h(){P(n,"name","hf:doc:metadata"),P(n,"content",ce),P(f,"class","flex justify-center")},m(e,t){oe(document.head,n),a(e,c,t),a(e,r,t),a(e,$,t),F(d,e,t),a(e,L,t),a(e,u,t),a(e,k,t),a(e,f,t),a(e,H,t),a(e,h,t),a(e,E,t),a(e,g,t),a(e,V,t),F(p,e,t),a(e,W,t),a(e,M,t),a(e,x,t),F(b,e,t),a(e,R,t),a(e,y,t),a(e,Q,t),F(v,e,t),a(e,B,t),F(w,e,t),a(e,z,t),a(e,T,t),I=!0},p(e,[t]){const K={};t&2&&(K.$$scope={dirty:t,ctx:e}),p.$set(K)},i(e){I||(j(d.$$.fragment,e),j(p.$$.fragment,e),j(b.$$.fragment,e),j(v.$$.fragment,e),j(w.$$.fragment,e),I=!0)},o(e){Z(d.$$.fragment,e),Z(p.$$.fragment,e),Z(b.$$.fragment,e),Z(v.$$.fragment,e),Z(w.$$.fragment,e),I=!1},d(e){e&&(l(c),l(r),l($),l(L),l(u),l(k),l(f),l(H),l(h),l(E),l(g),l(V),l(W),l(M),l(x),l(R),l(y),l(Q),l(B),l(z),l(T)),l(n),C(d,e),C(p,e),C(b,e),C(v,e),C(w,e)}}}const ce='{"title":"GGUF","local":"gguf","sections":[],"depth":1}';function de(J){return te(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ye extends ae{constructor(n){super(),ne(this,n,de,pe,ee,{})}}export{ye as component};
