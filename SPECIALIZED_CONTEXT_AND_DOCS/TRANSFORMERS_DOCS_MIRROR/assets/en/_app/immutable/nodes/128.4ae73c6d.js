import{s as Et,o as Nt,n as Qe}from"../chunks/scheduler.18a86fab.js";import{S as Dt,i as Rt,g as i,s,r as m,A as Gt,h as d,f as n,c as a,j as J,x as p,u,k as P,y as o,a as l,v as h,d as f,t as g,w as _}from"../chunks/index.98837b22.js";import{T as Wt}from"../chunks/Tip.77304350.js";import{D as R}from"../chunks/Docstring.a1ef7999.js";import{C as Ut}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ot}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as me,E as Zt}from"../chunks/getInferenceSnippets.06c2775f.js";function Qt(L){let r,v="Example:",c,b,w;return b=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENwbUFudE1vZGVsJTJDJTIwQ3BtQW50Q29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENQTUFudCUyMGNwbS1hbnQtMTBiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMENwbUFudENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMGNwbS1hbnQtMTBiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDcG1BbnRNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CpmAntModel, CpmAntConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CPMAnt cpm-ant-10b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CpmAntConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the cpm-ant-10b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CpmAntModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=i("p"),r.textContent=v,c=s(),m(b.$$.fragment)},l(C){r=d(C,"P",{"data-svelte-h":!0}),p(r)!=="svelte-11lpom8"&&(r.textContent=v),c=a(C),u(b.$$.fragment,C)},m(C,H){l(C,r,H),l(C,c,H),h(b,C,H),w=!0},p:Qe,i(C){w||(f(b.$$.fragment,C),w=!0)},o(C){g(b.$$.fragment,C),w=!1},d(C){C&&(n(r),n(c)),_(b,C)}}}function St(L){let r,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=v},l(c){r=d(c,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=v)},m(c,b){l(c,r,b)},p:Qe,d(c){c&&n(r)}}}function Vt(L){let r,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=v},l(c){r=d(c,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=v)},m(c,b){l(c,r,b)},p:Qe,d(c){c&&n(r)}}}function Xt(L){let r,v;return r=new Ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENQTUFudFRva2VuaXplciUyQyUyMENwbUFudEZvckNhdXNhbExNJTBBJTBBdGV4dHMlMjAlM0QlMjAlMjIlRTQlQkIlOEElRTUlQTQlQTklRTUlQTQlQTklRTYlQjAlOTQlRTQlQjglOEQlRTklOTQlOTklRUYlQkMlOEMlMjIlMEFtb2RlbCUyMCUzRCUyMENwbUFudEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYm1iJTJGY3BtLWFudC0xMGIlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQ1BNQW50VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYm1iJTJGY3BtLWFudC0xMGIlMjIpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHRzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMpJTBBb3V0cHV0X3RleHRzJTIwJTNEJTIwdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzKSUwQXByaW50KG91dHB1dF90ZXh0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CPMAntTokenizer, CpmAntForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>texts = <span class="hljs-string">&quot;今天天气不错，&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CpmAntForCausalLM.from_pretrained(<span class="hljs-string">&quot;openbmb/cpm-ant-10b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = CPMAntTokenizer.from_pretrained(<span class="hljs-string">&quot;openbmb/cpm-ant-10b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(**input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>output_texts = tokenizer.batch_decode(outputs)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output_texts)
[<span class="hljs-string">&#x27;今天天气不错，阳光明媚，我和妈妈一起去超市买东西。\\n在超市里，我看到了一个很好玩的玩具，它的名字叫“机器人”。它有一个圆圆的脑袋，两只圆圆的眼睛，还有一个圆圆的&#x27;</span>]`,wrap:!1}}),{c(){m(r.$$.fragment)},l(c){u(r.$$.fragment,c)},m(c,b){h(r,c,b),v=!0},p:Qe,i(c){v||(f(r.$$.fragment,c),v=!0)},o(c){g(r.$$.fragment,c),v=!1},d(c){_(r,c)}}}function Yt(L){let r,v,c,b,w,C="<em>This model was released on 2022-09-16 and added to Hugging Face Transformers on 2023-04-12.</em>",H,G,Le,W,bt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',qe,Z,Ie,Q,Ct='CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. It is also the first milestone of the live training process of CPM-Live. The training process is cost-effective and environment-friendly. CPM-Ant also achieves promising results with delta tuning on the CUGE benchmark. Besides the full model, we also provide various compressed versions to meet the requirements of different hardware configurations. <a href="https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live" rel="nofollow">See more</a>',Fe,S,Tt='This model was contributed by <a href="https://huggingface.co/openbmb" rel="nofollow">OpenBMB</a>. The original code can be found <a href="https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live" rel="nofollow">here</a>.',Be,V,Je,X,kt='<li>A tutorial on <a href="https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live" rel="nofollow">CPM-Live</a>.</li>',je,Y,He,$,K,Se,ue,yt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a>. It is used to instantiate an
CPMAnt model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CPMAnt
<a href="https://huggingface.co/openbmb/cpm-ant-10b" rel="nofollow">openbmb/cpm-ant-10b</a> architecture.`,Ve,he,$t=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Xe,O,We,ee,Oe,M,te,Ye,fe,Mt="Construct a CPMAnt tokenizer. Based on byte-level Byte-Pair-Encoding.",Ke,q,ne,et,ge,wt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A CPMAnt sequence has the following format:`,tt,_e,xt="<li>single sequence: <code>[BOS] Sequence</code>.</li>",nt,U,oe,ot,ve,At=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Ue,se,Ee,k,ae,st,be,zt="The bare Cpmant Model outputting raw hidden-states without any specific head on top.",at,Ce,Pt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,rt,Te,Lt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,it,I,re,dt,ke,qt='The <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> forward method, overrides the <code>__call__</code> special method.',lt,E,Ne,ie,De,y,de,ct,ye,It="The CPMAnt Model with a language modeling head on top (linear layer with weights tied to the input embeddings).",pt,$e,Ft=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mt,Me,Bt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ut,T,le,ht,we,Jt='The <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',ft,N,gt,xe,jt="Example:",_t,Ae,Ht="Text Generation with CpmAntForCausalLM.",vt,D,Re,ce,Ge,Pe,Ze;return G=new me({props:{title:"CPMAnt",local:"cpmant",headingTag:"h1"}}),Z=new me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),V=new me({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Y=new me({props:{title:"CpmAntConfig",local:"transformers.CpmAntConfig",headingTag:"h2"}}),K=new R({props:{name:"class transformers.CpmAntConfig",anchor:"transformers.CpmAntConfig",parameters:[{name:"vocab_size",val:": int = 30720"},{name:"hidden_size",val:": int = 4096"},{name:"num_attention_heads",val:": int = 32"},{name:"dim_head",val:": int = 128"},{name:"dim_ff",val:": int = 10240"},{name:"num_hidden_layers",val:": int = 48"},{name:"dropout_p",val:": int = 0.0"},{name:"position_bias_num_buckets",val:": int = 512"},{name:"position_bias_max_distance",val:": int = 2048"},{name:"eps",val:": int = 1e-06"},{name:"init_std",val:": float = 1.0"},{name:"prompt_types",val:": int = 32"},{name:"prompt_length",val:": int = 32"},{name:"segment_types",val:": int = 32"},{name:"use_cache",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CpmAntConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30720) &#x2014;
Vocabulary size of the CPMAnt model. Defines the number of different tokens that can be represented by the
<code>input</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a>.`,name:"vocab_size"},{anchor:"transformers.CpmAntConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the encoder layers.`,name:"hidden_size"},{anchor:"transformers.CpmAntConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CpmAntConfig.dim_head",description:`<strong>dim_head</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of attention heads for each attention layer in the Transformer encoder.`,name:"dim_head"},{anchor:"transformers.CpmAntConfig.dim_ff",description:`<strong>dim_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 10240) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"dim_ff"},{anchor:"transformers.CpmAntConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 48) &#x2014;
Number of layers of the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CpmAntConfig.dropout_p",description:`<strong>dropout_p</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder.`,name:"dropout_p"},{anchor:"transformers.CpmAntConfig.position_bias_num_buckets",description:`<strong>position_bias_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The number of position_bias buckets.`,name:"position_bias_num_buckets"},{anchor:"transformers.CpmAntConfig.position_bias_max_distance",description:`<strong>position_bias_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"position_bias_max_distance"},{anchor:"transformers.CpmAntConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"eps"},{anchor:"transformers.CpmAntConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Initialize parameters with std = init_std.`,name:"init_std"},{anchor:"transformers.CpmAntConfig.prompt_types",description:`<strong>prompt_types</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The type of prompt.`,name:"prompt_types"},{anchor:"transformers.CpmAntConfig.prompt_length",description:`<strong>prompt_length</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The length of prompt.`,name:"prompt_length"},{anchor:"transformers.CpmAntConfig.segment_types",description:`<strong>segment_types</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The type of segment.`,name:"segment_types"},{anchor:"transformers.CpmAntConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use cache.`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/configuration_cpmant.py#L24"}}),O=new Ot({props:{anchor:"transformers.CpmAntConfig.example",$$slots:{default:[Qt]},$$scope:{ctx:L}}}),ee=new me({props:{title:"CpmAntTokenizer",local:"transformers.CpmAntTokenizer",headingTag:"h2"}}),te=new R({props:{name:"class transformers.CpmAntTokenizer",anchor:"transformers.CpmAntTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bod_token",val:" = '<d>'"},{name:"eod_token",val:" = '</d>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"unk_token",val:" = '<unk>'"},{name:"line_token",val:" = '</n>'"},{name:"space_token",val:" = '</_>'"},{name:"padding_side",val:" = 'left'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CpmAntTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.CpmAntTokenizer.bod_token",description:`<strong>bod_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;d&gt;&quot;</code>) &#x2014;
The beginning of document token.`,name:"bod_token"},{anchor:"transformers.CpmAntTokenizer.eod_token",description:`<strong>eod_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/d&gt;&quot;</code>) &#x2014;
The end of document token.`,name:"eod_token"},{anchor:"transformers.CpmAntTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.CpmAntTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.CpmAntTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding.`,name:"pad_token"},{anchor:"transformers.CpmAntTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token.`,name:"unk_token"},{anchor:"transformers.CpmAntTokenizer.line_token",description:`<strong>line_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/n&gt;&quot;</code>) &#x2014;
The line token.`,name:"line_token"},{anchor:"transformers.CpmAntTokenizer.space_token",description:`<strong>space_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/_&gt;&quot;</code>) &#x2014;
The space token.`,name:"space_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/tokenization_cpmant.py#L79"}}),ne=new R({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CpmAntTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.CpmAntTokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014; The first tokenized sequence that special tokens will be added.",name:"token_ids_0"},{anchor:"transformers.CpmAntTokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>list[int]</code>) &#x2014; The optional second tokenized sequence that special tokens will be added.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/tokenization_cpmant.py#L225",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The model input with special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),oe=new R({props:{name:"get_special_tokens_mask",anchor:"transformers.CpmAntTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.CpmAntTokenizer.get_special_tokens_mask.token_ids_0",description:"<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014; List of IDs.",name:"token_ids_0"},{anchor:"transformers.CpmAntTokenizer.get_special_tokens_mask.token_ids_1",description:"<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014; Optional second list of IDs for sequence pairs.",name:"token_ids_1"},{anchor:"transformers.CpmAntTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/tokenization_cpmant.py#L245",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),se=new me({props:{title:"CpmAntModel",local:"transformers.CpmAntModel",headingTag:"h2"}}),ae=new R({props:{name:"class transformers.CpmAntModel",anchor:"transformers.CpmAntModel",parameters:[{name:"config",val:": CpmAntConfig"}],parametersDescription:[{anchor:"transformers.CpmAntModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/modeling_cpmant.py#L549"}}),re=new R({props:{name:"forward",anchor:"transformers.CpmAntModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": typing.Optional[tuple[tuple[torch.Tensor]]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CpmAntModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>CPMAntTokenizer</code>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CpmAntModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CpmAntModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CpmAntModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[tuple[torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.CpmAntModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.CpmAntModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CpmAntModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/modeling_cpmant.py#L587",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig"
>CpmAntConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new Wt({props:{$$slots:{default:[St]},$$scope:{ctx:L}}}),ie=new me({props:{title:"CpmAntForCausalLM",local:"transformers.CpmAntForCausalLM",headingTag:"h2"}}),de=new R({props:{name:"class transformers.CpmAntForCausalLM",anchor:"transformers.CpmAntForCausalLM",parameters:[{name:"config",val:": CpmAntConfig"}],parametersDescription:[{anchor:"transformers.CpmAntForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/modeling_cpmant.py#L708"}}),le=new R({props:{name:"forward",anchor:"transformers.CpmAntForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[list[tuple[torch.Tensor, torch.Tensor]]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CpmAntForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>CPMAntTokenizer</code>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CpmAntForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>list[tuple[torch.Tensor, torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.CpmAntForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.CpmAntForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CpmAntForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CpmAntForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss.`,name:"labels"},{anchor:"transformers.CpmAntForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CpmAntForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CpmAntForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/cpmant/modeling_cpmant.py#L721",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig"
>CpmAntConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),N=new Wt({props:{$$slots:{default:[Vt]},$$scope:{ctx:L}}}),D=new Ot({props:{anchor:"transformers.CpmAntForCausalLM.forward.example",$$slots:{default:[Xt]},$$scope:{ctx:L}}}),ce=new Zt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cpmant.md"}}),{c(){r=i("meta"),v=s(),c=i("p"),b=s(),w=i("p"),w.innerHTML=C,H=s(),m(G.$$.fragment),Le=s(),W=i("div"),W.innerHTML=bt,qe=s(),m(Z.$$.fragment),Ie=s(),Q=i("p"),Q.innerHTML=Ct,Fe=s(),S=i("p"),S.innerHTML=Tt,Be=s(),m(V.$$.fragment),Je=s(),X=i("ul"),X.innerHTML=kt,je=s(),m(Y.$$.fragment),He=s(),$=i("div"),m(K.$$.fragment),Se=s(),ue=i("p"),ue.innerHTML=yt,Ve=s(),he=i("p"),he.innerHTML=$t,Xe=s(),m(O.$$.fragment),We=s(),m(ee.$$.fragment),Oe=s(),M=i("div"),m(te.$$.fragment),Ye=s(),fe=i("p"),fe.textContent=Mt,Ke=s(),q=i("div"),m(ne.$$.fragment),et=s(),ge=i("p"),ge.textContent=wt,tt=s(),_e=i("ul"),_e.innerHTML=xt,nt=s(),U=i("div"),m(oe.$$.fragment),ot=s(),ve=i("p"),ve.innerHTML=At,Ue=s(),m(se.$$.fragment),Ee=s(),k=i("div"),m(ae.$$.fragment),st=s(),be=i("p"),be.textContent=zt,at=s(),Ce=i("p"),Ce.innerHTML=Pt,rt=s(),Te=i("p"),Te.innerHTML=Lt,it=s(),I=i("div"),m(re.$$.fragment),dt=s(),ke=i("p"),ke.innerHTML=qt,lt=s(),m(E.$$.fragment),Ne=s(),m(ie.$$.fragment),De=s(),y=i("div"),m(de.$$.fragment),ct=s(),ye=i("p"),ye.textContent=It,pt=s(),$e=i("p"),$e.innerHTML=Ft,mt=s(),Me=i("p"),Me.innerHTML=Bt,ut=s(),T=i("div"),m(le.$$.fragment),ht=s(),we=i("p"),we.innerHTML=Jt,ft=s(),m(N.$$.fragment),gt=s(),xe=i("p"),xe.textContent=jt,_t=s(),Ae=i("p"),Ae.textContent=Ht,vt=s(),m(D.$$.fragment),Re=s(),m(ce.$$.fragment),Ge=s(),Pe=i("p"),this.h()},l(e){const t=Gt("svelte-u9bgzb",document.head);r=d(t,"META",{name:!0,content:!0}),t.forEach(n),v=a(e),c=d(e,"P",{}),J(c).forEach(n),b=a(e),w=d(e,"P",{"data-svelte-h":!0}),p(w)!=="svelte-1cllry0"&&(w.innerHTML=C),H=a(e),u(G.$$.fragment,e),Le=a(e),W=d(e,"DIV",{class:!0,"data-svelte-h":!0}),p(W)!=="svelte-13t8s2t"&&(W.innerHTML=bt),qe=a(e),u(Z.$$.fragment,e),Ie=a(e),Q=d(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-s2n23o"&&(Q.innerHTML=Ct),Fe=a(e),S=d(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-5o33hr"&&(S.innerHTML=Tt),Be=a(e),u(V.$$.fragment,e),Je=a(e),X=d(e,"UL",{"data-svelte-h":!0}),p(X)!=="svelte-1b6hj70"&&(X.innerHTML=kt),je=a(e),u(Y.$$.fragment,e),He=a(e),$=d(e,"DIV",{class:!0});var A=J($);u(K.$$.fragment,A),Se=a(A),ue=d(A,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-182bhgi"&&(ue.innerHTML=yt),Ve=a(A),he=d(A,"P",{"data-svelte-h":!0}),p(he)!=="svelte-1ek1ss9"&&(he.innerHTML=$t),Xe=a(A),u(O.$$.fragment,A),A.forEach(n),We=a(e),u(ee.$$.fragment,e),Oe=a(e),M=d(e,"DIV",{class:!0});var z=J(M);u(te.$$.fragment,z),Ye=a(z),fe=d(z,"P",{"data-svelte-h":!0}),p(fe)!=="svelte-16scib2"&&(fe.textContent=Mt),Ke=a(z),q=d(z,"DIV",{class:!0});var j=J(q);u(ne.$$.fragment,j),et=a(j),ge=d(j,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-1noycua"&&(ge.textContent=wt),tt=a(j),_e=d(j,"UL",{"data-svelte-h":!0}),p(_e)!=="svelte-9q72go"&&(_e.innerHTML=xt),j.forEach(n),nt=a(z),U=d(z,"DIV",{class:!0});var pe=J(U);u(oe.$$.fragment,pe),ot=a(pe),ve=d(pe,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1f4f5kp"&&(ve.innerHTML=At),pe.forEach(n),z.forEach(n),Ue=a(e),u(se.$$.fragment,e),Ee=a(e),k=d(e,"DIV",{class:!0});var F=J(k);u(ae.$$.fragment,F),st=a(F),be=d(F,"P",{"data-svelte-h":!0}),p(be)!=="svelte-oq06uz"&&(be.textContent=zt),at=a(F),Ce=d(F,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-q52n56"&&(Ce.innerHTML=Pt),rt=a(F),Te=d(F,"P",{"data-svelte-h":!0}),p(Te)!=="svelte-hswkmf"&&(Te.innerHTML=Lt),it=a(F),I=d(F,"DIV",{class:!0});var ze=J(I);u(re.$$.fragment,ze),dt=a(ze),ke=d(ze,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-1t69pq8"&&(ke.innerHTML=qt),lt=a(ze),u(E.$$.fragment,ze),ze.forEach(n),F.forEach(n),Ne=a(e),u(ie.$$.fragment,e),De=a(e),y=d(e,"DIV",{class:!0});var B=J(y);u(de.$$.fragment,B),ct=a(B),ye=d(B,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-12po86d"&&(ye.textContent=It),pt=a(B),$e=d(B,"P",{"data-svelte-h":!0}),p($e)!=="svelte-q52n56"&&($e.innerHTML=Ft),mt=a(B),Me=d(B,"P",{"data-svelte-h":!0}),p(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Bt),ut=a(B),T=d(B,"DIV",{class:!0});var x=J(T);u(le.$$.fragment,x),ht=a(x),we=d(x,"P",{"data-svelte-h":!0}),p(we)!=="svelte-1ysfnqw"&&(we.innerHTML=Jt),ft=a(x),u(N.$$.fragment,x),gt=a(x),xe=d(x,"P",{"data-svelte-h":!0}),p(xe)!=="svelte-11lpom8"&&(xe.textContent=jt),_t=a(x),Ae=d(x,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-ri92c5"&&(Ae.textContent=Ht),vt=a(x),u(D.$$.fragment,x),x.forEach(n),B.forEach(n),Re=a(e),u(ce.$$.fragment,e),Ge=a(e),Pe=d(e,"P",{}),J(Pe).forEach(n),this.h()},h(){P(r,"name","hf:doc:metadata"),P(r,"content",Kt),P(W,"class","flex flex-wrap space-x-1"),P($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){o(document.head,r),l(e,v,t),l(e,c,t),l(e,b,t),l(e,w,t),l(e,H,t),h(G,e,t),l(e,Le,t),l(e,W,t),l(e,qe,t),h(Z,e,t),l(e,Ie,t),l(e,Q,t),l(e,Fe,t),l(e,S,t),l(e,Be,t),h(V,e,t),l(e,Je,t),l(e,X,t),l(e,je,t),h(Y,e,t),l(e,He,t),l(e,$,t),h(K,$,null),o($,Se),o($,ue),o($,Ve),o($,he),o($,Xe),h(O,$,null),l(e,We,t),h(ee,e,t),l(e,Oe,t),l(e,M,t),h(te,M,null),o(M,Ye),o(M,fe),o(M,Ke),o(M,q),h(ne,q,null),o(q,et),o(q,ge),o(q,tt),o(q,_e),o(M,nt),o(M,U),h(oe,U,null),o(U,ot),o(U,ve),l(e,Ue,t),h(se,e,t),l(e,Ee,t),l(e,k,t),h(ae,k,null),o(k,st),o(k,be),o(k,at),o(k,Ce),o(k,rt),o(k,Te),o(k,it),o(k,I),h(re,I,null),o(I,dt),o(I,ke),o(I,lt),h(E,I,null),l(e,Ne,t),h(ie,e,t),l(e,De,t),l(e,y,t),h(de,y,null),o(y,ct),o(y,ye),o(y,pt),o(y,$e),o(y,mt),o(y,Me),o(y,ut),o(y,T),h(le,T,null),o(T,ht),o(T,we),o(T,ft),h(N,T,null),o(T,gt),o(T,xe),o(T,_t),o(T,Ae),o(T,vt),h(D,T,null),l(e,Re,t),h(ce,e,t),l(e,Ge,t),l(e,Pe,t),Ze=!0},p(e,[t]){const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),O.$set(A);const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),E.$set(z);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),N.$set(j);const pe={};t&2&&(pe.$$scope={dirty:t,ctx:e}),D.$set(pe)},i(e){Ze||(f(G.$$.fragment,e),f(Z.$$.fragment,e),f(V.$$.fragment,e),f(Y.$$.fragment,e),f(K.$$.fragment,e),f(O.$$.fragment,e),f(ee.$$.fragment,e),f(te.$$.fragment,e),f(ne.$$.fragment,e),f(oe.$$.fragment,e),f(se.$$.fragment,e),f(ae.$$.fragment,e),f(re.$$.fragment,e),f(E.$$.fragment,e),f(ie.$$.fragment,e),f(de.$$.fragment,e),f(le.$$.fragment,e),f(N.$$.fragment,e),f(D.$$.fragment,e),f(ce.$$.fragment,e),Ze=!0)},o(e){g(G.$$.fragment,e),g(Z.$$.fragment,e),g(V.$$.fragment,e),g(Y.$$.fragment,e),g(K.$$.fragment,e),g(O.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(ne.$$.fragment,e),g(oe.$$.fragment,e),g(se.$$.fragment,e),g(ae.$$.fragment,e),g(re.$$.fragment,e),g(E.$$.fragment,e),g(ie.$$.fragment,e),g(de.$$.fragment,e),g(le.$$.fragment,e),g(N.$$.fragment,e),g(D.$$.fragment,e),g(ce.$$.fragment,e),Ze=!1},d(e){e&&(n(v),n(c),n(b),n(w),n(H),n(Le),n(W),n(qe),n(Ie),n(Q),n(Fe),n(S),n(Be),n(Je),n(X),n(je),n(He),n($),n(We),n(Oe),n(M),n(Ue),n(Ee),n(k),n(Ne),n(De),n(y),n(Re),n(Ge),n(Pe)),n(r),_(G,e),_(Z,e),_(V,e),_(Y,e),_(K),_(O),_(ee,e),_(te),_(ne),_(oe),_(se,e),_(ae),_(re),_(E),_(ie,e),_(de),_(le),_(N),_(D),_(ce,e)}}}const Kt='{"title":"CPMAnt","local":"cpmant","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CpmAntConfig","local":"transformers.CpmAntConfig","sections":[],"depth":2},{"title":"CpmAntTokenizer","local":"transformers.CpmAntTokenizer","sections":[],"depth":2},{"title":"CpmAntModel","local":"transformers.CpmAntModel","sections":[],"depth":2},{"title":"CpmAntForCausalLM","local":"transformers.CpmAntForCausalLM","sections":[],"depth":2}],"depth":1}';function en(L){return Nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ln extends Dt{constructor(r){super(),Rt(this,r,en,Yt,Et,{})}}export{ln as component};
