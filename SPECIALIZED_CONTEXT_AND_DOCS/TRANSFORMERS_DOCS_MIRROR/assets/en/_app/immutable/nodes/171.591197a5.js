import{s as Nt,o as It,n as pt}from"../chunks/scheduler.18a86fab.js";import{S as Ft,i as Rt,g as d,s as n,r as u,A as Vt,h as c,f as o,c as a,j as U,x as m,u as g,k as W,y as s,a as r,v as f,d as _,t as b,w as v}from"../chunks/index.98837b22.js";import{T as qt}from"../chunks/Tip.77304350.js";import{D as ce}from"../chunks/Docstring.a1ef7999.js";import{C as ht}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Zt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ee,E as Xt}from"../chunks/getInferenceSnippets.06c2775f.js";function Gt(Z){let l,T="Example:",h,p,y;return p=new ht({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEVuY29kZWNNb2RlbCUyQyUyMEVuY29kZWNDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwJTIyZmFjZWJvb2slMkZlbmNvZGVjXzI0a2h6JTIyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEVuY29kZWNDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwJTIyZmFjZWJvb2slMkZlbmNvZGVjXzI0a2h6JTIyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBFbmNvZGVjTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncodecModel, EncodecConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a &quot;facebook/encodec_24khz&quot; style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = EncodecConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the &quot;facebook/encodec_24khz&quot; style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncodecModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){l=d("p"),l.textContent=T,h=n(),u(p.$$.fragment)},l(i){l=c(i,"P",{"data-svelte-h":!0}),m(l)!=="svelte-11lpom8"&&(l.textContent=T),h=a(i),g(p.$$.fragment,i)},m(i,$){r(i,l,$),r(i,h,$),f(p,i,$),y=!0},p:pt,i(i){y||(_(p.$$.fragment,i),y=!0)},o(i){b(p.$$.fragment,i),y=!1},d(i){i&&(o(l),o(h)),v(p,i)}}}function Ht(Z){let l,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){l=d("p"),l.innerHTML=T},l(h){l=c(h,"P",{"data-svelte-h":!0}),m(l)!=="svelte-fincs2"&&(l.innerHTML=T)},m(h,p){r(h,l,p)},p:pt,d(h){h&&o(l)}}}function Bt(Z){let l,T="Examples:",h,p,y;return p=new ht({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBFbmNvZGVjTW9kZWwlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmhmLWludGVybmFsLXRlc3RpbmclMkZhc2hyYXEtZXNjNTAtMS1kb2ctZXhhbXBsZSUyMiklMEFhdWRpb19zYW1wbGUlMjAlM0QlMjBkYXRhc2V0JTVCJTIydHJhaW4lMjIlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QjAlNUQlNUIlMjJhcnJheSUyMiU1RCUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyZmFjZWJvb2slMkZlbmNvZGVjXzI0a2h6JTIyJTBBbW9kZWwlMjAlM0QlMjBFbmNvZGVjTW9kZWwuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihyYXdfYXVkaW8lM0RhdWRpb19zYW1wbGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFhdWRpb19jb2RlcyUyMCUzRCUyMG91dHB1dHMuYXVkaW9fY29kZXMlMEFhdWRpb192YWx1ZXMlMjAlM0QlMjBvdXRwdXRzLmF1ZGlvX3ZhbHVlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, EncodecModel

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/ashraq-esc50-1-dog-example&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_sample = dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;array&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;facebook/encodec_24khz&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncodecModel.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(model_id)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(raw_audio=audio_sample, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_codes = outputs.audio_codes
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = outputs.audio_values`,wrap:!1}}),{c(){l=d("p"),l.textContent=T,h=n(),u(p.$$.fragment)},l(i){l=c(i,"P",{"data-svelte-h":!0}),m(l)!=="svelte-kvfsh7"&&(l.textContent=T),h=a(i),g(p.$$.fragment,i)},m(i,$){r(i,l,$),r(i,h,$),f(p,i,$),y=!0},p:pt,i(i){y||(_(p.$$.fragment,i),y=!0)},o(i){b(p.$$.fragment,i),y=!1},d(i){i&&(o(l),o(h)),v(p,i)}}}function Yt(Z){let l,T,h,p,y,i="<em>This model was released on 2022-10-24 and added to Hugging Face Transformers on 2023-06-14.</em>",$,X,$e,N,ut='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',je,G,Ce,H,gt='The EnCodec neural codec model was proposed in <a href="https://huggingface.co/papers/2210.13438" rel="nofollow">High Fidelity Neural Audio Compression</a> by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.',ze,B,ft="The abstract from the paper is the following:",Je,Y,_t="<em>We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.</em>",We,L,bt=`This model was contributed by <a href="https://huggingface.co/Matthijs" rel="nofollow">Matthijs</a>, <a href="https://huggingface.co/patrickvonplaten" rel="nofollow">Patrick Von Platen</a> and <a href="https://huggingface.co/ArthurZ" rel="nofollow">Arthur Zucker</a>.
The original code can be found <a href="https://github.com/facebookresearch/encodec" rel="nofollow">here</a>.`,Ue,P,Ze,S,vt="Here is a quick example of how to encode and decode an audio using this model:",Ne,Q,Ie,D,Fe,E,A,Ye,le,wt=`This is the configuration class to store the configuration of an <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a>. It is used to instantiate a
Encodec model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/facebook/encodec_24khz" rel="nofollow">facebook/encodec_24khz</a> architecture.`,Le,ie,yt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Pe,I,Re,O,Ve,M,K,Se,me,Mt="Constructs an EnCodec feature extractor.",Qe,pe,kt=`This feature extractor inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods.`,De,he,Tt=`Instantiating a feature extractor with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/facebook/encodec_24khz" rel="nofollow">facebook/encodec_24khz</a> architecture.`,Ae,F,ee,Oe,ue,Et="Main method to featurize and prepare for the model one or several sequence(s).",qe,te,Xe,w,oe,Ke,ge,xt="The EnCodec neural audio codec model.",et,fe,$t=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,tt,_e,jt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ot,z,ne,nt,be,Ct="Decodes the given frames into an output audio waveform.",at,ve,zt=`Note that the output might be a bit bigger than the input. In that case, any extra steps at the end can be
trimmed.`,st,J,ae,rt,we,Jt=`Encodes the input audio waveform into discrete codes of shape
<code>(nb_frames, batch_size, nb_quantizers, frame_len)</code>.`,dt,ye,Wt=`<li><code>nb_frames=1</code> if <code>self.config.chunk_length=None</code> (as the encoder is applied on the full audio), which is the
case for the 24kHz model. Otherwise, <code>nb_frames=ceil(input_length/self.config.chunk_stride)</code>, which is the case
for the 48kHz model.</li> <li><code>frame_len</code> is the length of each frame, which is equal to <code>ceil(input_length/self.config.hop_length)</code> if
<code>self.config.chunk_length=None</code> (e.g., for the 24kHz model). Otherwise, if <code>self.config.chunk_length</code> is
defined, <code>frame_len=self.config.chunk_length/self.config.hop_length</code>, e.g., the case for the 48kHz model with
<code>frame_len=150</code>.</li>`,ct,j,se,lt,Me,Ut='The <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a> forward method, overrides the <code>__call__</code> special method.',it,R,mt,V,Ge,re,He,xe,Be;return X=new Ee({props:{title:"EnCodec",local:"encodec",headingTag:"h1"}}),G=new Ee({props:{title:"Overview",local:"overview",headingTag:"h2"}}),P=new Ee({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),Q=new ht({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwRW5jb2RlY01vZGVsJTJDJTIwQXV0b1Byb2Nlc3NvciUwQWxpYnJpc3BlZWNoX2R1bW15JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmhmLWludGVybmFsLXRlc3RpbmclMkZsaWJyaXNwZWVjaF9hc3JfZHVtbXklMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEElMEFtb2RlbCUyMCUzRCUyMEVuY29kZWNNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZlbmNvZGVjXzI0a2h6JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZW5jb2RlY18yNGtoeiUyMiklMEFsaWJyaXNwZWVjaF9kdW1teSUyMCUzRCUyMGxpYnJpc3BlZWNoX2R1bW15LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRHByb2Nlc3Nvci5zYW1wbGluZ19yYXRlKSklMEFhdWRpb19zYW1wbGUlMjAlM0QlMjBsaWJyaXNwZWVjaF9kdW1teSU1Qi0xJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihyYXdfYXVkaW8lM0RhdWRpb19zYW1wbGUlMkMlMjBzYW1wbGluZ19yYXRlJTNEcHJvY2Vzc29yLnNhbXBsaW5nX3JhdGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQWVuY29kZXJfb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmVuY29kZShpbnB1dHMlNUIlMjJpbnB1dF92YWx1ZXMlMjIlNUQlMkMlMjBpbnB1dHMlNUIlMjJwYWRkaW5nX21hc2slMjIlNUQpJTBBJTIzJTIwJTYwZW5jb2Rlcl9vdXRwdXRzLmF1ZGlvX2NvZGVzJTYwJTIwY29udGFpbnMlMjBkaXNjcmV0ZSUyMGNvZGVzJTBBYXVkaW9fdmFsdWVzJTIwJTNEJTIwbW9kZWwuZGVjb2RlKCoqZW5jb2Rlcl9vdXRwdXRzJTJDJTIwcGFkZGluZ19tYXNrJTNEaW5wdXRzJTVCJTIycGFkZGluZ19tYXNrJTIyJTVEKSU1QjAlNUQlMEElMjMlMjBvciUyMHRoZSUyMGVxdWl2YWxlbnQlMjB3aXRoJTIwYSUyMGZvcndhcmQlMjBwYXNzJTBBYXVkaW9fdmFsdWVzJTIwJTNEJTIwbW9kZWwoaW5wdXRzJTVCJTIyaW5wdXRfdmFsdWVzJTIyJTVEJTJDJTIwaW5wdXRzJTVCJTIycGFkZGluZ19tYXNrJTIyJTVEKS5hdWRpb192YWx1ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncodecModel, AutoProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncodecModel.from_pretrained(<span class="hljs-string">&quot;facebook/encodec_24khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/encodec_24khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = librispeech_dummy.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=processor.sampling_rate))
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_sample = librispeech_dummy[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_outputs = model.encode(inputs[<span class="hljs-string">&quot;input_values&quot;</span>], inputs[<span class="hljs-string">&quot;padding_mask&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># \`encoder_outputs.audio_codes\` contains discrete codes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model.decode(**encoder_outputs, padding_mask=inputs[<span class="hljs-string">&quot;padding_mask&quot;</span>])[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or the equivalent with a forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model(inputs[<span class="hljs-string">&quot;input_values&quot;</span>], inputs[<span class="hljs-string">&quot;padding_mask&quot;</span>]).audio_values`,wrap:!1}}),D=new Ee({props:{title:"EncodecConfig",local:"transformers.EncodecConfig",headingTag:"h2"}}),A=new ce({props:{name:"class transformers.EncodecConfig",anchor:"transformers.EncodecConfig",parameters:[{name:"target_bandwidths",val:" = [1.5, 3.0, 6.0, 12.0, 24.0]"},{name:"sampling_rate",val:" = 24000"},{name:"audio_channels",val:" = 1"},{name:"normalize",val:" = False"},{name:"chunk_length_s",val:" = None"},{name:"overlap",val:" = None"},{name:"hidden_size",val:" = 128"},{name:"num_filters",val:" = 32"},{name:"num_residual_layers",val:" = 1"},{name:"upsampling_ratios",val:" = [8, 5, 4, 2]"},{name:"norm_type",val:" = 'weight_norm'"},{name:"kernel_size",val:" = 7"},{name:"last_kernel_size",val:" = 7"},{name:"residual_kernel_size",val:" = 3"},{name:"dilation_growth_rate",val:" = 2"},{name:"use_causal_conv",val:" = True"},{name:"pad_mode",val:" = 'reflect'"},{name:"compress",val:" = 2"},{name:"num_lstm_layers",val:" = 2"},{name:"trim_right_ratio",val:" = 1.0"},{name:"codebook_size",val:" = 1024"},{name:"codebook_dim",val:" = None"},{name:"use_conv_shortcut",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EncodecConfig.target_bandwidths",description:`<strong>target_bandwidths</strong> (<code>list[float]</code>, <em>optional</em>, defaults to <code>[1.5, 3.0, 6.0, 12.0, 24.0]</code>) &#x2014;
The range of different bandwidths the model can encode audio with.`,name:"target_bandwidths"},{anchor:"transformers.EncodecConfig.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 24000) &#x2014;
The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.EncodecConfig.audio_channels",description:`<strong>audio_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of channels in the audio data. Either 1 for mono or 2 for stereo.`,name:"audio_channels"},{anchor:"transformers.EncodecConfig.normalize",description:`<strong>normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the audio shall be normalized when passed.`,name:"normalize"},{anchor:"transformers.EncodecConfig.chunk_length_s",description:`<strong>chunk_length_s</strong> (<code>float</code>, <em>optional</em>) &#x2014;
If defined the audio is pre-processed into chunks of lengths <code>chunk_length_s</code> and then encoded.`,name:"chunk_length_s"},{anchor:"transformers.EncodecConfig.overlap",description:`<strong>overlap</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Defines the overlap between each chunk. It is used to compute the <code>chunk_stride</code> using the following
formulae : <code>int((1.0 - self.overlap) * self.chunk_length)</code>.`,name:"overlap"},{anchor:"transformers.EncodecConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Intermediate representation dimension.`,name:"hidden_size"},{anchor:"transformers.EncodecConfig.num_filters",description:`<strong>num_filters</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of convolution kernels of first <code>EncodecConv1d</code> down sampling layer.`,name:"num_filters"},{anchor:"transformers.EncodecConfig.num_residual_layers",description:`<strong>num_residual_layers</strong> (<code>int</code>,  <em>optional</em>, defaults to 1) &#x2014;
Number of residual layers.`,name:"num_residual_layers"},{anchor:"transformers.EncodecConfig.upsampling_ratios",description:`<strong>upsampling_ratios</strong> (<code>Sequence[int]</code> , <em>optional</em>, defaults to <code>[8, 5, 4, 2]</code>) &#x2014;
Kernel size and stride ratios. The encoder uses downsampling ratios instead of upsampling ratios, hence it
will use the ratios in the reverse order to the ones specified here that must match the decoder order.`,name:"upsampling_ratios"},{anchor:"transformers.EncodecConfig.norm_type",description:`<strong>norm_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;weight_norm&quot;</code>) &#x2014;
Normalization method. Should be in <code>[&quot;weight_norm&quot;, &quot;time_group_norm&quot;]</code>`,name:"norm_type"},{anchor:"transformers.EncodecConfig.kernel_size",description:`<strong>kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Kernel size for the initial convolution.`,name:"kernel_size"},{anchor:"transformers.EncodecConfig.last_kernel_size",description:`<strong>last_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Kernel size for the last convolution layer.`,name:"last_kernel_size"},{anchor:"transformers.EncodecConfig.residual_kernel_size",description:`<strong>residual_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size for the residual layers.`,name:"residual_kernel_size"},{anchor:"transformers.EncodecConfig.dilation_growth_rate",description:`<strong>dilation_growth_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
How much to increase the dilation with each layer.`,name:"dilation_growth_rate"},{anchor:"transformers.EncodecConfig.use_causal_conv",description:`<strong>use_causal_conv</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use fully causal convolution.`,name:"use_causal_conv"},{anchor:"transformers.EncodecConfig.pad_mode",description:`<strong>pad_mode</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;reflect&quot;</code>) &#x2014;
Padding mode for the convolutions.`,name:"pad_mode"},{anchor:"transformers.EncodecConfig.compress",description:`<strong>compress</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Reduced dimensionality in residual branches (from Demucs v3).`,name:"compress"},{anchor:"transformers.EncodecConfig.num_lstm_layers",description:`<strong>num_lstm_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of LSTM layers at the end of the encoder.`,name:"num_lstm_layers"},{anchor:"transformers.EncodecConfig.trim_right_ratio",description:`<strong>trim_right_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Ratio for trimming at the right of the transposed convolution under the <code>use_causal_conv = True</code> setup. If
equal to 1.0, it means that all the trimming is done at the right.`,name:"trim_right_ratio"},{anchor:"transformers.EncodecConfig.codebook_size",description:`<strong>codebook_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Number of discret codes that make up VQVAE.`,name:"codebook_size"},{anchor:"transformers.EncodecConfig.codebook_dim",description:`<strong>codebook_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dimension of the codebook vectors. If not defined, uses <code>hidden_size</code>.`,name:"codebook_dim"},{anchor:"transformers.EncodecConfig.use_conv_shortcut",description:`<strong>use_conv_shortcut</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a convolutional layer as the &#x2018;skip&#x2019; connection in the <code>EncodecResnetBlock</code> block. If False,
an identity function will be used, giving a generic residual connection.`,name:"use_conv_shortcut"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/configuration_encodec.py#L29"}}),I=new Zt({props:{anchor:"transformers.EncodecConfig.example",$$slots:{default:[Gt]},$$scope:{ctx:Z}}}),O=new Ee({props:{title:"EncodecFeatureExtractor",local:"transformers.EncodecFeatureExtractor",headingTag:"h2"}}),K=new ce({props:{name:"class transformers.EncodecFeatureExtractor",anchor:"transformers.EncodecFeatureExtractor",parameters:[{name:"feature_size",val:": int = 1"},{name:"sampling_rate",val:": int = 24000"},{name:"padding_value",val:": float = 0.0"},{name:"chunk_length_s",val:": typing.Optional[float] = None"},{name:"overlap",val:": typing.Optional[float] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EncodecFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The feature dimension of the extracted features. Use 1 for mono, 2 for stereo.`,name:"feature_size"},{anchor:"transformers.EncodecFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 24000) &#x2014;
The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.EncodecFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The value that is used to fill the padding values.`,name:"padding_value"},{anchor:"transformers.EncodecFeatureExtractor.chunk_length_s",description:`<strong>chunk_length_s</strong> (<code>float</code>, <em>optional</em>) &#x2014;
If defined the audio is pre-processed into chunks of lengths <code>chunk_length_s</code> and then encoded.`,name:"chunk_length_s"},{anchor:"transformers.EncodecFeatureExtractor.overlap",description:`<strong>overlap</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Defines the overlap between each chunk. It is used to compute the <code>chunk_stride</code> using the following
formulae : <code>int((1.0 - self.overlap) * self.chunk_length)</code>.`,name:"overlap"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/feature_extraction_encodec.py#L29"}}),ee=new ce({props:{name:"__call__",anchor:"transformers.EncodecFeatureExtractor.__call__",parameters:[{name:"raw_audio",val:": typing.Union[numpy.ndarray, list[float], list[numpy.ndarray], list[list[float]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy, NoneType] = None"},{name:"truncation",val:": typing.Optional[bool] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"sampling_rate",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.EncodecFeatureExtractor.__call__.raw_audio",description:`<strong>raw_audio</strong> (<code>np.ndarray</code>, <code>list[float]</code>, <code>list[np.ndarray]</code>, <code>list[list[float]]</code>) &#x2014;
The sequence or batch of sequences to be processed. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values. The numpy array must be of shape
<code>(num_samples,)</code> for mono audio (<code>feature_size = 1</code>), or <code>(2, num_samples)</code> for stereo audio
(<code>feature_size = 2</code>).`,name:"raw_audio"},{anchor:"transformers.EncodecFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.EncodecFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.EncodecFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.EncodecFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.EncodecFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>audio</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/feature_extraction_encodec.py#L84"}}),te=new Ee({props:{title:"EncodecModel",local:"transformers.EncodecModel",headingTag:"h2"}}),oe=new ce({props:{name:"class transformers.EncodecModel",anchor:"transformers.EncodecModel",parameters:[{name:"config",val:": EncodecConfig"}],parametersDescription:[{anchor:"transformers.EncodecModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecConfig">EncodecConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/modeling_encodec.py#L482"}}),ne=new ce({props:{name:"decode",anchor:"transformers.EncodecModel.decode",parameters:[{name:"audio_codes",val:": LongTensor"},{name:"audio_scales",val:": Tensor"},{name:"padding_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"last_frame_pad_length",val:": typing.Optional[int] = 0"}],parametersDescription:[{anchor:"transformers.EncodecModel.decode.audio_codes",description:`<strong>audio_codes</strong> (<code>torch.LongTensor</code>  of shape <code>(nb_frames, batch_size, nb_quantizers, frame_len)</code>, <em>optional</em>) &#x2014;
Discrete code embeddings computed using <code>model.encode</code>.`,name:"audio_codes"},{anchor:"transformers.EncodecModel.decode.audio_scales",description:`<strong>audio_scales</strong> (list of length <code>nb_frames</code> of <code>torch.Tensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Scaling factor for each <code>audio_codes</code> input.`,name:"audio_scales"},{anchor:"transformers.EncodecModel.decode.padding_mask",description:`<strong>padding_mask</strong> (<code>torch.Tensor</code> of shape <code>(channels, sequence_length)</code>) &#x2014;
Padding mask used to pad the <code>input_values</code>.`,name:"padding_mask"},{anchor:"transformers.EncodecModel.decode.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.EncodecModel.decode.last_frame_pad_length",description:`<strong>last_frame_pad_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Integer representing the length of the padding in the last frame, which is removed during decoding.`,name:"last_frame_pad_length"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/modeling_encodec.py#L663"}}),ae=new ce({props:{name:"encode",anchor:"transformers.EncodecModel.encode",parameters:[{name:"input_values",val:": Tensor"},{name:"padding_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"bandwidth",val:": typing.Optional[float] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.EncodecModel.encode.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, channels, sequence_length)</code>) &#x2014;
Float values of the input audio waveform.`,name:"input_values"},{anchor:"transformers.EncodecModel.encode.padding_mask",description:`<strong>padding_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, channels, sequence_length)</code>) &#x2014;
Padding mask used to pad the <code>input_values</code>.`,name:"padding_mask"},{anchor:"transformers.EncodecModel.encode.bandwidth",description:`<strong>bandwidth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The target bandwidth. Must be one of <code>config.target_bandwidths</code>. If <code>None</code>, uses the smallest possible
bandwidth. bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented
as bandwidth == 6.0`,name:"bandwidth"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/modeling_encodec.py#L527",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li>audio_codes (<code>torch.LongTensor</code>  of shape <code>(nb_frames, batch_size, nb_quantizers, frame_len)</code>, <em>optional</em>),</li>
<li>audio_scales (list of length <code>nb_frames</code> of <code>torch.Tensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>),</li>
<li>last_frame_pad_length (<code>int</code>, <em>optional</em>).</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>EncodecEncoderOutput dict or a tuple containing</p>
`}}),se=new ce({props:{name:"forward",anchor:"transformers.EncodecModel.forward",parameters:[{name:"input_values",val:": FloatTensor"},{name:"padding_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"bandwidth",val:": typing.Optional[float] = None"},{name:"audio_codes",val:": typing.Optional[torch.LongTensor] = None"},{name:"audio_scales",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"last_frame_pad_length",val:": typing.Optional[int] = 0"}],parametersDescription:[{anchor:"transformers.EncodecModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, channels, sequence_length)</code>, <em>optional</em>) &#x2014;
Raw audio input converted to Float and padded to the appropriate length in order to be encoded using chunks
of length self.chunk_length and a stride of <code>config.chunk_stride</code>.`,name:"input_values"},{anchor:"transformers.EncodecModel.forward.padding_mask",description:`<strong>padding_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, channels, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid computing scaling factors on padding token indices (can we avoid computing conv on these+).
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>padding_mask</code> should always be passed, unless the input was truncated or not padded. This is because in
order to process tensors effectively, the input audio should be padded so that <code>input_length % stride = step</code> with <code>step = chunk_length-stride</code>. This ensures that all chunks are of the same shape</p>

					</div>`,name:"padding_mask"},{anchor:"transformers.EncodecModel.forward.bandwidth",description:`<strong>bandwidth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The target bandwidth. Must be one of <code>config.target_bandwidths</code>. If <code>None</code>, uses the smallest possible
bandwidth. bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented as
<code>bandwidth == 6.0</code>`,name:"bandwidth"},{anchor:"transformers.EncodecModel.forward.audio_codes",description:`<strong>audio_codes</strong> (<code>torch.LongTensor</code>  of shape <code>(nb_frames, batch_size, nb_quantizers, frame_len)</code>, <em>optional</em>) &#x2014;
Discrete code embeddings computed using <code>model.encode</code>.`,name:"audio_codes"},{anchor:"transformers.EncodecModel.forward.audio_scales",description:`<strong>audio_scales</strong> (<code>list</code> of length <code>nb_frames</code> of <code>torch.Tensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Scaling factor for each <code>audio_codes</code> input.`,name:"audio_scales"},{anchor:"transformers.EncodecModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return outputs as a dict.`,name:"return_dict"},{anchor:"transformers.EncodecModel.forward.last_frame_pad_length",description:`<strong>last_frame_pad_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the padding in the last frame, if any. This is used to ensure that the encoded frames can be
outputted as a tensor. This value should be passed during decoding to ensure padding is removed from the
encoded frames.`,name:"last_frame_pad_length"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/encodec/modeling_encodec.py#L718",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.encodec.modeling_encodec.EncodecOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecConfig"
>EncodecConfig</a>) and inputs.</p>
<ul>
<li><strong>audio_codes</strong> (<code>torch.LongTensor</code>  of shape <code>(nb_frames, batch_size, nb_quantizers, frame_len)</code>, <em>optional</em>) — Discrete code embeddings computed using <code>model.encode</code>.</li>
<li><strong>audio_values</strong> (<code>torch.FloatTensor</code>  of shape <code>(batch_size, segment_length)</code>, <em>optional</em>) — Decoded audio values, obtained using the decoder part of Encodec.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.encodec.modeling_encodec.EncodecOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new qt({props:{$$slots:{default:[Ht]},$$scope:{ctx:Z}}}),V=new Zt({props:{anchor:"transformers.EncodecModel.forward.example",$$slots:{default:[Bt]},$$scope:{ctx:Z}}}),re=new Xt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/encodec.md"}}),{c(){l=d("meta"),T=n(),h=d("p"),p=n(),y=d("p"),y.innerHTML=i,$=n(),u(X.$$.fragment),$e=n(),N=d("div"),N.innerHTML=ut,je=n(),u(G.$$.fragment),Ce=n(),H=d("p"),H.innerHTML=gt,ze=n(),B=d("p"),B.textContent=ft,Je=n(),Y=d("p"),Y.innerHTML=_t,We=n(),L=d("p"),L.innerHTML=bt,Ue=n(),u(P.$$.fragment),Ze=n(),S=d("p"),S.textContent=vt,Ne=n(),u(Q.$$.fragment),Ie=n(),u(D.$$.fragment),Fe=n(),E=d("div"),u(A.$$.fragment),Ye=n(),le=d("p"),le.innerHTML=wt,Le=n(),ie=d("p"),ie.innerHTML=yt,Pe=n(),u(I.$$.fragment),Re=n(),u(O.$$.fragment),Ve=n(),M=d("div"),u(K.$$.fragment),Se=n(),me=d("p"),me.textContent=Mt,Qe=n(),pe=d("p"),pe.innerHTML=kt,De=n(),he=d("p"),he.innerHTML=Tt,Ae=n(),F=d("div"),u(ee.$$.fragment),Oe=n(),ue=d("p"),ue.textContent=Et,qe=n(),u(te.$$.fragment),Xe=n(),w=d("div"),u(oe.$$.fragment),Ke=n(),ge=d("p"),ge.textContent=xt,et=n(),fe=d("p"),fe.innerHTML=$t,tt=n(),_e=d("p"),_e.innerHTML=jt,ot=n(),z=d("div"),u(ne.$$.fragment),nt=n(),be=d("p"),be.textContent=Ct,at=n(),ve=d("p"),ve.textContent=zt,st=n(),J=d("div"),u(ae.$$.fragment),rt=n(),we=d("p"),we.innerHTML=Jt,dt=n(),ye=d("ul"),ye.innerHTML=Wt,ct=n(),j=d("div"),u(se.$$.fragment),lt=n(),Me=d("p"),Me.innerHTML=Ut,it=n(),u(R.$$.fragment),mt=n(),u(V.$$.fragment),Ge=n(),u(re.$$.fragment),He=n(),xe=d("p"),this.h()},l(e){const t=Vt("svelte-u9bgzb",document.head);l=c(t,"META",{name:!0,content:!0}),t.forEach(o),T=a(e),h=c(e,"P",{}),U(h).forEach(o),p=a(e),y=c(e,"P",{"data-svelte-h":!0}),m(y)!=="svelte-ysvbdv"&&(y.innerHTML=i),$=a(e),g(X.$$.fragment,e),$e=a(e),N=c(e,"DIV",{class:!0,"data-svelte-h":!0}),m(N)!=="svelte-13t8s2t"&&(N.innerHTML=ut),je=a(e),g(G.$$.fragment,e),Ce=a(e),H=c(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-chkgxd"&&(H.innerHTML=gt),ze=a(e),B=c(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-vfdo9a"&&(B.textContent=ft),Je=a(e),Y=c(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-1hdoj6v"&&(Y.innerHTML=_t),We=a(e),L=c(e,"P",{"data-svelte-h":!0}),m(L)!=="svelte-1gknafi"&&(L.innerHTML=bt),Ue=a(e),g(P.$$.fragment,e),Ze=a(e),S=c(e,"P",{"data-svelte-h":!0}),m(S)!=="svelte-b4kgu7"&&(S.textContent=vt),Ne=a(e),g(Q.$$.fragment,e),Ie=a(e),g(D.$$.fragment,e),Fe=a(e),E=c(e,"DIV",{class:!0});var C=U(E);g(A.$$.fragment,C),Ye=a(C),le=c(C,"P",{"data-svelte-h":!0}),m(le)!=="svelte-1jbfw85"&&(le.innerHTML=wt),Le=a(C),ie=c(C,"P",{"data-svelte-h":!0}),m(ie)!=="svelte-1ek1ss9"&&(ie.innerHTML=yt),Pe=a(C),g(I.$$.fragment,C),C.forEach(o),Re=a(e),g(O.$$.fragment,e),Ve=a(e),M=c(e,"DIV",{class:!0});var x=U(M);g(K.$$.fragment,x),Se=a(x),me=c(x,"P",{"data-svelte-h":!0}),m(me)!=="svelte-5n3zz2"&&(me.textContent=Mt),Qe=a(x),pe=c(x,"P",{"data-svelte-h":!0}),m(pe)!=="svelte-ue5gbv"&&(pe.innerHTML=kt),De=a(x),he=c(x,"P",{"data-svelte-h":!0}),m(he)!=="svelte-7r1z10"&&(he.innerHTML=Tt),Ae=a(x),F=c(x,"DIV",{class:!0});var de=U(F);g(ee.$$.fragment,de),Oe=a(de),ue=c(de,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-1a6wgfx"&&(ue.textContent=Et),de.forEach(o),x.forEach(o),qe=a(e),g(te.$$.fragment,e),Xe=a(e),w=c(e,"DIV",{class:!0});var k=U(w);g(oe.$$.fragment,k),Ke=a(k),ge=c(k,"P",{"data-svelte-h":!0}),m(ge)!=="svelte-o6dr98"&&(ge.textContent=xt),et=a(k),fe=c(k,"P",{"data-svelte-h":!0}),m(fe)!=="svelte-q52n56"&&(fe.innerHTML=$t),tt=a(k),_e=c(k,"P",{"data-svelte-h":!0}),m(_e)!=="svelte-hswkmf"&&(_e.innerHTML=jt),ot=a(k),z=c(k,"DIV",{class:!0});var ke=U(z);g(ne.$$.fragment,ke),nt=a(ke),be=c(ke,"P",{"data-svelte-h":!0}),m(be)!=="svelte-18b2f30"&&(be.textContent=Ct),at=a(ke),ve=c(ke,"P",{"data-svelte-h":!0}),m(ve)!=="svelte-12sn3fy"&&(ve.textContent=zt),ke.forEach(o),st=a(k),J=c(k,"DIV",{class:!0});var Te=U(J);g(ae.$$.fragment,Te),rt=a(Te),we=c(Te,"P",{"data-svelte-h":!0}),m(we)!=="svelte-1s0n8vr"&&(we.innerHTML=Jt),dt=a(Te),ye=c(Te,"UL",{"data-svelte-h":!0}),m(ye)!=="svelte-e6l1jp"&&(ye.innerHTML=Wt),Te.forEach(o),ct=a(k),j=c(k,"DIV",{class:!0});var q=U(j);g(se.$$.fragment,q),lt=a(q),Me=c(q,"P",{"data-svelte-h":!0}),m(Me)!=="svelte-1kvaji2"&&(Me.innerHTML=Ut),it=a(q),g(R.$$.fragment,q),mt=a(q),g(V.$$.fragment,q),q.forEach(o),k.forEach(o),Ge=a(e),g(re.$$.fragment,e),He=a(e),xe=c(e,"P",{}),U(xe).forEach(o),this.h()},h(){W(l,"name","hf:doc:metadata"),W(l,"content",Lt),W(N,"class","flex flex-wrap space-x-1"),W(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,l),r(e,T,t),r(e,h,t),r(e,p,t),r(e,y,t),r(e,$,t),f(X,e,t),r(e,$e,t),r(e,N,t),r(e,je,t),f(G,e,t),r(e,Ce,t),r(e,H,t),r(e,ze,t),r(e,B,t),r(e,Je,t),r(e,Y,t),r(e,We,t),r(e,L,t),r(e,Ue,t),f(P,e,t),r(e,Ze,t),r(e,S,t),r(e,Ne,t),f(Q,e,t),r(e,Ie,t),f(D,e,t),r(e,Fe,t),r(e,E,t),f(A,E,null),s(E,Ye),s(E,le),s(E,Le),s(E,ie),s(E,Pe),f(I,E,null),r(e,Re,t),f(O,e,t),r(e,Ve,t),r(e,M,t),f(K,M,null),s(M,Se),s(M,me),s(M,Qe),s(M,pe),s(M,De),s(M,he),s(M,Ae),s(M,F),f(ee,F,null),s(F,Oe),s(F,ue),r(e,qe,t),f(te,e,t),r(e,Xe,t),r(e,w,t),f(oe,w,null),s(w,Ke),s(w,ge),s(w,et),s(w,fe),s(w,tt),s(w,_e),s(w,ot),s(w,z),f(ne,z,null),s(z,nt),s(z,be),s(z,at),s(z,ve),s(w,st),s(w,J),f(ae,J,null),s(J,rt),s(J,we),s(J,dt),s(J,ye),s(w,ct),s(w,j),f(se,j,null),s(j,lt),s(j,Me),s(j,it),f(R,j,null),s(j,mt),f(V,j,null),r(e,Ge,t),f(re,e,t),r(e,He,t),r(e,xe,t),Be=!0},p(e,[t]){const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),I.$set(C);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),R.$set(x);const de={};t&2&&(de.$$scope={dirty:t,ctx:e}),V.$set(de)},i(e){Be||(_(X.$$.fragment,e),_(G.$$.fragment,e),_(P.$$.fragment,e),_(Q.$$.fragment,e),_(D.$$.fragment,e),_(A.$$.fragment,e),_(I.$$.fragment,e),_(O.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(R.$$.fragment,e),_(V.$$.fragment,e),_(re.$$.fragment,e),Be=!0)},o(e){b(X.$$.fragment,e),b(G.$$.fragment,e),b(P.$$.fragment,e),b(Q.$$.fragment,e),b(D.$$.fragment,e),b(A.$$.fragment,e),b(I.$$.fragment,e),b(O.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(oe.$$.fragment,e),b(ne.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(R.$$.fragment,e),b(V.$$.fragment,e),b(re.$$.fragment,e),Be=!1},d(e){e&&(o(T),o(h),o(p),o(y),o($),o($e),o(N),o(je),o(Ce),o(H),o(ze),o(B),o(Je),o(Y),o(We),o(L),o(Ue),o(Ze),o(S),o(Ne),o(Ie),o(Fe),o(E),o(Re),o(Ve),o(M),o(qe),o(Xe),o(w),o(Ge),o(He),o(xe)),o(l),v(X,e),v(G,e),v(P,e),v(Q,e),v(D,e),v(A),v(I),v(O,e),v(K),v(ee),v(te,e),v(oe),v(ne),v(ae),v(se),v(R),v(V),v(re,e)}}}const Lt='{"title":"EnCodec","local":"encodec","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"EncodecConfig","local":"transformers.EncodecConfig","sections":[],"depth":2},{"title":"EncodecFeatureExtractor","local":"transformers.EncodecFeatureExtractor","sections":[],"depth":2},{"title":"EncodecModel","local":"transformers.EncodecModel","sections":[],"depth":2}],"depth":1}';function Pt(Z){return It(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class to extends Ft{constructor(l){super(),Rt(this,l,Pt,Yt,Nt,{})}}export{to as component};
