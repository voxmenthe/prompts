import{s as Ye,o as Ke,n as et}from"../chunks/scheduler.18a86fab.js";import{S as tt,i as nt,g as r,s as a,r as f,A as st,h as l,f as n,c as o,j as ee,x as d,u as h,k as S,y as i,a as s,v as _,d as g,t as k,w as b}from"../chunks/index.98837b22.js";import{T as at}from"../chunks/Tip.77304350.js";import{D as $e}from"../chunks/Docstring.a1ef7999.js";import{C as Fe}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as qe,E as ot}from"../chunks/getInferenceSnippets.06c2775f.js";function rt(ne){let p,j=`This implementation is the same as BERT, except for tokenization method. Refer to <a href="bert">BERT documentation</a> for
API reference information.`;return{c(){p=r("p"),p.innerHTML=j},l(u){p=l(u,"P",{"data-svelte-h":!0}),d(p)!=="svelte-1wytpqs"&&(p.innerHTML=j)},m(u,N){s(u,p,N)},p:et,d(u){u&&n(p)}}}function lt(ne){let p,j,u,N,x,Ue="<em>This model was released on 2019-03-24 and added to Hugging Face Transformers on 2020-11-16.</em>",se,J,ae,w,Le='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',oe,z,re,y,De="The BERT models trained on Japanese text.",le,C,He="There are models with two different tokenization methods:",ie,E,Re='<li>Tokenize with MeCab and WordPiece. This requires some extra dependencies, <a href="https://github.com/polm/fugashi" rel="nofollow">fugashi</a> which is a wrapper around <a href="https://taku910.github.io/mecab/" rel="nofollow">MeCab</a>.</li> <li>Tokenize into characters.</li>',pe,B,Ze=`To use <em>MecabTokenizer</em>, you should <code>pip install transformers[&quot;ja&quot;]</code> (or <code>pip install -e .[&quot;ja&quot;]</code> if you install
from source) to install dependencies.`,ce,I,Pe='See <a href="https://github.com/cl-tohoku/bert-japanese" rel="nofollow">details on cl-tohoku repository</a>.',de,q,Qe="Example of using a model with MeCab and WordPiece tokenization:",me,U,ue,L,Ve="Example of using a model with Character tokenization:",fe,D,he,H,Xe='This model was contributed by <a href="https://huggingface.co/cl-tohoku" rel="nofollow">cl-tohoku</a>.',_e,M,ge,R,ke,c,Z,je,O,Se="Construct a BERT tokenizer for Japanese text.",xe,W,Ne=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`,Je,T,P,ze,A,Oe=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,ye,G,We="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",Ce,v,Q,Ee,F,Ae="Converts a sequence of tokens (string) in a single string.",Be,$,V,Ie,Y,Ge=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,be,X,Te,te,we;return J=new qe({props:{title:"BertJapanese",local:"bertjapanese",headingTag:"h1"}}),z=new qe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),U=new Fe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQWJlcnRqYXBhbmVzZSUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyY2wtdG9ob2t1JTJGYmVydC1iYXNlLWphcGFuZXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmNsLXRvaG9rdSUyRmJlcnQtYmFzZS1qYXBhbmVzZSUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾輩 は 猫 で ある 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),D=new Fe({props:{code:"YmVydGphcGFuZXNlJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾 輩 は 猫 で あ る 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),M=new at({props:{$$slots:{default:[rt]},$$scope:{ctx:ne}}}),R=new qe({props:{title:"BertJapaneseTokenizer",local:"transformers.BertJapaneseTokenizer",headingTag:"h2"}}),Z=new $e({props:{name:"class transformers.BertJapaneseTokenizer",anchor:"transformers.BertJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"spm_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"do_word_tokenize",val:" = True"},{name:"do_subword_tokenize",val:" = True"},{name:"word_tokenizer_type",val:" = 'basic'"},{name:"subword_tokenizer_type",val:" = 'wordpiece'"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"mecab_kwargs",val:" = None"},{name:"sudachi_kwargs",val:" = None"},{name:"jumanpp_kwargs",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to a one-wordpiece-per-line vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertJapaneseTokenizer.spm_file",description:`<strong>spm_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .spm or .model
extension) that contains the vocabulary.`,name:"spm_file"},{anchor:"transformers.BertJapaneseTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lower case the input. Only has an effect when do_basic_tokenize=True.`,name:"do_lower_case"},{anchor:"transformers.BertJapaneseTokenizer.do_word_tokenize",description:`<strong>do_word_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do word tokenization.`,name:"do_word_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.do_subword_tokenize",description:`<strong>do_subword_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do subword tokenization.`,name:"do_subword_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.word_tokenizer_type",description:`<strong>word_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;basic&quot;</code>) &#x2014;
Type of word tokenizer. Choose from [&#x201C;basic&#x201D;, &#x201C;mecab&#x201D;, &#x201C;sudachi&#x201D;, &#x201C;jumanpp&#x201D;].`,name:"word_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.subword_tokenizer_type",description:`<strong>subword_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;wordpiece&quot;</code>) &#x2014;
Type of subword tokenizer. Choose from [&#x201C;wordpiece&#x201D;, &#x201C;character&#x201D;, &#x201C;sentencepiece&#x201D;,].`,name:"subword_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.mecab_kwargs",description:`<strong>mecab_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>MecabTokenizer</code> constructor.`,name:"mecab_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.sudachi_kwargs",description:`<strong>sudachi_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>SudachiTokenizer</code> constructor.`,name:"sudachi_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.jumanpp_kwargs",description:`<strong>jumanpp_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>JumanppTokenizer</code> constructor.`,name:"jumanpp_kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L61"}}),P=new $e({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L258",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Q=new $e({props:{name:"convert_tokens_to_string",anchor:"transformers.BertJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L250"}}),V=new $e({props:{name:"get_special_tokens_mask",anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L284",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),X=new ot({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert-japanese.md"}}),{c(){p=r("meta"),j=a(),u=r("p"),N=a(),x=r("p"),x.innerHTML=Ue,se=a(),f(J.$$.fragment),ae=a(),w=r("div"),w.innerHTML=Le,oe=a(),f(z.$$.fragment),re=a(),y=r("p"),y.textContent=De,le=a(),C=r("p"),C.textContent=He,ie=a(),E=r("ul"),E.innerHTML=Re,pe=a(),B=r("p"),B.innerHTML=Ze,ce=a(),I=r("p"),I.innerHTML=Pe,de=a(),q=r("p"),q.textContent=Qe,me=a(),f(U.$$.fragment),ue=a(),L=r("p"),L.textContent=Ve,fe=a(),f(D.$$.fragment),he=a(),H=r("p"),H.innerHTML=Xe,_e=a(),f(M.$$.fragment),ge=a(),f(R.$$.fragment),ke=a(),c=r("div"),f(Z.$$.fragment),je=a(),O=r("p"),O.textContent=Se,xe=a(),W=r("p"),W.innerHTML=Ne,Je=a(),T=r("div"),f(P.$$.fragment),ze=a(),A=r("p"),A.textContent=Oe,ye=a(),G=r("ul"),G.innerHTML=We,Ce=a(),v=r("div"),f(Q.$$.fragment),Ee=a(),F=r("p"),F.textContent=Ae,Be=a(),$=r("div"),f(V.$$.fragment),Ie=a(),Y=r("p"),Y.innerHTML=Ge,be=a(),f(X.$$.fragment),Te=a(),te=r("p"),this.h()},l(e){const t=st("svelte-u9bgzb",document.head);p=l(t,"META",{name:!0,content:!0}),t.forEach(n),j=o(e),u=l(e,"P",{}),ee(u).forEach(n),N=o(e),x=l(e,"P",{"data-svelte-h":!0}),d(x)!=="svelte-1wixclg"&&(x.innerHTML=Ue),se=o(e),h(J.$$.fragment,e),ae=o(e),w=l(e,"DIV",{class:!0,"data-svelte-h":!0}),d(w)!=="svelte-13t8s2t"&&(w.innerHTML=Le),oe=o(e),h(z.$$.fragment,e),re=o(e),y=l(e,"P",{"data-svelte-h":!0}),d(y)!=="svelte-q6gk8k"&&(y.textContent=De),le=o(e),C=l(e,"P",{"data-svelte-h":!0}),d(C)!=="svelte-130rfh4"&&(C.textContent=He),ie=o(e),E=l(e,"UL",{"data-svelte-h":!0}),d(E)!=="svelte-1nk5iha"&&(E.innerHTML=Re),pe=o(e),B=l(e,"P",{"data-svelte-h":!0}),d(B)!=="svelte-1dsac8d"&&(B.innerHTML=Ze),ce=o(e),I=l(e,"P",{"data-svelte-h":!0}),d(I)!=="svelte-u7nt0b"&&(I.innerHTML=Pe),de=o(e),q=l(e,"P",{"data-svelte-h":!0}),d(q)!=="svelte-vqnu8f"&&(q.textContent=Qe),me=o(e),h(U.$$.fragment,e),ue=o(e),L=l(e,"P",{"data-svelte-h":!0}),d(L)!=="svelte-m6bjin"&&(L.textContent=Ve),fe=o(e),h(D.$$.fragment,e),he=o(e),H=l(e,"P",{"data-svelte-h":!0}),d(H)!=="svelte-1ykd1l9"&&(H.innerHTML=Xe),_e=o(e),h(M.$$.fragment,e),ge=o(e),h(R.$$.fragment,e),ke=o(e),c=l(e,"DIV",{class:!0});var m=ee(c);h(Z.$$.fragment,m),je=o(m),O=l(m,"P",{"data-svelte-h":!0}),d(O)!=="svelte-1tm7ou1"&&(O.textContent=Se),xe=o(m),W=l(m,"P",{"data-svelte-h":!0}),d(W)!=="svelte-meqz30"&&(W.innerHTML=Ne),Je=o(m),T=l(m,"DIV",{class:!0});var K=ee(T);h(P.$$.fragment,K),ze=o(K),A=l(K,"P",{"data-svelte-h":!0}),d(A)!=="svelte-t7qurq"&&(A.textContent=Oe),ye=o(K),G=l(K,"UL",{"data-svelte-h":!0}),d(G)!=="svelte-xi6653"&&(G.innerHTML=We),K.forEach(n),Ce=o(m),v=l(m,"DIV",{class:!0});var Me=ee(v);h(Q.$$.fragment,Me),Ee=o(Me),F=l(Me,"P",{"data-svelte-h":!0}),d(F)!=="svelte-b3k2yi"&&(F.textContent=Ae),Me.forEach(n),Be=o(m),$=l(m,"DIV",{class:!0});var ve=ee($);h(V.$$.fragment,ve),Ie=o(ve),Y=l(ve,"P",{"data-svelte-h":!0}),d(Y)!=="svelte-1f4f5kp"&&(Y.innerHTML=Ge),ve.forEach(n),m.forEach(n),be=o(e),h(X.$$.fragment,e),Te=o(e),te=l(e,"P",{}),ee(te).forEach(n),this.h()},h(){S(p,"name","hf:doc:metadata"),S(p,"content",it),S(w,"class","flex flex-wrap space-x-1"),S(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),S(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),S($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),S(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){i(document.head,p),s(e,j,t),s(e,u,t),s(e,N,t),s(e,x,t),s(e,se,t),_(J,e,t),s(e,ae,t),s(e,w,t),s(e,oe,t),_(z,e,t),s(e,re,t),s(e,y,t),s(e,le,t),s(e,C,t),s(e,ie,t),s(e,E,t),s(e,pe,t),s(e,B,t),s(e,ce,t),s(e,I,t),s(e,de,t),s(e,q,t),s(e,me,t),_(U,e,t),s(e,ue,t),s(e,L,t),s(e,fe,t),_(D,e,t),s(e,he,t),s(e,H,t),s(e,_e,t),_(M,e,t),s(e,ge,t),_(R,e,t),s(e,ke,t),s(e,c,t),_(Z,c,null),i(c,je),i(c,O),i(c,xe),i(c,W),i(c,Je),i(c,T),_(P,T,null),i(T,ze),i(T,A),i(T,ye),i(T,G),i(c,Ce),i(c,v),_(Q,v,null),i(v,Ee),i(v,F),i(c,Be),i(c,$),_(V,$,null),i($,Ie),i($,Y),s(e,be,t),_(X,e,t),s(e,Te,t),s(e,te,t),we=!0},p(e,[t]){const m={};t&2&&(m.$$scope={dirty:t,ctx:e}),M.$set(m)},i(e){we||(g(J.$$.fragment,e),g(z.$$.fragment,e),g(U.$$.fragment,e),g(D.$$.fragment,e),g(M.$$.fragment,e),g(R.$$.fragment,e),g(Z.$$.fragment,e),g(P.$$.fragment,e),g(Q.$$.fragment,e),g(V.$$.fragment,e),g(X.$$.fragment,e),we=!0)},o(e){k(J.$$.fragment,e),k(z.$$.fragment,e),k(U.$$.fragment,e),k(D.$$.fragment,e),k(M.$$.fragment,e),k(R.$$.fragment,e),k(Z.$$.fragment,e),k(P.$$.fragment,e),k(Q.$$.fragment,e),k(V.$$.fragment,e),k(X.$$.fragment,e),we=!1},d(e){e&&(n(j),n(u),n(N),n(x),n(se),n(ae),n(w),n(oe),n(re),n(y),n(le),n(C),n(ie),n(E),n(pe),n(B),n(ce),n(I),n(de),n(q),n(me),n(ue),n(L),n(fe),n(he),n(H),n(_e),n(ge),n(ke),n(c),n(be),n(Te),n(te)),n(p),b(J,e),b(z,e),b(U,e),b(D,e),b(M,e),b(R,e),b(Z),b(P),b(Q),b(V),b(X,e)}}}const it='{"title":"BertJapanese","local":"bertjapanese","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"BertJapaneseTokenizer","local":"transformers.BertJapaneseTokenizer","sections":[],"depth":2}],"depth":1}';function pt(ne){return Ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _t extends tt{constructor(p){super(),nt(this,p,pt,lt,Ye,{})}}export{_t as component};
