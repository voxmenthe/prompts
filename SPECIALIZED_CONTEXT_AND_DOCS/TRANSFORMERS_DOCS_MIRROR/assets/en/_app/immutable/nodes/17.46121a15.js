import{s as nt,o as lt,n as st}from"../chunks/scheduler.18a86fab.js";import{S as ot,i as it,g as i,s as n,r as d,A as rt,h as r,f as a,c as l,j as tt,u as c,x as m,k as Ze,y as mt,a as s,v as h,d as u,t as f,w as y}from"../chunks/index.98837b22.js";import{T as at}from"../chunks/Tip.77304350.js";import{C as K}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as He,E as pt}from"../chunks/getInferenceSnippets.06c2775f.js";function dt(D){let o,M='Refer to the <a href="./quantization/overview">Quantization</a> docs for more information about the different quantization backends available.';return{c(){o=i("p"),o.innerHTML=M},l(p){o=r(p,"P",{"data-svelte-h":!0}),m(o)!=="svelte-inxikh"&&(o.innerHTML=M)},m(p,g){s(p,o,g)},p:st,d(p){p&&a(o)}}}function ct(D){let o,M='Mixture-of-Expert (MoE) models such as <a href="./model_doc/mixtral">Mixtral</a>, <a href="./model_doc/qwen2_moe">Qwen2MoE</a>, and <a href="./model_doc/gpt-oss">GPT-OSS</a> have lots of parameters, but only “activate” a small fraction of them to generate each token. As a result, MoE models generally have much lower memory bandwidth requirements and can be faster than a regular LLM of the same size. However, techniques like speculative decoding are ineffective with MoE models because more parameters become activated with each new speculated token.';return{c(){o=i("p"),o.innerHTML=M},l(p){o=r(p,"P",{"data-svelte-h":!0}),m(o)!=="svelte-1fqwpdd"&&(o.innerHTML=M)},m(p,g){s(p,o,g)},p:st,d(p){p&&a(o)}}}function ht(D){let o,M,p,g,$,ee,v,Ge="Chat models are conversational models you can send a message to and receive a response. Most language models from mid-2023 onwards are chat models and may be referred to as “instruct” or “instruction-tuned” models. Models that do not support chat are often referred to as “base” or “pretrained” models.",te,J,Be='Larger and newer models are generally more capable, but models specialized in certain domains (medical, legal text, non-English languages, etc.) can often outperform these larger models. Try leaderboards like <a href="https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow">OpenLLM</a> and <a href="https://chat.lmsys.org/?leaderboard" rel="nofollow">LMSys Chatbot Arena</a> to help you identify the best model for your use case.',ae,U,Le='This guide shows you how to quickly load chat models in Transformers from the command line, how to build and format a conversation, and how to chat using the <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.TextGenerationPipeline">TextGenerationPipeline</a>.',se,C,ne,j,ze='After you’ve <a href="./installation">installed Transformers</a>, you can chat with a model directly from the command line. The command below launches an interactive session with a model, with a few base commands listed at the start of the session.',le,_,oe,w,We='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers-chat-cli.png"/>',ie,q,Qe="You can launch the CLI with arbitrary <code>generate</code> flags, with the format <code>arg_1=value_1 arg_2=value_2 ...</code>",re,x,me,I,Ee="For a full list of options, run the command below.",pe,k,de,H,Ve='The chat is implemented on top of the <a href="./model_doc/auto">AutoClass</a>, using tooling from <a href="./llm_tutorial">text generation</a> and <a href="./chat_templating">chat</a>. It uses the <code>transformers serve</code> CLI under the hood (<a href="./serving.md#serve-cli">docs</a>).',ce,Z,he,G,Xe='<a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.TextGenerationPipeline">TextGenerationPipeline</a> is a high-level text generation class with a “chat mode”. Chat mode is enabled when a conversational model is detected and the chat prompt is <a href="./llm_tutorial#wrong-prompt-format">properly formatted</a>.',ue,B,Ne=`Chat models accept a list of messages (the chat history) as the input. Each message is a dictionary with <code>role</code> and <code>content</code> keys.
To start the chat, add a single <code>user</code> message. You can also optionally include a <code>system</code> message to give the model directions on how to behave.`,fe,L,ye,z,Re='Create the <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.TextGenerationPipeline">TextGenerationPipeline</a> and pass <code>chat</code> to it. For large models, setting <a href="./models#big-model-inference">device_map=“auto”</a> helps load the model quicker and automatically places it on the fastest device available.',Me,W,ge,Q,Ye=`If this works successfully, you should see a response from the model! If you want to continue the conversation,
you need to update the chat history with the model’s response. You can do this either by appending the text
to <code>chat</code> (use the <code>assistant</code> role), or by reading <code>response[0][&quot;generated_text&quot;]</code>, which contains
the full chat history, including the most recent response.`,we,E,Pe="Once you have the model’s response, you can continue the conversation by appending a new <code>user</code> message to the chat history.",Te,V,be,X,Se=`By repeating this process, you can continue the conversation as long as you like, at least until the model runs out of context window
or you run out of memory.`,$e,N,ve,R,Fe="Transformers load models in full <code>float32</code> precision by default, and for a 8B model, this requires ~32GB of memory! Use the <code>torch_dtype=&quot;auto&quot;</code> argument, which generally uses <code>bfloat16</code> for models that were trained with it, to reduce your memory usage.",Je,T,Ue,Y,Ae='To lower memory usage even lower, you can quantize the model to 8-bit or 4-bit with <a href="https://hf.co/docs/bitsandbytes/index" rel="nofollow">bitsandbytes</a>. Create a <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a> with your desired quantization settings and pass it to the pipelines <code>model_kwargs</code> parameter. The example below quantizes a model to 8-bits.',Ce,P,je,S,Ke=`In general, model size and performance are directly correlated. Larger models are slower in addition to requiring more memory because each active parameter must be read from memory for every generated token.
This is a bottleneck for LLM text generation and the main options for improving generation speed are to either quantize a model or use hardware with higher memory bandwidth. Adding more compute power doesn’t meaningfully help.`,_e,F,De='You can also try techniques like <a href="./generation_strategies#speculative-decoding">speculative decoding</a>, where a smaller model generates candidate tokens that are verified by the larger model. If the candidate tokens are correct, the larger model can generate more than one token at a time. This significantly alleviates the bandwidth bottleneck and improves generation speed.',qe,b,xe,A,Ie,O,ke;return $=new He({props:{title:"Chat basics",local:"chat-basics",headingTag:"h1"}}),C=new He({props:{title:"chat CLI",local:"chat-cli",headingTag:"h2"}}),_=new K({props:{code:"dHJhbnNmb3JtZXJzJTIwY2hhdCUyMFF3ZW4lMkZRd2VuMi41LTAuNUItSW5zdHJ1Y3Q=",highlighted:"transformers chat Qwen/Qwen2.5-0.5B-Instruct",wrap:!1}}),x=new K({props:{code:"dHJhbnNmb3JtZXJzJTIwY2hhdCUyMFF3ZW4lMkZRd2VuMi41LTAuNUItSW5zdHJ1Y3QlMjBkb19zYW1wbGUlM0RGYWxzZSUyMG1heF9uZXdfdG9rZW5zJTNEMTA=",highlighted:"transformers chat Qwen/Qwen2.5-0.5B-Instruct do_sample=False max_new_tokens=10",wrap:!1}}),k=new K({props:{code:"dHJhbnNmb3JtZXJzJTIwY2hhdCUyMC1o",highlighted:"transformers chat -h",wrap:!1}}),Z=new He({props:{title:"TextGenerationPipeline",local:"textgenerationpipeline",headingTag:"h2"}}),L=new K({props:{code:"Y2hhdCUyMCUzRCUyMCU1QiUwQSUyMCUyMCUyMCUyMCU3QiUyMnJvbGUlMjIlM0ElMjAlMjJzeXN0ZW0lMjIlMkMlMjAlMjJjb250ZW50JTIyJTNBJTIwJTIyWW91JTIwYXJlJTIwYSUyMGhlbHBmdWwlMjBzY2llbmNlJTIwYXNzaXN0YW50LiUyMiU3RCUyQyUwQSUyMCUyMCUyMCUyMCU3QiUyMnJvbGUlMjIlM0ElMjAlMjJ1c2VyJTIyJTJDJTIwJTIyY29udGVudCUyMiUzQSUyMCUyMkhleSUyQyUyMGNhbiUyMHlvdSUyMGV4cGxhaW4lMjBncmF2aXR5JTIwdG8lMjBtZSUzRiUyMiU3RCUwQSU1RA==",highlighted:`chat = [
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful science assistant.&quot;</span>},
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hey, can you explain gravity to me?&quot;</span>}
]`,wrap:!1}}),W=new K({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0yLTEuN0ItSW5zdHJ1Y3QlMjIlMkMlMjBkdHlwZSUzRCUyMmF1dG8lMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFyZXNwb25zZSUyMCUzRCUyMHBpcGVsaW5lKGNoYXQlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUxMiklMEFwcmludChyZXNwb25zZSU1QjAlNUQlNUIlMjJnZW5lcmF0ZWRfdGV4dCUyMiU1RCU1Qi0xJTVEJTVCJTIyY29udGVudCUyMiU1RCk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(task=<span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;HuggingFaceTB/SmolLM2-1.7B-Instruct&quot;</span>, dtype=<span class="hljs-string">&quot;auto&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
response = pipeline(chat, max_new_tokens=<span class="hljs-number">512</span>)
<span class="hljs-built_in">print</span>(response[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;content&quot;</span>])`,wrap:!1}}),V=new K({props:{code:"Y2hhdCUyMCUzRCUyMHJlc3BvbnNlJTVCMCU1RCU1QiUyMmdlbmVyYXRlZF90ZXh0JTIyJTVEJTBBY2hhdC5hcHBlbmQoJTBBJTIwJTIwJTIwJTIwJTdCJTIycm9sZSUyMiUzQSUyMCUyMnVzZXIlMjIlMkMlMjAlMjJjb250ZW50JTIyJTNBJTIwJTIyV29haCElMjBCdXQlMjBjYW4lMjBpdCUyMGJlJTIwcmVjb25jaWxlZCUyMHdpdGglMjBxdWFudHVtJTIwbWVjaGFuaWNzJTNGJTIyJTdEJTBBKSUwQXJlc3BvbnNlJTIwJTNEJTIwcGlwZWxpbmUoY2hhdCUyQyUyMG1heF9uZXdfdG9rZW5zJTNENTEyKSUwQXByaW50KHJlc3BvbnNlJTVCMCU1RCU1QiUyMmdlbmVyYXRlZF90ZXh0JTIyJTVEJTVCLTElNUQlNUIlMjJjb250ZW50JTIyJTVEKQ==",highlighted:`chat = response[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>]
chat.append(
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Woah! But can it be reconciled with quantum mechanics?&quot;</span>}
)
response = pipeline(chat, max_new_tokens=<span class="hljs-number">512</span>)
<span class="hljs-built_in">print</span>(response[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;content&quot;</span>])`,wrap:!1}}),N=new He({props:{title:"Performance and memory usage",local:"performance-and-memory-usage",headingTag:"h2"}}),T=new at({props:{warning:!1,$$slots:{default:[dt]},$$scope:{ctx:D}}}),P=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzhiaXQlM0RUcnVlKSUwQXBpcGVsaW5lJTIwJTNEJTIwcGlwZWxpbmUodGFzayUzRCUyMnRleHQtZ2VuZXJhdGlvbiUyMiUyQyUyMG1vZGVsJTNEJTIybWV0YS1sbGFtYSUyRk1ldGEtTGxhbWEtMy04Qi1JbnN0cnVjdCUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwbW9kZWxfa3dhcmdzJTNEJTdCJTIycXVhbnRpemF0aW9uX2NvbmZpZyUyMiUzQSUyMHF1YW50aXphdGlvbl9jb25maWclN0Qp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=<span class="hljs-literal">True</span>)
pipeline = pipeline(task=<span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, model_kwargs={<span class="hljs-string">&quot;quantization_config&quot;</span>: quantization_config})`,wrap:!1}}),b=new at({props:{warning:!1,$$slots:{default:[ct]},$$scope:{ctx:D}}}),A=new pt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md"}}),{c(){o=i("meta"),M=n(),p=i("p"),g=n(),d($.$$.fragment),ee=n(),v=i("p"),v.textContent=Ge,te=n(),J=i("p"),J.innerHTML=Be,ae=n(),U=i("p"),U.innerHTML=Le,se=n(),d(C.$$.fragment),ne=n(),j=i("p"),j.innerHTML=ze,le=n(),d(_.$$.fragment),oe=n(),w=i("div"),w.innerHTML=We,ie=n(),q=i("p"),q.innerHTML=Qe,re=n(),d(x.$$.fragment),me=n(),I=i("p"),I.textContent=Ee,pe=n(),d(k.$$.fragment),de=n(),H=i("p"),H.innerHTML=Ve,ce=n(),d(Z.$$.fragment),he=n(),G=i("p"),G.innerHTML=Xe,ue=n(),B=i("p"),B.innerHTML=Ne,fe=n(),d(L.$$.fragment),ye=n(),z=i("p"),z.innerHTML=Re,Me=n(),d(W.$$.fragment),ge=n(),Q=i("p"),Q.innerHTML=Ye,we=n(),E=i("p"),E.innerHTML=Pe,Te=n(),d(V.$$.fragment),be=n(),X=i("p"),X.textContent=Se,$e=n(),d(N.$$.fragment),ve=n(),R=i("p"),R.innerHTML=Fe,Je=n(),d(T.$$.fragment),Ue=n(),Y=i("p"),Y.innerHTML=Ae,Ce=n(),d(P.$$.fragment),je=n(),S=i("p"),S.textContent=Ke,_e=n(),F=i("p"),F.innerHTML=De,qe=n(),d(b.$$.fragment),xe=n(),d(A.$$.fragment),Ie=n(),O=i("p"),this.h()},l(e){const t=rt("svelte-u9bgzb",document.head);o=r(t,"META",{name:!0,content:!0}),t.forEach(a),M=l(e),p=r(e,"P",{}),tt(p).forEach(a),g=l(e),c($.$$.fragment,e),ee=l(e),v=r(e,"P",{"data-svelte-h":!0}),m(v)!=="svelte-18t5y70"&&(v.textContent=Ge),te=l(e),J=r(e,"P",{"data-svelte-h":!0}),m(J)!=="svelte-whosqw"&&(J.innerHTML=Be),ae=l(e),U=r(e,"P",{"data-svelte-h":!0}),m(U)!=="svelte-tulvfz"&&(U.innerHTML=Le),se=l(e),c(C.$$.fragment,e),ne=l(e),j=r(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-1cejo8i"&&(j.innerHTML=ze),le=l(e),c(_.$$.fragment,e),oe=l(e),w=r(e,"DIV",{class:!0,"data-svelte-h":!0}),m(w)!=="svelte-1q6uti0"&&(w.innerHTML=We),ie=l(e),q=r(e,"P",{"data-svelte-h":!0}),m(q)!=="svelte-bhpdx1"&&(q.innerHTML=Qe),re=l(e),c(x.$$.fragment,e),me=l(e),I=r(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-1dduaqi"&&(I.textContent=Ee),pe=l(e),c(k.$$.fragment,e),de=l(e),H=r(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-uh59zh"&&(H.innerHTML=Ve),ce=l(e),c(Z.$$.fragment,e),he=l(e),G=r(e,"P",{"data-svelte-h":!0}),m(G)!=="svelte-oivt0q"&&(G.innerHTML=Xe),ue=l(e),B=r(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-162dhgx"&&(B.innerHTML=Ne),fe=l(e),c(L.$$.fragment,e),ye=l(e),z=r(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-106mo3j"&&(z.innerHTML=Re),Me=l(e),c(W.$$.fragment,e),ge=l(e),Q=r(e,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-oxbumd"&&(Q.innerHTML=Ye),we=l(e),E=r(e,"P",{"data-svelte-h":!0}),m(E)!=="svelte-dm1ays"&&(E.innerHTML=Pe),Te=l(e),c(V.$$.fragment,e),be=l(e),X=r(e,"P",{"data-svelte-h":!0}),m(X)!=="svelte-o0j64q"&&(X.textContent=Se),$e=l(e),c(N.$$.fragment,e),ve=l(e),R=r(e,"P",{"data-svelte-h":!0}),m(R)!=="svelte-a0t5dn"&&(R.innerHTML=Fe),Je=l(e),c(T.$$.fragment,e),Ue=l(e),Y=r(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-7k0p0k"&&(Y.innerHTML=Ae),Ce=l(e),c(P.$$.fragment,e),je=l(e),S=r(e,"P",{"data-svelte-h":!0}),m(S)!=="svelte-p70iv4"&&(S.textContent=Ke),_e=l(e),F=r(e,"P",{"data-svelte-h":!0}),m(F)!=="svelte-1kgq65n"&&(F.innerHTML=De),qe=l(e),c(b.$$.fragment,e),xe=l(e),c(A.$$.fragment,e),Ie=l(e),O=r(e,"P",{}),tt(O).forEach(a),this.h()},h(){Ze(o,"name","hf:doc:metadata"),Ze(o,"content",ut),Ze(w,"class","flex justify-center")},m(e,t){mt(document.head,o),s(e,M,t),s(e,p,t),s(e,g,t),h($,e,t),s(e,ee,t),s(e,v,t),s(e,te,t),s(e,J,t),s(e,ae,t),s(e,U,t),s(e,se,t),h(C,e,t),s(e,ne,t),s(e,j,t),s(e,le,t),h(_,e,t),s(e,oe,t),s(e,w,t),s(e,ie,t),s(e,q,t),s(e,re,t),h(x,e,t),s(e,me,t),s(e,I,t),s(e,pe,t),h(k,e,t),s(e,de,t),s(e,H,t),s(e,ce,t),h(Z,e,t),s(e,he,t),s(e,G,t),s(e,ue,t),s(e,B,t),s(e,fe,t),h(L,e,t),s(e,ye,t),s(e,z,t),s(e,Me,t),h(W,e,t),s(e,ge,t),s(e,Q,t),s(e,we,t),s(e,E,t),s(e,Te,t),h(V,e,t),s(e,be,t),s(e,X,t),s(e,$e,t),h(N,e,t),s(e,ve,t),s(e,R,t),s(e,Je,t),h(T,e,t),s(e,Ue,t),s(e,Y,t),s(e,Ce,t),h(P,e,t),s(e,je,t),s(e,S,t),s(e,_e,t),s(e,F,t),s(e,qe,t),h(b,e,t),s(e,xe,t),h(A,e,t),s(e,Ie,t),s(e,O,t),ke=!0},p(e,[t]){const Oe={};t&2&&(Oe.$$scope={dirty:t,ctx:e}),T.$set(Oe);const et={};t&2&&(et.$$scope={dirty:t,ctx:e}),b.$set(et)},i(e){ke||(u($.$$.fragment,e),u(C.$$.fragment,e),u(_.$$.fragment,e),u(x.$$.fragment,e),u(k.$$.fragment,e),u(Z.$$.fragment,e),u(L.$$.fragment,e),u(W.$$.fragment,e),u(V.$$.fragment,e),u(N.$$.fragment,e),u(T.$$.fragment,e),u(P.$$.fragment,e),u(b.$$.fragment,e),u(A.$$.fragment,e),ke=!0)},o(e){f($.$$.fragment,e),f(C.$$.fragment,e),f(_.$$.fragment,e),f(x.$$.fragment,e),f(k.$$.fragment,e),f(Z.$$.fragment,e),f(L.$$.fragment,e),f(W.$$.fragment,e),f(V.$$.fragment,e),f(N.$$.fragment,e),f(T.$$.fragment,e),f(P.$$.fragment,e),f(b.$$.fragment,e),f(A.$$.fragment,e),ke=!1},d(e){e&&(a(M),a(p),a(g),a(ee),a(v),a(te),a(J),a(ae),a(U),a(se),a(ne),a(j),a(le),a(oe),a(w),a(ie),a(q),a(re),a(me),a(I),a(pe),a(de),a(H),a(ce),a(he),a(G),a(ue),a(B),a(fe),a(ye),a(z),a(Me),a(ge),a(Q),a(we),a(E),a(Te),a(be),a(X),a($e),a(ve),a(R),a(Je),a(Ue),a(Y),a(Ce),a(je),a(S),a(_e),a(F),a(qe),a(xe),a(Ie),a(O)),a(o),y($,e),y(C,e),y(_,e),y(x,e),y(k,e),y(Z,e),y(L,e),y(W,e),y(V,e),y(N,e),y(T,e),y(P,e),y(b,e),y(A,e)}}}const ut='{"title":"Chat basics","local":"chat-basics","sections":[{"title":"chat CLI","local":"chat-cli","sections":[],"depth":2},{"title":"TextGenerationPipeline","local":"textgenerationpipeline","sections":[],"depth":2},{"title":"Performance and memory usage","local":"performance-and-memory-usage","sections":[],"depth":2}],"depth":1}';function ft(D){return lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bt extends ot{constructor(o){super(),it(this,o,ft,ht,nt,{})}}export{bt as component};
