import{s as Pe,o as Se,n as ze}from"../chunks/scheduler.18a86fab.js";import{S as Ge,i as Ue,g as p,s as r,r as h,A as je,h as c,f as n,c as l,j as de,x as $,u as w,k as Q,y as x,a as s,v as T,d as k,t as _,w as b}from"../chunks/index.98837b22.js";import{T as Ce}from"../chunks/Tip.77304350.js";import{D as ye}from"../chunks/Docstring.a1ef7999.js";import{C as xe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ie}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as N,E as Ve}from"../chunks/getInferenceSnippets.06c2775f.js";function He(W){let a,v=`The implementation uses the <code>GPT2Model</code> coupled with our <code>GPTSw3Tokenizer</code>. Refer to <a href="gpt2">GPT2Model documentation</a>
for API reference and examples.`,m,i,d="Note that sentencepiece is required to use our tokenizer and can be installed with <code>pip install transformers[sentencepiece]</code> or <code>pip install sentencepiece</code>";return{c(){a=p("p"),a.innerHTML=v,m=r(),i=p("p"),i.innerHTML=d},l(o){a=c(o,"P",{"data-svelte-h":!0}),$(a)!=="svelte-1yldgqy"&&(a.innerHTML=v),m=l(o),i=c(o,"P",{"data-svelte-h":!0}),$(i)!=="svelte-1oicrkv"&&(i.innerHTML=d)},m(o,u){s(o,a,u),s(o,m,u),s(o,i,u)},p:ze,d(o){o&&(n(a),n(m),n(i))}}}function Ee(W){let a,v="Example usage:",m,i,d;return i=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVFN3M1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEdQVFN3M1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyQUktU3dlZGVuLU1vZGVscyUyRmdwdC1zdzMtMTI2bSUyMiklMEF0b2tlbml6ZXIoJTIyU3ZlbnNrYSUyMCVDMyVBNHIlMjBrdWwhJTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTSw3Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTSw3Tokenizer.from_pretrained(<span class="hljs-string">&quot;AI-Sweden-Models/gpt-sw3-126m&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;Svenska är kul!&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">1814</span>, <span class="hljs-number">377</span>, <span class="hljs-number">3617</span>, <span class="hljs-number">63504</span>]`,wrap:!1}}),{c(){a=p("p"),a.textContent=v,m=r(),h(i.$$.fragment)},l(o){a=c(o,"P",{"data-svelte-h":!0}),$(a)!=="svelte-1ni337v"&&(a.textContent=v),m=l(o),w(i.$$.fragment,o)},m(o,u){s(o,a,u),s(o,m,u),T(i,o,u),d=!0},p:ze,i(o){d||(k(i.$$.fragment,o),d=!0)},o(o){_(i.$$.fragment,o),d=!1},d(o){o&&(n(a),n(m)),b(i,o)}}}function Le(W){let a,v,m,i,d,o="<em>This model was released on 2022-06-25 and added to Hugging Face Transformers on 2022-12-12.</em>",u,P,D,M,we='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Y,S,K,G,Te=`The GPT-Sw3 model was first proposed in
<a href="http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf" rel="nofollow">Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish</a>
by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman,
Fredrik Carlsson, Magnus Sahlgren.`,O,U,ke="Since that first paper the authors have extended their work and trained new models on their new 1.2TB corpora named The Nordic Pile.",ee,j,_e=`GPT-Sw3 is a collection of large decoder-only pretrained transformer language models that were developed by AI Sweden
in collaboration with RISE and the WASP WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing
320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code. The model was pretrained using a
causal language modeling (CLM) objective utilizing the NeMo Megatron GPT implementation.`,te,C,be='This model was contributed by <a href="https://huggingface.co/AI-Sweden-Models" rel="nofollow">AI Sweden Models</a>.',ne,I,se,V,oe,H,ae,E,$e='<li><a href="../tasks/sequence_classification">Text classification task guide</a></li> <li><a href="../tasks/token_classification">Token classification task guide</a></li> <li><a href="../tasks/language_modeling">Causal language modeling task guide</a></li>',re,y,le,L,ie,g,Z,ge,X,ve='Construct an GPTSw3 tokenizer. Based on <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.',fe,A,Me=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,ue,z,he,B,R,pe,J,ce,q,me;return P=new N({props:{title:"GPT-Sw3",local:"gpt-sw3",headingTag:"h1"}}),S=new N({props:{title:"Overview",local:"overview",headingTag:"h2"}}),I=new N({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),V=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMkFJLVN3ZWRlbi1Nb2RlbHMlMkZncHQtc3czLTM1Nm0lMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyQUktU3dlZGVuLU1vZGVscyUyRmdwdC1zdzMtMzU2bSUyMiklMEElMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVHIlQzMlQTRkJTIwJUMzJUE0ciUyMGZpbmElMjBmJUMzJUI2ciUyMGF0dCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBJTBBZ2VuZXJhdGVkX3Rva2VuX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0cyUzRGlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAlMkMlMjBkb19zYW1wbGUlM0RUcnVlKSU1QjAlNUQlMEElMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKGdlbmVyYXRlZF90b2tlbl9pZHMpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;AI-Sweden-Models/gpt-sw3-356m&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;AI-Sweden-Models/gpt-sw3-356m&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Träd är fina för att&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=<span class="hljs-number">10</span>, do_sample=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(generated_token_ids))
Träd är fina för att de är färgstarka. Men ibland är det fint`,wrap:!1}}),H=new N({props:{title:"Resources",local:"resources",headingTag:"h2"}}),y=new Ce({props:{$$slots:{default:[He]},$$scope:{ctx:W}}}),L=new N({props:{title:"GPTSw3Tokenizer",local:"transformers.GPTSw3Tokenizer",headingTag:"h2"}}),Z=new ye({props:{name:"class transformers.GPTSw3Tokenizer",anchor:"transformers.GPTSw3Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = False"},{name:"remove_space",val:" = False"},{name:"keep_accents",val:" = False"},{name:"pad_token",val:" = None"},{name:"unk_token",val:" = None"},{name:"eos_token",val:" = None"},{name:"bos_token",val:" = None"},{name:"sp_model_kwargs",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTSw3Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.GPTSw3Tokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.GPTSw3Tokenizer.remove_space",description:`<strong>remove_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to strip the text when tokenizing (removing excess spaces before and after the string).`,name:"remove_space"},{anchor:"transformers.GPTSw3Tokenizer.keep_accents",description:`<strong>keep_accents</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to keep accents when tokenizing.`,name:"keep_accents"},{anchor:"transformers.GPTSw3Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The token used for padding, for example when batching sequences of different lengths. If not provided, will
default to &#x2019;<pad>&#x2019; or &#x2019;<unk>&#x2019; depending on model size.</unk></pad>`,name:"pad_token"},{anchor:"transformers.GPTSw3Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead. If not provided, will default to &#x2019;<unk>&#x2018;.</unk>`,name:"unk_token"},{anchor:"transformers.GPTSw3Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The end of sequence token seen during pretraining. If not provided, will default to &#x2019;&lt;|endoftext|&gt;&#x2019;`,name:"eos_token"},{anchor:"transformers.GPTSw3Tokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The beginning of sequence token that can be used for downstream task, was not seen during pretraining. If
not provided, will default to &#x2019;<s>&#x2019; or &#x2019;&lt;|endoftext|&gt;&#x2019;, depending on model size.</s>`,name:"bos_token"},{anchor:"transformers.GPTSw3Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.GPTSw3Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"},{anchor:"transformers.GPTSw3Tokenizer.whitespaces",description:`<strong>whitespaces</strong> (<code>set</code>) &#x2014;
The whitespaces that are replaced in the whitespace normalization in preprocessing.`,name:"whitespaces"},{anchor:"transformers.GPTSw3Tokenizer.non_printing_characters_re",description:`<strong>non_printing_characters_re</strong> (<code>Pattern</code>) &#x2014;
The compiled regular expression to remove non-printing characters in preprocessing.`,name:"non_printing_characters_re"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L25"}}),z=new Ie({props:{anchor:"transformers.GPTSw3Tokenizer.example",$$slots:{default:[Ee]},$$scope:{ctx:W}}}),R=new ye({props:{name:"save_vocabulary",anchor:"transformers.GPTSw3Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L236"}}),J=new Ve({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt-sw3.md"}}),{c(){a=p("meta"),v=r(),m=p("p"),i=r(),d=p("p"),d.innerHTML=o,u=r(),h(P.$$.fragment),D=r(),M=p("div"),M.innerHTML=we,Y=r(),h(S.$$.fragment),K=r(),G=p("p"),G.innerHTML=Te,O=r(),U=p("p"),U.textContent=ke,ee=r(),j=p("p"),j.textContent=_e,te=r(),C=p("p"),C.innerHTML=be,ne=r(),h(I.$$.fragment),se=r(),h(V.$$.fragment),oe=r(),h(H.$$.fragment),ae=r(),E=p("ul"),E.innerHTML=$e,re=r(),h(y.$$.fragment),le=r(),h(L.$$.fragment),ie=r(),g=p("div"),h(Z.$$.fragment),ge=r(),X=p("p"),X.innerHTML=ve,fe=r(),A=p("p"),A.innerHTML=Me,ue=r(),h(z.$$.fragment),he=r(),B=p("div"),h(R.$$.fragment),pe=r(),h(J.$$.fragment),ce=r(),q=p("p"),this.h()},l(e){const t=je("svelte-u9bgzb",document.head);a=c(t,"META",{name:!0,content:!0}),t.forEach(n),v=l(e),m=c(e,"P",{}),de(m).forEach(n),i=l(e),d=c(e,"P",{"data-svelte-h":!0}),$(d)!=="svelte-4csx5z"&&(d.innerHTML=o),u=l(e),w(P.$$.fragment,e),D=l(e),M=c(e,"DIV",{class:!0,"data-svelte-h":!0}),$(M)!=="svelte-13t8s2t"&&(M.innerHTML=we),Y=l(e),w(S.$$.fragment,e),K=l(e),G=c(e,"P",{"data-svelte-h":!0}),$(G)!=="svelte-1qucz45"&&(G.innerHTML=Te),O=l(e),U=c(e,"P",{"data-svelte-h":!0}),$(U)!=="svelte-1uddber"&&(U.textContent=ke),ee=l(e),j=c(e,"P",{"data-svelte-h":!0}),$(j)!=="svelte-19zkpwq"&&(j.textContent=_e),te=l(e),C=c(e,"P",{"data-svelte-h":!0}),$(C)!=="svelte-p3ijej"&&(C.innerHTML=be),ne=l(e),w(I.$$.fragment,e),se=l(e),w(V.$$.fragment,e),oe=l(e),w(H.$$.fragment,e),ae=l(e),E=c(e,"UL",{"data-svelte-h":!0}),$(E)!=="svelte-v6nfyh"&&(E.innerHTML=$e),re=l(e),w(y.$$.fragment,e),le=l(e),w(L.$$.fragment,e),ie=l(e),g=c(e,"DIV",{class:!0});var f=de(g);w(Z.$$.fragment,f),ge=l(f),X=c(f,"P",{"data-svelte-h":!0}),$(X)!=="svelte-19f98fg"&&(X.innerHTML=ve),fe=l(f),A=c(f,"P",{"data-svelte-h":!0}),$(A)!=="svelte-ntrhio"&&(A.innerHTML=Me),ue=l(f),w(z.$$.fragment,f),he=l(f),B=c(f,"DIV",{class:!0});var F=de(B);w(R.$$.fragment,F),F.forEach(n),f.forEach(n),pe=l(e),w(J.$$.fragment,e),ce=l(e),q=c(e,"P",{}),de(q).forEach(n),this.h()},h(){Q(a,"name","hf:doc:metadata"),Q(a,"content",Ze),Q(M,"class","flex flex-wrap space-x-1"),Q(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){x(document.head,a),s(e,v,t),s(e,m,t),s(e,i,t),s(e,d,t),s(e,u,t),T(P,e,t),s(e,D,t),s(e,M,t),s(e,Y,t),T(S,e,t),s(e,K,t),s(e,G,t),s(e,O,t),s(e,U,t),s(e,ee,t),s(e,j,t),s(e,te,t),s(e,C,t),s(e,ne,t),T(I,e,t),s(e,se,t),T(V,e,t),s(e,oe,t),T(H,e,t),s(e,ae,t),s(e,E,t),s(e,re,t),T(y,e,t),s(e,le,t),T(L,e,t),s(e,ie,t),s(e,g,t),T(Z,g,null),x(g,ge),x(g,X),x(g,fe),x(g,A),x(g,ue),T(z,g,null),x(g,he),x(g,B),T(R,B,null),s(e,pe,t),T(J,e,t),s(e,ce,t),s(e,q,t),me=!0},p(e,[t]){const f={};t&2&&(f.$$scope={dirty:t,ctx:e}),y.$set(f);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),z.$set(F)},i(e){me||(k(P.$$.fragment,e),k(S.$$.fragment,e),k(I.$$.fragment,e),k(V.$$.fragment,e),k(H.$$.fragment,e),k(y.$$.fragment,e),k(L.$$.fragment,e),k(Z.$$.fragment,e),k(z.$$.fragment,e),k(R.$$.fragment,e),k(J.$$.fragment,e),me=!0)},o(e){_(P.$$.fragment,e),_(S.$$.fragment,e),_(I.$$.fragment,e),_(V.$$.fragment,e),_(H.$$.fragment,e),_(y.$$.fragment,e),_(L.$$.fragment,e),_(Z.$$.fragment,e),_(z.$$.fragment,e),_(R.$$.fragment,e),_(J.$$.fragment,e),me=!1},d(e){e&&(n(v),n(m),n(i),n(d),n(u),n(D),n(M),n(Y),n(K),n(G),n(O),n(U),n(ee),n(j),n(te),n(C),n(ne),n(se),n(oe),n(ae),n(E),n(re),n(le),n(ie),n(g),n(pe),n(ce),n(q)),n(a),b(P,e),b(S,e),b(I,e),b(V,e),b(H,e),b(y,e),b(L,e),b(Z),b(z),b(R),b(J,e)}}}const Ze='{"title":"GPT-Sw3","local":"gpt-sw3","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"GPTSw3Tokenizer","local":"transformers.GPTSw3Tokenizer","sections":[],"depth":2}],"depth":1}';function Re(W){return Se(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qe extends Ge{constructor(a){super(),Ue(this,a,Re,Le,Pe,{})}}export{Qe as component};
