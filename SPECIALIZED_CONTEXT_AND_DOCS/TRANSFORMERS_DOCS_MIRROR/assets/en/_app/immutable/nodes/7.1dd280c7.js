import{s as Lt,n as Dt,o as Kt}from"../chunks/scheduler.18a86fab.js";import{S as Ot,i as Pt,g as o,s as l,r as p,A as tn,h as i,f as e,c as a,j as xt,u as m,x as y,k as qt,y as nn,a as s,v as r,d as M,t as c,w as d}from"../chunks/index.98837b22.js";import{C as h}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as q,E as en}from"../chunks/getInferenceSnippets.06c2775f.js";function sn(Rt){let u,K,L,O,b,P,w,_t=`This page describes how to use the <code>AttentionInterface</code> in order to register custom attention functions to use with
supported models.`,tt,j,nt,f,Xt=`Most recent models can now switch from one attention function used in the Attention layer to the other, thanks to a simple mapping.
By default, we provide the implementation for <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>sdpa</code></a>,
<a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow"><code>flash_attention_2</code></a> and <a href="https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention" rel="nofollow"><code>flex_attention</code></a>
as well as <code>eager</code>, which is a simple matrix multiplication without any optimization on top.<br/>
This is the setting you can usually choose when instantiating a model:`,et,J,st,T,vt=`But what if you wanted to create your own attention function? Or simply play around with existing ones, adding
a few statements here and there? You can now do so with the <code>AttentionInterface</code>! Here is an example:`,lt,U,at,Z,Ct="You will see it prints “I just entered the attention computation” as many times as there are layers in the model (with this example, 16 times).",ot,g,it,k,Vt="You could dynamically change the model’s attention function as well:",pt,G,mt,I,Ft=`and it will stop printing the statements, as it now uses the <code>sdpa</code> attention.<br/>
This allows to quickly change an attention function, without needing to reload the model!`,rt,B,Mt,W,$t="For multimodal models different attention functions may work better for each backbone module. For example, some vision backbones perform better in fp32, but are incompatible with FlashAttention. To continue using FlashAttention while keeping the vision encoder in fp32, create a dict and map each config to an attention implementation as shown below.",ct,R,dt,_,yt,X,Nt=`But indeed, what if the new function requires a new arg to be properly used? It’s no issue! Models supporting the
<code>AttentionInterface</code> propagate kwargs all the way to the Attention layers, and to the used attention function. That way,
you can simply pass the arg (as a kwargs, i.e. you need to qualify the name of the arg) in the model’s forward, and it will be correctly used in the attention. However, custom attention functions have some limitations. In particular, it must follow the signature and return format of other attention functions, i.e.`,ut,v,ht,C,Yt='If in doubt about what args/kwargs a given model sends to the attention function, simply check that model’s modeling code on <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models" rel="nofollow">GitHub</a>!',bt,V,wt,F,Et=`Most of the time, you will simply need to <code>register</code> a new function. If, however, you need to access an existing one,
and/or perform a few checks, the preferred way is to use the global <code>ALL_ATTENTION_FUNCTIONS</code>. It behaves the same way you
would expect from a usual Python dictionary:`,jt,$,ft,N,Jt,Y,Ht=`Having a new attention function may mean that you need a new format of attention mask to decide what key and value tokens
the query tokens should attend to. This is now possible with the <code>AttentionMaskInterface</code>! It works in the same way as
the <code>AttentionInterface</code>:`,Tt,E,Ut,H,At=`The reason you have to register it is because we need to automatically correct your mask format based on the attention implementation (for example, flex attention uses a BlockMask format, while sdpa uses a 4D tensor).
By default, if you do not register an attention mask function along with your attention function, mask creation will be skipped
and <code>attention_mask=None</code> will be passed along to the Attention layers.`,Zt,A,Qt="The default signature of the attention mask functions is the following:",gt,Q,kt,z,zt='It mostly works thanks to the <code>mask_function</code>, which is a <code>Callable</code> in the form of <a href="https://pytorch.org/blog/flexattention/" rel="nofollow">torch’s mask_mod functions</a>, taking 4 indices as input and returning a boolean to indicate if this position should take part in the attention computation.',Gt,S,St='If you cannot use the <code>mask_function</code> to create your mask for some reason, you can try to work around it by doing something similar to our <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py" rel="nofollow">torch export workaround</a>.',It,x,Bt,D,Wt;return b=new q({props:{title:"Attention Interface",local:"attention-interface",headingTag:"h1"}}),j=new q({props:{title:"Customizing attention function",local:"customizing-attention-function",headingTag:"h2"}}),J=new h({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4yLTFCJTIyJTBBJTBBJTIzJTIwSGVyZSUyQyUyMHVzaW5nJTIwZmxhc2glMjBhdHRlbnRpb24lMjBhcyUyMGFuJTIwZXhhbXBsZSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_id = <span class="hljs-string">&quot;meta-llama/Llama-3.2-1B&quot;</span>

<span class="hljs-comment"># Here, using flash attention as an example</span>
model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>)`,wrap:!1}}),U=new h({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXR0ZW50aW9uSW50ZXJmYWNlJTBBZnJvbSUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMuc2RwYV9hdHRlbnRpb24lMjBpbXBvcnQlMjBzZHBhX2F0dGVudGlvbl9mb3J3YXJkJTBBaW1wb3J0JTIwdG9yY2glMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjItMUIlMjIlMEElMEFkZWYlMjBteV9uZXdfc2RwYSgqYXJncyUyQyUyMCoqa3dhcmdzKSUzQSUwQSUyMCUyMCUyMCUyMHByaW50KCUyMkklMjBqdXN0JTIwZW50ZXJlZCUyMHRoZSUyMGF0dGVudGlvbiUyMGNvbXB1dGF0aW9uJTIyKSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHNkcGFfYXR0ZW50aW9uX2ZvcndhcmQoKmFyZ3MlMkMlMjAqKmt3YXJncyklMEElMEFBdHRlbnRpb25JbnRlcmZhY2UucmVnaXN0ZXIoJTIybXlfbmV3X3NkcGElMjIlMkMlMjBteV9uZXdfc2RwYSklMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJteV9uZXdfc2RwYSUyMiklMEElMjMlMjBUcnklMjBydW5uaW5nJTIwdGhlJTIwZm9yd2FyZCUyMHdpdGglMjB0aGUlMjBuZXclMjBhdHRlbnRpb24lMjBmdW5jdGlvbiUwQW1vZGVsKHRvcmNoLm9uZXMoMSUyQyUyMDUlMkMlMjBkdHlwZSUzRGludCkp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AttentionInterface
<span class="hljs-keyword">from</span> transformers.integrations.sdpa_attention <span class="hljs-keyword">import</span> sdpa_attention_forward
<span class="hljs-keyword">import</span> torch

model_id = <span class="hljs-string">&quot;meta-llama/Llama-3.2-1B&quot;</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_new_sdpa</span>(<span class="hljs-params">*args, **kwargs</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;I just entered the attention computation&quot;</span>)
    <span class="hljs-keyword">return</span> sdpa_attention_forward(*args, **kwargs)

AttentionInterface.register(<span class="hljs-string">&quot;my_new_sdpa&quot;</span>, my_new_sdpa)

model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=<span class="hljs-string">&quot;my_new_sdpa&quot;</span>)
<span class="hljs-comment"># Try running the forward with the new attention function</span>
model(torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=<span class="hljs-built_in">int</span>))`,wrap:!1}}),g=new q({props:{title:"Dynamically switching attention function",local:"dynamically-switching-attention-function",headingTag:"h2"}}),G=new h({props:{code:"JTIzJTIwQmFjayUyMHRvJTIwdXNlJTIwb3JpZ2luYWwlMjBzZHBhJTIwaW1wbGVtZW50YXRpb24lMEFtb2RlbC5zZXRfYXR0bl9pbXBsZW1lbnRhdGlvbiglMjJzZHBhJTIyKSUwQSUwQW1vZGVsKHRvcmNoLm9uZXMoMSUyQyUyMDUlMkMlMjBkdHlwZSUzRGludCkp",highlighted:`<span class="hljs-comment"># Back to use original sdpa implementation</span>
model.set_attn_implementation(<span class="hljs-string">&quot;sdpa&quot;</span>)

model(torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=<span class="hljs-built_in">int</span>))`,wrap:!1}}),B=new q({props:{title:"Different attention per backbone in multimodal models",local:"different-attention-per-backbone-in-multimodal-models",headingTag:"h2"}}),R=new h({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageTextToText

model_id = <span class="hljs-string">&quot;facebook/chameleon-7b&quot;</span>

attention_implementation_per_backbone = {<span class="hljs-string">&quot;vision_config&quot;</span>: <span class="hljs-string">&quot;sdpa&quot;</span>, <span class="hljs-string">&quot;text_config&quot;</span>: <span class="hljs-string">&quot;flash_attention_2&quot;</span>}
model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=attention_implementation_per_backbone)

<span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> keys in the attention implementation have to be the same as the sub-config names</span>
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> attention_implementation_per_backbone:
    <span class="hljs-keyword">assert</span> key <span class="hljs-keyword">in</span> model.config.sub_configs, <span class="hljs-string">f&quot;Invalid key in \`attention_implementation\`&quot;</span>

<span class="hljs-comment"># You can omit certain backbones - the default attention function (SDPA) will be used</span>
<span class="hljs-comment"># This is equivalent to the previous example</span>
model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={<span class="hljs-string">&quot;text_config&quot;</span>: <span class="hljs-string">&quot;flash_attention_2&quot;</span>})


<span class="hljs-comment"># Set the same attention implementation for all backbones with single string, same as in non-multimodal models</span>
model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=<span class="hljs-string">&quot;eager&quot;</span>)

<span class="hljs-comment"># Alternatively use a dict with an empty key for global configuration</span>
model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-string">&quot;eager&quot;</span>})`,wrap:!1}}),_=new q({props:{title:"What about new args needed in my custom attention function?",local:"what-about-new-args-needed-in-my-custom-attention-function",headingTag:"h2"}}),v=new h({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXR0ZW50aW9uSW50ZXJmYWNlJTBBZnJvbSUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMuc2RwYV9hdHRlbnRpb24lMjBpbXBvcnQlMjBzZHBhX2F0dGVudGlvbl9mb3J3YXJkJTBBaW1wb3J0JTIwdG9yY2glMEElMEFkZWYlMjBjdXN0b21fYXR0ZW50aW9uKCUwQSUyMCUyMCUyMCUyMG1vZHVsZSUzQSUyMHRvcmNoLm5uLk1vZHVsZSUyQyUyMCUyMCUyMyUyMHJlcXVpcmVkJTIwYXJnJTBBJTIwJTIwJTIwJTIwcXVlcnklM0ElMjB0b3JjaC5UZW5zb3IlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMGtleSUzQSUyMHRvcmNoLlRlbnNvciUyQyUyMCUyMCUyMyUyMHJlcXVpcmVkJTIwYXJnJTBBJTIwJTIwJTIwJTIwdmFsdWUlM0ElMjB0b3JjaC5UZW5zb3IlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMGF0dGVudGlvbl9tYXNrJTNBJTIwT3B0aW9uYWwlNUJ0b3JjaC5UZW5zb3IlNUQlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMGFfbmV3X2t3YXJncyUyMCUzRCUyME5vbmUlMkMlMjAlMjAlMjMlMjBZb3UlMjBjYW4lMjBub3clMjBhZGQlMjBhcyUyMG1hbnklMjBrd2FyZ3MlMjBhcyUyMHlvdSUyMG5lZWQlMEElMjAlMjAlMjAlMjBhbm90aGVyX25ld19rd2FyZ3MlMjAlM0QlMjBOb25lJTJDJTIwJTIwJTIzJTIwWW91JTIwY2FuJTIwbm93JTIwYWRkJTIwYXMlMjBtYW55JTIwa3dhcmdzJTIwYXMlMjB5b3UlMjBuZWVkJTBBJTIwJTIwJTIwJTIwKiprd2FyZ3MlMkMlMjAlMjAlMjMlMjBZb3UlMjBuZWVkJTIwdG8lMjBhY2NlcHQlMjAqKmt3YXJncyUyMGFzJTIwbW9kZWxzJTIwd2lsbCUyMHBhc3MlMjBvdGhlciUyMGFyZ3MlMEEpJTIwLSUzRSUyMHR1cGxlJTVCdG9yY2guVGVuc29yJTJDJTIwT3B0aW9uYWwlNUJ0b3JjaC5UZW5zb3IlNUQlNUQlMEElMjAlMjAlMjAlMjAuLi4lMjAlMjAlMjMlMjBkbyUyMHlvdXIlMjBtYWdpYyElMEElMjAlMjAlMjAlMjByZXR1cm4lMjBhdHRuX291dHB1dCUyQyUyMGF0dG5fd2VpZ2h0cyUyMCUyMCUyMyUyMGF0dG5fd2VpZ2h0cyUyMGFyZSUyMG9wdGlvbmFsJTIwaGVyZSUwQSUwQUF0dGVudGlvbkludGVyZmFjZS5yZWdpc3RlciglMjJjdXN0b20lMjIlMkMlMjBjdXN0b21fYXR0ZW50aW9uKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmN1c3RvbSUyMiklMEElMjMlMjBGb3J3YXJkJTIwcGFzcyUyMHdpdGglMjB0aGUlMjBuZXclMjBrd2FyZ3MlMEFtb2RlbCh0b3JjaC5vbmVzKDElMkMlMjA1JTJDJTIwZHR5cGUlM0RpbnQpJTJDJTIwYV9uZXdfa3dhcmdzJTNELi4uJTJDJTIwYW5vdGhlcl9uZXdfa3dhcmdzJTNELi4uKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AttentionInterface
<span class="hljs-keyword">from</span> transformers.integrations.sdpa_attention <span class="hljs-keyword">import</span> sdpa_attention_forward
<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_attention</span>(<span class="hljs-params">
    module: torch.nn.Module,  <span class="hljs-comment"># required arg</span>
    query: torch.Tensor,  <span class="hljs-comment"># required arg</span>
    key: torch.Tensor,  <span class="hljs-comment"># required arg</span>
    value: torch.Tensor,  <span class="hljs-comment"># required arg</span>
    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor],  <span class="hljs-comment"># required arg</span>
    a_new_kwargs = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># You can now add as many kwargs as you need</span>
    another_new_kwargs = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># You can now add as many kwargs as you need</span>
    **kwargs,  <span class="hljs-comment"># You need to accept **kwargs as models will pass other args</span>
</span>) -&gt; <span class="hljs-built_in">tuple</span>[torch.Tensor, <span class="hljs-type">Optional</span>[torch.Tensor]]
    ...  <span class="hljs-comment"># do your magic!</span>
    <span class="hljs-keyword">return</span> attn_output, attn_weights  <span class="hljs-comment"># attn_weights are optional here</span>

AttentionInterface.register(<span class="hljs-string">&quot;custom&quot;</span>, custom_attention)

model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=<span class="hljs-string">&quot;custom&quot;</span>)
<span class="hljs-comment"># Forward pass with the new kwargs</span>
model(torch.ones(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=<span class="hljs-built_in">int</span>), a_new_kwargs=..., another_new_kwargs=...)`,wrap:!1}}),V=new q({props:{title:"Accessing current available implementations",local:"accessing-current-available-implementations",headingTag:"h2"}}),$=new h({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5tb2RlbGluZ191dGlscyUyMGltcG9ydCUyMEFMTF9BVFRFTlRJT05fRlVOQ1RJT05TJTBBJTBBbGlzdChBTExfQVRURU5USU9OX0ZVTkNUSU9OUy5rZXlzKCkpJTBBJTVCJ2ZsYXNoX2F0dGVudGlvbl8yJyUyQyUyMCdmbGV4X2F0dGVudGlvbiclMkMlMjAnc2RwYSclNUQlMEElMEFBTExfQVRURU5USU9OX0ZVTkNUSU9OUyU1QiUyMnNkcGElMjIlNUQlMEElM0NmdW5jdGlvbiUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMuc2RwYV9hdHRlbnRpb24uc2RwYV9hdHRlbnRpb25fZm9yd2FyZCUzRSUwQSUwQUFMTF9BVFRFTlRJT05fRlVOQ1RJT05TLmdldCglMjJzZHBhJTIyJTJDJTIwTm9uZSklMEElM0NmdW5jdGlvbiUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMuc2RwYV9hdHRlbnRpb24uc2RwYV9hdHRlbnRpb25fZm9yd2FyZCUzRSUwQSUwQUFMTF9BVFRFTlRJT05fRlVOQ1RJT05TLnJlZ2lzdGVyKCUyMm5ld19mdW5jJTIyJTJDJTIwbmV3X2Z1bmMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.modeling_utils <span class="hljs-keyword">import</span> ALL_ATTENTION_FUNCTIONS

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(ALL_ATTENTION_FUNCTIONS.keys())
<span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-string">&#x27;flash_attention_2&#x27;</span>, <span class="hljs-string">&#x27;flex_attention&#x27;</span>, <span class="hljs-string">&#x27;sdpa&#x27;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>ALL_ATTENTION_FUNCTIONS[<span class="hljs-string">&quot;sdpa&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>&lt;function transformers.integrations.sdpa_attention.sdpa_attention_forward&gt;

<span class="hljs-meta">&gt;&gt;&gt; </span>ALL_ATTENTION_FUNCTIONS.get(<span class="hljs-string">&quot;sdpa&quot;</span>, <span class="hljs-literal">None</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>&lt;function transformers.integrations.sdpa_attention.sdpa_attention_forward&gt;

<span class="hljs-comment"># You can also globally \`register\` a new function directly on it</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ALL_ATTENTION_FUNCTIONS.register(<span class="hljs-string">&quot;new_func&quot;</span>, new_func)`,wrap:!1}}),N=new q({props:{title:"Attention Mask Interface",local:"attention-mask-interface",headingTag:"h2"}}),E=new h({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF0dGVudGlvbk1hc2tJbnRlcmZhY2UlMEFmcm9tJTIwdHJhbnNmb3JtZXJzLm1hc2tpbmdfdXRpbHMlMjBpbXBvcnQlMjBzZHBhX21hc2slMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWRlZiUyMG15X25ld19zZHBhX21hc2soKmFyZ3MlMkMlMjAqKmt3YXJncyklM0ElMEElMjAlMjAlMjAlMjBwcmludCglMjJJJTIwanVzdCUyMGVudGVyZWQlMjB0aGUlMjBhdHRlbnRpb24lMjBtYXNrJTIwY29tcHV0YXRpb24lMjIpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwc2RwYV9tYXNrKCphcmdzJTJDJTIwKiprd2FyZ3MpJTBBJTBBQXR0ZW50aW9uTWFza0ludGVyZmFjZS5yZWdpc3RlciglMjJteV9uZXdfc2RwYV9tYXNrJTIyJTJDJTIwbXlfbmV3X3NkcGFfbWFzayk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AttentionMaskInterface
<span class="hljs-keyword">from</span> transformers.masking_utils <span class="hljs-keyword">import</span> sdpa_mask
<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">my_new_sdpa_mask</span>(<span class="hljs-params">*args, **kwargs</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;I just entered the attention mask computation&quot;</span>)
    <span class="hljs-keyword">return</span> sdpa_mask(*args, **kwargs)

AttentionMaskInterface.register(<span class="hljs-string">&quot;my_new_sdpa_mask&quot;</span>, my_new_sdpa_mask)`,wrap:!1}}),Q=new h({props:{code:"ZGVmJTIwY3VzdG9tX2F0dGVudGlvbl9tYXNrKCUwQSUyMCUyMCUyMCUyMGJhdGNoX3NpemUlM0ElMjBpbnQlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMGNhY2hlX3Bvc2l0aW9uJTNBJTIwdG9yY2guVGVuc29yJTJDJTIwJTIwJTIzJTIwcmVxdWlyZWQlMjBhcmclMEElMjAlMjAlMjAlMjBrdl9sZW5ndGglM0ElMjBpbnQlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMGt2X29mZnNldCUzQSUyMGludCUyMCUzRCUyMDAlMkMlMjAlMjAlMjMlMjByZXF1aXJlZCUyMGFyZyUwQSUyMCUyMCUyMCUyMG1hc2tfZnVuY3Rpb24lM0ElMjBDYWxsYWJsZSUyMCUzRCUyMGNhdXNhbF9tYXNrX2Z1bmN0aW9uJTJDJTIwJTIwJTIzJTIwcmVxdWlyZWQlMjBhcmclMEElMjAlMjAlMjAlMjBhdHRlbnRpb25fbWFzayUzQSUyME9wdGlvbmFsJTVCdG9yY2guVGVuc29yJTVEJTIwJTNEJTIwTm9uZSUyQyUyMCUyMCUyMyUyMHJlcXVpcmVkJTIwYXJnJTBBJTIwJTIwJTIwJTIwKiprd2FyZ3MlMkMlMjAlMjAlMjMlMjBhJTIwZmV3JTIwYWRkaXRpb25hbCUyMGFyZ3MlMjBtYXklMjBiZSUyMHBhc3NlZCUyMGFzJTIwa3dhcmdzJTJDJTIwZXNwZWNpYWxseSUyMHRoZSUyMG1vZGVsJ3MlMjBjb25maWclMjBpcyUyMGFsd2F5cyUyMHBhc3NlZCUwQSklMjAtJTNFJTIwT3B0aW9uYWwlNUJ0b3JjaC5UZW5zb3IlNUQlM0E=",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_attention_mask</span>(<span class="hljs-params">
    batch_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># required arg</span>
    cache_position: torch.Tensor,  <span class="hljs-comment"># required arg</span>
    kv_length: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># required arg</span>
    kv_offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,  <span class="hljs-comment"># required arg</span>
    mask_function: <span class="hljs-type">Callable</span> = causal_mask_function,  <span class="hljs-comment"># required arg</span>
    attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># required arg</span>
    **kwargs,  <span class="hljs-comment"># a few additional args may be passed as kwargs, especially the model&#x27;s config is always passed</span>
</span>) -&gt; <span class="hljs-type">Optional</span>[torch.Tensor]:`,wrap:!1}}),x=new en({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/attention_interface.md"}}),{c(){u=o("meta"),K=l(),L=o("p"),O=l(),p(b.$$.fragment),P=l(),w=o("p"),w.innerHTML=_t,tt=l(),p(j.$$.fragment),nt=l(),f=o("p"),f.innerHTML=Xt,et=l(),p(J.$$.fragment),st=l(),T=o("p"),T.innerHTML=vt,lt=l(),p(U.$$.fragment),at=l(),Z=o("p"),Z.textContent=Ct,ot=l(),p(g.$$.fragment),it=l(),k=o("p"),k.textContent=Vt,pt=l(),p(G.$$.fragment),mt=l(),I=o("p"),I.innerHTML=Ft,rt=l(),p(B.$$.fragment),Mt=l(),W=o("p"),W.textContent=$t,ct=l(),p(R.$$.fragment),dt=l(),p(_.$$.fragment),yt=l(),X=o("p"),X.innerHTML=Nt,ut=l(),p(v.$$.fragment),ht=l(),C=o("p"),C.innerHTML=Yt,bt=l(),p(V.$$.fragment),wt=l(),F=o("p"),F.innerHTML=Et,jt=l(),p($.$$.fragment),ft=l(),p(N.$$.fragment),Jt=l(),Y=o("p"),Y.innerHTML=Ht,Tt=l(),p(E.$$.fragment),Ut=l(),H=o("p"),H.innerHTML=At,Zt=l(),A=o("p"),A.textContent=Qt,gt=l(),p(Q.$$.fragment),kt=l(),z=o("p"),z.innerHTML=zt,Gt=l(),S=o("p"),S.innerHTML=St,It=l(),p(x.$$.fragment),Bt=l(),D=o("p"),this.h()},l(t){const n=tn("svelte-u9bgzb",document.head);u=i(n,"META",{name:!0,content:!0}),n.forEach(e),K=a(t),L=i(t,"P",{}),xt(L).forEach(e),O=a(t),m(b.$$.fragment,t),P=a(t),w=i(t,"P",{"data-svelte-h":!0}),y(w)!=="svelte-q2swd"&&(w.innerHTML=_t),tt=a(t),m(j.$$.fragment,t),nt=a(t),f=i(t,"P",{"data-svelte-h":!0}),y(f)!=="svelte-tb8yar"&&(f.innerHTML=Xt),et=a(t),m(J.$$.fragment,t),st=a(t),T=i(t,"P",{"data-svelte-h":!0}),y(T)!=="svelte-6n784k"&&(T.innerHTML=vt),lt=a(t),m(U.$$.fragment,t),at=a(t),Z=i(t,"P",{"data-svelte-h":!0}),y(Z)!=="svelte-seluzu"&&(Z.textContent=Ct),ot=a(t),m(g.$$.fragment,t),it=a(t),k=i(t,"P",{"data-svelte-h":!0}),y(k)!=="svelte-y4p893"&&(k.textContent=Vt),pt=a(t),m(G.$$.fragment,t),mt=a(t),I=i(t,"P",{"data-svelte-h":!0}),y(I)!=="svelte-1fjssz0"&&(I.innerHTML=Ft),rt=a(t),m(B.$$.fragment,t),Mt=a(t),W=i(t,"P",{"data-svelte-h":!0}),y(W)!=="svelte-1ygkhpi"&&(W.textContent=$t),ct=a(t),m(R.$$.fragment,t),dt=a(t),m(_.$$.fragment,t),yt=a(t),X=i(t,"P",{"data-svelte-h":!0}),y(X)!=="svelte-1mzkqc4"&&(X.innerHTML=Nt),ut=a(t),m(v.$$.fragment,t),ht=a(t),C=i(t,"P",{"data-svelte-h":!0}),y(C)!=="svelte-upb3ef"&&(C.innerHTML=Yt),bt=a(t),m(V.$$.fragment,t),wt=a(t),F=i(t,"P",{"data-svelte-h":!0}),y(F)!=="svelte-7h0zcj"&&(F.innerHTML=Et),jt=a(t),m($.$$.fragment,t),ft=a(t),m(N.$$.fragment,t),Jt=a(t),Y=i(t,"P",{"data-svelte-h":!0}),y(Y)!=="svelte-15uniki"&&(Y.innerHTML=Ht),Tt=a(t),m(E.$$.fragment,t),Ut=a(t),H=i(t,"P",{"data-svelte-h":!0}),y(H)!=="svelte-7vvru9"&&(H.innerHTML=At),Zt=a(t),A=i(t,"P",{"data-svelte-h":!0}),y(A)!=="svelte-10po5zn"&&(A.textContent=Qt),gt=a(t),m(Q.$$.fragment,t),kt=a(t),z=i(t,"P",{"data-svelte-h":!0}),y(z)!=="svelte-1gqwmwq"&&(z.innerHTML=zt),Gt=a(t),S=i(t,"P",{"data-svelte-h":!0}),y(S)!=="svelte-ks3jto"&&(S.innerHTML=St),It=a(t),m(x.$$.fragment,t),Bt=a(t),D=i(t,"P",{}),xt(D).forEach(e),this.h()},h(){qt(u,"name","hf:doc:metadata"),qt(u,"content",ln)},m(t,n){nn(document.head,u),s(t,K,n),s(t,L,n),s(t,O,n),r(b,t,n),s(t,P,n),s(t,w,n),s(t,tt,n),r(j,t,n),s(t,nt,n),s(t,f,n),s(t,et,n),r(J,t,n),s(t,st,n),s(t,T,n),s(t,lt,n),r(U,t,n),s(t,at,n),s(t,Z,n),s(t,ot,n),r(g,t,n),s(t,it,n),s(t,k,n),s(t,pt,n),r(G,t,n),s(t,mt,n),s(t,I,n),s(t,rt,n),r(B,t,n),s(t,Mt,n),s(t,W,n),s(t,ct,n),r(R,t,n),s(t,dt,n),r(_,t,n),s(t,yt,n),s(t,X,n),s(t,ut,n),r(v,t,n),s(t,ht,n),s(t,C,n),s(t,bt,n),r(V,t,n),s(t,wt,n),s(t,F,n),s(t,jt,n),r($,t,n),s(t,ft,n),r(N,t,n),s(t,Jt,n),s(t,Y,n),s(t,Tt,n),r(E,t,n),s(t,Ut,n),s(t,H,n),s(t,Zt,n),s(t,A,n),s(t,gt,n),r(Q,t,n),s(t,kt,n),s(t,z,n),s(t,Gt,n),s(t,S,n),s(t,It,n),r(x,t,n),s(t,Bt,n),s(t,D,n),Wt=!0},p:Dt,i(t){Wt||(M(b.$$.fragment,t),M(j.$$.fragment,t),M(J.$$.fragment,t),M(U.$$.fragment,t),M(g.$$.fragment,t),M(G.$$.fragment,t),M(B.$$.fragment,t),M(R.$$.fragment,t),M(_.$$.fragment,t),M(v.$$.fragment,t),M(V.$$.fragment,t),M($.$$.fragment,t),M(N.$$.fragment,t),M(E.$$.fragment,t),M(Q.$$.fragment,t),M(x.$$.fragment,t),Wt=!0)},o(t){c(b.$$.fragment,t),c(j.$$.fragment,t),c(J.$$.fragment,t),c(U.$$.fragment,t),c(g.$$.fragment,t),c(G.$$.fragment,t),c(B.$$.fragment,t),c(R.$$.fragment,t),c(_.$$.fragment,t),c(v.$$.fragment,t),c(V.$$.fragment,t),c($.$$.fragment,t),c(N.$$.fragment,t),c(E.$$.fragment,t),c(Q.$$.fragment,t),c(x.$$.fragment,t),Wt=!1},d(t){t&&(e(K),e(L),e(O),e(P),e(w),e(tt),e(nt),e(f),e(et),e(st),e(T),e(lt),e(at),e(Z),e(ot),e(it),e(k),e(pt),e(mt),e(I),e(rt),e(Mt),e(W),e(ct),e(dt),e(yt),e(X),e(ut),e(ht),e(C),e(bt),e(wt),e(F),e(jt),e(ft),e(Jt),e(Y),e(Tt),e(Ut),e(H),e(Zt),e(A),e(gt),e(kt),e(z),e(Gt),e(S),e(It),e(Bt),e(D)),e(u),d(b,t),d(j,t),d(J,t),d(U,t),d(g,t),d(G,t),d(B,t),d(R,t),d(_,t),d(v,t),d(V,t),d($,t),d(N,t),d(E,t),d(Q,t),d(x,t)}}}const ln='{"title":"Attention Interface","local":"attention-interface","sections":[{"title":"Customizing attention function","local":"customizing-attention-function","sections":[],"depth":2},{"title":"Dynamically switching attention function","local":"dynamically-switching-attention-function","sections":[],"depth":2},{"title":"Different attention per backbone in multimodal models","local":"different-attention-per-backbone-in-multimodal-models","sections":[],"depth":2},{"title":"What about new args needed in my custom attention function?","local":"what-about-new-args-needed-in-my-custom-attention-function","sections":[],"depth":2},{"title":"Accessing current available implementations","local":"accessing-current-available-implementations","sections":[],"depth":2},{"title":"Attention Mask Interface","local":"attention-mask-interface","sections":[],"depth":2}],"depth":1}';function an(Rt){return Kt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mn extends Ot{constructor(u){super(),Pt(this,u,an,sn,Lt,{})}}export{Mn as component};
