import{s as Et,o as qt,n as We}from"../chunks/scheduler.18a86fab.js";import{S as Lt,i as At,g as m,s,r as h,A as Ot,h as c,f as o,c as a,j,x as g,u as f,k as W,y as l,a as i,v as u,d as _,t as b,w as y}from"../chunks/index.98837b22.js";import{T as Qt}from"../chunks/Tip.77304350.js";import{D as Q}from"../chunks/Docstring.a1ef7999.js";import{C as tt}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Mt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as $e,E as Dt}from"../chunks/getInferenceSnippets.06c2775f.js";function Kt(U){let n,v="Example:",d,p,T;return p=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlZ0dwdENvbmZpZyUyQyUyMFNlZ0dwdE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFNlZ0dQVCUyMHNlZ2dwdC12aXQtbGFyZ2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwU2VnR3B0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMHNlZ2dwdC12aXQtbGFyZ2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFNlZ0dwdE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SegGptConfig, SegGptModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SegGPT seggpt-vit-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SegGptConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the seggpt-vit-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SegGptModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=v,d=s(),h(p.$$.fragment)},l(r){n=c(r,"P",{"data-svelte-h":!0}),g(n)!=="svelte-11lpom8"&&(n.textContent=v),d=a(r),f(p.$$.fragment,r)},m(r,w){i(r,n,w),i(r,d,w),u(p,r,w),T=!0},p:We,i(r){T||(_(p.$$.fragment,r),T=!0)},o(r){b(p.$$.fragment,r),T=!1},d(r){r&&(o(n),o(d)),y(p,r)}}}function eo(U){let n,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=v},l(d){n=c(d,"P",{"data-svelte-h":!0}),g(n)!=="svelte-fincs2"&&(n.innerHTML=v)},m(d,p){i(d,n,p)},p:We,d(d){d&&o(n)}}}function to(U){let n,v="Examples:",d,p,T;return p=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlZ0dwdEltYWdlUHJvY2Vzc29yJTJDJTIwU2VnR3B0TW9kZWwlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQWltYWdlX2lucHV0X3VybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRmJhYWl2aXNpb24lMkZQYWludGVyJTJGbWFpbiUyRlNlZ0dQVCUyRlNlZ0dQVF9pbmZlcmVuY2UlMkZleGFtcGxlcyUyRmhtYmJfMi5qcGclMjIlMEFpbWFnZV9wcm9tcHRfdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGYmFhaXZpc2lvbiUyRlBhaW50ZXIlMkZtYWluJTJGU2VnR1BUJTJGU2VnR1BUX2luZmVyZW5jZSUyRmV4YW1wbGVzJTJGaG1iYl8xLmpwZyUyMiUwQW1hc2tfcHJvbXB0X3VybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRmJhYWl2aXNpb24lMkZQYWludGVyJTJGbWFpbiUyRlNlZ0dQVCUyRlNlZ0dQVF9pbmZlcmVuY2UlMkZleGFtcGxlcyUyRmhtYmJfMV90YXJnZXQucG5nJTIyJTBBJTBBaW1hZ2VfaW5wdXQlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldChpbWFnZV9pbnB1dF91cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2VfcHJvbXB0JTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1hZ2VfcHJvbXB0X3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFtYXNrX3Byb21wdCUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KG1hc2tfcHJvbXB0X3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJMJTIyKSUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJCQUFJJTJGc2VnZ3B0LXZpdC1sYXJnZSUyMiUwQW1vZGVsJTIwJTNEJTIwU2VnR3B0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwU2VnR3B0SW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlX2lucHV0JTJDJTIwcHJvbXB0X2ltYWdlcyUzRGltYWdlX3Byb21wdCUyQyUyMHByb21wdF9tYXNrcyUzRG1hc2tfcHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGlzdChvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SegGptImageProcessor, SegGptModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>image_input_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_2.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_prompt_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_1.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_prompt_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_1_target.png&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>image_input = Image.<span class="hljs-built_in">open</span>(requests.get(image_input_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_prompt = Image.<span class="hljs-built_in">open</span>(requests.get(image_prompt_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_prompt = Image.<span class="hljs-built_in">open</span>(requests.get(mask_prompt_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;L&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;BAAI/seggpt-vit-large&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SegGptModel.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = SegGptImageProcessor.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image_input, prompt_images=image_prompt, prompt_masks=mask_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(outputs.last_hidden_state.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">56</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1024</span>]`,wrap:!1}}),{c(){n=m("p"),n.textContent=v,d=s(),h(p.$$.fragment)},l(r){n=c(r,"P",{"data-svelte-h":!0}),g(n)!=="svelte-kvfsh7"&&(n.textContent=v),d=a(r),f(p.$$.fragment,r)},m(r,w){i(r,n,w),i(r,d,w),u(p,r,w),T=!0},p:We,i(r){T||(_(p.$$.fragment,r),T=!0)},o(r){b(p.$$.fragment,r),T=!1},d(r){r&&(o(n),o(d)),y(p,r)}}}function oo(U){let n,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=v},l(d){n=c(d,"P",{"data-svelte-h":!0}),g(n)!=="svelte-fincs2"&&(n.innerHTML=v)},m(d,p){i(d,n,p)},p:We,d(d){d&&o(n)}}}function no(U){let n,v="Examples:",d,p,T;return p=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlZ0dwdEltYWdlUHJvY2Vzc29yJTJDJTIwU2VnR3B0Rm9ySW1hZ2VTZWdtZW50YXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQWltYWdlX2lucHV0X3VybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRmJhYWl2aXNpb24lMkZQYWludGVyJTJGbWFpbiUyRlNlZ0dQVCUyRlNlZ0dQVF9pbmZlcmVuY2UlMkZleGFtcGxlcyUyRmhtYmJfMi5qcGclMjIlMEFpbWFnZV9wcm9tcHRfdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGYmFhaXZpc2lvbiUyRlBhaW50ZXIlMkZtYWluJTJGU2VnR1BUJTJGU2VnR1BUX2luZmVyZW5jZSUyRmV4YW1wbGVzJTJGaG1iYl8xLmpwZyUyMiUwQW1hc2tfcHJvbXB0X3VybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRmJhYWl2aXNpb24lMkZQYWludGVyJTJGbWFpbiUyRlNlZ0dQVCUyRlNlZ0dQVF9pbmZlcmVuY2UlMkZleGFtcGxlcyUyRmhtYmJfMV90YXJnZXQucG5nJTIyJTBBJTBBaW1hZ2VfaW5wdXQlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldChpbWFnZV9pbnB1dF91cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2VfcHJvbXB0JTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1hZ2VfcHJvbXB0X3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFtYXNrX3Byb21wdCUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KG1hc2tfcHJvbXB0X3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJMJTIyKSUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJCQUFJJTJGc2VnZ3B0LXZpdC1sYXJnZSUyMiUwQW1vZGVsJTIwJTNEJTIwU2VnR3B0Rm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwU2VnR3B0SW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlX2lucHV0JTJDJTIwcHJvbXB0X2ltYWdlcyUzRGltYWdlX3Byb21wdCUyQyUyMHByb21wdF9tYXNrcyUzRG1hc2tfcHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBcmVzdWx0JTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19zZW1hbnRpY19zZWdtZW50YXRpb24ob3V0cHV0cyUyQyUyMHRhcmdldF9zaXplcyUzRCU1QihpbWFnZV9pbnB1dC5oZWlnaHQlMkMlMjBpbWFnZV9pbnB1dC53aWR0aCklNUQpJTVCMCU1RCUwQXByaW50KGxpc3QocmVzdWx0LnNoYXBlKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SegGptImageProcessor, SegGptForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>image_input_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_2.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_prompt_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_1.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_prompt_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/baaivision/Painter/main/SegGPT/SegGPT_inference/examples/hmbb_1_target.png&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>image_input = Image.<span class="hljs-built_in">open</span>(requests.get(image_input_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_prompt = Image.<span class="hljs-built_in">open</span>(requests.get(image_prompt_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_prompt = Image.<span class="hljs-built_in">open</span>(requests.get(mask_prompt_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;L&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;BAAI/seggpt-vit-large&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SegGptForImageSegmentation.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = SegGptImageProcessor.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image_input, prompt_images=image_prompt, prompt_masks=mask_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>result = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[(image_input.height, image_input.width)])[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(result.shape))
[<span class="hljs-number">170</span>, <span class="hljs-number">297</span>]`,wrap:!1}}),{c(){n=m("p"),n.textContent=v,d=s(),h(p.$$.fragment)},l(r){n=c(r,"P",{"data-svelte-h":!0}),g(n)!=="svelte-kvfsh7"&&(n.textContent=v),d=a(r),f(p.$$.fragment,r)},m(r,w){i(r,n,w),i(r,d,w),u(p,r,w),T=!0},p:We,i(r){T||(_(p.$$.fragment,r),T=!0)},o(r){b(p.$$.fragment,r),T=!1},d(r){r&&(o(n),o(d)),y(p,r)}}}function so(U){let n,v,d,p,T,r="<em>This model was released on 2023-04-06 and added to Hugging Face Transformers on 2024-02-26.</em>",w,E,Ze,k,Jt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',je,q,ke,L,St='The SegGPT model was proposed in <a href="https://huggingface.co/papers/2304.03284" rel="nofollow">SegGPT: Segmenting Everything In Context</a> by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. SegGPT employs a decoder-only Transformer that can generate a segmentation mask given an input image, a prompt image and its corresponding prompt mask. The model achieves remarkable one-shot results with 56.1 mIoU on COCO-20 and 85.6 mIoU on FSS-1000.',Pe,A,It="The abstract from the paper is the following:",Ne,O,Ut="<em>We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of</em>",Ve,D,Ct="Tips:",Fe,K,xt='<li>One can use <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a> to prepare image input, prompt and mask to the model.</li> <li>One can either use segmentation maps or RGB images as prompt masks. If using the latter make sure to set <code>do_convert_rgb=False</code> in the <code>preprocess</code> method.</li> <li>It’s highly advisable to pass <code>num_labels</code> when using <code>segmentation_maps</code> (not considering background) during preprocessing and postprocessing with <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a> for your use case.</li> <li>When doing inference with <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptForImageSegmentation">SegGptForImageSegmentation</a> if your <code>batch_size</code> is greater than 1 you can use feature ensemble across your images by passing <code>feature_ensemble=True</code> in the forward method.</li>',Be,ee,$t="Here’s how to use the model for one-shot semantic segmentation:",Re,te,Xe,oe,zt=`This model was contributed by <a href="https://huggingface.co/EduardoPacheco" rel="nofollow">EduardoPacheco</a>.
The original code can be found <a href="%5B(https://github.com/baaivision/Painter/tree/main)">here</a>.`,He,ne,Ye,J,se,ot,be,Wt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptModel">SegGptModel</a>. It is used to instantiate a SegGPT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the SegGPT
<a href="https://huggingface.co/BAAI/seggpt-vit-large" rel="nofollow">BAAI/seggpt-vit-large</a> architecture.`,nt,ye,Zt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,st,P,Qe,ae,Ee,S,re,at,Te,jt="Constructs a SegGpt image processor.",rt,N,ie,it,ve,kt="Preprocess an image or batch of images.",lt,V,le,mt,we,Pt=`Converts the output of <code>SegGptImageSegmentationOutput</code> into segmentation maps. Only supports
PyTorch.`,qe,me,Le,G,ce,ct,Ge,Nt="The bare Seggpt Model outputting raw hidden-states without any specific head on top.",dt,Me,Vt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,pt,Je,Ft=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,gt,C,de,ht,Se,Bt='The <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptModel">SegGptModel</a> forward method, overrides the <code>__call__</code> special method.',ft,F,ut,B,Ae,pe,Oe,M,ge,_t,Ie,Rt="SegGpt model with a decoder on top for one-shot image segmentation.",bt,Ue,Xt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,yt,Ce,Ht=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Tt,x,he,vt,xe,Yt='The <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptForImageSegmentation">SegGptForImageSegmentation</a> forward method, overrides the <code>__call__</code> special method.',wt,R,Gt,X,De,fe,Ke,ze,et;return E=new $e({props:{title:"SegGPT",local:"seggpt",headingTag:"h1"}}),q=new $e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),te=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwU2VnR3B0SW1hZ2VQcm9jZXNzb3IlMkMlMjBTZWdHcHRGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJCQUFJJTJGc2VnZ3B0LXZpdC1sYXJnZSUyMiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMFNlZ0dwdEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KSUwQW1vZGVsJTIwJTNEJTIwU2VnR3B0Rm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBJTBBZGF0YXNldF9pZCUyMCUzRCUyMCUyMkVkdWFyZG9QYWNoZWNvJTJGRm9vZFNlZzEwMyUyMiUwQWRzJTIwJTNEJTIwbG9hZF9kYXRhc2V0KGRhdGFzZXRfaWQlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQSUyMyUyME51bWJlciUyMG9mJTIwbGFiZWxzJTIwaW4lMjBGb29kU2VnMTAzJTIwKG5vdCUyMGluY2x1ZGluZyUyMGJhY2tncm91bmQpJTBBbnVtX2xhYmVscyUyMCUzRCUyMDEwMyUwQSUwQWltYWdlX2lucHV0JTIwJTNEJTIwZHMlNUI0JTVEJTVCJTIyaW1hZ2UlMjIlNUQlMEFncm91bmRfdHJ1dGglMjAlM0QlMjBkcyU1QjQlNUQlNUIlMjJsYWJlbCUyMiU1RCUwQWltYWdlX3Byb21wdCUyMCUzRCUyMGRzJTVCMjklNUQlNUIlMjJpbWFnZSUyMiU1RCUwQW1hc2tfcHJvbXB0JTIwJTNEJTIwZHMlNUIyOSU1RCU1QiUyMmxhYmVsJTIyJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKCUwQSUyMCUyMCUyMCUyMGltYWdlcyUzRGltYWdlX2lucHV0JTJDJTIwJTBBJTIwJTIwJTIwJTIwcHJvbXB0X2ltYWdlcyUzRGltYWdlX3Byb21wdCUyQyUwQSUyMCUyMCUyMCUyMHNlZ21lbnRhdGlvbl9tYXBzJTNEbWFza19wcm9tcHQlMkMlMjAlMEElMjAlMjAlMjAlMjBudW1fbGFiZWxzJTNEbnVtX2xhYmVscyUyQyUwQSUyMCUyMCUyMCUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEF0YXJnZXRfc2l6ZXMlMjAlM0QlMjAlNUJpbWFnZV9pbnB1dC5zaXplJTVCJTNBJTNBLTElNUQlNUQlMEFtYXNrJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19zZW1hbnRpY19zZWdtZW50YXRpb24ob3V0cHV0cyUyQyUyMHRhcmdldF9zaXplcyUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzKSU1QjAlNUQ=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SegGptImageProcessor, SegGptForImageSegmentation

checkpoint = <span class="hljs-string">&quot;BAAI/seggpt-vit-large&quot;</span>
image_processor = SegGptImageProcessor.from_pretrained(checkpoint)
model = SegGptForImageSegmentation.from_pretrained(checkpoint)

dataset_id = <span class="hljs-string">&quot;EduardoPacheco/FoodSeg103&quot;</span>
ds = load_dataset(dataset_id, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-comment"># Number of labels in FoodSeg103 (not including background)</span>
num_labels = <span class="hljs-number">103</span>

image_input = ds[<span class="hljs-number">4</span>][<span class="hljs-string">&quot;image&quot;</span>]
ground_truth = ds[<span class="hljs-number">4</span>][<span class="hljs-string">&quot;label&quot;</span>]
image_prompt = ds[<span class="hljs-number">29</span>][<span class="hljs-string">&quot;image&quot;</span>]
mask_prompt = ds[<span class="hljs-number">29</span>][<span class="hljs-string">&quot;label&quot;</span>]

inputs = image_processor(
    images=image_input, 
    prompt_images=image_prompt,
    segmentation_maps=mask_prompt, 
    num_labels=num_labels,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

target_sizes = [image_input.size[::-<span class="hljs-number">1</span>]]
mask = image_processor.post_process_semantic_segmentation(outputs, target_sizes, num_labels=num_labels)[<span class="hljs-number">0</span>]`,wrap:!1}}),ne=new $e({props:{title:"SegGptConfig",local:"transformers.SegGptConfig",headingTag:"h2"}}),se=new Q({props:{name:"class transformers.SegGptConfig",anchor:"transformers.SegGptConfig",parameters:[{name:"hidden_size",val:" = 1024"},{name:"num_hidden_layers",val:" = 24"},{name:"num_attention_heads",val:" = 16"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = [896, 448]"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"mlp_dim",val:" = None"},{name:"drop_path_rate",val:" = 0.1"},{name:"pretrain_image_size",val:" = 224"},{name:"decoder_hidden_size",val:" = 64"},{name:"use_relative_position_embeddings",val:" = True"},{name:"merge_index",val:" = 2"},{name:"intermediate_hidden_state_indices",val:" = [5, 11, 17, 23]"},{name:"beta",val:" = 0.01"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SegGptConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.SegGptConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.SegGptConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.SegGptConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.SegGptConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.SegGptConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.SegGptConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.SegGptConfig.image_size",description:`<strong>image_size</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[896, 448]</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.SegGptConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.SegGptConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.SegGptConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.SegGptConfig.mlp_dim",description:`<strong>mlp_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The dimensionality of the MLP layer in the Transformer encoder. If unset, defaults to
<code>hidden_size</code> * 4.`,name:"mlp_dim"},{anchor:"transformers.SegGptConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The drop path rate for the dropout layers.`,name:"drop_path_rate"},{anchor:"transformers.SegGptConfig.pretrain_image_size",description:`<strong>pretrain_image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The pretrained size of the absolute position embeddings.`,name:"pretrain_image_size"},{anchor:"transformers.SegGptConfig.decoder_hidden_size",description:`<strong>decoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Hidden size for decoder.`,name:"decoder_hidden_size"},{anchor:"transformers.SegGptConfig.use_relative_position_embeddings",description:`<strong>use_relative_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use relative position embeddings in the attention layers.`,name:"use_relative_position_embeddings"},{anchor:"transformers.SegGptConfig.merge_index",description:`<strong>merge_index</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The index of the encoder layer to merge the embeddings.`,name:"merge_index"},{anchor:"transformers.SegGptConfig.intermediate_hidden_state_indices",description:`<strong>intermediate_hidden_state_indices</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[5, 11, 17, 23]</code>) &#x2014;
The indices of the encoder layers which we store as features for the decoder.`,name:"intermediate_hidden_state_indices"},{anchor:"transformers.SegGptConfig.beta",description:`<strong>beta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.01) &#x2014;
Regularization factor for SegGptLoss (smooth-l1 loss).`,name:"beta"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/configuration_seggpt.py#L24"}}),P=new Mt({props:{anchor:"transformers.SegGptConfig.example",$$slots:{default:[Kt]},$$scope:{ctx:U}}}),ae=new $e({props:{title:"SegGptImageProcessor",local:"transformers.SegGptImageProcessor",headingTag:"h2"}}),re=new Q({props:{name:"class transformers.SegGptImageProcessor",anchor:"transformers.SegGptImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_convert_rgb",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SegGptImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. Can be overridden by the <code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.SegGptImageProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 448, &quot;width&quot;: 448}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.SegGptImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.SegGptImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.SegGptImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.SegGptImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.SegGptImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.SegGptImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.SegGptImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the prompt mask to RGB format. Can be overridden by the <code>do_convert_rgb</code> parameter in the
<code>preprocess</code> method.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/image_processing_seggpt.py#L95"}}),ie=new Q({props:{name:"preprocess",anchor:"transformers.SegGptImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"prompt_images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"prompt_masks",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"num_labels",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension] = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SegGptImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to _preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.SegGptImageProcessor.preprocess.prompt_images",description:`<strong>prompt_images</strong> (<code>ImageInput</code>) &#x2014;
Prompt image to _preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"prompt_images"},{anchor:"transformers.SegGptImageProcessor.preprocess.prompt_masks",description:`<strong>prompt_masks</strong> (<code>ImageInput</code>) &#x2014;
Prompt mask from prompt image to _preprocess that specify prompt_masks value in the preprocessed output.
Can either be in the format of segmentation maps (no channels) or RGB images. If in the format of
RGB images, <code>do_convert_rgb</code> should be set to <code>False</code>. If in the format of segmentation maps, <code>num_labels</code>
specifying <code>num_labels</code> is recommended to build a palette to map the prompt mask from a single channel to
a 3 channel RGB. If <code>num_labels</code> is not specified, the prompt mask will be duplicated across the channel
dimension.`,name:"prompt_masks"},{anchor:"transformers.SegGptImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.SegGptImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Dictionary in the format <code>{&quot;height&quot;: h, &quot;width&quot;: w}</code> specifying the size of the output image after
resizing.`,name:"size"},{anchor:"transformers.SegGptImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code> filter, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
<code>PILImageResampling</code> filter to use if resizing the image e.g. <code>PILImageResampling.BICUBIC</code>. Only has
an effect if <code>do_resize</code> is set to <code>True</code>. Doesn&#x2019;t apply to prompt mask as it is resized using nearest.`,name:"resample"},{anchor:"transformers.SegGptImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.SegGptImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.SegGptImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.SegGptImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.SegGptImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.SegGptImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the prompt mask to RGB format. If <code>num_labels</code> is specified, a palette will be built
to map the prompt mask from a single channel to a 3 channel RGB. If unset, the prompt mask is duplicated
across the channel dimension. Must be set to <code>False</code> if the prompt mask is already in RGB format.`,name:"do_convert_rgb"},{anchor:"transformers.SegGptImageProcessor.preprocess.num_labels",description:`<strong>num_labels</strong> &#x2014; (<code>int</code>, <em>optional</em>):
Number of classes in the segmentation task (excluding the background). If specified, a palette will be
built, assuming that class_idx 0 is the background, to map the prompt mask from a plain segmentation map
with no channels to a 3 channel RGB. Not specifying this will result in the prompt mask either being passed
through as is if it is already in RGB format (if <code>do_convert_rgb</code> is false) or being duplicated
across the channel dimension.`,name:"num_labels"},{anchor:"transformers.SegGptImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.SegGptImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SegGptImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/image_processing_seggpt.py#L388"}}),le=new Q({props:{name:"post_process_semantic_segmentation",anchor:"transformers.SegGptImageProcessor.post_process_semantic_segmentation",parameters:[{name:"outputs",val:""},{name:"target_sizes",val:": typing.Optional[list[tuple[int, int]]] = None"},{name:"num_labels",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.SegGptImageProcessor.post_process_semantic_segmentation.outputs",description:`<strong>outputs</strong> (<code>SegGptImageSegmentationOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.SegGptImageProcessor.post_process_semantic_segmentation.target_sizes",description:`<strong>target_sizes</strong> (<code>list[tuple[int, int]]</code>, <em>optional</em>) &#x2014;
List of length (batch_size), where each list item (<code>tuple[int, int]</code>) corresponds to the requested
final size (height, width) of each prediction. If left to None, predictions will not be resized.`,name:"target_sizes"},{anchor:"transformers.SegGptImageProcessor.post_process_semantic_segmentation.num_labels",description:`<strong>num_labels</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of classes in the segmentation task (excluding the background). If specified, a palette will be
built, assuming that class_idx 0 is the background, to map prediction masks from RGB values to class
indices. This value should be the same used when preprocessing inputs.`,name:"num_labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/image_processing_seggpt.py#L539",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[torch.Tensor]</code> of length <code>batch_size</code>, where each item is a semantic
segmentation map of shape (height, width) corresponding to the target_sizes entry (if <code>target_sizes</code> is
specified). Each entry of each <code>torch.Tensor</code> correspond to a semantic class id.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>semantic_segmentation</p>
`}}),me=new $e({props:{title:"SegGptModel",local:"transformers.SegGptModel",headingTag:"h2"}}),ce=new Q({props:{name:"class transformers.SegGptModel",anchor:"transformers.SegGptModel",parameters:[{name:"config",val:": SegGptConfig"}],parametersDescription:[{anchor:"transformers.SegGptModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig">SegGptConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/modeling_seggpt.py#L645"}}),de=new Q({props:{name:"forward",anchor:"transformers.SegGptModel.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"prompt_pixel_values",val:": Tensor"},{name:"prompt_masks",val:": Tensor"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"feature_ensemble",val:": typing.Optional[bool] = None"},{name:"embedding_type",val:": typing.Optional[str] = None"},{name:"labels",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SegGptModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SegGptModel.forward.prompt_pixel_values",description:`<strong>prompt_pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Prompt pixel values. Prompt pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for details.`,name:"prompt_pixel_values"},{anchor:"transformers.SegGptModel.forward.prompt_masks",description:`<strong>prompt_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Prompt mask. Prompt mask can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for
details.`,name:"prompt_masks"},{anchor:"transformers.SegGptModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.SegGptModel.forward.feature_ensemble",description:`<strong>feature_ensemble</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Boolean indicating whether to use feature ensemble or not. If <code>True</code>, the model will use feature ensemble
if we have at least two prompts. If <code>False</code>, the model will not use feature ensemble. This argument should
be considered when doing few-shot inference on an input image i.e. more than one prompt for the same image.`,name:"feature_ensemble"},{anchor:"transformers.SegGptModel.forward.embedding_type",description:`<strong>embedding_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Embedding type. Indicates whether the prompt is a semantic or instance embedding. Can be either
instance or semantic.`,name:"embedding_type"},{anchor:"transformers.SegGptModel.forward.labels",description:`<strong>labels</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>, <code>optional</code>) &#x2014;
Ground truth mask for input images.`,name:"labels"},{anchor:"transformers.SegGptModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.SegGptModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SegGptModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/modeling_seggpt.py#L667",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.seggpt.modeling_seggpt.SegGptEncoderOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig"
>SegGptConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_height, patch_width, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <code>optional</code>, returned when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, patch_height, patch_width, hidden_size)</code>.</li>
<li><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <code>optional</code>, returned when <code>config.output_attentions=True</code>) — Tuple of <em>torch.FloatTensor</em> (one for each layer) of shape
<code>(batch_size, num_heads, seq_len, seq_len)</code>.</li>
<li><strong>intermediate_hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>config.intermediate_hidden_state_indices</code> is set) — Tuple of <code>torch.FloatTensor</code> of shape <code>(batch_size, patch_height, patch_width, hidden_size)</code>.
Each element in the Tuple corresponds to the output of the layer specified in <code>config.intermediate_hidden_state_indices</code>.
Additionally, each feature passes through a LayerNorm.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.seggpt.modeling_seggpt.SegGptEncoderOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),F=new Qt({props:{$$slots:{default:[eo]},$$scope:{ctx:U}}}),B=new Mt({props:{anchor:"transformers.SegGptModel.forward.example",$$slots:{default:[to]},$$scope:{ctx:U}}}),pe=new $e({props:{title:"SegGptForImageSegmentation",local:"transformers.SegGptForImageSegmentation",headingTag:"h2"}}),ge=new Q({props:{name:"class transformers.SegGptForImageSegmentation",anchor:"transformers.SegGptForImageSegmentation",parameters:[{name:"config",val:": SegGptConfig"}],parametersDescription:[{anchor:"transformers.SegGptForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig">SegGptConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/modeling_seggpt.py#L852"}}),he=new Q({props:{name:"forward",anchor:"transformers.SegGptForImageSegmentation.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"prompt_pixel_values",val:": Tensor"},{name:"prompt_masks",val:": Tensor"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"feature_ensemble",val:": typing.Optional[bool] = None"},{name:"embedding_type",val:": typing.Optional[str] = None"},{name:"labels",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SegGptForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SegGptForImageSegmentation.forward.prompt_pixel_values",description:`<strong>prompt_pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Prompt pixel values. Prompt pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for details.`,name:"prompt_pixel_values"},{anchor:"transformers.SegGptForImageSegmentation.forward.prompt_masks",description:`<strong>prompt_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Prompt mask. Prompt mask can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SegGptImageProcessor.<strong>call</strong>()</a> for
details.`,name:"prompt_masks"},{anchor:"transformers.SegGptForImageSegmentation.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.SegGptForImageSegmentation.forward.feature_ensemble",description:`<strong>feature_ensemble</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Boolean indicating whether to use feature ensemble or not. If <code>True</code>, the model will use feature ensemble
if we have at least two prompts. If <code>False</code>, the model will not use feature ensemble. This argument should
be considered when doing few-shot inference on an input image i.e. more than one prompt for the same image.`,name:"feature_ensemble"},{anchor:"transformers.SegGptForImageSegmentation.forward.embedding_type",description:`<strong>embedding_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Embedding type. Indicates whether the prompt is a semantic or instance embedding. Can be either
instance or semantic.`,name:"embedding_type"},{anchor:"transformers.SegGptForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>, <code>optional</code>) &#x2014;
Ground truth mask for input images.`,name:"labels"},{anchor:"transformers.SegGptForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.SegGptForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SegGptForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/seggpt/modeling_seggpt.py#L863",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.seggpt.modeling_seggpt.SegGptImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig"
>SegGptConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided) — The loss value.</li>
<li><strong>pred_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — The predicted masks.</li>
<li><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <code>optional</code>, returned when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, patch_height, patch_width, hidden_size)</code>.</li>
<li><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <code>optional</code>, returned when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape
<code>(batch_size, num_heads, seq_len, seq_len)</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.seggpt.modeling_seggpt.SegGptImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new Qt({props:{$$slots:{default:[oo]},$$scope:{ctx:U}}}),X=new Mt({props:{anchor:"transformers.SegGptForImageSegmentation.forward.example",$$slots:{default:[no]},$$scope:{ctx:U}}}),fe=new Dt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seggpt.md"}}),{c(){n=m("meta"),v=s(),d=m("p"),p=s(),T=m("p"),T.innerHTML=r,w=s(),h(E.$$.fragment),Ze=s(),k=m("div"),k.innerHTML=Jt,je=s(),h(q.$$.fragment),ke=s(),L=m("p"),L.innerHTML=St,Pe=s(),A=m("p"),A.textContent=It,Ne=s(),O=m("p"),O.innerHTML=Ut,Ve=s(),D=m("p"),D.textContent=Ct,Fe=s(),K=m("ul"),K.innerHTML=xt,Be=s(),ee=m("p"),ee.textContent=$t,Re=s(),h(te.$$.fragment),Xe=s(),oe=m("p"),oe.innerHTML=zt,He=s(),h(ne.$$.fragment),Ye=s(),J=m("div"),h(se.$$.fragment),ot=s(),be=m("p"),be.innerHTML=Wt,nt=s(),ye=m("p"),ye.innerHTML=Zt,st=s(),h(P.$$.fragment),Qe=s(),h(ae.$$.fragment),Ee=s(),S=m("div"),h(re.$$.fragment),at=s(),Te=m("p"),Te.textContent=jt,rt=s(),N=m("div"),h(ie.$$.fragment),it=s(),ve=m("p"),ve.textContent=kt,lt=s(),V=m("div"),h(le.$$.fragment),mt=s(),we=m("p"),we.innerHTML=Pt,qe=s(),h(me.$$.fragment),Le=s(),G=m("div"),h(ce.$$.fragment),ct=s(),Ge=m("p"),Ge.textContent=Nt,dt=s(),Me=m("p"),Me.innerHTML=Vt,pt=s(),Je=m("p"),Je.innerHTML=Ft,gt=s(),C=m("div"),h(de.$$.fragment),ht=s(),Se=m("p"),Se.innerHTML=Bt,ft=s(),h(F.$$.fragment),ut=s(),h(B.$$.fragment),Ae=s(),h(pe.$$.fragment),Oe=s(),M=m("div"),h(ge.$$.fragment),_t=s(),Ie=m("p"),Ie.textContent=Rt,bt=s(),Ue=m("p"),Ue.innerHTML=Xt,yt=s(),Ce=m("p"),Ce.innerHTML=Ht,Tt=s(),x=m("div"),h(he.$$.fragment),vt=s(),xe=m("p"),xe.innerHTML=Yt,wt=s(),h(R.$$.fragment),Gt=s(),h(X.$$.fragment),De=s(),h(fe.$$.fragment),Ke=s(),ze=m("p"),this.h()},l(e){const t=Ot("svelte-u9bgzb",document.head);n=c(t,"META",{name:!0,content:!0}),t.forEach(o),v=a(e),d=c(e,"P",{}),j(d).forEach(o),p=a(e),T=c(e,"P",{"data-svelte-h":!0}),g(T)!=="svelte-ucuku3"&&(T.innerHTML=r),w=a(e),f(E.$$.fragment,e),Ze=a(e),k=c(e,"DIV",{class:!0,"data-svelte-h":!0}),g(k)!=="svelte-13t8s2t"&&(k.innerHTML=Jt),je=a(e),f(q.$$.fragment,e),ke=a(e),L=c(e,"P",{"data-svelte-h":!0}),g(L)!=="svelte-1ottgdf"&&(L.innerHTML=St),Pe=a(e),A=c(e,"P",{"data-svelte-h":!0}),g(A)!=="svelte-vfdo9a"&&(A.textContent=It),Ne=a(e),O=c(e,"P",{"data-svelte-h":!0}),g(O)!=="svelte-1wjy0wn"&&(O.innerHTML=Ut),Ve=a(e),D=c(e,"P",{"data-svelte-h":!0}),g(D)!=="svelte-axv494"&&(D.textContent=Ct),Fe=a(e),K=c(e,"UL",{"data-svelte-h":!0}),g(K)!=="svelte-1rpfnm5"&&(K.innerHTML=xt),Be=a(e),ee=c(e,"P",{"data-svelte-h":!0}),g(ee)!=="svelte-7cf6zy"&&(ee.textContent=$t),Re=a(e),f(te.$$.fragment,e),Xe=a(e),oe=c(e,"P",{"data-svelte-h":!0}),g(oe)!=="svelte-ymhs7r"&&(oe.innerHTML=zt),He=a(e),f(ne.$$.fragment,e),Ye=a(e),J=c(e,"DIV",{class:!0});var $=j(J);f(se.$$.fragment,$),ot=a($),be=c($,"P",{"data-svelte-h":!0}),g(be)!=="svelte-1mjawz1"&&(be.innerHTML=Wt),nt=a($),ye=c($,"P",{"data-svelte-h":!0}),g(ye)!=="svelte-1ek1ss9"&&(ye.innerHTML=Zt),st=a($),f(P.$$.fragment,$),$.forEach(o),Qe=a(e),f(ae.$$.fragment,e),Ee=a(e),S=c(e,"DIV",{class:!0});var z=j(S);f(re.$$.fragment,z),at=a(z),Te=c(z,"P",{"data-svelte-h":!0}),g(Te)!=="svelte-1agvcs2"&&(Te.textContent=jt),rt=a(z),N=c(z,"DIV",{class:!0});var ue=j(N);f(ie.$$.fragment,ue),it=a(ue),ve=c(ue,"P",{"data-svelte-h":!0}),g(ve)!=="svelte-1x3yxsa"&&(ve.textContent=kt),ue.forEach(o),lt=a(z),V=c(z,"DIV",{class:!0});var _e=j(V);f(le.$$.fragment,_e),mt=a(_e),we=c(_e,"P",{"data-svelte-h":!0}),g(we)!=="svelte-1wij86i"&&(we.innerHTML=Pt),_e.forEach(o),z.forEach(o),qe=a(e),f(me.$$.fragment,e),Le=a(e),G=c(e,"DIV",{class:!0});var I=j(G);f(ce.$$.fragment,I),ct=a(I),Ge=c(I,"P",{"data-svelte-h":!0}),g(Ge)!=="svelte-1c0dhfy"&&(Ge.textContent=Nt),dt=a(I),Me=c(I,"P",{"data-svelte-h":!0}),g(Me)!=="svelte-q52n56"&&(Me.innerHTML=Vt),pt=a(I),Je=c(I,"P",{"data-svelte-h":!0}),g(Je)!=="svelte-hswkmf"&&(Je.innerHTML=Ft),gt=a(I),C=c(I,"DIV",{class:!0});var H=j(C);f(de.$$.fragment,H),ht=a(H),Se=c(H,"P",{"data-svelte-h":!0}),g(Se)!=="svelte-z1ahnt"&&(Se.innerHTML=Bt),ft=a(H),f(F.$$.fragment,H),ut=a(H),f(B.$$.fragment,H),H.forEach(o),I.forEach(o),Ae=a(e),f(pe.$$.fragment,e),Oe=a(e),M=c(e,"DIV",{class:!0});var Z=j(M);f(ge.$$.fragment,Z),_t=a(Z),Ie=c(Z,"P",{"data-svelte-h":!0}),g(Ie)!=="svelte-1jeiktv"&&(Ie.textContent=Rt),bt=a(Z),Ue=c(Z,"P",{"data-svelte-h":!0}),g(Ue)!=="svelte-q52n56"&&(Ue.innerHTML=Xt),yt=a(Z),Ce=c(Z,"P",{"data-svelte-h":!0}),g(Ce)!=="svelte-hswkmf"&&(Ce.innerHTML=Ht),Tt=a(Z),x=c(Z,"DIV",{class:!0});var Y=j(x);f(he.$$.fragment,Y),vt=a(Y),xe=c(Y,"P",{"data-svelte-h":!0}),g(xe)!=="svelte-trttvl"&&(xe.innerHTML=Yt),wt=a(Y),f(R.$$.fragment,Y),Gt=a(Y),f(X.$$.fragment,Y),Y.forEach(o),Z.forEach(o),De=a(e),f(fe.$$.fragment,e),Ke=a(e),ze=c(e,"P",{}),j(ze).forEach(o),this.h()},h(){W(n,"name","hf:doc:metadata"),W(n,"content",ao),W(k,"class","flex flex-wrap space-x-1"),W(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){l(document.head,n),i(e,v,t),i(e,d,t),i(e,p,t),i(e,T,t),i(e,w,t),u(E,e,t),i(e,Ze,t),i(e,k,t),i(e,je,t),u(q,e,t),i(e,ke,t),i(e,L,t),i(e,Pe,t),i(e,A,t),i(e,Ne,t),i(e,O,t),i(e,Ve,t),i(e,D,t),i(e,Fe,t),i(e,K,t),i(e,Be,t),i(e,ee,t),i(e,Re,t),u(te,e,t),i(e,Xe,t),i(e,oe,t),i(e,He,t),u(ne,e,t),i(e,Ye,t),i(e,J,t),u(se,J,null),l(J,ot),l(J,be),l(J,nt),l(J,ye),l(J,st),u(P,J,null),i(e,Qe,t),u(ae,e,t),i(e,Ee,t),i(e,S,t),u(re,S,null),l(S,at),l(S,Te),l(S,rt),l(S,N),u(ie,N,null),l(N,it),l(N,ve),l(S,lt),l(S,V),u(le,V,null),l(V,mt),l(V,we),i(e,qe,t),u(me,e,t),i(e,Le,t),i(e,G,t),u(ce,G,null),l(G,ct),l(G,Ge),l(G,dt),l(G,Me),l(G,pt),l(G,Je),l(G,gt),l(G,C),u(de,C,null),l(C,ht),l(C,Se),l(C,ft),u(F,C,null),l(C,ut),u(B,C,null),i(e,Ae,t),u(pe,e,t),i(e,Oe,t),i(e,M,t),u(ge,M,null),l(M,_t),l(M,Ie),l(M,bt),l(M,Ue),l(M,yt),l(M,Ce),l(M,Tt),l(M,x),u(he,x,null),l(x,vt),l(x,xe),l(x,wt),u(R,x,null),l(x,Gt),u(X,x,null),i(e,De,t),u(fe,e,t),i(e,Ke,t),i(e,ze,t),et=!0},p(e,[t]){const $={};t&2&&($.$$scope={dirty:t,ctx:e}),P.$set($);const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),F.$set(z);const ue={};t&2&&(ue.$$scope={dirty:t,ctx:e}),B.$set(ue);const _e={};t&2&&(_e.$$scope={dirty:t,ctx:e}),R.$set(_e);const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),X.$set(I)},i(e){et||(_(E.$$.fragment,e),_(q.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(P.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(me.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(F.$$.fragment,e),_(B.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(he.$$.fragment,e),_(R.$$.fragment,e),_(X.$$.fragment,e),_(fe.$$.fragment,e),et=!0)},o(e){b(E.$$.fragment,e),b(q.$$.fragment,e),b(te.$$.fragment,e),b(ne.$$.fragment,e),b(se.$$.fragment,e),b(P.$$.fragment,e),b(ae.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(le.$$.fragment,e),b(me.$$.fragment,e),b(ce.$$.fragment,e),b(de.$$.fragment,e),b(F.$$.fragment,e),b(B.$$.fragment,e),b(pe.$$.fragment,e),b(ge.$$.fragment,e),b(he.$$.fragment,e),b(R.$$.fragment,e),b(X.$$.fragment,e),b(fe.$$.fragment,e),et=!1},d(e){e&&(o(v),o(d),o(p),o(T),o(w),o(Ze),o(k),o(je),o(ke),o(L),o(Pe),o(A),o(Ne),o(O),o(Ve),o(D),o(Fe),o(K),o(Be),o(ee),o(Re),o(Xe),o(oe),o(He),o(Ye),o(J),o(Qe),o(Ee),o(S),o(qe),o(Le),o(G),o(Ae),o(Oe),o(M),o(De),o(Ke),o(ze)),o(n),y(E,e),y(q,e),y(te,e),y(ne,e),y(se),y(P),y(ae,e),y(re),y(ie),y(le),y(me,e),y(ce),y(de),y(F),y(B),y(pe,e),y(ge),y(he),y(R),y(X),y(fe,e)}}}const ao='{"title":"SegGPT","local":"seggpt","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"SegGptConfig","local":"transformers.SegGptConfig","sections":[],"depth":2},{"title":"SegGptImageProcessor","local":"transformers.SegGptImageProcessor","sections":[],"depth":2},{"title":"SegGptModel","local":"transformers.SegGptModel","sections":[],"depth":2},{"title":"SegGptForImageSegmentation","local":"transformers.SegGptForImageSegmentation","sections":[],"depth":2}],"depth":1}';function ro(U){return qt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fo extends Lt{constructor(n){super(),At(this,n,ro,so,Et,{})}}export{fo as component};
