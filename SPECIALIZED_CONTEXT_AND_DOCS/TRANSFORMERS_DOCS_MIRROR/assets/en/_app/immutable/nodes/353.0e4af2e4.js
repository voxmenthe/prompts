import{s as Gt,o as qt,n as tt}from"../chunks/scheduler.18a86fab.js";import{S as Yt,i as St,g as i,s,r as h,A as Xt,h as l,f as o,c as a,j as pe,x as m,u,k as z,y as c,a as n,v as g,d as v,t as _,w as b}from"../chunks/index.98837b22.js";import{T as Ht}from"../chunks/Tip.77304350.js";import{D as $e}from"../chunks/Docstring.a1ef7999.js";import{C as et}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Lt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as fe,E as Et}from"../chunks/getInferenceSnippets.06c2775f.js";function At(x){let r,y="Example:",p,f,T;return f=new et({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFB2dFYyTW9kZWwlMkMlMjBQdnRWMkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBwdnRfdjJfYjAlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwUHZ0VjJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjBPcGVuR1ZMYWIlMkZwdnRfdjJfYjAlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFB2dFYyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PvtV2Model, PvtV2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a pvt_v2_b0 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = PvtV2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the OpenGVLab/pvt_v2_b0 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PvtV2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,p=s(),h(f.$$.fragment)},l(d){r=l(d,"P",{"data-svelte-h":!0}),m(r)!=="svelte-11lpom8"&&(r.textContent=y),p=a(d),u(f.$$.fragment,d)},m(d,C){n(d,r,C),n(d,p,C),g(f,d,C),T=!0},p:tt,i(d){T||(v(f.$$.fragment,d),T=!0)},o(d){_(f.$$.fragment,d),T=!1},d(d){d&&(o(r),o(p)),b(f,d)}}}function Qt(x){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(p,f){n(p,r,f)},p:tt,d(p){p&&o(r)}}}function Dt(x){let r,y="Example:",p,f,T;return f=new et({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFB2dFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyT3BlbkdWTGFiJTJGcHZ0X3YyX2IwJTIyKSUwQW1vZGVsJTIwJTNEJTIwUHZ0VjJGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJPcGVuR1ZMYWIlMkZwdnRfdjJfYjAlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, PvtV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;OpenGVLab/pvt_v2_b0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PvtV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;OpenGVLab/pvt_v2_b0&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,p=s(),h(f.$$.fragment)},l(d){r=l(d,"P",{"data-svelte-h":!0}),m(r)!=="svelte-11lpom8"&&(r.textContent=y),p=a(d),u(f.$$.fragment,d)},m(d,C){n(d,r,C),n(d,p,C),g(f,d,C),T=!0},p:tt,i(d){T||(v(f.$$.fragment,d),T=!0)},o(d){_(f.$$.fragment,d),T=!1},d(d){d&&(o(r),o(p)),b(f,d)}}}function Ot(x){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(p,f){n(p,r,f)},p:tt,d(p){p&&o(r)}}}function Kt(x){let r,y,p,f,T,d="<em>This model was released on 2021-06-25 and added to Hugging Face Transformers on 2024-03-13.</em>",C,N,Ce,U,_t='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Je,R,Ie,H,bt=`The PVTv2 model was proposed in
<a href="https://huggingface.co/papers/2106.13797" rel="nofollow">PVT v2: Improved Baselines with Pyramid Vision Transformer</a> by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. As an improved variant of PVT, it eschews position embeddings, relying instead on positional information encoded through zero-padding and overlapping patch embeddings. This lack of reliance on position embeddings simplifies the architecture, and enables running inference at any resolution without needing to interpolate them.`,je,L,Tt='The PVTv2 encoder structure has been successfully deployed to achieve state-of-the-art scores in <a href="https://huggingface.co/papers/2105.15203" rel="nofollow">Segformer</a> for semantic segmentation, <a href="https://huggingface.co/papers/2201.07436" rel="nofollow">GLPN</a> for monocular depth, and <a href="https://huggingface.co/papers/2109.03814" rel="nofollow">Panoptic Segformer</a> for panoptic segmentation.',xe,G,yt='PVTv2 belongs to a family of models called <a href="https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f" rel="nofollow">hierarchical transformers</a> , which make adaptations to transformer layers in order to generate multi-scale feature maps. Unlike the columnal structure of Vision Transformer (<a href="https://huggingface.co/papers/2010.11929" rel="nofollow">ViT</a>) which loses fine-grained detail, multi-scale feature maps are known preserve this detail and aid performance in dense prediction tasks. In the case of PVTv2, this is achieved by generating image patch tokens using 2D convolution with overlapping kernels in each encoder layer.',Ze,q,wt="The multi-scale features of hierarchical transformers allow them to be easily swapped in for traditional workhorse computer vision backbone models like ResNet in larger architectures. Both Segformer and Panoptic Segformer demonstrated that configurations using PVTv2 for a backbone consistently outperformed those with similarly sized ResNet backbones.",Ue,Y,Mt="Another powerful feature of the PVTv2 is the complexity reduction in the self-attention layers called Spatial Reduction Attention (SRA), which uses 2D convolution layers to project hidden states to a smaller resolution before attending to them with the queries, improving the $O(n^2)$ complexity of self-attention to $O(n^2/R)$, with $R$ being the spatial reduction ratio (<code>sr_ratio</code>, aka kernel size and stride in the 2D convolution).",We,S,Pt="SRA was introduced in PVT, and is the default attention complexity reduction method used in PVTv2. However, PVTv2 also introduced the option of using a self-attention mechanism with linear complexity related to image size, which they called “Linear SRA”. This method uses average pooling to reduce the hidden states to a fixed size that is invariant to their original resolution (although this is inherently more lossy than regular SRA). This option can be enabled by setting <code>linear_attention</code> to <code>True</code> in the PVTv2Config.",ke,X,Be,E,Vt='<em>Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVT v2 achieves comparable or better performances than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at <a href="https://github.com/whai362/PVT" rel="nofollow">https://github.com/whai362/PVT</a>.</em>',Fe,A,$t='This model was contributed by <a href="https://huggingface.co/FoamoftheSea" rel="nofollow">FoamoftheSea</a>. The original code can be found <a href="https://github.com/whai362/PVT" rel="nofollow">here</a>.',ze,Q,Ne,D,Ct='<li><p><a href="https://huggingface.co/papers/2106.13797" rel="nofollow">PVTv2</a> is a hierarchical transformer model which has demonstrated powerful performance in image classification and multiple other tasks, used as a backbone for semantic segmentation in <a href="https://huggingface.co/papers/2105.15203" rel="nofollow">Segformer</a>, monocular depth estimation in <a href="https://huggingface.co/papers/2201.07436" rel="nofollow">GLPN</a>, and panoptic segmentation in <a href="https://huggingface.co/papers/2109.03814" rel="nofollow">Panoptic Segformer</a>, consistently showing higher performance than similar ResNet configurations.</p></li> <li><p>Hierarchical transformers like PVTv2 achieve superior data and parameter efficiency on image data compared with pure transformer architectures by incorporating design elements of convolutional neural networks (CNNs) into their encoders. This creates a best-of-both-worlds architecture that infuses the useful inductive biases of CNNs like translation equivariance and locality into the network while still enjoying the benefits of dynamic data response and global relationship modeling provided by the self-attention mechanism of <a href="https://huggingface.co/papers/1706.03762" rel="nofollow">transformers</a>.</p></li> <li><p>PVTv2 uses overlapping patch embeddings to create multi-scale feature maps, which are infused with location information using zero-padding and depth-wise convolutions.</p></li> <li><p>To reduce the complexity in the attention layers, PVTv2 performs a spatial reduction on the hidden states using either strided 2D convolution (SRA) or fixed-size average pooling (Linear SRA). Although inherently more lossy, Linear SRA provides impressive performance with a linear complexity with respect to image size. To use Linear SRA in the self-attention layers, set <code>linear_attention=True</code> in the <code>PvtV2Config</code>.</p></li> <li><p><a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Model">PvtV2Model</a> is the hierarchical transformer encoder (which is also often referred to as Mix Transformer or MiT in the literature). <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification">PvtV2ForImageClassification</a> adds a simple classifier head on top to perform Image Classification. <code>PvtV2Backbone</code> can be used with the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a> system in larger architectures like Deformable DETR.</p></li> <li><p>ImageNet pretrained weights for all model sizes can be found on the <a href="https://huggingface.co/models?other=pvt_v2" rel="nofollow">hub</a>.</p> <p>The best way to get started with the PVTv2 is to load the pretrained checkpoint with the size of your choosing using <code>AutoModelForImageClassification</code>:</p></li>',Re,O,He,K,Jt="To use the PVTv2 as a backbone for more complex architectures like DeformableDETR, you can use AutoBackbone (this model would need fine-tuning as you’re replacing the backbone in the pretrained model):",Le,ee,Ge,te,It='<a href="https://github.com/whai362/PVT/tree/v2" rel="nofollow">PVTv2</a> performance on ImageNet-1K by model size (B0-B5):',qe,oe,jt='<thead><tr><th>Method</th> <th align="center">Size</th> <th align="center">Acc@1</th> <th align="center">#Params (M)</th></tr></thead> <tbody><tr><td>PVT-V2-B0</td> <td align="center">224</td> <td align="center">70.5</td> <td align="center">3.7</td></tr> <tr><td>PVT-V2-B1</td> <td align="center">224</td> <td align="center">78.7</td> <td align="center">14.0</td></tr> <tr><td>PVT-V2-B2-Linear</td> <td align="center">224</td> <td align="center">82.1</td> <td align="center">22.6</td></tr> <tr><td>PVT-V2-B2</td> <td align="center">224</td> <td align="center">82.0</td> <td align="center">25.4</td></tr> <tr><td>PVT-V2-B3</td> <td align="center">224</td> <td align="center">83.1</td> <td align="center">45.2</td></tr> <tr><td>PVT-V2-B4</td> <td align="center">224</td> <td align="center">83.6</td> <td align="center">62.6</td></tr> <tr><td>PVT-V2-B5</td> <td align="center">224</td> <td align="center">83.8</td> <td align="center">82.0</td></tr></tbody>',Ye,ne,Se,P,se,ot,he,xt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Model">PvtV2Model</a>. It is used to instantiate a Pvt V2
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Pvt V2 B0
<a href="https://huggingface.co/OpenGVLab/pvt_v2_b0" rel="nofollow">OpenGVLab/pvt_v2_b0</a> architecture.`,nt,ue,Zt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,st,W,Xe,ae,Ee,w,re,at,ge,Ut=`Pvt-v2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,rt,ve,Wt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,it,_e,kt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,lt,J,ie,dt,be,Bt='The <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification">PvtV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',ct,k,mt,B,Ae,le,Qe,M,de,pt,Te,Ft="The bare Pvt V2 Model outputting raw hidden-states without any specific head on top.",ft,ye,zt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ht,we,Nt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ut,Z,ce,gt,Me,Rt='The <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Model">PvtV2Model</a> forward method, overrides the <code>__call__</code> special method.',vt,F,De,me,Oe,Ve,Ke;return N=new fe({props:{title:"Pyramid Vision Transformer V2 (PVTv2)",local:"pyramid-vision-transformer-v2-pvtv2",headingTag:"h1"}}),R=new fe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),X=new fe({props:{title:"Abstract from the paper:",local:"abstract-from-the-paper",headingTag:"h3"}}),Q=new fe({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),O=new et({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTJDJTIwQXV0b0ltYWdlUHJvY2Vzc29yJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJPcGVuR1ZMYWIlMkZwdnRfdjJfYjAlMjIpJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJPcGVuR1ZMYWIlMkZwdnRfdjJfYjAlMjIpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXByb2Nlc3NlZCUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwodG9yY2gudGVuc29yKHByb2Nlc3NlZCU1QiUyMnBpeGVsX3ZhbHVlcyUyMiU1RCkp",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, AutoImageProcessor
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;OpenGVLab/pvt_v2_b0&quot;</span>)
image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;OpenGVLab/pvt_v2_b0&quot;</span>)
url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
processed = image_processor(image)
outputs = model(torch.tensor(processed[<span class="hljs-string">&quot;pixel_values&quot;</span>]))`,wrap:!1}}),ee=new et({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uJTJDJTIwQXV0b0ltYWdlUHJvY2Vzc29yJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24uZnJvbV9jb25maWcoJTBBJTIwJTIwJTIwJTIwY29uZmlnJTNEQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyU2Vuc2VUaW1lJTJGZGVmb3JtYWJsZS1kZXRyJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYmFja2JvbmVfY29uZmlnJTNEQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyT3BlbkdWTGFiJTJGcHZ0X3YyX2I1JTIyKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHVzZV90aW1tX2JhY2tib25lJTNERmFsc2UlMEElMjAlMjAlMjAlMjApJTJDJTBBKSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2Vuc2VUaW1lJTJGZGVmb3JtYWJsZS1kZXRyJTIyKSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFwcm9jZXNzZWQlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHRvcmNoLnRlbnNvcihwcm9jZXNzZWQlNUIlMjJwaXhlbF92YWx1ZXMlMjIlNUQpKQ==",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection, AutoImageProcessor
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

model = AutoModelForObjectDetection.from_config(
    config=AutoConfig.from_pretrained(
        <span class="hljs-string">&quot;SenseTime/deformable-detr&quot;</span>,
        backbone_config=AutoConfig.from_pretrained(<span class="hljs-string">&quot;OpenGVLab/pvt_v2_b5&quot;</span>),
        use_timm_backbone=<span class="hljs-literal">False</span>
    ),
)

image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;SenseTime/deformable-detr&quot;</span>)
url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
processed = image_processor(image)
outputs = model(torch.tensor(processed[<span class="hljs-string">&quot;pixel_values&quot;</span>]))`,wrap:!1}}),ne=new fe({props:{title:"PvtV2Config",local:"transformers.PvtV2Config",headingTag:"h2"}}),se=new $e({props:{name:"class transformers.PvtV2Config",anchor:"transformers.PvtV2Config",parameters:[{name:"image_size",val:": typing.Union[int, tuple[int, int]] = 224"},{name:"num_channels",val:": int = 3"},{name:"num_encoder_blocks",val:": int = 4"},{name:"depths",val:": list = [2, 2, 2, 2]"},{name:"sr_ratios",val:": list = [8, 4, 2, 1]"},{name:"hidden_sizes",val:": list = [32, 64, 160, 256]"},{name:"patch_sizes",val:": list = [7, 3, 3, 3]"},{name:"strides",val:": list = [4, 2, 2, 2]"},{name:"num_attention_heads",val:": list = [1, 2, 5, 8]"},{name:"mlp_ratios",val:": list = [8, 8, 4, 4]"},{name:"hidden_act",val:": typing.Union[str, typing.Callable] = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"drop_path_rate",val:": float = 0.0"},{name:"layer_norm_eps",val:": float = 1e-06"},{name:"qkv_bias",val:": bool = True"},{name:"linear_attention",val:": bool = False"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PvtV2Config.image_size",description:`<strong>image_size</strong> (<code>Union[int, tuple[int, int]]</code>, <em>optional</em>, defaults to 224) &#x2014;
The input image size. Pass int value for square image, or tuple of (height, width).`,name:"image_size"},{anchor:"transformers.PvtV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.PvtV2Config.num_encoder_blocks",description:`<strong>num_encoder_blocks</strong> (<code>[int]</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of encoder blocks (i.e. stages in the Mix Transformer encoder).`,name:"num_encoder_blocks"},{anchor:"transformers.PvtV2Config.depths",description:`<strong>depths</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 2, 2, 2]</code>) &#x2014;
The number of layers in each encoder block.`,name:"depths"},{anchor:"transformers.PvtV2Config.sr_ratios",description:`<strong>sr_ratios</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[8, 4, 2, 1]</code>) &#x2014;
Spatial reduction ratios in each encoder block.`,name:"sr_ratios"},{anchor:"transformers.PvtV2Config.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[32, 64, 160, 256]</code>) &#x2014;
Dimension of each of the encoder blocks.`,name:"hidden_sizes"},{anchor:"transformers.PvtV2Config.patch_sizes",description:`<strong>patch_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3, 3]</code>) &#x2014;
Patch size for overlapping patch embedding before each encoder block.`,name:"patch_sizes"},{anchor:"transformers.PvtV2Config.strides",description:`<strong>strides</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2, 2]</code>) &#x2014;
Stride for overlapping patch embedding before each encoder block.`,name:"strides"},{anchor:"transformers.PvtV2Config.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 5, 8]</code>) &#x2014;
Number of attention heads for each attention layer in each block of the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.PvtV2Config.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4, 4]</code>) &#x2014;
Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the
encoder blocks.`,name:"mlp_ratios"},{anchor:"transformers.PvtV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.PvtV2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.PvtV2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.PvtV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.PvtV2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.`,name:"drop_path_rate"},{anchor:"transformers.PvtV2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.PvtV2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not a learnable bias should be added to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.PvtV2Config.linear_attention",description:`<strong>linear_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Use linear attention complexity. If set to True, <code>sr_ratio</code> is ignored and average pooling is used for
dimensionality reduction in the attention layers rather than strided convolution.`,name:"linear_attention"},{anchor:"transformers.PvtV2Config.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage.`,name:"out_features"},{anchor:"transformers.PvtV2Config.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage.`,name:"out_indices"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/pvt_v2/configuration_pvt_v2.py#L29"}}),W=new Lt({props:{anchor:"transformers.PvtV2Config.example",$$slots:{default:[At]},$$scope:{ctx:x}}}),ae=new fe({props:{title:"PvtForImageClassification",local:"transformers.PvtV2ForImageClassification",headingTag:"h2"}}),re=new $e({props:{name:"class transformers.PvtV2ForImageClassification",anchor:"transformers.PvtV2ForImageClassification",parameters:[{name:"config",val:": PvtV2Config"}],parametersDescription:[{anchor:"transformers.PvtV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config">PvtV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/pvt_v2/modeling_pvt_v2.py#L473"}}),ie=new $e({props:{name:"forward",anchor:"transformers.PvtV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor]"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.PvtV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">PvtImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.PvtV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.PvtV2ForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PvtV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PvtV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/pvt_v2/modeling_pvt_v2.py#L488",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config"
>PvtV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),k=new Ht({props:{$$slots:{default:[Qt]},$$scope:{ctx:x}}}),B=new Lt({props:{anchor:"transformers.PvtV2ForImageClassification.forward.example",$$slots:{default:[Dt]},$$scope:{ctx:x}}}),le=new fe({props:{title:"PvtModel",local:"transformers.PvtV2Model",headingTag:"h2"}}),de=new $e({props:{name:"class transformers.PvtV2Model",anchor:"transformers.PvtV2Model",parameters:[{name:"config",val:": PvtV2Config"}],parametersDescription:[{anchor:"transformers.PvtV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config">PvtV2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/pvt_v2/modeling_pvt_v2.py#L416"}}),ce=new $e({props:{name:"forward",anchor:"transformers.PvtV2Model.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.PvtV2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">PvtImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.PvtV2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PvtV2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PvtV2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/pvt_v2/modeling_pvt_v2.py#L435",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config"
>PvtV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),F=new Ht({props:{$$slots:{default:[Ot]},$$scope:{ctx:x}}}),me=new Et({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pvt_v2.md"}}),{c(){r=i("meta"),y=s(),p=i("p"),f=s(),T=i("p"),T.innerHTML=d,C=s(),h(N.$$.fragment),Ce=s(),U=i("div"),U.innerHTML=_t,Je=s(),h(R.$$.fragment),Ie=s(),H=i("p"),H.innerHTML=bt,je=s(),L=i("p"),L.innerHTML=Tt,xe=s(),G=i("p"),G.innerHTML=yt,Ze=s(),q=i("p"),q.textContent=wt,Ue=s(),Y=i("p"),Y.innerHTML=Mt,We=s(),S=i("p"),S.innerHTML=Pt,ke=s(),h(X.$$.fragment),Be=s(),E=i("p"),E.innerHTML=Vt,Fe=s(),A=i("p"),A.innerHTML=$t,ze=s(),h(Q.$$.fragment),Ne=s(),D=i("ul"),D.innerHTML=Ct,Re=s(),h(O.$$.fragment),He=s(),K=i("p"),K.textContent=Jt,Le=s(),h(ee.$$.fragment),Ge=s(),te=i("p"),te.innerHTML=It,qe=s(),oe=i("table"),oe.innerHTML=jt,Ye=s(),h(ne.$$.fragment),Se=s(),P=i("div"),h(se.$$.fragment),ot=s(),he=i("p"),he.innerHTML=xt,nt=s(),ue=i("p"),ue.innerHTML=Zt,st=s(),h(W.$$.fragment),Xe=s(),h(ae.$$.fragment),Ee=s(),w=i("div"),h(re.$$.fragment),at=s(),ge=i("p"),ge.textContent=Ut,rt=s(),ve=i("p"),ve.innerHTML=Wt,it=s(),_e=i("p"),_e.innerHTML=kt,lt=s(),J=i("div"),h(ie.$$.fragment),dt=s(),be=i("p"),be.innerHTML=Bt,ct=s(),h(k.$$.fragment),mt=s(),h(B.$$.fragment),Ae=s(),h(le.$$.fragment),Qe=s(),M=i("div"),h(de.$$.fragment),pt=s(),Te=i("p"),Te.textContent=Ft,ft=s(),ye=i("p"),ye.innerHTML=zt,ht=s(),we=i("p"),we.innerHTML=Nt,ut=s(),Z=i("div"),h(ce.$$.fragment),gt=s(),Me=i("p"),Me.innerHTML=Rt,vt=s(),h(F.$$.fragment),De=s(),h(me.$$.fragment),Oe=s(),Ve=i("p"),this.h()},l(e){const t=Xt("svelte-u9bgzb",document.head);r=l(t,"META",{name:!0,content:!0}),t.forEach(o),y=a(e),p=l(e,"P",{}),pe(p).forEach(o),f=a(e),T=l(e,"P",{"data-svelte-h":!0}),m(T)!=="svelte-d0asib"&&(T.innerHTML=d),C=a(e),u(N.$$.fragment,e),Ce=a(e),U=l(e,"DIV",{class:!0,"data-svelte-h":!0}),m(U)!=="svelte-13t8s2t"&&(U.innerHTML=_t),Je=a(e),u(R.$$.fragment,e),Ie=a(e),H=l(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-1u1iutv"&&(H.innerHTML=bt),je=a(e),L=l(e,"P",{"data-svelte-h":!0}),m(L)!=="svelte-19a7x0h"&&(L.innerHTML=Tt),xe=a(e),G=l(e,"P",{"data-svelte-h":!0}),m(G)!=="svelte-ywsxnj"&&(G.innerHTML=yt),Ze=a(e),q=l(e,"P",{"data-svelte-h":!0}),m(q)!=="svelte-lqldog"&&(q.textContent=wt),Ue=a(e),Y=l(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-hpdejr"&&(Y.innerHTML=Mt),We=a(e),S=l(e,"P",{"data-svelte-h":!0}),m(S)!=="svelte-c9tslx"&&(S.innerHTML=Pt),ke=a(e),u(X.$$.fragment,e),Be=a(e),E=l(e,"P",{"data-svelte-h":!0}),m(E)!=="svelte-1mx0m5v"&&(E.innerHTML=Vt),Fe=a(e),A=l(e,"P",{"data-svelte-h":!0}),m(A)!=="svelte-9en9f"&&(A.innerHTML=$t),ze=a(e),u(Q.$$.fragment,e),Ne=a(e),D=l(e,"UL",{"data-svelte-h":!0}),m(D)!=="svelte-1jmnto1"&&(D.innerHTML=Ct),Re=a(e),u(O.$$.fragment,e),He=a(e),K=l(e,"P",{"data-svelte-h":!0}),m(K)!=="svelte-8pq4o0"&&(K.textContent=Jt),Le=a(e),u(ee.$$.fragment,e),Ge=a(e),te=l(e,"P",{"data-svelte-h":!0}),m(te)!=="svelte-sjiatm"&&(te.innerHTML=It),qe=a(e),oe=l(e,"TABLE",{"data-svelte-h":!0}),m(oe)!=="svelte-qmb8e6"&&(oe.innerHTML=jt),Ye=a(e),u(ne.$$.fragment,e),Se=a(e),P=l(e,"DIV",{class:!0});var I=pe(P);u(se.$$.fragment,I),ot=a(I),he=l(I,"P",{"data-svelte-h":!0}),m(he)!=="svelte-vi0os2"&&(he.innerHTML=xt),nt=a(I),ue=l(I,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-1ek1ss9"&&(ue.innerHTML=Zt),st=a(I),u(W.$$.fragment,I),I.forEach(o),Xe=a(e),u(ae.$$.fragment,e),Ee=a(e),w=l(e,"DIV",{class:!0});var V=pe(w);u(re.$$.fragment,V),at=a(V),ge=l(V,"P",{"data-svelte-h":!0}),m(ge)!=="svelte-hqhjf4"&&(ge.textContent=Ut),rt=a(V),ve=l(V,"P",{"data-svelte-h":!0}),m(ve)!=="svelte-q52n56"&&(ve.innerHTML=Wt),it=a(V),_e=l(V,"P",{"data-svelte-h":!0}),m(_e)!=="svelte-hswkmf"&&(_e.innerHTML=kt),lt=a(V),J=l(V,"DIV",{class:!0});var j=pe(J);u(ie.$$.fragment,j),dt=a(j),be=l(j,"P",{"data-svelte-h":!0}),m(be)!=="svelte-gfzyp4"&&(be.innerHTML=Bt),ct=a(j),u(k.$$.fragment,j),mt=a(j),u(B.$$.fragment,j),j.forEach(o),V.forEach(o),Ae=a(e),u(le.$$.fragment,e),Qe=a(e),M=l(e,"DIV",{class:!0});var $=pe(M);u(de.$$.fragment,$),pt=a($),Te=l($,"P",{"data-svelte-h":!0}),m(Te)!=="svelte-nrky1e"&&(Te.textContent=Ft),ft=a($),ye=l($,"P",{"data-svelte-h":!0}),m(ye)!=="svelte-q52n56"&&(ye.innerHTML=zt),ht=a($),we=l($,"P",{"data-svelte-h":!0}),m(we)!=="svelte-hswkmf"&&(we.innerHTML=Nt),ut=a($),Z=l($,"DIV",{class:!0});var Pe=pe(Z);u(ce.$$.fragment,Pe),gt=a(Pe),Me=l(Pe,"P",{"data-svelte-h":!0}),m(Me)!=="svelte-1f3jsiq"&&(Me.innerHTML=Rt),vt=a(Pe),u(F.$$.fragment,Pe),Pe.forEach(o),$.forEach(o),De=a(e),u(me.$$.fragment,e),Oe=a(e),Ve=l(e,"P",{}),pe(Ve).forEach(o),this.h()},h(){z(r,"name","hf:doc:metadata"),z(r,"content",eo),z(U,"class","flex flex-wrap space-x-1"),z(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,r),n(e,y,t),n(e,p,t),n(e,f,t),n(e,T,t),n(e,C,t),g(N,e,t),n(e,Ce,t),n(e,U,t),n(e,Je,t),g(R,e,t),n(e,Ie,t),n(e,H,t),n(e,je,t),n(e,L,t),n(e,xe,t),n(e,G,t),n(e,Ze,t),n(e,q,t),n(e,Ue,t),n(e,Y,t),n(e,We,t),n(e,S,t),n(e,ke,t),g(X,e,t),n(e,Be,t),n(e,E,t),n(e,Fe,t),n(e,A,t),n(e,ze,t),g(Q,e,t),n(e,Ne,t),n(e,D,t),n(e,Re,t),g(O,e,t),n(e,He,t),n(e,K,t),n(e,Le,t),g(ee,e,t),n(e,Ge,t),n(e,te,t),n(e,qe,t),n(e,oe,t),n(e,Ye,t),g(ne,e,t),n(e,Se,t),n(e,P,t),g(se,P,null),c(P,ot),c(P,he),c(P,nt),c(P,ue),c(P,st),g(W,P,null),n(e,Xe,t),g(ae,e,t),n(e,Ee,t),n(e,w,t),g(re,w,null),c(w,at),c(w,ge),c(w,rt),c(w,ve),c(w,it),c(w,_e),c(w,lt),c(w,J),g(ie,J,null),c(J,dt),c(J,be),c(J,ct),g(k,J,null),c(J,mt),g(B,J,null),n(e,Ae,t),g(le,e,t),n(e,Qe,t),n(e,M,t),g(de,M,null),c(M,pt),c(M,Te),c(M,ft),c(M,ye),c(M,ht),c(M,we),c(M,ut),c(M,Z),g(ce,Z,null),c(Z,gt),c(Z,Me),c(Z,vt),g(F,Z,null),n(e,De,t),g(me,e,t),n(e,Oe,t),n(e,Ve,t),Ke=!0},p(e,[t]){const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),W.$set(I);const V={};t&2&&(V.$$scope={dirty:t,ctx:e}),k.$set(V);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),B.$set(j);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),F.$set($)},i(e){Ke||(v(N.$$.fragment,e),v(R.$$.fragment,e),v(X.$$.fragment,e),v(Q.$$.fragment,e),v(O.$$.fragment,e),v(ee.$$.fragment,e),v(ne.$$.fragment,e),v(se.$$.fragment,e),v(W.$$.fragment,e),v(ae.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(k.$$.fragment,e),v(B.$$.fragment,e),v(le.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(F.$$.fragment,e),v(me.$$.fragment,e),Ke=!0)},o(e){_(N.$$.fragment,e),_(R.$$.fragment,e),_(X.$$.fragment,e),_(Q.$$.fragment,e),_(O.$$.fragment,e),_(ee.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(W.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(k.$$.fragment,e),_(B.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(F.$$.fragment,e),_(me.$$.fragment,e),Ke=!1},d(e){e&&(o(y),o(p),o(f),o(T),o(C),o(Ce),o(U),o(Je),o(Ie),o(H),o(je),o(L),o(xe),o(G),o(Ze),o(q),o(Ue),o(Y),o(We),o(S),o(ke),o(Be),o(E),o(Fe),o(A),o(ze),o(Ne),o(D),o(Re),o(He),o(K),o(Le),o(Ge),o(te),o(qe),o(oe),o(Ye),o(Se),o(P),o(Xe),o(Ee),o(w),o(Ae),o(Qe),o(M),o(De),o(Oe),o(Ve)),o(r),b(N,e),b(R,e),b(X,e),b(Q,e),b(O,e),b(ee,e),b(ne,e),b(se),b(W),b(ae,e),b(re),b(ie),b(k),b(B),b(le,e),b(de),b(ce),b(F),b(me,e)}}}const eo='{"title":"Pyramid Vision Transformer V2 (PVTv2)","local":"pyramid-vision-transformer-v2-pvtv2","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Abstract from the paper:","local":"abstract-from-the-paper","sections":[],"depth":3}],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"PvtV2Config","local":"transformers.PvtV2Config","sections":[],"depth":2},{"title":"PvtForImageClassification","local":"transformers.PvtV2ForImageClassification","sections":[],"depth":2},{"title":"PvtModel","local":"transformers.PvtV2Model","sections":[],"depth":2}],"depth":1}';function to(x){return qt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class co extends Yt{constructor(r){super(),St(this,r,to,Kt,Gt,{})}}export{co as component};
