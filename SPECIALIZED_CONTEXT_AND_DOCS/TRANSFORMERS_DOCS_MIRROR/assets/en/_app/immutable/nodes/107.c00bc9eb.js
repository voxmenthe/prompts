import{s as Mt,o as Tt,n as He}from"../chunks/scheduler.18a86fab.js";import{S as _t,i as kt,g as d,s as a,r as f,A as wt,h as c,f as o,c as l,j as x,x as _,u as h,k as oe,l as Jt,y as p,a as i,v as g,d as b,t as y,w as M}from"../chunks/index.98837b22.js";import{T as $t}from"../chunks/Tip.77304350.js";import{D as Me}from"../chunks/Docstring.a1ef7999.js";import{C as ge}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as Re,E as Ut}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as vt,a as st}from"../chunks/HfOption.6641485e.js";function jt($){let t,m='Refer to the <a href="./t5">T5</a> docs for more examples of how to apply ByT5 to different language tasks.';return{c(){t=d("p"),t.innerHTML=m},l(n){t=c(n,"P",{"data-svelte-h":!0}),_(t)!=="svelte-1vzdgo0"&&(t.innerHTML=m)},m(n,k){i(n,t,k)},p:He,d(n){n&&o(t)}}}function xt($){let t,m;return t=new ge({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJ0ZXh0MnRleHQtZ2VuZXJhdGlvbiUyMiUyQyUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEJTIyZ29vZ2xlJTJGYnl0NS1zbWFsbCUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZSUzRDAlMEEpJTBBcGlwZWxpbmUoJTIydHJhbnNsYXRlJTIwRW5nbGlzaCUyMHRvJTIwRnJlbmNoJTNBJTIwVGhlJTIwd2VhdGhlciUyMGlzJTIwbmljZSUyMHRvZGF5JTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;text2text-generation&quot;</span>,
    model=<span class="hljs-string">&quot;google/byt5-small&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;translate English to French: The weather is nice today&quot;</span>)`,wrap:!1}}),{c(){f(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,k){g(t,n,k),m=!0},p:He,i(n){m||(b(t.$$.fragment,n),m=!0)},o(n){y(t.$$.fragment,n),m=!1},d(n){M(t,n)}}}function It($){let t,m;return t=new ge({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmdvb2dsZSUyRmJ5dDUtc21hbGwlMjIlMEEpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmdvb2dsZSUyRmJ5dDUtc21hbGwlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUwQSklMEElMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyc3VtbWFyaXplJTNBJTIwUGhvdG9zeW50aGVzaXMlMjBpcyUyMHRoZSUyMHByb2Nlc3MlMjBieSUyMHdoaWNoJTIwcGxhbnRzJTJDJTIwYWxnYWUlMkMlMjBhbmQlMjBzb21lJTIwYmFjdGVyaWElMjBjb252ZXJ0JTIwbGlnaHQlMjBlbmVyZ3klMjBpbnRvJTIwY2hlbWljYWwlMjBlbmVyZ3kuJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    <span class="hljs-string">&quot;google/byt5-small&quot;</span>
)
model = AutoModelForSeq2SeqLM.from_pretrained(
    <span class="hljs-string">&quot;google/byt5-small&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)

input_ids = tokenizer(<span class="hljs-string">&quot;summarize: Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

output = model.generate(**input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){f(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,k){g(t,n,k),m=!0},p:He,i(n){m||(b(t.$$.fragment,n),m=!0)},o(n){y(t.$$.fragment,n),m=!1},d(n){M(t,n)}}}function Bt($){let t,m;return t=new ge({props:{code:"ZWNobyUyMC1lJTIwJTIydHJhbnNsYXRlJTIwRW5nbGlzaCUyMHRvJTIwRnJlbmNoJTNBJTIwTGlmZSUyMGlzJTIwYmVhdXRpZnVsLiUyMiUyMCU3QyUyMHRyYW5zZm9ybWVycy1jbGklMjBydW4lMjAtLXRhc2slMjB0ZXh0MnRleHQtZ2VuZXJhdGlvbiUyMC0tbW9kZWwlMjBnb29nbGUlMkZieXQ1LXNtYWxsJTIwLS1kZXZpY2UlMjAw",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;translate English to French: Life is beautiful.&quot;</span> | transformers-cli run --task text2text-generation --model google/byt5-small --device 0',wrap:!1}}),{c(){f(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,k){g(t,n,k),m=!0},p:He,i(n){m||(b(t.$$.fragment,n),m=!0)},o(n){y(t.$$.fragment,n),m=!1},d(n){M(t,n)}}}function zt($){let t,m,n,k,w,V;return t=new st({props:{id:"usage",option:"Pipeline",$$slots:{default:[xt]},$$scope:{ctx:$}}}),n=new st({props:{id:"usage",option:"AutoModel",$$slots:{default:[It]},$$scope:{ctx:$}}}),w=new st({props:{id:"usage",option:"transformers-cli",$$slots:{default:[Bt]},$$scope:{ctx:$}}}),{c(){f(t.$$.fragment),m=a(),f(n.$$.fragment),k=a(),f(w.$$.fragment)},l(r){h(t.$$.fragment,r),m=l(r),h(n.$$.fragment,r),k=l(r),h(w.$$.fragment,r)},m(r,u){g(t,r,u),i(r,m,u),g(n,r,u),i(r,k,u),g(w,r,u),V=!0},p(r,u){const be={};u&2&&(be.$$scope={dirty:u,ctx:r}),t.$set(be);const W={};u&2&&(W.$$scope={dirty:u,ctx:r}),n.$set(W);const U={};u&2&&(U.$$scope={dirty:u,ctx:r}),w.$set(U)},i(r){V||(b(t.$$.fragment,r),b(n.$$.fragment,r),b(w.$$.fragment,r),V=!0)},o(r){y(t.$$.fragment,r),y(n.$$.fragment,r),y(w.$$.fragment,r),V=!1},d(r){r&&(o(m),o(k)),M(t,r),M(n,r),M(w,r)}}}function Ct($){let t,m,n,k,w,V="<em>This model was released on 2021-05-28 and added to Hugging Face Transformers on 2021-06-01.</em>",r,u,be='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',W,U,Te,G,ot='<a href="https://huggingface.co/papers/2105.13626" rel="nofollow">ByT5</a> is tokenizer-free version of the <a href="./t5">T5</a> model designed to works directly on raw UTF-8 bytes. This means it can process any language, more robust to noise like typos, and simpler to use because it doesn’t require a preprocessing pipeline.',_e,X,at='You can find all the original ByT5 checkpoints under the <a href="https://huggingface.co/google?search_models=byt5" rel="nofollow">Google</a> organization.',ke,B,we,R,lt='The example below demonstrates how to generate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> and from the command line.',Je,z,$e,H,Ue,Q,rt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',ve,N,it='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to int4.',je,E,xe,L,Ie,v,ae,pt="<p>It is recommended to use the tokenizer for batched inference and training.</p>",Qe,S,le,dt="The example below shows how to use the model without a tokenizer.",Ne,F,Ee,Y,re,ct="ByT5 uses the top byte values (258, 257, etc.) for masking instead of sentinel tokens like <code>{extra_id_0}</code>.",Le,A,Be,D,ze,T,P,Se,ie,mt="Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding.",Fe,pe,ut=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Ye,j,K,Ae,de,ft=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`,De,ce,ht="<li>single sequence: <code>X &lt;/s&gt;</code></li> <li>pair of sequences: <code>A &lt;/s&gt; B &lt;/s&gt;</code></li>",Pe,C,O,Ke,me,gt="Converts a sequence of tokens (string) in a single string.",Oe,Z,ee,et,ue,bt=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. ByT5 does not
make use of token type ids, therefore a list of zeros is returned.`,tt,q,te,nt,fe,yt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Ce,ne,Ze,ye,qe;return U=new Re({props:{title:"ByT5",local:"byt5",headingTag:"h1"}}),B=new $t({props:{warning:!1,$$slots:{default:[jt]},$$scope:{ctx:$}}}),z=new vt({props:{id:"usage",options:["Pipeline","AutoModel","transformers-cli"],$$slots:{default:[zt]},$$scope:{ctx:$}}}),H=new Re({props:{title:"Quantization",local:"quantization",headingTag:"h2"}}),E=new ge({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRvcmNoYW8lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBUb3JjaEFvQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBUb3JjaEFvQ29uZmlnKCUyMmludDRfd2VpZ2h0X29ubHklMjIlMkMlMjBncm91cF9zaXplJTNEMTI4KSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJnb29nbGUlMkZieXQ1LXhsJTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGYnl0NS14bCUyMiklMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIydHJhbnNsYXRlJTIwRW5nbGlzaCUyMHRvJTIwRnJlbmNoJTNBJTIwVGhlJTIwd2VhdGhlciUyMGlzJTIwbmljZSUyMHRvZGF5LiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFvdXRwdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-comment"># pip install torchao</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer

quantization_config = TorchAoConfig(<span class="hljs-string">&quot;int4_weight_only&quot;</span>, group_size=<span class="hljs-number">128</span>)

model = AutoModelForSeq2SeqLM.from_pretrained(
    <span class="hljs-string">&quot;google/byt5-xl&quot;</span>,
    dtype=torch.bfloat16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/byt5-xl&quot;</span>)
input_ids = tokenizer(<span class="hljs-string">&quot;translate English to French: The weather is nice today.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

output = model.generate(**input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),L=new Re({props:{title:"Notes",local:"notes",headingTag:"h2"}}),F=new ge({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmJ5dDUtc21hbGwlMjIpJTBBJTBBbnVtX3NwZWNpYWxfdG9rZW5zJTIwJTNEJTIwMyUwQSUwQWlucHV0X2lkcyUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUJsaXN0KCUyMkxpZmUlMjBpcyUyMGxpa2UlMjBhJTIwYm94JTIwb2YlMjBjaG9jb2xhdGVzLiUyMi5lbmNvZGUoJTIydXRmLTglMjIpKSU1RCklMjAlMkIlMjBudW1fc3BlY2lhbF90b2tlbnMlMEFsYWJlbHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCbGlzdCglMjJMYSUyMHZpZSUyMGVzdCUyMGNvbW1lJTIwdW5lJTIwYm8lQzMlQUV0ZSUyMGRlJTIwY2hvY29sYXQuJTIyLmVuY29kZSglMjJ1dGYtOCUyMikpJTVEKSUyMCUyQiUyMG51bV9zcGVjaWFsX3Rva2VucyUwQWxvc3MlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMlMkMlMjBsYWJlbHMlM0RsYWJlbHMpLmxvc3MlMEFsb3NzLml0ZW0oKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google/byt5-small&quot;</span>)

num_special_tokens = <span class="hljs-number">3</span>

input_ids = torch.tensor([<span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;Life is like a box of chocolates.&quot;</span>.encode(<span class="hljs-string">&quot;utf-8&quot;</span>))]) + num_special_tokens
labels = torch.tensor([<span class="hljs-built_in">list</span>(<span class="hljs-string">&quot;La vie est comme une boîte de chocolat.&quot;</span>.encode(<span class="hljs-string">&quot;utf-8&quot;</span>))]) + num_special_tokens
loss = model(input_ids, labels=labels).loss
loss.item()`,wrap:!1}}),A=new ge({props:{code:"JTIzJTIwRXhhbXBsZSUzQSUyMGNoYXJhY3Rlci1sZXZlbCUyMGRlbm9pc2luZyUyMHdpdGglMjBtYXNrJTIwdG9rZW5zJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMlRoZSUyMGRvZyUyMGNoYXNlcyUyMGElMjBiYWxsJTIwaW4lMjB0aGUlMjBwYXJrLiUyMikuaW5wdXRfaWRzJTBBbWFza2VkX2lucHV0JTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QmlucHV0X2lkcyU1QiUzQTglNUQlMjAlMkIlMjAlNUIyNTglNUQlMjAlMkIlMjBpbnB1dF9pZHMlNUIxNCUzQTIxJTVEJTIwJTJCJTIwJTVCMjU3JTVEJTIwJTJCJTIwaW5wdXRfaWRzJTVCMjglM0ElNUQlNUQpJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUobWFza2VkX2lucHV0JTJDJTIwbWF4X2xlbmd0aCUzRDEwMCk=",highlighted:`<span class="hljs-comment"># Example: character-level denoising with mask tokens</span>
input_ids = tokenizer(<span class="hljs-string">&quot;The dog chases a ball in the park.&quot;</span>).input_ids
masked_input = torch.tensor([input_ids[:<span class="hljs-number">8</span>] + [<span class="hljs-number">258</span>] + input_ids[<span class="hljs-number">14</span>:<span class="hljs-number">21</span>] + [<span class="hljs-number">257</span>] + input_ids[<span class="hljs-number">28</span>:]])
output = model.generate(masked_input, max_length=<span class="hljs-number">100</span>)`,wrap:!1}}),D=new Re({props:{title:"ByT5Tokenizer",local:"transformers.ByT5Tokenizer",headingTag:"h2"}}),P=new Me({props:{name:"class transformers.ByT5Tokenizer",anchor:"transformers.ByT5Tokenizer",parameters:[{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 125"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ByT5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.ByT5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ByT5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ByT5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 125) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in ByT5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.ByT5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/byt5/tokenization_byt5.py#L27"}}),K=new Me({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/byt5/tokenization_byt5.py#L171",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),O=new Me({props:{name:"convert_tokens_to_string",anchor:"transformers.ByT5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/byt5/tokenization_byt5.py#L217"}}),ee=new Me({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/byt5/tokenization_byt5.py#L149",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),te=new Me({props:{name:"get_special_tokens_mask",anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ByT5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/byt5/tokenization_byt5.py#L110",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),ne=new Ut({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md"}}),{c(){t=d("meta"),m=a(),n=d("p"),k=a(),w=d("p"),w.innerHTML=V,r=a(),u=d("div"),u.innerHTML=be,W=a(),f(U.$$.fragment),Te=a(),G=d("p"),G.innerHTML=ot,_e=a(),X=d("p"),X.innerHTML=at,ke=a(),f(B.$$.fragment),we=a(),R=d("p"),R.innerHTML=lt,Je=a(),f(z.$$.fragment),$e=a(),f(H.$$.fragment),Ue=a(),Q=d("p"),Q.innerHTML=rt,ve=a(),N=d("p"),N.innerHTML=it,je=a(),f(E.$$.fragment),xe=a(),f(L.$$.fragment),Ie=a(),v=d("ul"),ae=d("li"),ae.innerHTML=pt,Qe=a(),S=d("li"),le=d("p"),le.textContent=dt,Ne=a(),f(F.$$.fragment),Ee=a(),Y=d("li"),re=d("p"),re.innerHTML=ct,Le=a(),f(A.$$.fragment),Be=a(),f(D.$$.fragment),ze=a(),T=d("div"),f(P.$$.fragment),Se=a(),ie=d("p"),ie.textContent=mt,Fe=a(),pe=d("p"),pe.innerHTML=ut,Ye=a(),j=d("div"),f(K.$$.fragment),Ae=a(),de=d("p"),de.textContent=ft,De=a(),ce=d("ul"),ce.innerHTML=ht,Pe=a(),C=d("div"),f(O.$$.fragment),Ke=a(),me=d("p"),me.textContent=gt,Oe=a(),Z=d("div"),f(ee.$$.fragment),et=a(),ue=d("p"),ue.textContent=bt,tt=a(),q=d("div"),f(te.$$.fragment),nt=a(),fe=d("p"),fe.innerHTML=yt,Ce=a(),f(ne.$$.fragment),Ze=a(),ye=d("p"),this.h()},l(e){const s=wt("svelte-u9bgzb",document.head);t=c(s,"META",{name:!0,content:!0}),s.forEach(o),m=l(e),n=c(e,"P",{}),x(n).forEach(o),k=l(e),w=c(e,"P",{"data-svelte-h":!0}),_(w)!=="svelte-nybcqs"&&(w.innerHTML=V),r=l(e),u=c(e,"DIV",{style:!0,"data-svelte-h":!0}),_(u)!=="svelte-100e61"&&(u.innerHTML=be),W=l(e),h(U.$$.fragment,e),Te=l(e),G=c(e,"P",{"data-svelte-h":!0}),_(G)!=="svelte-1agooob"&&(G.innerHTML=ot),_e=l(e),X=c(e,"P",{"data-svelte-h":!0}),_(X)!=="svelte-wv8c1w"&&(X.innerHTML=at),ke=l(e),h(B.$$.fragment,e),we=l(e),R=c(e,"P",{"data-svelte-h":!0}),_(R)!=="svelte-s90gxn"&&(R.innerHTML=lt),Je=l(e),h(z.$$.fragment,e),$e=l(e),h(H.$$.fragment,e),Ue=l(e),Q=c(e,"P",{"data-svelte-h":!0}),_(Q)!=="svelte-nf5ooi"&&(Q.innerHTML=rt),ve=l(e),N=c(e,"P",{"data-svelte-h":!0}),_(N)!=="svelte-w36i1c"&&(N.innerHTML=it),je=l(e),h(E.$$.fragment,e),xe=l(e),h(L.$$.fragment,e),Ie=l(e),v=c(e,"UL",{});var I=x(v);ae=c(I,"LI",{"data-svelte-h":!0}),_(ae)!=="svelte-dw96zr"&&(ae.innerHTML=pt),Qe=l(I),S=c(I,"LI",{});var se=x(S);le=c(se,"P",{"data-svelte-h":!0}),_(le)!=="svelte-1emta2o"&&(le.textContent=dt),Ne=l(se),h(F.$$.fragment,se),se.forEach(o),Ee=l(I),Y=c(I,"LI",{});var Ve=x(Y);re=c(Ve,"P",{"data-svelte-h":!0}),_(re)!=="svelte-1jnmrqj"&&(re.innerHTML=ct),Le=l(Ve),h(A.$$.fragment,Ve),Ve.forEach(o),I.forEach(o),Be=l(e),h(D.$$.fragment,e),ze=l(e),T=c(e,"DIV",{class:!0});var J=x(T);h(P.$$.fragment,J),Se=l(J),ie=c(J,"P",{"data-svelte-h":!0}),_(ie)!=="svelte-ab2b05"&&(ie.textContent=mt),Fe=l(J),pe=c(J,"P",{"data-svelte-h":!0}),_(pe)!=="svelte-ntrhio"&&(pe.innerHTML=ut),Ye=l(J),j=c(J,"DIV",{class:!0});var he=x(j);h(K.$$.fragment,he),Ae=l(he),de=c(he,"P",{"data-svelte-h":!0}),_(de)!=="svelte-1wjq39d"&&(de.textContent=ft),De=l(he),ce=c(he,"UL",{"data-svelte-h":!0}),_(ce)!=="svelte-8gh3n2"&&(ce.innerHTML=ht),he.forEach(o),Pe=l(J),C=c(J,"DIV",{class:!0});var We=x(C);h(O.$$.fragment,We),Ke=l(We),me=c(We,"P",{"data-svelte-h":!0}),_(me)!=="svelte-b3k2yi"&&(me.textContent=gt),We.forEach(o),Oe=l(J),Z=c(J,"DIV",{class:!0});var Ge=x(Z);h(ee.$$.fragment,Ge),et=l(Ge),ue=c(Ge,"P",{"data-svelte-h":!0}),_(ue)!=="svelte-on9fcl"&&(ue.textContent=bt),Ge.forEach(o),tt=l(J),q=c(J,"DIV",{class:!0});var Xe=x(q);h(te.$$.fragment,Xe),nt=l(Xe),fe=c(Xe,"P",{"data-svelte-h":!0}),_(fe)!=="svelte-1f4f5kp"&&(fe.innerHTML=yt),Xe.forEach(o),J.forEach(o),Ce=l(e),h(ne.$$.fragment,e),Ze=l(e),ye=c(e,"P",{}),x(ye).forEach(o),this.h()},h(){oe(t,"name","hf:doc:metadata"),oe(t,"content",Zt),Jt(u,"float","right"),oe(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),oe(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),oe(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),oe(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),oe(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){p(document.head,t),i(e,m,s),i(e,n,s),i(e,k,s),i(e,w,s),i(e,r,s),i(e,u,s),i(e,W,s),g(U,e,s),i(e,Te,s),i(e,G,s),i(e,_e,s),i(e,X,s),i(e,ke,s),g(B,e,s),i(e,we,s),i(e,R,s),i(e,Je,s),g(z,e,s),i(e,$e,s),g(H,e,s),i(e,Ue,s),i(e,Q,s),i(e,ve,s),i(e,N,s),i(e,je,s),g(E,e,s),i(e,xe,s),g(L,e,s),i(e,Ie,s),i(e,v,s),p(v,ae),p(v,Qe),p(v,S),p(S,le),p(S,Ne),g(F,S,null),p(v,Ee),p(v,Y),p(Y,re),p(Y,Le),g(A,Y,null),i(e,Be,s),g(D,e,s),i(e,ze,s),i(e,T,s),g(P,T,null),p(T,Se),p(T,ie),p(T,Fe),p(T,pe),p(T,Ye),p(T,j),g(K,j,null),p(j,Ae),p(j,de),p(j,De),p(j,ce),p(T,Pe),p(T,C),g(O,C,null),p(C,Ke),p(C,me),p(T,Oe),p(T,Z),g(ee,Z,null),p(Z,et),p(Z,ue),p(T,tt),p(T,q),g(te,q,null),p(q,nt),p(q,fe),i(e,Ce,s),g(ne,e,s),i(e,Ze,s),i(e,ye,s),qe=!0},p(e,[s]){const I={};s&2&&(I.$$scope={dirty:s,ctx:e}),B.$set(I);const se={};s&2&&(se.$$scope={dirty:s,ctx:e}),z.$set(se)},i(e){qe||(b(U.$$.fragment,e),b(B.$$.fragment,e),b(z.$$.fragment,e),b(H.$$.fragment,e),b(E.$$.fragment,e),b(L.$$.fragment,e),b(F.$$.fragment,e),b(A.$$.fragment,e),b(D.$$.fragment,e),b(P.$$.fragment,e),b(K.$$.fragment,e),b(O.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(ne.$$.fragment,e),qe=!0)},o(e){y(U.$$.fragment,e),y(B.$$.fragment,e),y(z.$$.fragment,e),y(H.$$.fragment,e),y(E.$$.fragment,e),y(L.$$.fragment,e),y(F.$$.fragment,e),y(A.$$.fragment,e),y(D.$$.fragment,e),y(P.$$.fragment,e),y(K.$$.fragment,e),y(O.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(ne.$$.fragment,e),qe=!1},d(e){e&&(o(m),o(n),o(k),o(w),o(r),o(u),o(W),o(Te),o(G),o(_e),o(X),o(ke),o(we),o(R),o(Je),o($e),o(Ue),o(Q),o(ve),o(N),o(je),o(xe),o(Ie),o(v),o(Be),o(ze),o(T),o(Ce),o(Ze),o(ye)),o(t),M(U,e),M(B,e),M(z,e),M(H,e),M(E,e),M(L,e),M(F),M(A),M(D,e),M(P),M(K),M(O),M(ee),M(te),M(ne,e)}}}const Zt='{"title":"ByT5","local":"byt5","sections":[{"title":"Quantization","local":"quantization","sections":[],"depth":2},{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"ByT5Tokenizer","local":"transformers.ByT5Tokenizer","sections":[],"depth":2}],"depth":1}';function qt($){return Tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nt extends _t{constructor(t){super(),kt(this,t,qt,Ct,Mt,{})}}export{Nt as component};
