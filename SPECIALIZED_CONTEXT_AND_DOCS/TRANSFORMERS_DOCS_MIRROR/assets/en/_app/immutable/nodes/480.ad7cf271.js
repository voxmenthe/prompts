import{s as F,n as K,o as Q}from"../chunks/scheduler.18a86fab.js";import{S as ee,i as te,g as i,s as o,r as S,A as ne,h as r,f as n,c as a,j as X,u as E,x as w,k as A,y as se,a as s,v as J,d as z,t as q,w as j}from"../chunks/index.98837b22.js";import{C as oe}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ae,E as ie}from"../chunks/getInferenceSnippets.06c2775f.js";function re(R){let l,$,y,T,p,x,h,B='This guide shows how to do audio transcription for chat purposes, using <code>transformers serve</code> and <a href="https://openwebui.com/" rel="nofollow">Open WebUI</a>. This guide assumes you have Open WebUI installed on your machine and ready to run. Please refer to the examples above to use the text functionalities of <code>transformer serve</code> with Open WebUI — the instructions are the same.',C,d,G='To start, let’s launch the server. Some of Open WebUI’s requests require <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS" rel="nofollow">CORS</a>, which is disabled by default for security reasons, so you need to enable it:',H,f,k,m,V='Before you can speak into Open WebUI, you need to update its settings to use your server for speech to text (STT) tasks. Launch Open WebUI, and navigate to the audio tab inside the admin settings. If you’re using Open WebUI with the default ports, <a href="http://localhost:3000/admin/settings/audio" rel="nofollow">this link (default)</a> or <a href="http://localhost:8080/admin/settings/audio" rel="nofollow">this link (python deployment)</a> will take you there. Do the following changes there:',L,c,Y='<li>Change the type of “Speech-to-Text Engine” to “OpenAI”;</li> <li>Update the address to your server’s address — <code>http://localhost:8000/v1</code> by default;</li> <li>Type your model of choice into the “STT Model” field, e.g. <code>openai/whisper-large-v3</code> (<a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&amp;sort=trending" rel="nofollow">available models</a>).</li>',M,g,Z="If you’ve done everything correctly, the audio tab should look like this",I,u,D='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_openwebui_stt_settings.png"/>',O,b,N="You’re now ready to speak! Open a new chat, utter a few words after hitting the microphone button, and you should see the corresponding text on the chat input after the model transcribes it.",P,v,U,_,W;return p=new ae({props:{title:"Audio transcriptions with WebUI and transformers serve",local:"audio-transcriptions-with-webui-and-transformers-serve",headingTag:"h1"}}),f=new oe({props:{code:"dHJhbnNmb3JtZXJzJTIwc2VydmUlMjAtLWVuYWJsZS1jb3Jz",highlighted:"transformers serve --enable-cors",wrap:!1}}),v=new ie({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/open_webui.md"}}),{c(){l=i("meta"),$=o(),y=i("p"),T=o(),S(p.$$.fragment),x=o(),h=i("p"),h.innerHTML=B,C=o(),d=i("p"),d.innerHTML=G,H=o(),S(f.$$.fragment),k=o(),m=i("p"),m.innerHTML=V,L=o(),c=i("ol"),c.innerHTML=Y,M=o(),g=i("p"),g.textContent=Z,I=o(),u=i("h3"),u.innerHTML=D,O=o(),b=i("p"),b.textContent=N,P=o(),S(v.$$.fragment),U=o(),_=i("p"),this.h()},l(e){const t=ne("svelte-u9bgzb",document.head);l=r(t,"META",{name:!0,content:!0}),t.forEach(n),$=a(e),y=r(e,"P",{}),X(y).forEach(n),T=a(e),E(p.$$.fragment,e),x=a(e),h=r(e,"P",{"data-svelte-h":!0}),w(h)!=="svelte-11kl1d0"&&(h.innerHTML=B),C=a(e),d=r(e,"P",{"data-svelte-h":!0}),w(d)!=="svelte-ca125k"&&(d.innerHTML=G),H=a(e),E(f.$$.fragment,e),k=a(e),m=r(e,"P",{"data-svelte-h":!0}),w(m)!=="svelte-1p65l20"&&(m.innerHTML=V),L=a(e),c=r(e,"OL",{"data-svelte-h":!0}),w(c)!=="svelte-ubr0l1"&&(c.innerHTML=Y),M=a(e),g=r(e,"P",{"data-svelte-h":!0}),w(g)!=="svelte-1o8pqg9"&&(g.textContent=Z),I=a(e),u=r(e,"H3",{align:!0,"data-svelte-h":!0}),w(u)!=="svelte-icd9k0"&&(u.innerHTML=D),O=a(e),b=r(e,"P",{"data-svelte-h":!0}),w(b)!=="svelte-16zuhmd"&&(b.textContent=N),P=a(e),E(v.$$.fragment,e),U=a(e),_=r(e,"P",{}),X(_).forEach(n),this.h()},h(){A(l,"name","hf:doc:metadata"),A(l,"content",le),A(u,"align","center")},m(e,t){se(document.head,l),s(e,$,t),s(e,y,t),s(e,T,t),J(p,e,t),s(e,x,t),s(e,h,t),s(e,C,t),s(e,d,t),s(e,H,t),J(f,e,t),s(e,k,t),s(e,m,t),s(e,L,t),s(e,c,t),s(e,M,t),s(e,g,t),s(e,I,t),s(e,u,t),s(e,O,t),s(e,b,t),s(e,P,t),J(v,e,t),s(e,U,t),s(e,_,t),W=!0},p:K,i(e){W||(z(p.$$.fragment,e),z(f.$$.fragment,e),z(v.$$.fragment,e),W=!0)},o(e){q(p.$$.fragment,e),q(f.$$.fragment,e),q(v.$$.fragment,e),W=!1},d(e){e&&(n($),n(y),n(T),n(x),n(h),n(C),n(d),n(H),n(k),n(m),n(L),n(c),n(M),n(g),n(I),n(u),n(O),n(b),n(P),n(U),n(_)),n(l),j(p,e),j(f,e),j(v,e)}}}const le='{"title":"Audio transcriptions with WebUI and transformers serve","local":"audio-transcriptions-with-webui-and-transformers-serve","sections":[],"depth":1}';function ue(R){return Q(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class me extends ee{constructor(l){super(),te(this,l,ue,re,F,{})}}export{me as component};
