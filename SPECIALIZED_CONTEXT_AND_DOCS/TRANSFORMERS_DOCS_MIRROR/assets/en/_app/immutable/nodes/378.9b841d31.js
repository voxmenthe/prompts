import{s as ca,z as da,o as la,n as st}from"../chunks/scheduler.18a86fab.js";import{S as pa,i as ga,g as r,s as n,r as d,A as ha,h as i,f as o,c as a,j as T,x as c,u as l,k as y,y as s,a as m,v as p,d as g,t as h,w as f}from"../chunks/index.98837b22.js";import{T as ma}from"../chunks/Tip.77304350.js";import{D as M}from"../chunks/Docstring.a1ef7999.js";import{C as tt}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as cn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as P,E as fa}from"../chunks/getInferenceSnippets.06c2775f.js";function ua(N){let u,x="Example:",b,v,I;return v=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBTYW1WaXNpb25Db25maWclMkMlMEElMjAlMjAlMjAlMjBTYW1Qcm9tcHRFbmNvZGVyQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwU2FtTWFza0RlY29kZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBTYW1Nb2RlbCUyQyUwQSklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwU2FtQ29uZmlnJTIwd2l0aCUyMCU2MCUyMmZhY2Vib29rJTJGc2FtLXZpdC1odWdlJTIyJTYwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFNhbUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFNhbU1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjAlNjAlMjJmYWNlYm9vayUyRnNhbS12aXQtaHVnZSUyMiU2MCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwU2FtTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBTYW1Db25maWclMjBmcm9tJTIwYSUyMFNhbVZpc2lvbkNvbmZpZyUyQyUyMFNhbVByb21wdEVuY29kZXJDb25maWclMkMlMjBhbmQlMjBTYW1NYXNrRGVjb2RlckNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMFNBTSUyMHZpc2lvbiUyQyUyMFNBTSUyMFEtRm9ybWVyJTIwYW5kJTIwbGFuZ3VhZ2UlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb25zJTBBdmlzaW9uX2NvbmZpZyUyMCUzRCUyMFNhbVZpc2lvbkNvbmZpZygpJTBBcHJvbXB0X2VuY29kZXJfY29uZmlnJTIwJTNEJTIwU2FtUHJvbXB0RW5jb2RlckNvbmZpZygpJTBBbWFza19kZWNvZGVyX2NvbmZpZyUyMCUzRCUyMFNhbU1hc2tEZWNvZGVyQ29uZmlnKCklMEElMEFjb25maWclMjAlM0QlMjBTYW1Db25maWcodmlzaW9uX2NvbmZpZyUyQyUyMHByb21wdF9lbmNvZGVyX2NvbmZpZyUyQyUyMG1hc2tfZGVjb2Rlcl9jb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    SamVisionConfig,
<span class="hljs-meta">... </span>    SamPromptEncoderConfig,
<span class="hljs-meta">... </span>    SamMaskDecoderConfig,
<span class="hljs-meta">... </span>    SamModel,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SamConfig with \`&quot;facebook/sam-vit-huge&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SamConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SamModel (with random weights) from the \`&quot;facebook/sam-vit-huge&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SamModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a SamConfig from a SamVisionConfig, SamPromptEncoderConfig, and SamMaskDecoderConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing SAM vision, SAM Q-Former and language model configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = SamVisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_encoder_config = SamPromptEncoderConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_decoder_config = SamMaskDecoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = SamConfig(vision_config, prompt_encoder_config, mask_decoder_config)`,wrap:!1}}),{c(){u=r("p"),u.textContent=x,b=n(),d(v.$$.fragment)},l(_){u=i(_,"P",{"data-svelte-h":!0}),c(u)!=="svelte-11lpom8"&&(u.textContent=x),b=a(_),l(v.$$.fragment,_)},m(_,$){m(_,u,$),m(_,b,$),p(v,_,$),I=!0},p:st,i(_){I||(g(v.$$.fragment,_),I=!0)},o(_){h(v.$$.fragment,_),I=!1},d(_){_&&(o(u),o(b)),f(v,_)}}}function _a(N){let u,x="Example:",b,v,I;return v=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBTYW1WaXNpb25Db25maWclMkMlMEElMjAlMjAlMjAlMjBTYW1WaXNpb25Nb2RlbCUyQyUwQSklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwU2FtVmlzaW9uQ29uZmlnJTIwd2l0aCUyMCU2MCUyMmZhY2Vib29rJTJGc2FtLXZpdC1odWdlJTIyJTYwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFNhbVZpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFNhbVZpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjAlNjAlMjJmYWNlYm9vayUyRnNhbS12aXQtaHVnZSUyMiU2MCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwU2FtVmlzaW9uTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    SamVisionConfig,
<span class="hljs-meta">... </span>    SamVisionModel,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SamVisionConfig with \`&quot;facebook/sam-vit-huge&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SamVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SamVisionModel (with random weights) from the \`&quot;facebook/sam-vit-huge&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SamVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){u=r("p"),u.textContent=x,b=n(),d(v.$$.fragment)},l(_){u=i(_,"P",{"data-svelte-h":!0}),c(u)!=="svelte-11lpom8"&&(u.textContent=x),b=a(_),l(v.$$.fragment,_)},m(_,$){m(_,u,$),m(_,b,$),p(v,_,$),I=!0},p:st,i(_){I||(g(v.$$.fragment,_),I=!0)},o(_){h(v.$$.fragment,_),I=!1},d(_){_&&(o(u),o(b)),f(v,_)}}}function ba(N){let u,x=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){u=r("p"),u.innerHTML=x},l(b){u=i(b,"P",{"data-svelte-h":!0}),c(u)!=="svelte-fincs2"&&(u.innerHTML=x)},m(b,v){m(b,u,v)},p:st,d(b){b&&o(u)}}}function va(N){let u,x=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){u=r("p"),u.innerHTML=x},l(b){u=i(b,"P",{"data-svelte-h":!0}),c(u)!=="svelte-fincs2"&&(u.innerHTML=x)},m(b,v){m(b,u,v)},p:st,d(b){b&&o(u)}}}function ya(N){let u,x="Example:",b,v,I;return v=new tt({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Byb2Nlc3NvciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnNhbS12aXQtYmFzZSUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnNhbS12aXQtYmFzZSUyMiklMEElMEFpbWdfdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGdHJhbnNmb3JtZXJzJTJGbW9kZWxfZG9jJTJGc2FtLWNhci5wbmclMjIlMEFyYXdfaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldChpbWdfdXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMlJHQiUyMiklMEFpbnB1dF9wb2ludHMlMjAlM0QlMjAlNUIlNUIlNUI0MDAlMkMlMjA2NTAlNUQlNUQlNUQlMjAlMjAlMjMlMjAyRCUyMGxvY2F0aW9uJTIwb2YlMjBhJTIwd2luZG93JTIwb24lMjB0aGUlMjBjYXIlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlJTJDJTIwaW5wdXRfcG9pbnRzJTNEaW5wdXRfcG9pbnRzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBHZXQlMjBzZWdtZW50YXRpb24lMjBtYXNrJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQSUyMyUyMFBvc3Rwcm9jZXNzJTIwbWFza3MlMEFtYXNrcyUyMCUzRCUyMHByb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfbWFza3MoJTBBJTIwJTIwJTIwJTIwb3V0cHV0cy5wcmVkX21hc2tzJTJDJTIwaW5wdXRzJTVCJTIyb3JpZ2luYWxfc2l6ZXMlMjIlNUQlMkMlMjBpbnB1dHMlNUIlMjJyZXNoYXBlZF9pbnB1dF9zaXplcyUyMiU1RCUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>img_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_image = Image.<span class="hljs-built_in">open</span>(requests.get(img_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[<span class="hljs-number">400</span>, <span class="hljs-number">650</span>]]]  <span class="hljs-comment"># 2D location of a window on the car</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get segmentation mask</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Postprocess masks</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(
<span class="hljs-meta">... </span>    outputs.pred_masks, inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>], inputs[<span class="hljs-string">&quot;reshaped_input_sizes&quot;</span>]
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){u=r("p"),u.textContent=x,b=n(),d(v.$$.fragment)},l(_){u=i(_,"P",{"data-svelte-h":!0}),c(u)!=="svelte-11lpom8"&&(u.textContent=x),b=a(_),l(v.$$.fragment,_)},m(_,$){m(_,u,$),m(_,b,$),p(v,_,$),I=!0},p:st,i(_){I||(g(v.$$.fragment,_),I=!0)},o(_){h(v.$$.fragment,_),I=!1},d(_){_&&(o(u),o(b)),f(v,_)}}}function Ta(N){let u,x,b,v,I,_="<em>This model was released on 2023-04-05 and added to Hugging Face Transformers on 2023-04-19.</em>",$,fe,nt,X,dn='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',at,ue,rt,_e,ln='SAM (Segment Anything Model) was proposed in <a href="https://huggingface.co/papers/2304.02643" rel="nofollow">Segment Anything</a> by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.',it,be,pn="The model can be used to predict segmentation masks of any object of interest given an input image.",mt,ve,gn='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png" alt="example image"/>',ct,ye,hn="The abstract from the paper is the following:",dt,Te,fn='<em>We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive ‚Äî often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at <a href="https://segment-anything.com" rel="nofollow">https://segment-anything.com</a> to foster research into foundation models for computer vision.</em>',lt,Me,un="Tips:",pt,we,_n='<li>The model predicts binary masks that states the presence or not of the object of interest given an image.</li> <li>The model predicts much better results if input 2D points and/or input bounding boxes are provided</li> <li>You can prompt multiple points for the same image, and predict a single mask.</li> <li>Fine-tuning the model is not supported yet</li> <li>According to the paper, textual input should be also supported. However, at this time of writing this seems not to be supported according to <a href="https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844" rel="nofollow">the official repository</a>.</li>',gt,Se,bn=`This model was contributed by <a href="https://huggingface.co/ybelkada" rel="nofollow">ybelkada</a> and <a href="https://huggingface.co/ArthurZ" rel="nofollow">ArthurZ</a>.
The original code can be found <a href="https://github.com/facebookresearch/segment-anything" rel="nofollow">here</a>.`,ht,Ie,vn="Below is an example on how to run mask generation given an image and a 2D point:",ft,xe,ut,ze,yn="You can also process your own masks alongside the input images in the processor to be passed to the model.",_t,ke,bt,$e,vt,Ce,Tn="A list of official Hugging Face and community (indicated by üåé) resources to help you get started with SAM.",yt,je,Mn='<li><a href="https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb" rel="nofollow">Demo notebook</a> for using the model.</li> <li><a href="https://github.com/huggingface/notebooks/blob/main/examples/automatic_mask_generation.ipynb" rel="nofollow">Demo notebook</a> for using the automatic mask generation pipeline.</li> <li><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb" rel="nofollow">Demo notebook</a> for inference with MedSAM, a fine-tuned version of SAM on the medical domain. üåé</li> <li><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb" rel="nofollow">Demo notebook</a> for fine-tuning the model on custom data. üåé</li>',Tt,Pe,Mt,Je,wn='SlimSAM, a pruned version of SAM, was proposed in <a href="https://huggingface.co/papers/2312.05284" rel="nofollow">0.1% Data Makes Segment Anything Slim</a> by Zigeng Chen et al. SlimSAM reduces the size of the SAM models considerably while maintaining the same performance.',wt,Ue,Sn='Checkpoints can be found on the <a href="https://huggingface.co/models?other=slimsam" rel="nofollow">hub</a>, and they can be used as a drop-in replacement of SAM.',St,Ne,It,Fe,In='One can combine <a href="grounding-dino">Grounding DINO</a> with SAM for text-based mask generation as introduced in <a href="https://huggingface.co/papers/2401.14159" rel="nofollow">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</a>. You can refer to this <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb" rel="nofollow">demo notebook</a> üåç for details.',xt,G,xn,zt,We,zn='Grounded SAM overview. Taken from the <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">original repository</a>.',kt,Ze,$t,J,Re,ms,yo,kn=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig">SamConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamModel">SamModel</a>. It is used to instantiate a
SAM model according to the specified arguments, defining the vision model, prompt-encoder model and mask decoder
configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the
SAM-ViT-H <a href="https://huggingface.co/facebook/sam-vit-huge" rel="nofollow">facebook/sam-vit-huge</a> architecture.`,cs,To,$n=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ds,Y,Ct,Ve,jt,U,Be,ls,Mo,Cn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionModel">SamVisionModel</a>. It is used to instantiate a SAM
vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
defaults will yield a similar configuration to that of the SAM ViT-h
<a href="https://huggingface.co/facebook/sam-vit-huge" rel="nofollow">facebook/sam-vit-huge</a> architecture.`,ps,wo,jn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,gs,Q,Pt,Ee,Jt,W,De,hs,So,Pn=`This is the configuration class to store the configuration of a <code>SamMaskDecoder</code>. It is used to instantiate a SAM
mask decoder to the specified arguments, defining the model architecture. Instantiating a configuration defaults
will yield a similar configuration to that of the SAM-vit-h
<a href="https://huggingface.co/facebook/sam-vit-huge" rel="nofollow">facebook/sam-vit-huge</a> architecture.`,fs,Io,Jn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ut,Le,Nt,Z,qe,us,xo,Un=`This is the configuration class to store the configuration of a <code>SamPromptEncoder</code>. The <code>SamPromptEncoder</code>
module is used to encode the input 2D points and bounding boxes. Instantiating a configuration defaults will yield
a similar configuration to that of the SAM-vit-h
<a href="https://huggingface.co/facebook/sam-vit-huge" rel="nofollow">facebook/sam-vit-huge</a> architecture.`,_s,zo,Nn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ft,He,Wt,R,Ae,bs,ko,Fn=`Constructs a SAM processor which wraps a SAM image processor and an 2D points & Bounding boxes processor into a
single processor.`,vs,$o,Wn=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamProcessor">SamProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a>. See the docstring of
<code>__call__()</code> for more information.`,Zt,Xe,Rt,w,Ge,ys,Co,Zn="Constructs a SAM image processor.",Ts,O,Ye,Ms,jo,Rn=`Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being
that the iou scores needs to be greater than <code>pred_iou_thresh</code>. The second criterion is that the stability
score needs to be greater than <code>stability_score_thresh</code>. The method also converts the predicted masks to
bounding boxes and pad the predicted masks if necessary.`,ws,K,Qe,Ss,Po,Vn="Generates a list of crop boxes of different sizes. Each layer has (2<strong>i)</strong>2 boxes for the ith layer.",Is,ee,Oe,xs,Jo,Bn="Pad an image to <code>(pad_size[&quot;height&quot;], pad_size[&quot;width&quot;])</code> with zeros to the right and bottom.",zs,oe,Ke,ks,Uo,En="Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.",$s,te,eo,Cs,No,Dn="Remove padding and upscale masks to the original image size.",js,se,oo,Ps,Fo,Ln="Preprocess an image or batch of images.",Js,ne,to,Us,Wo,qn="Resize an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>.",Vt,so,Bt,S,no,Ns,Zo,Hn="Constructs a fast Sam image processor.",Fs,ae,ao,Ws,Ro,An=`Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being
that the iou scores needs to be greater than <code>pred_iou_thresh</code>. The second criterion is that the stability
score needs to be greater than <code>stability_score_thresh</code>. The method also converts the predicted masks to
bounding boxes and pad the predicted masks if necessary.`,Zs,re,ro,Rs,Vo,Xn="Generates a list of crop boxes of different sizes. Each layer has (2<strong>i)</strong>2 boxes for the ith layer.",Vs,ie,io,Bs,Bo,Gn="Pad images to the specified size.",Es,me,mo,Ds,Eo,Yn="Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.",Ls,ce,co,qs,Do,Qn="Remove padding and upscale masks to the original image size.",Hs,Lo,lo,As,de,po,Xs,qo,On="Resize an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>.",Et,go,Dt,C,ho,Gs,Ho,Kn="The vision model from Sam without any head or projection on top.",Ys,Ao,ea=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Qs,Xo,oa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Os,E,fo,Ks,Go,ta='The <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionModel">SamVisionModel</a> forward method, overrides the <code>__call__</code> special method.',en,le,Lt,uo,qt,j,_o,on,Yo,sa=`Segment Anything Model (SAM) for generating segmentation masks, given an input image and
input points and labels, boxes, or masks.`,tn,Qo,na=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,sn,Oo,aa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,nn,F,bo,an,Ko,ra='The <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamModel">SamModel</a> forward method, overrides the <code>__call__</code> special method.',rn,pe,mn,ge,Ht,vo,At,ot,Xt;return fe=new P({props:{title:"SAM",local:"sam",headingTag:"h1"}}),ue=new P({props:{title:"Overview",local:"overview",headingTag:"h2"}}),xe=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBTYW1Nb2RlbCUyQyUyMFNhbVByb2Nlc3NvciUyQyUyMGluZmVyX2RldmljZSUwQSUwQWRldmljZSUyMCUzRCUyMGluZmVyX2RldmljZSgpJTBBbW9kZWwlMjAlM0QlMjBTYW1Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZzYW0tdml0LWh1Z2UlMjIpLnRvKGRldmljZSklMEFwcm9jZXNzb3IlMjAlM0QlMjBTYW1Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGc2FtLXZpdC1odWdlJTIyKSUwQSUwQWltZ191cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGeWJlbGthZGElMkZzZWdtZW50LWFueXRoaW5nJTJGcmVzb2x2ZSUyRm1haW4lMkZhc3NldHMlMkZjYXIucG5nJTIyJTBBcmF3X2ltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1nX3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJSR0IlMjIpJTBBaW5wdXRfcG9pbnRzJTIwJTNEJTIwJTVCJTVCJTVCNDUwJTJDJTIwNjAwJTVEJTVEJTVEJTIwJTIwJTIzJTIwMkQlMjBsb2NhdGlvbiUyMG9mJTIwYSUyMHdpbmRvdyUyMGluJTIwdGhlJTIwaW1hZ2UlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IocmF3X2ltYWdlJTJDJTIwaW5wdXRfcG9pbnRzJTNEaW5wdXRfcG9pbnRzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBbWFza3MlMjAlM0QlMjBwcm9jZXNzb3IuaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyglMEElMjAlMjAlMjAlMjBvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RC5jcHUoKSUyQyUyMGlucHV0cyU1QiUyMnJlc2hhcGVkX2lucHV0X3NpemVzJTIyJTVELmNwdSgpJTBBKSUwQXNjb3JlcyUyMCUzRCUyMG91dHB1dHMuaW91X3Njb3Jlcw==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SamModel, SamProcessor, infer_device

device = infer_device()
model = SamModel.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-huge&quot;</span>).to(device)
processor = SamProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-huge&quot;</span>)

img_url = <span class="hljs-string">&quot;https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png&quot;</span>
raw_image = Image.<span class="hljs-built_in">open</span>(requests.get(img_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
input_points = [[[<span class="hljs-number">450</span>, <span class="hljs-number">600</span>]]]  <span class="hljs-comment"># 2D location of a window in the image</span>

inputs = processor(raw_image, input_points=input_points, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>].cpu(), inputs[<span class="hljs-string">&quot;reshaped_input_sizes&quot;</span>].cpu()
)
scores = outputs.iou_scores`,wrap:!1}}),ke=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBTYW1Nb2RlbCUyQyUyMFNhbVByb2Nlc3NvciUyQyUyMGluZmVyX2RldmljZSUwQSUwQWRldmljZSUyMCUzRCUyMGluZmVyX2RldmljZSgpJTBBbW9kZWwlMjAlM0QlMjBTYW1Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZzYW0tdml0LWh1Z2UlMjIpLnRvKGRldmljZSklMEFwcm9jZXNzb3IlMjAlM0QlMjBTYW1Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGc2FtLXZpdC1odWdlJTIyKSUwQSUwQWltZ191cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGeWJlbGthZGElMkZzZWdtZW50LWFueXRoaW5nJTJGcmVzb2x2ZSUyRm1haW4lMkZhc3NldHMlMkZjYXIucG5nJTIyJTBBcmF3X2ltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1nX3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJSR0IlMjIpJTBBbWFza191cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGeWJlbGthZGElMkZzZWdtZW50LWFueXRoaW5nJTJGcmVzb2x2ZSUyRm1haW4lMkZhc3NldHMlMkZjYXIucG5nJTIyJTBBc2VnbWVudGF0aW9uX21hcCUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KG1hc2tfdXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMjElMjIpJTBBaW5wdXRfcG9pbnRzJTIwJTNEJTIwJTVCJTVCJTVCNDUwJTJDJTIwNjAwJTVEJTVEJTVEJTIwJTIwJTIzJTIwMkQlMjBsb2NhdGlvbiUyMG9mJTIwYSUyMHdpbmRvdyUyMGluJTIwdGhlJTIwaW1hZ2UlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IocmF3X2ltYWdlJTJDJTIwaW5wdXRfcG9pbnRzJTNEaW5wdXRfcG9pbnRzJTJDJTIwc2VnbWVudGF0aW9uX21hcHMlM0RzZWdtZW50YXRpb25fbWFwJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBbWFza3MlMjAlM0QlMjBwcm9jZXNzb3IuaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyglMEElMjAlMjAlMjAlMjBvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RC5jcHUoKSUyQyUyMGlucHV0cyU1QiUyMnJlc2hhcGVkX2lucHV0X3NpemVzJTIyJTVELmNwdSgpJTBBKSUwQXNjb3JlcyUyMCUzRCUyMG91dHB1dHMuaW91X3Njb3Jlcw==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SamModel, SamProcessor, infer_device

device = infer_device()
model = SamModel.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-huge&quot;</span>).to(device)
processor = SamProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/sam-vit-huge&quot;</span>)

img_url = <span class="hljs-string">&quot;https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png&quot;</span>
raw_image = Image.<span class="hljs-built_in">open</span>(requests.get(img_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
mask_url = <span class="hljs-string">&quot;https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png&quot;</span>
segmentation_map = Image.<span class="hljs-built_in">open</span>(requests.get(mask_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;1&quot;</span>)
input_points = [[[<span class="hljs-number">450</span>, <span class="hljs-number">600</span>]]]  <span class="hljs-comment"># 2D location of a window in the image</span>

inputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>].cpu(), inputs[<span class="hljs-string">&quot;reshaped_input_sizes&quot;</span>].cpu()
)
scores = outputs.iou_scores`,wrap:!1}}),$e=new P({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Pe=new P({props:{title:"SlimSAM",local:"slimsam",headingTag:"h2"}}),Ne=new P({props:{title:"Grounded SAM",local:"grounded-sam",headingTag:"h2"}}),Ze=new P({props:{title:"SamConfig",local:"transformers.SamConfig",headingTag:"h2"}}),Re=new M({props:{name:"class transformers.SamConfig",anchor:"transformers.SamConfig",parameters:[{name:"vision_config",val:" = None"},{name:"prompt_encoder_config",val:" = None"},{name:"mask_decoder_config",val:" = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamConfig.vision_config",description:`<strong>vision_config</strong> (Union[<code>dict</code>, <code>SamVisionConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionConfig">SamVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.SamConfig.prompt_encoder_config",description:`<strong>prompt_encoder_config</strong> (Union[<code>dict</code>, <code>SamPromptEncoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamPromptEncoderConfig">SamPromptEncoderConfig</a>.`,name:"prompt_encoder_config"},{anchor:"transformers.SamConfig.mask_decoder_config",description:`<strong>mask_decoder_config</strong> (Union[<code>dict</code>, <code>SamMaskDecoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamMaskDecoderConfig">SamMaskDecoderConfig</a>.`,name:"mask_decoder_config"},{anchor:"transformers.SamConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/configuration_sam.py#L254"}}),Y=new cn({props:{anchor:"transformers.SamConfig.example",$$slots:{default:[ua]},$$scope:{ctx:N}}}),Ve=new P({props:{title:"SamVisionConfig",local:"transformers.SamVisionConfig",headingTag:"h2"}}),Be=new M({props:{name:"class transformers.SamVisionConfig",anchor:"transformers.SamVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"output_channels",val:" = 256"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 1024"},{name:"patch_size",val:" = 16"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 1e-10"},{name:"qkv_bias",val:" = True"},{name:"mlp_ratio",val:" = 4.0"},{name:"use_abs_pos",val:" = True"},{name:"use_rel_pos",val:" = True"},{name:"window_size",val:" = 14"},{name:"global_attn_indexes",val:" = [2, 5, 8, 11]"},{name:"num_pos_feats",val:" = 128"},{name:"mlp_dim",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.SamVisionConfig.output_channels",description:`<strong>output_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the output channels in the Patch Encoder.`,name:"output_channels"},{anchor:"transformers.SamVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.SamVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.SamVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of channels in the input image.`,name:"num_channels"},{anchor:"transformers.SamVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Expected resolution. Target size of the resized input image.`,name:"image_size"},{anchor:"transformers.SamVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Size of the patches to be extracted from the input image.`,name:"patch_size"},{anchor:"transformers.SamVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string)`,name:"hidden_act"},{anchor:"transformers.SamVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.SamVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.SamVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-10) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.SamVisionConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to query, key, value projections.`,name:"qkv_bias"},{anchor:"transformers.SamVisionConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of mlp hidden dim to embedding dim.`,name:"mlp_ratio"},{anchor:"transformers.SamVisionConfig.use_abs_pos",description:`<strong>use_abs_pos</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use absolute position embedding.`,name:"use_abs_pos"},{anchor:"transformers.SamVisionConfig.use_rel_pos",description:`<strong>use_rel_pos</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use relative position embedding.`,name:"use_rel_pos"},{anchor:"transformers.SamVisionConfig.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Window size for relative position.`,name:"window_size"},{anchor:"transformers.SamVisionConfig.global_attn_indexes",description:`<strong>global_attn_indexes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 5, 8, 11]</code>) &#x2014;
The indexes of the global attention layers.`,name:"global_attn_indexes"},{anchor:"transformers.SamVisionConfig.num_pos_feats",description:`<strong>num_pos_feats</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The dimensionality of the position embedding.`,name:"num_pos_feats"},{anchor:"transformers.SamVisionConfig.mlp_dim",description:`<strong>mlp_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The dimensionality of the MLP layer in the Transformer encoder. If <code>None</code>, defaults to <code>mlp_ratio * hidden_size</code>.`,name:"mlp_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/configuration_sam.py#L136"}}),Q=new cn({props:{anchor:"transformers.SamVisionConfig.example",$$slots:{default:[_a]},$$scope:{ctx:N}}}),Ee=new P({props:{title:"SamMaskDecoderConfig",local:"transformers.SamMaskDecoderConfig",headingTag:"h2"}}),De=new M({props:{name:"class transformers.SamMaskDecoderConfig",anchor:"transformers.SamMaskDecoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"hidden_act",val:" = 'relu'"},{name:"mlp_dim",val:" = 2048"},{name:"num_hidden_layers",val:" = 2"},{name:"num_attention_heads",val:" = 8"},{name:"attention_downsample_rate",val:" = 2"},{name:"num_multimask_outputs",val:" = 3"},{name:"iou_head_depth",val:" = 3"},{name:"iou_head_hidden_dim",val:" = 256"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamMaskDecoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.SamMaskDecoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function used inside the <code>SamMaskDecoder</code> module.`,name:"hidden_act"},{anchor:"transformers.SamMaskDecoderConfig.mlp_dim",description:`<strong>mlp_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"mlp_dim"},{anchor:"transformers.SamMaskDecoderConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.SamMaskDecoderConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.SamMaskDecoderConfig.attention_downsample_rate",description:`<strong>attention_downsample_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The downsampling rate of the attention layer.`,name:"attention_downsample_rate"},{anchor:"transformers.SamMaskDecoderConfig.num_multimask_outputs",description:`<strong>num_multimask_outputs</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of outputs from the <code>SamMaskDecoder</code> module. In the Segment Anything paper, this is set to 3.`,name:"num_multimask_outputs"},{anchor:"transformers.SamMaskDecoderConfig.iou_head_depth",description:`<strong>iou_head_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of layers in the IoU head module.`,name:"iou_head_depth"},{anchor:"transformers.SamMaskDecoderConfig.iou_head_hidden_dim",description:`<strong>iou_head_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimensionality of the hidden states in the IoU head module.`,name:"iou_head_hidden_dim"},{anchor:"transformers.SamMaskDecoderConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/configuration_sam.py#L73"}}),Le=new P({props:{title:"SamPromptEncoderConfig",local:"transformers.SamPromptEncoderConfig",headingTag:"h2"}}),qe=new M({props:{name:"class transformers.SamPromptEncoderConfig",anchor:"transformers.SamPromptEncoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"image_size",val:" = 1024"},{name:"patch_size",val:" = 16"},{name:"mask_input_channels",val:" = 16"},{name:"num_point_embeddings",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamPromptEncoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.SamPromptEncoderConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The expected output resolution of the image.`,name:"image_size"},{anchor:"transformers.SamPromptEncoderConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.SamPromptEncoderConfig.mask_input_channels",description:`<strong>mask_input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The number of channels to be fed to the <code>MaskDecoder</code> module.`,name:"mask_input_channels"},{anchor:"transformers.SamPromptEncoderConfig.num_point_embeddings",description:`<strong>num_point_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of point embeddings to be used.`,name:"num_point_embeddings"},{anchor:"transformers.SamPromptEncoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the encoder and pooler.`,name:"hidden_act"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/configuration_sam.py#L24"}}),He=new P({props:{title:"SamProcessor",local:"transformers.SamProcessor",headingTag:"h2"}}),Ae=new M({props:{name:"class transformers.SamProcessor",anchor:"transformers.SamProcessor",parameters:[{name:"image_processor",val:""}],parametersDescription:[{anchor:"transformers.SamProcessor.image_processor",description:`<strong>image_processor</strong> (<code>SamImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a>. The image processor is a required input.`,name:"image_processor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/processing_sam.py#L55"}}),Xe=new P({props:{title:"SamImageProcessor",local:"transformers.SamImageProcessor",headingTag:"h2"}}),Ge=new M({props:{name:"class transformers.SamImageProcessor",anchor:"transformers.SamImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"mask_size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_pad",val:": bool = True"},{name:"pad_size",val:": typing.Optional[int] = None"},{name:"mask_pad_size",val:": typing.Optional[int] = None"},{name:"do_convert_rgb",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.SamImageProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;longest_edge&quot; -- 1024}</code>):
Size of the output image after resizing. Resizes the longest edge of the image to match
<code>size[&quot;longest_edge&quot;]</code> while maintaining the aspect ratio. Can be overridden by the <code>size</code> parameter in the
<code>preprocess</code> method.`,name:"size"},{anchor:"transformers.SamImageProcessor.mask_size",description:`<strong>mask_size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;longest_edge&quot; -- 256}</code>):
Size of the output segmentation map after resizing. Resizes the longest edge of the image to match
<code>size[&quot;longest_edge&quot;]</code> while maintaining the aspect ratio. Can be overridden by the <code>mask_size</code> parameter
in the <code>preprocess</code> method.`,name:"mask_size"},{anchor:"transformers.SamImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.SamImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Wwhether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.SamImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.SamImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.SamImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.SamImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.SamImageProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to pad the image to the specified <code>pad_size</code>. Can be overridden by the <code>do_pad</code> parameter in the
<code>preprocess</code> method.`,name:"do_pad"},{anchor:"transformers.SamImageProcessor.pad_size",description:`<strong>pad_size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 1024, &quot;width&quot;: 1024}</code>):
Size of the output image after padding. Can be overridden by the <code>pad_size</code> parameter in the <code>preprocess</code>
method.`,name:"pad_size"},{anchor:"transformers.SamImageProcessor.mask_pad_size",description:`<strong>mask_pad_size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 256, &quot;width&quot;: 256}</code>):
Size of the output segmentation map after padding. Can be overridden by the <code>mask_pad_size</code> parameter in
the <code>preprocess</code> method.`,name:"mask_pad_size"},{anchor:"transformers.SamImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L67"}}),Ye=new M({props:{name:"filter_masks",anchor:"transformers.SamImageProcessor.filter_masks",parameters:[{name:"masks",val:""},{name:"iou_scores",val:""},{name:"original_size",val:""},{name:"cropped_box_image",val:""},{name:"pred_iou_thresh",val:" = 0.88"},{name:"stability_score_thresh",val:" = 0.95"},{name:"mask_threshold",val:" = 0"},{name:"stability_score_offset",val:" = 1"},{name:"return_tensors",val:" = 'pt'"}],parametersDescription:[{anchor:"transformers.SamImageProcessor.filter_masks.masks",description:`<strong>masks</strong> (<code>Union[torch.Tensor, tf.Tensor]</code>) &#x2014;
Input masks.`,name:"masks"},{anchor:"transformers.SamImageProcessor.filter_masks.iou_scores",description:`<strong>iou_scores</strong> (<code>Union[torch.Tensor, tf.Tensor]</code>) &#x2014;
List of IoU scores.`,name:"iou_scores"},{anchor:"transformers.SamImageProcessor.filter_masks.original_size",description:`<strong>original_size</strong> (<code>tuple[int,int]</code>) &#x2014;
Size of the original image.`,name:"original_size"},{anchor:"transformers.SamImageProcessor.filter_masks.cropped_box_image",description:`<strong>cropped_box_image</strong> (<code>np.array</code>) &#x2014;
The cropped image.`,name:"cropped_box_image"},{anchor:"transformers.SamImageProcessor.filter_masks.pred_iou_thresh",description:`<strong>pred_iou_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.88) &#x2014;
The threshold for the iou scores.`,name:"pred_iou_thresh"},{anchor:"transformers.SamImageProcessor.filter_masks.stability_score_thresh",description:`<strong>stability_score_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.95) &#x2014;
The threshold for the stability score.`,name:"stability_score_thresh"},{anchor:"transformers.SamImageProcessor.filter_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The threshold for the predicted masks.`,name:"mask_threshold"},{anchor:"transformers.SamImageProcessor.filter_masks.stability_score_offset",description:`<strong>stability_score_offset</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The offset for the stability score used in the <code>_compute_stability_score</code> method.`,name:"stability_score_offset"},{anchor:"transformers.SamImageProcessor.filter_masks.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>pt</code>) &#x2014;
If <code>pt</code>, returns <code>torch.Tensor</code>. If <code>tf</code>, returns <code>tf.Tensor</code>.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L811"}}),Qe=new M({props:{name:"generate_crop_boxes",anchor:"transformers.SamImageProcessor.generate_crop_boxes",parameters:[{name:"image",val:""},{name:"target_size",val:""},{name:"crop_n_layers",val:": int = 0"},{name:"overlap_ratio",val:": float = 0.3413333333333333"},{name:"points_per_crop",val:": typing.Optional[int] = 32"},{name:"crop_n_points_downscale_factor",val:": typing.Optional[list[int]] = 1"},{name:"device",val:": typing.Optional[ForwardRef('torch.device')] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.SamImageProcessor.generate_crop_boxes.image",description:`<strong>image</strong> (<code>np.array</code>) &#x2014;
Input original image`,name:"image"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.target_size",description:`<strong>target_size</strong> (<code>int</code>) &#x2014;
Target size of the resized image`,name:"target_size"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.crop_n_layers",description:`<strong>crop_n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where
each layer has 2**i_layer number of image crops.`,name:"crop_n_layers"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.overlap_ratio",description:`<strong>overlap_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 512/1500) &#x2014;
Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of
the image length. Later layers with more crops scale down this overlap.`,name:"overlap_ratio"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.points_per_crop",description:`<strong>points_per_crop</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of points to sample from each crop.`,name:"points_per_crop"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.crop_n_points_downscale_factor",description:`<strong>crop_n_points_downscale_factor</strong> (<code>list[int]</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.`,name:"crop_n_points_downscale_factor"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to None) &#x2014;
Device to use for the computation. If None, cpu will be used.`,name:"device"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred.`,name:"input_data_format"},{anchor:"transformers.SamImageProcessor.generate_crop_boxes.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>pt</code>) &#x2014;
If <code>pt</code>, returns <code>torch.Tensor</code>. If <code>tf</code>, returns <code>tf.Tensor</code>.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L746"}}),Oe=new M({props:{name:"pad_image",anchor:"transformers.SamImageProcessor.pad_image",parameters:[{name:"image",val:": ndarray"},{name:"pad_size",val:": dict"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamImageProcessor.pad_image.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to pad.`,name:"image"},{anchor:"transformers.SamImageProcessor.pad_image.pad_size",description:`<strong>pad_size</strong> (<code>dict[str, int]</code>) &#x2014;
Size of the output image after padding.`,name:"pad_size"},{anchor:"transformers.SamImageProcessor.pad_image.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The data format of the image. Can be either &#x201C;channels_first&#x201D; or &#x201C;channels_last&#x201D;. If <code>None</code>, the
<code>data_format</code> of the <code>image</code> will be used.`,name:"data_format"},{anchor:"transformers.SamImageProcessor.pad_image.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred.`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L166"}}),Ke=new M({props:{name:"post_process_for_mask_generation",anchor:"transformers.SamImageProcessor.post_process_for_mask_generation",parameters:[{name:"all_masks",val:""},{name:"all_scores",val:""},{name:"all_boxes",val:""},{name:"crops_nms_thresh",val:""},{name:"return_tensors",val:" = 'pt'"}],parametersDescription:[{anchor:"transformers.SamImageProcessor.post_process_for_mask_generation.all_masks",description:`<strong>all_masks</strong> (<code>Union[list[torch.Tensor], list[tf.Tensor]]</code>) &#x2014;
List of all predicted segmentation masks`,name:"all_masks"},{anchor:"transformers.SamImageProcessor.post_process_for_mask_generation.all_scores",description:`<strong>all_scores</strong> (<code>Union[list[torch.Tensor], list[tf.Tensor]]</code>) &#x2014;
List of all predicted iou scores`,name:"all_scores"},{anchor:"transformers.SamImageProcessor.post_process_for_mask_generation.all_boxes",description:`<strong>all_boxes</strong> (<code>Union[list[torch.Tensor], list[tf.Tensor]]</code>) &#x2014;
List of all bounding boxes of the predicted masks`,name:"all_boxes"},{anchor:"transformers.SamImageProcessor.post_process_for_mask_generation.crops_nms_thresh",description:`<strong>crops_nms_thresh</strong> (<code>float</code>) &#x2014;
Threshold for NMS (Non Maximum Suppression) algorithm.`,name:"crops_nms_thresh"},{anchor:"transformers.SamImageProcessor.post_process_for_mask_generation.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>pt</code>) &#x2014;
If <code>pt</code>, returns <code>torch.Tensor</code>. If <code>tf</code>, returns <code>tf.Tensor</code>.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L723"}}),eo=new M({props:{name:"post_process_masks",anchor:"transformers.SamImageProcessor.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"reshaped_input_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"pad_size",val:" = None"},{name:"return_tensors",val:" = 'pt'"}],parametersDescription:[{anchor:"transformers.SamImageProcessor.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[list[torch.Tensor], list[np.ndarray], list[tf.Tensor]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.SamImageProcessor.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, tf.Tensor, list[tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.SamImageProcessor.post_process_masks.reshaped_input_sizes",description:`<strong>reshaped_input_sizes</strong> (<code>Union[torch.Tensor, tf.Tensor, list[tuple[int,int]]]</code>) &#x2014;
The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.`,name:"reshaped_input_sizes"},{anchor:"transformers.SamImageProcessor.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The threshold to use for binarizing the masks.`,name:"mask_threshold"},{anchor:"transformers.SamImageProcessor.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.SamImageProcessor.post_process_masks.pad_size",description:`<strong>pad_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.pad_size</code>) &#x2014;
The target size the images were padded to before being passed to the model. If None, the target size is
assumed to be the processor&#x2019;s <code>pad_size</code>.`,name:"pad_size"},{anchor:"transformers.SamImageProcessor.post_process_masks.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
If <code>&quot;pt&quot;</code>, return PyTorch tensors. If <code>&quot;tf&quot;</code>, return TensorFlow tensors.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L579",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where
(height, width) is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>Union[torch.Tensor, tf.Tensor]</code>)</p>
`}}),oo=new M({props:{name:"preprocess",anchor:"transformers.SamImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"segmentation_maps",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"mask_size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": typing.Optional[ForwardRef('PILImageResampling')] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Union[int, float, NoneType] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_pad",val:": typing.Optional[bool] = None"},{name:"pad_size",val:": typing.Optional[dict[str, int]] = None"},{name:"mask_pad_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SamImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.SamImageProcessor.preprocess.segmentation_maps",description:`<strong>segmentation_maps</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
Segmentation map to preprocess.`,name:"segmentation_maps"},{anchor:"transformers.SamImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.SamImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Controls the size of the image after <code>resize</code>. The longest edge of the image is resized to
<code>size[&quot;longest_edge&quot;]</code> whilst preserving the aspect ratio.`,name:"size"},{anchor:"transformers.SamImageProcessor.preprocess.mask_size",description:`<strong>mask_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.mask_size</code>) &#x2014;
Controls the size of the segmentation map after <code>resize</code>. The longest edge of the image is resized to
<code>size[&quot;longest_edge&quot;]</code> whilst preserving the aspect ratio.`,name:"mask_size"},{anchor:"transformers.SamImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
<code>PILImageResampling</code> filter to use when resizing the image e.g. <code>PILImageResampling.BILINEAR</code>.`,name:"resample"},{anchor:"transformers.SamImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image pixel values by rescaling factor.`,name:"do_rescale"},{anchor:"transformers.SamImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to apply to the image pixel values.`,name:"rescale_factor"},{anchor:"transformers.SamImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.SamImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.SamImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.SamImageProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_pad</code>) &#x2014;
Whether to pad the image.`,name:"do_pad"},{anchor:"transformers.SamImageProcessor.preprocess.pad_size",description:`<strong>pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.pad_size</code>) &#x2014;
Controls the size of the padding applied to the image. The image is padded to <code>pad_size[&quot;height&quot;]</code> and
<code>pad_size[&quot;width&quot;]</code> if <code>do_pad</code> is set to <code>True</code>.`,name:"pad_size"},{anchor:"transformers.SamImageProcessor.preprocess.mask_pad_size",description:`<strong>mask_pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.mask_pad_size</code>) &#x2014;
Controls the size of the padding applied to the segmentation map. The image is padded to
<code>mask_pad_size[&quot;height&quot;]</code> and <code>mask_pad_size[&quot;width&quot;]</code> if <code>do_pad</code> is set to <code>True</code>.`,name:"mask_pad_size"},{anchor:"transformers.SamImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.SamImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.SamImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SamImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L395"}}),to=new M({props:{name:"resize",anchor:"transformers.SamImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.SamImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary in the format <code>{&quot;longest_edge&quot;: int}</code> specifying the size of the output image. The longest
edge of the image will be resized to the specified size, while the other edge will be resized to
maintain the aspect ratio.`,name:"size"},{anchor:"transformers.SamImageProcessor.resize.resample",description:`<strong>resample</strong> &#x2014;
<code>PILImageResampling</code> filter to use when resizing the image e.g. <code>PILImageResampling.BILINEAR</code>.`,name:"resample"},{anchor:"transformers.SamImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SamImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam.py#L214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),so=new P({props:{title:"SamImageProcessorFast",local:"transformers.SamImageProcessorFast",headingTag:"h2"}}),no=new M({props:{name:"class transformers.SamImageProcessorFast",anchor:"transformers.SamImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.sam.image_processing_sam_fast.SamFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L85"}}),ao=new M({props:{name:"filter_masks",anchor:"transformers.SamImageProcessorFast.filter_masks",parameters:[{name:"masks",val:""},{name:"iou_scores",val:""},{name:"original_size",val:""},{name:"cropped_box_image",val:""},{name:"pred_iou_thresh",val:" = 0.88"},{name:"stability_score_thresh",val:" = 0.95"},{name:"mask_threshold",val:" = 0"},{name:"stability_score_offset",val:" = 1"}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.filter_masks.masks",description:`<strong>masks</strong> (<code>torch.Tensor</code>) &#x2014;
Input masks.`,name:"masks"},{anchor:"transformers.SamImageProcessorFast.filter_masks.iou_scores",description:`<strong>iou_scores</strong> (<code>torch.Tensor</code>) &#x2014;
List of IoU scores.`,name:"iou_scores"},{anchor:"transformers.SamImageProcessorFast.filter_masks.original_size",description:`<strong>original_size</strong> (<code>tuple[int,int]</code>) &#x2014;
Size of the original image.`,name:"original_size"},{anchor:"transformers.SamImageProcessorFast.filter_masks.cropped_box_image",description:`<strong>cropped_box_image</strong> (<code>torch.Tensor</code>) &#x2014;
The cropped image.`,name:"cropped_box_image"},{anchor:"transformers.SamImageProcessorFast.filter_masks.pred_iou_thresh",description:`<strong>pred_iou_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.88) &#x2014;
The threshold for the iou scores.`,name:"pred_iou_thresh"},{anchor:"transformers.SamImageProcessorFast.filter_masks.stability_score_thresh",description:`<strong>stability_score_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.95) &#x2014;
The threshold for the stability score.`,name:"stability_score_thresh"},{anchor:"transformers.SamImageProcessorFast.filter_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The threshold for the predicted masks.`,name:"mask_threshold"},{anchor:"transformers.SamImageProcessorFast.filter_masks.stability_score_offset",description:`<strong>stability_score_offset</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The offset for the stability score used in the <code>_compute_stability_score</code> method.`,name:"stability_score_offset"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L366"}}),ro=new M({props:{name:"generate_crop_boxes",anchor:"transformers.SamImageProcessorFast.generate_crop_boxes",parameters:[{name:"image",val:": torch.Tensor"},{name:"target_size",val:""},{name:"crop_n_layers",val:": int = 0"},{name:"overlap_ratio",val:": float = 0.3413333333333333"},{name:"points_per_crop",val:": typing.Optional[int] = 32"},{name:"crop_n_points_downscale_factor",val:": typing.Optional[list[int]] = 1"},{name:"device",val:": typing.Optional[ForwardRef('torch.device')] = None"}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.image",description:`<strong>image</strong> (<code>torch.Tensor</code>) &#x2014;
Input original image`,name:"image"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.target_size",description:`<strong>target_size</strong> (<code>int</code>) &#x2014;
Target size of the resized image`,name:"target_size"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.crop_n_layers",description:`<strong>crop_n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where
each layer has 2**i_layer number of image crops.`,name:"crop_n_layers"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.overlap_ratio",description:`<strong>overlap_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 512/1500) &#x2014;
Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of
the image length. Later layers with more crops scale down this overlap.`,name:"overlap_ratio"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.points_per_crop",description:`<strong>points_per_crop</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of points to sample from each crop.`,name:"points_per_crop"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.crop_n_points_downscale_factor",description:`<strong>crop_n_points_downscale_factor</strong> (<code>list[int]</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.`,name:"crop_n_points_downscale_factor"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to None) &#x2014;
Device to use for the computation. If None, cpu will be used.`,name:"device"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred.`,name:"input_data_format"},{anchor:"transformers.SamImageProcessorFast.generate_crop_boxes.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>pt</code>) &#x2014;
If <code>pt</code>, returns <code>torch.Tensor</code>. If <code>tf</code>, returns <code>tf.Tensor</code>.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L313"}}),io=new M({props:{name:"pad_image",anchor:"transformers.SamImageProcessorFast.pad_image",parameters:[{name:"images",val:": torch.Tensor"},{name:"pad_size",val:": SizeDict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L105"}}),mo=new M({props:{name:"post_process_for_mask_generation",anchor:"transformers.SamImageProcessorFast.post_process_for_mask_generation",parameters:[{name:"all_masks",val:""},{name:"all_scores",val:""},{name:"all_boxes",val:""},{name:"crops_nms_thresh",val:""}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.post_process_for_mask_generation.all_masks",description:`<strong>all_masks</strong> (<code>torch.Tensor</code>) &#x2014;
List of all predicted segmentation masks`,name:"all_masks"},{anchor:"transformers.SamImageProcessorFast.post_process_for_mask_generation.all_scores",description:`<strong>all_scores</strong> (<code>torch.Tensor</code>) &#x2014;
List of all predicted iou scores`,name:"all_scores"},{anchor:"transformers.SamImageProcessorFast.post_process_for_mask_generation.all_boxes",description:`<strong>all_boxes</strong> (<code>torch.Tensor</code>) &#x2014;
List of all bounding boxes of the predicted masks`,name:"all_boxes"},{anchor:"transformers.SamImageProcessorFast.post_process_for_mask_generation.crops_nms_thresh",description:`<strong>crops_nms_thresh</strong> (<code>float</code>) &#x2014;
Threshold for NMS (Non Maximum Suppression) algorithm.`,name:"crops_nms_thresh"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L498"}}),co=new M({props:{name:"post_process_masks",anchor:"transformers.SamImageProcessorFast.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"reshaped_input_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"pad_size",val:" = None"}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[List[torch.Tensor], List[np.ndarray]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.SamImageProcessorFast.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.SamImageProcessorFast.post_process_masks.reshaped_input_sizes",description:`<strong>reshaped_input_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.`,name:"reshaped_input_sizes"},{anchor:"transformers.SamImageProcessorFast.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The threshold to use for binarizing the masks.`,name:"mask_threshold"},{anchor:"transformers.SamImageProcessorFast.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.SamImageProcessorFast.post_process_masks.pad_size",description:`<strong>pad_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.pad_size</code>) &#x2014;
The target size the images were padded to before being passed to the model. If None, the target size is
assumed to be the processor&#x2019;s <code>pad_size</code>.`,name:"pad_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L445",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where (height, width)
is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>torch.Tensor</code>)</p>
`}}),lo=new M({props:{name:"preprocess",anchor:"transformers.SamImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"segmentation_maps",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.sam.image_processing_sam_fast.SamFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.SamImageProcessorFast.preprocess.segmentation_maps",description:`<strong>segmentation_maps</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The segmentation maps to preprocess.`,name:"segmentation_maps"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.SamImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.SamImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.SamImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.SamImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.SamImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.SamImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.SamImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.SamImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.SamImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.SamImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.SamImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.SamImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"},{anchor:"transformers.SamImageProcessorFast.preprocess.mask_size",description:`<strong>mask_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
The size <code>{&quot;longest_edge&quot;: int}</code> to resize the segmentation maps to.`,name:"mask_size"},{anchor:"transformers.SamImageProcessorFast.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to pad the image. Can be overridden by the <code>do_pad</code> parameter in the <code>preprocess</code>
method. If <code>True</code>, padding will be applied to the bottom and right of the image with zeros.`,name:"do_pad"},{anchor:"transformers.SamImageProcessorFast.preprocess.pad_size",description:`<strong>pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
The size <code>{&quot;height&quot;: int, &quot;width&quot; int}</code> to pad the images to. Must be larger than any image size
provided for preprocessing.`,name:"pad_size"},{anchor:"transformers.SamImageProcessorFast.preprocess.mask_pad_size",description:`<strong>mask_pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
The size <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code> to pad the segmentation maps to. Must be larger than any segmentation
map size provided for preprocessing.`,name:"mask_pad_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L204",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) ‚Äî Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‚Äòpixel_values‚Äô, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) ‚Äî You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),po=new M({props:{name:"resize",anchor:"transformers.SamImageProcessorFast.resize",parameters:[{name:"image",val:": torch.Tensor"},{name:"size",val:": SizeDict"},{name:"interpolation",val:": typing.Optional[ForwardRef('F_t.InterpolationMode')]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SamImageProcessorFast.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.SamImageProcessorFast.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary in the format <code>{&quot;longest_edge&quot;: int}</code> specifying the size of the output image. The longest
edge of the image will be resized to the specified size, while the other edge will be resized to
maintain the aspect ratio.`,name:"size"},{anchor:"transformers.SamImageProcessorFast.resize.interpolation",description:`<strong>interpolation</strong> &#x2014;
<code>F_t.InterpolationMode</code> filter to use when resizing the image e.g. <code>F_t.InterpolationMode.BICUBIC</code>.`,name:"interpolation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/image_processing_sam_fast.py#L125",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),go=new P({props:{title:"SamVisionModel",local:"transformers.SamVisionModel",headingTag:"h2"}}),ho=new M({props:{name:"class transformers.SamVisionModel",anchor:"transformers.SamVisionModel",parameters:[{name:"config",val:": SamVisionConfig"}],parametersDescription:[{anchor:"transformers.SamVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionConfig">SamVisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/modeling_sam.py#L1085"}}),fo=new M({props:{name:"forward",anchor:"transformers.SamVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.SamVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a>. See <code>SamImageProcessor.__call__()</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamProcessor">SamProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a> for processing images).`,name:"pixel_values"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/modeling_sam.py#L1097",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.sam.modeling_sam.SamVisionEncoderOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig"
>SamConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) ‚Äî The image embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.sam.modeling_sam.SamVisionEncoderOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new ma({props:{$$slots:{default:[ba]},$$scope:{ctx:N}}}),uo=new P({props:{title:"SamModel",local:"transformers.SamModel",headingTag:"h2"}}),_o=new M({props:{name:"class transformers.SamModel",anchor:"transformers.SamModel",parameters:[{name:"config",val:": SamConfig"}],parametersDescription:[{anchor:"transformers.SamModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig">SamConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/modeling_sam.py#L1112"}}),bo=new M({props:{name:"forward",anchor:"transformers.SamModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_points",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_boxes",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_masks",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_embeddings",val:": typing.Optional[torch.FloatTensor] = None"},{name:"multimask_output",val:": bool = True"},{name:"attention_similarity",val:": typing.Optional[torch.FloatTensor] = None"},{name:"target_embedding",val:": typing.Optional[torch.FloatTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.SamModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a>. See <code>SamImageProcessor.__call__()</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamProcessor">SamProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SamModel.forward.input_points",description:`<strong>input_points</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_points, 2)</code>) &#x2014;
Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much
better results. The points can be obtained by passing a list of list of list to the processor that will
create corresponding <code>torch</code> tensors of dimension 4. The first dimension is the image batch size, the
second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict
per input point), the third dimension is the number of points per segmentation mask (it is possible to pass
multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)
coordinates of the point. If a different number of points is passed either for each image, or for each
mask, the processor will create &#x201C;PAD&#x201D; points that will correspond to the (0, 0) coordinate, and the
computation of the embedding will be skipped for these points using the labels.`,name:"input_points"},{anchor:"transformers.SamModel.forward.input_labels",description:`<strong>input_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, point_batch_size, num_points)</code>) &#x2014;
Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the
official implementation, there are 3 types of labels</p>
<ul>
<li><code>1</code>: the point is a point that contains the object of interest</li>
<li><code>0</code>: the point is a point that does not contain the object of interest</li>
<li><code>-1</code>: the point corresponds to the background</li>
</ul>
<p>We added the label:</p>
<ul>
<li><code>-10</code>: the point is a padding point, thus should be ignored by the prompt encoder</li>
</ul>
<p>The padding labels should be automatically done by the processor.`,name:"input_labels"},{anchor:"transformers.SamModel.forward.input_boxes",description:`<strong>input_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_boxes, 4)</code>) &#x2014;
Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to
much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,
that will generate a <code>torch</code> tensor, with each dimension corresponding respectively to the image batch
size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.
In the order (<code>x1</code>, <code>y1</code>, <code>x2</code>, <code>y2</code>):</p>
<ul>
<li><code>x1</code>: the x coordinate of the top left point of the input box</li>
<li><code>y1</code>: the y coordinate of the top left point of the input box</li>
<li><code>x2</code>: the x coordinate of the bottom right point of the input box</li>
<li><code>y2</code>: the y coordinate of the bottom right point of the input box</li>
</ul>`,name:"input_boxes"},{anchor:"transformers.SamModel.forward.input_masks",description:`<strong>input_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_size, image_size)</code>) &#x2014;
SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to
generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be
manually fed by the user, and they need to be of shape (<code>batch_size</code>, <code>image_size</code>, <code>image_size</code>).`,name:"input_masks"},{anchor:"transformers.SamModel.forward.image_embeddings",description:`<strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_channels, window_size, window_size)</code>) &#x2014;
Image embeddings, this is used by the mask decder to generate masks and iou scores. For more memory
efficient computation, users can first retrieve the image embeddings using the <code>get_image_embeddings</code>
method, and then feed them to the <code>forward</code> method instead of feeding the <code>pixel_values</code>.`,name:"image_embeddings"},{anchor:"transformers.SamModel.forward.multimask_output",description:`<strong>multimask_output</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
In the original implementation and paper, the model always outputs 3 masks per image (or per point / per
bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the
&#x201C;best&#x201D; mask, by specifying <code>multimask_output=False</code>.`,name:"multimask_output"},{anchor:"transformers.SamModel.forward.attention_similarity",description:`<strong>attention_similarity</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the
model is used for personalization as introduced in <a href="https://huggingface.co/papers/2305.03048" rel="nofollow">PerSAM</a>.`,name:"attention_similarity"},{anchor:"transformers.SamModel.forward.target_embedding",description:`<strong>target_embedding</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case
the model is used for personalization as introduced in <a href="https://huggingface.co/papers/2305.03048" rel="nofollow">PerSAM</a>.`,name:"target_embedding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam/modeling_sam.py#L1200",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.sam.modeling_sam.SamImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig"
>SamConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>iou_scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_masks)</code>) ‚Äî The iou scores of the predicted masks.</p>
</li>
<li>
<p><strong>pred_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_masks, height, width)</code>) ‚Äî The predicted low resolutions masks. Needs to be post-processed by the processor</p>
</li>
<li>
<p><strong>vision_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>vision_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>mask_decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.sam.modeling_sam.SamImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new ma({props:{$$slots:{default:[va]},$$scope:{ctx:N}}}),ge=new cn({props:{anchor:"transformers.SamModel.forward.example",$$slots:{default:[ya]},$$scope:{ctx:N}}}),vo=new fa({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam.md"}}),{c(){u=r("meta"),x=n(),b=r("p"),v=n(),I=r("p"),I.innerHTML=_,$=n(),d(fe.$$.fragment),nt=n(),X=r("div"),X.innerHTML=dn,at=n(),d(ue.$$.fragment),rt=n(),_e=r("p"),_e.innerHTML=ln,it=n(),be=r("p"),be.textContent=pn,mt=n(),ve=r("p"),ve.innerHTML=gn,ct=n(),ye=r("p"),ye.textContent=hn,dt=n(),Te=r("p"),Te.innerHTML=fn,lt=n(),Me=r("p"),Me.textContent=un,pt=n(),we=r("ul"),we.innerHTML=_n,gt=n(),Se=r("p"),Se.innerHTML=bn,ht=n(),Ie=r("p"),Ie.textContent=vn,ft=n(),d(xe.$$.fragment),ut=n(),ze=r("p"),ze.textContent=yn,_t=n(),d(ke.$$.fragment),bt=n(),d($e.$$.fragment),vt=n(),Ce=r("p"),Ce.textContent=Tn,yt=n(),je=r("ul"),je.innerHTML=Mn,Tt=n(),d(Pe.$$.fragment),Mt=n(),Je=r("p"),Je.innerHTML=wn,wt=n(),Ue=r("p"),Ue.innerHTML=Sn,St=n(),d(Ne.$$.fragment),It=n(),Fe=r("p"),Fe.innerHTML=In,xt=n(),G=r("img"),zt=n(),We=r("small"),We.innerHTML=zn,kt=n(),d(Ze.$$.fragment),$t=n(),J=r("div"),d(Re.$$.fragment),ms=n(),yo=r("p"),yo.innerHTML=kn,cs=n(),To=r("p"),To.innerHTML=$n,ds=n(),d(Y.$$.fragment),Ct=n(),d(Ve.$$.fragment),jt=n(),U=r("div"),d(Be.$$.fragment),ls=n(),Mo=r("p"),Mo.innerHTML=Cn,ps=n(),wo=r("p"),wo.innerHTML=jn,gs=n(),d(Q.$$.fragment),Pt=n(),d(Ee.$$.fragment),Jt=n(),W=r("div"),d(De.$$.fragment),hs=n(),So=r("p"),So.innerHTML=Pn,fs=n(),Io=r("p"),Io.innerHTML=Jn,Ut=n(),d(Le.$$.fragment),Nt=n(),Z=r("div"),d(qe.$$.fragment),us=n(),xo=r("p"),xo.innerHTML=Un,_s=n(),zo=r("p"),zo.innerHTML=Nn,Ft=n(),d(He.$$.fragment),Wt=n(),R=r("div"),d(Ae.$$.fragment),bs=n(),ko=r("p"),ko.textContent=Fn,vs=n(),$o=r("p"),$o.innerHTML=Wn,Zt=n(),d(Xe.$$.fragment),Rt=n(),w=r("div"),d(Ge.$$.fragment),ys=n(),Co=r("p"),Co.textContent=Zn,Ts=n(),O=r("div"),d(Ye.$$.fragment),Ms=n(),jo=r("p"),jo.innerHTML=Rn,ws=n(),K=r("div"),d(Qe.$$.fragment),Ss=n(),Po=r("p"),Po.innerHTML=Vn,Is=n(),ee=r("div"),d(Oe.$$.fragment),xs=n(),Jo=r("p"),Jo.innerHTML=Bn,zs=n(),oe=r("div"),d(Ke.$$.fragment),ks=n(),Uo=r("p"),Uo.textContent=En,$s=n(),te=r("div"),d(eo.$$.fragment),Cs=n(),No=r("p"),No.textContent=Dn,js=n(),se=r("div"),d(oo.$$.fragment),Ps=n(),Fo=r("p"),Fo.textContent=Ln,Js=n(),ne=r("div"),d(to.$$.fragment),Us=n(),Wo=r("p"),Wo.innerHTML=qn,Vt=n(),d(so.$$.fragment),Bt=n(),S=r("div"),d(no.$$.fragment),Ns=n(),Zo=r("p"),Zo.textContent=Hn,Fs=n(),ae=r("div"),d(ao.$$.fragment),Ws=n(),Ro=r("p"),Ro.innerHTML=An,Zs=n(),re=r("div"),d(ro.$$.fragment),Rs=n(),Vo=r("p"),Vo.innerHTML=Xn,Vs=n(),ie=r("div"),d(io.$$.fragment),Bs=n(),Bo=r("p"),Bo.textContent=Gn,Es=n(),me=r("div"),d(mo.$$.fragment),Ds=n(),Eo=r("p"),Eo.textContent=Yn,Ls=n(),ce=r("div"),d(co.$$.fragment),qs=n(),Do=r("p"),Do.textContent=Qn,Hs=n(),Lo=r("div"),d(lo.$$.fragment),As=n(),de=r("div"),d(po.$$.fragment),Xs=n(),qo=r("p"),qo.innerHTML=On,Et=n(),d(go.$$.fragment),Dt=n(),C=r("div"),d(ho.$$.fragment),Gs=n(),Ho=r("p"),Ho.textContent=Kn,Ys=n(),Ao=r("p"),Ao.innerHTML=ea,Qs=n(),Xo=r("p"),Xo.innerHTML=oa,Os=n(),E=r("div"),d(fo.$$.fragment),Ks=n(),Go=r("p"),Go.innerHTML=ta,en=n(),d(le.$$.fragment),Lt=n(),d(uo.$$.fragment),qt=n(),j=r("div"),d(_o.$$.fragment),on=n(),Yo=r("p"),Yo.textContent=sa,tn=n(),Qo=r("p"),Qo.innerHTML=na,sn=n(),Oo=r("p"),Oo.innerHTML=aa,nn=n(),F=r("div"),d(bo.$$.fragment),an=n(),Ko=r("p"),Ko.innerHTML=ra,rn=n(),d(pe.$$.fragment),mn=n(),d(ge.$$.fragment),Ht=n(),d(vo.$$.fragment),At=n(),ot=r("p"),this.h()},l(e){const t=ha("svelte-u9bgzb",document.head);u=i(t,"META",{name:!0,content:!0}),t.forEach(o),x=a(e),b=i(e,"P",{}),T(b).forEach(o),v=a(e),I=i(e,"P",{"data-svelte-h":!0}),c(I)!=="svelte-12dw1vl"&&(I.innerHTML=_),$=a(e),l(fe.$$.fragment,e),nt=a(e),X=i(e,"DIV",{class:!0,"data-svelte-h":!0}),c(X)!=="svelte-13t8s2t"&&(X.innerHTML=dn),at=a(e),l(ue.$$.fragment,e),rt=a(e),_e=i(e,"P",{"data-svelte-h":!0}),c(_e)!=="svelte-101gy51"&&(_e.innerHTML=ln),it=a(e),be=i(e,"P",{"data-svelte-h":!0}),c(be)!=="svelte-ibyk3z"&&(be.textContent=pn),mt=a(e),ve=i(e,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-8e5g7x"&&(ve.innerHTML=gn),ct=a(e),ye=i(e,"P",{"data-svelte-h":!0}),c(ye)!=="svelte-vfdo9a"&&(ye.textContent=hn),dt=a(e),Te=i(e,"P",{"data-svelte-h":!0}),c(Te)!=="svelte-1o5y69o"&&(Te.innerHTML=fn),lt=a(e),Me=i(e,"P",{"data-svelte-h":!0}),c(Me)!=="svelte-axv494"&&(Me.textContent=un),pt=a(e),we=i(e,"UL",{"data-svelte-h":!0}),c(we)!=="svelte-hq35k"&&(we.innerHTML=_n),gt=a(e),Se=i(e,"P",{"data-svelte-h":!0}),c(Se)!=="svelte-3s0ihy"&&(Se.innerHTML=bn),ht=a(e),Ie=i(e,"P",{"data-svelte-h":!0}),c(Ie)!=="svelte-j41zpc"&&(Ie.textContent=vn),ft=a(e),l(xe.$$.fragment,e),ut=a(e),ze=i(e,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-eylhz1"&&(ze.textContent=yn),_t=a(e),l(ke.$$.fragment,e),bt=a(e),l($e.$$.fragment,e),vt=a(e),Ce=i(e,"P",{"data-svelte-h":!0}),c(Ce)!=="svelte-r7mpci"&&(Ce.textContent=Tn),yt=a(e),je=i(e,"UL",{"data-svelte-h":!0}),c(je)!=="svelte-su2mt3"&&(je.innerHTML=Mn),Tt=a(e),l(Pe.$$.fragment,e),Mt=a(e),Je=i(e,"P",{"data-svelte-h":!0}),c(Je)!=="svelte-1eeostr"&&(Je.innerHTML=wn),wt=a(e),Ue=i(e,"P",{"data-svelte-h":!0}),c(Ue)!=="svelte-1kuacy9"&&(Ue.innerHTML=Sn),St=a(e),l(Ne.$$.fragment,e),It=a(e),Fe=i(e,"P",{"data-svelte-h":!0}),c(Fe)!=="svelte-ee7vuf"&&(Fe.innerHTML=In),xt=a(e),G=i(e,"IMG",{src:!0,alt:!0,width:!0}),zt=a(e),We=i(e,"SMALL",{"data-svelte-h":!0}),c(We)!=="svelte-16zie7b"&&(We.innerHTML=zn),kt=a(e),l(Ze.$$.fragment,e),$t=a(e),J=i(e,"DIV",{class:!0});var V=T(J);l(Re.$$.fragment,V),ms=a(V),yo=i(V,"P",{"data-svelte-h":!0}),c(yo)!=="svelte-1jm7mmu"&&(yo.innerHTML=kn),cs=a(V),To=i(V,"P",{"data-svelte-h":!0}),c(To)!=="svelte-1ek1ss9"&&(To.innerHTML=$n),ds=a(V),l(Y.$$.fragment,V),V.forEach(o),Ct=a(e),l(Ve.$$.fragment,e),jt=a(e),U=i(e,"DIV",{class:!0});var B=T(U);l(Be.$$.fragment,B),ls=a(B),Mo=i(B,"P",{"data-svelte-h":!0}),c(Mo)!=="svelte-j6vhy4"&&(Mo.innerHTML=Cn),ps=a(B),wo=i(B,"P",{"data-svelte-h":!0}),c(wo)!=="svelte-1ek1ss9"&&(wo.innerHTML=jn),gs=a(B),l(Q.$$.fragment,B),B.forEach(o),Pt=a(e),l(Ee.$$.fragment,e),Jt=a(e),W=i(e,"DIV",{class:!0});var q=T(W);l(De.$$.fragment,q),hs=a(q),So=i(q,"P",{"data-svelte-h":!0}),c(So)!=="svelte-c8g7g5"&&(So.innerHTML=Pn),fs=a(q),Io=i(q,"P",{"data-svelte-h":!0}),c(Io)!=="svelte-1ek1ss9"&&(Io.innerHTML=Jn),q.forEach(o),Ut=a(e),l(Le.$$.fragment,e),Nt=a(e),Z=i(e,"DIV",{class:!0});var H=T(Z);l(qe.$$.fragment,H),us=a(H),xo=i(H,"P",{"data-svelte-h":!0}),c(xo)!=="svelte-y6v7jy"&&(xo.innerHTML=Un),_s=a(H),zo=i(H,"P",{"data-svelte-h":!0}),c(zo)!=="svelte-1ek1ss9"&&(zo.innerHTML=Nn),H.forEach(o),Ft=a(e),l(He.$$.fragment,e),Wt=a(e),R=i(e,"DIV",{class:!0});var A=T(R);l(Ae.$$.fragment,A),bs=a(A),ko=i(A,"P",{"data-svelte-h":!0}),c(ko)!=="svelte-i07qcs"&&(ko.textContent=Fn),vs=a(A),$o=i(A,"P",{"data-svelte-h":!0}),c($o)!=="svelte-1srl0bz"&&($o.innerHTML=Wn),A.forEach(o),Zt=a(e),l(Xe.$$.fragment,e),Rt=a(e),w=i(e,"DIV",{class:!0});var z=T(w);l(Ge.$$.fragment,z),ys=a(z),Co=i(z,"P",{"data-svelte-h":!0}),c(Co)!=="svelte-iyanpn"&&(Co.textContent=Zn),Ts=a(z),O=i(z,"DIV",{class:!0});var Gt=T(O);l(Ye.$$.fragment,Gt),Ms=a(Gt),jo=i(Gt,"P",{"data-svelte-h":!0}),c(jo)!=="svelte-jpa0fq"&&(jo.innerHTML=Rn),Gt.forEach(o),ws=a(z),K=i(z,"DIV",{class:!0});var Yt=T(K);l(Qe.$$.fragment,Yt),Ss=a(Yt),Po=i(Yt,"P",{"data-svelte-h":!0}),c(Po)!=="svelte-1j7xts3"&&(Po.innerHTML=Vn),Yt.forEach(o),Is=a(z),ee=i(z,"DIV",{class:!0});var Qt=T(ee);l(Oe.$$.fragment,Qt),xs=a(Qt),Jo=i(Qt,"P",{"data-svelte-h":!0}),c(Jo)!=="svelte-1g2f7kg"&&(Jo.innerHTML=Bn),Qt.forEach(o),zs=a(z),oe=i(z,"DIV",{class:!0});var Ot=T(oe);l(Ke.$$.fragment,Ot),ks=a(Ot),Uo=i(Ot,"P",{"data-svelte-h":!0}),c(Uo)!=="svelte-wwrho9"&&(Uo.textContent=En),Ot.forEach(o),$s=a(z),te=i(z,"DIV",{class:!0});var Kt=T(te);l(eo.$$.fragment,Kt),Cs=a(Kt),No=i(Kt,"P",{"data-svelte-h":!0}),c(No)!=="svelte-juomob"&&(No.textContent=Dn),Kt.forEach(o),js=a(z),se=i(z,"DIV",{class:!0});var es=T(se);l(oo.$$.fragment,es),Ps=a(es),Fo=i(es,"P",{"data-svelte-h":!0}),c(Fo)!=="svelte-1x3yxsa"&&(Fo.textContent=Ln),es.forEach(o),Js=a(z),ne=i(z,"DIV",{class:!0});var os=T(ne);l(to.$$.fragment,os),Us=a(os),Wo=i(os,"P",{"data-svelte-h":!0}),c(Wo)!=="svelte-1oee9wu"&&(Wo.innerHTML=qn),os.forEach(o),z.forEach(o),Vt=a(e),l(so.$$.fragment,e),Bt=a(e),S=i(e,"DIV",{class:!0});var k=T(S);l(no.$$.fragment,k),Ns=a(k),Zo=i(k,"P",{"data-svelte-h":!0}),c(Zo)!=="svelte-7j9unx"&&(Zo.textContent=Hn),Fs=a(k),ae=i(k,"DIV",{class:!0});var ts=T(ae);l(ao.$$.fragment,ts),Ws=a(ts),Ro=i(ts,"P",{"data-svelte-h":!0}),c(Ro)!=="svelte-jpa0fq"&&(Ro.innerHTML=An),ts.forEach(o),Zs=a(k),re=i(k,"DIV",{class:!0});var ss=T(re);l(ro.$$.fragment,ss),Rs=a(ss),Vo=i(ss,"P",{"data-svelte-h":!0}),c(Vo)!=="svelte-1j7xts3"&&(Vo.innerHTML=Xn),ss.forEach(o),Vs=a(k),ie=i(k,"DIV",{class:!0});var ns=T(ie);l(io.$$.fragment,ns),Bs=a(ns),Bo=i(ns,"P",{"data-svelte-h":!0}),c(Bo)!=="svelte-1xjbov6"&&(Bo.textContent=Gn),ns.forEach(o),Es=a(k),me=i(k,"DIV",{class:!0});var as=T(me);l(mo.$$.fragment,as),Ds=a(as),Eo=i(as,"P",{"data-svelte-h":!0}),c(Eo)!=="svelte-wwrho9"&&(Eo.textContent=Yn),as.forEach(o),Ls=a(k),ce=i(k,"DIV",{class:!0});var rs=T(ce);l(co.$$.fragment,rs),qs=a(rs),Do=i(rs,"P",{"data-svelte-h":!0}),c(Do)!=="svelte-juomob"&&(Do.textContent=Qn),rs.forEach(o),Hs=a(k),Lo=i(k,"DIV",{class:!0});var ia=T(Lo);l(lo.$$.fragment,ia),ia.forEach(o),As=a(k),de=i(k,"DIV",{class:!0});var is=T(de);l(po.$$.fragment,is),Xs=a(is),qo=i(is,"P",{"data-svelte-h":!0}),c(qo)!=="svelte-1oee9wu"&&(qo.innerHTML=On),is.forEach(o),k.forEach(o),Et=a(e),l(go.$$.fragment,e),Dt=a(e),C=i(e,"DIV",{class:!0});var D=T(C);l(ho.$$.fragment,D),Gs=a(D),Ho=i(D,"P",{"data-svelte-h":!0}),c(Ho)!=="svelte-1g2jbi1"&&(Ho.textContent=Kn),Ys=a(D),Ao=i(D,"P",{"data-svelte-h":!0}),c(Ao)!=="svelte-q52n56"&&(Ao.innerHTML=ea),Qs=a(D),Xo=i(D,"P",{"data-svelte-h":!0}),c(Xo)!=="svelte-hswkmf"&&(Xo.innerHTML=oa),Os=a(D),E=i(D,"DIV",{class:!0});var et=T(E);l(fo.$$.fragment,et),Ks=a(et),Go=i(et,"P",{"data-svelte-h":!0}),c(Go)!=="svelte-5vvcaq"&&(Go.innerHTML=ta),en=a(et),l(le.$$.fragment,et),et.forEach(o),D.forEach(o),Lt=a(e),l(uo.$$.fragment,e),qt=a(e),j=i(e,"DIV",{class:!0});var L=T(j);l(_o.$$.fragment,L),on=a(L),Yo=i(L,"P",{"data-svelte-h":!0}),c(Yo)!=="svelte-y4snng"&&(Yo.textContent=sa),tn=a(L),Qo=i(L,"P",{"data-svelte-h":!0}),c(Qo)!=="svelte-q52n56"&&(Qo.innerHTML=na),sn=a(L),Oo=i(L,"P",{"data-svelte-h":!0}),c(Oo)!=="svelte-hswkmf"&&(Oo.innerHTML=aa),nn=a(L),F=i(L,"DIV",{class:!0});var he=T(F);l(bo.$$.fragment,he),an=a(he),Ko=i(he,"P",{"data-svelte-h":!0}),c(Ko)!=="svelte-1ft1vey"&&(Ko.innerHTML=ra),rn=a(he),l(pe.$$.fragment,he),mn=a(he),l(ge.$$.fragment,he),he.forEach(o),L.forEach(o),Ht=a(e),l(vo.$$.fragment,e),At=a(e),ot=i(e,"P",{}),T(ot).forEach(o),this.h()},h(){y(u,"name","hf:doc:metadata"),y(u,"content",Ma),y(X,"class","flex flex-wrap space-x-1"),da(G.src,xn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png")||y(G,"src",xn),y(G,"alt","drawing"),y(G,"width","900"),y(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Lo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,u),m(e,x,t),m(e,b,t),m(e,v,t),m(e,I,t),m(e,$,t),p(fe,e,t),m(e,nt,t),m(e,X,t),m(e,at,t),p(ue,e,t),m(e,rt,t),m(e,_e,t),m(e,it,t),m(e,be,t),m(e,mt,t),m(e,ve,t),m(e,ct,t),m(e,ye,t),m(e,dt,t),m(e,Te,t),m(e,lt,t),m(e,Me,t),m(e,pt,t),m(e,we,t),m(e,gt,t),m(e,Se,t),m(e,ht,t),m(e,Ie,t),m(e,ft,t),p(xe,e,t),m(e,ut,t),m(e,ze,t),m(e,_t,t),p(ke,e,t),m(e,bt,t),p($e,e,t),m(e,vt,t),m(e,Ce,t),m(e,yt,t),m(e,je,t),m(e,Tt,t),p(Pe,e,t),m(e,Mt,t),m(e,Je,t),m(e,wt,t),m(e,Ue,t),m(e,St,t),p(Ne,e,t),m(e,It,t),m(e,Fe,t),m(e,xt,t),m(e,G,t),m(e,zt,t),m(e,We,t),m(e,kt,t),p(Ze,e,t),m(e,$t,t),m(e,J,t),p(Re,J,null),s(J,ms),s(J,yo),s(J,cs),s(J,To),s(J,ds),p(Y,J,null),m(e,Ct,t),p(Ve,e,t),m(e,jt,t),m(e,U,t),p(Be,U,null),s(U,ls),s(U,Mo),s(U,ps),s(U,wo),s(U,gs),p(Q,U,null),m(e,Pt,t),p(Ee,e,t),m(e,Jt,t),m(e,W,t),p(De,W,null),s(W,hs),s(W,So),s(W,fs),s(W,Io),m(e,Ut,t),p(Le,e,t),m(e,Nt,t),m(e,Z,t),p(qe,Z,null),s(Z,us),s(Z,xo),s(Z,_s),s(Z,zo),m(e,Ft,t),p(He,e,t),m(e,Wt,t),m(e,R,t),p(Ae,R,null),s(R,bs),s(R,ko),s(R,vs),s(R,$o),m(e,Zt,t),p(Xe,e,t),m(e,Rt,t),m(e,w,t),p(Ge,w,null),s(w,ys),s(w,Co),s(w,Ts),s(w,O),p(Ye,O,null),s(O,Ms),s(O,jo),s(w,ws),s(w,K),p(Qe,K,null),s(K,Ss),s(K,Po),s(w,Is),s(w,ee),p(Oe,ee,null),s(ee,xs),s(ee,Jo),s(w,zs),s(w,oe),p(Ke,oe,null),s(oe,ks),s(oe,Uo),s(w,$s),s(w,te),p(eo,te,null),s(te,Cs),s(te,No),s(w,js),s(w,se),p(oo,se,null),s(se,Ps),s(se,Fo),s(w,Js),s(w,ne),p(to,ne,null),s(ne,Us),s(ne,Wo),m(e,Vt,t),p(so,e,t),m(e,Bt,t),m(e,S,t),p(no,S,null),s(S,Ns),s(S,Zo),s(S,Fs),s(S,ae),p(ao,ae,null),s(ae,Ws),s(ae,Ro),s(S,Zs),s(S,re),p(ro,re,null),s(re,Rs),s(re,Vo),s(S,Vs),s(S,ie),p(io,ie,null),s(ie,Bs),s(ie,Bo),s(S,Es),s(S,me),p(mo,me,null),s(me,Ds),s(me,Eo),s(S,Ls),s(S,ce),p(co,ce,null),s(ce,qs),s(ce,Do),s(S,Hs),s(S,Lo),p(lo,Lo,null),s(S,As),s(S,de),p(po,de,null),s(de,Xs),s(de,qo),m(e,Et,t),p(go,e,t),m(e,Dt,t),m(e,C,t),p(ho,C,null),s(C,Gs),s(C,Ho),s(C,Ys),s(C,Ao),s(C,Qs),s(C,Xo),s(C,Os),s(C,E),p(fo,E,null),s(E,Ks),s(E,Go),s(E,en),p(le,E,null),m(e,Lt,t),p(uo,e,t),m(e,qt,t),m(e,j,t),p(_o,j,null),s(j,on),s(j,Yo),s(j,tn),s(j,Qo),s(j,sn),s(j,Oo),s(j,nn),s(j,F),p(bo,F,null),s(F,an),s(F,Ko),s(F,rn),p(pe,F,null),s(F,mn),p(ge,F,null),m(e,Ht,t),p(vo,e,t),m(e,At,t),m(e,ot,t),Xt=!0},p(e,[t]){const V={};t&2&&(V.$$scope={dirty:t,ctx:e}),Y.$set(V);const B={};t&2&&(B.$$scope={dirty:t,ctx:e}),Q.$set(B);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),le.$set(q);const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),pe.$set(H);const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),ge.$set(A)},i(e){Xt||(g(fe.$$.fragment,e),g(ue.$$.fragment,e),g(xe.$$.fragment,e),g(ke.$$.fragment,e),g($e.$$.fragment,e),g(Pe.$$.fragment,e),g(Ne.$$.fragment,e),g(Ze.$$.fragment,e),g(Re.$$.fragment,e),g(Y.$$.fragment,e),g(Ve.$$.fragment,e),g(Be.$$.fragment,e),g(Q.$$.fragment,e),g(Ee.$$.fragment,e),g(De.$$.fragment,e),g(Le.$$.fragment,e),g(qe.$$.fragment,e),g(He.$$.fragment,e),g(Ae.$$.fragment,e),g(Xe.$$.fragment,e),g(Ge.$$.fragment,e),g(Ye.$$.fragment,e),g(Qe.$$.fragment,e),g(Oe.$$.fragment,e),g(Ke.$$.fragment,e),g(eo.$$.fragment,e),g(oo.$$.fragment,e),g(to.$$.fragment,e),g(so.$$.fragment,e),g(no.$$.fragment,e),g(ao.$$.fragment,e),g(ro.$$.fragment,e),g(io.$$.fragment,e),g(mo.$$.fragment,e),g(co.$$.fragment,e),g(lo.$$.fragment,e),g(po.$$.fragment,e),g(go.$$.fragment,e),g(ho.$$.fragment,e),g(fo.$$.fragment,e),g(le.$$.fragment,e),g(uo.$$.fragment,e),g(_o.$$.fragment,e),g(bo.$$.fragment,e),g(pe.$$.fragment,e),g(ge.$$.fragment,e),g(vo.$$.fragment,e),Xt=!0)},o(e){h(fe.$$.fragment,e),h(ue.$$.fragment,e),h(xe.$$.fragment,e),h(ke.$$.fragment,e),h($e.$$.fragment,e),h(Pe.$$.fragment,e),h(Ne.$$.fragment,e),h(Ze.$$.fragment,e),h(Re.$$.fragment,e),h(Y.$$.fragment,e),h(Ve.$$.fragment,e),h(Be.$$.fragment,e),h(Q.$$.fragment,e),h(Ee.$$.fragment,e),h(De.$$.fragment,e),h(Le.$$.fragment,e),h(qe.$$.fragment,e),h(He.$$.fragment,e),h(Ae.$$.fragment,e),h(Xe.$$.fragment,e),h(Ge.$$.fragment,e),h(Ye.$$.fragment,e),h(Qe.$$.fragment,e),h(Oe.$$.fragment,e),h(Ke.$$.fragment,e),h(eo.$$.fragment,e),h(oo.$$.fragment,e),h(to.$$.fragment,e),h(so.$$.fragment,e),h(no.$$.fragment,e),h(ao.$$.fragment,e),h(ro.$$.fragment,e),h(io.$$.fragment,e),h(mo.$$.fragment,e),h(co.$$.fragment,e),h(lo.$$.fragment,e),h(po.$$.fragment,e),h(go.$$.fragment,e),h(ho.$$.fragment,e),h(fo.$$.fragment,e),h(le.$$.fragment,e),h(uo.$$.fragment,e),h(_o.$$.fragment,e),h(bo.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(vo.$$.fragment,e),Xt=!1},d(e){e&&(o(x),o(b),o(v),o(I),o($),o(nt),o(X),o(at),o(rt),o(_e),o(it),o(be),o(mt),o(ve),o(ct),o(ye),o(dt),o(Te),o(lt),o(Me),o(pt),o(we),o(gt),o(Se),o(ht),o(Ie),o(ft),o(ut),o(ze),o(_t),o(bt),o(vt),o(Ce),o(yt),o(je),o(Tt),o(Mt),o(Je),o(wt),o(Ue),o(St),o(It),o(Fe),o(xt),o(G),o(zt),o(We),o(kt),o($t),o(J),o(Ct),o(jt),o(U),o(Pt),o(Jt),o(W),o(Ut),o(Nt),o(Z),o(Ft),o(Wt),o(R),o(Zt),o(Rt),o(w),o(Vt),o(Bt),o(S),o(Et),o(Dt),o(C),o(Lt),o(qt),o(j),o(Ht),o(At),o(ot)),o(u),f(fe,e),f(ue,e),f(xe,e),f(ke,e),f($e,e),f(Pe,e),f(Ne,e),f(Ze,e),f(Re),f(Y),f(Ve,e),f(Be),f(Q),f(Ee,e),f(De),f(Le,e),f(qe),f(He,e),f(Ae),f(Xe,e),f(Ge),f(Ye),f(Qe),f(Oe),f(Ke),f(eo),f(oo),f(to),f(so,e),f(no),f(ao),f(ro),f(io),f(mo),f(co),f(lo),f(po),f(go,e),f(ho),f(fo),f(le),f(uo,e),f(_o),f(bo),f(pe),f(ge),f(vo,e)}}}const Ma='{"title":"SAM","local":"sam","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"SlimSAM","local":"slimsam","sections":[],"depth":2},{"title":"Grounded SAM","local":"grounded-sam","sections":[],"depth":2},{"title":"SamConfig","local":"transformers.SamConfig","sections":[],"depth":2},{"title":"SamVisionConfig","local":"transformers.SamVisionConfig","sections":[],"depth":2},{"title":"SamMaskDecoderConfig","local":"transformers.SamMaskDecoderConfig","sections":[],"depth":2},{"title":"SamPromptEncoderConfig","local":"transformers.SamPromptEncoderConfig","sections":[],"depth":2},{"title":"SamProcessor","local":"transformers.SamProcessor","sections":[],"depth":2},{"title":"SamImageProcessor","local":"transformers.SamImageProcessor","sections":[],"depth":2},{"title":"SamImageProcessorFast","local":"transformers.SamImageProcessorFast","sections":[],"depth":2},{"title":"SamVisionModel","local":"transformers.SamVisionModel","sections":[],"depth":2},{"title":"SamModel","local":"transformers.SamModel","sections":[],"depth":2}],"depth":1}';function wa(N){return la(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ja extends pa{constructor(u){super(),ga(this,u,wa,Ta,ca,{})}}export{ja as component};
