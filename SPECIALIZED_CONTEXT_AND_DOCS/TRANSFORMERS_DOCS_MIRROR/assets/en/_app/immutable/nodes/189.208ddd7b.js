import{s as Fr,o as xr,n as H}from"../chunks/scheduler.18a86fab.js";import{S as wr,i as $r,g as l,s as n,r as g,A as Ir,h as d,f as a,c as r,j as F,x as b,u as f,k as M,y as t,a as c,v as h,d as u,t as _,w as v}from"../chunks/index.98837b22.js";import{T as $t}from"../chunks/Tip.77304350.js";import{D as w}from"../chunks/Docstring.a1ef7999.js";import{C as $e}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as we}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as z,E as zr}from"../chunks/getInferenceSnippets.06c2775f.js";function Cr(x){let i,y="Example:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhQ29uZmlnJTJDJTIwRmxhdmFNb2RlbCUyQyUyMEZsYXZhRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFDb25maWclMjB3aXRoJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEZsYXZhQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFNb2RlbCUyMGFuZCUyMEZsYXZhRm9yUHJlVHJhaW5pbmclMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBGbGF2YU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBbW9kZWxfcHJlJTIwJTNEJTIwRmxhdmFGb3JQcmVUcmFpbmluZyhjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWclMEFjb25maWd1cmF0aW9uX3ByZSUyMCUzRCUyMG1vZGVsX3ByZS5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaConfig, FlavaModel, FlavaForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FlavaForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-11lpom8"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Pr(x){let i,y="Example:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhVGV4dENvbmZpZyUyQyUyMEZsYXZhVGV4dE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEZsYXZhVGV4dE1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YVRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBGbGF2YVRleHRNb2RlbCUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEZsYXZhVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaTextConfig, FlavaTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-11lpom8"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function jr(x){let i,y="Example:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhSW1hZ2VDb25maWclMkMlMjBGbGF2YUltYWdlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFJbWFnZU1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YUltYWdlQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFJbWFnZU1vZGVsJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRmxhdmFJbWFnZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaImageConfig, FlavaImageModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-11lpom8"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Jr(x){let i,y="Example:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhTXVsdGltb2RhbENvbmZpZyUyQyUyMEZsYXZhTXVsdGltb2RhbE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEZsYXZhTXVsdGltb2RhbE1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YU11bHRpbW9kYWxDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBGbGF2YU11bHRpbW9kYWxNb2RlbCUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEZsYXZhTXVsdGltb2RhbE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaMultimodalConfig, FlavaMultimodalModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-11lpom8"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Wr(x){let i,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=y},l(p){i=d(p,"P",{"data-svelte-h":!0}),b(i)!=="svelte-fincs2"&&(i.innerHTML=y)},m(p,m){c(p,i,m)},p:H,d(p){p&&a(i)}}}function Ur(x){let i,y="Examples:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwRmxhdmFGb3JQcmVUcmFpbmluZyUyQyUyMEF1dG9Qcm9jZXNzb3IlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBbW9kZWwlMjAlM0QlMjBGbGF2YUZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmZsYXZhLWZ1bGwlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZmbGF2YS1mdWxsJTIyKSUwQSUwQXRleHQlMjAlM0QlMjAlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTBBJTIwJTIwJTIwJTIwaW1hZ2VzJTNEJTVCaW1hZ2UlNUQlMkMlMEElMjAlMjAlMjAlMjB0ZXh0JTNEdGV4dCUyQyUwQSUyMCUyMCUyMCUyMHJldHVybl9tYXNrcyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjByZXR1cm5fY29kZWJvb2tfcGl4ZWxzJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwbWF4X2xlbmd0aCUzRDc3JTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSklMEElMEElMEFvdXRwdXQlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaForPreTraining, AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaForPreTraining.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>text = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    images=[image],
<span class="hljs-meta">... </span>    text=text,
<span class="hljs-meta">... </span>    return_masks=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    return_codebook_pixels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">77</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(**inputs)`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-kvfsh7"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Zr(x){let i,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=y},l(p){i=d(p,"P",{"data-svelte-h":!0}),b(i)!=="svelte-fincs2"&&(i.innerHTML=y)},m(p,m){c(p,i,m)},p:H,d(p){p&&a(i)}}}function Nr(x){let i,y="Examples:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXZhTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXZhTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZmxhdmEtZnVsbCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmZsYXZhLWZ1bGwlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWltYWdlX2VtYmVkZGluZ3MlMjAlM0QlMjBvdXRwdXRzLmltYWdlX2VtYmVkZGluZ3MlMEF0ZXh0X2VtYmVkZGluZ3MlMjAlM0QlMjBvdXRwdXRzLnRleHRfZW1iZWRkaW5ncyUwQW11bHRpbW9kYWxfZW1iZWRkaW5ncyUyMCUzRCUyMG91dHB1dHMubXVsdGltb2RhbF9lbWJlZGRpbmdzJTBBJTBBb3V0cHV0cy5pbWFnZV9lbWJlZGRpbmdzLnNoYXBlJTBBJTBBdGV4dF9lbWJlZGRpbmdzLnNoYXBlJTBBJTBBbXVsdGltb2RhbF9lbWJlZGRpbmdzLnNoYXBl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_embeddings = outputs.image_embeddings
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeddings = outputs.text_embeddings
<span class="hljs-meta">&gt;&gt;&gt; </span>multimodal_embeddings = outputs.multimodal_embeddings

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs.image_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">768</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>multimodal_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">205</span>, <span class="hljs-number">768</span>])`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-kvfsh7"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Br(x){let i,y="Examples:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBGbGF2YU1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBGbGF2YU1vZGVsLmZyb21fcHJldHJhaW5lZCglMjIlN0IwJTdEJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMiU3QjAlN0QlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUwQSUyMCUyMCUyMCUyMHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBtYXhfbGVuZ3RoJTNENzclMkMlMjBwYWRkaW5nJTNEJTIybWF4X2xlbmd0aCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;{0}&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;{0}&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], max_length=<span class="hljs-number">77</span>, padding=<span class="hljs-string">&quot;max_length&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-kvfsh7"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function Lr(x){let i,y="Examples:",p,m,T;return m=new $e({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXZhTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXZhTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMiU3QjAlN0QlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyJTdCMCU3RCUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFpbWFnZV9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF9pbWFnZV9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;{0}&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;{0}&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){i=l("p"),i.textContent=y,p=n(),g(m.$$.fragment)},l(o){i=d(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-kvfsh7"&&(i.textContent=y),p=r(o),f(m.$$.fragment,o)},m(o,k){c(o,i,k),c(o,p,k),h(m,o,k),T=!0},p:H,i(o){T||(u(m.$$.fragment,o),T=!0)},o(o){_(m.$$.fragment,o),T=!1},d(o){o&&(a(i),a(p)),v(m,o)}}}function qr(x){let i,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=y},l(p){i=d(p,"P",{"data-svelte-h":!0}),b(i)!=="svelte-fincs2"&&(i.innerHTML=y)},m(p,m){c(p,i,m)},p:H,d(p){p&&a(i)}}}function Vr(x){let i,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=y},l(p){i=d(p,"P",{"data-svelte-h":!0}),b(i)!=="svelte-fincs2"&&(i.innerHTML=y)},m(p,m){c(p,i,m)},p:H,d(p){p&&a(i)}}}function Rr(x){let i,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=y},l(p){i=d(p,"P",{"data-svelte-h":!0}),b(i)!=="svelte-fincs2"&&(i.innerHTML=y)},m(p,m){c(p,i,m)},p:H,d(p){p&&a(i)}}}function Er(x){let i,y,p,m,T,o="<em>This model was released on 2021-12-08 and added to Hugging Face Transformers on 2022-05-11.</em>",k,Ie,It,se,zn='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',zt,ze,Ct,Ce,Cn='The FLAVA model was proposed in <a href="https://huggingface.co/papers/2112.04482" rel="nofollow">FLAVA: A Foundational Language And Vision Alignment Model</a> by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.',Pt,Pe,Pn=`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`,jt,je,jn="The abstract from the paper is the following:",Jt,Je,Jn=`<em>State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a “foundation”, that targets all modalities
at once — a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.</em>`,Wt,We,Wn='This model was contributed by <a href="https://huggingface.co/aps" rel="nofollow">aps</a>. The original code can be found <a href="https://github.com/facebookresearch/multimodal/tree/main/examples/flava" rel="nofollow">here</a>.',Ut,Ue,Zt,C,Ze,ua,Co,Un=`<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a>. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA <a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,_a,Po,Zn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,va,ie,ba,le,Ne,Ta,jo,Nn=`Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`,Nt,Be,Bt,P,Le,ya,Jo,Bn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`,ka,Wo,Ln=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,Ma,Uo,qn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Fa,de,Lt,qe,qt,j,Ve,xa,Zo,Vn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a>. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`,wa,No,Rn=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,$a,Bo,En=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ia,ce,Vt,Re,Rt,J,Ee,za,Lo,Gn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalModel">FlavaMultimodalModel</a>. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`,Ca,qo,Hn=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,Pa,Vo,An=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ja,me,Et,Ge,Gt,He,Ae,Ht,Oe,At,D,De,Ja,Ro,On="Constructs a FLAVA processor which wraps a FLAVA image processor and a FLAVA tokenizer into a single processor.",Wa,Eo,Dn=`<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Ot,Se,Dt,Xe,Ye,St,Qe,Xt,S,Ke,Ua,Go,Sn="Constructs a Flava image processor.",Za,pe,eo,Na,Ho,Xn="Preprocess an image or batch of images.",Yt,oo,Qt,X,to,Ba,Ao,Yn="Constructs a fast Flava image processor.",La,Oo,ao,Kt,no,ea,W,ro,qa,Do,Qn="The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs.",Va,So,Kn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ra,Xo,er=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ea,A,so,Ga,Yo,or='The <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> forward method, overrides the <code>__call__</code> special method.',Ha,ge,Aa,fe,oa,io,ta,$,lo,Oa,Qo,tr="The bare Flava Model outputting raw hidden-states without any specific head on top.",Da,Ko,ar=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Sa,et,nr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Xa,O,co,Ya,ot,rr='The <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> forward method, overrides the <code>__call__</code> special method.',Qa,he,Ka,ue,en,_e,mo,on,ve,tn,be,po,an,Te,aa,go,na,I,fo,nn,tt,sr=`The FLAVA’s image codebook model inspired from DALL-E’s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E’s vocab. Used to generate labels for MIM. Use
<code>get_codebook_indices</code> to get image tokens for an image.`,rn,at,ir=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,sn,nt,lr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ln,rt,ho,dn,st,uo,cn,it,_o,ra,vo,sa,U,bo,mn,lt,dr="The bare Flava Text Model outputting raw hidden-states without any specific head on to.",pn,dt,cr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,gn,ct,mr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,fn,Y,To,hn,mt,pr='The <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a> forward method, overrides the <code>__call__</code> special method.',un,ye,ia,yo,la,Z,ko,_n,pt,gr="The bare Flava Model outputting raw hidden-states without any specific head on top.",vn,gt,fr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,bn,ft,hr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Tn,Q,Mo,yn,ht,ur='The <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> forward method, overrides the <code>__call__</code> special method.',kn,ke,da,Fo,ca,N,xo,Mn,ut,_r="The bare Flava Model outputting raw hidden-states without any specific head on top.",Fn,_t,vr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xn,vt,br=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,wn,K,wo,$n,bt,Tr='The <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalModel">FlavaMultimodalModel</a> forward method, overrides the <code>__call__</code> special method.',In,Me,ma,$o,pa,Mt,ga;return Ie=new z({props:{title:"FLAVA",local:"flava",headingTag:"h1"}}),ze=new z({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ue=new z({props:{title:"FlavaConfig",local:"transformers.FlavaConfig",headingTag:"h2"}}),Ze=new w({props:{name:"class transformers.FlavaConfig",anchor:"transformers.FlavaConfig",parameters:[{name:"image_config",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"text_config",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"multimodal_config",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"image_codebook_config",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"hidden_size",val:": int = 768"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"projection_dim",val:": int = 768"},{name:"init_codebook",val:": bool = True"},{name:"logit_scale_init_value",val:": float = 2.6592"},{name:"initializer_range",val:": float = 0.02"},{name:"ce_ignore_index",val:": int = -100"},{name:"mim_weight",val:": float = 1.0"},{name:"mlm_weight",val:": float = 1.0"},{name:"global_contrastive_weight",val:": float = 1.0"},{name:"itm_weight",val:": float = 1.0"},{name:"mmm_image_weight",val:": float = 1.0"},{name:"mmm_text_weight",val:": float = 1.0"},{name:"global_backprop_contrastive",val:": bool = True"},{name:"skip_unmasked_multimodal_encoder",val:": bool = True"},{name:"return_loss",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>.`,name:"text_config"},{anchor:"transformers.FlavaConfig.image_config",description:`<strong>image_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>.`,name:"image_config"},{anchor:"transformers.FlavaConfig.multimodal_config",description:`<strong>multimodal_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>.`,name:"multimodal_config"},{anchor:"transformers.FlavaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FlavaConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The initial value of the <em>logit_scale</em> parameter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FlavaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FlavaConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM (Masked Image Modeling) unimodal loss`,name:"mim_weight"},{anchor:"transformers.FlavaConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM (Masked Language Modeling) unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FlavaConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FlavaConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FlavaConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FlavaConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FlavaConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FlavaConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FlavaConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L395"}}),ie=new we({props:{anchor:"transformers.FlavaConfig.example",$$slots:{default:[Cr]},$$scope:{ctx:x}}}),Ne=new w({props:{name:"from_configs",anchor:"transformers.FlavaConfig.from_configs",parameters:[{name:"image_config",val:": FlavaImageConfig"},{name:"text_config",val:": FlavaTextConfig"},{name:"multimodal_config",val:": FlavaMultimodalConfig"},{name:"image_codebook_config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L675",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a></p>
`}}),Be=new z({props:{title:"FlavaTextConfig",local:"transformers.FlavaTextConfig",headingTag:"h2"}}),Le=new w({props:{name:"class transformers.FlavaTextConfig",anchor:"transformers.FlavaTextConfig",parameters:[{name:"vocab_size",val:": int = 30522"},{name:"type_vocab_size",val:": int = 2"},{name:"max_position_embeddings",val:": int = 512"},{name:"position_embedding_type",val:": str = 'absolute'"},{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": str = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"pad_token_id",val:": int = 0"},{name:"qkv_bias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FlavaTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FlavaTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FlavaTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FlavaTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaTextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaTextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaTextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaTextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L128"}}),de=new we({props:{anchor:"transformers.FlavaTextConfig.example",$$slots:{default:[Pr]},$$scope:{ctx:x}}}),qe=new z({props:{title:"FlavaImageConfig",local:"transformers.FlavaImageConfig",headingTag:"h2"}}),Ve=new w({props:{name:"class transformers.FlavaImageConfig",anchor:"transformers.FlavaImageConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"image_size",val:": int = 224"},{name:"patch_size",val:": int = 16"},{name:"num_channels",val:": int = 3"},{name:"qkv_bias",val:": bool = True"},{name:"mask_token",val:": bool = True"},{name:"vocab_size",val:": int = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a mask token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.`,name:"mask_token"},{anchor:"transformers.FlavaImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> used in conjunction with <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> for MIM (Masked
Image Modeling) loss for FLAVA.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L26"}}),ce=new we({props:{anchor:"transformers.FlavaImageConfig.example",$$slots:{default:[jr]},$$scope:{ctx:x}}}),Re=new z({props:{title:"FlavaMultimodalConfig",local:"transformers.FlavaMultimodalConfig",headingTag:"h2"}}),Ee=new w({props:{name:"class transformers.FlavaMultimodalConfig",anchor:"transformers.FlavaMultimodalConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 6"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": int = 0.0"},{name:"attention_probs_dropout_prob",val:": int = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"qkv_bias",val:": bool = True"},{name:"use_cls_token",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L241"}}),me=new we({props:{anchor:"transformers.FlavaMultimodalConfig.example",$$slots:{default:[Jr]},$$scope:{ctx:x}}}),Ge=new z({props:{title:"FlavaImageCodebookConfig",local:"transformers.FlavaImageCodebookConfig",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.FlavaImageCodebookConfig",anchor:"transformers.FlavaImageCodebookConfig",parameters:[{name:"num_groups",val:": int = 4"},{name:"input_channels",val:": int = 3"},{name:"num_blocks_per_group",val:": int = 2"},{name:"hidden_size",val:": int = 256"},{name:"vocab_size",val:": int = 8192"},{name:"freeze",val:": int = True"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/configuration_flava.py#L327"}}),Oe=new z({props:{title:"FlavaProcessor",local:"transformers.FlavaProcessor",headingTag:"h2"}}),De=new w({props:{name:"class transformers.FlavaProcessor",anchor:"transformers.FlavaProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaProcessor.image_processor",description:'<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>, <em>optional</em>) &#x2014; The image processor is a required input.',name:"image_processor"},{anchor:"transformers.FlavaProcessor.tokenizer",description:'<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>, <em>optional</em>) &#x2014; The tokenizer is a required input.',name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/processing_flava.py#L28"}}),Se=new z({props:{title:"FlavaFeatureExtractor",local:"transformers.FlavaFeatureExtractor",headingTag:"h2"}}),Ye=new w({props:{name:"class transformers.FlavaFeatureExtractor",anchor:"transformers.FlavaFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/feature_extraction_flava.py#L28"}}),Qe=new z({props:{title:"FlavaImageProcessor",local:"transformers.FlavaImageProcessor",headingTag:"h2"}}),Ke=new w({props:{name:"class transformers.FlavaImageProcessor",anchor:"transformers.FlavaImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, collections.abc.Iterable[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, collections.abc.Iterable[float], NoneType] = None"},{name:"return_image_mask",val:": bool = False"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": bool = False"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": typing.Optional[bool] = None"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": typing.Optional[int] = None"},{name:"codebook_do_rescale",val:": bool = True"},{name:"codebook_rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": typing.Union[float, collections.abc.Iterable[float], NoneType] = None"},{name:"codebook_image_std",val:": typing.Union[float, collections.abc.Iterable[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of the image after resizing. Can be overridden by the <code>size</code> parameter in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in
<code>preprocess</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the images. Can be overridden by the <code>do_center_crop</code> parameter in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of image after the center crop <code>(crop_size[&quot;height&quot;], crop_size[&quot;width&quot;])</code>. Can be overridden by the
<code>crop_size</code> parameter in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in <code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in
<code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in <code>preprocess</code>.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to return the image mask. Can be overridden by the <code>return_image_mask</code> parameter in <code>preprocess</code>.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden
by the <code>input_size_patches</code> parameter in <code>preprocess</code>.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked. Can be overridden by the <code>total_mask_patches</code> parameter in
<code>preprocess</code>.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked. Can be overridden by the <code>mask_group_min_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of patches that should be masked. Can be overridden by the <code>mask_group_max_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window. Can be overridden by the <code>mask_group_min_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum aspect ratio of the mask window. Can be overridden by the <code>mask_group_max_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain. Can be overridden by the <code>codebook_do_resize</code>
parameter in <code>preprocess</code>. <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.codebook_size",description:`<strong>codebook_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Resize the input for codebook to the given size. Can be overridden by the <code>codebook_size</code> parameter in
<code>preprocess</code>.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.LANCZOS</code>) &#x2014;
Resampling filter to use if resizing the codebook image. Can be overridden by the <code>codebook_resample</code>
parameter in <code>preprocess</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped. Can be
overridden by the <code>codebook_do_center_crop</code> parameter in <code>preprocess</code>.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size for codebook input when applying center-cropping. Can be overridden by the
<code>codebook_crop_size</code> parameter in <code>preprocess</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the input for codebook by the specified scale <code>codebook_rescale_factor</code>. Can be
overridden by the <code>codebook_do_rescale</code> parameter in <code>preprocess</code>.`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the codebook image. Can be overridden by the
<code>codebook_rescale_factor</code> parameter in <code>preprocess</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the
<code>codebook_do_map_pixels</code> parameter in <code>preprocess</code>.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>. Can
be overridden by the <code>codebook_do_normalize</code> parameter in <code>preprocess</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden
by the <code>codebook_image_mean</code> parameter in <code>preprocess</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can
be overridden by the <code>codebook_image_std</code> parameter in <code>preprocess</code>.`,name:"codebook_image_std"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/image_processing_flava.py#L139"}}),eo=new w({props:{name:"preprocess",anchor:"transformers.FlavaImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_image_mask",val:": typing.Optional[bool] = None"},{name:"input_size_patches",val:": typing.Optional[int] = None"},{name:"total_mask_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_patches",val:": typing.Optional[int] = None"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": typing.Optional[float] = None"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_resize",val:": typing.Optional[bool] = None"},{name:"codebook_size",val:": typing.Optional[dict[str, int]] = None"},{name:"codebook_resample",val:": typing.Optional[int] = None"},{name:"codebook_do_center_crop",val:": typing.Optional[bool] = None"},{name:"codebook_crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"codebook_do_rescale",val:": typing.Optional[bool] = None"},{name:"codebook_rescale_factor",val:": typing.Optional[float] = None"},{name:"codebook_do_map_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_normalize",val:": typing.Optional[bool] = None"},{name:"codebook_image_mean",val:": typing.Optional[collections.abc.Iterable[float]] = None"},{name:"codebook_image_std",val:": typing.Optional[collections.abc.Iterable[float]] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_image_mask</code>) &#x2014;
Whether to return the image mask.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.input_size_patches</code>) &#x2014;
Size of the patches to extract from the image.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.total_mask_patches</code>) &#x2014;
Total number of patches to extract from the image.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_min_patches</code>) &#x2014;
Minimum number of patches to extract from the image.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_max_patches</code>) &#x2014;
Maximum number of patches to extract from the image.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_min_aspect_ratio</code>) &#x2014;
Minimum aspect ratio of the patches to extract from the image.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_max_aspect_ratio</code>) &#x2014;
Maximum aspect ratio of the patches to extract from the image.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_codebook_pixels",description:`<strong>return_codebook_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_codebook_pixels</code>) &#x2014;
Whether to return the codebook pixels.`,name:"return_codebook_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_resize</code>) &#x2014;
Whether to resize the codebook pixels.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_size",description:`<strong>codebook_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_size</code>) &#x2014;
Size of the codebook pixels.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.codebook_resample</code>) &#x2014;
Resampling filter to use if resizing the codebook pixels. This can be one of the enum
<code>PILImageResampling</code>, Only has an effect if <code>codebook_do_resize</code> is set to <code>True</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_center_crop</code>) &#x2014;
Whether to center crop the codebook pixels.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_crop_size</code>) &#x2014;
Size of the center crop of the codebook pixels. Only has an effect if <code>codebook_do_center_crop</code> is set
to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_rescale</code>) &#x2014;
Whether to rescale the codebook pixels values between [0 - 1].`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.codebook_rescale_factor</code>) &#x2014;
Rescale factor to rescale the codebook pixels by if <code>codebook_do_rescale</code> is set to <code>True</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_map_pixels</code>) &#x2014;
Whether to map the codebook pixels values.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_normalize</code>) &#x2014;
Whether to normalize the codebook pixels.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_mean</code>) &#x2014;
Codebook pixels mean to normalize the codebook pixels by if <code>codebook_do_normalize</code> is set to <code>True</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_std</code>) &#x2014;
Codebook pixels standard deviation to normalize the codebook pixels by if <code>codebook_do_normalize</code> is
set to <code>True</code>.`,name:"codebook_image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.FlavaImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/image_processing_flava.py#L456"}}),oo=new z({props:{title:"FlavaImageProcessorFast",local:"transformers.FlavaImageProcessorFast",headingTag:"h2"}}),to=new w({props:{name:"class transformers.FlavaImageProcessorFast",anchor:"transformers.FlavaImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.flava.image_processing_flava_fast.FlavaFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/image_processing_flava_fast.py#L221"}}),ao=new w({props:{name:"preprocess",anchor:"transformers.FlavaImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.image_processing_utils_fast.DefaultFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.FlavaImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.FlavaImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/image_processing_flava_fast.py#L259",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),no=new z({props:{title:"FlavaForPreTraining",local:"transformers.FlavaForPreTraining",headingTag:"h2"}}),ro=new w({props:{name:"class transformers.FlavaForPreTraining",anchor:"transformers.FlavaForPreTraining",parameters:[{name:"config",val:": FlavaConfig"},{name:"image_codebook",val:": typing.Optional[torch.nn.modules.module.Module] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaForPreTraining.image_codebook",description:`<strong>image_codebook</strong> (<code>torch.nn.modules.module.Module</code>, <em>optional</em>) &#x2014;
If passed, the image codebook will be set to this. Otherwise, it will be initialized using the
image_codebook_config defined in the config first as the first parameter.`,name:"image_codebook"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1616"}}),so=new w({props:{name:"forward",anchor:"transformers.FlavaForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"codebook_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"},{anchor:"transformers.FlavaForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.codebook_pixel_values",description:`<strong>codebook_pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, patch_size, patch_size, 3)</code>, <em>optional</em>) &#x2014;
Pixel values for image patches that are used to compute the image codebook labels for masked image modeling.`,name:"codebook_pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaForPreTraining.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaForPreTraining.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlavaForPreTraining.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>*bool*</code>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn&#x2019;t need unmasked
multimodal embeddings or outputs as of now.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaForPreTraining.forward.mlm_labels",description:`<strong>mlm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
Indices should be in <code>[-100, 0, ..., text_config.vocab_size - 1]</code> (see <code>input_ids</code> docstring). Tokens with
indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., text_config.vocab_size - 1]</code>.`,name:"mlm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.mim_labels",description:`<strong>mim_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Labels for computing the image and multimodal masked modeling loss. Indices should be in <code>[-100, 0, ..., image_config.vocab_size - 1]</code>. Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only
computed for the tokens with labels in <code>[0, ..., image_config.vocab_size - 1]</code>. If not passed, they are
generated automatically using the image codebook assigned to the model. By default, it uses
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> to understand how to generate mim_labels.`,name:"mim_labels"},{anchor:"transformers.FlavaForPreTraining.forward.itm_labels",description:`<strong>itm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.`,name:"itm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FlavaForPreTraining.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, default to None) &#x2014;
Whether to return calculated loss or not.`,name:"return_loss"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1665",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) — Total loss calculated for this model.</li>
<li><strong>loss_info</strong> (<code>&lt;class '~models.flava.modeling_flava.FlavaLosses'&gt;.loss_info</code>, defaults to <code>None</code>) — Detailed info for FLAVA Pretraining losses. Check <code>FlavaLosses</code> class description for the information on
the keys.</li>
<li><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
<li><strong>image_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</li>
<li><strong>image_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</li>
<li><strong>text_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) — The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) — The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
<li><strong>mim_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape <code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not) — The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened output is
returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mlm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and <code>pixel_values</code> are not) — The logits for MLM unimodal loss. The flattened output is returned when <code>input_ids_masked</code> has some of
the tokens masked.</li>
<li><strong>itm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) — The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.</li>
<li><strong>contrastive_logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA’s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</li>
<li><strong>contrastive_logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA’s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</li>
<li><strong>mmm_image_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code> are present) — The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened
output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mmm_text_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present) -- The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has
some of the tokens masked.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new $t({props:{$$slots:{default:[Wr]},$$scope:{ctx:x}}}),fe=new we({props:{anchor:"transformers.FlavaForPreTraining.forward.example",$$slots:{default:[Ur]},$$scope:{ctx:x}}}),io=new z({props:{title:"FlavaModel",local:"transformers.FlavaModel",headingTag:"h2"}}),lo=new w({props:{name:"class transformers.FlavaModel",anchor:"transformers.FlavaModel",parameters:[{name:"config",val:": FlavaConfig"}],parametersDescription:[{anchor:"transformers.FlavaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1038"}}),co=new w({props:{name:"forward",anchor:"transformers.FlavaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.FlavaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlavaModel.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values for image inputs. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixel values that are real (i.e., <strong>not masked</strong>),</li>
<li>0 for pixel values that are padding (i.e., <strong>masked</strong>).</li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<code>*bool*</code>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"},{anchor:"transformers.FlavaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1194",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Union[tuple, transformers.models.flava.modeling_flava.FlavaOutput]</code></p>
`}}),he=new $t({props:{$$slots:{default:[Zr]},$$scope:{ctx:x}}}),ue=new we({props:{anchor:"transformers.FlavaModel.forward.example",$$slots:{default:[Nr]},$$scope:{ctx:x}}}),mo=new w({props:{name:"get_text_features",anchor:"transformers.FlavaModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlavaModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1084",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ve=new we({props:{anchor:"transformers.FlavaModel.get_text_features.example",$$slots:{default:[Br]},$$scope:{ctx:x}}}),po=new w({props:{name:"get_image_features",anchor:"transformers.FlavaModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.FlavaModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1140",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Te=new we({props:{anchor:"transformers.FlavaModel.get_image_features.example",$$slots:{default:[Lr]},$$scope:{ctx:x}}}),go=new z({props:{title:"FlavaImageCodebook",local:"transformers.FlavaImageCodebook",headingTag:"h2"}}),fo=new w({props:{name:"class transformers.FlavaImageCodebook",anchor:"transformers.FlavaImageCodebook",parameters:[{name:"config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FlavaImageCodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageCodebookConfig">FlavaImageCodebookConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1402"}}),ho=new w({props:{name:"forward",anchor:"transformers.FlavaImageCodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1484"}}),uo=new w({props:{name:"get_codebook_indices",anchor:"transformers.FlavaImageCodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1452"}}),_o=new w({props:{name:"get_codebook_probs",anchor:"transformers.FlavaImageCodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L1480"}}),vo=new z({props:{title:"FlavaTextModel",local:"transformers.FlavaTextModel",headingTag:"h2"}}),bo=new w({props:{name:"class transformers.FlavaTextModel",anchor:"transformers.FlavaTextModel",parameters:[{name:"config",val:": FlavaTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaTextModel.add_pooling_layer",description:`<strong>add_pooling_layer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a pooling layer`,name:"add_pooling_layer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L829"}}),To=new w({props:{name:"forward",anchor:"transformers.FlavaTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlavaTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L864",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ye=new $t({props:{$$slots:{default:[qr]},$$scope:{ctx:x}}}),yo=new z({props:{title:"FlavaImageModel",local:"transformers.FlavaImageModel",headingTag:"h2"}}),ko=new w({props:{name:"class transformers.FlavaImageModel",anchor:"transformers.FlavaImageModel",parameters:[{name:"config",val:": FlavaImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaImageModel.add_pooling_layer",description:`<strong>add_pooling_layer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a pooling layer`,name:"add_pooling_layer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L732"}}),Mo=new w({props:{name:"forward",anchor:"transformers.FlavaImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.FlavaImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L769",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ke=new $t({props:{$$slots:{default:[Vr]},$$scope:{ctx:x}}}),Fo=new z({props:{title:"FlavaMultimodalModel",local:"transformers.FlavaMultimodalModel",headingTag:"h2"}}),xo=new w({props:{name:"class transformers.FlavaMultimodalModel",anchor:"transformers.FlavaMultimodalModel",parameters:[{name:"config",val:": FlavaMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaMultimodalModel.add_pooling_layer",description:`<strong>add_pooling_layer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a pooling layer`,name:"add_pooling_layer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L942"}}),wo=new w({props:{name:"forward",anchor:"transformers.FlavaMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/flava/modeling_flava.py#L974",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new $t({props:{$$slots:{default:[Rr]},$$scope:{ctx:x}}}),$o=new zr({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/flava.md"}}),{c(){i=l("meta"),y=n(),p=l("p"),m=n(),T=l("p"),T.innerHTML=o,k=n(),g(Ie.$$.fragment),It=n(),se=l("div"),se.innerHTML=zn,zt=n(),g(ze.$$.fragment),Ct=n(),Ce=l("p"),Ce.innerHTML=Cn,Pt=n(),Pe=l("p"),Pe.textContent=Pn,jt=n(),je=l("p"),je.textContent=jn,Jt=n(),Je=l("p"),Je.innerHTML=Jn,Wt=n(),We=l("p"),We.innerHTML=Wn,Ut=n(),g(Ue.$$.fragment),Zt=n(),C=l("div"),g(Ze.$$.fragment),ua=n(),Co=l("p"),Co.innerHTML=Un,_a=n(),Po=l("p"),Po.innerHTML=Zn,va=n(),g(ie.$$.fragment),ba=n(),le=l("div"),g(Ne.$$.fragment),Ta=n(),jo=l("p"),jo.innerHTML=Nn,Nt=n(),g(Be.$$.fragment),Bt=n(),P=l("div"),g(Le.$$.fragment),ya=n(),Jo=l("p"),Jo.innerHTML=Bn,ka=n(),Wo=l("p"),Wo.innerHTML=Ln,Ma=n(),Uo=l("p"),Uo.innerHTML=qn,Fa=n(),g(de.$$.fragment),Lt=n(),g(qe.$$.fragment),qt=n(),j=l("div"),g(Ve.$$.fragment),xa=n(),Zo=l("p"),Zo.innerHTML=Vn,wa=n(),No=l("p"),No.innerHTML=Rn,$a=n(),Bo=l("p"),Bo.innerHTML=En,Ia=n(),g(ce.$$.fragment),Vt=n(),g(Re.$$.fragment),Rt=n(),J=l("div"),g(Ee.$$.fragment),za=n(),Lo=l("p"),Lo.innerHTML=Gn,Ca=n(),qo=l("p"),qo.innerHTML=Hn,Pa=n(),Vo=l("p"),Vo.innerHTML=An,ja=n(),g(me.$$.fragment),Et=n(),g(Ge.$$.fragment),Gt=n(),He=l("div"),g(Ae.$$.fragment),Ht=n(),g(Oe.$$.fragment),At=n(),D=l("div"),g(De.$$.fragment),Ja=n(),Ro=l("p"),Ro.textContent=On,Wa=n(),Eo=l("p"),Eo.innerHTML=Dn,Ot=n(),g(Se.$$.fragment),Dt=n(),Xe=l("div"),g(Ye.$$.fragment),St=n(),g(Qe.$$.fragment),Xt=n(),S=l("div"),g(Ke.$$.fragment),Ua=n(),Go=l("p"),Go.textContent=Sn,Za=n(),pe=l("div"),g(eo.$$.fragment),Na=n(),Ho=l("p"),Ho.textContent=Xn,Yt=n(),g(oo.$$.fragment),Qt=n(),X=l("div"),g(to.$$.fragment),Ba=n(),Ao=l("p"),Ao.textContent=Yn,La=n(),Oo=l("div"),g(ao.$$.fragment),Kt=n(),g(no.$$.fragment),ea=n(),W=l("div"),g(ro.$$.fragment),qa=n(),Do=l("p"),Do.textContent=Qn,Va=n(),So=l("p"),So.innerHTML=Kn,Ra=n(),Xo=l("p"),Xo.innerHTML=er,Ea=n(),A=l("div"),g(so.$$.fragment),Ga=n(),Yo=l("p"),Yo.innerHTML=or,Ha=n(),g(ge.$$.fragment),Aa=n(),g(fe.$$.fragment),oa=n(),g(io.$$.fragment),ta=n(),$=l("div"),g(lo.$$.fragment),Oa=n(),Qo=l("p"),Qo.textContent=tr,Da=n(),Ko=l("p"),Ko.innerHTML=ar,Sa=n(),et=l("p"),et.innerHTML=nr,Xa=n(),O=l("div"),g(co.$$.fragment),Ya=n(),ot=l("p"),ot.innerHTML=rr,Qa=n(),g(he.$$.fragment),Ka=n(),g(ue.$$.fragment),en=n(),_e=l("div"),g(mo.$$.fragment),on=n(),g(ve.$$.fragment),tn=n(),be=l("div"),g(po.$$.fragment),an=n(),g(Te.$$.fragment),aa=n(),g(go.$$.fragment),na=n(),I=l("div"),g(fo.$$.fragment),nn=n(),tt=l("p"),tt.innerHTML=sr,rn=n(),at=l("p"),at.innerHTML=ir,sn=n(),nt=l("p"),nt.innerHTML=lr,ln=n(),rt=l("div"),g(ho.$$.fragment),dn=n(),st=l("div"),g(uo.$$.fragment),cn=n(),it=l("div"),g(_o.$$.fragment),ra=n(),g(vo.$$.fragment),sa=n(),U=l("div"),g(bo.$$.fragment),mn=n(),lt=l("p"),lt.textContent=dr,pn=n(),dt=l("p"),dt.innerHTML=cr,gn=n(),ct=l("p"),ct.innerHTML=mr,fn=n(),Y=l("div"),g(To.$$.fragment),hn=n(),mt=l("p"),mt.innerHTML=pr,un=n(),g(ye.$$.fragment),ia=n(),g(yo.$$.fragment),la=n(),Z=l("div"),g(ko.$$.fragment),_n=n(),pt=l("p"),pt.textContent=gr,vn=n(),gt=l("p"),gt.innerHTML=fr,bn=n(),ft=l("p"),ft.innerHTML=hr,Tn=n(),Q=l("div"),g(Mo.$$.fragment),yn=n(),ht=l("p"),ht.innerHTML=ur,kn=n(),g(ke.$$.fragment),da=n(),g(Fo.$$.fragment),ca=n(),N=l("div"),g(xo.$$.fragment),Mn=n(),ut=l("p"),ut.textContent=_r,Fn=n(),_t=l("p"),_t.innerHTML=vr,xn=n(),vt=l("p"),vt.innerHTML=br,wn=n(),K=l("div"),g(wo.$$.fragment),$n=n(),bt=l("p"),bt.innerHTML=Tr,In=n(),g(Me.$$.fragment),ma=n(),g($o.$$.fragment),pa=n(),Mt=l("p"),this.h()},l(e){const s=Ir("svelte-u9bgzb",document.head);i=d(s,"META",{name:!0,content:!0}),s.forEach(a),y=r(e),p=d(e,"P",{}),F(p).forEach(a),m=r(e),T=d(e,"P",{"data-svelte-h":!0}),b(T)!=="svelte-1bpb2v7"&&(T.innerHTML=o),k=r(e),f(Ie.$$.fragment,e),It=r(e),se=d(e,"DIV",{class:!0,"data-svelte-h":!0}),b(se)!=="svelte-13t8s2t"&&(se.innerHTML=zn),zt=r(e),f(ze.$$.fragment,e),Ct=r(e),Ce=d(e,"P",{"data-svelte-h":!0}),b(Ce)!=="svelte-zy6myl"&&(Ce.innerHTML=Cn),Pt=r(e),Pe=d(e,"P",{"data-svelte-h":!0}),b(Pe)!=="svelte-d5xhfu"&&(Pe.textContent=Pn),jt=r(e),je=d(e,"P",{"data-svelte-h":!0}),b(je)!=="svelte-vfdo9a"&&(je.textContent=jn),Jt=r(e),Je=d(e,"P",{"data-svelte-h":!0}),b(Je)!=="svelte-32wzl0"&&(Je.innerHTML=Jn),Wt=r(e),We=d(e,"P",{"data-svelte-h":!0}),b(We)!=="svelte-zltroy"&&(We.innerHTML=Wn),Ut=r(e),f(Ue.$$.fragment,e),Zt=r(e),C=d(e,"DIV",{class:!0});var q=F(C);f(Ze.$$.fragment,q),ua=r(q),Co=d(q,"P",{"data-svelte-h":!0}),b(Co)!=="svelte-cu434n"&&(Co.innerHTML=Un),_a=r(q),Po=d(q,"P",{"data-svelte-h":!0}),b(Po)!=="svelte-1ek1ss9"&&(Po.innerHTML=Zn),va=r(q),f(ie.$$.fragment,q),ba=r(q),le=d(q,"DIV",{class:!0});var Io=F(le);f(Ne.$$.fragment,Io),Ta=r(Io),jo=d(Io,"P",{"data-svelte-h":!0}),b(jo)!=="svelte-1ru9o5v"&&(jo.innerHTML=Nn),Io.forEach(a),q.forEach(a),Nt=r(e),f(Be.$$.fragment,e),Bt=r(e),P=d(e,"DIV",{class:!0});var V=F(P);f(Le.$$.fragment,V),ya=r(V),Jo=d(V,"P",{"data-svelte-h":!0}),b(Jo)!=="svelte-kxrk05"&&(Jo.innerHTML=Bn),ka=r(V),Wo=d(V,"P",{"data-svelte-h":!0}),b(Wo)!=="svelte-mil706"&&(Wo.innerHTML=Ln),Ma=r(V),Uo=d(V,"P",{"data-svelte-h":!0}),b(Uo)!=="svelte-1ek1ss9"&&(Uo.innerHTML=qn),Fa=r(V),f(de.$$.fragment,V),V.forEach(a),Lt=r(e),f(qe.$$.fragment,e),qt=r(e),j=d(e,"DIV",{class:!0});var R=F(j);f(Ve.$$.fragment,R),xa=r(R),Zo=d(R,"P",{"data-svelte-h":!0}),b(Zo)!=="svelte-1ctcz25"&&(Zo.innerHTML=Vn),wa=r(R),No=d(R,"P",{"data-svelte-h":!0}),b(No)!=="svelte-mil706"&&(No.innerHTML=Rn),$a=r(R),Bo=d(R,"P",{"data-svelte-h":!0}),b(Bo)!=="svelte-1ek1ss9"&&(Bo.innerHTML=En),Ia=r(R),f(ce.$$.fragment,R),R.forEach(a),Vt=r(e),f(Re.$$.fragment,e),Rt=r(e),J=d(e,"DIV",{class:!0});var E=F(J);f(Ee.$$.fragment,E),za=r(E),Lo=d(E,"P",{"data-svelte-h":!0}),b(Lo)!=="svelte-msuwrj"&&(Lo.innerHTML=Gn),Ca=r(E),qo=d(E,"P",{"data-svelte-h":!0}),b(qo)!=="svelte-mil706"&&(qo.innerHTML=Hn),Pa=r(E),Vo=d(E,"P",{"data-svelte-h":!0}),b(Vo)!=="svelte-1ek1ss9"&&(Vo.innerHTML=An),ja=r(E),f(me.$$.fragment,E),E.forEach(a),Et=r(e),f(Ge.$$.fragment,e),Gt=r(e),He=d(e,"DIV",{class:!0});var Ft=F(He);f(Ae.$$.fragment,Ft),Ft.forEach(a),Ht=r(e),f(Oe.$$.fragment,e),At=r(e),D=d(e,"DIV",{class:!0});var ae=F(D);f(De.$$.fragment,ae),Ja=r(ae),Ro=d(ae,"P",{"data-svelte-h":!0}),b(Ro)!=="svelte-6nczxn"&&(Ro.textContent=On),Wa=r(ae),Eo=d(ae,"P",{"data-svelte-h":!0}),b(Eo)!=="svelte-i388tn"&&(Eo.innerHTML=Dn),ae.forEach(a),Ot=r(e),f(Se.$$.fragment,e),Dt=r(e),Xe=d(e,"DIV",{class:!0});var xt=F(Xe);f(Ye.$$.fragment,xt),xt.forEach(a),St=r(e),f(Qe.$$.fragment,e),Xt=r(e),S=d(e,"DIV",{class:!0});var ne=F(S);f(Ke.$$.fragment,ne),Ua=r(ne),Go=d(ne,"P",{"data-svelte-h":!0}),b(Go)!=="svelte-18n2ywm"&&(Go.textContent=Sn),Za=r(ne),pe=d(ne,"DIV",{class:!0});var zo=F(pe);f(eo.$$.fragment,zo),Na=r(zo),Ho=d(zo,"P",{"data-svelte-h":!0}),b(Ho)!=="svelte-1x3yxsa"&&(Ho.textContent=Xn),zo.forEach(a),ne.forEach(a),Yt=r(e),f(oo.$$.fragment,e),Qt=r(e),X=d(e,"DIV",{class:!0});var re=F(X);f(to.$$.fragment,re),Ba=r(re),Ao=d(re,"P",{"data-svelte-h":!0}),b(Ao)!=="svelte-7624tm"&&(Ao.textContent=Yn),La=r(re),Oo=d(re,"DIV",{class:!0});var wt=F(Oo);f(ao.$$.fragment,wt),wt.forEach(a),re.forEach(a),Kt=r(e),f(no.$$.fragment,e),ea=r(e),W=d(e,"DIV",{class:!0});var G=F(W);f(ro.$$.fragment,G),qa=r(G),Do=d(G,"P",{"data-svelte-h":!0}),b(Do)!=="svelte-1ishsx8"&&(Do.textContent=Qn),Va=r(G),So=d(G,"P",{"data-svelte-h":!0}),b(So)!=="svelte-q52n56"&&(So.innerHTML=Kn),Ra=r(G),Xo=d(G,"P",{"data-svelte-h":!0}),b(Xo)!=="svelte-hswkmf"&&(Xo.innerHTML=er),Ea=r(G),A=d(G,"DIV",{class:!0});var Fe=F(A);f(so.$$.fragment,Fe),Ga=r(Fe),Yo=d(Fe,"P",{"data-svelte-h":!0}),b(Yo)!=="svelte-v2gxlj"&&(Yo.innerHTML=or),Ha=r(Fe),f(ge.$$.fragment,Fe),Aa=r(Fe),f(fe.$$.fragment,Fe),Fe.forEach(a),G.forEach(a),oa=r(e),f(io.$$.fragment,e),ta=r(e),$=d(e,"DIV",{class:!0});var B=F($);f(lo.$$.fragment,B),Oa=r(B),Qo=d(B,"P",{"data-svelte-h":!0}),b(Qo)!=="svelte-h7hici"&&(Qo.textContent=tr),Da=r(B),Ko=d(B,"P",{"data-svelte-h":!0}),b(Ko)!=="svelte-q52n56"&&(Ko.innerHTML=ar),Sa=r(B),et=d(B,"P",{"data-svelte-h":!0}),b(et)!=="svelte-hswkmf"&&(et.innerHTML=nr),Xa=r(B),O=d(B,"DIV",{class:!0});var xe=F(O);f(co.$$.fragment,xe),Ya=r(xe),ot=d(xe,"P",{"data-svelte-h":!0}),b(ot)!=="svelte-1ryp6qx"&&(ot.innerHTML=rr),Qa=r(xe),f(he.$$.fragment,xe),Ka=r(xe),f(ue.$$.fragment,xe),xe.forEach(a),en=r(B),_e=d(B,"DIV",{class:!0});var fa=F(_e);f(mo.$$.fragment,fa),on=r(fa),f(ve.$$.fragment,fa),fa.forEach(a),tn=r(B),be=d(B,"DIV",{class:!0});var ha=F(be);f(po.$$.fragment,ha),an=r(ha),f(Te.$$.fragment,ha),ha.forEach(a),B.forEach(a),aa=r(e),f(go.$$.fragment,e),na=r(e),I=d(e,"DIV",{class:!0});var L=F(I);f(fo.$$.fragment,L),nn=r(L),tt=d(L,"P",{"data-svelte-h":!0}),b(tt)!=="svelte-8jac64"&&(tt.innerHTML=sr),rn=r(L),at=d(L,"P",{"data-svelte-h":!0}),b(at)!=="svelte-q52n56"&&(at.innerHTML=ir),sn=r(L),nt=d(L,"P",{"data-svelte-h":!0}),b(nt)!=="svelte-hswkmf"&&(nt.innerHTML=lr),ln=r(L),rt=d(L,"DIV",{class:!0});var yr=F(rt);f(ho.$$.fragment,yr),yr.forEach(a),dn=r(L),st=d(L,"DIV",{class:!0});var kr=F(st);f(uo.$$.fragment,kr),kr.forEach(a),cn=r(L),it=d(L,"DIV",{class:!0});var Mr=F(it);f(_o.$$.fragment,Mr),Mr.forEach(a),L.forEach(a),ra=r(e),f(vo.$$.fragment,e),sa=r(e),U=d(e,"DIV",{class:!0});var ee=F(U);f(bo.$$.fragment,ee),mn=r(ee),lt=d(ee,"P",{"data-svelte-h":!0}),b(lt)!=="svelte-1m77oib"&&(lt.textContent=dr),pn=r(ee),dt=d(ee,"P",{"data-svelte-h":!0}),b(dt)!=="svelte-q52n56"&&(dt.innerHTML=cr),gn=r(ee),ct=d(ee,"P",{"data-svelte-h":!0}),b(ct)!=="svelte-hswkmf"&&(ct.innerHTML=mr),fn=r(ee),Y=d(ee,"DIV",{class:!0});var Tt=F(Y);f(To.$$.fragment,Tt),hn=r(Tt),mt=d(Tt,"P",{"data-svelte-h":!0}),b(mt)!=="svelte-c7bvs7"&&(mt.innerHTML=pr),un=r(Tt),f(ye.$$.fragment,Tt),Tt.forEach(a),ee.forEach(a),ia=r(e),f(yo.$$.fragment,e),la=r(e),Z=d(e,"DIV",{class:!0});var oe=F(Z);f(ko.$$.fragment,oe),_n=r(oe),pt=d(oe,"P",{"data-svelte-h":!0}),b(pt)!=="svelte-h7hici"&&(pt.textContent=gr),vn=r(oe),gt=d(oe,"P",{"data-svelte-h":!0}),b(gt)!=="svelte-q52n56"&&(gt.innerHTML=fr),bn=r(oe),ft=d(oe,"P",{"data-svelte-h":!0}),b(ft)!=="svelte-hswkmf"&&(ft.innerHTML=hr),Tn=r(oe),Q=d(oe,"DIV",{class:!0});var yt=F(Q);f(Mo.$$.fragment,yt),yn=r(yt),ht=d(yt,"P",{"data-svelte-h":!0}),b(ht)!=="svelte-1dccibb"&&(ht.innerHTML=ur),kn=r(yt),f(ke.$$.fragment,yt),yt.forEach(a),oe.forEach(a),da=r(e),f(Fo.$$.fragment,e),ca=r(e),N=d(e,"DIV",{class:!0});var te=F(N);f(xo.$$.fragment,te),Mn=r(te),ut=d(te,"P",{"data-svelte-h":!0}),b(ut)!=="svelte-h7hici"&&(ut.textContent=_r),Fn=r(te),_t=d(te,"P",{"data-svelte-h":!0}),b(_t)!=="svelte-q52n56"&&(_t.innerHTML=vr),xn=r(te),vt=d(te,"P",{"data-svelte-h":!0}),b(vt)!=="svelte-hswkmf"&&(vt.innerHTML=br),wn=r(te),K=d(te,"DIV",{class:!0});var kt=F(K);f(wo.$$.fragment,kt),$n=r(kt),bt=d(kt,"P",{"data-svelte-h":!0}),b(bt)!=="svelte-d2sh99"&&(bt.innerHTML=Tr),In=r(kt),f(Me.$$.fragment,kt),kt.forEach(a),te.forEach(a),ma=r(e),f($o.$$.fragment,e),pa=r(e),Mt=d(e,"P",{}),F(Mt).forEach(a),this.h()},h(){M(i,"name","hf:doc:metadata"),M(i,"content",Gr),M(se,"class","flex flex-wrap space-x-1"),M(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(st,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){t(document.head,i),c(e,y,s),c(e,p,s),c(e,m,s),c(e,T,s),c(e,k,s),h(Ie,e,s),c(e,It,s),c(e,se,s),c(e,zt,s),h(ze,e,s),c(e,Ct,s),c(e,Ce,s),c(e,Pt,s),c(e,Pe,s),c(e,jt,s),c(e,je,s),c(e,Jt,s),c(e,Je,s),c(e,Wt,s),c(e,We,s),c(e,Ut,s),h(Ue,e,s),c(e,Zt,s),c(e,C,s),h(Ze,C,null),t(C,ua),t(C,Co),t(C,_a),t(C,Po),t(C,va),h(ie,C,null),t(C,ba),t(C,le),h(Ne,le,null),t(le,Ta),t(le,jo),c(e,Nt,s),h(Be,e,s),c(e,Bt,s),c(e,P,s),h(Le,P,null),t(P,ya),t(P,Jo),t(P,ka),t(P,Wo),t(P,Ma),t(P,Uo),t(P,Fa),h(de,P,null),c(e,Lt,s),h(qe,e,s),c(e,qt,s),c(e,j,s),h(Ve,j,null),t(j,xa),t(j,Zo),t(j,wa),t(j,No),t(j,$a),t(j,Bo),t(j,Ia),h(ce,j,null),c(e,Vt,s),h(Re,e,s),c(e,Rt,s),c(e,J,s),h(Ee,J,null),t(J,za),t(J,Lo),t(J,Ca),t(J,qo),t(J,Pa),t(J,Vo),t(J,ja),h(me,J,null),c(e,Et,s),h(Ge,e,s),c(e,Gt,s),c(e,He,s),h(Ae,He,null),c(e,Ht,s),h(Oe,e,s),c(e,At,s),c(e,D,s),h(De,D,null),t(D,Ja),t(D,Ro),t(D,Wa),t(D,Eo),c(e,Ot,s),h(Se,e,s),c(e,Dt,s),c(e,Xe,s),h(Ye,Xe,null),c(e,St,s),h(Qe,e,s),c(e,Xt,s),c(e,S,s),h(Ke,S,null),t(S,Ua),t(S,Go),t(S,Za),t(S,pe),h(eo,pe,null),t(pe,Na),t(pe,Ho),c(e,Yt,s),h(oo,e,s),c(e,Qt,s),c(e,X,s),h(to,X,null),t(X,Ba),t(X,Ao),t(X,La),t(X,Oo),h(ao,Oo,null),c(e,Kt,s),h(no,e,s),c(e,ea,s),c(e,W,s),h(ro,W,null),t(W,qa),t(W,Do),t(W,Va),t(W,So),t(W,Ra),t(W,Xo),t(W,Ea),t(W,A),h(so,A,null),t(A,Ga),t(A,Yo),t(A,Ha),h(ge,A,null),t(A,Aa),h(fe,A,null),c(e,oa,s),h(io,e,s),c(e,ta,s),c(e,$,s),h(lo,$,null),t($,Oa),t($,Qo),t($,Da),t($,Ko),t($,Sa),t($,et),t($,Xa),t($,O),h(co,O,null),t(O,Ya),t(O,ot),t(O,Qa),h(he,O,null),t(O,Ka),h(ue,O,null),t($,en),t($,_e),h(mo,_e,null),t(_e,on),h(ve,_e,null),t($,tn),t($,be),h(po,be,null),t(be,an),h(Te,be,null),c(e,aa,s),h(go,e,s),c(e,na,s),c(e,I,s),h(fo,I,null),t(I,nn),t(I,tt),t(I,rn),t(I,at),t(I,sn),t(I,nt),t(I,ln),t(I,rt),h(ho,rt,null),t(I,dn),t(I,st),h(uo,st,null),t(I,cn),t(I,it),h(_o,it,null),c(e,ra,s),h(vo,e,s),c(e,sa,s),c(e,U,s),h(bo,U,null),t(U,mn),t(U,lt),t(U,pn),t(U,dt),t(U,gn),t(U,ct),t(U,fn),t(U,Y),h(To,Y,null),t(Y,hn),t(Y,mt),t(Y,un),h(ye,Y,null),c(e,ia,s),h(yo,e,s),c(e,la,s),c(e,Z,s),h(ko,Z,null),t(Z,_n),t(Z,pt),t(Z,vn),t(Z,gt),t(Z,bn),t(Z,ft),t(Z,Tn),t(Z,Q),h(Mo,Q,null),t(Q,yn),t(Q,ht),t(Q,kn),h(ke,Q,null),c(e,da,s),h(Fo,e,s),c(e,ca,s),c(e,N,s),h(xo,N,null),t(N,Mn),t(N,ut),t(N,Fn),t(N,_t),t(N,xn),t(N,vt),t(N,wn),t(N,K),h(wo,K,null),t(K,$n),t(K,bt),t(K,In),h(Me,K,null),c(e,ma,s),h($o,e,s),c(e,pa,s),c(e,Mt,s),ga=!0},p(e,[s]){const q={};s&2&&(q.$$scope={dirty:s,ctx:e}),ie.$set(q);const Io={};s&2&&(Io.$$scope={dirty:s,ctx:e}),de.$set(Io);const V={};s&2&&(V.$$scope={dirty:s,ctx:e}),ce.$set(V);const R={};s&2&&(R.$$scope={dirty:s,ctx:e}),me.$set(R);const E={};s&2&&(E.$$scope={dirty:s,ctx:e}),ge.$set(E);const Ft={};s&2&&(Ft.$$scope={dirty:s,ctx:e}),fe.$set(Ft);const ae={};s&2&&(ae.$$scope={dirty:s,ctx:e}),he.$set(ae);const xt={};s&2&&(xt.$$scope={dirty:s,ctx:e}),ue.$set(xt);const ne={};s&2&&(ne.$$scope={dirty:s,ctx:e}),ve.$set(ne);const zo={};s&2&&(zo.$$scope={dirty:s,ctx:e}),Te.$set(zo);const re={};s&2&&(re.$$scope={dirty:s,ctx:e}),ye.$set(re);const wt={};s&2&&(wt.$$scope={dirty:s,ctx:e}),ke.$set(wt);const G={};s&2&&(G.$$scope={dirty:s,ctx:e}),Me.$set(G)},i(e){ga||(u(Ie.$$.fragment,e),u(ze.$$.fragment,e),u(Ue.$$.fragment,e),u(Ze.$$.fragment,e),u(ie.$$.fragment,e),u(Ne.$$.fragment,e),u(Be.$$.fragment,e),u(Le.$$.fragment,e),u(de.$$.fragment,e),u(qe.$$.fragment,e),u(Ve.$$.fragment,e),u(ce.$$.fragment,e),u(Re.$$.fragment,e),u(Ee.$$.fragment,e),u(me.$$.fragment,e),u(Ge.$$.fragment,e),u(Ae.$$.fragment,e),u(Oe.$$.fragment,e),u(De.$$.fragment,e),u(Se.$$.fragment,e),u(Ye.$$.fragment,e),u(Qe.$$.fragment,e),u(Ke.$$.fragment,e),u(eo.$$.fragment,e),u(oo.$$.fragment,e),u(to.$$.fragment,e),u(ao.$$.fragment,e),u(no.$$.fragment,e),u(ro.$$.fragment,e),u(so.$$.fragment,e),u(ge.$$.fragment,e),u(fe.$$.fragment,e),u(io.$$.fragment,e),u(lo.$$.fragment,e),u(co.$$.fragment,e),u(he.$$.fragment,e),u(ue.$$.fragment,e),u(mo.$$.fragment,e),u(ve.$$.fragment,e),u(po.$$.fragment,e),u(Te.$$.fragment,e),u(go.$$.fragment,e),u(fo.$$.fragment,e),u(ho.$$.fragment,e),u(uo.$$.fragment,e),u(_o.$$.fragment,e),u(vo.$$.fragment,e),u(bo.$$.fragment,e),u(To.$$.fragment,e),u(ye.$$.fragment,e),u(yo.$$.fragment,e),u(ko.$$.fragment,e),u(Mo.$$.fragment,e),u(ke.$$.fragment,e),u(Fo.$$.fragment,e),u(xo.$$.fragment,e),u(wo.$$.fragment,e),u(Me.$$.fragment,e),u($o.$$.fragment,e),ga=!0)},o(e){_(Ie.$$.fragment,e),_(ze.$$.fragment,e),_(Ue.$$.fragment,e),_(Ze.$$.fragment,e),_(ie.$$.fragment,e),_(Ne.$$.fragment,e),_(Be.$$.fragment,e),_(Le.$$.fragment,e),_(de.$$.fragment,e),_(qe.$$.fragment,e),_(Ve.$$.fragment,e),_(ce.$$.fragment,e),_(Re.$$.fragment,e),_(Ee.$$.fragment,e),_(me.$$.fragment,e),_(Ge.$$.fragment,e),_(Ae.$$.fragment,e),_(Oe.$$.fragment,e),_(De.$$.fragment,e),_(Se.$$.fragment,e),_(Ye.$$.fragment,e),_(Qe.$$.fragment,e),_(Ke.$$.fragment,e),_(eo.$$.fragment,e),_(oo.$$.fragment,e),_(to.$$.fragment,e),_(ao.$$.fragment,e),_(no.$$.fragment,e),_(ro.$$.fragment,e),_(so.$$.fragment,e),_(ge.$$.fragment,e),_(fe.$$.fragment,e),_(io.$$.fragment,e),_(lo.$$.fragment,e),_(co.$$.fragment,e),_(he.$$.fragment,e),_(ue.$$.fragment,e),_(mo.$$.fragment,e),_(ve.$$.fragment,e),_(po.$$.fragment,e),_(Te.$$.fragment,e),_(go.$$.fragment,e),_(fo.$$.fragment,e),_(ho.$$.fragment,e),_(uo.$$.fragment,e),_(_o.$$.fragment,e),_(vo.$$.fragment,e),_(bo.$$.fragment,e),_(To.$$.fragment,e),_(ye.$$.fragment,e),_(yo.$$.fragment,e),_(ko.$$.fragment,e),_(Mo.$$.fragment,e),_(ke.$$.fragment,e),_(Fo.$$.fragment,e),_(xo.$$.fragment,e),_(wo.$$.fragment,e),_(Me.$$.fragment,e),_($o.$$.fragment,e),ga=!1},d(e){e&&(a(y),a(p),a(m),a(T),a(k),a(It),a(se),a(zt),a(Ct),a(Ce),a(Pt),a(Pe),a(jt),a(je),a(Jt),a(Je),a(Wt),a(We),a(Ut),a(Zt),a(C),a(Nt),a(Bt),a(P),a(Lt),a(qt),a(j),a(Vt),a(Rt),a(J),a(Et),a(Gt),a(He),a(Ht),a(At),a(D),a(Ot),a(Dt),a(Xe),a(St),a(Xt),a(S),a(Yt),a(Qt),a(X),a(Kt),a(ea),a(W),a(oa),a(ta),a($),a(aa),a(na),a(I),a(ra),a(sa),a(U),a(ia),a(la),a(Z),a(da),a(ca),a(N),a(ma),a(pa),a(Mt)),a(i),v(Ie,e),v(ze,e),v(Ue,e),v(Ze),v(ie),v(Ne),v(Be,e),v(Le),v(de),v(qe,e),v(Ve),v(ce),v(Re,e),v(Ee),v(me),v(Ge,e),v(Ae),v(Oe,e),v(De),v(Se,e),v(Ye),v(Qe,e),v(Ke),v(eo),v(oo,e),v(to),v(ao),v(no,e),v(ro),v(so),v(ge),v(fe),v(io,e),v(lo),v(co),v(he),v(ue),v(mo),v(ve),v(po),v(Te),v(go,e),v(fo),v(ho),v(uo),v(_o),v(vo,e),v(bo),v(To),v(ye),v(yo,e),v(ko),v(Mo),v(ke),v(Fo,e),v(xo),v(wo),v(Me),v($o,e)}}}const Gr='{"title":"FLAVA","local":"flava","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"FlavaConfig","local":"transformers.FlavaConfig","sections":[],"depth":2},{"title":"FlavaTextConfig","local":"transformers.FlavaTextConfig","sections":[],"depth":2},{"title":"FlavaImageConfig","local":"transformers.FlavaImageConfig","sections":[],"depth":2},{"title":"FlavaMultimodalConfig","local":"transformers.FlavaMultimodalConfig","sections":[],"depth":2},{"title":"FlavaImageCodebookConfig","local":"transformers.FlavaImageCodebookConfig","sections":[],"depth":2},{"title":"FlavaProcessor","local":"transformers.FlavaProcessor","sections":[],"depth":2},{"title":"FlavaFeatureExtractor","local":"transformers.FlavaFeatureExtractor","sections":[],"depth":2},{"title":"FlavaImageProcessor","local":"transformers.FlavaImageProcessor","sections":[],"depth":2},{"title":"FlavaImageProcessorFast","local":"transformers.FlavaImageProcessorFast","sections":[],"depth":2},{"title":"FlavaForPreTraining","local":"transformers.FlavaForPreTraining","sections":[],"depth":2},{"title":"FlavaModel","local":"transformers.FlavaModel","sections":[],"depth":2},{"title":"FlavaImageCodebook","local":"transformers.FlavaImageCodebook","sections":[],"depth":2},{"title":"FlavaTextModel","local":"transformers.FlavaTextModel","sections":[],"depth":2},{"title":"FlavaImageModel","local":"transformers.FlavaImageModel","sections":[],"depth":2},{"title":"FlavaMultimodalModel","local":"transformers.FlavaMultimodalModel","sections":[],"depth":2}],"depth":1}';function Hr(x){return xr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kr extends wr{constructor(i){super(),$r(this,i,Hr,Er,Fr,{})}}export{Kr as component};
