import{s as en,z as nn,o as an,n as Be}from"../chunks/scheduler.18a86fab.js";import{S as ln,i as tn,g as o,s as l,r as d,A as on,h as r,f as n,c as t,j as v,x as M,u as y,k,l as rn,y as i,a,v as J,d as T,t as h,w}from"../chunks/index.98837b22.js";import{T as cn}from"../chunks/Tip.77304350.js";import{D as Zs}from"../chunks/Docstring.a1ef7999.js";import{C as zs}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as sn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ns,E as pn}from"../chunks/getInferenceSnippets.06c2775f.js";function Mn(Z){let c,j="Example:",U,m,u;return m=new zs({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpdFBvc2VDb25maWclMkMlMjBWaXRQb3NlRm9yUG9zZUVzdGltYXRpb24lMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwVml0UG9zZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwVml0UG9zZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBWaXRQb3NlRm9yUG9zZUVzdGltYXRpb24oY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VitPoseConfig, VitPoseForPoseEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VitPose configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VitPoseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitPoseForPoseEstimation(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){c=o("p"),c.textContent=j,U=l(),d(m.$$.fragment)},l(p){c=r(p,"P",{"data-svelte-h":!0}),M(c)!=="svelte-11lpom8"&&(c.textContent=j),U=t(p),y(m.$$.fragment,p)},m(p,I){a(p,c,I),a(p,U,I),J(m,p,I),u=!0},p:Be,i(p){u||(T(m.$$.fragment,p),u=!0)},o(p){h(m.$$.fragment,p),u=!1},d(p){p&&(n(c),n(U)),w(m,p)}}}function mn(Z){let c,j=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){c=o("p"),c.innerHTML=j},l(U){c=r(U,"P",{"data-svelte-h":!0}),M(c)!=="svelte-fincs2"&&(c.innerHTML=j)},m(U,m){a(U,c,m)},p:Be,d(U){U&&n(c)}}}function dn(Z){let c,j="Examples:",U,m,u;return m=new zs({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpdFBvc2VGb3JQb3NlRXN0aW1hdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMnVzeWQtY29tbXVuaXR5JTJGdml0cG9zZS1iYXNlLXNpbXBsZSUyMiklMEFtb2RlbCUyMCUzRCUyMFZpdFBvc2VGb3JQb3NlRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIydXN5ZC1jb21tdW5pdHklMkZ2aXRwb3NlLWJhc2Utc2ltcGxlJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFib3hlcyUyMCUzRCUyMCU1QiU1QiU1QjQxMi44JTJDJTIwMTU3LjYxJTJDJTIwNTMuMDUlMkMlMjAxMzguMDElNUQlMkMlMjAlNUIzODQuNDMlMkMlMjAxNzIuMjElMkMlMjAxNS4xMiUyQyUyMDM1Ljc0JTVEJTVEJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlJTJDJTIwYm94ZXMlM0Rib3hlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFoZWF0bWFwcyUyMCUzRCUyMG91dHB1dHMuaGVhdG1hcHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, VitPoseForPoseEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-base-simple&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitPoseForPoseEstimation.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-base-simple&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = [[[<span class="hljs-number">412.8</span>, <span class="hljs-number">157.61</span>, <span class="hljs-number">53.05</span>, <span class="hljs-number">138.01</span>], [<span class="hljs-number">384.43</span>, <span class="hljs-number">172.21</span>, <span class="hljs-number">15.12</span>, <span class="hljs-number">35.74</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(image, boxes=boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>heatmaps = outputs.heatmaps`,wrap:!1}}),{c(){c=o("p"),c.textContent=j,U=l(),d(m.$$.fragment)},l(p){c=r(p,"P",{"data-svelte-h":!0}),M(c)!=="svelte-kvfsh7"&&(c.textContent=j),U=t(p),y(m.$$.fragment,p)},m(p,I){a(p,c,I),a(p,U,I),J(m,p,I),u=!0},p:Be,i(p){u||(T(m.$$.fragment,p),u=!0)},o(p){h(m.$$.fragment,p),u=!1},d(p){p&&(n(c),n(U)),w(m,p)}}}function yn(Z){let c,j,U,m,u,p="<em>This model was released on 2022-04-26 and added to Hugging Face Transformers on 2025-01-08.</em>",I,N,ve='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Ws,Y,Xs,F,Ve='<a href="https://huggingface.co/papers/2204.12484" rel="nofollow">ViTPose</a> is a vision transformer-based model for keypoint (pose) estimation. It uses a simple, non-hierarchical <a href="./vit">ViT</a> backbone and a lightweight decoder head. This architecture simplifies model design, takes advantage of transformer scalability, and can be adapted to different training strategies.',Es,H,Ze='<a href="https://huggingface.co/papers/2212.04246" rel="nofollow">ViTPose++</a> improves on ViTPose by incorporating a mixture-of-experts (MoE) module in the backbone and using more diverse pretraining data.',As,z,Ne,Gs,S,ze='You can find all ViTPose and ViTPose++ checkpoints under the <a href="https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335" rel="nofollow">ViTPose collection</a>.',Rs,$,Qe='The example below demonstrates pose estimation with the <a href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseForPoseEstimation">VitPoseForPoseEstimation</a> class.',xs,D,Ps,Q,We='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose.png"/>',Ys,L,Xe='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',Fs,K,Ee='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to int4.',Hs,q,Ss,O,$s,C,Us,Ae='<p>Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> to automatically prepare bounding box and image inputs.</p>',ce,us,Ge="<p>ViTPose is a top-down pose estimator. It uses a object detector to detect individuals first before keypoint prediction.</p>",pe,ss,bs,Re="ViTPose++ has 6 different MoE expert heads (COCO validation <code>0</code>, AiC <code>1</code>, MPII <code>2</code>, AP-10K <code>3</code>, APT-36K <code>4</code>, COCO-WholeBody <code>5</code>) which supports 6 different datasets. Pass a specific value corresponding to the dataset to the <code>dataset_index</code> to indicate which expert to use.",Me,es,me,ns,js,xe='<a href="https://opencv.org/" rel="nofollow">OpenCV</a> is an alternative option for visualizing the estimated pose.',de,as,Ds,ls,Ls,ts,Pe="Refer to resources below to learn more about using ViTPose.",Ks,os,Ye='<li>This <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTPose/Inference_with_ViTPose_for_body_pose_estimation.ipynb" rel="nofollow">notebook</a> demonstrates inference and visualization.</li> <li>This <a href="https://huggingface.co/spaces/hysts/ViTPose-transformers" rel="nofollow">Space</a> demonstrates ViTPose on images and video.</li>',qs,rs,Os,f,is,ye,fs,Fe="Constructs a VitPose image processor.",Je,W,cs,Te,gs,He="Preprocess an image or batch of images.",he,X,ps,we,Is,Se="Transform the heatmaps into keypoint predictions and transform them back to the image.",se,Ms,ee,g,ms,Ue,Cs,$e=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseForPoseEstimation">VitPoseForPoseEstimation</a>. It is used to instantiate a
VitPose model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VitPose
<a href="https://huggingface.co/usyd-community/vitpose-base-simple" rel="nofollow">usyd-community/vitpose-base-simple</a> architecture.`,ue,_s,De=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,be,E,ne,ds,ae,b,ys,je,ks,Le="The VitPose model with a pose estimation head on top.",fe,Bs,Ke=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ge,vs,qe=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ie,_,Js,Ce,Vs,Oe='The <a href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseForPoseEstimation">VitPoseForPoseEstimation</a> forward method, overrides the <code>__call__</code> special method.',_e,A,ke,G,le,Ts,te,Qs,oe;return Y=new Ns({props:{title:"ViTPose",local:"vitpose",headingTag:"h1"}}),D=new zs({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> supervision <span class="hljs-keyword">as</span> sv
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, infer_device

device = infer_device()

url = <span class="hljs-string">&quot;https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-comment"># Detect humans in the image</span>
person_image_processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;PekingU/rtdetr_r50vd_coco_o365&quot;</span>)
person_model = RTDetrForObjectDetection.from_pretrained(<span class="hljs-string">&quot;PekingU/rtdetr_r50vd_coco_o365&quot;</span>, device_map=device)

inputs = person_image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(person_model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = person_model(**inputs)

results = person_image_processor.post_process_object_detection(
    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=<span class="hljs-number">0.3</span>
)
result = results[<span class="hljs-number">0</span>]

<span class="hljs-comment"># Human label refers 0 index in COCO dataset</span>
person_boxes = result[<span class="hljs-string">&quot;boxes&quot;</span>][result[<span class="hljs-string">&quot;labels&quot;</span>] == <span class="hljs-number">0</span>]
person_boxes = person_boxes.cpu().numpy()

<span class="hljs-comment"># Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format</span>
person_boxes[:, <span class="hljs-number">2</span>] = person_boxes[:, <span class="hljs-number">2</span>] - person_boxes[:, <span class="hljs-number">0</span>]
person_boxes[:, <span class="hljs-number">3</span>] = person_boxes[:, <span class="hljs-number">3</span>] - person_boxes[:, <span class="hljs-number">1</span>]

<span class="hljs-comment"># Detect keypoints for each person found</span>
image_processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-base-simple&quot;</span>)
model = VitPoseForPoseEstimation.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-base-simple&quot;</span>, device_map=device)

inputs = image_processor(image, boxes=[person_boxes], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])
image_pose_result = pose_results[<span class="hljs-number">0</span>]

xy = torch.stack([pose_result[<span class="hljs-string">&#x27;keypoints&#x27;</span>] <span class="hljs-keyword">for</span> pose_result <span class="hljs-keyword">in</span> image_pose_result]).cpu().numpy()
scores = torch.stack([pose_result[<span class="hljs-string">&#x27;scores&#x27;</span>] <span class="hljs-keyword">for</span> pose_result <span class="hljs-keyword">in</span> image_pose_result]).cpu().numpy()

key_points = sv.KeyPoints(
    xy=xy, confidence=scores
)

edge_annotator = sv.EdgeAnnotator(
    color=sv.Color.GREEN,
    thickness=<span class="hljs-number">1</span>
)
vertex_annotator = sv.VertexAnnotator(
    color=sv.Color.RED,
    radius=<span class="hljs-number">2</span>
)
annotated_frame = edge_annotator.annotate(
    scene=image.copy(),
    key_points=key_points
)
annotated_frame = vertex_annotator.annotate(
    scene=annotated_frame,
    key_points=key_points
)
annotated_frame`,wrap:!1}}),q=new zs({props:{code:"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",highlighted:`<span class="hljs-comment"># pip install torchao</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, TorchAoConfig

url = <span class="hljs-string">&quot;https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

person_image_processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;PekingU/rtdetr_r50vd_coco_o365&quot;</span>)
person_model = RTDetrForObjectDetection.from_pretrained(<span class="hljs-string">&quot;PekingU/rtdetr_r50vd_coco_o365&quot;</span>, device_map=device)

inputs = person_image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = person_model(**inputs)

results = person_image_processor.post_process_object_detection(
    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=<span class="hljs-number">0.3</span>
)
result = results[<span class="hljs-number">0</span>]

person_boxes = result[<span class="hljs-string">&quot;boxes&quot;</span>][result[<span class="hljs-string">&quot;labels&quot;</span>] == <span class="hljs-number">0</span>]
person_boxes = person_boxes.cpu().numpy()

person_boxes[:, <span class="hljs-number">2</span>] = person_boxes[:, <span class="hljs-number">2</span>] - person_boxes[:, <span class="hljs-number">0</span>]
person_boxes[:, <span class="hljs-number">3</span>] = person_boxes[:, <span class="hljs-number">3</span>] - person_boxes[:, <span class="hljs-number">1</span>]

quantization_config = TorchAoConfig(<span class="hljs-string">&quot;int4_weight_only&quot;</span>, group_size=<span class="hljs-number">128</span>)

image_processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-plus-huge&quot;</span>)
model = VitPoseForPoseEstimation.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-plus-huge&quot;</span>, device_map=device, quantization_config=quantization_config)

inputs = image_processor(image, boxes=[person_boxes], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])
image_pose_result = pose_results[<span class="hljs-number">0</span>]`,wrap:!1}}),O=new Ns({props:{title:"Notes",local:"notes",headingTag:"h2"}}),es=new zs({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBWaXRQb3NlRm9yUG9zZUVzdGltYXRpb24lMkMlMjBpbmZlcl9kZXZpY2UlMEElMEFkZXZpY2UlMjAlM0QlMjBpbmZlcl9kZXZpY2UoKSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMnVzeWQtY29tbXVuaXR5JTJGdml0cG9zZS1wbHVzLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBWaXRQb3NlRm9yUG9zZUVzdGltYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnVzeWQtY29tbXVuaXR5JTJGdml0cG9zZS1wbHVzLWJhc2UlMjIlMkMlMjBkZXZpY2UlM0RkZXZpY2UpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwYm94ZXMlM0QlNUJwZXJzb25fYm94ZXMlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBZGF0YXNldF9pbmRleCUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIwJTVEJTJDJTIwZGV2aWNlJTNEZGV2aWNlKSUyMCUyMyUyMG11c3QlMjBiZSUyMGElMjB0ZW5zb3IlMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGRhdGFzZXRfaW5kZXglM0RkYXRhc2V0X2luZGV4KQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, VitPoseForPoseEstimation, infer_device

device = infer_device()

image_processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-plus-base&quot;</span>)
model = VitPoseForPoseEstimation.from_pretrained(<span class="hljs-string">&quot;usyd-community/vitpose-plus-base&quot;</span>, device=device)

inputs = image_processor(image, boxes=[person_boxes], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
dataset_index = torch.tensor([<span class="hljs-number">0</span>], device=device) <span class="hljs-comment"># must be a tensor of shape (batch_size,)</span>

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs, dataset_index=dataset_index)`,wrap:!1}}),as=new zs({props:{code:"%23%20pip%20install%20opencv-python%0Aimport%20math%0Aimport%20cv2%0A%0Adef%20draw_points(image%2C%20keypoints%2C%20scores%2C%20pose_keypoint_color%2C%20keypoint_score_threshold%2C%20radius%2C%20show_keypoint_weight)%3A%0A%20%20%20%20if%20pose_keypoint_color%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20assert%20len(pose_keypoint_color)%20%3D%3D%20len(keypoints)%0A%20%20%20%20for%20kid%2C%20(kpt%2C%20kpt_score)%20in%20enumerate(zip(keypoints%2C%20scores))%3A%0A%20%20%20%20%20%20%20%20x_coord%2C%20y_coord%20%3D%20int(kpt%5B0%5D)%2C%20int(kpt%5B1%5D)%0A%20%20%20%20%20%20%20%20if%20kpt_score%20%3E%20keypoint_score_threshold%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20color%20%3D%20tuple(int(c)%20for%20c%20in%20pose_keypoint_color%5Bkid%5D)%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20show_keypoint_weight%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.circle(image%2C%20(int(x_coord)%2C%20int(y_coord))%2C%20radius%2C%20color%2C%20-1)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20transparency%20%3D%20max(0%2C%20min(1%2C%20kpt_score))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.addWeighted(image%2C%20transparency%2C%20image%2C%201%20-%20transparency%2C%200%2C%20dst%3Dimage)%0A%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.circle(image%2C%20(int(x_coord)%2C%20int(y_coord))%2C%20radius%2C%20color%2C%20-1)%0A%0Adef%20draw_links(image%2C%20keypoints%2C%20scores%2C%20keypoint_edges%2C%20link_colors%2C%20keypoint_score_threshold%2C%20thickness%2C%20show_keypoint_weight%2C%20stick_width%20%3D%202)%3A%0A%20%20%20%20height%2C%20width%2C%20_%20%3D%20image.shape%0A%20%20%20%20if%20keypoint_edges%20is%20not%20None%20and%20link_colors%20is%20not%20None%3A%0A%20%20%20%20%20%20%20%20assert%20len(link_colors)%20%3D%3D%20len(keypoint_edges)%0A%20%20%20%20%20%20%20%20for%20sk_id%2C%20sk%20in%20enumerate(keypoint_edges)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20x1%2C%20y1%2C%20score1%20%3D%20(int(keypoints%5Bsk%5B0%5D%2C%200%5D)%2C%20int(keypoints%5Bsk%5B0%5D%2C%201%5D)%2C%20scores%5Bsk%5B0%5D%5D)%0A%20%20%20%20%20%20%20%20%20%20%20%20x2%2C%20y2%2C%20score2%20%3D%20(int(keypoints%5Bsk%5B1%5D%2C%200%5D)%2C%20int(keypoints%5Bsk%5B1%5D%2C%201%5D)%2C%20scores%5Bsk%5B1%5D%5D)%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x1%20%3E%200%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20x1%20%3C%20width%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20y1%20%3E%200%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20y1%20%3C%20height%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20x2%20%3E%200%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20x2%20%3C%20width%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20y2%20%3E%200%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20y2%20%3C%20height%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20score1%20%3E%20keypoint_score_threshold%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20and%20score2%20%3E%20keypoint_score_threshold%0A%20%20%20%20%20%20%20%20%20%20%20%20)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20color%20%3D%20tuple(int(c)%20for%20c%20in%20link_colors%5Bsk_id%5D)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20show_keypoint_weight%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20X%20%3D%20(x1%2C%20x2)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20Y%20%3D%20(y1%2C%20y2)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20mean_x%20%3D%20np.mean(X)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20mean_y%20%3D%20np.mean(Y)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20length%20%3D%20((Y%5B0%5D%20-%20Y%5B1%5D)%20**%202%20%2B%20(X%5B0%5D%20-%20X%5B1%5D)%20**%202)%20**%200.5%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20angle%20%3D%20math.degrees(math.atan2(Y%5B0%5D%20-%20Y%5B1%5D%2C%20X%5B0%5D%20-%20X%5B1%5D))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20polygon%20%3D%20cv2.ellipse2Poly(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(int(mean_x)%2C%20int(mean_y))%2C%20(int(length%20%2F%202)%2C%20int(stick_width))%2C%20int(angle)%2C%200%2C%20360%2C%201%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.fillConvexPoly(image%2C%20polygon%2C%20color)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20transparency%20%3D%20max(0%2C%20min(1%2C%200.5%20*%20(keypoints%5Bsk%5B0%5D%2C%202%5D%20%2B%20keypoints%5Bsk%5B1%5D%2C%202%5D)))%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.addWeighted(image%2C%20transparency%2C%20image%2C%201%20-%20transparency%2C%200%2C%20dst%3Dimage)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20cv2.line(image%2C%20(x1%2C%20y1)%2C%20(x2%2C%20y2)%2C%20color%2C%20thickness%3Dthickness)%0A%0A%23%20Note%3A%20keypoint_edges%20and%20color%20palette%20are%20dataset-specific%0Akeypoint_edges%20%3D%20model.config.edges%0A%0Apalette%20%3D%20np.array(%0A%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%5B255%2C%20128%2C%200%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20153%2C%2051%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20178%2C%20102%5D%2C%0A%20%20%20%20%20%20%20%20%5B230%2C%20230%2C%200%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20153%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B153%2C%20204%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20102%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%2051%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B102%2C%20178%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B51%2C%20153%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20153%2C%20153%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20102%2C%20102%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%2051%2C%2051%5D%2C%0A%20%20%20%20%20%20%20%20%5B153%2C%20255%2C%20153%5D%2C%0A%20%20%20%20%20%20%20%20%5B102%2C%20255%2C%20102%5D%2C%0A%20%20%20%20%20%20%20%20%5B51%2C%20255%2C%2051%5D%2C%0A%20%20%20%20%20%20%20%20%5B0%2C%20255%2C%200%5D%2C%0A%20%20%20%20%20%20%20%20%5B0%2C%200%2C%20255%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%200%2C%200%5D%2C%0A%20%20%20%20%20%20%20%20%5B255%2C%20255%2C%20255%5D%2C%0A%20%20%20%20%5D%0A)%0A%0Alink_colors%20%3D%20palette%5B%5B0%2C%200%2C%200%2C%200%2C%207%2C%207%2C%207%2C%209%2C%209%2C%209%2C%209%2C%209%2C%2016%2C%2016%2C%2016%2C%2016%2C%2016%2C%2016%2C%2016%5D%5D%0Akeypoint_colors%20%3D%20palette%5B%5B16%2C%2016%2C%2016%2C%2016%2C%2016%2C%209%2C%209%2C%209%2C%209%2C%209%2C%209%2C%200%2C%200%2C%200%2C%200%2C%200%2C%200%5D%5D%0A%0Anumpy_image%20%3D%20np.array(image)%0A%0Afor%20pose_result%20in%20image_pose_result%3A%0A%20%20%20%20scores%20%3D%20np.array(pose_result%5B%22scores%22%5D)%0A%20%20%20%20keypoints%20%3D%20np.array(pose_result%5B%22keypoints%22%5D)%0A%0A%20%20%20%20%23%20draw%20each%20point%20on%20image%0A%20%20%20%20draw_points(numpy_image%2C%20keypoints%2C%20scores%2C%20keypoint_colors%2C%20keypoint_score_threshold%3D0.3%2C%20radius%3D4%2C%20show_keypoint_weight%3DFalse)%0A%0A%20%20%20%20%23%20draw%20links%0A%20%20%20%20draw_links(numpy_image%2C%20keypoints%2C%20scores%2C%20keypoint_edges%2C%20link_colors%2C%20keypoint_score_threshold%3D0.3%2C%20thickness%3D1%2C%20show_keypoint_weight%3DFalse)%0A%0Apose_image%20%3D%20Image.fromarray(numpy_image)%0Apose_image",highlighted:`<span class="hljs-comment"># pip install opencv-python</span>
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> cv2

<span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_points</span>(<span class="hljs-params">image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight</span>):
    <span class="hljs-keyword">if</span> pose_keypoint_color <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(pose_keypoint_color) == <span class="hljs-built_in">len</span>(keypoints)
    <span class="hljs-keyword">for</span> kid, (kpt, kpt_score) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(keypoints, scores)):
        x_coord, y_coord = <span class="hljs-built_in">int</span>(kpt[<span class="hljs-number">0</span>]), <span class="hljs-built_in">int</span>(kpt[<span class="hljs-number">1</span>])
        <span class="hljs-keyword">if</span> kpt_score &gt; keypoint_score_threshold:
            color = <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">int</span>(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> pose_keypoint_color[kid])
            <span class="hljs-keyword">if</span> show_keypoint_weight:
                cv2.circle(image, (<span class="hljs-built_in">int</span>(x_coord), <span class="hljs-built_in">int</span>(y_coord)), radius, color, -<span class="hljs-number">1</span>)
                transparency = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">min</span>(<span class="hljs-number">1</span>, kpt_score))
                cv2.addWeighted(image, transparency, image, <span class="hljs-number">1</span> - transparency, <span class="hljs-number">0</span>, dst=image)
            <span class="hljs-keyword">else</span>:
                cv2.circle(image, (<span class="hljs-built_in">int</span>(x_coord), <span class="hljs-built_in">int</span>(y_coord)), radius, color, -<span class="hljs-number">1</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">draw_links</span>(<span class="hljs-params">image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = <span class="hljs-number">2</span></span>):
    height, width, _ = image.shape
    <span class="hljs-keyword">if</span> keypoint_edges <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> link_colors <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(link_colors) == <span class="hljs-built_in">len</span>(keypoint_edges)
        <span class="hljs-keyword">for</span> sk_id, sk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(keypoint_edges):
            x1, y1, score1 = (<span class="hljs-built_in">int</span>(keypoints[sk[<span class="hljs-number">0</span>], <span class="hljs-number">0</span>]), <span class="hljs-built_in">int</span>(keypoints[sk[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>]), scores[sk[<span class="hljs-number">0</span>]])
            x2, y2, score2 = (<span class="hljs-built_in">int</span>(keypoints[sk[<span class="hljs-number">1</span>], <span class="hljs-number">0</span>]), <span class="hljs-built_in">int</span>(keypoints[sk[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>]), scores[sk[<span class="hljs-number">1</span>]])
            <span class="hljs-keyword">if</span> (
                x1 &gt; <span class="hljs-number">0</span>
                <span class="hljs-keyword">and</span> x1 &lt; width
                <span class="hljs-keyword">and</span> y1 &gt; <span class="hljs-number">0</span>
                <span class="hljs-keyword">and</span> y1 &lt; height
                <span class="hljs-keyword">and</span> x2 &gt; <span class="hljs-number">0</span>
                <span class="hljs-keyword">and</span> x2 &lt; width
                <span class="hljs-keyword">and</span> y2 &gt; <span class="hljs-number">0</span>
                <span class="hljs-keyword">and</span> y2 &lt; height
                <span class="hljs-keyword">and</span> score1 &gt; keypoint_score_threshold
                <span class="hljs-keyword">and</span> score2 &gt; keypoint_score_threshold
            ):
                color = <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">int</span>(c) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> link_colors[sk_id])
                <span class="hljs-keyword">if</span> show_keypoint_weight:
                    X = (x1, x2)
                    Y = (y1, y2)
                    mean_x = np.mean(X)
                    mean_y = np.mean(Y)
                    length = ((Y[<span class="hljs-number">0</span>] - Y[<span class="hljs-number">1</span>]) ** <span class="hljs-number">2</span> + (X[<span class="hljs-number">0</span>] - X[<span class="hljs-number">1</span>]) ** <span class="hljs-number">2</span>) ** <span class="hljs-number">0.5</span>
                    angle = math.degrees(math.atan2(Y[<span class="hljs-number">0</span>] - Y[<span class="hljs-number">1</span>], X[<span class="hljs-number">0</span>] - X[<span class="hljs-number">1</span>]))
                    polygon = cv2.ellipse2Poly(
                        (<span class="hljs-built_in">int</span>(mean_x), <span class="hljs-built_in">int</span>(mean_y)), (<span class="hljs-built_in">int</span>(length / <span class="hljs-number">2</span>), <span class="hljs-built_in">int</span>(stick_width)), <span class="hljs-built_in">int</span>(angle), <span class="hljs-number">0</span>, <span class="hljs-number">360</span>, <span class="hljs-number">1</span>
                    )
                    cv2.fillConvexPoly(image, polygon, color)
                    transparency = <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">min</span>(<span class="hljs-number">1</span>, <span class="hljs-number">0.5</span> * (keypoints[sk[<span class="hljs-number">0</span>], <span class="hljs-number">2</span>] + keypoints[sk[<span class="hljs-number">1</span>], <span class="hljs-number">2</span>])))
                    cv2.addWeighted(image, transparency, image, <span class="hljs-number">1</span> - transparency, <span class="hljs-number">0</span>, dst=image)
                <span class="hljs-keyword">else</span>:
                    cv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)

<span class="hljs-comment"># Note: keypoint_edges and color palette are dataset-specific</span>
keypoint_edges = model.config.edges

palette = np.array(
    [
        [<span class="hljs-number">255</span>, <span class="hljs-number">128</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">153</span>, <span class="hljs-number">51</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">178</span>, <span class="hljs-number">102</span>],
        [<span class="hljs-number">230</span>, <span class="hljs-number">230</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">153</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">153</span>, <span class="hljs-number">204</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">102</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">51</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">102</span>, <span class="hljs-number">178</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">51</span>, <span class="hljs-number">153</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">153</span>, <span class="hljs-number">153</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">102</span>, <span class="hljs-number">102</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">51</span>, <span class="hljs-number">51</span>],
        [<span class="hljs-number">153</span>, <span class="hljs-number">255</span>, <span class="hljs-number">153</span>],
        [<span class="hljs-number">102</span>, <span class="hljs-number">255</span>, <span class="hljs-number">102</span>],
        [<span class="hljs-number">51</span>, <span class="hljs-number">255</span>, <span class="hljs-number">51</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">255</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>],
    ]
)

link_colors = palette[[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]]
keypoint_colors = palette[[<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]

numpy_image = np.array(image)

<span class="hljs-keyword">for</span> pose_result <span class="hljs-keyword">in</span> image_pose_result:
    scores = np.array(pose_result[<span class="hljs-string">&quot;scores&quot;</span>])
    keypoints = np.array(pose_result[<span class="hljs-string">&quot;keypoints&quot;</span>])

    <span class="hljs-comment"># draw each point on image</span>
    draw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=<span class="hljs-number">0.3</span>, radius=<span class="hljs-number">4</span>, show_keypoint_weight=<span class="hljs-literal">False</span>)

    <span class="hljs-comment"># draw links</span>
    draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=<span class="hljs-number">0.3</span>, thickness=<span class="hljs-number">1</span>, show_keypoint_weight=<span class="hljs-literal">False</span>)

pose_image = Image.fromarray(numpy_image)
pose_image`,wrap:!1}}),ls=new Ns({props:{title:"Resources",local:"resources",headingTag:"h2"}}),rs=new Ns({props:{title:"VitPoseImageProcessor",local:"transformers.VitPoseImageProcessor",headingTag:"h2"}}),is=new Zs({props:{name:"class transformers.VitPoseImageProcessor",anchor:"transformers.VitPoseImageProcessor",parameters:[{name:"do_affine_transform",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VitPoseImageProcessor.do_affine_transform",description:`<strong>do_affine_transform</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply an affine transformation to the input images.`,name:"do_affine_transform"},{anchor:"transformers.VitPoseImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 256, &quot;width&quot;: 192}</code>):
Resolution of the image after <code>affine_transform</code> is applied. Only has an effect if <code>do_affine_transform</code> is set to <code>True</code>. Can
be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.VitPoseImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.).`,name:"do_rescale"},{anchor:"transformers.VitPoseImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.VitPoseImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with mean and standard deviation.`,name:"do_normalize"},{anchor:"transformers.VitPoseImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>list[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>, <em>optional</em>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.VitPoseImageProcessor.image_std",description:`<strong>image_std</strong> (<code>list[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>, <em>optional</em>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/image_processing_vitpose.py#L328"}}),cs=new Zs({props:{name:"preprocess",anchor:"transformers.VitPoseImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"boxes",val:": typing.Union[list[list[float]], numpy.ndarray]"},{name:"do_affine_transform",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension] = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.VitPoseImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.VitPoseImageProcessor.preprocess.boxes",description:`<strong>boxes</strong> (<code>list[list[list[float]]]</code> or <code>np.ndarray</code>) &#x2014;
List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the bounding
box coordinates in COCO format (top_left_x, top_left_y, width, height).`,name:"boxes"},{anchor:"transformers.VitPoseImageProcessor.preprocess.do_affine_transform",description:`<strong>do_affine_transform</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_affine_transform</code>) &#x2014;
Whether to apply an affine transformation to the input images.`,name:"do_affine_transform"},{anchor:"transformers.VitPoseImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Dictionary in the format <code>{&quot;height&quot;: h, &quot;width&quot;: w}</code> specifying the size of the output image after
resizing.`,name:"size"},{anchor:"transformers.VitPoseImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.VitPoseImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.VitPoseImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.VitPoseImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.VitPoseImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.VitPoseImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/image_processing_vitpose.py#L423",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong>  Pixel values to be fed to a model, of shape (batch_size, num_channels, height,
width).</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),ps=new Zs({props:{name:"post_process_pose_estimation",anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation",parameters:[{name:"outputs",val:": VitPoseEstimatorOutput"},{name:"boxes",val:": typing.Union[list[list[list[float]]], numpy.ndarray]"},{name:"kernel_size",val:": int = 11"},{name:"threshold",val:": typing.Optional[float] = None"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]] = None"}],parametersDescription:[{anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation.outputs",description:`<strong>outputs</strong> (<code>VitPoseEstimatorOutput</code>) &#x2014;
VitPoseForPoseEstimation model outputs.`,name:"outputs"},{anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation.boxes",description:`<strong>boxes</strong> (<code>list[list[list[float]]]</code> or <code>np.ndarray</code>) &#x2014;
List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the bounding
box coordinates in COCO format (top_left_x, top_left_y, width, height).`,name:"boxes"},{anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation.kernel_size",description:`<strong>kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 11) &#x2014;
Gaussian kernel size (K) for modulation.`,name:"kernel_size"},{anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation.threshold",description:`<strong>threshold</strong> (<code>float</code>, <em>optional</em>, defaults to None) &#x2014;
Score threshold to keep object detection predictions.`,name:"threshold"},{anchor:"transformers.VitPoseImageProcessor.post_process_pose_estimation.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>list[tuple[int, int]]</code>, <em>optional</em>) &#x2014;
Tensor of shape <code>(batch_size, 2)</code> or list of tuples (<code>tuple[int, int]</code>) containing the target size
<code>(height, width)</code> of each image in the batch. If unset, predictions will be resize with the default value.`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/image_processing_vitpose.py#L597",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints and boxes for an image
in the batch as predicted by the model.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[list[Dict]]</code></p>
`}}),Ms=new Ns({props:{title:"VitPoseConfig",local:"transformers.VitPoseConfig",headingTag:"h2"}}),ms=new Zs({props:{name:"class transformers.VitPoseConfig",anchor:"transformers.VitPoseConfig",parameters:[{name:"backbone_config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"backbone",val:": typing.Optional[str] = None"},{name:"use_pretrained_backbone",val:": bool = False"},{name:"use_timm_backbone",val:": bool = False"},{name:"backbone_kwargs",val:": typing.Optional[dict] = None"},{name:"initializer_range",val:": float = 0.02"},{name:"scale_factor",val:": int = 4"},{name:"use_simple_decoder",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VitPoseConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>, defaults to <code>VitPoseBackboneConfig()</code>) &#x2014;
The configuration of the backbone model. Currently, only <code>backbone_config</code> with <code>vitpose_backbone</code> as <code>model_type</code> is supported.`,name:"backbone_config"},{anchor:"transformers.VitPoseConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.VitPoseConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.VitPoseConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to load <code>backbone</code> from the timm library. If <code>False</code>, the backbone is loaded from the transformers
library.`,name:"use_timm_backbone"},{anchor:"transformers.VitPoseConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.VitPoseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VitPoseConfig.scale_factor",description:`<strong>scale_factor</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Factor to upscale the feature maps coming from the ViT backbone.`,name:"scale_factor"},{anchor:"transformers.VitPoseConfig.use_simple_decoder",description:`<strong>use_simple_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a <code>VitPoseSimpleDecoder</code> to decode the feature maps from the backbone into heatmaps. Otherwise it uses <code>VitPoseClassicDecoder</code>.`,name:"use_simple_decoder"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/configuration_vitpose.py#L28"}}),E=new sn({props:{anchor:"transformers.VitPoseConfig.example",$$slots:{default:[Mn]},$$scope:{ctx:Z}}}),ds=new Ns({props:{title:"VitPoseForPoseEstimation",local:"transformers.VitPoseForPoseEstimation",headingTag:"h2"}}),ys=new Zs({props:{name:"class transformers.VitPoseForPoseEstimation",anchor:"transformers.VitPoseForPoseEstimation",parameters:[{name:"config",val:": VitPoseConfig"}],parametersDescription:[{anchor:"transformers.VitPoseForPoseEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseConfig">VitPoseConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/modeling_vitpose.py#L194"}}),Js=new Zs({props:{name:"forward",anchor:"transformers.VitPoseForPoseEstimation.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"dataset_index",val:": typing.Optional[torch.Tensor] = None"},{name:"flip_pairs",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.VitPoseForPoseEstimation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.VitPoseForPoseEstimation.forward.dataset_index",description:`<strong>dataset_index</strong> (<code>torch.Tensor</code> of shape <code>(batch_size,)</code>) &#x2014;
Index to use in the Mixture-of-Experts (MoE) blocks of the backbone.</p>
<p>This corresponds to the dataset index used during training, e.g. For the single dataset index 0 refers to the corresponding dataset. For the multiple datasets index 0 refers to dataset A (e.g. MPII) and index 1 refers to dataset B (e.g. CrowdPose).`,name:"dataset_index"},{anchor:"transformers.VitPoseForPoseEstimation.forward.flip_pairs",description:`<strong>flip_pairs</strong> (<code>torch.tensor</code>, <em>optional</em>) &#x2014;
Whether to mirror pairs of keypoints (for example, left ear &#x2014; right ear).`,name:"flip_pairs"},{anchor:"transformers.VitPoseForPoseEstimation.forward.labels",description:`<strong>labels</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitpose/modeling_vitpose.py#L213",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.vitpose.modeling_vitpose.VitPoseEstimatorOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseConfig"
>VitPoseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  Loss is not supported at this moment. See <a
  href="https://github.com/ViTAE-Transformer/ViTPose/tree/main/mmpose/models/losses"
  rel="nofollow"
>https://github.com/ViTAE-Transformer/ViTPose/tree/main/mmpose/models/losses</a> for further detail.</p>
</li>
<li>
<p><strong>heatmaps</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, height, width)</code>)  Heatmaps as predicted by the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.vitpose.modeling_vitpose.VitPoseEstimatorOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),A=new cn({props:{$$slots:{default:[mn]},$$scope:{ctx:Z}}}),G=new sn({props:{anchor:"transformers.VitPoseForPoseEstimation.forward.example",$$slots:{default:[dn]},$$scope:{ctx:Z}}}),Ts=new pn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vitpose.md"}}),{c(){c=o("meta"),j=l(),U=o("p"),m=l(),u=o("p"),u.innerHTML=p,I=l(),N=o("div"),N.innerHTML=ve,Ws=l(),d(Y.$$.fragment),Xs=l(),F=o("p"),F.innerHTML=Ve,Es=l(),H=o("p"),H.innerHTML=Ze,As=l(),z=o("img"),Gs=l(),S=o("p"),S.innerHTML=ze,Rs=l(),$=o("p"),$.innerHTML=Qe,xs=l(),d(D.$$.fragment),Ps=l(),Q=o("div"),Q.innerHTML=We,Ys=l(),L=o("p"),L.innerHTML=Xe,Fs=l(),K=o("p"),K.innerHTML=Ee,Hs=l(),d(q.$$.fragment),Ss=l(),d(O.$$.fragment),$s=l(),C=o("ul"),Us=o("li"),Us.innerHTML=Ae,ce=l(),us=o("li"),us.innerHTML=Ge,pe=l(),ss=o("li"),bs=o("p"),bs.innerHTML=Re,Me=l(),d(es.$$.fragment),me=l(),ns=o("li"),js=o("p"),js.innerHTML=xe,de=l(),d(as.$$.fragment),Ds=l(),d(ls.$$.fragment),Ls=l(),ts=o("p"),ts.textContent=Pe,Ks=l(),os=o("ul"),os.innerHTML=Ye,qs=l(),d(rs.$$.fragment),Os=l(),f=o("div"),d(is.$$.fragment),ye=l(),fs=o("p"),fs.textContent=Fe,Je=l(),W=o("div"),d(cs.$$.fragment),Te=l(),gs=o("p"),gs.textContent=He,he=l(),X=o("div"),d(ps.$$.fragment),we=l(),Is=o("p"),Is.textContent=Se,se=l(),d(Ms.$$.fragment),ee=l(),g=o("div"),d(ms.$$.fragment),Ue=l(),Cs=o("p"),Cs.innerHTML=$e,ue=l(),_s=o("p"),_s.innerHTML=De,be=l(),d(E.$$.fragment),ne=l(),d(ds.$$.fragment),ae=l(),b=o("div"),d(ys.$$.fragment),je=l(),ks=o("p"),ks.textContent=Le,fe=l(),Bs=o("p"),Bs.innerHTML=Ke,ge=l(),vs=o("p"),vs.innerHTML=qe,Ie=l(),_=o("div"),d(Js.$$.fragment),Ce=l(),Vs=o("p"),Vs.innerHTML=Oe,_e=l(),d(A.$$.fragment),ke=l(),d(G.$$.fragment),le=l(),d(Ts.$$.fragment),te=l(),Qs=o("p"),this.h()},l(s){const e=on("svelte-u9bgzb",document.head);c=r(e,"META",{name:!0,content:!0}),e.forEach(n),j=t(s),U=r(s,"P",{}),v(U).forEach(n),m=t(s),u=r(s,"P",{"data-svelte-h":!0}),M(u)!=="svelte-1a7zm26"&&(u.innerHTML=p),I=t(s),N=r(s,"DIV",{style:!0,"data-svelte-h":!0}),M(N)!=="svelte-100e61"&&(N.innerHTML=ve),Ws=t(s),y(Y.$$.fragment,s),Xs=t(s),F=r(s,"P",{"data-svelte-h":!0}),M(F)!=="svelte-1v22yin"&&(F.innerHTML=Ve),Es=t(s),H=r(s,"P",{"data-svelte-h":!0}),M(H)!=="svelte-1w9zs1n"&&(H.innerHTML=Ze),As=t(s),z=r(s,"IMG",{src:!0,alt:!0,width:!0}),Gs=t(s),S=r(s,"P",{"data-svelte-h":!0}),M(S)!=="svelte-9e850o"&&(S.innerHTML=ze),Rs=t(s),$=r(s,"P",{"data-svelte-h":!0}),M($)!=="svelte-1byn2tr"&&($.innerHTML=Qe),xs=t(s),y(D.$$.fragment,s),Ps=t(s),Q=r(s,"DIV",{class:!0,"data-svelte-h":!0}),M(Q)!=="svelte-f93tab"&&(Q.innerHTML=We),Ys=t(s),L=r(s,"P",{"data-svelte-h":!0}),M(L)!=="svelte-nf5ooi"&&(L.innerHTML=Xe),Fs=t(s),K=r(s,"P",{"data-svelte-h":!0}),M(K)!=="svelte-w36i1c"&&(K.innerHTML=Ee),Hs=t(s),y(q.$$.fragment,s),Ss=t(s),y(O.$$.fragment,s),$s=t(s),C=r(s,"UL",{});var B=v(C);Us=r(B,"LI",{"data-svelte-h":!0}),M(Us)!=="svelte-1byosop"&&(Us.innerHTML=Ae),ce=t(B),us=r(B,"LI",{"data-svelte-h":!0}),M(us)!=="svelte-8xam7b"&&(us.innerHTML=Ge),pe=t(B),ss=r(B,"LI",{});var hs=v(ss);bs=r(hs,"P",{"data-svelte-h":!0}),M(bs)!=="svelte-15141u4"&&(bs.innerHTML=Re),Me=t(hs),y(es.$$.fragment,hs),hs.forEach(n),me=t(B),ns=r(B,"LI",{});var ws=v(ns);js=r(ws,"P",{"data-svelte-h":!0}),M(js)!=="svelte-y4ytp0"&&(js.innerHTML=xe),de=t(ws),y(as.$$.fragment,ws),ws.forEach(n),B.forEach(n),Ds=t(s),y(ls.$$.fragment,s),Ls=t(s),ts=r(s,"P",{"data-svelte-h":!0}),M(ts)!=="svelte-sux81q"&&(ts.textContent=Pe),Ks=t(s),os=r(s,"UL",{"data-svelte-h":!0}),M(os)!=="svelte-1lwv20u"&&(os.innerHTML=Ye),qs=t(s),y(rs.$$.fragment,s),Os=t(s),f=r(s,"DIV",{class:!0});var R=v(f);y(is.$$.fragment,R),ye=t(R),fs=r(R,"P",{"data-svelte-h":!0}),M(fs)!=="svelte-1uf40pk"&&(fs.textContent=Fe),Je=t(R),W=r(R,"DIV",{class:!0});var re=v(W);y(cs.$$.fragment,re),Te=t(re),gs=r(re,"P",{"data-svelte-h":!0}),M(gs)!=="svelte-1x3yxsa"&&(gs.textContent=He),re.forEach(n),he=t(R),X=r(R,"DIV",{class:!0});var ie=v(X);y(ps.$$.fragment,ie),we=t(ie),Is=r(ie,"P",{"data-svelte-h":!0}),M(Is)!=="svelte-1qvlu0g"&&(Is.textContent=Se),ie.forEach(n),R.forEach(n),se=t(s),y(Ms.$$.fragment,s),ee=t(s),g=r(s,"DIV",{class:!0});var x=v(g);y(ms.$$.fragment,x),Ue=t(x),Cs=r(x,"P",{"data-svelte-h":!0}),M(Cs)!=="svelte-dc60bx"&&(Cs.innerHTML=$e),ue=t(x),_s=r(x,"P",{"data-svelte-h":!0}),M(_s)!=="svelte-1ek1ss9"&&(_s.innerHTML=De),be=t(x),y(E.$$.fragment,x),x.forEach(n),ne=t(s),y(ds.$$.fragment,s),ae=t(s),b=r(s,"DIV",{class:!0});var V=v(b);y(ys.$$.fragment,V),je=t(V),ks=r(V,"P",{"data-svelte-h":!0}),M(ks)!=="svelte-er1y03"&&(ks.textContent=Le),fe=t(V),Bs=r(V,"P",{"data-svelte-h":!0}),M(Bs)!=="svelte-q52n56"&&(Bs.innerHTML=Ke),ge=t(V),vs=r(V,"P",{"data-svelte-h":!0}),M(vs)!=="svelte-hswkmf"&&(vs.innerHTML=qe),Ie=t(V),_=r(V,"DIV",{class:!0});var P=v(_);y(Js.$$.fragment,P),Ce=t(P),Vs=r(P,"P",{"data-svelte-h":!0}),M(Vs)!=="svelte-10sozvj"&&(Vs.innerHTML=Oe),_e=t(P),y(A.$$.fragment,P),ke=t(P),y(G.$$.fragment,P),P.forEach(n),V.forEach(n),le=t(s),y(Ts.$$.fragment,s),te=t(s),Qs=r(s,"P",{}),v(Qs).forEach(n),this.h()},h(){k(c,"name","hf:doc:metadata"),k(c,"content",Jn),rn(N,"float","right"),nn(z.src,Ne="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png")||k(z,"src",Ne),k(z,"alt","drawing"),k(z,"width","600"),k(Q,"class","flex justify-center"),k(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,e){i(document.head,c),a(s,j,e),a(s,U,e),a(s,m,e),a(s,u,e),a(s,I,e),a(s,N,e),a(s,Ws,e),J(Y,s,e),a(s,Xs,e),a(s,F,e),a(s,Es,e),a(s,H,e),a(s,As,e),a(s,z,e),a(s,Gs,e),a(s,S,e),a(s,Rs,e),a(s,$,e),a(s,xs,e),J(D,s,e),a(s,Ps,e),a(s,Q,e),a(s,Ys,e),a(s,L,e),a(s,Fs,e),a(s,K,e),a(s,Hs,e),J(q,s,e),a(s,Ss,e),J(O,s,e),a(s,$s,e),a(s,C,e),i(C,Us),i(C,ce),i(C,us),i(C,pe),i(C,ss),i(ss,bs),i(ss,Me),J(es,ss,null),i(C,me),i(C,ns),i(ns,js),i(ns,de),J(as,ns,null),a(s,Ds,e),J(ls,s,e),a(s,Ls,e),a(s,ts,e),a(s,Ks,e),a(s,os,e),a(s,qs,e),J(rs,s,e),a(s,Os,e),a(s,f,e),J(is,f,null),i(f,ye),i(f,fs),i(f,Je),i(f,W),J(cs,W,null),i(W,Te),i(W,gs),i(f,he),i(f,X),J(ps,X,null),i(X,we),i(X,Is),a(s,se,e),J(Ms,s,e),a(s,ee,e),a(s,g,e),J(ms,g,null),i(g,Ue),i(g,Cs),i(g,ue),i(g,_s),i(g,be),J(E,g,null),a(s,ne,e),J(ds,s,e),a(s,ae,e),a(s,b,e),J(ys,b,null),i(b,je),i(b,ks),i(b,fe),i(b,Bs),i(b,ge),i(b,vs),i(b,Ie),i(b,_),J(Js,_,null),i(_,Ce),i(_,Vs),i(_,_e),J(A,_,null),i(_,ke),J(G,_,null),a(s,le,e),J(Ts,s,e),a(s,te,e),a(s,Qs,e),oe=!0},p(s,[e]){const B={};e&2&&(B.$$scope={dirty:e,ctx:s}),E.$set(B);const hs={};e&2&&(hs.$$scope={dirty:e,ctx:s}),A.$set(hs);const ws={};e&2&&(ws.$$scope={dirty:e,ctx:s}),G.$set(ws)},i(s){oe||(T(Y.$$.fragment,s),T(D.$$.fragment,s),T(q.$$.fragment,s),T(O.$$.fragment,s),T(es.$$.fragment,s),T(as.$$.fragment,s),T(ls.$$.fragment,s),T(rs.$$.fragment,s),T(is.$$.fragment,s),T(cs.$$.fragment,s),T(ps.$$.fragment,s),T(Ms.$$.fragment,s),T(ms.$$.fragment,s),T(E.$$.fragment,s),T(ds.$$.fragment,s),T(ys.$$.fragment,s),T(Js.$$.fragment,s),T(A.$$.fragment,s),T(G.$$.fragment,s),T(Ts.$$.fragment,s),oe=!0)},o(s){h(Y.$$.fragment,s),h(D.$$.fragment,s),h(q.$$.fragment,s),h(O.$$.fragment,s),h(es.$$.fragment,s),h(as.$$.fragment,s),h(ls.$$.fragment,s),h(rs.$$.fragment,s),h(is.$$.fragment,s),h(cs.$$.fragment,s),h(ps.$$.fragment,s),h(Ms.$$.fragment,s),h(ms.$$.fragment,s),h(E.$$.fragment,s),h(ds.$$.fragment,s),h(ys.$$.fragment,s),h(Js.$$.fragment,s),h(A.$$.fragment,s),h(G.$$.fragment,s),h(Ts.$$.fragment,s),oe=!1},d(s){s&&(n(j),n(U),n(m),n(u),n(I),n(N),n(Ws),n(Xs),n(F),n(Es),n(H),n(As),n(z),n(Gs),n(S),n(Rs),n($),n(xs),n(Ps),n(Q),n(Ys),n(L),n(Fs),n(K),n(Hs),n(Ss),n($s),n(C),n(Ds),n(Ls),n(ts),n(Ks),n(os),n(qs),n(Os),n(f),n(se),n(ee),n(g),n(ne),n(ae),n(b),n(le),n(te),n(Qs)),n(c),w(Y,s),w(D,s),w(q,s),w(O,s),w(es),w(as),w(ls,s),w(rs,s),w(is),w(cs),w(ps),w(Ms,s),w(ms),w(E),w(ds,s),w(ys),w(Js),w(A),w(G),w(Ts,s)}}}const Jn='{"title":"ViTPose","local":"vitpose","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"VitPoseImageProcessor","local":"transformers.VitPoseImageProcessor","sections":[],"depth":2},{"title":"VitPoseConfig","local":"transformers.VitPoseConfig","sections":[],"depth":2},{"title":"VitPoseForPoseEstimation","local":"transformers.VitPoseForPoseEstimation","sections":[],"depth":2}],"depth":1}';function Tn(Z){return an(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gn extends ln{constructor(c){super(),tn(this,c,Tn,yn,en,{})}}export{gn as component};
