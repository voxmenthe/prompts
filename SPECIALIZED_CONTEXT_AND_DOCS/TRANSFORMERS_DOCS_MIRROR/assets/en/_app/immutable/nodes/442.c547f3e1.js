import{s as At,z as kt,o as Zt,n as N}from"../chunks/scheduler.18a86fab.js";import{S as xt,i as Ft,g as m,s as l,r as g,A as Jt,h as p,f as s,c as d,j as de,x as v,u,k as F,l as zt,y as h,a as r,v as _,d as M,t as T,w as y}from"../chunks/index.98837b22.js";import{T as Ye}from"../chunks/Tip.77304350.js";import{D as be}from"../chunks/Docstring.a1ef7999.js";import{C as qe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as dt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ye,E as Wt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Bt,a as Pt}from"../chunks/HfOption.6641485e.js";function Ht(b){let t,c="Click on the ViTMAE models in the right sidebar for more examples of how to apply ViTMAE to vision tasks.";return{c(){t=m("p"),t.textContent=c},l(n){t=p(n,"P",{"data-svelte-h":!0}),v(t)!=="svelte-12zzffq"&&(t.textContent=c)},m(n,i){r(n,t,i)},p:N,d(n){n&&s(t)}}}function Rt(b){let t,c;return t=new qe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBpbmZlcl9kZXZpY2UlMkMlMjBWaVRJbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1BRUZvclByZVRyYWluaW5nJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZwaXBlbGluZS1jYXQtY2hvbmsuanBlZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMFZpVEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiklMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQWlucHV0cyUyMCUzRCUyMCU3QmslM0ElMjB2LnRvKGRldmljZSklMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwaW5wdXRzLml0ZW1zKCklN0QlMEElMEFtb2RlbCUyMCUzRCUyMFZpVE1BRUZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJzZHBhJTIyKS50byhkZXZpY2UpJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFyZWNvbnN0cnVjdGlvbiUyMCUzRCUyMG91dHB1dHMubG9naXRz",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> infer_device, ViTImageProcessor, ViTMAEForPreTraining

device = infer_device()

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = ViTImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>)
inputs = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
inputs = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}

model = ViTMAEForPreTraining.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>, attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>).to(device)
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

reconstruction = outputs.logits`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,i){_(t,n,i),c=!0},p:N,i(n){c||(M(t.$$.fragment,n),c=!0)},o(n){T(t.$$.fragment,n),c=!1},d(n){y(t,n)}}}function It(b){let t,c;return t=new Pt({props:{id:"usage",option:"AutoModel",$$slots:{default:[Rt]},$$scope:{ctx:b}}}),{c(){g(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,i){_(t,n,i),c=!0},p(n,i){const f={};i&2&&(f.$$scope={dirty:i,ctx:n}),t.$set(f)},i(n){c||(M(t.$$.fragment,n),c=!0)},o(n){T(t.$$.fragment,n),c=!1},d(n){y(t,n)}}}function Yt(b){let t,c="Example:",n,i,f;return i=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpVE1BRUNvbmZpZyUyQyUyMFZpVE1BRU1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZpVCUyME1BRSUyMHZpdC1tYWUtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBWaVRNQUVDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwdml0LW1hZS1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBWaVRNQUVNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ViTMAEConfig, ViTMAEModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT MAE vit-mae-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ViTMAEConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the vit-mae-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMAEModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),g(i.$$.fragment)},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=c),n=d(a),u(i.$$.fragment,a)},m(a,w){r(a,t,w),r(a,n,w),_(i,a,w),f=!0},p:N,i(a){f||(M(i.$$.fragment,a),f=!0)},o(a){T(i.$$.fragment,a),f=!1},d(a){a&&(s(t),s(n)),y(i,a)}}}function qt(b){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=c},l(n){t=p(n,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,i){r(n,t,i)},p:N,d(n){n&&s(t)}}}function Nt(b){let t,c="Examples:",n,i,f;return i=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1BRU1vZGVsJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMFZpVE1BRU1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMAEModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMAEModel.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),g(i.$$.fragment)},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),u(i.$$.fragment,a)},m(a,w){r(a,t,w),r(a,n,w),_(i,a,w),f=!0},p:N,i(a){f||(M(i.$$.fragment,a),f=!0)},o(a){T(i.$$.fragment,a),f=!1},d(a){a&&(s(t),s(n)),y(i,a)}}}function Lt(b){let t,c=`Note that we provide a script to pre-train this model on custom data in our <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">examples
directory</a>.`;return{c(){t=m("p"),t.innerHTML=c},l(n){t=p(n,"P",{"data-svelte-h":!0}),v(t)!=="svelte-7i3y9o"&&(t.innerHTML=c)},m(n,i){r(n,t,i)},p:N,d(n){n&&s(t)}}}function Gt(b){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=c},l(n){t=p(n,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,i){r(n,t,i)},p:N,d(n){n&&s(t)}}}function St(b){let t,c="Examples:",n,i,f;return i=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1BRUZvclByZVRyYWluaW5nJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMFZpVE1BRUZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tYWUtYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb3NzJTIwJTNEJTIwb3V0cHV0cy5sb3NzJTBBbWFzayUyMCUzRCUyMG91dHB1dHMubWFzayUwQWlkc19yZXN0b3JlJTIwJTNEJTIwb3V0cHV0cy5pZHNfcmVzdG9yZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMAEForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMAEForPreTraining.from_pretrained(<span class="hljs-string">&quot;facebook/vit-mae-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = outputs.mask
<span class="hljs-meta">&gt;&gt;&gt; </span>ids_restore = outputs.ids_restore`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),g(i.$$.fragment)},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),u(i.$$.fragment,a)},m(a,w){r(a,t,w),r(a,n,w),_(i,a,w),f=!0},p:N,i(a){f||(M(i.$$.fragment,a),f=!0)},o(a){T(i.$$.fragment,a),f=!1},d(a){a&&(s(t),s(n)),y(i,a)}}}function Qt(b){let t,c,n,i,f,a="<em>This model was released on 2021-11-11 and added to Hugging Face Transformers on 2022-01-18.</em>",w,J,ct='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',$e,L,we,G,mt='<a href="https://huggingface.co/papers/2111.06377" rel="nofollow">ViTMAE</a> is a self-supervised vision model that is pretrained by masking large portions of an image (~75%). An encoder processes the visible image patches and a decoder reconstructs the missing pixels from the encoded patches and mask tokens. After pretraining, the encoder can be reused for downstream tasks like image classification or object detection — often outperforming models trained with supervised learning.',Ve,z,pt,Ee,S,ht='You can find all the original ViTMAE checkpoints under the <a href="https://huggingface.co/facebook?search_models=vit-mae" rel="nofollow">AI at Meta</a> organization.',je,W,Ue,Q,ft='The example below demonstrates how to reconstruct the missing pixels with the <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> class.',Ce,B,Ae,X,ke,D,gt='<li>ViTMAE is typically used in two stages. Self-supervised pretraining with <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a>, and then discarding the decoder and fine-tuning the encoder. After fine-tuning, the weights can be plugged into a model like <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a>.</li> <li>Use <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for input preparation.</li>',Ze,K,xe,O,ut='<li>Refer to this <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb" rel="nofollow">notebook</a> to learn how to visualize the reconstructed pixels from <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a>.</li>',Fe,ee,Je,j,te,Ne,ce,_t=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a>. It is used to instantiate an ViT
MAE model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the ViT
<a href="https://huggingface.co/facebook/vit-mae-base" rel="nofollow">facebook/vit-mae-base</a> architecture.`,Le,me,Mt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ge,P,ze,ne,We,V,oe,Se,pe,Tt="The bare Vit Mae Model outputting raw hidden-states without any specific head on top.",Qe,he,yt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Xe,fe,vt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,De,C,se,Ke,ge,bt='The <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> forward method, overrides the <code>__call__</code> special method.',Oe,H,et,R,Be,ae,Pe,$,re,tt,ue,$t="The ViTMAE Model transformer with the decoder on top for self-supervised pre-training.",nt,I,ot,_e,wt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,st,Me,Vt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,at,A,ie,rt,Te,Et='The <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> forward method, overrides the <code>__call__</code> special method.',it,Y,lt,q,He,le,Re,ve,Ie;return L=new ye({props:{title:"ViTMAE",local:"vitmae",headingTag:"h1"}}),W=new Ye({props:{warning:!1,$$slots:{default:[Ht]},$$scope:{ctx:b}}}),B=new Bt({props:{id:"usage",options:["AutoModel"],$$slots:{default:[It]},$$scope:{ctx:b}}}),X=new ye({props:{title:"Notes",local:"notes",headingTag:"h2"}}),K=new ye({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ee=new ye({props:{title:"ViTMAEConfig",local:"transformers.ViTMAEConfig",headingTag:"h2"}}),te=new be({props:{name:"class transformers.ViTMAEConfig",anchor:"transformers.ViTMAEConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"decoder_num_attention_heads",val:" = 16"},{name:"decoder_hidden_size",val:" = 512"},{name:"decoder_num_hidden_layers",val:" = 8"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"mask_ratio",val:" = 0.75"},{name:"norm_pix_loss",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ViTMAEConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ViTMAEConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.ViTMAEConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ViTMAEConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.ViTMAEConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ViTMAEConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ViTMAEConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.ViTMAEConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ViTMAEConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ViTMAEConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.ViTMAEConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.ViTMAEConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ViTMAEConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.ViTMAEConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the decoder.`,name:"decoder_num_attention_heads"},{anchor:"transformers.ViTMAEConfig.decoder_hidden_size",description:`<strong>decoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the decoder.`,name:"decoder_hidden_size"},{anchor:"transformers.ViTMAEConfig.decoder_num_hidden_layers",description:`<strong>decoder_num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the decoder.`,name:"decoder_num_hidden_layers"},{anchor:"transformers.ViTMAEConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.ViTMAEConfig.mask_ratio",description:`<strong>mask_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.75) &#x2014;
The ratio of the number of masked tokens in the input sequence.`,name:"mask_ratio"},{anchor:"transformers.ViTMAEConfig.norm_pix_loss",description:`<strong>norm_pix_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to train with normalized pixels (see Table 3 in the paper). Using normalized pixels improved
representation quality in the experiments of the authors.`,name:"norm_pix_loss"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_mae/configuration_vit_mae.py#L24"}}),P=new dt({props:{anchor:"transformers.ViTMAEConfig.example",$$slots:{default:[Yt]},$$scope:{ctx:b}}}),ne=new ye({props:{title:"ViTMAEModel",local:"transformers.ViTMAEModel",headingTag:"h2"}}),oe=new be({props:{name:"class transformers.ViTMAEModel",anchor:"transformers.ViTMAEModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ViTMAEModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L575"}}),se=new be({props:{name:"forward",anchor:"transformers.ViTMAEModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.ViTMAEModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.ViTMAEModel.forward.noise",description:`<strong>noise</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mainly used for testing purposes to control randomness and maintain the reproducibility`,name:"noise"},{anchor:"transformers.ViTMAEModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMAEModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>, default <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings. This is mainly used to use the model on higher
resolution images.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L599",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig"
>ViTMAEConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) — Tensor indicating which patches are masked (1) and which are not (0).</p>
</li>
<li>
<p><strong>ids_restore</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) — Tensor containing the original index of the (shuffled) masked patches.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new Ye({props:{$$slots:{default:[qt]},$$scope:{ctx:b}}}),R=new dt({props:{anchor:"transformers.ViTMAEModel.forward.example",$$slots:{default:[Nt]},$$scope:{ctx:b}}}),ae=new ye({props:{title:"ViTMAEForPreTraining",local:"transformers.ViTMAEForPreTraining",headingTag:"h2"}}),re=new be({props:{name:"class transformers.ViTMAEForPreTraining",anchor:"transformers.ViTMAEForPreTraining",parameters:[{name:"config",val:": ViTMAEConfig"}],parametersDescription:[{anchor:"transformers.ViTMAEForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L777"}}),I=new Ye({props:{$$slots:{default:[Lt]},$$scope:{ctx:b}}}),ie=new be({props:{name:"forward",anchor:"transformers.ViTMAEForPreTraining.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.ViTMAEForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.ViTMAEForPreTraining.forward.noise",description:`<strong>noise</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mainly used for testing purposes to control randomness and maintain the reproducibility`,name:"noise"},{anchor:"transformers.ViTMAEForPreTraining.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMAEForPreTraining.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>, default <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings. This is mainly used to use the model on higher
resolution images.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L907",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig"
>ViTMAEConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>) — Pixel reconstruction loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, patch_size ** 2 * num_channels)</code>) — Pixel reconstruction logits.</p>
</li>
<li>
<p><strong>mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) — Tensor indicating which patches are masked (1) and which are not (0).</p>
</li>
<li>
<p><strong>ids_restore</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) — Tensor containing the original index of the (shuffled) masked patches.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Y=new Ye({props:{$$slots:{default:[Gt]},$$scope:{ctx:b}}}),q=new dt({props:{anchor:"transformers.ViTMAEForPreTraining.forward.example",$$slots:{default:[St]},$$scope:{ctx:b}}}),le=new Wt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit_mae.md"}}),{c(){t=m("meta"),c=l(),n=m("p"),i=l(),f=m("p"),f.innerHTML=a,w=l(),J=m("div"),J.innerHTML=ct,$e=l(),g(L.$$.fragment),we=l(),G=m("p"),G.innerHTML=mt,Ve=l(),z=m("img"),Ee=l(),S=m("p"),S.innerHTML=ht,je=l(),g(W.$$.fragment),Ue=l(),Q=m("p"),Q.innerHTML=ft,Ce=l(),g(B.$$.fragment),Ae=l(),g(X.$$.fragment),ke=l(),D=m("ul"),D.innerHTML=gt,Ze=l(),g(K.$$.fragment),xe=l(),O=m("ul"),O.innerHTML=ut,Fe=l(),g(ee.$$.fragment),Je=l(),j=m("div"),g(te.$$.fragment),Ne=l(),ce=m("p"),ce.innerHTML=_t,Le=l(),me=m("p"),me.innerHTML=Mt,Ge=l(),g(P.$$.fragment),ze=l(),g(ne.$$.fragment),We=l(),V=m("div"),g(oe.$$.fragment),Se=l(),pe=m("p"),pe.textContent=Tt,Qe=l(),he=m("p"),he.innerHTML=yt,Xe=l(),fe=m("p"),fe.innerHTML=vt,De=l(),C=m("div"),g(se.$$.fragment),Ke=l(),ge=m("p"),ge.innerHTML=bt,Oe=l(),g(H.$$.fragment),et=l(),g(R.$$.fragment),Be=l(),g(ae.$$.fragment),Pe=l(),$=m("div"),g(re.$$.fragment),tt=l(),ue=m("p"),ue.textContent=$t,nt=l(),g(I.$$.fragment),ot=l(),_e=m("p"),_e.innerHTML=wt,st=l(),Me=m("p"),Me.innerHTML=Vt,at=l(),A=m("div"),g(ie.$$.fragment),rt=l(),Te=m("p"),Te.innerHTML=Et,it=l(),g(Y.$$.fragment),lt=l(),g(q.$$.fragment),He=l(),g(le.$$.fragment),Re=l(),ve=m("p"),this.h()},l(e){const o=Jt("svelte-u9bgzb",document.head);t=p(o,"META",{name:!0,content:!0}),o.forEach(s),c=d(e),n=p(e,"P",{}),de(n).forEach(s),i=d(e),f=p(e,"P",{"data-svelte-h":!0}),v(f)!=="svelte-1ac2qif"&&(f.innerHTML=a),w=d(e),J=p(e,"DIV",{style:!0,"data-svelte-h":!0}),v(J)!=="svelte-2m0t7r"&&(J.innerHTML=ct),$e=d(e),u(L.$$.fragment,e),we=d(e),G=p(e,"P",{"data-svelte-h":!0}),v(G)!=="svelte-15f1ra6"&&(G.innerHTML=mt),Ve=d(e),z=p(e,"IMG",{src:!0,alt:!0,width:!0}),Ee=d(e),S=p(e,"P",{"data-svelte-h":!0}),v(S)!=="svelte-17frgj3"&&(S.innerHTML=ht),je=d(e),u(W.$$.fragment,e),Ue=d(e),Q=p(e,"P",{"data-svelte-h":!0}),v(Q)!=="svelte-uvwhl"&&(Q.innerHTML=ft),Ce=d(e),u(B.$$.fragment,e),Ae=d(e),u(X.$$.fragment,e),ke=d(e),D=p(e,"UL",{"data-svelte-h":!0}),v(D)!=="svelte-1yaey94"&&(D.innerHTML=gt),Ze=d(e),u(K.$$.fragment,e),xe=d(e),O=p(e,"UL",{"data-svelte-h":!0}),v(O)!=="svelte-1p4u9wi"&&(O.innerHTML=ut),Fe=d(e),u(ee.$$.fragment,e),Je=d(e),j=p(e,"DIV",{class:!0});var k=de(j);u(te.$$.fragment,k),Ne=d(k),ce=p(k,"P",{"data-svelte-h":!0}),v(ce)!=="svelte-wfb2gj"&&(ce.innerHTML=_t),Le=d(k),me=p(k,"P",{"data-svelte-h":!0}),v(me)!=="svelte-1ek1ss9"&&(me.innerHTML=Mt),Ge=d(k),u(P.$$.fragment,k),k.forEach(s),ze=d(e),u(ne.$$.fragment,e),We=d(e),V=p(e,"DIV",{class:!0});var U=de(V);u(oe.$$.fragment,U),Se=d(U),pe=p(U,"P",{"data-svelte-h":!0}),v(pe)!=="svelte-1jgrh6g"&&(pe.textContent=Tt),Qe=d(U),he=p(U,"P",{"data-svelte-h":!0}),v(he)!=="svelte-q52n56"&&(he.innerHTML=yt),Xe=d(U),fe=p(U,"P",{"data-svelte-h":!0}),v(fe)!=="svelte-hswkmf"&&(fe.innerHTML=vt),De=d(U),C=p(U,"DIV",{class:!0});var Z=de(C);u(se.$$.fragment,Z),Ke=d(Z),ge=p(Z,"P",{"data-svelte-h":!0}),v(ge)!=="svelte-owx8s0"&&(ge.innerHTML=bt),Oe=d(Z),u(H.$$.fragment,Z),et=d(Z),u(R.$$.fragment,Z),Z.forEach(s),U.forEach(s),Be=d(e),u(ae.$$.fragment,e),Pe=d(e),$=p(e,"DIV",{class:!0});var E=de($);u(re.$$.fragment,E),tt=d(E),ue=p(E,"P",{"data-svelte-h":!0}),v(ue)!=="svelte-10cjq04"&&(ue.textContent=$t),nt=d(E),u(I.$$.fragment,E),ot=d(E),_e=p(E,"P",{"data-svelte-h":!0}),v(_e)!=="svelte-q52n56"&&(_e.innerHTML=wt),st=d(E),Me=p(E,"P",{"data-svelte-h":!0}),v(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Vt),at=d(E),A=p(E,"DIV",{class:!0});var x=de(A);u(ie.$$.fragment,x),rt=d(x),Te=p(x,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-1s75uwo"&&(Te.innerHTML=Et),it=d(x),u(Y.$$.fragment,x),lt=d(x),u(q.$$.fragment,x),x.forEach(s),E.forEach(s),He=d(e),u(le.$$.fragment,e),Re=d(e),ve=p(e,"P",{}),de(ve).forEach(s),this.h()},h(){F(t,"name","hf:doc:metadata"),F(t,"content",Xt),zt(J,"float","right"),kt(z.src,pt="https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png")||F(z,"src",pt),F(z,"alt","drawing"),F(z,"width","600"),F(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){h(document.head,t),r(e,c,o),r(e,n,o),r(e,i,o),r(e,f,o),r(e,w,o),r(e,J,o),r(e,$e,o),_(L,e,o),r(e,we,o),r(e,G,o),r(e,Ve,o),r(e,z,o),r(e,Ee,o),r(e,S,o),r(e,je,o),_(W,e,o),r(e,Ue,o),r(e,Q,o),r(e,Ce,o),_(B,e,o),r(e,Ae,o),_(X,e,o),r(e,ke,o),r(e,D,o),r(e,Ze,o),_(K,e,o),r(e,xe,o),r(e,O,o),r(e,Fe,o),_(ee,e,o),r(e,Je,o),r(e,j,o),_(te,j,null),h(j,Ne),h(j,ce),h(j,Le),h(j,me),h(j,Ge),_(P,j,null),r(e,ze,o),_(ne,e,o),r(e,We,o),r(e,V,o),_(oe,V,null),h(V,Se),h(V,pe),h(V,Qe),h(V,he),h(V,Xe),h(V,fe),h(V,De),h(V,C),_(se,C,null),h(C,Ke),h(C,ge),h(C,Oe),_(H,C,null),h(C,et),_(R,C,null),r(e,Be,o),_(ae,e,o),r(e,Pe,o),r(e,$,o),_(re,$,null),h($,tt),h($,ue),h($,nt),_(I,$,null),h($,ot),h($,_e),h($,st),h($,Me),h($,at),h($,A),_(ie,A,null),h(A,rt),h(A,Te),h(A,it),_(Y,A,null),h(A,lt),_(q,A,null),r(e,He,o),_(le,e,o),r(e,Re,o),r(e,ve,o),Ie=!0},p(e,[o]){const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),W.$set(k);const U={};o&2&&(U.$$scope={dirty:o,ctx:e}),B.$set(U);const Z={};o&2&&(Z.$$scope={dirty:o,ctx:e}),P.$set(Z);const E={};o&2&&(E.$$scope={dirty:o,ctx:e}),H.$set(E);const x={};o&2&&(x.$$scope={dirty:o,ctx:e}),R.$set(x);const jt={};o&2&&(jt.$$scope={dirty:o,ctx:e}),I.$set(jt);const Ut={};o&2&&(Ut.$$scope={dirty:o,ctx:e}),Y.$set(Ut);const Ct={};o&2&&(Ct.$$scope={dirty:o,ctx:e}),q.$set(Ct)},i(e){Ie||(M(L.$$.fragment,e),M(W.$$.fragment,e),M(B.$$.fragment,e),M(X.$$.fragment,e),M(K.$$.fragment,e),M(ee.$$.fragment,e),M(te.$$.fragment,e),M(P.$$.fragment,e),M(ne.$$.fragment,e),M(oe.$$.fragment,e),M(se.$$.fragment,e),M(H.$$.fragment,e),M(R.$$.fragment,e),M(ae.$$.fragment,e),M(re.$$.fragment,e),M(I.$$.fragment,e),M(ie.$$.fragment,e),M(Y.$$.fragment,e),M(q.$$.fragment,e),M(le.$$.fragment,e),Ie=!0)},o(e){T(L.$$.fragment,e),T(W.$$.fragment,e),T(B.$$.fragment,e),T(X.$$.fragment,e),T(K.$$.fragment,e),T(ee.$$.fragment,e),T(te.$$.fragment,e),T(P.$$.fragment,e),T(ne.$$.fragment,e),T(oe.$$.fragment,e),T(se.$$.fragment,e),T(H.$$.fragment,e),T(R.$$.fragment,e),T(ae.$$.fragment,e),T(re.$$.fragment,e),T(I.$$.fragment,e),T(ie.$$.fragment,e),T(Y.$$.fragment,e),T(q.$$.fragment,e),T(le.$$.fragment,e),Ie=!1},d(e){e&&(s(c),s(n),s(i),s(f),s(w),s(J),s($e),s(we),s(G),s(Ve),s(z),s(Ee),s(S),s(je),s(Ue),s(Q),s(Ce),s(Ae),s(ke),s(D),s(Ze),s(xe),s(O),s(Fe),s(Je),s(j),s(ze),s(We),s(V),s(Be),s(Pe),s($),s(He),s(Re),s(ve)),s(t),y(L,e),y(W,e),y(B,e),y(X,e),y(K,e),y(ee,e),y(te),y(P),y(ne,e),y(oe),y(se),y(H),y(R),y(ae,e),y(re),y(I),y(ie),y(Y),y(q),y(le,e)}}}const Xt='{"title":"ViTMAE","local":"vitmae","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ViTMAEConfig","local":"transformers.ViTMAEConfig","sections":[],"depth":2},{"title":"ViTMAEModel","local":"transformers.ViTMAEModel","sections":[],"depth":2},{"title":"ViTMAEForPreTraining","local":"transformers.ViTMAEForPreTraining","sections":[],"depth":2}],"depth":1}';function Dt(b){return Zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rn extends xt{constructor(t){super(),Ft(this,t,Dt,Qt,At,{})}}export{rn as component};
