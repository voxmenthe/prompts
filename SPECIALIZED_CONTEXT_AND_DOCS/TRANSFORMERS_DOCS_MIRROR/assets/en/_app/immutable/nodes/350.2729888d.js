import{s as At,z as xt,o as kt,n as dt}from"../chunks/scheduler.18a86fab.js";import{S as Zt,i as Gt,g as i,s,r as g,A as Wt,h as l,f as o,c as a,j as U,x as d,u as f,k as R,y as m,a as n,v as u,d as _,t as y,w as b}from"../chunks/index.98837b22.js";import{T as Bt}from"../chunks/Tip.77304350.js";import{D as de}from"../chunks/Docstring.a1ef7999.js";import{C as ht}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as zt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as he,E as Ft}from"../chunks/getInferenceSnippets.06c2775f.js";function Et($){let r,M="Example:",h,c,w;return c=new ht({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByb21wdERlcHRoQW55dGhpbmdDb25maWclMkMlMjBQcm9tcHREZXB0aEFueXRoaW5nRm9yRGVwdGhFc3RpbWF0aW9uJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFByb21wdERlcHRoQW55dGhpbmclMjBzbWFsbCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBQcm9tcHREZXB0aEFueXRoaW5nQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwUHJvbXB0RGVwdGhBbnl0aGluZyUyMHNtYWxsJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBQcm9tcHREZXB0aEFueXRoaW5nRm9yRGVwdGhFc3RpbWF0aW9uKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PromptDepthAnythingConfig, PromptDepthAnythingForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a PromptDepthAnything small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = PromptDepthAnythingConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the PromptDepthAnything small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PromptDepthAnythingForDepthEstimation(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=i("p"),r.textContent=M,h=s(),g(c.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),d(r)!=="svelte-11lpom8"&&(r.textContent=M),h=a(p),f(c.$$.fragment,p)},m(p,j){n(p,r,j),n(p,h,j),u(c,p,j),w=!0},p:dt,i(p){w||(_(c.$$.fragment,p),w=!0)},o(p){y(c.$$.fragment,p),w=!1},d(p){p&&(o(r),o(h)),b(c,p)}}}function Nt($){let r,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=M},l(h){r=l(h,"P",{"data-svelte-h":!0}),d(r)!=="svelte-fincs2"&&(r.innerHTML=M)},m(h,c){n(h,r,c)},p:dt,d(h){h&&o(r)}}}function Ht($){let r,M="Example:",h,c,w;return c=new ht({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/image.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;depth-anything/prompt-depth-anything-vits-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;depth-anything/prompt-depth-anything-vits-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_depth_url = <span class="hljs-string">&quot;https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_depth = Image.<span class="hljs-built_in">open</span>(requests.get(prompt_depth_url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, prompt_depth=prompt_depth)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>post_processed_output = image_processor.post_process_depth_estimation(
<span class="hljs-meta">... </span>    outputs,
<span class="hljs-meta">... </span>    target_sizes=[(image.height, image.width)],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># visualize the prediction</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = predicted_depth * <span class="hljs-number">1000.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth.detach().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint16&quot;</span>)) <span class="hljs-comment"># mm</span>`,wrap:!1}}),{c(){r=i("p"),r.textContent=M,h=s(),g(c.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),d(r)!=="svelte-11lpom8"&&(r.textContent=M),h=a(p),f(c.$$.fragment,p)},m(p,j){n(p,r,j),n(p,h,j),u(c,p,j),w=!0},p:dt,i(p){w||(_(c.$$.fragment,p),w=!0)},o(p){y(c.$$.fragment,p),w=!1},d(p){p&&(o(r),o(h)),b(c,p)}}}function Xt($){let r,M,h,c,w,p="<em>This model was released on 2024-12-18 and added to Hugging Face Transformers on 2025-03-21.</em>",j,F,Je,E,Pe,N,gt='The Prompt Depth Anything model was introduced in <a href="https://huggingface.co/papers/2412.14015" rel="nofollow">Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</a> by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang.',je,H,ft="The abstract from the paper is as follows:",De,X,ut="<em>Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.</em>",Re,C,_t,Ue,V,yt='Prompt Depth Anything overview. Taken from the <a href="https://huggingface.co/papers/2412.14015">original paper</a>.',$e,S,Ce,L,bt="The Transformers library allows you to use the model with just a few lines of code:",ze,Q,Ae,Y,xe,q,wt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Prompt Depth Anything.",ke,O,Tt='<li><a href="https://huggingface.co/spaces/depth-anything/PromptDA" rel="nofollow">Prompt Depth Anything Demo</a></li> <li><a href="https://promptda.github.io/interactive.html" rel="nofollow">Prompt Depth Anything Interactive Results</a></li>',Ze,K,vt="If you are interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Ge,ee,We,T,te,Qe,ge,Mt=`This is the configuration class to store the configuration of a <code>PromptDepthAnythingModel</code>. It is used to instantiate a PromptDepthAnything
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the PromptDepthAnything
<a href="https://huggingface.co/LiheYoung/depth-anything-small-hf" rel="nofollow">LiheYoung/depth-anything-small-hf</a> architecture.`,Ye,fe,It=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,qe,z,Oe,A,oe,Ke,ue,Jt=`Serializes this instance to a Python dictionary. Override the default <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict">to_dict()</a>. Returns:
<code>dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,`,Be,ne,Fe,v,se,et,_e,Pt="Prompt Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.",tt,ye,jt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ot,be,Dt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,nt,D,ae,st,we,Rt='The <a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation">PromptDepthAnythingForDepthEstimation</a> forward method, overrides the <code>__call__</code> special method.',at,x,rt,k,Ee,re,Ne,I,ie,it,Te,Ut="Constructs a PromptDepthAnything image processor.",lt,Z,le,pt,ve,$t="Preprocess an image or batch of images.",mt,G,pe,ct,Me,Ct=`Converts the raw output of <code>DepthEstimatorOutput</code> into final depth predictions and depth PIL images.
Only supports PyTorch.`,He,me,Xe,Ie,Ve;return F=new he({props:{title:"Prompt Depth Anything",local:"prompt-depth-anything",headingTag:"h1"}}),E=new he({props:{title:"Overview",local:"overview",headingTag:"h2"}}),S=new he({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),Q=new ht({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/image.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;depth-anything/prompt-depth-anything-vits-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;depth-anything/prompt-depth-anything-vits-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_depth_url = <span class="hljs-string">&quot;https://github.com/DepthAnything/PromptDA/blob/main/assets/example_images/arkit_depth.png?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_depth = Image.<span class="hljs-built_in">open</span>(requests.get(prompt_depth_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the prompt depth can be None, and the model will output a monocular relative depth.</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, prompt_depth=prompt_depth)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>post_processed_output = image_processor.post_process_depth_estimation(
<span class="hljs-meta">... </span>    outputs,
<span class="hljs-meta">... </span>    target_sizes=[(image.height, image.width)],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># visualize the prediction</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = predicted_depth * <span class="hljs-number">1000</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth.detach().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint16&quot;</span>)) <span class="hljs-comment"># mm</span>`,wrap:!1}}),Y=new he({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ee=new he({props:{title:"PromptDepthAnythingConfig",local:"transformers.PromptDepthAnythingConfig",headingTag:"h2"}}),te=new de({props:{name:"class transformers.PromptDepthAnythingConfig",anchor:"transformers.PromptDepthAnythingConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"patch_size",val:" = 14"},{name:"initializer_range",val:" = 0.02"},{name:"reassemble_hidden_size",val:" = 384"},{name:"reassemble_factors",val:" = [4, 2, 1, 0.5]"},{name:"neck_hidden_sizes",val:" = [48, 96, 192, 384]"},{name:"fusion_hidden_size",val:" = 64"},{name:"head_in_index",val:" = -1"},{name:"head_hidden_size",val:" = 32"},{name:"depth_estimation_type",val:" = 'relative'"},{name:"max_depth",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>Union[dict[str, Any], PretrainedConfig]</code>, <em>optional</em>) &#x2014;
The configuration of the backbone model. Only used in case <code>is_hybrid</code> is <code>True</code> or in case you want to
leverage the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a> API.`,name:"backbone_config"},{anchor:"transformers.PromptDepthAnythingConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.PromptDepthAnythingConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.PromptDepthAnythingConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the <code>timm</code> library for the backbone. If set to <code>False</code>, will use the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a>
API.`,name:"use_timm_backbone"},{anchor:"transformers.PromptDepthAnythingConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.PromptDepthAnythingConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size of the patches to extract from the backbone features.`,name:"patch_size"},{anchor:"transformers.PromptDepthAnythingConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.PromptDepthAnythingConfig.reassemble_hidden_size",description:`<strong>reassemble_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
The number of input channels of the reassemble layers.`,name:"reassemble_hidden_size"},{anchor:"transformers.PromptDepthAnythingConfig.reassemble_factors",description:`<strong>reassemble_factors</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 1, 0.5]</code>) &#x2014;
The up/downsampling factors of the reassemble layers.`,name:"reassemble_factors"},{anchor:"transformers.PromptDepthAnythingConfig.neck_hidden_sizes",description:`<strong>neck_hidden_sizes</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[48, 96, 192, 384]</code>) &#x2014;
The hidden sizes to project to for the feature maps of the backbone.`,name:"neck_hidden_sizes"},{anchor:"transformers.PromptDepthAnythingConfig.fusion_hidden_size",description:`<strong>fusion_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of channels before fusion.`,name:"fusion_hidden_size"},{anchor:"transformers.PromptDepthAnythingConfig.head_in_index",description:`<strong>head_in_index</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the features to use in the depth estimation head.`,name:"head_in_index"},{anchor:"transformers.PromptDepthAnythingConfig.head_hidden_size",description:`<strong>head_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of output channels in the second convolution of the depth estimation head.`,name:"head_hidden_size"},{anchor:"transformers.PromptDepthAnythingConfig.depth_estimation_type",description:`<strong>depth_estimation_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative&quot;</code>) &#x2014;
The type of depth estimation to use. Can be one of <code>[&quot;relative&quot;, &quot;metric&quot;]</code>.`,name:"depth_estimation_type"},{anchor:"transformers.PromptDepthAnythingConfig.max_depth",description:`<strong>max_depth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The maximum depth to use for the &#x201C;metric&#x201D; depth estimation head. 20 should be used for indoor models
and 80 for outdoor models. For &#x201C;relative&#x201D; depth estimation, this value is ignored.`,name:"max_depth"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py#L31"}}),z=new zt({props:{anchor:"transformers.PromptDepthAnythingConfig.example",$$slots:{default:[Et]},$$scope:{ctx:$}}}),oe=new de({props:{name:"to_dict",anchor:"transformers.PromptDepthAnythingConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py#L165"}}),ne=new he({props:{title:"PromptDepthAnythingForDepthEstimation",local:"transformers.PromptDepthAnythingForDepthEstimation",headingTag:"h2"}}),se=new de({props:{name:"class transformers.PromptDepthAnythingForDepthEstimation",anchor:"transformers.PromptDepthAnythingForDepthEstimation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingForDepthEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation">PromptDepthAnythingForDepthEstimation</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py#L373"}}),ae=new de({props:{name:"forward",anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"prompt_depth",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingImageProcessor">PromptDepthAnythingImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">PromptDepthAnythingImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingImageProcessor">PromptDepthAnythingImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.prompt_depth",description:`<strong>prompt_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 1, height, width)</code>, <em>optional</em>) &#x2014;
Prompt depth is the sparse or low-resolution depth obtained from multi-view geometry or a
low-resolution depth sensor. It generally has shape (height, width), where height
and width can be smaller than those of the images. It is optional and can be None, which means no prompt depth
will be used. If it is None, the output will be a monocular relative depth.
The values are recommended to be in meters, but this is not necessary.`,name:"prompt_depth"},{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py#L386",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig"
>PromptDepthAnythingConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>) â€” Predicted depth for each pixel.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),x=new Bt({props:{$$slots:{default:[Nt]},$$scope:{ctx:$}}}),k=new zt({props:{anchor:"transformers.PromptDepthAnythingForDepthEstimation.forward.example",$$slots:{default:[Ht]},$$scope:{ctx:$}}}),re=new he({props:{title:"PromptDepthAnythingImageProcessor",local:"transformers.PromptDepthAnythingImageProcessor",headingTag:"h2"}}),ie=new de({props:{name:"class transformers.PromptDepthAnythingImageProcessor",anchor:"transformers.PromptDepthAnythingImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"keep_aspect_ratio",val:": bool = False"},{name:"ensure_multiple_of",val:": int = 1"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_pad",val:": bool = False"},{name:"size_divisor",val:": typing.Optional[int] = None"},{name:"prompt_scale_to_meter",val:": float = 0.001"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions. Can be overridden by <code>do_resize</code> in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.PromptDepthAnythingImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 384, &quot;width&quot;: 384}</code>):
Size of the image after resizing. Can be overridden by <code>size</code> in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.PromptDepthAnythingImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Defines the resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in <code>preprocess</code>.`,name:"resample"},{anchor:"transformers.PromptDepthAnythingImageProcessor.keep_aspect_ratio",description:`<strong>keep_aspect_ratio</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>True</code>, the image is resized to the largest possible size such that the aspect ratio is preserved. Can
be overridden by <code>keep_aspect_ratio</code> in <code>preprocess</code>.`,name:"keep_aspect_ratio"},{anchor:"transformers.PromptDepthAnythingImageProcessor.ensure_multiple_of",description:`<strong>ensure_multiple_of</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
If <code>do_resize</code> is <code>True</code>, the image is resized to a size that is a multiple of this value. Can be overridden
by <code>ensure_multiple_of</code> in <code>preprocess</code>.`,name:"ensure_multiple_of"},{anchor:"transformers.PromptDepthAnythingImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
<code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.PromptDepthAnythingImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in <code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.PromptDepthAnythingImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.PromptDepthAnythingImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.PromptDepthAnythingImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.PromptDepthAnythingImageProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in
combination with DPT.`,name:"do_pad"},{anchor:"transformers.PromptDepthAnythingImageProcessor.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If <code>do_pad</code> is <code>True</code>, pads the image dimensions to be divisible by this value. This was introduced in the
DINOv2 paper, which uses the model in combination with DPT.`,name:"size_divisor"},{anchor:"transformers.PromptDepthAnythingImageProcessor.prompt_scale_to_meter",description:`<strong>prompt_scale_to_meter</strong> (<code>float</code>, <em>optional</em>, defaults to 0.001) &#x2014;
Scale factor to convert the prompt depth to meters.`,name:"prompt_scale_to_meter"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py#L100"}}),le=new de({props:{name:"preprocess",anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"prompt_depth",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[int] = None"},{name:"keep_aspect_ratio",val:": typing.Optional[bool] = None"},{name:"ensure_multiple_of",val:": typing.Optional[int] = None"},{name:"resample",val:": typing.Optional[PIL.Image.Resampling] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_pad",val:": typing.Optional[bool] = None"},{name:"size_divisor",val:": typing.Optional[int] = None"},{name:"prompt_scale_to_meter",val:": typing.Optional[float] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.prompt_depth",description:`<strong>prompt_depth</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
Prompt depth to preprocess, which can be sparse depth obtained from multi-view geometry or
low-resolution depth from a depth sensor. Generally has shape (height, width), where height
and width can be smaller than those of the images. It&#x2019;s optional and can be None, which means no prompt depth
is used. If it is None, the output depth will be a monocular relative depth.
It is recommended to provide a prompt_scale_to_meter value, which is the scale factor to convert the prompt depth
to meters. This is useful when the prompt depth is not in meters.`,name:"prompt_depth"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing. If <code>keep_aspect_ratio</code> is <code>True</code>, the image is resized to the largest
possible size such that the aspect ratio is preserved. If <code>ensure_multiple_of</code> is set, the image is
resized to a size that is a multiple of this value.`,name:"size"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.keep_aspect_ratio",description:`<strong>keep_aspect_ratio</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.keep_aspect_ratio</code>) &#x2014;
Whether to keep the aspect ratio of the image. If False, the image will be resized to (size, size). If
True, the image will be resized to keep the aspect ratio and the size will be the maximum possible.`,name:"keep_aspect_ratio"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.ensure_multiple_of",description:`<strong>ensure_multiple_of</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.ensure_multiple_of</code>) &#x2014;
Ensure that the image size is a multiple of this value.`,name:"ensure_multiple_of"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.prompt_scale_to_meter",description:`<strong>prompt_scale_to_meter</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.prompt_scale_to_meter</code>) &#x2014;
Scale factor to convert the prompt depth to meters.`,name:"prompt_scale_to_meter"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.PromptDepthAnythingImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py#L277"}}),pe=new de({props:{name:"post_process_depth_estimation",anchor:"transformers.PromptDepthAnythingImageProcessor.post_process_depth_estimation",parameters:[{name:"outputs",val:": DepthEstimatorOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple[int, int]], NoneType] = None"}],parametersDescription:[{anchor:"transformers.PromptDepthAnythingImageProcessor.post_process_depth_estimation.outputs",description:`<strong>outputs</strong> (<code>DepthEstimatorOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.PromptDepthAnythingImageProcessor.post_process_depth_estimation.target_sizes",description:`<strong>target_sizes</strong> (<code>TensorType</code> or <code>list[tuple[int, int]]</code>, <em>optional</em>) &#x2014;
Tensor of shape <code>(batch_size, 2)</code> or list of tuples (<code>tuple[int, int]</code>) containing the target size
(height, width) of each image in the batch. If left to None, predictions will not be resized.`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py#L463",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries of tensors representing the processed depth
predictions.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[dict[str, TensorType]]</code></p>
`}}),me=new Ft({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/prompt_depth_anything.md"}}),{c(){r=i("meta"),M=s(),h=i("p"),c=s(),w=i("p"),w.innerHTML=p,j=s(),g(F.$$.fragment),Je=s(),g(E.$$.fragment),Pe=s(),N=i("p"),N.innerHTML=gt,je=s(),H=i("p"),H.textContent=ft,De=s(),X=i("p"),X.innerHTML=ut,Re=s(),C=i("img"),Ue=s(),V=i("small"),V.innerHTML=yt,$e=s(),g(S.$$.fragment),Ce=s(),L=i("p"),L.textContent=bt,ze=s(),g(Q.$$.fragment),Ae=s(),g(Y.$$.fragment),xe=s(),q=i("p"),q.textContent=wt,ke=s(),O=i("ul"),O.innerHTML=Tt,Ze=s(),K=i("p"),K.textContent=vt,Ge=s(),g(ee.$$.fragment),We=s(),T=i("div"),g(te.$$.fragment),Qe=s(),ge=i("p"),ge.innerHTML=Mt,Ye=s(),fe=i("p"),fe.innerHTML=It,qe=s(),g(z.$$.fragment),Oe=s(),A=i("div"),g(oe.$$.fragment),Ke=s(),ue=i("p"),ue.innerHTML=Jt,Be=s(),g(ne.$$.fragment),Fe=s(),v=i("div"),g(se.$$.fragment),et=s(),_e=i("p"),_e.textContent=Pt,tt=s(),ye=i("p"),ye.innerHTML=jt,ot=s(),be=i("p"),be.innerHTML=Dt,nt=s(),D=i("div"),g(ae.$$.fragment),st=s(),we=i("p"),we.innerHTML=Rt,at=s(),g(x.$$.fragment),rt=s(),g(k.$$.fragment),Ee=s(),g(re.$$.fragment),Ne=s(),I=i("div"),g(ie.$$.fragment),it=s(),Te=i("p"),Te.textContent=Ut,lt=s(),Z=i("div"),g(le.$$.fragment),pt=s(),ve=i("p"),ve.textContent=$t,mt=s(),G=i("div"),g(pe.$$.fragment),ct=s(),Me=i("p"),Me.innerHTML=Ct,He=s(),g(me.$$.fragment),Xe=s(),Ie=i("p"),this.h()},l(e){const t=Wt("svelte-u9bgzb",document.head);r=l(t,"META",{name:!0,content:!0}),t.forEach(o),M=a(e),h=l(e,"P",{}),U(h).forEach(o),c=a(e),w=l(e,"P",{"data-svelte-h":!0}),d(w)!=="svelte-12w12wp"&&(w.innerHTML=p),j=a(e),f(F.$$.fragment,e),Je=a(e),f(E.$$.fragment,e),Pe=a(e),N=l(e,"P",{"data-svelte-h":!0}),d(N)!=="svelte-vzqwvt"&&(N.innerHTML=gt),je=a(e),H=l(e,"P",{"data-svelte-h":!0}),d(H)!=="svelte-alsz90"&&(H.textContent=ft),De=a(e),X=l(e,"P",{"data-svelte-h":!0}),d(X)!=="svelte-urjegz"&&(X.innerHTML=ut),Re=a(e),C=l(e,"IMG",{src:!0,alt:!0,width:!0}),Ue=a(e),V=l(e,"SMALL",{"data-svelte-h":!0}),d(V)!=="svelte-1yogyb8"&&(V.innerHTML=yt),$e=a(e),f(S.$$.fragment,e),Ce=a(e),L=l(e,"P",{"data-svelte-h":!0}),d(L)!=="svelte-yntg63"&&(L.textContent=bt),ze=a(e),f(Q.$$.fragment,e),Ae=a(e),f(Y.$$.fragment,e),xe=a(e),q=l(e,"P",{"data-svelte-h":!0}),d(q)!=="svelte-1ptedny"&&(q.textContent=wt),ke=a(e),O=l(e,"UL",{"data-svelte-h":!0}),d(O)!=="svelte-1m2okqw"&&(O.innerHTML=Tt),Ze=a(e),K=l(e,"P",{"data-svelte-h":!0}),d(K)!=="svelte-a5h4y"&&(K.textContent=vt),Ge=a(e),f(ee.$$.fragment,e),We=a(e),T=l(e,"DIV",{class:!0});var J=U(T);f(te.$$.fragment,J),Qe=a(J),ge=l(J,"P",{"data-svelte-h":!0}),d(ge)!=="svelte-1cafvc7"&&(ge.innerHTML=Mt),Ye=a(J),fe=l(J,"P",{"data-svelte-h":!0}),d(fe)!=="svelte-1ek1ss9"&&(fe.innerHTML=It),qe=a(J),f(z.$$.fragment,J),Oe=a(J),A=l(J,"DIV",{class:!0});var ce=U(A);f(oe.$$.fragment,ce),Ke=a(ce),ue=l(ce,"P",{"data-svelte-h":!0}),d(ue)!=="svelte-1rfbzy4"&&(ue.innerHTML=Jt),ce.forEach(o),J.forEach(o),Be=a(e),f(ne.$$.fragment,e),Fe=a(e),v=l(e,"DIV",{class:!0});var P=U(v);f(se.$$.fragment,P),et=a(P),_e=l(P,"P",{"data-svelte-h":!0}),d(_e)!=="svelte-155faoo"&&(_e.textContent=Pt),tt=a(P),ye=l(P,"P",{"data-svelte-h":!0}),d(ye)!=="svelte-q52n56"&&(ye.innerHTML=jt),ot=a(P),be=l(P,"P",{"data-svelte-h":!0}),d(be)!=="svelte-hswkmf"&&(be.innerHTML=Dt),nt=a(P),D=l(P,"DIV",{class:!0});var W=U(D);f(ae.$$.fragment,W),st=a(W),we=l(W,"P",{"data-svelte-h":!0}),d(we)!=="svelte-18jxls6"&&(we.innerHTML=Rt),at=a(W),f(x.$$.fragment,W),rt=a(W),f(k.$$.fragment,W),W.forEach(o),P.forEach(o),Ee=a(e),f(re.$$.fragment,e),Ne=a(e),I=l(e,"DIV",{class:!0});var B=U(I);f(ie.$$.fragment,B),it=a(B),Te=l(B,"P",{"data-svelte-h":!0}),d(Te)!=="svelte-1cdhbmx"&&(Te.textContent=Ut),lt=a(B),Z=l(B,"DIV",{class:!0});var Se=U(Z);f(le.$$.fragment,Se),pt=a(Se),ve=l(Se,"P",{"data-svelte-h":!0}),d(ve)!=="svelte-1x3yxsa"&&(ve.textContent=$t),Se.forEach(o),mt=a(B),G=l(B,"DIV",{class:!0});var Le=U(G);f(pe.$$.fragment,Le),ct=a(Le),Me=l(Le,"P",{"data-svelte-h":!0}),d(Me)!=="svelte-1m4nocr"&&(Me.innerHTML=Ct),Le.forEach(o),B.forEach(o),He=a(e),f(me.$$.fragment,e),Xe=a(e),Ie=l(e,"P",{}),U(Ie).forEach(o),this.h()},h(){R(r,"name","hf:doc:metadata"),R(r,"content",Vt),xt(C.src,_t="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/prompt_depth_anything_architecture.jpg")||R(C,"src",_t),R(C,"alt","drawing"),R(C,"width","600"),R(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){m(document.head,r),n(e,M,t),n(e,h,t),n(e,c,t),n(e,w,t),n(e,j,t),u(F,e,t),n(e,Je,t),u(E,e,t),n(e,Pe,t),n(e,N,t),n(e,je,t),n(e,H,t),n(e,De,t),n(e,X,t),n(e,Re,t),n(e,C,t),n(e,Ue,t),n(e,V,t),n(e,$e,t),u(S,e,t),n(e,Ce,t),n(e,L,t),n(e,ze,t),u(Q,e,t),n(e,Ae,t),u(Y,e,t),n(e,xe,t),n(e,q,t),n(e,ke,t),n(e,O,t),n(e,Ze,t),n(e,K,t),n(e,Ge,t),u(ee,e,t),n(e,We,t),n(e,T,t),u(te,T,null),m(T,Qe),m(T,ge),m(T,Ye),m(T,fe),m(T,qe),u(z,T,null),m(T,Oe),m(T,A),u(oe,A,null),m(A,Ke),m(A,ue),n(e,Be,t),u(ne,e,t),n(e,Fe,t),n(e,v,t),u(se,v,null),m(v,et),m(v,_e),m(v,tt),m(v,ye),m(v,ot),m(v,be),m(v,nt),m(v,D),u(ae,D,null),m(D,st),m(D,we),m(D,at),u(x,D,null),m(D,rt),u(k,D,null),n(e,Ee,t),u(re,e,t),n(e,Ne,t),n(e,I,t),u(ie,I,null),m(I,it),m(I,Te),m(I,lt),m(I,Z),u(le,Z,null),m(Z,pt),m(Z,ve),m(I,mt),m(I,G),u(pe,G,null),m(G,ct),m(G,Me),n(e,He,t),u(me,e,t),n(e,Xe,t),n(e,Ie,t),Ve=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),z.$set(J);const ce={};t&2&&(ce.$$scope={dirty:t,ctx:e}),x.$set(ce);const P={};t&2&&(P.$$scope={dirty:t,ctx:e}),k.$set(P)},i(e){Ve||(_(F.$$.fragment,e),_(E.$$.fragment,e),_(S.$$.fragment,e),_(Q.$$.fragment,e),_(Y.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(z.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(x.$$.fragment,e),_(k.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),_(me.$$.fragment,e),Ve=!0)},o(e){y(F.$$.fragment,e),y(E.$$.fragment,e),y(S.$$.fragment,e),y(Q.$$.fragment,e),y(Y.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(z.$$.fragment,e),y(oe.$$.fragment,e),y(ne.$$.fragment,e),y(se.$$.fragment,e),y(ae.$$.fragment,e),y(x.$$.fragment,e),y(k.$$.fragment,e),y(re.$$.fragment,e),y(ie.$$.fragment,e),y(le.$$.fragment,e),y(pe.$$.fragment,e),y(me.$$.fragment,e),Ve=!1},d(e){e&&(o(M),o(h),o(c),o(w),o(j),o(Je),o(Pe),o(N),o(je),o(H),o(De),o(X),o(Re),o(C),o(Ue),o(V),o($e),o(Ce),o(L),o(ze),o(Ae),o(xe),o(q),o(ke),o(O),o(Ze),o(K),o(Ge),o(We),o(T),o(Be),o(Fe),o(v),o(Ee),o(Ne),o(I),o(He),o(Xe),o(Ie)),o(r),b(F,e),b(E,e),b(S,e),b(Q,e),b(Y,e),b(ee,e),b(te),b(z),b(oe),b(ne,e),b(se),b(ae),b(x),b(k),b(re,e),b(ie),b(le),b(pe),b(me,e)}}}const Vt='{"title":"Prompt Depth Anything","local":"prompt-depth-anything","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"PromptDepthAnythingConfig","local":"transformers.PromptDepthAnythingConfig","sections":[],"depth":2},{"title":"PromptDepthAnythingForDepthEstimation","local":"transformers.PromptDepthAnythingForDepthEstimation","sections":[],"depth":2},{"title":"PromptDepthAnythingImageProcessor","local":"transformers.PromptDepthAnythingImageProcessor","sections":[],"depth":2}],"depth":1}';function St($){return kt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class to extends Zt{constructor(r){super(),Gt(this,r,St,Xt,At,{})}}export{to as component};
