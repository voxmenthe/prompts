import{s as ve,o as xe,n as Ge}from"../chunks/scheduler.18a86fab.js";import{S as _e,i as ke,g as M,s as n,r as j,A as We,h as u,f as t,c as p,j as Ue,u as T,x as w,k as Ze,y as Ce,a,v as $,d as b,t as U,w as Z}from"../chunks/index.98837b22.js";import{C as x}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as fe,E as Ie}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Xe,a as ge}from"../chunks/HfOption.6641485e.js";function Ve(G){let l,y='The <a href="./model_doc/auto">AutoClass</a> API provides a simple interface to load processors without directly specifying the specific model class it belongs to.',d,m,r='Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained">from_pretrained()</a> to load a processor.',c,f,J;return f=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZwYWxpZ2VtbWEtM2ItcHQtMjI0JTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;google/paligemma-3b-pt-224&quot;</span>)`,wrap:!1}}),{c(){l=M("p"),l.innerHTML=y,d=n(),m=M("p"),m.innerHTML=r,c=n(),j(f.$$.fragment)},l(o){l=u(o,"P",{"data-svelte-h":!0}),w(l)!=="svelte-1jo1sfh"&&(l.innerHTML=y),d=p(o),m=u(o,"P",{"data-svelte-h":!0}),w(m)!=="svelte-10cr8hb"&&(m.innerHTML=r),c=p(o),T(f.$$.fragment,o)},m(o,h){a(o,l,h),a(o,d,h),a(o,m,h),a(o,c,h),$(f,o,h),J=!0},p:Ge,i(o){J||(b(f.$$.fragment,o),J=!0)},o(o){U(f.$$.fragment,o),J=!1},d(o){o&&(t(l),t(d),t(m),t(c)),Z(f,o)}}}function Re(G){let l,y='Processors are also associated with a specific pretrained multimodal model class. You can load a processor directly from the model class with <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained">from_pretrained()</a>.',d,m,r,c,f='You could also separately load the two preprocessor types, <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperTokenizerFast">WhisperTokenizerFast</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a>.',J,o,h;return m=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFdoaXNwZXJQcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBXaGlzcGVyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZ3aGlzcGVyLXRpbnklMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> WhisperProcessor

processor = WhisperProcessor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)`,wrap:!1}}),o=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFdoaXNwZXJUb2tlbml6ZXJGYXN0JTJDJTIwV2hpc3BlckZlYXR1cmVFeHRyYWN0b3IlMkMlMjBXaGlzcGVyUHJvY2Vzc29yJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwV2hpc3BlclRva2VuaXplckZhc3QuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRndoaXNwZXItdGlueSUyMiklMEFmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMFdoaXNwZXJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZ3aGlzcGVyLXRpbnklMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwV2hpc3BlclByb2Nlc3NvcihmZWF0dXJlX2V4dHJhY3RvciUzRGZlYXR1cmVfZXh0cmFjdG9yJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> WhisperTokenizerFast, WhisperFeatureExtractor, WhisperProcessor

tokenizer = WhisperTokenizerFast.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)
feature_extractor = WhisperFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)
processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)`,wrap:!1}}),{c(){l=M("p"),l.innerHTML=y,d=n(),j(m.$$.fragment),r=n(),c=M("p"),c.innerHTML=f,J=n(),j(o.$$.fragment)},l(i){l=u(i,"P",{"data-svelte-h":!0}),w(l)!=="svelte-j9t8ea"&&(l.innerHTML=y),d=p(i),T(m.$$.fragment,i),r=p(i),c=u(i,"P",{"data-svelte-h":!0}),w(c)!=="svelte-7xjszn"&&(c.innerHTML=f),J=p(i),T(o.$$.fragment,i)},m(i,g){a(i,l,g),a(i,d,g),$(m,i,g),a(i,r,g),a(i,c,g),a(i,J,g),$(o,i,g),h=!0},p:Ge,i(i){h||(b(m.$$.fragment,i),b(o.$$.fragment,i),h=!0)},o(i){U(m.$$.fragment,i),U(o.$$.fragment,i),h=!1},d(i){i&&(t(l),t(d),t(r),t(c),t(J)),Z(m,i),Z(o,i)}}}function Fe(G){let l,y,d,m;return l=new ge({props:{id:"processor-class",option:"AutoProcessor",$$slots:{default:[Ve]},$$scope:{ctx:G}}}),d=new ge({props:{id:"processor-class",option:"model-specific processor",$$slots:{default:[Re]},$$scope:{ctx:G}}}),{c(){j(l.$$.fragment),y=n(),j(d.$$.fragment)},l(r){T(l.$$.fragment,r),y=p(r),T(d.$$.fragment,r)},m(r,c){$(l,r,c),a(r,y,c),$(d,r,c),m=!0},p(r,c){const f={};c&2&&(f.$$scope={dirty:c,ctx:r}),l.$set(f);const J={};c&2&&(J.$$scope={dirty:c,ctx:r}),d.$set(J)},i(r){m||(b(l.$$.fragment,r),b(d.$$.fragment,r),m=!0)},o(r){U(l.$$.fragment,r),U(d.$$.fragment,r),m=!1},d(r){r&&t(y),Z(l,r),Z(d,r)}}}function Be(G){let l,y,d,m,r,c,f,J="Multimodal models require a preprocessor capable of handling inputs that combine more than one modality. Depending on the input modality, a processor needs to convert text into an array of tensors, images into pixel values, and audio into an array with tensors with the correct sampling rate.",o,h,i='For example, <a href="./model_doc/paligemma">PaliGemma</a> is a vision-language model that uses the <a href="./model_doc/siglip">SigLIP</a> image processor and the <a href="./model_doc/llama">Llama</a> tokenizer. A <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin">ProcessorMixin</a> class wraps both of these preprocessor types, providing a single and unified processor class for a multimodal model.',g,_,he='Call <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained">from_pretrained()</a> to load a processor. Pass the input type to the processor to generate the expected model inputs, input ids and pixel values.',q,k,L,W,Me="This guide describes the processor class and how to preprocess multimodal inputs.",S,C,D,I,ue='All processors inherit from the <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin">ProcessorMixin</a> class which provides methods like <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained">from_pretrained()</a>, <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.save_pretrained">save_pretrained()</a>, and <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> for loading, saving, and sharing processors to the Hub.',K,X,ye='There are two ways to load a processor, with an <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> and with a model-specific processor class.',O,v,ee,V,se,R,Je="Processors preprocess multimodal inputs into the expected Transformers format. There are a couple combinations of input modalities that a processor can handle such as text and audio or text and image.",te,F,we="Automatic speech recognition (ASR) tasks require a processor that can handle text and audio inputs. Load a dataset and take a look at the <code>audio</code> and <code>text</code> columns (you can remove the other columns which arenâ€™t needed).",ae,B,re,H,je="Remember to resample the sampling rate to match the pretrained models required sampling rate.",le,N,oe,z,Te="Load a processor and pass the audio <code>array</code> and <code>text</code> columns to it.",ne,E,pe,Y,$e="Apply the <code>prepare_dataset</code> function to preprocess the dataset. The processor returns <code>input_features</code> for the <code>audio</code> column and <code>labels</code> for the text column.",ie,P,ce,Q,me,A,de;return r=new fe({props:{title:"Processors",local:"processors",headingTag:"h1"}}),k=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBQYWxpR2VtbWFGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnBhbGlnZW1tYS0zYi1wdC0yMjQlMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyYW5zd2VyJTIwZW4lMjBXaGVyZSUyMGlzJTIwdGhlJTIwY2F0JTIwc3RhbmRpbmclM0YlMjIlMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZwaXBlbGluZS1jYXQtY2hvbmsuanBlZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcHJvbXB0JTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQWlucHV0cw==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, PaliGemmaForConditionalGeneration
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;google/paligemma-3b-pt-224&quot;</span>)

prompt = <span class="hljs-string">&quot;answer en Where is the cat standing?&quot;</span>
url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
inputs`,wrap:!1}}),C=new fe({props:{title:"Processor classes",local:"processor-classes",headingTag:"h2"}}),v=new Xe({props:{id:"processor-class",options:["AutoProcessor","model-specific processor"],$$slots:{default:[Fe]},$$scope:{ctx:G}}}),V=new fe({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),B=new x({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJsal9zcGVlY2glMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0Lm1hcChyZW1vdmVfY29sdW1ucyUzRCU1QiUyMmZpbGUlMjIlMkMlMjAlMjJpZCUyMiUyQyUyMCUyMm5vcm1hbGl6ZWRfdGV4dCUyMiU1RCklMEFkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTBBJTdCJ2FycmF5JyUzQSUyMGFycmF5KCU1Qi03LjMyNDIxODhlLTA0JTJDJTIwLTcuNjI5Mzk0NWUtMDQlMkMlMjAtNi40MDg2OTE0ZS0wNCUyQyUyMC4uLiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDcuMzI0MjE4OGUtMDQlMkMlMjAlMjAyLjEzNjIzMDVlLTA0JTJDJTIwJTIwNi4xMDM1MTU2ZS0wNSU1RCUyQyUyMGR0eXBlJTNEZmxvYXQzMiklMkMlMEElMjAncGF0aCclM0ElMjAnJTJGcm9vdCUyRi5jYWNoZSUyRmh1Z2dpbmdmYWNlJTJGZGF0YXNldHMlMkZkb3dubG9hZHMlMkZleHRyYWN0ZWQlMkY5MTdlY2UwOGM5NWNmMGM0MTE1ZTQ1Mjk0ZTNjZDBkZWU3MjRhMTE2NWI3ZmMxMTc5ODM2OTMwOGE0NjViZDI2JTJGTEpTcGVlY2gtMS4xJTJGd2F2cyUyRkxKMDAxLTAwMDEud2F2JyUyQyUwQSUyMCdzYW1wbGluZ19yYXRlJyUzQSUyMDIyMDUwJTdEJTBBJTBBZGF0YXNldCU1QjAlNUQlNUIlMjJ0ZXh0JTIyJTVEJTBBJ1ByaW50aW5nJTJDJTIwaW4lMjB0aGUlMjBvbmx5JTIwc2Vuc2UlMjB3aXRoJTIwd2hpY2glMjB3ZSUyMGFyZSUyMGF0JTIwcHJlc2VudCUyMGNvbmNlcm5lZCUyQyUyMGRpZmZlcnMlMjBmcm9tJTIwbW9zdCUyMGlmJTIwbm90JTIwZnJvbSUyMGFsbCUyMHRoZSUyMGFydHMlMjBhbmQlMjBjcmFmdHMlMjByZXByZXNlbnRlZCUyMGluJTIwdGhlJTIwRXhoaWJpdGlvbic=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;lj_speech&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
dataset = dataset.<span class="hljs-built_in">map</span>(remove_columns=[<span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;normalized_text&quot;</span>])
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">7.3242188e-04</span>, -<span class="hljs-number">7.6293945e-04</span>, -<span class="hljs-number">6.4086914e-04</span>, ...,
         <span class="hljs-number">7.3242188e-04</span>,  <span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">22050</span>}

dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
<span class="hljs-string">&#x27;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition&#x27;</span>`,wrap:!1}}),N=new x({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0QxNjAwMCkp",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Audio

dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))`,wrap:!1}}),E=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZ3aGlzcGVyLXRpbnklMjIpJTBBJTBBZGVmJTIwcHJlcGFyZV9kYXRhc2V0KGV4YW1wbGUpJTNBJTBBJTIwJTIwJTIwJTIwYXVkaW8lMjAlM0QlMjBleGFtcGxlJTVCJTIyYXVkaW8lMjIlNUQlMEElMjAlMjAlMjAlMjBleGFtcGxlLnVwZGF0ZShwcm9jZXNzb3IoYXVkaW8lM0RhdWRpbyU1QiUyMmFycmF5JTIyJTVEJTJDJTIwdGV4dCUzRGV4YW1wbGUlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRDE2MDAwKSklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBleGFtcGxl",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">example</span>):
    audio = example[<span class="hljs-string">&quot;audio&quot;</span>]
    example.update(processor(audio=audio[<span class="hljs-string">&quot;array&quot;</span>], text=example[<span class="hljs-string">&quot;text&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>))
    <span class="hljs-keyword">return</span> example`,wrap:!1}}),P=new x({props:{code:"cHJlcGFyZV9kYXRhc2V0KGRhdGFzZXQlNUIwJTVEKQ==",highlighted:'prepare_dataset(dataset[<span class="hljs-number">0</span>])',wrap:!1}}),Q=new Ie({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md"}}),{c(){l=M("meta"),y=n(),d=M("p"),m=n(),j(r.$$.fragment),c=n(),f=M("p"),f.textContent=J,o=n(),h=M("p"),h.innerHTML=i,g=n(),_=M("p"),_.innerHTML=he,q=n(),j(k.$$.fragment),L=n(),W=M("p"),W.textContent=Me,S=n(),j(C.$$.fragment),D=n(),I=M("p"),I.innerHTML=ue,K=n(),X=M("p"),X.innerHTML=ye,O=n(),j(v.$$.fragment),ee=n(),j(V.$$.fragment),se=n(),R=M("p"),R.textContent=Je,te=n(),F=M("p"),F.innerHTML=we,ae=n(),j(B.$$.fragment),re=n(),H=M("p"),H.textContent=je,le=n(),j(N.$$.fragment),oe=n(),z=M("p"),z.innerHTML=Te,ne=n(),j(E.$$.fragment),pe=n(),Y=M("p"),Y.innerHTML=$e,ie=n(),j(P.$$.fragment),ce=n(),j(Q.$$.fragment),me=n(),A=M("p"),this.h()},l(e){const s=We("svelte-u9bgzb",document.head);l=u(s,"META",{name:!0,content:!0}),s.forEach(t),y=p(e),d=u(e,"P",{}),Ue(d).forEach(t),m=p(e),T(r.$$.fragment,e),c=p(e),f=u(e,"P",{"data-svelte-h":!0}),w(f)!=="svelte-1wtpgpx"&&(f.textContent=J),o=p(e),h=u(e,"P",{"data-svelte-h":!0}),w(h)!=="svelte-1jd1s4b"&&(h.innerHTML=i),g=p(e),_=u(e,"P",{"data-svelte-h":!0}),w(_)!=="svelte-l9xqa6"&&(_.innerHTML=he),q=p(e),T(k.$$.fragment,e),L=p(e),W=u(e,"P",{"data-svelte-h":!0}),w(W)!=="svelte-1gb0hfe"&&(W.textContent=Me),S=p(e),T(C.$$.fragment,e),D=p(e),I=u(e,"P",{"data-svelte-h":!0}),w(I)!=="svelte-1kfyp88"&&(I.innerHTML=ue),K=p(e),X=u(e,"P",{"data-svelte-h":!0}),w(X)!=="svelte-79x6b1"&&(X.innerHTML=ye),O=p(e),T(v.$$.fragment,e),ee=p(e),T(V.$$.fragment,e),se=p(e),R=u(e,"P",{"data-svelte-h":!0}),w(R)!=="svelte-jx4i2i"&&(R.textContent=Je),te=p(e),F=u(e,"P",{"data-svelte-h":!0}),w(F)!=="svelte-4xjjb6"&&(F.innerHTML=we),ae=p(e),T(B.$$.fragment,e),re=p(e),H=u(e,"P",{"data-svelte-h":!0}),w(H)!=="svelte-t3ixwg"&&(H.textContent=je),le=p(e),T(N.$$.fragment,e),oe=p(e),z=u(e,"P",{"data-svelte-h":!0}),w(z)!=="svelte-2pg53k"&&(z.innerHTML=Te),ne=p(e),T(E.$$.fragment,e),pe=p(e),Y=u(e,"P",{"data-svelte-h":!0}),w(Y)!=="svelte-144ra7k"&&(Y.innerHTML=$e),ie=p(e),T(P.$$.fragment,e),ce=p(e),T(Q.$$.fragment,e),me=p(e),A=u(e,"P",{}),Ue(A).forEach(t),this.h()},h(){Ze(l,"name","hf:doc:metadata"),Ze(l,"content",He)},m(e,s){Ce(document.head,l),a(e,y,s),a(e,d,s),a(e,m,s),$(r,e,s),a(e,c,s),a(e,f,s),a(e,o,s),a(e,h,s),a(e,g,s),a(e,_,s),a(e,q,s),$(k,e,s),a(e,L,s),a(e,W,s),a(e,S,s),$(C,e,s),a(e,D,s),a(e,I,s),a(e,K,s),a(e,X,s),a(e,O,s),$(v,e,s),a(e,ee,s),$(V,e,s),a(e,se,s),a(e,R,s),a(e,te,s),a(e,F,s),a(e,ae,s),$(B,e,s),a(e,re,s),a(e,H,s),a(e,le,s),$(N,e,s),a(e,oe,s),a(e,z,s),a(e,ne,s),$(E,e,s),a(e,pe,s),a(e,Y,s),a(e,ie,s),$(P,e,s),a(e,ce,s),$(Q,e,s),a(e,me,s),a(e,A,s),de=!0},p(e,[s]){const be={};s&2&&(be.$$scope={dirty:s,ctx:e}),v.$set(be)},i(e){de||(b(r.$$.fragment,e),b(k.$$.fragment,e),b(C.$$.fragment,e),b(v.$$.fragment,e),b(V.$$.fragment,e),b(B.$$.fragment,e),b(N.$$.fragment,e),b(E.$$.fragment,e),b(P.$$.fragment,e),b(Q.$$.fragment,e),de=!0)},o(e){U(r.$$.fragment,e),U(k.$$.fragment,e),U(C.$$.fragment,e),U(v.$$.fragment,e),U(V.$$.fragment,e),U(B.$$.fragment,e),U(N.$$.fragment,e),U(E.$$.fragment,e),U(P.$$.fragment,e),U(Q.$$.fragment,e),de=!1},d(e){e&&(t(y),t(d),t(m),t(c),t(f),t(o),t(h),t(g),t(_),t(q),t(L),t(W),t(S),t(D),t(I),t(K),t(X),t(O),t(ee),t(se),t(R),t(te),t(F),t(ae),t(re),t(H),t(le),t(oe),t(z),t(ne),t(pe),t(Y),t(ie),t(ce),t(me),t(A)),t(l),Z(r,e),Z(k,e),Z(C,e),Z(v,e),Z(V,e),Z(B,e),Z(N,e),Z(E,e),Z(P,e),Z(Q,e)}}}const He='{"title":"Processors","local":"processors","sections":[{"title":"Processor classes","local":"processor-classes","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2}],"depth":1}';function Ne(G){return xe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ae extends _e{constructor(l){super(),ke(this,l,Ne,Be,ve,{})}}export{Ae as component};
