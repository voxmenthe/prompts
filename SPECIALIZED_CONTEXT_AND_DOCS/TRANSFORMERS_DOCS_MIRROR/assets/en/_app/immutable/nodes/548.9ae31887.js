import{s as It,o as Ct,n as Qs}from"../chunks/scheduler.18a86fab.js";import{S as $t,i as _t,g as p,s as l,r as M,A as Zt,h as i,f as t,c as n,j as gt,u as m,x as o,k as bt,y as xt,a,v as c,d as h,t as u,w as d}from"../chunks/index.98837b22.js";import{T as Es}from"../chunks/Tip.77304350.js";import{Y as Ut}from"../chunks/Youtube.14fb207c.js";import{C as j}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as kt}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as Ze,E as At}from"../chunks/getInferenceSnippets.06c2775f.js";function vt(T){let r,y='To see all architectures and checkpoints compatible with this task, we recommend checking the <a href="https://huggingface.co/tasks/question-answering" rel="nofollow">task-page</a>';return{c(){r=p("p"),r.innerHTML=y},l(w){r=i(w,"P",{"data-svelte-h":!0}),o(r)!=="svelte-19lbpy7"&&(r.innerHTML=y)},m(w,f){a(w,r,f)},p:Qs,d(w){w&&t(r)}}}function Bt(T){let r,y='If you arenâ€™t familiar with finetuning a model with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#train-with-pytorch-trainer">here</a>!';return{c(){r=p("p"),r.innerHTML=y},l(w){r=i(w,"P",{"data-svelte-h":!0}),o(r)!=="svelte-p303g8"&&(r.innerHTML=y)},m(w,f){a(w,r,f)},p:Qs,d(w){w&&t(r)}}}function Xt(T){let r,y=`For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb" rel="nofollow">PyTorch notebook</a>.`;return{c(){r=p("p"),r.innerHTML=y},l(w){r=i(w,"P",{"data-svelte-h":!0}),o(r)!=="svelte-1me7u83"&&(r.innerHTML=y)},m(w,f){a(w,r,f)},p:Qs,d(w){w&&t(r)}}}function Wt(T){let r,y,w,f,U,ke,I,Ae,C,ve,$,Hs="Question answering tasks return an answer given a question. If youâ€™ve ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then youâ€™ve used a question answering model before. There are two common types of question answering tasks:",Be,_,qs="<li>Extractive: extract the answer from the given context.</li> <li>Abstractive: generate an answer from the context that correctly answers the question.</li>",Xe,Z,Vs="This guide will show you how to:",We,x,zs='<li>Finetune <a href="https://huggingface.co/distilbert/distilbert-base-uncased" rel="nofollow">DistilBERT</a> on the <a href="https://huggingface.co/datasets/squad" rel="nofollow">SQuAD</a> dataset for extractive question answering.</li> <li>Use your finetuned model for inference.</li>',Ge,J,Re,k,Fs="Before you begin, make sure you have all the necessary libraries installed:",Ne,A,Ee,v,Ys="We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:",Qe,B,He,X,qe,W,Ss="Start by loading a smaller subset of the SQuAD dataset from the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.",Ve,G,ze,R,Ls='Split the datasetâ€™s <code>train</code> split into a train and test set with the <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.train_test_split" rel="nofollow">train_test_split</a> method:',Fe,N,Ye,E,Ds="Then take a look at an example:",Se,Q,Le,H,Ps="There are several important fields here:",De,q,Ks="<li><code>answers</code>: the starting location of the answer token and the answer text.</li> <li><code>context</code>: background information from which the model needs to extract the answer.</li> <li><code>question</code>: the question a model should answer.</li>",Pe,V,Ke,z,Oe,F,Os="The next step is to load a DistilBERT tokenizer to process the <code>question</code> and <code>context</code> fields:",es,Y,ss,S,et="There are a few preprocessing steps particular to question answering tasks you should be aware of:",ts,L,st=`<li>Some examples in a dataset may have a very long <code>context</code> that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the <code>context</code> by setting <code>truncation=&quot;only_second&quot;</code>.</li> <li>Next, map the start and end positions of the answer to the original <code>context</code> by setting
<code>return_offset_mapping=True</code>.</li> <li>With the mapping in hand, now you can find the start and end tokens of the answer. Use the <code>sequence_ids</code> method to
find which part of the offset corresponds to the <code>question</code> and which corresponds to the <code>context</code>.</li>`,as,D,tt="Here is how you can create a function to truncate and map the start and end tokens of the <code>answer</code> to the <code>context</code>:",ls,P,ns,K,at='To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> function. You can speed up the <code>map</code> function by setting <code>batched=True</code> to process multiple elements of the dataset at once. Remove any columns you donâ€™t need:',ps,O,is,ee,lt='Now create a batch of examples using <a href="/docs/transformers/v4.56.2/en/main_classes/data_collator#transformers.DefaultDataCollator">DefaultDataCollator</a>. Unlike other data collators in ðŸ¤— Transformers, the <a href="/docs/transformers/v4.56.2/en/main_classes/data_collator#transformers.DefaultDataCollator">DefaultDataCollator</a> does not apply any additional preprocessing such as padding.',os,se,rs,te,Ms,g,ms,ae,nt='Youâ€™re ready to start training your model now! Load DistilBERT with <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModelForQuestionAnswering">AutoModelForQuestionAnswering</a>:',cs,le,hs,ne,pt="At this point, only three steps remain:",us,pe,it='<li>Define your training hyperparameters in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. The only required parameter is <code>output_dir</code> which specifies where to save your model. Youâ€™ll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model).</li> <li>Pass the training arguments to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, and data collator.</li> <li>Call <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.train">train()</a> to finetune your model.</li>',ds,ie,ws,oe,ot='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so everyone can use your model:',js,re,ys,b,fs,Me,Ts,me,rt='Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> still calculates the evaluation loss during training so youâ€™re not completely in the dark about your modelâ€™s performance.',Js,ce,Mt='If you have more time and youâ€™re interested in how to evaluate your model for question answering, take a look at the <a href="https://huggingface.co/course/chapter7/7?fw=pt#post-processing" rel="nofollow">Question answering</a> chapter from the ðŸ¤— Hugging Face Course!',gs,he,bs,ue,mt="Great, now that youâ€™ve finetuned a model, you can use it for inference!",Us,de,ct="Come up with a question and some context youâ€™d like the model to predict:",Is,we,Cs,je,ht='The simplest way to try out your finetuned model for inference is to use it in a <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>. Instantiate a <code>pipeline</code> for question answering with your model, and pass your text to it:',$s,ye,_s,fe,ut="You can also manually replicate the results of the <code>pipeline</code> if youâ€™d like:",Zs,Te,dt="Tokenize the text and return PyTorch tensors:",xs,Je,ks,ge,wt="Pass your inputs to the model and return the <code>logits</code>:",As,be,vs,Ue,jt="Get the highest probability from the model output for the start and end positions:",Bs,Ie,Xs,Ce,yt="Decode the predicted tokens to get the answer:",Ws,$e,Gs,_e,Rs,xe,Ns;return U=new Ze({props:{title:"Question answering",local:"question-answering",headingTag:"h1"}}),I=new kt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/question_answering.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/question_answering.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/question_answering.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/question_answering.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/question_answering.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/question_answering.ipynb"}]}}),C=new Ut({props:{id:"ajPx5LwJD-I"}}),J=new Es({props:{$$slots:{default:[vt]},$$scope:{ctx:T}}}),A=new j({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGU=",highlighted:"pip install transformers datasets evaluate",wrap:!1}}),B=new j({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),X=new Ze({props:{title:"Load SQuAD dataset",local:"load-squad-dataset",headingTag:"h2"}}),G=new j({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBc3F1YWQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyc3F1YWQlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTVCJTNBNTAwMCU1RCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>squad = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`,wrap:!1}}),N=new j({props:{code:"c3F1YWQlMjAlM0QlMjBzcXVhZC50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>squad = squad.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),Q=new j({props:{code:"c3F1YWQlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>squad[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;answer_start&#x27;</span>: [<span class="hljs-number">515</span>], <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;Saint Bernadette Soubirous&#x27;</span>]},
 <span class="hljs-string">&#x27;context&#x27;</span>: <span class="hljs-string">&#x27;Architecturally, the school has a Catholic character. Atop the Main Building\\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;</span>,
 <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;5733be284776f41900661182&#x27;</span>,
 <span class="hljs-string">&#x27;question&#x27;</span>: <span class="hljs-string">&#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;University_of_Notre_Dame&#x27;</span>
}`,wrap:!1}}),V=new Ze({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),z=new Ut({props:{id:"qgaM0weJHpA"}}),Y=new j({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),P=new j({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    questions = [q.strip() <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;question&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = tokenizer(
<span class="hljs-meta">... </span>        questions,
<span class="hljs-meta">... </span>        examples[<span class="hljs-string">&quot;context&quot;</span>],
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">384</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
<span class="hljs-meta">... </span>        return_offsets_mapping=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-string">&quot;max_length&quot;</span>,
<span class="hljs-meta">... </span>    )

<span class="hljs-meta">... </span>    offset_mapping = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)
<span class="hljs-meta">... </span>    answers = examples[<span class="hljs-string">&quot;answers&quot;</span>]
<span class="hljs-meta">... </span>    start_positions = []
<span class="hljs-meta">... </span>    end_positions = []

<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(offset_mapping):
<span class="hljs-meta">... </span>        answer = answers[i]
<span class="hljs-meta">... </span>        start_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        end_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>] + <span class="hljs-built_in">len</span>(answer[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
<span class="hljs-meta">... </span>        sequence_ids = inputs.sequence_ids(i)

<span class="hljs-meta">... </span>        <span class="hljs-comment"># Find the start and end of the context</span>
<span class="hljs-meta">... </span>        idx = <span class="hljs-number">0</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">while</span> sequence_ids[idx] != <span class="hljs-number">1</span>:
<span class="hljs-meta">... </span>            idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>        context_start = idx
<span class="hljs-meta">... </span>        <span class="hljs-keyword">while</span> sequence_ids[idx] == <span class="hljs-number">1</span>:
<span class="hljs-meta">... </span>            idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>        context_end = idx - <span class="hljs-number">1</span>

<span class="hljs-meta">... </span>        <span class="hljs-comment"># If the answer is not fully inside the context, label it (0, 0)</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> offset[context_start][<span class="hljs-number">0</span>] &gt; end_char <span class="hljs-keyword">or</span> offset[context_end][<span class="hljs-number">1</span>] &lt; start_char:
<span class="hljs-meta">... </span>            start_positions.append(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>            end_positions.append(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>        <span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>            <span class="hljs-comment"># Otherwise it&#x27;s the start and end token positions</span>
<span class="hljs-meta">... </span>            idx = context_start
<span class="hljs-meta">... </span>            <span class="hljs-keyword">while</span> idx &lt;= context_end <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">0</span>] &lt;= start_char:
<span class="hljs-meta">... </span>                idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>            start_positions.append(idx - <span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>            idx = context_end
<span class="hljs-meta">... </span>            <span class="hljs-keyword">while</span> idx &gt;= context_start <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">1</span>] &gt;= end_char:
<span class="hljs-meta">... </span>                idx -= <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>            end_positions.append(idx + <span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;start_positions&quot;</span>] = start_positions
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;end_positions&quot;</span>] = end_positions
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`,wrap:!1}}),O=new j({props:{code:"dG9rZW5pemVkX3NxdWFkJTIwJTNEJTIwc3F1YWQubWFwKHByZXByb2Nlc3NfZnVuY3Rpb24lMkMlMjBiYXRjaGVkJTNEVHJ1ZSUyQyUyMHJlbW92ZV9jb2x1bW5zJTNEc3F1YWQlNUIlMjJ0cmFpbiUyMiU1RC5jb2x1bW5fbmFtZXMp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_squad = squad.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>, remove_columns=squad[<span class="hljs-string">&quot;train&quot;</span>].column_names)',wrap:!1}}),se=new j({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlZmF1bHREYXRhQ29sbGF0b3IlMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGVmYXVsdERhdGFDb2xsYXRvcigp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator()`,wrap:!1}}),te=new Ze({props:{title:"Train",local:"train",headingTag:"h2"}}),g=new Es({props:{$$slots:{default:[Bt]},$$scope:{ctx:T}}}),le=new j({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTJDJTIwVHJhaW5pbmdBcmd1bWVudHMlMkMlMjBUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForQuestionAnswering, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),ie=new j({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX3FhX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMyUyQyUwQSUyMCUyMCUyMCUyMHdlaWdodF9kZWNheSUzRDAuMDElMkMlMEElMjAlMjAlMjAlMjBwdXNoX3RvX2h1YiUzRFRydWUlMkMlMEEpJTBBJTBBdHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHRva2VuaXplZF9zcXVhZCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEdG9rZW5pemVkX3NxdWFkJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHByb2Nlc3NpbmdfY2xhc3MlM0R0b2tlbml6ZXIlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_qa_model&quot;</span>,
<span class="hljs-meta">... </span>    eval_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_squad[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    processing_class=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),re=new j({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),b=new Es({props:{$$slots:{default:[Xt]},$$scope:{ctx:T}}}),Me=new Ze({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),he=new Ze({props:{title:"Inference",local:"inference",headingTag:"h2"}}),we=new j({props:{code:"cXVlc3Rpb24lMjAlM0QlMjAlMjJIb3clMjBtYW55JTIwcHJvZ3JhbW1pbmclMjBsYW5ndWFnZXMlMjBkb2VzJTIwQkxPT00lMjBzdXBwb3J0JTNGJTIyJTBBY29udGV4dCUyMCUzRCUyMCUyMkJMT09NJTIwaGFzJTIwMTc2JTIwYmlsbGlvbiUyMHBhcmFtZXRlcnMlMjBhbmQlMjBjYW4lMjBnZW5lcmF0ZSUyMHRleHQlMjBpbiUyMDQ2JTIwbGFuZ3VhZ2VzJTIwbmF0dXJhbCUyMGxhbmd1YWdlcyUyMGFuZCUyMDEzJTIwcHJvZ3JhbW1pbmclMjBsYW5ndWFnZXMuJTIy",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;How many programming languages does BLOOM support?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = <span class="hljs-string">&quot;BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.&quot;</span>`,wrap:!1}}),ye=new j({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBcXVlc3Rpb25fYW5zd2VyZXIlMjAlM0QlMjBwaXBlbGluZSglMjJxdWVzdGlvbi1hbnN3ZXJpbmclMjIlMkMlMjBtb2RlbCUzRCUyMm15X2F3ZXNvbWVfcWFfbW9kZWwlMjIpJTBBcXVlc3Rpb25fYW5zd2VyZXIocXVlc3Rpb24lM0RxdWVzdGlvbiUyQyUyMGNvbnRleHQlM0Rjb250ZXh0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=<span class="hljs-string">&quot;my_awesome_qa_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>question_answerer(question=question, context=context)
{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.2058267742395401</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">10</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">95</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;176 billion parameters and can generate text in 46 languages natural languages and 13&#x27;</span>}`,wrap:!1}}),Je=new j({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX3FhX21vZGVsJTIyKSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihxdWVzdGlvbiUyQyUyMGNvbnRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;my_awesome_qa_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),be=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX3FhX21vZGVsJTIyKSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;my_awesome_qa_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)`,wrap:!1}}),Ie=new j({props:{code:"YW5zd2VyX3N0YXJ0X2luZGV4JTIwJTNEJTIwb3V0cHV0cy5zdGFydF9sb2dpdHMuYXJnbWF4KCklMEFhbnN3ZXJfZW5kX2luZGV4JTIwJTNEJTIwb3V0cHV0cy5lbmRfbG9naXRzLmFyZ21heCgp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()`,wrap:!1}}),$e=new j({props:{code:"cHJlZGljdF9hbnN3ZXJfdG9rZW5zJTIwJTNEJTIwaW5wdXRzLmlucHV0X2lkcyU1QjAlMkMlMjBhbnN3ZXJfc3RhcnRfaW5kZXglMjAlM0ElMjBhbnN3ZXJfZW5kX2luZGV4JTIwJTJCJTIwMSU1RCUwQXRva2VuaXplci5kZWNvZGUocHJlZGljdF9hbnN3ZXJfdG9rZW5zKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
<span class="hljs-string">&#x27;176 billion parameters and can generate text in 46 languages natural languages and 13&#x27;</span>`,wrap:!1}}),_e=new At({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md"}}),{c(){r=p("meta"),y=l(),w=p("p"),f=l(),M(U.$$.fragment),ke=l(),M(I.$$.fragment),Ae=l(),M(C.$$.fragment),ve=l(),$=p("p"),$.textContent=Hs,Be=l(),_=p("ul"),_.innerHTML=qs,Xe=l(),Z=p("p"),Z.textContent=Vs,We=l(),x=p("ol"),x.innerHTML=zs,Ge=l(),M(J.$$.fragment),Re=l(),k=p("p"),k.textContent=Fs,Ne=l(),M(A.$$.fragment),Ee=l(),v=p("p"),v.textContent=Ys,Qe=l(),M(B.$$.fragment),He=l(),M(X.$$.fragment),qe=l(),W=p("p"),W.textContent=Ss,Ve=l(),M(G.$$.fragment),ze=l(),R=p("p"),R.innerHTML=Ls,Fe=l(),M(N.$$.fragment),Ye=l(),E=p("p"),E.textContent=Ds,Se=l(),M(Q.$$.fragment),Le=l(),H=p("p"),H.textContent=Ps,De=l(),q=p("ul"),q.innerHTML=Ks,Pe=l(),M(V.$$.fragment),Ke=l(),M(z.$$.fragment),Oe=l(),F=p("p"),F.innerHTML=Os,es=l(),M(Y.$$.fragment),ss=l(),S=p("p"),S.textContent=et,ts=l(),L=p("ol"),L.innerHTML=st,as=l(),D=p("p"),D.innerHTML=tt,ls=l(),M(P.$$.fragment),ns=l(),K=p("p"),K.innerHTML=at,ps=l(),M(O.$$.fragment),is=l(),ee=p("p"),ee.innerHTML=lt,os=l(),M(se.$$.fragment),rs=l(),M(te.$$.fragment),Ms=l(),M(g.$$.fragment),ms=l(),ae=p("p"),ae.innerHTML=nt,cs=l(),M(le.$$.fragment),hs=l(),ne=p("p"),ne.textContent=pt,us=l(),pe=p("ol"),pe.innerHTML=it,ds=l(),M(ie.$$.fragment),ws=l(),oe=p("p"),oe.innerHTML=ot,js=l(),M(re.$$.fragment),ys=l(),M(b.$$.fragment),fs=l(),M(Me.$$.fragment),Ts=l(),me=p("p"),me.innerHTML=rt,Js=l(),ce=p("p"),ce.innerHTML=Mt,gs=l(),M(he.$$.fragment),bs=l(),ue=p("p"),ue.textContent=mt,Us=l(),de=p("p"),de.textContent=ct,Is=l(),M(we.$$.fragment),Cs=l(),je=p("p"),je.innerHTML=ht,$s=l(),M(ye.$$.fragment),_s=l(),fe=p("p"),fe.innerHTML=ut,Zs=l(),Te=p("p"),Te.textContent=dt,xs=l(),M(Je.$$.fragment),ks=l(),ge=p("p"),ge.innerHTML=wt,As=l(),M(be.$$.fragment),vs=l(),Ue=p("p"),Ue.textContent=jt,Bs=l(),M(Ie.$$.fragment),Xs=l(),Ce=p("p"),Ce.textContent=yt,Ws=l(),M($e.$$.fragment),Gs=l(),M(_e.$$.fragment),Rs=l(),xe=p("p"),this.h()},l(e){const s=Zt("svelte-u9bgzb",document.head);r=i(s,"META",{name:!0,content:!0}),s.forEach(t),y=n(e),w=i(e,"P",{}),gt(w).forEach(t),f=n(e),m(U.$$.fragment,e),ke=n(e),m(I.$$.fragment,e),Ae=n(e),m(C.$$.fragment,e),ve=n(e),$=i(e,"P",{"data-svelte-h":!0}),o($)!=="svelte-wddg3q"&&($.textContent=Hs),Be=n(e),_=i(e,"UL",{"data-svelte-h":!0}),o(_)!=="svelte-juxaac"&&(_.innerHTML=qs),Xe=n(e),Z=i(e,"P",{"data-svelte-h":!0}),o(Z)!=="svelte-1aff4p7"&&(Z.textContent=Vs),We=n(e),x=i(e,"OL",{"data-svelte-h":!0}),o(x)!=="svelte-1par97w"&&(x.innerHTML=zs),Ge=n(e),m(J.$$.fragment,e),Re=n(e),k=i(e,"P",{"data-svelte-h":!0}),o(k)!=="svelte-1c9nexd"&&(k.textContent=Fs),Ne=n(e),m(A.$$.fragment,e),Ee=n(e),v=i(e,"P",{"data-svelte-h":!0}),o(v)!=="svelte-k76o1m"&&(v.textContent=Ys),Qe=n(e),m(B.$$.fragment,e),He=n(e),m(X.$$.fragment,e),qe=n(e),W=i(e,"P",{"data-svelte-h":!0}),o(W)!=="svelte-86lsp1"&&(W.textContent=Ss),Ve=n(e),m(G.$$.fragment,e),ze=n(e),R=i(e,"P",{"data-svelte-h":!0}),o(R)!=="svelte-1imurcc"&&(R.innerHTML=Ls),Fe=n(e),m(N.$$.fragment,e),Ye=n(e),E=i(e,"P",{"data-svelte-h":!0}),o(E)!=="svelte-1m91ua0"&&(E.textContent=Ds),Se=n(e),m(Q.$$.fragment,e),Le=n(e),H=i(e,"P",{"data-svelte-h":!0}),o(H)!=="svelte-ixla8d"&&(H.textContent=Ps),De=n(e),q=i(e,"UL",{"data-svelte-h":!0}),o(q)!=="svelte-13sugm7"&&(q.innerHTML=Ks),Pe=n(e),m(V.$$.fragment,e),Ke=n(e),m(z.$$.fragment,e),Oe=n(e),F=i(e,"P",{"data-svelte-h":!0}),o(F)!=="svelte-1l87utj"&&(F.innerHTML=Os),es=n(e),m(Y.$$.fragment,e),ss=n(e),S=i(e,"P",{"data-svelte-h":!0}),o(S)!=="svelte-6fi1jw"&&(S.textContent=et),ts=n(e),L=i(e,"OL",{"data-svelte-h":!0}),o(L)!=="svelte-1pbu9ad"&&(L.innerHTML=st),as=n(e),D=i(e,"P",{"data-svelte-h":!0}),o(D)!=="svelte-1l18kqg"&&(D.innerHTML=tt),ls=n(e),m(P.$$.fragment,e),ns=n(e),K=i(e,"P",{"data-svelte-h":!0}),o(K)!=="svelte-8pdvdb"&&(K.innerHTML=at),ps=n(e),m(O.$$.fragment,e),is=n(e),ee=i(e,"P",{"data-svelte-h":!0}),o(ee)!=="svelte-1use00w"&&(ee.innerHTML=lt),os=n(e),m(se.$$.fragment,e),rs=n(e),m(te.$$.fragment,e),Ms=n(e),m(g.$$.fragment,e),ms=n(e),ae=i(e,"P",{"data-svelte-h":!0}),o(ae)!=="svelte-l3lpse"&&(ae.innerHTML=nt),cs=n(e),m(le.$$.fragment,e),hs=n(e),ne=i(e,"P",{"data-svelte-h":!0}),o(ne)!=="svelte-l42k0i"&&(ne.textContent=pt),us=n(e),pe=i(e,"OL",{"data-svelte-h":!0}),o(pe)!=="svelte-um6a06"&&(pe.innerHTML=it),ds=n(e),m(ie.$$.fragment,e),ws=n(e),oe=i(e,"P",{"data-svelte-h":!0}),o(oe)!=="svelte-yclg6q"&&(oe.innerHTML=ot),js=n(e),m(re.$$.fragment,e),ys=n(e),m(b.$$.fragment,e),fs=n(e),m(Me.$$.fragment,e),Ts=n(e),me=i(e,"P",{"data-svelte-h":!0}),o(me)!=="svelte-11gqg57"&&(me.innerHTML=rt),Js=n(e),ce=i(e,"P",{"data-svelte-h":!0}),o(ce)!=="svelte-14u5qx8"&&(ce.innerHTML=Mt),gs=n(e),m(he.$$.fragment,e),bs=n(e),ue=i(e,"P",{"data-svelte-h":!0}),o(ue)!=="svelte-633ppb"&&(ue.textContent=mt),Us=n(e),de=i(e,"P",{"data-svelte-h":!0}),o(de)!=="svelte-1wy7p4p"&&(de.textContent=ct),Is=n(e),m(we.$$.fragment,e),Cs=n(e),je=i(e,"P",{"data-svelte-h":!0}),o(je)!=="svelte-ehcabs"&&(je.innerHTML=ht),$s=n(e),m(ye.$$.fragment,e),_s=n(e),fe=i(e,"P",{"data-svelte-h":!0}),o(fe)!=="svelte-1njl8vm"&&(fe.innerHTML=ut),Zs=n(e),Te=i(e,"P",{"data-svelte-h":!0}),o(Te)!=="svelte-1qcz1wr"&&(Te.textContent=dt),xs=n(e),m(Je.$$.fragment,e),ks=n(e),ge=i(e,"P",{"data-svelte-h":!0}),o(ge)!=="svelte-f3g043"&&(ge.innerHTML=wt),As=n(e),m(be.$$.fragment,e),vs=n(e),Ue=i(e,"P",{"data-svelte-h":!0}),o(Ue)!=="svelte-v8itmt"&&(Ue.textContent=jt),Bs=n(e),m(Ie.$$.fragment,e),Xs=n(e),Ce=i(e,"P",{"data-svelte-h":!0}),o(Ce)!=="svelte-66bsyj"&&(Ce.textContent=yt),Ws=n(e),m($e.$$.fragment,e),Gs=n(e),m(_e.$$.fragment,e),Rs=n(e),xe=i(e,"P",{}),gt(xe).forEach(t),this.h()},h(){bt(r,"name","hf:doc:metadata"),bt(r,"content",Gt)},m(e,s){xt(document.head,r),a(e,y,s),a(e,w,s),a(e,f,s),c(U,e,s),a(e,ke,s),c(I,e,s),a(e,Ae,s),c(C,e,s),a(e,ve,s),a(e,$,s),a(e,Be,s),a(e,_,s),a(e,Xe,s),a(e,Z,s),a(e,We,s),a(e,x,s),a(e,Ge,s),c(J,e,s),a(e,Re,s),a(e,k,s),a(e,Ne,s),c(A,e,s),a(e,Ee,s),a(e,v,s),a(e,Qe,s),c(B,e,s),a(e,He,s),c(X,e,s),a(e,qe,s),a(e,W,s),a(e,Ve,s),c(G,e,s),a(e,ze,s),a(e,R,s),a(e,Fe,s),c(N,e,s),a(e,Ye,s),a(e,E,s),a(e,Se,s),c(Q,e,s),a(e,Le,s),a(e,H,s),a(e,De,s),a(e,q,s),a(e,Pe,s),c(V,e,s),a(e,Ke,s),c(z,e,s),a(e,Oe,s),a(e,F,s),a(e,es,s),c(Y,e,s),a(e,ss,s),a(e,S,s),a(e,ts,s),a(e,L,s),a(e,as,s),a(e,D,s),a(e,ls,s),c(P,e,s),a(e,ns,s),a(e,K,s),a(e,ps,s),c(O,e,s),a(e,is,s),a(e,ee,s),a(e,os,s),c(se,e,s),a(e,rs,s),c(te,e,s),a(e,Ms,s),c(g,e,s),a(e,ms,s),a(e,ae,s),a(e,cs,s),c(le,e,s),a(e,hs,s),a(e,ne,s),a(e,us,s),a(e,pe,s),a(e,ds,s),c(ie,e,s),a(e,ws,s),a(e,oe,s),a(e,js,s),c(re,e,s),a(e,ys,s),c(b,e,s),a(e,fs,s),c(Me,e,s),a(e,Ts,s),a(e,me,s),a(e,Js,s),a(e,ce,s),a(e,gs,s),c(he,e,s),a(e,bs,s),a(e,ue,s),a(e,Us,s),a(e,de,s),a(e,Is,s),c(we,e,s),a(e,Cs,s),a(e,je,s),a(e,$s,s),c(ye,e,s),a(e,_s,s),a(e,fe,s),a(e,Zs,s),a(e,Te,s),a(e,xs,s),c(Je,e,s),a(e,ks,s),a(e,ge,s),a(e,As,s),c(be,e,s),a(e,vs,s),a(e,Ue,s),a(e,Bs,s),c(Ie,e,s),a(e,Xs,s),a(e,Ce,s),a(e,Ws,s),c($e,e,s),a(e,Gs,s),c(_e,e,s),a(e,Rs,s),a(e,xe,s),Ns=!0},p(e,[s]){const ft={};s&2&&(ft.$$scope={dirty:s,ctx:e}),J.$set(ft);const Tt={};s&2&&(Tt.$$scope={dirty:s,ctx:e}),g.$set(Tt);const Jt={};s&2&&(Jt.$$scope={dirty:s,ctx:e}),b.$set(Jt)},i(e){Ns||(h(U.$$.fragment,e),h(I.$$.fragment,e),h(C.$$.fragment,e),h(J.$$.fragment,e),h(A.$$.fragment,e),h(B.$$.fragment,e),h(X.$$.fragment,e),h(G.$$.fragment,e),h(N.$$.fragment,e),h(Q.$$.fragment,e),h(V.$$.fragment,e),h(z.$$.fragment,e),h(Y.$$.fragment,e),h(P.$$.fragment,e),h(O.$$.fragment,e),h(se.$$.fragment,e),h(te.$$.fragment,e),h(g.$$.fragment,e),h(le.$$.fragment,e),h(ie.$$.fragment,e),h(re.$$.fragment,e),h(b.$$.fragment,e),h(Me.$$.fragment,e),h(he.$$.fragment,e),h(we.$$.fragment,e),h(ye.$$.fragment,e),h(Je.$$.fragment,e),h(be.$$.fragment,e),h(Ie.$$.fragment,e),h($e.$$.fragment,e),h(_e.$$.fragment,e),Ns=!0)},o(e){u(U.$$.fragment,e),u(I.$$.fragment,e),u(C.$$.fragment,e),u(J.$$.fragment,e),u(A.$$.fragment,e),u(B.$$.fragment,e),u(X.$$.fragment,e),u(G.$$.fragment,e),u(N.$$.fragment,e),u(Q.$$.fragment,e),u(V.$$.fragment,e),u(z.$$.fragment,e),u(Y.$$.fragment,e),u(P.$$.fragment,e),u(O.$$.fragment,e),u(se.$$.fragment,e),u(te.$$.fragment,e),u(g.$$.fragment,e),u(le.$$.fragment,e),u(ie.$$.fragment,e),u(re.$$.fragment,e),u(b.$$.fragment,e),u(Me.$$.fragment,e),u(he.$$.fragment,e),u(we.$$.fragment,e),u(ye.$$.fragment,e),u(Je.$$.fragment,e),u(be.$$.fragment,e),u(Ie.$$.fragment,e),u($e.$$.fragment,e),u(_e.$$.fragment,e),Ns=!1},d(e){e&&(t(y),t(w),t(f),t(ke),t(Ae),t(ve),t($),t(Be),t(_),t(Xe),t(Z),t(We),t(x),t(Ge),t(Re),t(k),t(Ne),t(Ee),t(v),t(Qe),t(He),t(qe),t(W),t(Ve),t(ze),t(R),t(Fe),t(Ye),t(E),t(Se),t(Le),t(H),t(De),t(q),t(Pe),t(Ke),t(Oe),t(F),t(es),t(ss),t(S),t(ts),t(L),t(as),t(D),t(ls),t(ns),t(K),t(ps),t(is),t(ee),t(os),t(rs),t(Ms),t(ms),t(ae),t(cs),t(hs),t(ne),t(us),t(pe),t(ds),t(ws),t(oe),t(js),t(ys),t(fs),t(Ts),t(me),t(Js),t(ce),t(gs),t(bs),t(ue),t(Us),t(de),t(Is),t(Cs),t(je),t($s),t(_s),t(fe),t(Zs),t(Te),t(xs),t(ks),t(ge),t(As),t(vs),t(Ue),t(Bs),t(Xs),t(Ce),t(Ws),t(Gs),t(Rs),t(xe)),t(r),d(U,e),d(I,e),d(C,e),d(J,e),d(A,e),d(B,e),d(X,e),d(G,e),d(N,e),d(Q,e),d(V,e),d(z,e),d(Y,e),d(P,e),d(O,e),d(se,e),d(te,e),d(g,e),d(le,e),d(ie,e),d(re,e),d(b,e),d(Me,e),d(he,e),d(we,e),d(ye,e),d(Je,e),d(be,e),d(Ie,e),d($e,e),d(_e,e)}}}const Gt='{"title":"Question answering","local":"question-answering","sections":[{"title":"Load SQuAD dataset","local":"load-squad-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Rt(T){return Ct(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ft extends $t{constructor(r){super(),_t(this,r,Rt,Wt,It,{})}}export{Ft as component};
