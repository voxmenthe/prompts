import{s as it,o as lt,n as Le}from"../chunks/scheduler.18a86fab.js";import{S as dt,i as mt,g as i,s as r,r as h,A as ct,h as l,f as n,c as a,j as te,x as c,u,k as H,y as d,a as o,v as g,d as _,t as T,w as b}from"../chunks/index.98837b22.js";import{T as rt}from"../chunks/Tip.77304350.js";import{D as pe}from"../chunks/Docstring.a1ef7999.js";import{C as at}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as pt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as fe,E as ft}from"../chunks/getInferenceSnippets.06c2775f.js";function ht(U){let s,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=i("p"),s.innerHTML=w},l(m){s=l(m,"P",{"data-svelte-h":!0}),c(s)!=="svelte-fincs2"&&(s.innerHTML=w)},m(m,f){o(m,s,f)},p:Le,d(m){m&&n(s)}}}function ut(U){let s,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=i("p"),s.innerHTML=w},l(m){s=l(m,"P",{"data-svelte-h":!0}),c(s)!=="svelte-fincs2"&&(s.innerHTML=w)},m(m,f){o(m,s,f)},p:Le,d(m){m&&n(s)}}}function gt(U){let s,w="Example:",m,f,y;return f=new at({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRpbWVzRm1Nb2RlbEZvclByZWRpY3Rpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRpbWVzRm1Nb2RlbEZvclByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnRpbWVzZm0tMi4wLTUwMG0tcHl0b3JjaCUyMiklMEElMEFmb3JlY2FzdF9pbnB1dCUyMCUzRCUyMCU1QnRvcmNoLmxpbnNwYWNlKDAlMkMlMjAyMCUyQyUyMDEwMCkuc2luKCklMkMlMjB0b3JjaC5saW5zcGFjZSgwJTJDJTIwMjAlMkMlMjAyMDApLnNpbigpJTJDJTIwdG9yY2gubGluc3BhY2UoMCUyQyUyMDIwJTJDJTIwNDAwKS5zaW4oKSU1RCUwQWZyZXF1ZW5jeV9pbnB1dCUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIwJTJDJTIwMSUyQyUyMDIlNUQlMkMlMjBkdHlwZSUzRHRvcmNoLmxvbmcpJTBBJTBBJTIzJTIwR2VuZXJhdGUlMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBhc3RfdmFsdWVzJTNEZm9yZWNhc3RfaW5wdXQlMkMlMjBmcmVxJTNEZnJlcXVlbmN5X2lucHV0JTJDJTIwcmV0dXJuX2RpY3QlM0RUcnVlKSUwQSUyMCUyMCUyMCUyMHBvaW50X2ZvcmVjYXN0X2NvbnYlMjAlM0QlMjBvdXRwdXRzLm1lYW5fcHJlZGljdGlvbnMlMEElMjAlMjAlMjAlMjBxdWFudGlsZV9mb3JlY2FzdF9jb252JTIwJTNEJTIwb3V0cHV0cy5mdWxsX3ByZWRpY3Rpb25z",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TimesFmModelForPrediction

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TimesFmModelForPrediction.from_pretrained(<span class="hljs-string">&quot;google/timesfm-2.0-500m-pytorch&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>forecast_input = [torch.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">100</span>).sin(), torch.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">200</span>).sin(), torch.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">400</span>).sin()]
<span class="hljs-meta">&gt;&gt;&gt; </span>frequency_input = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=torch.long)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">&gt;&gt;&gt; </span>    outputs = model(past_values=forecast_input, freq=frequency_input, return_dict=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>    point_forecast_conv = outputs.mean_predictions
<span class="hljs-meta">&gt;&gt;&gt; </span>    quantile_forecast_conv = outputs.full_predictions`,wrap:!1}}),{c(){s=i("p"),s.textContent=w,m=r(),h(f.$$.fragment)},l(p){s=l(p,"P",{"data-svelte-h":!0}),c(s)!=="svelte-11lpom8"&&(s.textContent=w),m=a(p),u(f.$$.fragment,p)},m(p,z){o(p,s,z),o(p,m,z),g(f,p,z),y=!0},p:Le,i(p){y||(_(f.$$.fragment,p),y=!0)},o(p){T(f.$$.fragment,p),y=!1},d(p){p&&(n(s),n(m)),b(f,p)}}}function _t(U){let s,w,m,f,y,p="<em>This model was released on 2023-10-14 and added to Hugging Face Transformers on 2025-04-16.</em>",z,P,he,Z,Se='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ue,W,ge,k,Xe='TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model proposed in <a href="https://huggingface.co/papers/2310.10688" rel="nofollow">A decoder-only foundation model for time-series forecasting</a> by Abhimanyu Das, Weihao Kong, Rajat Sen, and  Yichen Zhou. It is a decoder only model that uses non-overlapping patches of time-series data as input and outputs some output patch length prediction in an autoregressive fashion.',_e,N,Ee="The abstract from the paper is the following:",Te,G,Qe="<em>Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.</em>",be,V,Ae=`This model was contributed by <a href="https://huggingface.co/kashif" rel="nofollow">kashif</a>.
The original code can be found <a href="https://github.com/google-research/timesfm" rel="nofollow">here</a>.`,Me,L,De="To use the model:",we,S,ve,X,ye,J,E,ze,ne,Ye=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModelForPrediction">TimesFmModelForPrediction</a> or a <code>TFTimesFmModel</code>. It is used to
instantiate a TimesFM model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the TimesFM
<a href="https://huggingface.co/google/timesfm-2.0-500m-pytorch" rel="nofollow">google/timesfm-2.0-500m-pytorch</a> architecture.`,Ze,oe,Ke=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Fe,Q,Je,M,A,Ie,se,Oe="The bare Timesfm Model outputting raw hidden-states without any specific head on top.",Re,re,et=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Be,ae,tt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qe,C,D,He,ie,nt='The <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModel">TimesFmModel</a> forward method, overrides the <code>__call__</code> special method.',Pe,I,$e,Y,Ce,$,K,We,le,ot="TimesFM model for quantile and mean prediction.",ke,F,O,Ne,de,st='The <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModelForPrediction">TimesFmModelForPrediction</a> forward method, overrides the <code>__call__</code> special method.',Ge,R,Ve,B,je,ee,xe,ce,Ue;return P=new fe({props:{title:"TimesFM",local:"timesfm",headingTag:"h1"}}),W=new fe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),S=new at({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVGltZXNGbU1vZGVsRm9yUHJlZGljdGlvbiUwQSUwQSUwQW1vZGVsJTIwJTNEJTIwVGltZXNGbU1vZGVsRm9yUHJlZGljdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZ29vZ2xlJTJGdGltZXNmbS0yLjAtNTAwbS1weXRvcmNoJTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJzZHBhJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMEEpJTBBJTBBJTBBJTIwJTIzJTIwQ3JlYXRlJTIwZHVtbXklMjBpbnB1dHMlMEFmb3JlY2FzdF9pbnB1dCUyMCUzRCUyMCU1QiUwQSUyMCUyMCUyMCUyMG5wLnNpbihucC5saW5zcGFjZSgwJTJDJTIwMjAlMkMlMjAxMDApKSUyQyUwQSUyMCUyMCUyMCUyMG5wLnNpbihucC5saW5zcGFjZSgwJTJDJTIwMjAlMkMlMjAyMDApKSUyQyUwQSUyMCUyMCUyMCUyMG5wLnNpbihucC5saW5zcGFjZSgwJTJDJTIwMjAlMkMlMjA0MDApKSUyQyUwQSU1RCUwQWZyZXF1ZW5jeV9pbnB1dCUyMCUzRCUyMCU1QjAlMkMlMjAxJTJDJTIwMiU1RCUwQSUwQSUyMyUyMENvbnZlcnQlMjBpbnB1dHMlMjB0byUyMHNlcXVlbmNlJTIwb2YlMjB0ZW5zb3JzJTBBZm9yZWNhc3RfaW5wdXRfdGVuc29yJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwdG9yY2gudGVuc29yKHRzJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNikudG8obW9kZWwuZGV2aWNlKSUwQSUyMCUyMCUyMCUyMGZvciUyMHRzJTIwaW4lMjBmb3JlY2FzdF9pbnB1dCUwQSU1RCUwQWZyZXF1ZW5jeV9pbnB1dF90ZW5zb3IlMjAlM0QlMjB0b3JjaC50ZW5zb3IoZnJlcXVlbmN5X2lucHV0JTJDJTIwZHR5cGUlM0R0b3JjaC5sb25nKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBJTIzJTIwR2V0JTIwcHJlZGljdGlvbnMlMjBmcm9tJTIwdGhlJTIwcHJlLXRyYWluZWQlMjBtb2RlbCUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGFzdF92YWx1ZXMlM0Rmb3JlY2FzdF9pbnB1dF90ZW5zb3IlMkMlMjBmcmVxJTNEZnJlcXVlbmN5X2lucHV0X3RlbnNvciUyQyUyMHJldHVybl9kaWN0JTNEVHJ1ZSklMEElMjAlMjAlMjAlMjBwb2ludF9mb3JlY2FzdF9jb252JTIwJTNEJTIwb3V0cHV0cy5tZWFuX3ByZWRpY3Rpb25zLmZsb2F0KCkuY3B1KCkubnVtcHkoKSUwQSUyMCUyMCUyMCUyMHF1YW50aWxlX2ZvcmVjYXN0X2NvbnYlMjAlM0QlMjBvdXRwdXRzLmZ1bGxfcHJlZGljdGlvbnMuZmxvYXQoKS5jcHUoKS5udW1weSgp",highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TimesFmModelForPrediction


model = TimesFmModelForPrediction.from_pretrained(
    <span class="hljs-string">&quot;google/timesfm-2.0-500m-pytorch&quot;</span>,
    dtype=torch.bfloat16,
    attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)


 <span class="hljs-comment"># Create dummy inputs</span>
forecast_input = [
    np.sin(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">100</span>)),
    np.sin(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">200</span>)),
    np.sin(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">20</span>, <span class="hljs-number">400</span>)),
]
frequency_input = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]

<span class="hljs-comment"># Convert inputs to sequence of tensors</span>
forecast_input_tensor = [
    torch.tensor(ts, dtype=torch.bfloat16).to(model.device)
    <span class="hljs-keyword">for</span> ts <span class="hljs-keyword">in</span> forecast_input
]
frequency_input_tensor = torch.tensor(frequency_input, dtype=torch.long).to(model.device)

<span class="hljs-comment"># Get predictions from the pre-trained model</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(past_values=forecast_input_tensor, freq=frequency_input_tensor, return_dict=<span class="hljs-literal">True</span>)
    point_forecast_conv = outputs.mean_predictions.<span class="hljs-built_in">float</span>().cpu().numpy()
    quantile_forecast_conv = outputs.full_predictions.<span class="hljs-built_in">float</span>().cpu().numpy()`,wrap:!1}}),X=new fe({props:{title:"TimesFmConfig",local:"transformers.TimesFmConfig",headingTag:"h2"}}),E=new pe({props:{name:"class transformers.TimesFmConfig",anchor:"transformers.TimesFmConfig",parameters:[{name:"patch_length",val:": int = 32"},{name:"context_length",val:": int = 512"},{name:"horizon_length",val:": int = 128"},{name:"freq_size",val:": int = 3"},{name:"num_hidden_layers",val:": int = 50"},{name:"hidden_size",val:": int = 1280"},{name:"intermediate_size",val:": int = 1280"},{name:"head_dim",val:": int = 80"},{name:"num_attention_heads",val:": int = 16"},{name:"tolerance",val:": float = 1e-06"},{name:"rms_norm_eps",val:": float = 1e-06"},{name:"quantiles",val:": list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"},{name:"pad_val",val:": float = 1123581321.0"},{name:"attention_dropout",val:": float = 0.0"},{name:"use_positional_embedding",val:": bool = False"},{name:"initializer_range",val:": float = 0.02"},{name:"min_timescale",val:": int = 1"},{name:"max_timescale",val:": int = 10000"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TimesFmConfig.patch_length",description:`<strong>patch_length</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The length of one patch in the input sequence.`,name:"patch_length"},{anchor:"transformers.TimesFmConfig.context_length",description:`<strong>context_length</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The length of the input context.`,name:"context_length"},{anchor:"transformers.TimesFmConfig.horizon_length",description:`<strong>horizon_length</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The length of the prediction horizon.`,name:"horizon_length"},{anchor:"transformers.TimesFmConfig.freq_size",description:`<strong>freq_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of frequency embeddings.`,name:"freq_size"},{anchor:"transformers.TimesFmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
Number of Transformer layers.`,name:"num_hidden_layers"},{anchor:"transformers.TimesFmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
Size of the hidden layers in the feed-forward networks.`,name:"hidden_size"},{anchor:"transformers.TimesFmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.TimesFmConfig.head_dim",description:`<strong>head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 80) &#x2014;
Size of the key, query, value projections per attention head. The <code>inner_dim</code> of the projection layer will
be defined as <code>num_attention_heads * head_dim</code>.`,name:"head_dim"},{anchor:"transformers.TimesFmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.TimesFmConfig.tolerance",description:`<strong>tolerance</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The tolerance for the quantile loss.`,name:"tolerance"},{anchor:"transformers.TimesFmConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the RMS normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.TimesFmConfig.quantiles",description:`<strong>quantiles</strong> (<code>list[float]</code>, <em>optional</em>, defaults to <code>[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</code>) &#x2014;
The quantiles to predict.`,name:"quantiles"},{anchor:"transformers.TimesFmConfig.pad_val",description:`<strong>pad_val</strong> (<code>float</code>, <em>optional</em>, defaults to 1123581321.0) &#x2014;
The value used to pad the predictions.`,name:"pad_val"},{anchor:"transformers.TimesFmConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for the attention scores.`,name:"attention_dropout"},{anchor:"transformers.TimesFmConfig.use_positional_embedding",description:`<strong>use_positional_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add positional embeddings.`,name:"use_positional_embedding"},{anchor:"transformers.TimesFmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.TimesFmConfig.min_timescale",description:`<strong>min_timescale</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The start of the geometric positional index. Determines the periodicity of
the added signal.`,name:"min_timescale"},{anchor:"transformers.TimesFmConfig.max_timescale",description:`<strong>max_timescale</strong> (<code>int</code>, <em>optional</em>, defaults to 10000) &#x2014;
The end of the geometric positional index. Determines the frequency of the
added signal.`,name:"max_timescale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timesfm/configuration_timesfm.py#L24"}}),Q=new fe({props:{title:"TimesFmModel",local:"transformers.TimesFmModel",headingTag:"h2"}}),A=new pe({props:{name:"class transformers.TimesFmModel",anchor:"transformers.TimesFmModel",parameters:[{name:"config",val:": TimesFmConfig"}],parametersDescription:[{anchor:"transformers.TimesFmModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig">TimesFmConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timesfm/modeling_timesfm.py#L316"}}),D=new pe({props:{name:"forward",anchor:"transformers.TimesFmModel.forward",parameters:[{name:"past_values",val:": Tensor"},{name:"past_values_padding",val:": LongTensor"},{name:"freq",val:": Tensor"},{name:"output_attentions",val:": bool = False"},{name:"output_hidden_states",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TimesFmModel.forward.past_values",description:`<strong>past_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Past values of the time series that serves as input to the model.`,name:"past_values"},{anchor:"transformers.TimesFmModel.forward.past_values_padding",description:`<strong>past_values_padding</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The padding indicator of the time series.`,name:"past_values_padding"},{anchor:"transformers.TimesFmModel.forward.freq",description:`<strong>freq</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
Frequency indices for the time series data.`,name:"freq"},{anchor:"transformers.TimesFmModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TimesFmModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timesfm/modeling_timesfm.py#L356",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.timesfm.modeling_timesfm.TimesFmOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig"
>TimesFmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>loc</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, )</code>) — The mean of the time series inputs.</p>
</li>
<li>
<p><strong>scale</strong> (<code>torch.Tensor</code> of shape <code>(batch_size,)</code>) — The scale of the time series inputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.timesfm.modeling_timesfm.TimesFmOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),I=new rt({props:{$$slots:{default:[ht]},$$scope:{ctx:U}}}),Y=new fe({props:{title:"TimesFmModelForPrediction",local:"transformers.TimesFmModelForPrediction",headingTag:"h2"}}),K=new pe({props:{name:"class transformers.TimesFmModelForPrediction",anchor:"transformers.TimesFmModelForPrediction",parameters:[{name:"config",val:": TimesFmConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timesfm/modeling_timesfm.py#L579"}}),O=new pe({props:{name:"forward",anchor:"transformers.TimesFmModelForPrediction.forward",parameters:[{name:"past_values",val:": Sequence"},{name:"freq",val:": typing.Optional[collections.abc.Sequence[typing.Union[torch.Tensor, int]]] = None"},{name:"window_size",val:": typing.Optional[int] = None"},{name:"future_values",val:": typing.Optional[torch.Tensor] = None"},{name:"forecast_context_len",val:": typing.Optional[int] = None"},{name:"return_forecast_on_context",val:": bool = False"},{name:"truncate_negative",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.TimesFmModelForPrediction.forward.past_values",description:`<strong>past_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Past values of the time series that serves as input to the model.`,name:"past_values"},{anchor:"transformers.TimesFmModelForPrediction.forward.freq",description:`<strong>freq</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
Frequency indices for the time series data.`,name:"freq"},{anchor:"transformers.TimesFmModelForPrediction.forward.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Window size of trend + residual decomposition. If None then we do not do decomposition.`,name:"window_size"},{anchor:"transformers.TimesFmModelForPrediction.forward.future_values",description:`<strong>future_values</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Optional future time series values to be used for loss computation.`,name:"future_values"},{anchor:"transformers.TimesFmModelForPrediction.forward.forecast_context_len",description:`<strong>forecast_context_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Optional max context length.`,name:"forecast_context_len"},{anchor:"transformers.TimesFmModelForPrediction.forward.return_forecast_on_context",description:`<strong>return_forecast_on_context</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
True to return the forecast on the context when available, i.e. after the first input patch.`,name:"return_forecast_on_context"},{anchor:"transformers.TimesFmModelForPrediction.forward.truncate_negative",description:`<strong>truncate_negative</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Truncate to only non-negative values if any of the contexts have non-negative values,
otherwise do nothing.`,name:"truncate_negative"},{anchor:"transformers.TimesFmModelForPrediction.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to output the attentions.`,name:"output_attentions"},{anchor:"transformers.TimesFmModelForPrediction.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to output the hidden states.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timesfm/modeling_timesfm.py#L667",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.timesfm.modeling_timesfm.TimesFmOutputForPrediction</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig"
>TimesFmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>mean_predictions</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) — The mean predictions of the time series.</p>
</li>
<li>
<p><strong>full_predictions</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) — The full predictions of the time series including the mean and the quantiles.</p>
</li>
<li>
<p><strong>loss</strong> (<code>torch.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>future_values</code> is provided) — The loss of the TimesFM model.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.timesfm.modeling_timesfm.TimesFmOutputForPrediction</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new rt({props:{$$slots:{default:[ut]},$$scope:{ctx:U}}}),B=new pt({props:{anchor:"transformers.TimesFmModelForPrediction.forward.example",$$slots:{default:[gt]},$$scope:{ctx:U}}}),ee=new ft({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/timesfm.md"}}),{c(){s=i("meta"),w=r(),m=i("p"),f=r(),y=i("p"),y.innerHTML=p,z=r(),h(P.$$.fragment),he=r(),Z=i("div"),Z.innerHTML=Se,ue=r(),h(W.$$.fragment),ge=r(),k=i("p"),k.innerHTML=Xe,_e=r(),N=i("p"),N.textContent=Ee,Te=r(),G=i("p"),G.innerHTML=Qe,be=r(),V=i("p"),V.innerHTML=Ae,Me=r(),L=i("p"),L.textContent=De,we=r(),h(S.$$.fragment),ve=r(),h(X.$$.fragment),ye=r(),J=i("div"),h(E.$$.fragment),ze=r(),ne=i("p"),ne.innerHTML=Ye,Ze=r(),oe=i("p"),oe.innerHTML=Ke,Fe=r(),h(Q.$$.fragment),Je=r(),M=i("div"),h(A.$$.fragment),Ie=r(),se=i("p"),se.textContent=Oe,Re=r(),re=i("p"),re.innerHTML=et,Be=r(),ae=i("p"),ae.innerHTML=tt,qe=r(),C=i("div"),h(D.$$.fragment),He=r(),ie=i("p"),ie.innerHTML=nt,Pe=r(),h(I.$$.fragment),$e=r(),h(Y.$$.fragment),Ce=r(),$=i("div"),h(K.$$.fragment),We=r(),le=i("p"),le.textContent=ot,ke=r(),F=i("div"),h(O.$$.fragment),Ne=r(),de=i("p"),de.innerHTML=st,Ge=r(),h(R.$$.fragment),Ve=r(),h(B.$$.fragment),je=r(),h(ee.$$.fragment),xe=r(),ce=i("p"),this.h()},l(e){const t=ct("svelte-u9bgzb",document.head);s=l(t,"META",{name:!0,content:!0}),t.forEach(n),w=a(e),m=l(e,"P",{}),te(m).forEach(n),f=a(e),y=l(e,"P",{"data-svelte-h":!0}),c(y)!=="svelte-vsqkzp"&&(y.innerHTML=p),z=a(e),u(P.$$.fragment,e),he=a(e),Z=l(e,"DIV",{class:!0,"data-svelte-h":!0}),c(Z)!=="svelte-13t8s2t"&&(Z.innerHTML=Se),ue=a(e),u(W.$$.fragment,e),ge=a(e),k=l(e,"P",{"data-svelte-h":!0}),c(k)!=="svelte-1i46uh2"&&(k.innerHTML=Xe),_e=a(e),N=l(e,"P",{"data-svelte-h":!0}),c(N)!=="svelte-vfdo9a"&&(N.textContent=Ee),Te=a(e),G=l(e,"P",{"data-svelte-h":!0}),c(G)!=="svelte-g2ilxp"&&(G.innerHTML=Qe),be=a(e),V=l(e,"P",{"data-svelte-h":!0}),c(V)!=="svelte-1wn4265"&&(V.innerHTML=Ae),Me=a(e),L=l(e,"P",{"data-svelte-h":!0}),c(L)!=="svelte-q2qfla"&&(L.textContent=De),we=a(e),u(S.$$.fragment,e),ve=a(e),u(X.$$.fragment,e),ye=a(e),J=l(e,"DIV",{class:!0});var j=te(J);u(E.$$.fragment,j),ze=a(j),ne=l(j,"P",{"data-svelte-h":!0}),c(ne)!=="svelte-1hr1e0f"&&(ne.innerHTML=Ye),Ze=a(j),oe=l(j,"P",{"data-svelte-h":!0}),c(oe)!=="svelte-1ek1ss9"&&(oe.innerHTML=Ke),j.forEach(n),Fe=a(e),u(Q.$$.fragment,e),Je=a(e),M=l(e,"DIV",{class:!0});var v=te(M);u(A.$$.fragment,v),Ie=a(v),se=l(v,"P",{"data-svelte-h":!0}),c(se)!=="svelte-1cqmr5n"&&(se.textContent=Oe),Re=a(v),re=l(v,"P",{"data-svelte-h":!0}),c(re)!=="svelte-q52n56"&&(re.innerHTML=et),Be=a(v),ae=l(v,"P",{"data-svelte-h":!0}),c(ae)!=="svelte-hswkmf"&&(ae.innerHTML=tt),qe=a(v),C=l(v,"DIV",{class:!0});var x=te(C);u(D.$$.fragment,x),He=a(x),ie=l(x,"P",{"data-svelte-h":!0}),c(ie)!=="svelte-1ah6khm"&&(ie.innerHTML=nt),Pe=a(x),u(I.$$.fragment,x),x.forEach(n),v.forEach(n),$e=a(e),u(Y.$$.fragment,e),Ce=a(e),$=l(e,"DIV",{class:!0});var me=te($);u(K.$$.fragment,me),We=a(me),le=l(me,"P",{"data-svelte-h":!0}),c(le)!=="svelte-1ux4bpd"&&(le.textContent=ot),ke=a(me),F=l(me,"DIV",{class:!0});var q=te(F);u(O.$$.fragment,q),Ne=a(q),de=l(q,"P",{"data-svelte-h":!0}),c(de)!=="svelte-16fcfgy"&&(de.innerHTML=st),Ge=a(q),u(R.$$.fragment,q),Ve=a(q),u(B.$$.fragment,q),q.forEach(n),me.forEach(n),je=a(e),u(ee.$$.fragment,e),xe=a(e),ce=l(e,"P",{}),te(ce).forEach(n),this.h()},h(){H(s,"name","hf:doc:metadata"),H(s,"content",Tt),H(Z,"class","flex flex-wrap space-x-1"),H(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),H(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),H(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),H(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),H($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){d(document.head,s),o(e,w,t),o(e,m,t),o(e,f,t),o(e,y,t),o(e,z,t),g(P,e,t),o(e,he,t),o(e,Z,t),o(e,ue,t),g(W,e,t),o(e,ge,t),o(e,k,t),o(e,_e,t),o(e,N,t),o(e,Te,t),o(e,G,t),o(e,be,t),o(e,V,t),o(e,Me,t),o(e,L,t),o(e,we,t),g(S,e,t),o(e,ve,t),g(X,e,t),o(e,ye,t),o(e,J,t),g(E,J,null),d(J,ze),d(J,ne),d(J,Ze),d(J,oe),o(e,Fe,t),g(Q,e,t),o(e,Je,t),o(e,M,t),g(A,M,null),d(M,Ie),d(M,se),d(M,Re),d(M,re),d(M,Be),d(M,ae),d(M,qe),d(M,C),g(D,C,null),d(C,He),d(C,ie),d(C,Pe),g(I,C,null),o(e,$e,t),g(Y,e,t),o(e,Ce,t),o(e,$,t),g(K,$,null),d($,We),d($,le),d($,ke),d($,F),g(O,F,null),d(F,Ne),d(F,de),d(F,Ge),g(R,F,null),d(F,Ve),g(B,F,null),o(e,je,t),g(ee,e,t),o(e,xe,t),o(e,ce,t),Ue=!0},p(e,[t]){const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),I.$set(j);const v={};t&2&&(v.$$scope={dirty:t,ctx:e}),R.$set(v);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),B.$set(x)},i(e){Ue||(_(P.$$.fragment,e),_(W.$$.fragment,e),_(S.$$.fragment,e),_(X.$$.fragment,e),_(E.$$.fragment,e),_(Q.$$.fragment,e),_(A.$$.fragment,e),_(D.$$.fragment,e),_(I.$$.fragment,e),_(Y.$$.fragment,e),_(K.$$.fragment,e),_(O.$$.fragment,e),_(R.$$.fragment,e),_(B.$$.fragment,e),_(ee.$$.fragment,e),Ue=!0)},o(e){T(P.$$.fragment,e),T(W.$$.fragment,e),T(S.$$.fragment,e),T(X.$$.fragment,e),T(E.$$.fragment,e),T(Q.$$.fragment,e),T(A.$$.fragment,e),T(D.$$.fragment,e),T(I.$$.fragment,e),T(Y.$$.fragment,e),T(K.$$.fragment,e),T(O.$$.fragment,e),T(R.$$.fragment,e),T(B.$$.fragment,e),T(ee.$$.fragment,e),Ue=!1},d(e){e&&(n(w),n(m),n(f),n(y),n(z),n(he),n(Z),n(ue),n(ge),n(k),n(_e),n(N),n(Te),n(G),n(be),n(V),n(Me),n(L),n(we),n(ve),n(ye),n(J),n(Fe),n(Je),n(M),n($e),n(Ce),n($),n(je),n(xe),n(ce)),n(s),b(P,e),b(W,e),b(S,e),b(X,e),b(E),b(Q,e),b(A),b(D),b(I),b(Y,e),b(K),b(O),b(R),b(B),b(ee,e)}}}const Tt='{"title":"TimesFM","local":"timesfm","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"TimesFmConfig","local":"transformers.TimesFmConfig","sections":[],"depth":2},{"title":"TimesFmModel","local":"transformers.TimesFmModel","sections":[],"depth":2},{"title":"TimesFmModelForPrediction","local":"transformers.TimesFmModelForPrediction","sections":[],"depth":2}],"depth":1}';function bt(U){return lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ct extends dt{constructor(s){super(),mt(this,s,bt,_t,it,{})}}export{Ct as component};
