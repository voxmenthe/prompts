import{s as xt,n as yt,o as Pt}from"../chunks/scheduler.18a86fab.js";import{S as wt,i as $t,g as r,s as a,r as v,A as Tt,h as i,f as d,c as o,j as b,u as x,x as l,k as y,y as e,a as P,v as w,d as $,t as T,w as M}from"../chunks/index.98837b22.js";import{D as E}from"../chunks/Docstring.a1ef7999.js";import{H as Mt,E as At}from"../chunks/getInferenceSnippets.06c2775f.js";function Ct(Qe){let A,ue,ge,_e,L,ve,F,Xe='The <a href="/docs/transformers/v4.56.2/en/main_classes/peft#transformers.integrations.PeftAdapterMixin">PeftAdapterMixin</a> provides functions from the <a href="https://huggingface.co/docs/peft/index" rel="nofollow">PEFT</a> library for managing adapters with Transformers. This mixin currently supports LoRA, IA3, and AdaLora. Prefix tuning methods (prompt tuning, prompt learning) aren’t supported because they can’t be injected into a torch module.',be,n,k,we,V,Ze=`A class containing all functions for loading and using adapters weights that are supported in PEFT library. For
more details about adapters and injecting them on a transformer-based model, check out the documentation of PEFT
library: <a href="https://huggingface.co/docs/peft/index" rel="nofollow">https://huggingface.co/docs/peft/index</a>`,$e,j,et=`Currently supported PEFT methods are all non-prefix tuning methods. Below is the list of supported PEFT methods
that anyone can load, train and run with this mixin class:`,Te,G,tt='<li>Low Rank Adapters (LoRA): <a href="https://huggingface.co/docs/peft/conceptual_guides/lora" rel="nofollow">https://huggingface.co/docs/peft/conceptual_guides/lora</a></li> <li>IA3: <a href="https://huggingface.co/docs/peft/conceptual_guides/ia3" rel="nofollow">https://huggingface.co/docs/peft/conceptual_guides/ia3</a></li> <li>AdaLora: <a href="https://huggingface.co/papers/2303.10512" rel="nofollow">https://huggingface.co/papers/2303.10512</a></li>',Me,z,at=`Other PEFT models such as prompt tuning, prompt learning are out of scope as these adapters are not “injectable”
into a torch module. For using these methods, please refer to the usage guide of PEFT library.`,Ae,W,ot="With this mixin, if the correct PEFT version is installed, it is possible to:",Ce,S,nt="<li>Load an adapter stored on a local path or in a remote Hub repository, and inject it in the model</li> <li>Attach new adapters in the model and train them with Trainer or by your own.</li> <li>Attach multiple adapters and iteratively activate / deactivate them</li> <li>Activate / deactivate all adapters from the model.</li> <li>Get the <code>state_dict</code> of the active adapter.</li>",Ee,f,H,Le,B,rt=`Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we
invite you to read more about them on PEFT official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Fe,Y,it="Requires peft as a backend to load the adapter weights.",ke,m,I,He,J,st=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Ie,K,dt=`Adds a fresh new adapter to the current model for training purpose. If no adapter name is passed, a default
name is assigned to the adapter to follow the convention of PEFT library (in PEFT we use “default” as the
default adapter name).`,qe,g,q,De,Q,lt=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Oe,X,pt="Sets a specific adapter by forcing the model to use a that adapter and disable the other adapters.",Ne,h,D,Ue,Z,ct=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Re,ee,ft="Disable all adapters that are attached to the model. This leads to inferring with the base model only.",Ve,u,O,je,te,mt=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Ge,ae,gt="Enable adapters that are attached to the model.",ze,c,N,We,oe,ht=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Se,ne,ut=`Gets the current active adapters of the model. In case of multi-adapter inference (combining multiple adapters
for inference) returns the list of all active adapters so that users can deal with them accordingly.`,Be,re,_t=`For previous PEFT versions (that does not support multi-adapter inference), <code>module.active_adapter</code> will return
a single string.`,Ye,_,U,Je,ie,vt=`If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
official documentation: <a href="https://huggingface.co/docs/peft" rel="nofollow">https://huggingface.co/docs/peft</a>`,Ke,se,bt=`Gets the adapter state dict that should only contain the weights tensors of the specified adapter_name adapter.
If no adapter_name is passed, the active adapter is used.`,xe,R,ye,he,Pe;return L=new Mt({props:{title:"PEFT",local:"transformers.integrations.PeftAdapterMixin",headingTag:"h1"}}),k=new E({props:{name:"class transformers.integrations.PeftAdapterMixin",anchor:"transformers.integrations.PeftAdapterMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L67"}}),H=new E({props:{name:"load_adapter",anchor:"transformers.integrations.PeftAdapterMixin.load_adapter",parameters:[{name:"peft_model_id",val:": typing.Optional[str] = None"},{name:"adapter_name",val:": typing.Optional[str] = None"},{name:"revision",val:": typing.Optional[str] = None"},{name:"token",val:": typing.Optional[str] = None"},{name:"device_map",val:": typing.Optional[str] = 'auto'"},{name:"max_memory",val:": typing.Optional[str] = None"},{name:"offload_folder",val:": typing.Optional[str] = None"},{name:"offload_index",val:": typing.Optional[int] = None"},{name:"peft_config",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"adapter_state_dict",val:": typing.Optional[dict[str, 'torch.Tensor']] = None"},{name:"low_cpu_mem_usage",val:": bool = False"},{name:"is_trainable",val:": bool = False"},{name:"adapter_kwargs",val:": typing.Optional[dict[str, typing.Any]] = None"}],parametersDescription:[{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.peft_model_id",description:`<strong>peft_model_id</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The identifier of the model to look for on the Hub, or a local path to the saved adapter config file
and adapter weights.`,name:"peft_model_id"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.adapter_name",description:`<strong>adapter_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The adapter name to use. If not set, will use the default adapter.`,name:"adapter_name"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>To test a pull request you made on the Hub, you can pass <code>revision=&quot;refs/pr/&lt;pr_number&gt;&quot;</code>.</p>

					</div>`,name:"revision"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.token",description:`<strong>token</strong> (<code>str</code>, <code>optional</code>) &#x2014;
Whether to use authentication token to load the remote folder. Useful to load private repositories
that are on HuggingFace Hub. You might need to call <code>hf auth login</code> and paste your tokens to
cache it.`,name:"token"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.device_map",description:`<strong>device_map</strong> (<code>str</code> or <code>dict[str, Union[int, str, torch.device]]</code> or <code>int</code> or <code>torch.device</code>, <em>optional</em>) &#x2014;
A map that specifies where each submodule should go. It doesn&#x2019;t need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device. If we only pass the device (<em>e.g.</em>, <code>&quot;cpu&quot;</code>, <code>&quot;cuda:1&quot;</code>, <code>&quot;mps&quot;</code>, or a GPU ordinal rank
like <code>1</code>) on which the model will be allocated, the device map will map the entire model to this
device. Passing <code>device_map = 0</code> means put the whole model on GPU 0.</p>
<p>To have Accelerate compute the most optimized <code>device_map</code> automatically, set <code>device_map=&quot;auto&quot;</code>. For
more information about each option see <a href="https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map" rel="nofollow">designing a device
map</a>.`,name:"device_map"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.max_memory",description:`<strong>max_memory</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
A dictionary device identifier to maximum memory. Will default to the maximum memory available for each
GPU and the available CPU RAM if unset.`,name:"max_memory"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.offload_folder",description:`<strong>offload_folder</strong> (<code>str</code> or <code>os.PathLike</code>, <code>optional</code>) &#x2014;
If the <code>device_map</code> contains any value <code>&quot;disk&quot;</code>, the folder where we will offload weights.`,name:"offload_folder"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.offload_index",description:`<strong>offload_index</strong> (<code>int</code>, <code>optional</code>) &#x2014;
<code>offload_index</code> argument to be passed to <code>accelerate.dispatch_model</code> method.`,name:"offload_index"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.peft_config",description:`<strong>peft_config</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts
methods. This argument is used in case users directly pass PEFT state dicts`,name:"peft_config"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.adapter_state_dict",description:`<strong>adapter_state_dict</strong> (<code>dict[str, torch.Tensor]</code>, <em>optional</em>) &#x2014;
The state dict of the adapter to load. This argument is used in case users directly pass PEFT state
dicts`,name:"adapter_state_dict"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.low_cpu_mem_usage",description:`<strong>low_cpu_mem_usage</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.
Requires PEFT version 0.13.0 or higher.`,name:"low_cpu_mem_usage"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.is_trainable",description:`<strong>is_trainable</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the adapter should be trainable or not. If <code>False</code>, the adapter will be frozen and can only be
used for inference.`,name:"is_trainable"},{anchor:"transformers.integrations.PeftAdapterMixin.load_adapter.adapter_kwargs",description:`<strong>adapter_kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional keyword arguments passed along to the <code>from_pretrained</code> method of the adapter config and
<code>find_adapter_config_file</code> method.`,name:"adapter_kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L93"}}),I=new E({props:{name:"add_adapter",anchor:"transformers.integrations.PeftAdapterMixin.add_adapter",parameters:[{name:"adapter_config",val:""},{name:"adapter_name",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.integrations.PeftAdapterMixin.add_adapter.adapter_config",description:`<strong>adapter_config</strong> (<code>~peft.PeftConfig</code>) &#x2014;
The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts
methods`,name:"adapter_config"},{anchor:"transformers.integrations.PeftAdapterMixin.add_adapter.adapter_name",description:`<strong>adapter_name</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;default&quot;</code>) &#x2014;
The name of the adapter to add. If no name is passed, a default name is assigned to the adapter.`,name:"adapter_name"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L314"}}),q=new E({props:{name:"set_adapter",anchor:"transformers.integrations.PeftAdapterMixin.set_adapter",parameters:[{name:"adapter_name",val:": typing.Union[list[str], str]"}],parametersDescription:[{anchor:"transformers.integrations.PeftAdapterMixin.set_adapter.adapter_name",description:`<strong>adapter_name</strong> (<code>Union[list[str], str]</code>) &#x2014;
The name of the adapter to set. Can be also a list of strings to set multiple adapters.`,name:"adapter_name"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L351"}}),D=new E({props:{name:"disable_adapters",anchor:"transformers.integrations.PeftAdapterMixin.disable_adapters",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L396"}}),O=new E({props:{name:"enable_adapters",anchor:"transformers.integrations.PeftAdapterMixin.enable_adapters",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L419"}}),N=new E({props:{name:"active_adapters",anchor:"transformers.integrations.PeftAdapterMixin.active_adapters",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L441"}}),U=new E({props:{name:"get_adapter_state_dict",anchor:"transformers.integrations.PeftAdapterMixin.get_adapter_state_dict",parameters:[{name:"adapter_name",val:": typing.Optional[str] = None"},{name:"state_dict",val:": typing.Optional[dict] = None"}],parametersDescription:[{anchor:"transformers.integrations.PeftAdapterMixin.get_adapter_state_dict.adapter_name",description:`<strong>adapter_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the adapter to get the state dict from. If no name is passed, the active adapter is used.`,name:"adapter_name"},{anchor:"transformers.integrations.PeftAdapterMixin.get_adapter_state_dict.state_dict",description:`<strong>state_dict</strong> (nested dictionary of <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The state dictionary of the model. Will default to <code>self.state_dict()</code>, but can be used if special
precautions need to be taken when recovering the state dictionary of a model (like when using model
parallelism).`,name:"state_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/integrations/peft.py#L480"}}),R=new At({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/peft.md"}}),{c(){A=r("meta"),ue=a(),ge=r("p"),_e=a(),v(L.$$.fragment),ve=a(),F=r("p"),F.innerHTML=Xe,be=a(),n=r("div"),v(k.$$.fragment),we=a(),V=r("p"),V.innerHTML=Ze,$e=a(),j=r("p"),j.textContent=et,Te=a(),G=r("ul"),G.innerHTML=tt,Me=a(),z=r("p"),z.textContent=at,Ae=a(),W=r("p"),W.textContent=ot,Ce=a(),S=r("ul"),S.innerHTML=nt,Ee=a(),f=r("div"),v(H.$$.fragment),Le=a(),B=r("p"),B.innerHTML=rt,Fe=a(),Y=r("p"),Y.textContent=it,ke=a(),m=r("div"),v(I.$$.fragment),He=a(),J=r("p"),J.innerHTML=st,Ie=a(),K=r("p"),K.textContent=dt,qe=a(),g=r("div"),v(q.$$.fragment),De=a(),Q=r("p"),Q.innerHTML=lt,Oe=a(),X=r("p"),X.textContent=pt,Ne=a(),h=r("div"),v(D.$$.fragment),Ue=a(),Z=r("p"),Z.innerHTML=ct,Re=a(),ee=r("p"),ee.textContent=ft,Ve=a(),u=r("div"),v(O.$$.fragment),je=a(),te=r("p"),te.innerHTML=mt,Ge=a(),ae=r("p"),ae.textContent=gt,ze=a(),c=r("div"),v(N.$$.fragment),We=a(),oe=r("p"),oe.innerHTML=ht,Se=a(),ne=r("p"),ne.textContent=ut,Be=a(),re=r("p"),re.innerHTML=_t,Ye=a(),_=r("div"),v(U.$$.fragment),Je=a(),ie=r("p"),ie.innerHTML=vt,Ke=a(),se=r("p"),se.textContent=bt,xe=a(),v(R.$$.fragment),ye=a(),he=r("p"),this.h()},l(t){const p=Tt("svelte-u9bgzb",document.head);A=i(p,"META",{name:!0,content:!0}),p.forEach(d),ue=o(t),ge=i(t,"P",{}),b(ge).forEach(d),_e=o(t),x(L.$$.fragment,t),ve=o(t),F=i(t,"P",{"data-svelte-h":!0}),l(F)!=="svelte-16a4oj8"&&(F.innerHTML=Xe),be=o(t),n=i(t,"DIV",{class:!0});var s=b(n);x(k.$$.fragment,s),we=o(s),V=i(s,"P",{"data-svelte-h":!0}),l(V)!=="svelte-1lavptw"&&(V.innerHTML=Ze),$e=o(s),j=i(s,"P",{"data-svelte-h":!0}),l(j)!=="svelte-2z4u5p"&&(j.textContent=et),Te=o(s),G=i(s,"UL",{"data-svelte-h":!0}),l(G)!=="svelte-ienalu"&&(G.innerHTML=tt),Me=o(s),z=i(s,"P",{"data-svelte-h":!0}),l(z)!=="svelte-1nqkdi3"&&(z.textContent=at),Ae=o(s),W=i(s,"P",{"data-svelte-h":!0}),l(W)!=="svelte-wzrwkl"&&(W.textContent=ot),Ce=o(s),S=i(s,"UL",{"data-svelte-h":!0}),l(S)!=="svelte-4kner1"&&(S.innerHTML=nt),Ee=o(s),f=i(s,"DIV",{class:!0});var de=b(f);x(H.$$.fragment,de),Le=o(de),B=i(de,"P",{"data-svelte-h":!0}),l(B)!=="svelte-k3pc0b"&&(B.innerHTML=rt),Fe=o(de),Y=i(de,"P",{"data-svelte-h":!0}),l(Y)!=="svelte-qhx0qk"&&(Y.textContent=it),de.forEach(d),ke=o(s),m=i(s,"DIV",{class:!0});var le=b(m);x(I.$$.fragment,le),He=o(le),J=i(le,"P",{"data-svelte-h":!0}),l(J)!=="svelte-esrgfk"&&(J.innerHTML=st),Ie=o(le),K=i(le,"P",{"data-svelte-h":!0}),l(K)!=="svelte-11mk5e0"&&(K.textContent=dt),le.forEach(d),qe=o(s),g=i(s,"DIV",{class:!0});var pe=b(g);x(q.$$.fragment,pe),De=o(pe),Q=i(pe,"P",{"data-svelte-h":!0}),l(Q)!=="svelte-esrgfk"&&(Q.innerHTML=lt),Oe=o(pe),X=i(pe,"P",{"data-svelte-h":!0}),l(X)!=="svelte-dcd4bp"&&(X.textContent=pt),pe.forEach(d),Ne=o(s),h=i(s,"DIV",{class:!0});var ce=b(h);x(D.$$.fragment,ce),Ue=o(ce),Z=i(ce,"P",{"data-svelte-h":!0}),l(Z)!=="svelte-esrgfk"&&(Z.innerHTML=ct),Re=o(ce),ee=i(ce,"P",{"data-svelte-h":!0}),l(ee)!=="svelte-1h3d7ho"&&(ee.textContent=ft),ce.forEach(d),Ve=o(s),u=i(s,"DIV",{class:!0});var fe=b(u);x(O.$$.fragment,fe),je=o(fe),te=i(fe,"P",{"data-svelte-h":!0}),l(te)!=="svelte-esrgfk"&&(te.innerHTML=mt),Ge=o(fe),ae=i(fe,"P",{"data-svelte-h":!0}),l(ae)!=="svelte-fvwnbf"&&(ae.textContent=gt),fe.forEach(d),ze=o(s),c=i(s,"DIV",{class:!0});var C=b(c);x(N.$$.fragment,C),We=o(C),oe=i(C,"P",{"data-svelte-h":!0}),l(oe)!=="svelte-esrgfk"&&(oe.innerHTML=ht),Se=o(C),ne=i(C,"P",{"data-svelte-h":!0}),l(ne)!=="svelte-kqotd"&&(ne.textContent=ut),Be=o(C),re=i(C,"P",{"data-svelte-h":!0}),l(re)!=="svelte-13oobru"&&(re.innerHTML=_t),C.forEach(d),Ye=o(s),_=i(s,"DIV",{class:!0});var me=b(_);x(U.$$.fragment,me),Je=o(me),ie=i(me,"P",{"data-svelte-h":!0}),l(ie)!=="svelte-esrgfk"&&(ie.innerHTML=vt),Ke=o(me),se=i(me,"P",{"data-svelte-h":!0}),l(se)!=="svelte-tj66gs"&&(se.textContent=bt),me.forEach(d),s.forEach(d),xe=o(t),x(R.$$.fragment,t),ye=o(t),he=i(t,"P",{}),b(he).forEach(d),this.h()},h(){y(A,"name","hf:doc:metadata"),y(A,"content",Et),y(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(n,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,A),P(t,ue,p),P(t,ge,p),P(t,_e,p),w(L,t,p),P(t,ve,p),P(t,F,p),P(t,be,p),P(t,n,p),w(k,n,null),e(n,we),e(n,V),e(n,$e),e(n,j),e(n,Te),e(n,G),e(n,Me),e(n,z),e(n,Ae),e(n,W),e(n,Ce),e(n,S),e(n,Ee),e(n,f),w(H,f,null),e(f,Le),e(f,B),e(f,Fe),e(f,Y),e(n,ke),e(n,m),w(I,m,null),e(m,He),e(m,J),e(m,Ie),e(m,K),e(n,qe),e(n,g),w(q,g,null),e(g,De),e(g,Q),e(g,Oe),e(g,X),e(n,Ne),e(n,h),w(D,h,null),e(h,Ue),e(h,Z),e(h,Re),e(h,ee),e(n,Ve),e(n,u),w(O,u,null),e(u,je),e(u,te),e(u,Ge),e(u,ae),e(n,ze),e(n,c),w(N,c,null),e(c,We),e(c,oe),e(c,Se),e(c,ne),e(c,Be),e(c,re),e(n,Ye),e(n,_),w(U,_,null),e(_,Je),e(_,ie),e(_,Ke),e(_,se),P(t,xe,p),w(R,t,p),P(t,ye,p),P(t,he,p),Pe=!0},p:yt,i(t){Pe||($(L.$$.fragment,t),$(k.$$.fragment,t),$(H.$$.fragment,t),$(I.$$.fragment,t),$(q.$$.fragment,t),$(D.$$.fragment,t),$(O.$$.fragment,t),$(N.$$.fragment,t),$(U.$$.fragment,t),$(R.$$.fragment,t),Pe=!0)},o(t){T(L.$$.fragment,t),T(k.$$.fragment,t),T(H.$$.fragment,t),T(I.$$.fragment,t),T(q.$$.fragment,t),T(D.$$.fragment,t),T(O.$$.fragment,t),T(N.$$.fragment,t),T(U.$$.fragment,t),T(R.$$.fragment,t),Pe=!1},d(t){t&&(d(ue),d(ge),d(_e),d(ve),d(F),d(be),d(n),d(xe),d(ye),d(he)),d(A),M(L,t),M(k),M(H),M(I),M(q),M(D),M(O),M(N),M(U),M(R,t)}}}const Et='{"title":"PEFT","local":"transformers.integrations.PeftAdapterMixin","sections":[],"depth":1}';function Lt(Qe){return Pt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qt extends wt{constructor(A){super(),$t(this,A,Lt,Ct,xt,{})}}export{qt as component};
