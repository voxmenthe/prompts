import{s as Me,z as $e,o as ge,n as we}from"../chunks/scheduler.18a86fab.js";import{S as ve,i as Te,g as n,s,r as c,A as ke,h as i,f as l,c as a,j as re,u as h,x as y,k as R,y as We,a as o,v as d,d as f,t as u,w as b}from"../chunks/index.98837b22.js";import{T as _e}from"../chunks/Tip.77304350.js";import{C as ne}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ae,E as Ze}from"../chunks/getInferenceSnippets.06c2775f.js";function xe(F){let r,$="The initial call to torch.compile is slow because the model needs to be compiled. Subsequent calls to the compiled model are much faster because it doesn’t need to compile again.";return{c(){r=n("p"),r.textContent=$},l(m){r=i(m,"P",{"data-svelte-h":!0}),y(r)!=="svelte-1iqtu9r"&&(r.textContent=$)},m(m,N){o(m,r,N)},p:we,d(m){m&&l(r)}}}function Je(F){let r,$,m,N,g,L,w,ie='<a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a> compiles PyTorch code into optimized kernels that significantly speed up inference. This feature relies on <a href="https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html" rel="nofollow">TorchDynamo</a> to compile the code into graphs and <a href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747" rel="nofollow">TorchInductor</a> to further compile the graphs into optimized kernels. It is a powerful optimization tool, and in many cases, only requires adding a single line of code.',G,v,me="Wrap a model with torch.compile to compile and return an optimized model.",z,T,I,M,H,k,pe='There are several parameters to customize the compilation process. Two of the more important ones are listed below. For a full list of parameters, refer to the torch.compile <a href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="nofollow">documentation</a>.',P,W,V,_,ce="The <code>mode</code> parameter offers several performance options for compiling. Try different modes to see which one works best for your use case.",Y,Z,he="<li><code>default</code> is a balanced option between speed and memory.</li> <li><code>reduce-overhead</code> reduces the Python overhead at the expense of a little more memory, but it can be faster.</li> <li><code>max-autotune</code> offers the fastest speed, but compilation takes longer.</li>",S,x,A,J,Q,C,de="Fullgraph attempts to compile the entire model into a single graph to maximize performance. torch.compile raises an error if it encounters a graph break, which means it can’t compile the model into a single graph.",D,j,K,B,O,E,fe="Refer to the table below for performance benchmarks comparing the mean inference time in milliseconds with torch.compile enabled and disabled across various GPUs and batch sizes on the same image for different vision tasks.",ee,U,ue='Select <strong>Subset</strong> in the table below to switch between different GPUs, as well as benchmarks on <a href="https://download.pytorch.org/whl/nightly/cu118" rel="nofollow">PyTorch nightly</a> 2.1.0dev and torch.compile with <code>reduce-overhead</code> mode enabled.',te,p,be,le,X,oe,q,se;return g=new ae({props:{title:"torch.compile",local:"torchcompile",headingTag:"h1"}}),T=new ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMmIlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFjb21waWxlZF9tb2RlbCUyMCUzRCUyMHRvcmNoLmNvbXBpbGUobW9kZWwp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-2b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
compiled_model = torch.<span class="hljs-built_in">compile</span>(model)`,wrap:!1}}),M=new _e({props:{warning:!1,$$slots:{default:[xe]},$$scope:{ctx:F}}}),W=new ae({props:{title:"Modes",local:"modes",headingTag:"h2"}}),x=new ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMmIlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFjb21waWxlZF9tb2RlbCUyMCUzRCUyMHRvcmNoLmNvbXBpbGUobW9kZWwlMkMlMjBtb2RlJTNEJTIycmVkdWNlLW92ZXJoZWFkJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-2b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
compiled_model = torch.<span class="hljs-built_in">compile</span>(model, mode=<span class="hljs-string">&quot;reduce-overhead&quot;</span>)`,wrap:!1}}),J=new ae({props:{title:"Fullgraph",local:"fullgraph",headingTag:"h2"}}),j=new ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMmIlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFjb21waWxlZF9tb2RlbCUyMCUzRCUyMHRvcmNoLmNvbXBpbGUobW9kZWwlMkMlMjBtb2RlJTNEJTIycmVkdWNlLW92ZXJoZWFkJTIyJTJDJTIwZnVsbGdyYXBoJTNEVHJ1ZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-2b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
compiled_model = torch.<span class="hljs-built_in">compile</span>(model, mode=<span class="hljs-string">&quot;reduce-overhead&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)`,wrap:!1}}),B=new ae({props:{title:"Benchmarks",local:"benchmarks",headingTag:"h2"}}),X=new Ze({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_torch_compile.md"}}),{c(){r=n("meta"),$=s(),m=n("p"),N=s(),c(g.$$.fragment),L=s(),w=n("p"),w.innerHTML=ie,G=s(),v=n("p"),v.textContent=me,z=s(),c(T.$$.fragment),I=s(),c(M.$$.fragment),H=s(),k=n("p"),k.innerHTML=pe,P=s(),c(W.$$.fragment),V=s(),_=n("p"),_.innerHTML=ce,Y=s(),Z=n("ul"),Z.innerHTML=he,S=s(),c(x.$$.fragment),A=s(),c(J.$$.fragment),Q=s(),C=n("p"),C.textContent=de,D=s(),c(j.$$.fragment),K=s(),c(B.$$.fragment),O=s(),E=n("p"),E.textContent=fe,ee=s(),U=n("p"),U.innerHTML=ue,te=s(),p=n("iframe"),le=s(),c(X.$$.fragment),oe=s(),q=n("p"),this.h()},l(e){const t=ke("svelte-u9bgzb",document.head);r=i(t,"META",{name:!0,content:!0}),t.forEach(l),$=a(e),m=i(e,"P",{}),re(m).forEach(l),N=a(e),h(g.$$.fragment,e),L=a(e),w=i(e,"P",{"data-svelte-h":!0}),y(w)!=="svelte-l62zdv"&&(w.innerHTML=ie),G=a(e),v=i(e,"P",{"data-svelte-h":!0}),y(v)!=="svelte-zm6ubd"&&(v.textContent=me),z=a(e),h(T.$$.fragment,e),I=a(e),h(M.$$.fragment,e),H=a(e),k=i(e,"P",{"data-svelte-h":!0}),y(k)!=="svelte-11wkqq2"&&(k.innerHTML=pe),P=a(e),h(W.$$.fragment,e),V=a(e),_=i(e,"P",{"data-svelte-h":!0}),y(_)!=="svelte-1xvw3go"&&(_.innerHTML=ce),Y=a(e),Z=i(e,"UL",{"data-svelte-h":!0}),y(Z)!=="svelte-qtuy41"&&(Z.innerHTML=he),S=a(e),h(x.$$.fragment,e),A=a(e),h(J.$$.fragment,e),Q=a(e),C=i(e,"P",{"data-svelte-h":!0}),y(C)!=="svelte-fwkp52"&&(C.textContent=de),D=a(e),h(j.$$.fragment,e),K=a(e),h(B.$$.fragment,e),O=a(e),E=i(e,"P",{"data-svelte-h":!0}),y(E)!=="svelte-1rdhs4z"&&(E.textContent=fe),ee=a(e),U=i(e,"P",{"data-svelte-h":!0}),y(U)!=="svelte-fdi2jq"&&(U.innerHTML=ue),te=a(e),p=i(e,"IFRAME",{src:!0,frameborder:!0,width:!0,height:!0}),re(p).forEach(l),le=a(e),h(X.$$.fragment,e),oe=a(e),q=i(e,"P",{}),re(q).forEach(l),this.h()},h(){R(r,"name","hf:doc:metadata"),R(r,"content",Ce),$e(p.src,be="https://huggingface.co/datasets/stevhliu/compile-benchmarks/embed/viewer/t4/train")||R(p,"src",be),R(p,"frameborder","0"),R(p,"width","100%"),R(p,"height","560px")},m(e,t){We(document.head,r),o(e,$,t),o(e,m,t),o(e,N,t),d(g,e,t),o(e,L,t),o(e,w,t),o(e,G,t),o(e,v,t),o(e,z,t),d(T,e,t),o(e,I,t),d(M,e,t),o(e,H,t),o(e,k,t),o(e,P,t),d(W,e,t),o(e,V,t),o(e,_,t),o(e,Y,t),o(e,Z,t),o(e,S,t),d(x,e,t),o(e,A,t),d(J,e,t),o(e,Q,t),o(e,C,t),o(e,D,t),d(j,e,t),o(e,K,t),d(B,e,t),o(e,O,t),o(e,E,t),o(e,ee,t),o(e,U,t),o(e,te,t),o(e,p,t),o(e,le,t),d(X,e,t),o(e,oe,t),o(e,q,t),se=!0},p(e,[t]){const ye={};t&2&&(ye.$$scope={dirty:t,ctx:e}),M.$set(ye)},i(e){se||(f(g.$$.fragment,e),f(T.$$.fragment,e),f(M.$$.fragment,e),f(W.$$.fragment,e),f(x.$$.fragment,e),f(J.$$.fragment,e),f(j.$$.fragment,e),f(B.$$.fragment,e),f(X.$$.fragment,e),se=!0)},o(e){u(g.$$.fragment,e),u(T.$$.fragment,e),u(M.$$.fragment,e),u(W.$$.fragment,e),u(x.$$.fragment,e),u(J.$$.fragment,e),u(j.$$.fragment,e),u(B.$$.fragment,e),u(X.$$.fragment,e),se=!1},d(e){e&&(l($),l(m),l(N),l(L),l(w),l(G),l(v),l(z),l(I),l(H),l(k),l(P),l(V),l(_),l(Y),l(Z),l(S),l(A),l(Q),l(C),l(D),l(K),l(O),l(E),l(ee),l(U),l(te),l(p),l(le),l(oe),l(q)),l(r),b(g,e),b(T,e),b(M,e),b(W,e),b(x,e),b(J,e),b(j,e),b(B,e),b(X,e)}}}const Ce='{"title":"torch.compile","local":"torchcompile","sections":[{"title":"Modes","local":"modes","sections":[],"depth":2},{"title":"Fullgraph","local":"fullgraph","sections":[],"depth":2},{"title":"Benchmarks","local":"benchmarks","sections":[],"depth":2}],"depth":1}';function je(F){return ge(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Re extends ve{constructor(r){super(),Te(this,r,je,Je,Me,{})}}export{Re as component};
