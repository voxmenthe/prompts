import{s as ht,o as kt,n as Ee}from"../chunks/scheduler.18a86fab.js";import{S as _t,i as wt,g as d,s as a,r as g,A as bt,h as c,f as o,c as i,j as J,x as y,u as h,k as U,l as $t,y as r,a as m,v as k,d as _,t as w,w as b}from"../chunks/index.98837b22.js";import{T as Tt}from"../chunks/Tip.77304350.js";import{D as V}from"../chunks/Docstring.a1ef7999.js";import{C as tt}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as Re,E as yt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as vt,a as et}from"../chunks/HfOption.6641485e.js";function Mt(M){let t,p='Refer to the <a href="./bert">BERT</a> docs for more examples of how to apply BERTweet to different language tasks.';return{c(){t=d("p"),t.innerHTML=p},l(n){t=c(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-1m2g610"&&(t.innerHTML=p)},m(n,T){m(n,t,T)},p:Ee,d(n){n&&o(t)}}}function zt(M){let t,p;return t=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJmaWxsLW1hc2slMjIlMkMlMEElMjAlMjAlMjAlMjBtb2RlbCUzRCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZSUzRDAlMEEpJTBBcGlwZWxpbmUoJTIyUGxhbnRzJTIwY3JlYXRlJTIwJTNDbWFzayUzRSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjBwaG90b3N5bnRoZXNpcy4lMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;fill-mask&quot;</span>,
    model=<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;Plants create &lt;mask&gt; through a process known as photosynthesis.&quot;</span>)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,T){k(t,n,T),p=!0},p:Ee,i(n){p||(_(t.$$.fragment,n),p=!0)},o(n){w(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function xt(M){let t,p;return t=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yTWFza2VkTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIydmluYWklMkZiZXJ0d2VldC1iYXNlJTIyJTJDJTBBKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTBBKSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJQbGFudHMlMjBjcmVhdGUlMjAlM0NtYXNrJTNFJTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMHBob3Rvc3ludGhlc2lzLiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMEFtYXNrZWRfaW5kZXglMjAlM0QlMjB0b3JjaC53aGVyZShpbnB1dHMlNUInaW5wdXRfaWRzJyU1RCUyMCUzRCUzRCUyMHRva2VuaXplci5tYXNrX3Rva2VuX2lkKSU1QjElNUQlMEFwcmVkaWN0ZWRfdG9rZW5faWQlMjAlM0QlMjBwcmVkaWN0aW9ucyU1QjAlMkMlMjBtYXNrZWRfaW5kZXglNUQuYXJnbWF4KGRpbSUzRC0xKSUwQXByZWRpY3RlZF90b2tlbiUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUocHJlZGljdGVkX3Rva2VuX2lkKSUwQSUwQXByaW50KGYlMjJUaGUlMjBwcmVkaWN0ZWQlMjB0b2tlbiUyMGlzJTNBJTIwJTdCcHJlZGljdGVkX3Rva2VuJTdEJTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
   <span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>,
)
model = AutoModelForMaskedLM.from_pretrained(
    <span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)
inputs = tokenizer(<span class="hljs-string">&quot;Plants create &lt;mask&gt; through a process known as photosynthesis.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
predicted_token_id = predictions[<span class="hljs-number">0</span>, masked_index].argmax(dim=-<span class="hljs-number">1</span>)
predicted_token = tokenizer.decode(predicted_token_id)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted token is: <span class="hljs-subst">{predicted_token}</span>&quot;</span>)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,T){k(t,n,T),p=!0},p:Ee,i(n){p||(_(t.$$.fragment,n),p=!0)},o(n){w(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function Jt(M){let t,p;return t=new tt({props:{code:"ZWNobyUyMC1lJTIwJTIyUGxhbnRzJTIwY3JlYXRlJTIwJTNDbWFzayUzRSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjBwaG90b3N5bnRoZXNpcy4lMjIlMjAlN0MlMjB0cmFuc2Zvcm1lcnMtY2xpJTIwcnVuJTIwLS10YXNrJTIwZmlsbC1tYXNrJTIwLS1tb2RlbCUyMHZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMC0tZGV2aWNlJTIwMA==",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Plants create &lt;mask&gt; through a process known as photosynthesis.&quot;</span> | transformers-cli run --task fill-mask --model vinai/bertweet-base --device 0',wrap:!1}}),{c(){g(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,T){k(t,n,T),p=!0},p:Ee,i(n){p||(_(t.$$.fragment,n),p=!0)},o(n){w(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function Ut(M){let t,p,n,T,v,H;return t=new et({props:{id:"usage",option:"Pipeline",$$slots:{default:[zt]},$$scope:{ctx:M}}}),n=new et({props:{id:"usage",option:"AutoModel",$$slots:{default:[xt]},$$scope:{ctx:M}}}),v=new et({props:{id:"usage",option:"transformers CLI",$$slots:{default:[Jt]},$$scope:{ctx:M}}}),{c(){g(t.$$.fragment),p=a(),g(n.$$.fragment),T=a(),g(v.$$.fragment)},l(l){h(t.$$.fragment,l),p=i(l),h(n.$$.fragment,l),T=i(l),h(v.$$.fragment,l)},m(l,$){k(t,l,$),m(l,p,$),k(n,l,$),m(l,T,$),k(v,l,$),H=!0},p(l,$){const X={};$&2&&(X.$$scope={dirty:$,ctx:l}),t.$set(X);const z={};$&2&&(z.$$scope={dirty:$,ctx:l}),n.$set(z);const ge={};$&2&&(ge.$$scope={dirty:$,ctx:l}),v.$set(ge)},i(l){H||(_(t.$$.fragment,l),_(n.$$.fragment,l),_(v.$$.fragment,l),H=!0)},o(l){w(t.$$.fragment,l),w(n.$$.fragment,l),w(v.$$.fragment,l),H=!1},d(l){l&&(o(p),o(T)),b(t,l),b(n,l),b(v,l)}}}function Bt(M){let t,p,n,T,v,H="<em>This model was released on 2020-05-20 and added to Hugging Face Transformers on 2020-11-16.</em>",l,$,X,z,ge='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ke,G,_e,L,nt='<a href="https://huggingface.co/papers/2005.10200" rel="nofollow">BERTweet</a> shares the same architecture as <a href="./bert">BERT-base</a>, but it’s pretrained like <a href="./roberta">RoBERTa</a> on English Tweets. It performs really well on Tweet-related tasks like part-of-speech tagging, named entity recognition, and text classification.',we,W,st='You can find all the original BERTweet checkpoints under the <a href="https://huggingface.co/vinai?search_models=BERTweet" rel="nofollow">VinAI Research</a> organization.',be,B,$e,N,ot='The example below demonstrates how to predict the <code>&lt;mask&gt;</code> token with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',Te,C,ye,Q,ve,Y,rt='<li>Use the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bertweet#transformers.BertweetTokenizer">BertweetTokenizer</a> because it’s preloaded with a custom vocabulary adapted to tweet-specific tokens like hashtags (#), mentions (@), emojis, and common abbreviations. Make sure to also install the <a href="https://pypi.org/project/emoji/" rel="nofollow">emoji</a> library.</li> <li>Inputs should be padded on the right (<code>padding=&quot;max_length&quot;</code>) because BERT uses absolute position embeddings.</li>',Me,P,ze,f,D,Ze,oe,at="Constructs a BERTweet tokenizer, using Byte-Pair-Encoding.",Ve,re,it=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,He,q,F,Xe,ae,lt="Loads a pre-existing dictionary from a text file and adds its symbols to this instance.",Ge,x,S,Le,ie,dt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`,We,le,ct="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Ne,I,A,Qe,de,mt="Converts a sequence of tokens (string) in a single string.",Ye,j,K,Pe,ce,pt=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`,De,R,O,Fe,me,ft=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Se,E,ee,Ae,pe,ut="Normalize tokens in a Tweet",Ke,Z,te,Oe,fe,gt="Normalize a raw Tweet",xe,ne,Je,he,Ue;return $=new Re({props:{title:"BERTweet",local:"bertweet",headingTag:"h1"}}),G=new Re({props:{title:"BERTweet",local:"bertweet",headingTag:"h2"}}),B=new Tt({props:{warning:!1,$$slots:{default:[Mt]},$$scope:{ctx:M}}}),C=new vt({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Ut]},$$scope:{ctx:M}}}),Q=new Re({props:{title:"Notes",local:"notes",headingTag:"h2"}}),P=new Re({props:{title:"BertweetTokenizer",local:"transformers.BertweetTokenizer",headingTag:"h2"}}),D=new V({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BertweetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BertweetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BertweetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BertweetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BertweetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BertweetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L54"}}),F=new V({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L402"}}),S=new V({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L167",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),A=new V({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L368"}}),K=new V({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L221",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),O=new V({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L193",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),ee=new V({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L341"}}),te=new V({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bertweet/tokenization_bertweet.py#L307"}}),ne=new yt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bertweet.md"}}),{c(){t=d("meta"),p=a(),n=d("p"),T=a(),v=d("p"),v.innerHTML=H,l=a(),g($.$$.fragment),X=a(),z=d("div"),z.innerHTML=ge,ke=a(),g(G.$$.fragment),_e=a(),L=d("p"),L.innerHTML=nt,we=a(),W=d("p"),W.innerHTML=st,be=a(),g(B.$$.fragment),$e=a(),N=d("p"),N.innerHTML=ot,Te=a(),g(C.$$.fragment),ye=a(),g(Q.$$.fragment),ve=a(),Y=d("ul"),Y.innerHTML=rt,Me=a(),g(P.$$.fragment),ze=a(),f=d("div"),g(D.$$.fragment),Ze=a(),oe=d("p"),oe.textContent=at,Ve=a(),re=d("p"),re.innerHTML=it,He=a(),q=d("div"),g(F.$$.fragment),Xe=a(),ae=d("p"),ae.textContent=lt,Ge=a(),x=d("div"),g(S.$$.fragment),Le=a(),ie=d("p"),ie.textContent=dt,We=a(),le=d("ul"),le.innerHTML=ct,Ne=a(),I=d("div"),g(A.$$.fragment),Qe=a(),de=d("p"),de.textContent=mt,Ye=a(),j=d("div"),g(K.$$.fragment),Pe=a(),ce=d("p"),ce.textContent=pt,De=a(),R=d("div"),g(O.$$.fragment),Fe=a(),me=d("p"),me.innerHTML=ft,Se=a(),E=d("div"),g(ee.$$.fragment),Ae=a(),pe=d("p"),pe.textContent=ut,Ke=a(),Z=d("div"),g(te.$$.fragment),Oe=a(),fe=d("p"),fe.textContent=gt,xe=a(),g(ne.$$.fragment),Je=a(),he=d("p"),this.h()},l(e){const s=bt("svelte-u9bgzb",document.head);t=c(s,"META",{name:!0,content:!0}),s.forEach(o),p=i(e),n=c(e,"P",{}),J(n).forEach(o),T=i(e),v=c(e,"P",{"data-svelte-h":!0}),y(v)!=="svelte-oovnlm"&&(v.innerHTML=H),l=i(e),h($.$$.fragment,e),X=i(e),z=c(e,"DIV",{style:!0,"data-svelte-h":!0}),y(z)!=="svelte-z7iatx"&&(z.innerHTML=ge),ke=i(e),h(G.$$.fragment,e),_e=i(e),L=c(e,"P",{"data-svelte-h":!0}),y(L)!=="svelte-1oj883o"&&(L.innerHTML=nt),we=i(e),W=c(e,"P",{"data-svelte-h":!0}),y(W)!=="svelte-d98q7r"&&(W.innerHTML=st),be=i(e),h(B.$$.fragment,e),$e=i(e),N=c(e,"P",{"data-svelte-h":!0}),y(N)!=="svelte-10lshn2"&&(N.innerHTML=ot),Te=i(e),h(C.$$.fragment,e),ye=i(e),h(Q.$$.fragment,e),ve=i(e),Y=c(e,"UL",{"data-svelte-h":!0}),y(Y)!=="svelte-mjnaay"&&(Y.innerHTML=rt),Me=i(e),h(P.$$.fragment,e),ze=i(e),f=c(e,"DIV",{class:!0});var u=J(f);h(D.$$.fragment,u),Ze=i(u),oe=c(u,"P",{"data-svelte-h":!0}),y(oe)!=="svelte-b8riyv"&&(oe.textContent=at),Ve=i(u),re=c(u,"P",{"data-svelte-h":!0}),y(re)!=="svelte-ntrhio"&&(re.innerHTML=it),He=i(u),q=c(u,"DIV",{class:!0});var se=J(q);h(F.$$.fragment,se),Xe=i(se),ae=c(se,"P",{"data-svelte-h":!0}),y(ae)!=="svelte-ooaeix"&&(ae.textContent=lt),se.forEach(o),Ge=i(u),x=c(u,"DIV",{class:!0});var ue=J(x);h(S.$$.fragment,ue),Le=i(ue),ie=c(ue,"P",{"data-svelte-h":!0}),y(ie)!=="svelte-zjm6uf"&&(ie.textContent=dt),We=i(ue),le=c(ue,"UL",{"data-svelte-h":!0}),y(le)!=="svelte-rq8uot"&&(le.innerHTML=ct),ue.forEach(o),Ne=i(u),I=c(u,"DIV",{class:!0});var Be=J(I);h(A.$$.fragment,Be),Qe=i(Be),de=c(Be,"P",{"data-svelte-h":!0}),y(de)!=="svelte-b3k2yi"&&(de.textContent=mt),Be.forEach(o),Ye=i(u),j=c(u,"DIV",{class:!0});var Ce=J(j);h(K.$$.fragment,Ce),Pe=i(Ce),ce=c(Ce,"P",{"data-svelte-h":!0}),y(ce)!=="svelte-vpfvn5"&&(ce.textContent=pt),Ce.forEach(o),De=i(u),R=c(u,"DIV",{class:!0});var qe=J(R);h(O.$$.fragment,qe),Fe=i(qe),me=c(qe,"P",{"data-svelte-h":!0}),y(me)!=="svelte-1f4f5kp"&&(me.innerHTML=ft),qe.forEach(o),Se=i(u),E=c(u,"DIV",{class:!0});var Ie=J(E);h(ee.$$.fragment,Ie),Ae=i(Ie),pe=c(Ie,"P",{"data-svelte-h":!0}),y(pe)!=="svelte-1jdrmaw"&&(pe.textContent=ut),Ie.forEach(o),Ke=i(u),Z=c(u,"DIV",{class:!0});var je=J(Z);h(te.$$.fragment,je),Oe=i(je),fe=c(je,"P",{"data-svelte-h":!0}),y(fe)!=="svelte-15su17z"&&(fe.textContent=gt),je.forEach(o),u.forEach(o),xe=i(e),h(ne.$$.fragment,e),Je=i(e),he=c(e,"P",{}),J(he).forEach(o),this.h()},h(){U(t,"name","hf:doc:metadata"),U(t,"content",Ct),$t(z,"float","right"),U(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){r(document.head,t),m(e,p,s),m(e,n,s),m(e,T,s),m(e,v,s),m(e,l,s),k($,e,s),m(e,X,s),m(e,z,s),m(e,ke,s),k(G,e,s),m(e,_e,s),m(e,L,s),m(e,we,s),m(e,W,s),m(e,be,s),k(B,e,s),m(e,$e,s),m(e,N,s),m(e,Te,s),k(C,e,s),m(e,ye,s),k(Q,e,s),m(e,ve,s),m(e,Y,s),m(e,Me,s),k(P,e,s),m(e,ze,s),m(e,f,s),k(D,f,null),r(f,Ze),r(f,oe),r(f,Ve),r(f,re),r(f,He),r(f,q),k(F,q,null),r(q,Xe),r(q,ae),r(f,Ge),r(f,x),k(S,x,null),r(x,Le),r(x,ie),r(x,We),r(x,le),r(f,Ne),r(f,I),k(A,I,null),r(I,Qe),r(I,de),r(f,Ye),r(f,j),k(K,j,null),r(j,Pe),r(j,ce),r(f,De),r(f,R),k(O,R,null),r(R,Fe),r(R,me),r(f,Se),r(f,E),k(ee,E,null),r(E,Ae),r(E,pe),r(f,Ke),r(f,Z),k(te,Z,null),r(Z,Oe),r(Z,fe),m(e,xe,s),k(ne,e,s),m(e,Je,s),m(e,he,s),Ue=!0},p(e,[s]){const u={};s&2&&(u.$$scope={dirty:s,ctx:e}),B.$set(u);const se={};s&2&&(se.$$scope={dirty:s,ctx:e}),C.$set(se)},i(e){Ue||(_($.$$.fragment,e),_(G.$$.fragment,e),_(B.$$.fragment,e),_(C.$$.fragment,e),_(Q.$$.fragment,e),_(P.$$.fragment,e),_(D.$$.fragment,e),_(F.$$.fragment,e),_(S.$$.fragment,e),_(A.$$.fragment,e),_(K.$$.fragment,e),_(O.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),Ue=!0)},o(e){w($.$$.fragment,e),w(G.$$.fragment,e),w(B.$$.fragment,e),w(C.$$.fragment,e),w(Q.$$.fragment,e),w(P.$$.fragment,e),w(D.$$.fragment,e),w(F.$$.fragment,e),w(S.$$.fragment,e),w(A.$$.fragment,e),w(K.$$.fragment,e),w(O.$$.fragment,e),w(ee.$$.fragment,e),w(te.$$.fragment,e),w(ne.$$.fragment,e),Ue=!1},d(e){e&&(o(p),o(n),o(T),o(v),o(l),o(X),o(z),o(ke),o(_e),o(L),o(we),o(W),o(be),o($e),o(N),o(Te),o(ye),o(ve),o(Y),o(Me),o(ze),o(f),o(xe),o(Je),o(he)),o(t),b($,e),b(G,e),b(B,e),b(C,e),b(Q,e),b(P,e),b(D),b(F),b(S),b(A),b(K),b(O),b(ee),b(te),b(ne,e)}}}const Ct='{"title":"BERTweet","local":"bertweet","sections":[{"title":"BERTweet","local":"bertweet","sections":[],"depth":2},{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"BertweetTokenizer","local":"transformers.BertweetTokenizer","sections":[],"depth":2}],"depth":1}';function qt(M){return kt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xt extends _t{constructor(t){super(),wt(this,t,qt,Bt,ht,{})}}export{Xt as component};
