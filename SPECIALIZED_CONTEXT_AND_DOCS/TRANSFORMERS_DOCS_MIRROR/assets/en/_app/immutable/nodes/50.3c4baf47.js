import{s as Rw,o as qw,n as Aw}from"../chunks/scheduler.18a86fab.js";import{S as Ew,i as Yw,g as i,s as n,r as d,m as r,H as c,A as Sw,h as p,f as a,c as l,j as M,u as g,x as o,n as h,B as u,k as L,y as m,a as e,v as y,d as f,t as w,w as b}from"../chunks/index.98837b22.js";import{T as Fw}from"../chunks/Tip.77304350.js";import{C as v}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as Pw}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as is,E as Kw}from"../chunks/getInferenceSnippets.06c2775f.js";function Dw(cl){let G,Cs='Note that, despite our advice to use key-value caches, your LLM output may be slightly different when you use them. This is a property of the matrix multiplication kernels themselves ‚Äî you can read more about it <a href="https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535" rel="nofollow">here</a>.';return{c(){G=i("p"),G.innerHTML=Cs},l(P){G=p(P,"P",{"data-svelte-h":!0}),o(G)!=="svelte-18twd2j"&&(G.innerHTML=Cs)},m(P,fn){e(P,G,fn)},p:Aw,d(P){P&&a(G)}}}function Ow(cl){let G,Cs,P,fn,zs,ul,Ls,dl,Gs,Fu=`Large Language Models (LLMs) such as GPT3/4, <a href="https://huggingface.co/tiiuae/falcon-40b" rel="nofollow">Falcon</a>, and <a href="https://huggingface.co/meta-llama/Llama-2-70b-hf" rel="nofollow">Llama</a> are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.
Deploying these models in real-world tasks remains challenging, however:`,gl,Bs,Pu='<li>To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see <a href="https://huggingface.co/papers/2001.08361" rel="nofollow">Kaplan et al</a>, <a href="https://huggingface.co/papers/2206.07682" rel="nofollow">Wei et. al</a>). This consequently amplifies the memory demands for inference.</li> <li>In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model‚Äôs capability to manage very long input sequences during inference.</li>',yl,$s,Ku="The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.",fl,Zs,Du="In this guide, we will go over the effective techniques for efficient LLM deployment:",wl,Is,Ou='<li><p><strong>Lower Precision:</strong> Research has shown that operating at reduced numerical precision, namely <a href="./main_classes/quantization">8-bit and 4-bit</a> can achieve computational advantages without a considerable decline in model performance.</p></li> <li><p><strong>Flash Attention:</strong> Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.</p></li> <li><p><strong>Architectural Innovations:</strong> Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are <a href="https://huggingface.co/papers/2108.12409" rel="nofollow">Alibi</a>, <a href="https://huggingface.co/papers/2104.09864" rel="nofollow">Rotary embeddings</a>, <a href="https://huggingface.co/papers/1911.02150" rel="nofollow">Multi-Query Attention (MQA)</a> and <a href="https://huggingface.co/papers/2305.13245" rel="nofollow">Grouped-Query-Attention (GQA)</a>.</p></li>',bl,Ws,sd="Throughout this guide, we will offer an analysis of auto-regressive generation from a tensor‚Äôs perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.",vl,Qs,Ml,Hs,td="Memory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition <em>weights</em> will be used to signify all model weight matrices and vectors.",xl,Xs,ad='At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. <code>4.5689</code> which is usually stored in either <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" rel="nofollow">float32</a>, <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="nofollow">bfloat16</a>, or <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" rel="nofollow">float16</a> format. This allows us to easily compute the memory requirement to load the LLM into memory:',kl,Ns,ed="<p><em>Loading the weights of a model having X billion parameters requires roughly 4</em> X GB of VRAM in float32 precision*</p>",Tl,Vs,nd="Nowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:",jl,Rs,ld="<p><em>Loading the weights of a model having X billion parameters requires roughly 2</em> X GB of VRAM in bfloat16/float16 precision*</p>",_l,qs,id="For shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let‚Äôs assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.",Jl,As,pd="To give some examples of how much VRAM it roughly takes to load a model in bfloat16:",Ul,Es,md='<li><strong>GPT3</strong> requires 2 * 175 GB = <strong>350 GB</strong> VRAM</li> <li><a href="https://huggingface.co/bigscience/bloom" rel="nofollow"><strong>Bloom</strong></a> requires 2 * 176 GB = <strong>352 GB</strong> VRAM</li> <li><a href="https://huggingface.co/meta-llama/Llama-2-70b-hf" rel="nofollow"><strong>Llama-2-70b</strong></a> requires 2 * 70 GB = <strong>140 GB</strong> VRAM</li> <li><a href="https://huggingface.co/tiiuae/falcon-40b" rel="nofollow"><strong>Falcon-40b</strong></a> requires 2 * 40 GB = <strong>80 GB</strong> VRAM</li> <li><a href="https://huggingface.co/mosaicml/mpt-30b" rel="nofollow"><strong>MPT-30b</strong></a> requires 2 * 30 GB = <strong>60 GB</strong> VRAM</li> <li><a href="https://huggingface.co/bigcode/starcoder" rel="nofollow"><strong>bigcode/starcoder</strong></a> requires 2 * 15.5 = <strong>31 GB</strong> VRAM</li>',Cl,Ys,od='As of writing this document, the largest GPU chip on the market is the A100 &amp; H100 offering 80GB of VRAM. Most of the models listed before require more than 80GB just to be loaded and therefore necessarily require <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism" rel="nofollow">tensor parallelism</a> and/or <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism" rel="nofollow">pipeline parallelism</a>.',zl,Ss,rd='ü§ó Transformers now supports tensor parallelism for supported models having <code>base_tp_plan</code> in their respective config classes. Learn more about Tensor Parallelism <a href="perf_train_gpu_many#tensor-parallelism">here</a>. Furthermore, if you‚Äôre interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at <a href="https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling" rel="nofollow">the text-generation-inference library</a>.',Ll,Fs,hd=`Naive pipeline parallelism is supported out of the box. For this, simply load the model with <code>device=&quot;auto&quot;</code> which will automatically place the different layers on the available GPUs as explained <a href="https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference" rel="nofollow">here</a>.
Note, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained <a href="https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism" rel="nofollow">here</a>.`,Gl,Ps,cd="If you have access to an 8 x 80GB A100 node, you could load BLOOM as follows",Bl,Ks,$l,Ds,Zl,Os,ud="By using <code>device_map=&quot;auto&quot;</code> the attention layers would be equally distributed over all available GPUs.",Il,st,dd='In this guide, we will use <a href="https://huggingface.co/bigcode/octocoder" rel="nofollow">bigcode/octocoder</a> as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.',Wl,tt,gd="Since the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with <code>bigcode/octocoder</code> to be around 31 GB VRAM. Let‚Äôs give it a try.",Ql,at,yd='We first load the model and tokenizer and then pass both to Transformers‚Äô <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" rel="nofollow">pipeline</a> object.',Hl,et,Xl,nt,Nl,lt,fd="<strong>Output</strong>:",Vl,it,Rl,pt,wd="Nice, we can now directly use the result to convert bytes into Gigabytes.",ql,mt,Al,ot,bd='Let‚Äôs call <a href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html" rel="nofollow"><code>torch.cuda.max_memory_allocated</code></a> to measure the peak GPU memory allocation.',El,rt,Yl,ht,vd="<strong>Output</strong>:",Sl,ct,Fl,ut,Md=`Close enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000. Therefore the back-of-the-envelope formula can also be understood as an ‚Äúat most X GB‚Äù computation.
Note that if we had tried to run the model in full float32 precision, a whopping 64 GB of VRAM would have been required.`,Pl,dt,xd='<p>Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if <a href="https://discuss.pytorch.org/t/bfloat16-native-support/117155/5" rel="nofollow">your GPU supports bfloat16</a>. Float32 won‚Äôt give better inference results than the precision that was used to train the model.</p>',Kl,gt,kd='If you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint‚Äôs config under <code>&quot;dtype&quot;</code>, <em>e.g.</em> <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21" rel="nofollow">here</a>. It is recommended to set the model to the same precision type as written in the config when loading with <code>from_pretrained(..., dtype=...)</code> except when the original type is float32 in which case one can use both <code>float16</code> or <code>bfloat16</code> for inference.',Dl,yt,Td="Let‚Äôs define a <code>flush(...)</code> function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.",Ol,ft,si,wt,jd="Let‚Äôs call it now for the next experiment.",ti,bt,ai,vt,_d='From the Accelerate library, you can also use a device-agnostic utility method called <a href="https://github.com/huggingface/accelerate/blob/29be4788629b772a3b722076e433b5b3b5c85da3/src/accelerate/utils/memory.py#L63" rel="nofollow">release_memory</a>, which takes various hardware backends like XPU, MLU, NPU, MPS, and more into account.',ei,Mt,ni,xt,Jd=`Now what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see <a href="https://huggingface.co/papers/2208.07339" rel="nofollow">Dettmers et al.</a>).
Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent <a href="https://huggingface.co/papers/2210.17323" rel="nofollow">GPTQ paper</a> ü§Ø.`,li,kt,Ud=`Without going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model‚Äôs inference results as accurate as possible (<em>a.k.a</em> as close as possible to bfloat16).
Note that quantization works especially well for text generation since all we care about is choosing the <em>set of most likely next tokens</em> and don‚Äôt really care about the exact values of the next token <em>logit</em> distribution.
All that matters is that the next token <em>logit</em> distribution stays roughly the same so that an <code>argmax</code> or <code>topk</code> operation gives the same results.`,ii,Tt,Cd="There are various quantization techniques, which we won‚Äôt discuss in detail here, but in general, all quantization techniques work as follows:",pi,jt,zd='<li><ol><li>Quantize all weights to the target precision</li></ol></li> <li><ol start="2"><li>Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision</li></ol></li> <li><ol start="3"><li>Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision</li></ol></li>',mi,B,zc,wn,Ld="inputs-weight matrix",Lc,oi,ff='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex"> X </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>',ri,bn,Gd="inputs",Gc,hi,wf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex"> W </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>',ci,ui,bf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex"> Y </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span>',di,gi,vf='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>‚àó</mo><mi>W</mi></mrow><annotation encoding="application/x-tex"> Y = X * W </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span>',yi,_t,Bc,fi,Mf='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>‚àó</mo><mtext>dequantize</mtext><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> Y = X * \\text{dequantize}(W) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">dequantize</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></span>',wi,Jt,Bd="for every matrix multiplication. Dequantization and re-quantization is performed sequentially for all weight matrices as the inputs run through the network graph.",bi,Ut,$d=`Therefore, inference time is often <strong>not</strong> reduced when using quantized weights, but rather increases.
Enough theory, let‚Äôs give it a try! To quantize the weights with Transformers, you need to make sure that
the <a href="https://github.com/bitsandbytes-foundation/bitsandbytes" rel="nofollow"><code>bitsandbytes</code></a> library is installed.`,vi,Ct,Mi,zt,Zd="We can then load models in 8-bit quantization by simply adding a <code>load_in_8bit=True</code> flag to <code>from_pretrained</code>.",xi,Lt,ki,Gt,Id="Now, let‚Äôs run our example again and measure the memory usage.",Ti,Bt,ji,$t,Wd="<strong>Output</strong>:",_i,Zt,Ji,It,Qd="Nice, we‚Äôre getting the same result as before, so no loss in accuracy! Let‚Äôs look at how much memory was used this time.",Ui,Wt,Ci,Qt,Hd="<strong>Output</strong>:",zi,Ht,Li,Xt,Xd=`Significantly less! We‚Äôre down to just a bit over 15 GBs and could therefore run this model on consumer GPUs like the 4090.
We‚Äôre seeing a very nice gain in memory efficiency and more or less no degradation to the model‚Äôs output. However, we can also notice a slight slow-down during inference.`,Gi,Nt,Nd="We delete the models and flush the memory again.",Bi,Vt,$i,Rt,Zi,qt,Vd="Let‚Äôs see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing <code>load_in_4bit=True</code> instead of <code>load_in_8bit=True</code>.",Ii,At,Wi,Et,Rd="<strong>Output</strong>:",Qi,Yt,Hi,St,qd="We‚Äôre almost seeing the same output text as before - just the <code>python</code> is missing just before the code snippet. Let‚Äôs see how much memory was required.",Xi,Ft,Ni,Pt,Ad="<strong>Output</strong>:",Vi,Kt,Ri,Dt,Ed="Just 9.5GB! That‚Äôs really not a lot for a >15 billion parameter model.",qi,Ot,Yd="While we see very little degradation in accuracy for our model here, 4-bit quantization can in practice often lead to different results compared to 8-bit quantization or full <code>bfloat16</code> inference. It is up to the user to try it out.",Ai,K,$c,Ei,xf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>quantize</mtext></mrow><annotation encoding="application/x-tex"> \\text{quantize} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">quantize</span></span></span></span></span>',Yi,Si,kf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>dequantize</mtext></mrow><annotation encoding="application/x-tex"> \\text{dequantize} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">dequantize</span></span></span></span></span>',Fi,Pi,sa,Ki,ta,Di,aa,Sd="Overall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32G GPU VRAM to only 15GB and running the model in 4-bit precision further reduces the required GPU VRAM to just a bit over 9GB.",Oi,ea,Fd="4-bit quantization allows the model to be run on GPUs such as RTX3090, V100, and T4 which are quite accessible for most people.",sp,na,Pd='For more information on quantization and to see how one can quantize models to require even less GPU VRAM memory than 4-bit, we recommend looking into the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60" rel="nofollow"><code>AutoGPTQ</code></a> implementation.',tp,la,Kd="<p>As a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.</p>",ap,ia,Dd="If GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can‚Äôt run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.",ep,pa,Od=`For more in-detail usage information, we strongly recommend taking a look at the <a href="https://huggingface.co/docs/transformers/main_classes/quantization#general-usage" rel="nofollow">Transformers Quantization Docs</a>.
Next, let‚Äôs look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.`,np,ma,lp,oa,sg="Today‚Äôs top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.",ip,q,Zc,vn,tg="quadratically",Ic,Mn,ag="sequence length",Wc,pp,Tf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',mp,op,T,Qc,rp,jf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">O</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{O} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">O</span></span></span></span>',hp,cp,_f='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{X} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">X</span></span></span></span>',up,dp,Jf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',gp,yp,Uf='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext mathvariant="bold">O</mtext><mo>=</mo><mtext>Attn</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold">V</mi><mo>√ó</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo stretchy="false">)</mo><mtext>¬†with¬†</mtext><mi mathvariant="bold">Q</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>q</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>v</mi></msub><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo>=</mo><msub><mi mathvariant="bold">W</mi><mi>k</mi></msub><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex"> \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}_k \\mathbf{X} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord text"><span class="mord textbf">O</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attn</span></span><span class="mopen">(</span><span class="mord mathbf">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7694em;vertical-align:-0.0833em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">V</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1673em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord text"><span class="mord">¬†with¬†</span></span><span class="mord mathbf">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathbf">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathbf">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathbf">X</span></span></span></span></span>',fp,wp,Cf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">  \\mathbf{X} = (\\mathbf{x}_1, ... \\mathbf{x}_{N}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',bp,vp,zf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{Q} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord mathbf">Q</span></span></span></span>',Mp,xp,Lf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{K} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">K</span></span></span></span>',kp,Tp,Gf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',jp,_p,Bf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Jp,Up,$f='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> N^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>',Cp,zp,U,Hc,Lp,Zf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="bold">T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK^T} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0377em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">Q</span><span class="mord"><span class="mord mathbf">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8433em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">T</span></span></span></span></span></span></span></span></span></span></span></span>',Gp,Bp,If='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn><mo>‚àó</mo><mn>2</mn><mo>‚àó</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> 40 * 2 * N^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">40</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>',$p,Zp,Wf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex"> N=1000 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1000</span></span></span></span>',Ip,Wp,Qf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16000</mn></mrow><annotation encoding="application/x-tex"> N=16000 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16000</span></span></span></span>',Qp,Hp,Hf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex"> N=100,000 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">100</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span></span></span></span>',Xp,Np,Xf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Vp,Rp,ra,eg="Long story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts.",qp,ha,ng="As LLMs improve in text comprehension and generation, they are applied to increasingly complex tasks. While models once handled the translation or summarization of a few sentences, they now manage entire pages, demanding the capability to process extensive input lengths.",Ap,A,Xc,Ep,Nf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> QK^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Yp,hs,lg="Tri Dao et al.",Nc,xn,ig="Flash Attention",Vc,Sp,ps,Rc,Fp,Vf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi><mo>√ó</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7694em;vertical-align:-0.0833em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">V</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1673em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Pp,Kp,Rf='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">O</mtext><mi>i</mi></msub><mo>‚Üê</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup><mo>‚àó</mo><msub><mtext mathvariant="bold">O</mtext><mi>i</mi></msub><mo>+</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup><mo>‚àó</mo><msub><mi mathvariant="bold">V</mi><mi>j</mi></msub><mo>√ó</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><msubsup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo><mtext>¬†for¬†multiple¬†</mtext><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mtext>¬†iterations</mtext></mrow><annotation encoding="application/x-tex"> \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\times \\text{Softmax}(\\mathbf{QK}^T_{i,j}) \\text{ for multiple } i, j \\text{ iterations} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">O</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚Üê</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0975em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">O</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2822em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.3004em;vertical-align:-0.3831em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord text"><span class="mord">¬†for¬†multiple¬†</span></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mord text"><span class="mord">¬†iterations</span></span></span></span></span></span>',Dp,W,qc,Op,qf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>a</mi></msubsup></mrow><annotation encoding="application/x-tex"> s^a_{ij} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0592em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>',sm,tm,Af='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>b</mi></msubsup></mrow><annotation encoding="application/x-tex"> s^b_{ij} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2439em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>',am,em,Ef='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex"> i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>',nm,lm,Yf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex"> j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',im,pm,ca,pg='Please note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this guide. The reader is invited to take a look at the well-written <a href="https://huggingface.co/papers/2205.14135" rel="nofollow">Flash Attention paper</a> for more details.',mm,ua,mg="The main takeaway here is:",om,kn,ms,Ac,Tn,og="numerical identical",Ec,rm,Sf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',hm,cm,da,rg='Looking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see <a href="https://huggingface.co/papers/2205.14135" rel="nofollow">paper</a> for more details if interested)',um,ga,hg="<p>However, Flash Attention is much faster in inference compared to default attention which comes from its ability to significantly reduce the demands on the slower, high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip memory (SRAM).</p>",dm,D,Yc,jn,cg="on-chip",Sc,gm,Ff='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">O</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{O} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">O</span></span></span></span>',ym,fm,ya,ug="In practice, there is currently absolutely no reason to <strong>not</strong> use Flash Attention if available. The algorithm gives mathematically the same outputs, and is both faster and more memory-efficient.",wm,fa,dg="Let‚Äôs look at a practical example.",bm,wa,gg=`Our OctoCoder model now gets a significantly longer input prompt which includes a so-called <em>system prompt</em>. System prompts are used to steer the LLM into a better assistant that is tailored to the users‚Äô task.
In the following, we use a system prompt that will make OctoCoder a better coding assistant.`,vm,ba,Mm,va,yg=`For demonstration purposes, we duplicate the system prompt by ten so that the input length is long enough to observe Flash Attention‚Äôs memory savings.
We append the original text prompt <code>&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here&quot;</code>`,xm,Ma,km,xa,fg="We instantiate our model again in bfloat16 precision.",Tm,ka,jm,Ta,wg="Let‚Äôs now run the model just like before <em>without Flash Attention</em> and measure the peak GPU memory requirement and inference time.",_m,ja,Jm,_a,bg="<strong>Output</strong>:",Um,Ja,Cm,Ua,vg="We‚Äôre getting the same output as before, however this time, the model repeats the answer multiple times until it‚Äôs 60 tokens cut-off. This is not surprising as we‚Äôve repeated the system prompt ten times for demonstration purposes and thus cued the model to repeat itself.",zm,Ca,Mg="<strong>Note</strong> that the system prompt should not be repeated ten times in real-world applications - one time is enough!",Lm,za,xg="Let‚Äôs measure the peak GPU memory requirement.",Gm,La,Bm,Ga,kg="<strong>Output</strong>:",$m,Ba,Zm,$a,Tg="As we can see the peak GPU memory requirement is now significantly higher than in the beginning, which is largely due to the longer input sequence. Also the generation takes a little over a minute now.",Im,Za,jg="We call <code>flush()</code> to free GPU memory for our next experiment.",Wm,Ia,Qm,Wa,_g=`For comparison, let‚Äôs run the same function, but enable Flash Attention instead.
To do so, we convert the model to <a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">BetterTransformer</a> and by doing so enabling PyTorch‚Äôs <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow">SDPA self-attention</a> which in turn is able to use Flash Attention.`,Hm,Qa,Xm,Ha,Jg="Now we run the exact same code snippet as before and under the hood Transformers will make use of Flash Attention.",Nm,Xa,Vm,Na,Ug="<strong>Output</strong>:",Rm,Va,qm,Ra,Cg="We‚Äôre getting the exact same result as before, but can observe a very significant speed-up thanks to Flash Attention.",Am,qa,zg="Let‚Äôs measure the memory consumption one last time.",Em,Aa,Ym,Ea,Lg="<strong>Output</strong>:",Sm,Ya,Fm,Sa,Gg="And we‚Äôre almost back to our original 29GB peak GPU memory from the beginning.",Pm,Fa,Bg="We can observe that we only use roughly 100MB more GPU memory when passing a very long input sequence with Flash Attention compared to passing a short input sequence as done in the beginning.",Km,Pa,Dm,Ka,$g='For more information on how to use Flash Attention, please have a look at <a href="https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#flashattention-2" rel="nofollow">this doc page</a>.',Om,Da,so,Oa,Zg="So far we have looked into improving computational and memory efficiency by:",to,se,Ig="<li>Casting the weights to a lower precision format</li> <li>Replacing the self-attention algorithm with a more memory- and compute efficient version</li>",ao,te,Wg="Let‚Äôs now look into how we can change the architecture of an LLM so that it is most effective and efficient for task that require long text inputs, <em>e.g.</em>:",eo,ae,Qg="<li>Retrieval augmented Questions Answering,</li> <li>Summarization,</li> <li>Chat</li>",no,ee,Hg="Note that <em>chat</em> not only requires the LLM to handle long text inputs, but it also necessitates that the LLM is able to efficiently handle the back-and-forth dialogue between user and assistant (such as ChatGPT).",lo,ne,Xg=`Once trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM‚Äôs tasks beforehand and accordingly optimize the model‚Äôs architecture.
There are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.`,io,le,Ng="<li>The positional embeddings</li> <li>The key-value cache</li>",po,ie,Vg="Let‚Äôs go over each component in more detail",mo,pe,oo,O,Fc,ro,Pf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \\text{Softmax}(\\mathbf{QK}^T) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1673em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',ho,_n,Rg="‚ÄúHello‚Äù, ‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚Äúyou‚Äù",Pc,co,me,qg='<img src="/blog/assets/163_optimize_llm/self_attn_tokens.png"/>',uo,oe,Ag="Each word token is given a probability mass at which it attends all other word tokens and, therefore is put into relation with all other word tokens. E.g. the word <em>‚Äúlove‚Äù</em> attends to the word <em>‚ÄúHello‚Äù</em> with 5%, to <em>‚ÄúI‚Äù</em> with 30%, and to itself with 65%.",go,Z,Kc,yo,Kf='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',fo,wo,Df='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> O(1) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span>',bo,Jn,Eg="e.g.",Dc,Un,Yg="‚ÄúHello I love you‚Äù",Oc,Cn,Sg="‚ÄúYou love I hello‚Äù",su,vo,re,Fg=`For the LLM to understand sentence order, an additional <em>cue</em> is needed and is usually applied in the form of <em>positional encodings</em> (or also called <em>positional embeddings</em>).
Positional encodings, encode the position of each token into a numerical presentation that the LLM can leverage to better understand sentence order.`,Mo,C,tu,cs,Pg="<em>Attention Is All You Need</em>",au,xo,Of='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">P</mi><mo>=</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',ko,To,sw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{p}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',jo,_o,tw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex"> i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>',Jo,Uo,aw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="bold">X</mi><mo>^</mo></mover><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>N</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{\\hat{X}} = \\mathbf{\\hat{x}}_1, \\ldots, \\mathbf{\\hat{x}}_N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9495em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9495em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathbf">X</span></span><span style="top:-3.2551em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9023em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathbf">x</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathbf">x</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Co,zo,ew='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo>+</mo><msub><mi mathvariant="bold">p</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>N</mi></msub><mo>+</mo><msub><mi mathvariant="bold">p</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{x}_1 + \\mathbf{p}_1, \\ldots, \\mathbf{x}_N + \\mathbf{p}_N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Lo,Go,ss,eu,us,Kg="Devlin et al.",nu,Bo,nw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">P</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{P} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">P</span></span></span></span>',$o,Zo,he,Dg="Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:",Io,ds,V,lu,zn,Og="i.e.",iu,Wo,lw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex"> 0, \\ldots, N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',Qo,gs,sy="Huang et al.",pu,ys,ty="Su et al.",mu,ou,ce,ru,Ho,iw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex"> N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>',Xo,No,ue,ay="Recently, relative positional embeddings that can tackle the above mentioned problems have become more popular, most notably:",Vo,de,ey='<li><a href="https://huggingface.co/papers/2104.09864" rel="nofollow">Rotary Position Embedding (RoPE)</a></li> <li><a href="https://huggingface.co/papers/2108.12409" rel="nofollow">ALiBi</a></li>',Ro,E,hu,Ln,ny="RoPE",cu,Gn,ly="ALiBi",uu,qo,pw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Ao,Eo,k,du,Bn,iy="RoPE",gu,$n,py="e.g.",Yo,mw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',So,Fo,ow='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{x}_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',Po,Ko,rw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi><mo>‚àó</mo><mi>i</mi></mrow><annotation encoding="application/x-tex"> \\theta * i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Œ∏</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>',Do,Oo,hw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi><mo>‚àó</mo><mi>j</mi></mrow><annotation encoding="application/x-tex"> \\theta * j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Œ∏</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àó</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',sr,tr,cw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex"> i, j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',ar,er,uw='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mover accent="true"><mi mathvariant="bold">q</mi><mo>^</mo></mover><mi>i</mi><mi>T</mi></msubsup><msub><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi mathvariant="bold">R</mi><mrow><mi>Œ∏</mi><mo separator="true">,</mo><mi>i</mi><mo>‚àí</mo><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex"> \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta, i -j} \\mathbf{{x}}_j. </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2252em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathbf">q</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9391em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1608em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7079em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathbf">x</span></span><span style="top:-3.0134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2875em;"><span class="mord mathbf">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1774em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">Œ∏</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">.</span></span></span></span></span>',nr,lr,dw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">R</mi><mrow><mi>Œ∏</mi><mo separator="true">,</mo><mi>i</mi><mo>‚àí</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{R}_{\\theta, i - j} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">Œ∏</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">‚àí</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',ir,pr,gw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi></mrow><annotation encoding="application/x-tex"> \\theta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Œ∏</span></span></span></span>',mr,Zn,my="not",yu,or,In,$,fu,rr,yw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',hr,cr,fw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',ur,dr,ww='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo mathvariant="normal">‚â†</mo><mi>j</mi></mrow><annotation encoding="application/x-tex"> i \\ne j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel">ÓÄ†</span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',gr,yr,bw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>‚àí</mo><mi>j</mi></mrow><annotation encoding="application/x-tex"> i - j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',fr,wr,vw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex"> i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>',br,vr,Mw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex"> j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>',Mr,xr,ge,oy="<em>RoPE</em> is used in multiple of today‚Äôs most important LLMs, such as:",kr,ye,ry='<li><a href="https://huggingface.co/tiiuae/falcon-40b" rel="nofollow"><strong>Falcon</strong></a></li> <li><a href="https://huggingface.co/papers/2302.13971" rel="nofollow"><strong>Llama</strong></a></li> <li><a href="https://huggingface.co/papers/2204.02311" rel="nofollow"><strong>PaLM</strong></a></li>',Tr,Y,wu,Wn,hy="ALiBi",bu,Qn,cy="m",vu,jr,xw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',_r,Jr,fe,uy='<img src="/blog/assets/163_optimize_llm/alibi.png"/>',Ur,we,dy='As shown in the <a href="https://huggingface.co/papers/2108.12409" rel="nofollow">ALiBi</a> paper, this simple relative positional encoding allows the model to retain a high performance even at very long text input sequences.',Cr,be,gy="<em>ALiBi</em> is used in multiple of today‚Äôs most important LLMs, such as:",zr,ve,yy='<li><a href="https://huggingface.co/mosaicml/mpt-30b" rel="nofollow"><strong>MPT</strong></a></li> <li><a href="https://huggingface.co/bigscience/bloom" rel="nofollow"><strong>BLOOM</strong></a></li>',Lr,x,Mu,Hn,fy="RoPE",xu,Xn,wy="ALiBi",ku,Nn,by="ALiBi",Tu,Vn,vy="RoPE",ju,Rn,My="RoPE",_u,Gr,kw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi></mrow><annotation encoding="application/x-tex"> \\theta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Œ∏</span></span></span></span>',Br,qn,xy="c.f",Ju,fs,ky="Press et al.",Uu,$r,Tw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi></mrow><annotation encoding="application/x-tex"> \\theta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Œ∏</span></span></span></span>',Zr,An,Ty="RoPE",Cu,ws,jy="here",zu,Ir,bs,En,_y="Both RoPE and ALiBi are relative positional embeddings that are <em>not</em> learned during training, but instead are based on the following intuitions:",Lu,os,Me,Gu,Wr,jw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> QK^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Qr,Bu,Yn,Jy="The LLM should be incentivized to learn a constant <em>relative</em> distance positional encodings have to each other",$u,Sn,Uy="The further text input tokens are from each other, the lower the probability of their query-value probability. Both RoPE and ALiBi lower the query-key probability of tokens far away from each other. RoPE by decreasing their vector product by increasing the angle between the query-key vectors. ALiBi by adding large negative numbers to the vector product",Hr,S,Zu,Xr,_w='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex"> N_1 = 2048 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2048</span></span></span></span>',Nr,Vr,Jw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex"> N_1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Rr,qr,Uw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>=</mo><mn>8192</mn><mo>&gt;</mo><msub><mi>N</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex"> N_2 = 8192 &gt; N_1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6835em;vertical-align:-0.0391em;"></span><span class="mord">8192</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Ar,Er,xe,Yr,ke,Cy="Auto-regressive text generation with LLMs works by iteratively putting in an input sequence, sampling the next token, appending the next token to the input sequence, and continuing to do so until the LLM produces a token that signifies that the generation has finished.",Sr,Te,zy='Please have a look at <a href="https://huggingface.co/docs/transformers/llm_tutorial#generate-text" rel="nofollow">Transformer‚Äôs Generate Text Tutorial</a> to get a more visual explanation of how auto-regressive generation works.',Fr,je,Ly="Let‚Äôs run a quick code snippet to show how auto-regressive works in practice. We will simply take the most likely next token via <code>torch.argmax</code>.",Pr,_e,Kr,Je,Gy="<strong>Output</strong>:",Dr,Ue,Or,Ce,By="As we can see every time we increase the text input tokens by the just sampled token.",sh,ze,$y='With very few exceptions, LLMs are trained using the <a href="https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling" rel="nofollow">causal language modeling objective</a> and therefore mask the upper triangle matrix of the attention score - this is why in the two diagrams above the attention scores are left blank (<em>a.k.a</em> have 0 probability). For a quick recap on causal language modeling you can refer to the <a href="https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention" rel="nofollow"><em>Illustrated Self Attention blog</em></a>.',th,z,Iu,Fn,Zy="never",Wu,ah,Cw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',eh,nh,zw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{k}_j, \\mathbf{v}_j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',lh,ih,Lw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>&gt;</mo><mi>i</mi></mrow><annotation encoding="application/x-tex"> j &gt; i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>',ph,mh,Gw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',oh,rh,Bw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">k</mi><mrow><mi>m</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mrow><mi>m</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mtext>¬†,¬†for¬†</mtext><mi>m</mi><mo>‚àà</mo><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo>‚Ä¶</mo><mi>i</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex"> \\mathbf{k}_{m &lt; i}, \\mathbf{v}_{m &lt; i} \\text{ , for } m \\in \\{0, \\ldots i - 1\\} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mord text"><span class="mord">¬†,¬†for¬†</span></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span>',hh,ch,Le,Iy=`In the following, we will tell the LLM to make use of the key-value cache by retrieving and forwarding it for each forward pass.
In Transformers, we can retrieve the key-value cache by passing the <code>use_cache</code> flag to the <code>forward</code> call and can then pass it with the current token.`,uh,Ge,dh,Be,Wy="<strong>Output</strong>:",gh,$e,yh,Ze,Qy="As one can see, when using the key-value cache the text input tokens are <em>not</em> increased in length, but remain a single input vector. The length of the key-value cache on the other hand is increased by one at every decoding step.",fh,Pn,R,Qu,wh,$w='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',bh,vh,Zw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi mathvariant="bold">K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_c\\mathbf{K}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Mh,xh,Iw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_c </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',kh,Kn,Hy="always",Hu,Th,Ie,Xy="Using the key-value cache has two advantages:",jh,vs,We,Xu,_h,Ww='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Jh,Nu,Dn,Ny="The maximum required memory is not increased quadratically with the number of generated tokens, but only increases linearly.",Uh,Qe,Vy='<p>One should <em>always</em> make use of the key-value cache as it leads to identical results and a significant speed-up for longer input sequences. Transformers has the key-value cache enabled by default when making use of the text pipeline or the <a href="https://huggingface.co/docs/transformers/main_classes/text_generation" rel="nofollow"><code>generate</code> method</a>. We have an entire guide dedicated to caches <a href="./kv_cache">here</a>.</p>',Ch,Ms,zh,He,Lh,Xe,Ry="The key-value cache is especially useful for applications such as chat where multiple passes of auto-regressive decoding are required. Let‚Äôs look at an example.",Gh,Ne,Bh,Ve,qy="In this chat, the LLM runs auto-regressive decoding twice:",$h,Re,Ay="<li>The first time, the key-value cache is empty and the input prompt is <code>&quot;User: How many people live in France?&quot;</code> and the model auto-regressively generates the text <code>&quot;Roughly 75 million people live in France&quot;</code> while increasing the key-value cache at every decoding step.</li> <li>The second time the input prompt is <code>&quot;User: How many people live in France? \\n Assistant: Roughly 75 million people live in France \\n User: And how many in Germany?&quot;</code>. Thanks to the cache, all key-value vectors for the first two sentences are already computed. Therefore the input prompt only consists of <code>&quot;User: And how many in Germany?&quot;</code>. While processing the shortened input prompt, its computed key-value vectors are concatenated to the key-value cache of the first decoding. The second Assistant‚Äôs answer <code>&quot;Germany has ca. 81 million inhabitants&quot;</code> is then auto-regressively generated with the key-value cache consisting of encoded key-value vectors of <code>&quot;User: How many people live in France? \\n Assistant: Roughly 75 million people live in France \\n User: And how many are in Germany?&quot;</code>.</li>",Zh,qe,Ey="Two things should be noted here:",Ih,Ae,Yy="<li>Keeping all the context is crucial for LLMs deployed in chat so that the LLM understands all the previous context of the conversation. E.g. for the example above the LLM needs to understand that the user refers to the population when asking <code>&quot;And how many are in Germany&quot;</code>.</li> <li>The key-value cache is extremely useful for chat as it allows us to continuously grow the encoded chat history instead of having to re-encode the chat history again from scratch (as e.g. would be the case when using an encoder-decoder architecture).</li>",Wh,Ee,Sy="In <code>transformers</code>, a <code>generate</code> call will return <code>past_key_values</code> when <code>return_dict_in_generate=True</code> is passed, in addition to the default <code>use_cache=True</code>. Note that it is not yet available through the <code>pipeline</code> interface.",Qh,Ye,Hh,Se,Fy="<strong>Output</strong>:",Xh,Fe,Nh,ts,Vu,Vh,Qw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',Rh,qh,Hw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mtext>,¬†for¬†</mtext><mi>i</mi><mo>‚àà</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><mi>c</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex"> \\mathbf{x}_i \\text{, for } i \\in \\{1, \\ldots, c - 1\\} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord text"><span class="mord">,¬†for¬†</span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">‚Ä¶</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span>',Ah,Eh,Pe,Py=`Let‚Äôs compute the number of float values that need to be stored in the key-value cache for the LLM <code>bigcode/octocoder</code> that we used before.
The number of float values amounts to two times the sequence length times the number of attention heads times the attention head dimension and times the number of layers.
Computing this for our LLM at a hypothetical input sequence length of 16000 gives:`,Yh,Ke,Sh,De,Ky="<strong>Output</strong>:",Fh,Oe,Ph,sn,Dy=`Roughly 8 billion float values! Storing 8 billion float values in <code>float16</code> precision requires around 15 GB of RAM which is circa half as much as the model weights themselves!
Researchers have proposed two methods that allow to significantly reduce the memory cost of storing the key-value cache, which are explored in the next subsections.`,Kh,tn,Dh,an,Oy='<a href="https://huggingface.co/papers/1911.02150" rel="nofollow">Multi-Query-Attention</a> was proposed in Noam Shazeer‚Äôs <em>Fast Transformer Decoding: One Write-Head is All You Need</em> paper. As the title says, Noam found out that instead of using <code>n_head</code> key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model‚Äôs performance significantly degrades.',Oh,On,rs,Ru,sc,Xw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \\mathbf{k}_i, \\mathbf{v}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',tc,sl,sf="n_head",qu,ac,en,tf="As most LLMs use between 20 and 100 attention heads, MQA significantly reduces the memory consumption of the key-value cache. For the LLM used in this notebook we could therefore reduce the required memory consumption from 15 GB to less than 400 MB at an input sequence length of 16000.",ec,as,Au,nc,Nw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>c</mi></msub><msup><mi mathvariant="bold">K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{q}_c\\mathbf{K}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',lc,xs,af="Noam‚Äôs paper",Eu,ic,ks,Yu,pc,Vw='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">K</mi></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> \\mathbf{QK}^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1118em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">QK</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9173em;"><span style="top:-3.139em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>',mc,oc,nn,ef="MQA has seen wide adoption by the community and is now used by many of the most popular LLMs:",rc,ln,nf='<li><a href="https://huggingface.co/tiiuae/falcon-40b" rel="nofollow"><strong>Falcon</strong></a></li> <li><a href="https://huggingface.co/papers/2204.02311" rel="nofollow"><strong>PaLM</strong></a></li> <li><a href="https://huggingface.co/mosaicml/mpt-30b" rel="nofollow"><strong>MPT</strong></a></li> <li><a href="https://huggingface.co/bigscience/bloom" rel="nofollow"><strong>BLOOM</strong></a></li>',hc,pn,lf="Also, the checkpoint used in this notebook - <code>bigcode/octocoder</code> - makes use of MQA.",cc,mn,uc,on,pf='<a href="https://huggingface.co/papers/2305.13245" rel="nofollow">Grouped-Query-Attention</a>, as proposed by Ainslie et al. from Google, found that using MQA can often lead to quality degradation compared to using vanilla multi-key-value head projections. The paper argues that more model performance can be kept by less drastically reducing the number of query head projection weights. Instead of using just a single key-value projection weight, <code>n &lt; n_head</code> key-value projection weights should be used. By choosing <code>n</code> to a significantly smaller value than <code>n_head</code>, such as 2,4 or 8 almost all of the memory and speed gains from MQA can be kept while sacrificing less model capacity and thus arguably less performance.',dc,rn,mf="Moreover, the authors of GQA found out that existing model checkpoints can be <em>uptrained</em> to have a GQA architecture with as little as 5% of the original pre-training compute. While 5% of the original pre-training compute can still be a massive amount, GQA <em>uptraining</em> allows existing checkpoints to be useful for longer input sequences.",gc,hn,of=`GQA was only recently proposed which is why there is less adoption at the time of writing this notebook.
The most notable application of GQA is <a href="https://huggingface.co/meta-llama/Llama-2-70b-hf" rel="nofollow">Llama-v2</a>.`,yc,cn,rf="<p>As a conclusion, it is strongly recommended to make use of either GQA or MQA if the LLM is deployed with auto-regressive decoding and is required to handle large input sequences as is the case for example for chat.</p>",fc,un,wc,dn,hf='The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is <a href="https://huggingface.co/papers/2211.17192" rel="nofollow">speculative decoding</a> where ‚Äúeasy tokens‚Äù are generated by smaller, faster language models and only ‚Äúhard tokens‚Äù are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this <a href="https://huggingface.co/blog/assisted-generation" rel="nofollow">nice blog post</a>.',bc,gn,cf=`The reason massive LLMs such as GPT3/4, Llama-2-70b, Claude, PaLM can run so quickly in chat-interfaces such as <a href="https://huggingface.co/chat/" rel="nofollow">Hugging Face Chat</a> or ChatGPT is to a big part thanks to the above-mentioned improvements in precision, algorithms, and architecture.
Going forward, accelerators such as GPUs, TPUs, etc‚Ä¶ will only get faster and allow for more memory, but one should nevertheless always make sure to use the best available algorithms and architectures to get the most bang for your buck ü§ó`,vc,yn,Mc,rl,xc;return zs=new is({props:{title:"Optimizing LLMs for Speed and Memory",local:"optimizing-llms-for-speed-and-memory",headingTag:"h1"}}),Ls=new Pw({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/llm_tutorial_optimization.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/llm_tutorial_optimization.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/llm_tutorial_optimization.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/llm_tutorial_optimization.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/llm_tutorial_optimization.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/llm_tutorial_optimization.ipynb"}]}}),Qs=new is({props:{title:"1. Lower Precision",local:"1-lower-precision",headingTag:"h2"}}),Ks=new v({props:{code:"IXBpcCUyMGluc3RhbGwlMjB0cmFuc2Zvcm1lcnMlMjBhY2NlbGVyYXRlJTIwYml0c2FuZGJ5dGVzJTIwb3B0aW11bQ==",highlighted:"!pip install transformers accelerate bitsandbytes optimum",wrap:!1}}),Ds=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnc2NpZW5jZSUyRmJsb29tJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBwYWRfdG9rZW5faWQlM0QwKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, pad_token_id=<span class="hljs-number">0</span>)`,wrap:!1}}),et=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMHBpcGVsaW5lJTBBaW1wb3J0JTIwdG9yY2glMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJiaWdjb2RlJTJGb2N0b2NvZGVyJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwcGFkX3Rva2VuX2lkJTNEMCklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJiaWdjb2RlJTJGb2N0b2NvZGVyJTIyKSUwQSUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline
<span class="hljs-keyword">import</span> torch

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>, pad_token_id=<span class="hljs-number">0</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>)

pipe = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=model, tokenizer=tokenizer)`,wrap:!1}}),nt=new v({props:{code:"cHJvbXB0JTIwJTNEJTIwJTIyUXVlc3Rpb24lM0ElMjBQbGVhc2UlMjB3cml0ZSUyMGElMjBmdW5jdGlvbiUyMGluJTIwUHl0aG9uJTIwdGhhdCUyMHRyYW5zZm9ybXMlMjBieXRlcyUyMHRvJTIwR2lnYSUyMGJ5dGVzLiU1Q24lNUNuQW5zd2VyJTNBJTIyJTBBJTBBcmVzdWx0JTIwJTNEJTIwcGlwZShwcm9tcHQlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDYwKSU1QjAlNUQlNUIlMjJnZW5lcmF0ZWRfdGV4dCUyMiU1RCU1Qmxlbihwcm9tcHQpJTNBJTVEJTBBcmVzdWx0",highlighted:`prompt = <span class="hljs-string">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:&quot;</span>

result = pipe(prompt, max_new_tokens=<span class="hljs-number">60</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][<span class="hljs-built_in">len</span>(prompt):]
result`,wrap:!1}}),it=new v({props:{code:"SGVyZSUyMGlzJTIwYSUyMFB5dGhvbiUyMGZ1bmN0aW9uJTIwdGhhdCUyMHRyYW5zZm9ybXMlMjBieXRlcyUyMHRvJTIwR2lnYSUyMGJ5dGVzJTNBJTVDbiU1Q24lNjAlNjAlNjBweXRob24lNUNuZGVmJTIwYnl0ZXNfdG9fZ2lnYV9ieXRlcyhieXRlcyklM0ElNUNuJTIwJTIwJTIwJTIwcmV0dXJuJTIwYnl0ZXMlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCUyMCUyRiUyMDEwMjQlNUNuJTYwJTYwJTYwJTVDbiU1Q25UaGlzJTIwZnVuY3Rpb24lMjB0YWtlcyUyMGElMjBzaW5nbGU=",highlighted:'Here <span class="hljs-keyword">is</span> a Python <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> transforms bytes <span class="hljs-keyword">to</span> Giga bytes:<span class="hljs-string">\\n\\n```python\\ndef</span> bytes_to_giga_bytes(bytes):<span class="hljs-string">\\n</span>    <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span><span class="hljs-string">\\n```\\n\\nThis</span> <span class="hljs-keyword">function</span> takes a single',wrap:!1}}),mt=new v({props:{code:"ZGVmJTIwYnl0ZXNfdG9fZ2lnYV9ieXRlcyhieXRlcyklM0ElMEElMjAlMjByZXR1cm4lMjBieXRlcyUyMCUyRiUyMDEwMjQlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNA==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">bytes_to_giga_bytes</span>(<span class="hljs-params"><span class="hljs-built_in">bytes</span></span>):
  <span class="hljs-keyword">return</span> <span class="hljs-built_in">bytes</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span>`,wrap:!1}}),rt=new v({props:{code:"Ynl0ZXNfdG9fZ2lnYV9ieXRlcyh0b3JjaC5jdWRhLm1heF9tZW1vcnlfYWxsb2NhdGVkKCkp",highlighted:"bytes_to_giga_bytes(torch.cuda.max_memory_allocated())",wrap:!1}}),ct=new v({props:{code:"MjkuMDI2MDY0ODcyNzQxNw==",highlighted:"29.0260648727417",wrap:!1}}),ft=new v({props:{code:"ZGVsJTIwcGlwZSUwQWRlbCUyMG1vZGVsJTBBJTBBaW1wb3J0JTIwZ2MlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWRlZiUyMGZsdXNoKCklM0ElMEElMjAlMjBnYy5jb2xsZWN0KCklMEElMjAlMjB0b3JjaC5jdWRhLmVtcHR5X2NhY2hlKCklMEElMjAlMjB0b3JjaC5jdWRhLnJlc2V0X3BlYWtfbWVtb3J5X3N0YXRzKCk=",highlighted:`<span class="hljs-keyword">del</span> pipe
<span class="hljs-keyword">del</span> model

<span class="hljs-keyword">import</span> gc
<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">flush</span>():
  gc.collect()
  torch.cuda.empty_cache()
  torch.cuda.reset_peak_memory_stats()`,wrap:!1}}),bt=new v({props:{code:"Zmx1c2goKQ==",highlighted:"flush()",wrap:!1}}),Mt=new v({props:{code:"ZnJvbSUyMGFjY2VsZXJhdGUudXRpbHMlMjBpbXBvcnQlMjByZWxlYXNlX21lbW9yeSUwQSUyMyUyMC4uLiUwQSUwQXJlbGVhc2VfbWVtb3J5KG1vZGVsKQ==",highlighted:`<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> release_memory
<span class="hljs-comment"># ...</span>

release_memory(model)`,wrap:!1}}),Ct=new v({props:{code:"IXBpcCUyMGluc3RhbGwlMjBiaXRzYW5kYnl0ZXM=",highlighted:"!pip install bitsandbytes",wrap:!1}}),Lt=new v({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnY29kZSUyRm9jdG9jb2RlciUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUlMkMlMjBwYWRfdG9rZW5faWQlM0QwKQ==",highlighted:'model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, pad_token_id=<span class="hljs-number">0</span>)',wrap:!1}}),Bt=new v({props:{code:"cGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnRleHQtZ2VuZXJhdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBJTBBcmVzdWx0JTIwJTNEJTIwcGlwZShwcm9tcHQlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDYwKSU1QjAlNUQlNUIlMjJnZW5lcmF0ZWRfdGV4dCUyMiU1RCU1Qmxlbihwcm9tcHQpJTNBJTVEJTBBcmVzdWx0",highlighted:`pipe = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=<span class="hljs-number">60</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][<span class="hljs-built_in">len</span>(prompt):]
result`,wrap:!1}}),Zt=new v({props:{code:"SGVyZSUyMGlzJTIwYSUyMFB5dGhvbiUyMGZ1bmN0aW9uJTIwdGhhdCUyMHRyYW5zZm9ybXMlMjBieXRlcyUyMHRvJTIwR2lnYSUyMGJ5dGVzJTNBJTVDbiU1Q24lNjAlNjAlNjBweXRob24lNUNuZGVmJTIwYnl0ZXNfdG9fZ2lnYV9ieXRlcyhieXRlcyklM0ElNUNuJTIwJTIwJTIwJTIwcmV0dXJuJTIwYnl0ZXMlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCUyMCUyRiUyMDEwMjQlNUNuJTYwJTYwJTYwJTVDbiU1Q25UaGlzJTIwZnVuY3Rpb24lMjB0YWtlcyUyMGElMjBzaW5nbGU=",highlighted:'Here <span class="hljs-keyword">is</span> a Python <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> transforms bytes <span class="hljs-keyword">to</span> Giga bytes:<span class="hljs-string">\\n\\n```python\\ndef</span> bytes_to_giga_bytes(bytes):<span class="hljs-string">\\n</span>    <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span><span class="hljs-string">\\n```\\n\\nThis</span> <span class="hljs-keyword">function</span> takes a single',wrap:!1}}),Wt=new v({props:{code:"Ynl0ZXNfdG9fZ2lnYV9ieXRlcyh0b3JjaC5jdWRhLm1heF9tZW1vcnlfYWxsb2NhdGVkKCkp",highlighted:"bytes_to_giga_bytes(torch.cuda.max_memory_allocated())",wrap:!1}}),Ht=new v({props:{code:"MTUuMjE5MjM0NDY2NTUyNzM0",highlighted:'<span class="hljs-attribute">15</span>.<span class="hljs-number">219234466552734</span>',wrap:!1}}),Vt=new v({props:{code:"ZGVsJTIwbW9kZWwlMEFkZWwlMjBwaXBl",highlighted:`<span class="hljs-keyword">del</span> model
<span class="hljs-keyword">del</span> pipe`,wrap:!1}}),Rt=new v({props:{code:"Zmx1c2goKQ==",highlighted:"flush()",wrap:!1}}),At=new v({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnY29kZSUyRm9jdG9jb2RlciUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUlMkMlMjBwYWRfdG9rZW5faWQlM0QwKSUwQSUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQSUwQXJlc3VsdCUyMCUzRCUyMHBpcGUocHJvbXB0JTJDJTIwbWF4X25ld190b2tlbnMlM0Q2MCklNUIwJTVEJTVCJTIyZ2VuZXJhdGVkX3RleHQlMjIlNUQlNUJsZW4ocHJvbXB0KSUzQSU1RCUwQXJlc3VsdA==",highlighted:`model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>, pad_token_id=<span class="hljs-number">0</span>)

pipe = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=<span class="hljs-number">60</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][<span class="hljs-built_in">len</span>(prompt):]
result`,wrap:!1}}),Yt=new v({props:{code:"SGVyZSUyMGlzJTIwYSUyMFB5dGhvbiUyMGZ1bmN0aW9uJTIwdGhhdCUyMHRyYW5zZm9ybXMlMjBieXRlcyUyMHRvJTIwR2lnYSUyMGJ5dGVzJTNBJTVDbiU1Q24lNjAlNjAlNjAlNUNuZGVmJTIwYnl0ZXNfdG9fZ2lnYWJ5dGVzKGJ5dGVzKSUzQSU1Q24lMjAlMjAlMjAlMjByZXR1cm4lMjBieXRlcyUyMCUyRiUyMDEwMjQlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCU1Q24lNjAlNjAlNjAlNUNuJTVDblRoaXMlMjBmdW5jdGlvbiUyMHRha2VzJTIwYSUyMHNpbmdsZSUyMGFyZ3VtZW50",highlighted:'Here <span class="hljs-keyword">is</span> a Python <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> transforms bytes <span class="hljs-keyword">to</span> Giga bytes:<span class="hljs-string">\\n\\n```\\ndef</span> bytes_to_gigabytes(bytes):<span class="hljs-string">\\n</span>    <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span><span class="hljs-string">\\n```\\n\\nThis</span> <span class="hljs-keyword">function</span> takes a single argument',wrap:!1}}),Ft=new v({props:{code:"Ynl0ZXNfdG9fZ2lnYV9ieXRlcyh0b3JjaC5jdWRhLm1heF9tZW1vcnlfYWxsb2NhdGVkKCkp",highlighted:"bytes_to_giga_bytes(torch.cuda.max_memory_allocated())",wrap:!1}}),Kt=new v({props:{code:"OS41NDM1NzQzMzMxOTA5MTg=",highlighted:'<span class="hljs-attribute">9</span>.<span class="hljs-number">543574333190918</span>',wrap:!1}}),sa=new v({props:{code:"ZGVsJTIwbW9kZWwlMEFkZWwlMjBwaXBl",highlighted:`<span class="hljs-keyword">del</span> model
<span class="hljs-keyword">del</span> pipe`,wrap:!1}}),ta=new v({props:{code:"Zmx1c2goKQ==",highlighted:"flush()",wrap:!1}}),ma=new is({props:{title:"2. Flash Attention",local:"2-flash-attention",headingTag:"h2"}}),ba=new v({props:{code:"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",highlighted:`system_prompt = <span class="hljs-string">&quot;&quot;&quot;Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn&#x27;t entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn&#x27;t let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----

Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.

Answer: Sure. Here is a function that does that.

def alternating(list1, list2):
   results = []
   for i in range(len(list1)):
       results.append(list1[i])
       results.append(list2[i])
   return results

Question: Can you write some test cases for this function?

Answer: Sure, here are some tests.

assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]
assert alternating([True, False], [4, 5]) == [True, 4, False, 5]
assert alternating([], []) == []

Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.

Answer: Here is the modified function.

def alternating(list1, list2):
   results = []
   for i in range(min(len(list1), len(list2))):
       results.append(list1[i])
       results.append(list2[i])
   if len(list1) &gt; len(list2):
       results.extend(list1[i+1:])
   else:
       results.extend(list2[i+1:])
   return results

-----
&quot;&quot;&quot;</span>`,wrap:!1}}),Ma=new v({props:{code:"bG9uZ19wcm9tcHQlMjAlM0QlMjAxMCUyMColMjBzeXN0ZW1fcHJvbXB0JTIwJTJCJTIwcHJvbXB0",highlighted:'long_prompt = <span class="hljs-number">10</span> * system_prompt + prompt',wrap:!1}}),ka=new v({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnY29kZSUyRm9jdG9jb2RlciUyMiUyQyUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJiaWdjb2RlJTJGb2N0b2NvZGVyJTIyKSUwQSUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigcode/octocoder&quot;</span>)

pipe = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=model, tokenizer=tokenizer)`,wrap:!1}}),ja=new v({props:{code:"aW1wb3J0JTIwdGltZSUwQSUwQXN0YXJ0X3RpbWUlMjAlM0QlMjB0aW1lLnRpbWUoKSUwQXJlc3VsdCUyMCUzRCUyMHBpcGUobG9uZ19wcm9tcHQlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDYwKSU1QjAlNUQlNUIlMjJnZW5lcmF0ZWRfdGV4dCUyMiU1RCU1Qmxlbihsb25nX3Byb21wdCklM0ElNUQlMEElMEFwcmludChmJTIyR2VuZXJhdGVkJTIwaW4lMjAlN0J0aW1lLnRpbWUoKSUyMC0lMjBzdGFydF90aW1lJTdEJTIwc2Vjb25kcy4lMjIpJTBBcmVzdWx0",highlighted:`<span class="hljs-keyword">import</span> time

start_time = time.time()
result = pipe(long_prompt, max_new_tokens=<span class="hljs-number">60</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][<span class="hljs-built_in">len</span>(long_prompt):]

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated in <span class="hljs-subst">{time.time() - start_time}</span> seconds.&quot;</span>)
result`,wrap:!1}}),Ja=new v({props:{code:"R2VuZXJhdGVkJTIwaW4lMjAxMC45Njg1NDAxOTE2NTAzOSUyMHNlY29uZHMuJTBBU3VyZS4lMjBIZXJlJTIwaXMlMjBhJTIwZnVuY3Rpb24lMjB0aGF0JTIwZG9lcyUyMHRoYXQuJTVDbiU1Q25kZWYlMjBieXRlc190b19naWdhKGJ5dGVzKSUzQSU1Q24lMjAlMjAlMjByZXR1cm4lMjBieXRlcyUyMCUyRiUyMDEwMjQlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCU1Q24lNUNuQW5zd2VyJTNBJTIwU3VyZS4lMjBIZXJlJTIwaXMlMjBhJTIwZnVuY3Rpb24lMjB0aGF0JTIwZG9lcyUyMHRoYXQuJTVDbiU1Q25kZWY=",highlighted:`Generated <span class="hljs-keyword">in</span> <span class="hljs-number">10.96854019165039</span> seconds.
Sure. Here <span class="hljs-keyword">is</span> a <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> does <span class="hljs-literal">that</span>.<span class="hljs-string">\\n\\ndef</span> bytes_to_giga(bytes):<span class="hljs-string">\\n</span>   <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span><span class="hljs-string">\\n\\nAnswer:</span> Sure. Here <span class="hljs-keyword">is</span> a <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> does <span class="hljs-literal">that</span>.<span class="hljs-string">\\n\\ndef</span>`,wrap:!1}}),La=new v({props:{code:"Ynl0ZXNfdG9fZ2lnYV9ieXRlcyh0b3JjaC5jdWRhLm1heF9tZW1vcnlfYWxsb2NhdGVkKCkp",highlighted:"bytes_to_giga_bytes(torch.cuda.max_memory_allocated())",wrap:!1}}),Ba=new v({props:{code:"MzcuNjY4MTkzMzQwMzAxNTE0",highlighted:"37.668193340301514",wrap:!1}}),Ia=new v({props:{code:"Zmx1c2goKQ==",highlighted:"flush()",wrap:!1}}),Qa=new v({props:{code:"bW9kZWwudG9fYmV0dGVydHJhbnNmb3JtZXIoKQ==",highlighted:"model.to_bettertransformer()",wrap:!1}}),Xa=new v({props:{code:"c3RhcnRfdGltZSUyMCUzRCUyMHRpbWUudGltZSgpJTBBd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwcmVzdWx0JTIwJTNEJTIwcGlwZShsb25nX3Byb21wdCUyQyUyMG1heF9uZXdfdG9rZW5zJTNENjApJTVCMCU1RCU1QiUyMmdlbmVyYXRlZF90ZXh0JTIyJTVEJTVCbGVuKGxvbmdfcHJvbXB0KSUzQSU1RCUwQSUwQXByaW50KGYlMjJHZW5lcmF0ZWQlMjBpbiUyMCU3QnRpbWUudGltZSgpJTIwLSUyMHN0YXJ0X3RpbWUlN0QlMjBzZWNvbmRzLiUyMiklMEFyZXN1bHQ=",highlighted:`start_time = time.time()
<span class="hljs-keyword">with</span> torch.backends.cuda.sdp_kernel(enable_flash=<span class="hljs-literal">True</span>, enable_math=<span class="hljs-literal">False</span>, enable_mem_efficient=<span class="hljs-literal">False</span>):
    result = pipe(long_prompt, max_new_tokens=<span class="hljs-number">60</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>][<span class="hljs-built_in">len</span>(long_prompt):]

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated in <span class="hljs-subst">{time.time() - start_time}</span> seconds.&quot;</span>)
result`,wrap:!1}}),Va=new v({props:{code:"R2VuZXJhdGVkJTIwaW4lMjAzLjAyMTE2MTc5NDY2MjQ3NTYlMjBzZWNvbmRzLiUwQSUyMFN1cmUuJTIwSGVyZSUyMGlzJTIwYSUyMGZ1bmN0aW9uJTIwdGhhdCUyMGRvZXMlMjB0aGF0LiU1Q24lNUNuZGVmJTIwYnl0ZXNfdG9fZ2lnYShieXRlcyklM0ElNUNuJTIwJTIwJTIwcmV0dXJuJTIwYnl0ZXMlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCUyMCUyRiUyMDEwMjQlNUNuJTVDbkFuc3dlciUzQSUyMFN1cmUuJTIwSGVyZSUyMGlzJTIwYSUyMGZ1bmN0aW9uJTIwdGhhdCUyMGRvZXMlMjB0aGF0LiU1Q24lNUNuZGVm",highlighted:`Generated <span class="hljs-keyword">in</span> <span class="hljs-number">3.0211617946624756</span> seconds.
 Sure. Here <span class="hljs-keyword">is</span> a <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> does <span class="hljs-literal">that</span>.<span class="hljs-string">\\n\\ndef</span> bytes_to_giga(bytes):<span class="hljs-string">\\n</span>   <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span><span class="hljs-string">\\n\\nAnswer:</span> Sure. Here <span class="hljs-keyword">is</span> a <span class="hljs-keyword">function</span> <span class="hljs-literal">that</span> does <span class="hljs-literal">that</span>.<span class="hljs-string">\\n\\ndef</span>`,wrap:!1}}),Aa=new v({props:{code:"Ynl0ZXNfdG9fZ2lnYV9ieXRlcyh0b3JjaC5jdWRhLm1heF9tZW1vcnlfYWxsb2NhdGVkKCkp",highlighted:"bytes_to_giga_bytes(torch.cuda.max_memory_allocated())",wrap:!1}}),Ya=new v({props:{code:"MzIuNjE3MzMxOTgxNjU4OTM2",highlighted:'<span class="hljs-attribute">32</span>.<span class="hljs-number">617331981658936</span>',wrap:!1}}),Pa=new v({props:{code:"Zmx1c2goKQ==",highlighted:"flush()",wrap:!1}}),Da=new is({props:{title:"3. Architectural Innovations",local:"3-architectural-innovations",headingTag:"h2"}}),pe=new is({props:{title:"3.1 Improving positional embeddings of LLMs",local:"31-improving-positional-embeddings-of-llms",headingTag:"h3"}}),xe=new is({props:{title:"3.2 The key-value cache",local:"32-the-key-value-cache",headingTag:"h3"}}),_e=new v({props:{code:"aW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVELnRvKCUyMmN1ZGElMjIpJTBBJTBBZm9yJTIwXyUyMGluJTIwcmFuZ2UoNSklM0ElMEElMjAlMjBuZXh0X2xvZ2l0cyUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcyklNUIlMjJsb2dpdHMlMjIlNUQlNUIlM0ElMkMlMjAtMSUzQSU1RCUwQSUyMCUyMG5leHRfdG9rZW5faWQlMjAlM0QlMjB0b3JjaC5hcmdtYXgobmV4dF9sb2dpdHMlMkNkaW0lM0QtMSklMEElMEElMjAlMjBpbnB1dF9pZHMlMjAlM0QlMjB0b3JjaC5jYXQoJTVCaW5wdXRfaWRzJTJDJTIwbmV4dF90b2tlbl9pZCU1RCUyQyUyMGRpbSUzRC0xKSUwQSUyMCUyMHByaW50KCUyMnNoYXBlJTIwb2YlMjBpbnB1dF9pZHMlMjIlMkMlMjBpbnB1dF9pZHMuc2hhcGUpJTBBJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGlucHV0X2lkcyU1QiUzQSUyQyUyMC01JTNBJTVEKSUwQWdlbmVyYXRlZF90ZXh0",highlighted:`input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):
  next_logits = model(input_ids)[<span class="hljs-string">&quot;logits&quot;</span>][:, -<span class="hljs-number">1</span>:]
  next_token_id = torch.argmax(next_logits,dim=-<span class="hljs-number">1</span>)

  input_ids = torch.cat([input_ids, next_token_id], dim=-<span class="hljs-number">1</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;shape of input_ids&quot;</span>, input_ids.shape)

generated_text = tokenizer.batch_decode(input_ids[:, -<span class="hljs-number">5</span>:])
generated_text`,wrap:!1}}),Ue=new v({props:{code:"c2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDIxJTVEKSUwQXNoYXBlJTIwb2YlMjBpbnB1dF9pZHMlMjB0b3JjaC5TaXplKCU1QjElMkMlMjAyMiU1RCklMEFzaGFwZSUyMG9mJTIwaW5wdXRfaWRzJTIwdG9yY2guU2l6ZSglNUIxJTJDJTIwMjMlNUQpJTBBc2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDI0JTVEKSUwQXNoYXBlJTIwb2YlMjBpbnB1dF9pZHMlMjB0b3JjaC5TaXplKCU1QjElMkMlMjAyNSU1RCklMEElNUInJTIwSGVyZSUyMGlzJTIwYSUyMFB5dGhvbiUyMGZ1bmN0aW9uJyU1RA==",highlighted:`shape of input_ids torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 21]</span>)
shape of input_ids torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 22]</span>)
shape of input_ids torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 23]</span>)
shape of input_ids torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 24]</span>)
shape of input_ids torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 25]</span>)
<span class="hljs-selector-attr">[<span class="hljs-string">&#x27; Here is a Python function&#x27;</span>]</span>`,wrap:!1}}),Ge=new v({props:{code:"cGFzdF9rZXlfdmFsdWVzJTIwJTNEJTIwTm9uZSUyMCUyMyUyMHBhc3Rfa2V5X3ZhbHVlcyUyMGlzJTIwdGhlJTIwa2V5LXZhbHVlJTIwY2FjaGUlMEFnZW5lcmF0ZWRfdG9rZW5zJTIwJTNEJTIwJTVCJTVEJTBBbmV4dF90b2tlbl9pZCUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RC50byglMjJjdWRhJTIyKSUwQSUwQWZvciUyMF8lMjBpbiUyMHJhbmdlKDUpJTNBJTBBJTIwJTIwbmV4dF9sb2dpdHMlMkMlMjBwYXN0X2tleV92YWx1ZXMlMjAlM0QlMjBtb2RlbChuZXh0X3Rva2VuX2lkJTJDJTIwcGFzdF9rZXlfdmFsdWVzJTNEcGFzdF9rZXlfdmFsdWVzJTJDJTIwdXNlX2NhY2hlJTNEVHJ1ZSkudG9fdHVwbGUoKSUwQSUyMCUyMG5leHRfbG9naXRzJTIwJTNEJTIwbmV4dF9sb2dpdHMlNUIlM0ElMkMlMjAtMSUzQSU1RCUwQSUyMCUyMG5leHRfdG9rZW5faWQlMjAlM0QlMjB0b3JjaC5hcmdtYXgobmV4dF9sb2dpdHMlMkMlMjBkaW0lM0QtMSklMEElMEElMjAlMjBwcmludCglMjJzaGFwZSUyMG9mJTIwaW5wdXRfaWRzJTIyJTJDJTIwbmV4dF90b2tlbl9pZC5zaGFwZSklMEElMjAlMjBwcmludCglMjJsZW5ndGglMjBvZiUyMGtleS12YWx1ZSUyMGNhY2hlJTIyJTJDJTIwbGVuKHBhc3Rfa2V5X3ZhbHVlcyU1QjAlNUQlNUIwJTVEKSklMjAlMjAlMjMlMjBwYXN0X2tleV92YWx1ZXMlMjBhcmUlMjBvZiUyMHNoYXBlJTIwJTVCbnVtX2xheWVycyUyQyUyMDAlMjBmb3IlMjBrJTJDJTIwMSUyMGZvciUyMHYlMkMlMjBiYXRjaF9zaXplJTJDJTIwbGVuZ3RoJTJDJTIwaGlkZGVuX2RpbSU1RCUwQSUyMCUyMGdlbmVyYXRlZF90b2tlbnMuYXBwZW5kKG5leHRfdG9rZW5faWQuaXRlbSgpKSUwQSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfdG9rZW5zKSUwQWdlbmVyYXRlZF90ZXh0",highlighted:`past_key_values = <span class="hljs-literal">None</span> <span class="hljs-comment"># past_key_values is the key-value cache</span>
generated_tokens = []
next_token_id = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>].to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):
  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=<span class="hljs-literal">True</span>).to_tuple()
  next_logits = next_logits[:, -<span class="hljs-number">1</span>:]
  next_token_id = torch.argmax(next_logits, dim=-<span class="hljs-number">1</span>)

  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;shape of input_ids&quot;</span>, next_token_id.shape)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;length of key-value cache&quot;</span>, <span class="hljs-built_in">len</span>(past_key_values[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))  <span class="hljs-comment"># past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]</span>
  generated_tokens.append(next_token_id.item())

generated_text = tokenizer.batch_decode(generated_tokens)
generated_text`,wrap:!1}}),$e=new v({props:{code:"c2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDElNUQpJTBBbGVuZ3RoJTIwb2YlMjBrZXktdmFsdWUlMjBjYWNoZSUyMDIwJTBBc2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDElNUQpJTBBbGVuZ3RoJTIwb2YlMjBrZXktdmFsdWUlMjBjYWNoZSUyMDIxJTBBc2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDElNUQpJTBBbGVuZ3RoJTIwb2YlMjBrZXktdmFsdWUlMjBjYWNoZSUyMDIyJTBBc2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDElNUQpJTBBbGVuZ3RoJTIwb2YlMjBrZXktdmFsdWUlMjBjYWNoZSUyMDIzJTBBc2hhcGUlMjBvZiUyMGlucHV0X2lkcyUyMHRvcmNoLlNpemUoJTVCMSUyQyUyMDElNUQpJTBBbGVuZ3RoJTIwb2YlMjBrZXktdmFsdWUlMjBjYWNoZSUyMDI0JTBBJTVCJyUyMEhlcmUnJTJDJTIwJyUyMGlzJyUyQyUyMCclMjBhJyUyQyUyMCclMjBQeXRob24nJTJDJTIwJyUyMGZ1bmN0aW9uJyU1RA==",highlighted:`<span class="hljs-attribute">shape</span> of input_ids torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
<span class="hljs-attribute">length</span> of key-value cache <span class="hljs-number">20</span>
<span class="hljs-attribute">shape</span> of input_ids torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
<span class="hljs-attribute">length</span> of key-value cache <span class="hljs-number">21</span>
<span class="hljs-attribute">shape</span> of input_ids torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
<span class="hljs-attribute">length</span> of key-value cache <span class="hljs-number">22</span>
<span class="hljs-attribute">shape</span> of input_ids torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
<span class="hljs-attribute">length</span> of key-value cache <span class="hljs-number">23</span>
<span class="hljs-attribute">shape</span> of input_ids torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
<span class="hljs-attribute">length</span> of key-value cache <span class="hljs-number">24</span><span class="hljs-meta">
[&#x27; Here&#x27;, &#x27; is&#x27;, &#x27; a&#x27;, &#x27; Python&#x27;, &#x27; function&#x27;]</span>`,wrap:!1}}),Ms=new Fw({props:{warning:!0,$$slots:{default:[Dw]},$$scope:{ctx:cl}}}),He=new is({props:{title:"3.2.1 Multi-round conversation",local:"321-multi-round-conversation",headingTag:"h4"}}),Ne=new v({props:{code:"VXNlciUzQSUyMEhvdyUyMG1hbnklMjBwZW9wbGUlMjBsaXZlJTIwaW4lMjBGcmFuY2UlM0YlMEFBc3Npc3RhbnQlM0ElMjBSb3VnaGx5JTIwNzUlMjBtaWxsaW9uJTIwcGVvcGxlJTIwbGl2ZSUyMGluJTIwRnJhbmNlJTBBVXNlciUzQSUyMEFuZCUyMGhvdyUyMG1hbnklMjBhcmUlMjBpbiUyMEdlcm1hbnklM0YlMEFBc3Npc3RhbnQlM0ElMjBHZXJtYW55JTIwaGFzJTIwY2EuJTIwODElMjBtaWxsaW9uJTIwaW5oYWJpdGFudHM=",highlighted:`<span class="hljs-symbol">User:</span> How many people live <span class="hljs-keyword">in</span> France?
<span class="hljs-symbol">Assistant:</span> Roughly <span class="hljs-number">75</span> million people live <span class="hljs-keyword">in</span> France
<span class="hljs-symbol">User:</span> <span class="hljs-keyword">And</span> how many are <span class="hljs-keyword">in</span> Germany?
<span class="hljs-symbol">Assistant:</span> Germany has ca. <span class="hljs-number">81</span> million inhabitants`,wrap:!1}}),Ye=new v({props:{code:"JTIzJTIwR2VuZXJhdGlvbiUyMGFzJTIwdXN1YWwlMEFwcm9tcHQlMjAlM0QlMjBzeXN0ZW1fcHJvbXB0JTIwJTJCJTIwJTIyUXVlc3Rpb24lM0ElMjBQbGVhc2UlMjB3cml0ZSUyMGElMjBmdW5jdGlvbiUyMGluJTIwUHl0aG9uJTIwdGhhdCUyMHRyYW5zZm9ybXMlMjBieXRlcyUyMHRvJTIwR2lnYSUyMGJ5dGVzLiU1Q24lNUNuQW5zd2VyJTNBJTIwSGVyZSUyMiUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCdwdCcpJTBBZ2VuZXJhdGlvbl9vdXRwdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENjAlMkMlMjByZXR1cm5fZGljdF9pbl9nZW5lcmF0ZSUzRFRydWUpJTBBZGVjb2RlZF9vdXRwdXQlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRpb25fb3V0cHV0LnNlcXVlbmNlcyklNUIwJTVEJTBBJTBBJTIzJTIwUGlwaW5nJTIwdGhlJTIwcmV0dXJuZWQlMjAlNjBwYXN0X2tleV92YWx1ZXMlNjAlMjB0byUyMHNwZWVkJTIwdXAlMjB0aGUlMjBuZXh0JTIwY29udmVyc2F0aW9uJTIwcm91bmQlMEFwcm9tcHQlMjAlM0QlMjBkZWNvZGVkX291dHB1dCUyMCUyQiUyMCUyMiU1Q25RdWVzdGlvbiUzQSUyMEhvdyUyMGNhbiUyMEklMjBtb2RpZnklMjB0aGUlMjBmdW5jdGlvbiUyMGFib3ZlJTIwdG8lMjByZXR1cm4lMjBNZWdhJTIwYnl0ZXMlMjBpbnN0ZWFkJTNGJTVDbiU1Q25BbnN3ZXIlM0ElMjBIZXJlJTIyJTBBbW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJ3B0JyklMEFnZW5lcmF0aW9uX291dHB1dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCUwQSUyMCUyMCoqbW9kZWxfaW5wdXRzJTJDJTBBJTIwJTIwcGFzdF9rZXlfdmFsdWVzJTNEZ2VuZXJhdGlvbl9vdXRwdXQucGFzdF9rZXlfdmFsdWVzJTJDJTBBJTIwJTIwbWF4X25ld190b2tlbnMlM0Q2MCUyQyUwQSUyMCUyMHJldHVybl9kaWN0X2luX2dlbmVyYXRlJTNEVHJ1ZSUwQSklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRpb25fb3V0cHV0LnNlcXVlbmNlcyklNUIwJTVEJTVCbGVuKHByb21wdCklM0ElNUQ=",highlighted:`<span class="hljs-comment"># Generation as usual</span>
prompt = system_prompt + <span class="hljs-string">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here&quot;</span>
model_inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)
generation_output = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">60</span>, return_dict_in_generate=<span class="hljs-literal">True</span>)
decoded_output = tokenizer.batch_decode(generation_output.sequences)[<span class="hljs-number">0</span>]

<span class="hljs-comment"># Piping the returned \`past_key_values\` to speed up the next conversation round</span>
prompt = decoded_output + <span class="hljs-string">&quot;\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here&quot;</span>
model_inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)
generation_output = model.generate(
  **model_inputs,
  past_key_values=generation_output.past_key_values,
  max_new_tokens=<span class="hljs-number">60</span>,
  return_dict_in_generate=<span class="hljs-literal">True</span>
)
tokenizer.batch_decode(generation_output.sequences)[<span class="hljs-number">0</span>][<span class="hljs-built_in">len</span>(prompt):]`,wrap:!1}}),Fe=new v({props:{code:"JTIwaXMlMjBhJTIwbW9kaWZpZWQlMjB2ZXJzaW9uJTIwb2YlMjB0aGUlMjBmdW5jdGlvbiUyMHRoYXQlMjByZXR1cm5zJTIwTWVnYSUyMGJ5dGVzJTIwaW5zdGVhZC4lMEElMEFkZWYlMjBieXRlc190b19tZWdhYnl0ZXMoYnl0ZXMpJTNBJTBBJTIwJTIwJTIwcmV0dXJuJTIwYnl0ZXMlMjAlMkYlMjAxMDI0JTIwJTJGJTIwMTAyNCUwQSUwQUFuc3dlciUzQSUyMFRoZSUyMGZ1bmN0aW9uJTIwdGFrZXMlMjBhJTIwbnVtYmVyJTIwb2YlMjBieXRlcyUyMGFzJTIwaW5wdXQlMjBhbmQlMjByZXR1cm5zJTIwdGhlJTIwbnVtYmVyJTIwb2Y=",highlighted:` <span class="hljs-keyword">is</span> a modified <span class="hljs-keyword">version</span> <span class="hljs-keyword">of</span> the <span class="hljs-keyword">function</span> that <span class="hljs-keyword">returns</span> Mega bytes <span class="hljs-keyword">instead</span>.

def bytes_to_megabytes(bytes):
   <span class="hljs-keyword">return</span> bytes / <span class="hljs-number">1024</span> / <span class="hljs-number">1024</span>

Answer: The <span class="hljs-keyword">function</span> takes a number <span class="hljs-keyword">of</span> bytes <span class="hljs-keyword">as</span> <span class="hljs-keyword">input</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">returns</span> the number <span class="hljs-keyword">of</span>`,wrap:!1}}),Ke=new v({props:{code:"Y29uZmlnJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBMiUyMColMjAxNl8wMDAlMjAqJTIwY29uZmlnLm5fbGF5ZXIlMjAqJTIwY29uZmlnLm5faGVhZCUyMColMjBjb25maWcubl9lbWJkJTIwJTJGJTJGJTIwY29uZmlnLm5faGVhZA==",highlighted:`config = model.config
<span class="hljs-number">2</span> * <span class="hljs-number">16_000</span> * config.n_layer * config.n_head * config.n_embd // config.n_head`,wrap:!1}}),Oe=new v({props:{code:"Nzg2NDMyMDAwMA==",highlighted:'<span class="hljs-number">7864320000</span>',wrap:!1}}),tn=new is({props:{title:"3.2.2 Multi-Query-Attention (MQA)",local:"322-multi-query-attention-mqa",headingTag:"h4"}}),mn=new is({props:{title:"3.2.3 Grouped-Query-Attention (GQA)",local:"323-grouped-query-attention-gqa",headingTag:"h4"}}),un=new is({props:{title:"Conclusion",local:"conclusion",headingTag:"h2"}}),yn=new Kw({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md"}}),{c(){G=i("meta"),Cs=n(),P=i("p"),fn=n(),d(zs.$$.fragment),ul=n(),d(Ls.$$.fragment),dl=n(),Gs=i("p"),Gs.innerHTML=Fu,gl=n(),Bs=i("ul"),Bs.innerHTML=Pu,yl=n(),$s=i("p"),$s.textContent=Ku,fl=n(),Zs=i("p"),Zs.textContent=Du,wl=n(),Is=i("ol"),Is.innerHTML=Ou,bl=n(),Ws=i("p"),Ws.textContent=sd,vl=n(),d(Qs.$$.fragment),Ml=n(),Hs=i("p"),Hs.innerHTML=td,xl=n(),Xs=i("p"),Xs.innerHTML=ad,kl=n(),Ns=i("blockquote"),Ns.innerHTML=ed,Tl=n(),Vs=i("p"),Vs.textContent=nd,jl=n(),Rs=i("blockquote"),Rs.innerHTML=ld,_l=n(),qs=i("p"),qs.textContent=id,Jl=n(),As=i("p"),As.textContent=pd,Ul=n(),Es=i("ul"),Es.innerHTML=md,Cl=n(),Ys=i("p"),Ys.innerHTML=od,zl=n(),Ss=i("p"),Ss.innerHTML=rd,Ll=n(),Fs=i("p"),Fs.innerHTML=hd,Gl=n(),Ps=i("p"),Ps.textContent=cd,Bl=n(),d(Ks.$$.fragment),$l=n(),d(Ds.$$.fragment),Zl=n(),Os=i("p"),Os.innerHTML=ud,Il=n(),st=i("p"),st.innerHTML=dd,Wl=n(),tt=i("p"),tt.innerHTML=gd,Ql=n(),at=i("p"),at.innerHTML=yd,Hl=n(),d(et.$$.fragment),Xl=n(),d(nt.$$.fragment),Nl=n(),lt=i("p"),lt.innerHTML=fd,Vl=n(),d(it.$$.fragment),Rl=n(),pt=i("p"),pt.textContent=wd,ql=n(),d(mt.$$.fragment),Al=n(),ot=i("p"),ot.innerHTML=bd,El=n(),d(rt.$$.fragment),Yl=n(),ht=i("p"),ht.innerHTML=vd,Sl=n(),d(ct.$$.fragment),Fl=n(),ut=i("p"),ut.textContent=Md,Pl=n(),dt=i("blockquote"),dt.innerHTML=xd,Kl=n(),gt=i("p"),gt.innerHTML=kd,Dl=n(),yt=i("p"),yt.innerHTML=Td,Ol=n(),d(ft.$$.fragment),si=n(),wt=i("p"),wt.textContent=jd,ti=n(),d(bt.$$.fragment),ai=n(),vt=i("p"),vt.innerHTML=_d,ei=n(),d(Mt.$$.fragment),ni=n(),xt=i("p"),xt.innerHTML=Jd,li=n(),kt=i("p"),kt.innerHTML=Ud,ii=n(),Tt=i("p"),Tt.textContent=Cd,pi=n(),jt=i("ul"),jt.innerHTML=zd,mi=n(),B=i("p"),zc=r("In a nutshell, this means that "),wn=i("em"),wn.textContent=Ld,Lc=r(" multiplications, with"),oi=new c(!1),ri=r(" being the "),bn=i("em"),bn.textContent=Gd,Gc=r(","),hi=new c(!1),ci=r(" being a weight matrix and"),ui=new c(!1),di=r(` being the output:
`),gi=new c(!1),yi=n(),_t=i("p"),Bc=r(`are changed to
`),fi=new c(!1),wi=n(),Jt=i("p"),Jt.textContent=Bd,bi=n(),Ut=i("p"),Ut.innerHTML=$d,vi=n(),d(Ct.$$.fragment),Mi=n(),zt=i("p"),zt.innerHTML=Zd,xi=n(),d(Lt.$$.fragment),ki=n(),Gt=i("p"),Gt.textContent=Id,Ti=n(),d(Bt.$$.fragment),ji=n(),$t=i("p"),$t.innerHTML=Wd,_i=n(),d(Zt.$$.fragment),Ji=n(),It=i("p"),It.textContent=Qd,Ui=n(),d(Wt.$$.fragment),Ci=n(),Qt=i("p"),Qt.innerHTML=Hd,zi=n(),d(Ht.$$.fragment),Li=n(),Xt=i("p"),Xt.textContent=Xd,Gi=n(),Nt=i("p"),Nt.textContent=Nd,Bi=n(),d(Vt.$$.fragment),$i=n(),d(Rt.$$.fragment),Zi=n(),qt=i("p"),qt.innerHTML=Vd,Ii=n(),d(At.$$.fragment),Wi=n(),Et=i("p"),Et.innerHTML=Rd,Qi=n(),d(Yt.$$.fragment),Hi=n(),St=i("p"),St.innerHTML=qd,Xi=n(),d(Ft.$$.fragment),Ni=n(),Pt=i("p"),Pt.innerHTML=Ad,Vi=n(),d(Kt.$$.fragment),Ri=n(),Dt=i("p"),Dt.textContent=Ed,qi=n(),Ot=i("p"),Ot.innerHTML=Yd,Ai=n(),K=i("p"),$c=r("Also note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to"),Ei=new c(!1),Yi=r(" and"),Si=new c(!1),Fi=r(" taking longer during inference."),Pi=n(),d(sa.$$.fragment),Ki=n(),d(ta.$$.fragment),Di=n(),aa=i("p"),aa.textContent=Sd,Oi=n(),ea=i("p"),ea.textContent=Fd,sp=n(),na=i("p"),na.innerHTML=Pd,tp=n(),la=i("blockquote"),la.innerHTML=Kd,ap=n(),ia=i("p"),ia.textContent=Dd,ep=n(),pa=i("p"),pa.innerHTML=Od,np=n(),d(ma.$$.fragment),lp=n(),oa=i("p"),oa.textContent=sg,ip=n(),q=i("p"),Zc=r(`Self-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens.
However, the peak GPU memory consumption for self-attention layers grows `),vn=i("em"),vn.textContent=tg,Ic=r(" both in compute and memory complexity with number of input tokens (also called "),Mn=i("em"),Mn.textContent=ag,Wc=r(") that we denote in the following by"),pp=new c(!1),mp=r(` .
While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens).`),op=n(),T=i("p"),Qc=r("Let‚Äôs take a closer look. The formula to compute the output"),rp=new c(!1),hp=r(" of a self-attention layer for an input"),cp=new c(!1),up=r(" of length"),dp=new c(!1),gp=r(` is:
`),yp=new c(!1),fp=n(),wp=new c(!1),bp=r(" is thereby the input sequence to the attention layer. The projections"),vp=new c(!1),Mp=r(" and"),xp=new c(!1),kp=r(" will each consist of"),Tp=new c(!1),jp=r(" vectors resulting in the"),_p=new c(!1),Jp=r(" being of size"),Up=new c(!1),Cp=r(" ."),zp=n(),U=i("p"),Hc=r(`LLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.
Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the`),Lp=new c(!1),Gp=r(" matrices to be"),Bp=new c(!1),$p=r(" bytes. For"),Zp=new c(!1),Ip=r(" only around 50 MB of VRAM are needed, however, for"),Wp=new c(!1),Qp=r(" we would need 19 GB of VRAM, and for"),Hp=new c(!1),Xp=r(" we would need almost 1TB just to store the"),Np=new c(!1),Vp=r(" matrices."),Rp=n(),ra=i("p"),ra.textContent=eg,qp=n(),ha=i("p"),ha.textContent=ng,Ap=n(),A=i("p"),Xc=r("How can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the"),Ep=new c(!1),Yp=r(" matrix. "),hs=i("a"),hs.textContent=lg,Nc=r(" developed exactly such a new algorithm and called it "),xn=i("strong"),xn.textContent=ig,Vc=r("."),Sp=n(),ps=i("p"),Rc=r("In a nutshell, Flash Attention breaks the "),Fp=new c(!1),Pp=r(`) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:
`),Kp=new c(!1),Dp=n(),W=i("p"),qc=r("with"),Op=new c(!1),sm=r(" and"),tm=new c(!1),am=r(" being some softmax normalization statistics that need to be recomputed for every"),em=new c(!1),nm=r(" and"),lm=new c(!1),im=r(" ."),pm=n(),ca=i("p"),ca.innerHTML=pg,mm=n(),ua=i("p"),ua.textContent=mg,om=n(),kn=i("blockquote"),ms=i("p"),Ac=r("By keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives "),Tn=i("strong"),Tn.textContent=og,Ec=r(" outputs compared to the default self-attention layer at a memory cost that only increases linearly with"),rm=new c(!1),hm=r(" ."),cm=n(),da=i("p"),da.innerHTML=rg,um=n(),ga=i("blockquote"),ga.innerHTML=hg,dm=n(),D=i("p"),Yc=r("Essentially, Flash Attention makes sure that all intermediate write and read operations can be done using the fast "),jn=i("em"),jn.textContent=cg,Sc=r(" SRAM memory instead of having to access the slower VRAM memory to compute the output vector"),gm=new c(!1),ym=r(" ."),fm=n(),ya=i("p"),ya.innerHTML=ug,wm=n(),fa=i("p"),fa.textContent=dg,bm=n(),wa=i("p"),wa.innerHTML=gg,vm=n(),d(ba.$$.fragment),Mm=n(),va=i("p"),va.innerHTML=yg,xm=n(),d(Ma.$$.fragment),km=n(),xa=i("p"),xa.textContent=fg,Tm=n(),d(ka.$$.fragment),jm=n(),Ta=i("p"),Ta.innerHTML=wg,_m=n(),d(ja.$$.fragment),Jm=n(),_a=i("p"),_a.innerHTML=bg,Um=n(),d(Ja.$$.fragment),Cm=n(),Ua=i("p"),Ua.textContent=vg,zm=n(),Ca=i("p"),Ca.innerHTML=Mg,Lm=n(),za=i("p"),za.textContent=xg,Gm=n(),d(La.$$.fragment),Bm=n(),Ga=i("p"),Ga.innerHTML=kg,$m=n(),d(Ba.$$.fragment),Zm=n(),$a=i("p"),$a.textContent=Tg,Im=n(),Za=i("p"),Za.innerHTML=jg,Wm=n(),d(Ia.$$.fragment),Qm=n(),Wa=i("p"),Wa.innerHTML=_g,Hm=n(),d(Qa.$$.fragment),Xm=n(),Ha=i("p"),Ha.textContent=Jg,Nm=n(),d(Xa.$$.fragment),Vm=n(),Na=i("p"),Na.innerHTML=Ug,Rm=n(),d(Va.$$.fragment),qm=n(),Ra=i("p"),Ra.textContent=Cg,Am=n(),qa=i("p"),qa.textContent=zg,Em=n(),d(Aa.$$.fragment),Ym=n(),Ea=i("p"),Ea.innerHTML=Lg,Sm=n(),d(Ya.$$.fragment),Fm=n(),Sa=i("p"),Sa.textContent=Gg,Pm=n(),Fa=i("p"),Fa.textContent=Bg,Km=n(),d(Pa.$$.fragment),Dm=n(),Ka=i("p"),Ka.innerHTML=$g,Om=n(),d(Da.$$.fragment),so=n(),Oa=i("p"),Oa.textContent=Zg,to=n(),se=i("ul"),se.innerHTML=Ig,ao=n(),te=i("p"),te.innerHTML=Wg,eo=n(),ae=i("ul"),ae.innerHTML=Qg,no=n(),ee=i("p"),ee.innerHTML=Hg,lo=n(),ne=i("p"),ne.textContent=Xg,io=n(),le=i("ul"),le.innerHTML=Ng,po=n(),ie=i("p"),ie.textContent=Vg,mo=n(),d(pe.$$.fragment),oo=n(),O=i("p"),Fc=r(`Self-attention puts each token in relation to each other‚Äôs tokens.
As an example, the`),ro=new c(!1),ho=r(" matrix of the text input sequence "),_n=i("em"),_n.textContent=Rg,Pc=r(" could look as follows:"),co=n(),me=i("p"),me.innerHTML=qg,uo=n(),oe=i("p"),oe.innerHTML=Ag,go=n(),Z=i("p"),Kc=r(`A LLM based on self-attention, but without position embeddings would have great difficulties in understanding the positions of the text inputs to each other.
This is because the probability score computed by`),yo=new c(!1),fo=r(" relates each word token to each other word token in"),wo=new c(!1),bo=r(` computations regardless of their relative positional distance to each other.
Therefore, for the LLM without position embeddings each token appears to have the same distance to all other tokens, `),Jn=i("em"),Jn.textContent=Eg,Dc=r(" differentiating between "),Un=i("em"),Un.textContent=Yg,Oc=r(" and "),Cn=i("em"),Cn.textContent=Sg,su=r(" would be very challenging."),vo=n(),re=i("p"),re.innerHTML=Fg,Mo=n(),C=i("p"),tu=r("The authors of the "),cs=i("a"),cs.innerHTML=Pg,au=r(" paper introduced sinusoidal positional embeddings"),xo=new c(!1),ko=r(` .
where each vector`),To=new c(!1),jo=r(" is computed as a sinusoidal function of its position"),_o=new c(!1),Jo=r(` .
The positional encodings are then simply added to the input sequence vectors`),Uo=new c(!1),Co=r(" ="),zo=new c(!1),Lo=r(" thereby cueing the model to better learn sentence order."),Go=n(),ss=i("p"),eu=r("Instead of using fixed position embeddings, others (such as "),us=i("a"),us.textContent=Kg,nu=r(") used learned positional encodings for which the positional embeddings"),Bo=new c(!1),$o=r(" are learned during training."),Zo=n(),he=i("p"),he.textContent=Dg,Io=n(),ds=i("ol"),V=i("li"),lu=r("Sinusoidal and learned position embeddings are both absolute positional embeddings, "),zn=i("em"),zn.textContent=Og,iu=r(" encoding a unique embedding for each position id:"),Wo=new c(!1),Qo=r(" . As shown by "),gs=i("a"),gs.textContent=sy,pu=r(" and "),ys=i("a"),ys.textContent=ty,mu=r(", absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position."),ou=n(),ce=i("li"),ru=r("When using learned position embeddings, the LLM has to be trained on a fixed input length"),Ho=new c(!1),Xo=r(", which makes it difficult to extrapolate to an input length longer than what it was trained on."),No=n(),ue=i("p"),ue.textContent=ay,Vo=n(),de=i("ul"),de.innerHTML=ey,Ro=n(),E=i("p"),hu=r("Both "),Ln=i("em"),Ln.textContent=ny,cu=r(" and "),Gn=i("em"),Gn.textContent=ly,uu=r(" argue that it‚Äôs best to cue the LLM about sentence order directly in the self-attention algorithm as it‚Äôs there that word tokens are put into relation with each other. More specifically, sentence order should be cued by modifying the"),qo=new c(!1),Ao=r(" computation."),Eo=n(),k=i("p"),du=r("Without going into too many details, "),Bn=i("em"),Bn.textContent=iy,gu=r(" notes that positional information can be encoded into query-key pairs, "),$n=i("em"),$n.textContent=py,Yo=new c(!1),So=r(" and"),Fo=new c(!1),Po=r(" by rotating each vector by an angle"),Ko=new c(!1),Do=r(" and"),Oo=new c(!1),sr=r(" respectively with"),tr=new c(!1),ar=r(` describing each vectors sentence position:
`),er=new c(!1),nr=n(),lr=new c(!1),ir=r(" thereby represents a rotational matrix."),pr=new c(!1),mr=r(" is "),Zn=i("em"),Zn.textContent=my,yu=r(" learned during training, but instead set to a pre-defined value that depends on the maximum input sequence length during training."),or=n(),In=i("blockquote"),$=i("p"),fu=r("By doing so, the probability score between"),rr=new c(!1),hr=r(" and"),cr=new c(!1),ur=r(" is only affected if"),dr=new c(!1),gr=r(" and solely depends on the relative distance"),yr=new c(!1),fr=r(" regardless of each vector‚Äôs specific positions"),wr=new c(!1),br=r(" and"),vr=new c(!1),Mr=r(" ."),xr=n(),ge=i("p"),ge.innerHTML=oy,kr=n(),ye=i("ul"),ye.innerHTML=ry,Tr=n(),Y=i("p"),wu=r("As an alternative, "),Wn=i("em"),Wn.textContent=hy,bu=r(" proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value "),Qn=i("code"),Qn.textContent=cy,vu=r(" to each query-key entry of the"),jr=new c(!1),_r=r(" matrix right before the softmax computation."),Jr=n(),fe=i("p"),fe.innerHTML=uy,Ur=n(),we=i("p"),we.innerHTML=dy,Cr=n(),be=i("p"),be.innerHTML=gy,zr=n(),ve=i("ul"),ve.innerHTML=yy,Lr=n(),x=i("p"),Mu=r("Both "),Hn=i("em"),Hn.textContent=fy,xu=r(" and "),Xn=i("em"),Xn.textContent=wy,ku=r(" position encodings can extrapolate to input lengths not seen during training whereas it has been shown that extrapolation works much better out-of-the-box for "),Nn=i("em"),Nn.textContent=by,Tu=r(" as compared to "),Vn=i("em"),Vn.textContent=vy,ju=r(`.
For ALiBi, one simply increases the values of the lower triangular position matrix to match the length of the input sequence.
For `),Rn=i("em"),Rn.textContent=My,_u=r(", keeping the same"),Gr=new c(!1),Br=r(" that was used during training leads to poor results when passing text inputs much longer than those seen during training, "),qn=i("em"),qn.textContent=xy,Ju=n(),fs=i("a"),fs.textContent=ky,Uu=r(". However, the community has found a couple of effective tricks that adapt"),$r=new c(!1),Zr=r(", thereby allowing "),An=i("em"),An.textContent=Ty,Cu=r(" position embeddings to work well for extrapolated text input sequences (see "),ws=i("a"),ws.textContent=jy,zu=r(")."),Ir=n(),bs=i("blockquote"),En=i("p"),En.innerHTML=_y,Lu=n(),os=i("ul"),Me=i("li"),Gu=r("Positional cues about the text inputs should be given directly to the"),Wr=new c(!1),Qr=r(" matrix of the self-attention layer"),Bu=n(),Yn=i("li"),Yn.innerHTML=Jy,$u=n(),Sn=i("li"),Sn.textContent=Uy,Hr=n(),S=i("p"),Zu=r("In conclusion, LLMs that are intended to be deployed in tasks that require handling large text inputs are better trained with relative positional embeddings, such as RoPE and ALiBi. Also note that even if an LLM with RoPE and ALiBi has been trained only on a fixed length of say"),Xr=new c(!1),Nr=r(" it can still be used in practice with text inputs much larger than"),Vr=new c(!1),Rr=r(", like"),qr=new c(!1),Ar=r(" by extrapolating the positional embeddings."),Er=n(),d(xe.$$.fragment),Yr=n(),ke=i("p"),ke.textContent=Cy,Sr=n(),Te=i("p"),Te.innerHTML=zy,Fr=n(),je=i("p"),je.innerHTML=Ly,Pr=n(),d(_e.$$.fragment),Kr=n(),Je=i("p"),Je.innerHTML=Gy,Dr=n(),d(Ue.$$.fragment),Or=n(),Ce=i("p"),Ce.textContent=By,sh=n(),ze=i("p"),ze.innerHTML=$y,th=n(),z=i("p"),Iu=r("As a consequence, tokens "),Fn=i("em"),Fn.textContent=Zy,Wu=r(" depend on previous tokens, more specifically the"),ah=new c(!1),eh=r(" vector is never put in relation with any key, values vectors"),nh=new c(!1),lh=r(" if"),ih=new c(!1),ph=r(" . Instead"),mh=new c(!1),oh=r(" only attends to previous key-value vectors"),rh=new c(!1),hh=r(". In order to reduce unnecessary computation, one can therefore cache each layer‚Äôs key-value vectors for all previous timesteps."),ch=n(),Le=i("p"),Le.innerHTML=Iy,uh=n(),d(Ge.$$.fragment),dh=n(),Be=i("p"),Be.innerHTML=Wy,gh=n(),d($e.$$.fragment),yh=n(),Ze=i("p"),Ze.innerHTML=Qy,fh=n(),Pn=i("blockquote"),R=i("p"),Qu=r("Making use of the key-value cache means that the"),wh=new c(!1),bh=r(" is essentially reduced to"),vh=new c(!1),Mh=r(" with"),xh=new c(!1),kh=r(" being the query projection of the currently passed input token which is "),Kn=i("em"),Kn.textContent=Hy,Hu=r(" just a single vector."),Th=n(),Ie=i("p"),Ie.textContent=Xy,jh=n(),vs=i("ul"),We=i("li"),Xu=r("Significant increase in computational efficiency as less computations are performed compared to computing the full"),_h=new c(!1),Jh=r(" matrix. This leads to an increase in inference speed"),Nu=n(),Dn=i("li"),Dn.textContent=Ny,Uh=n(),Qe=i("blockquote"),Qe.innerHTML=Vy,Ch=n(),d(Ms.$$.fragment),zh=n(),d(He.$$.fragment),Lh=n(),Xe=i("p"),Xe.textContent=Ry,Gh=n(),d(Ne.$$.fragment),Bh=n(),Ve=i("p"),Ve.textContent=qy,$h=n(),Re=i("ol"),Re.innerHTML=Ay,Zh=n(),qe=i("p"),qe.textContent=Ey,Ih=n(),Ae=i("ol"),Ae.innerHTML=Yy,Wh=n(),Ee=i("p"),Ee.innerHTML=Sy,Qh=n(),d(Ye.$$.fragment),Hh=n(),Se=i("p"),Se.innerHTML=Fy,Xh=n(),d(Fe.$$.fragment),Nh=n(),ts=i("p"),Vu=r("Great, no additional time is spent recomputing the same key and values for the attention layer! There is however one catch. While the required peak memory for the"),Vh=new c(!1),Rh=r(" matrix is significantly reduced, holding the key-value cache in memory can become very memory expensive for long input sequences or multi-turn chat. Remember that the key-value cache needs to store the key-value vectors for all previous input vectors"),qh=new c(!1),Ah=r(" for all self-attention layers and for all attention heads."),Eh=n(),Pe=i("p"),Pe.innerHTML=Py,Yh=n(),d(Ke.$$.fragment),Sh=n(),De=i("p"),De.innerHTML=Ky,Fh=n(),d(Oe.$$.fragment),Ph=n(),sn=i("p"),sn.innerHTML=Dy,Kh=n(),d(tn.$$.fragment),Dh=n(),an=i("p"),an.innerHTML=Oy,Oh=n(),On=i("blockquote"),rs=i("p"),Ru=r("By using a single head-value projection weight pair, the key value vectors"),sc=new c(!1),tc=r(" have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of "),sl=i("code"),sl.textContent=sf,qu=r(" ones."),ac=n(),en=i("p"),en.textContent=tf,ec=n(),as=i("p"),Au=r(`In addition to memory savings, MQA also leads to improved computational efficiency as explained in the following.
In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the`),nc=new c(!1),lc=r(" computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at "),xs=i("a"),xs.textContent=af,Eu=r("."),ic=n(),ks=i("p"),Yu=r("The important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different"),pc=new c(!1),mc=r(" matrix."),oc=n(),nn=i("p"),nn.textContent=ef,rc=n(),ln=i("ul"),ln.innerHTML=nf,hc=n(),pn=i("p"),pn.innerHTML=lf,cc=n(),d(mn.$$.fragment),uc=n(),on=i("p"),on.innerHTML=pf,dc=n(),rn=i("p"),rn.innerHTML=mf,gc=n(),hn=i("p"),hn.innerHTML=of,yc=n(),cn=i("blockquote"),cn.innerHTML=rf,fc=n(),d(un.$$.fragment),wc=n(),dn=i("p"),dn.innerHTML=hf,bc=n(),gn=i("p"),gn.innerHTML=cf,vc=n(),d(yn.$$.fragment),Mc=n(),rl=i("p"),this.h()},l(s){const t=Sw("svelte-u9bgzb",document.head);G=p(t,"META",{name:!0,content:!0}),t.forEach(a),Cs=l(s),P=p(s,"P",{}),M(P).forEach(a),fn=l(s),g(zs.$$.fragment,s),ul=l(s),g(Ls.$$.fragment,s),dl=l(s),Gs=p(s,"P",{"data-svelte-h":!0}),o(Gs)!=="svelte-1b2qao5"&&(Gs.innerHTML=Fu),gl=l(s),Bs=p(s,"UL",{"data-svelte-h":!0}),o(Bs)!=="svelte-19aoyea"&&(Bs.innerHTML=Pu),yl=l(s),$s=p(s,"P",{"data-svelte-h":!0}),o($s)!=="svelte-1cl588t"&&($s.textContent=Ku),fl=l(s),Zs=p(s,"P",{"data-svelte-h":!0}),o(Zs)!=="svelte-f1zoss"&&(Zs.textContent=Du),wl=l(s),Is=p(s,"OL",{"data-svelte-h":!0}),o(Is)!=="svelte-m6c1n8"&&(Is.innerHTML=Ou),bl=l(s),Ws=p(s,"P",{"data-svelte-h":!0}),o(Ws)!=="svelte-ihy2a8"&&(Ws.textContent=sd),vl=l(s),g(Qs.$$.fragment,s),Ml=l(s),Hs=p(s,"P",{"data-svelte-h":!0}),o(Hs)!=="svelte-qqgdso"&&(Hs.innerHTML=td),xl=l(s),Xs=p(s,"P",{"data-svelte-h":!0}),o(Xs)!=="svelte-ljb3iz"&&(Xs.innerHTML=ad),kl=l(s),Ns=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(Ns)!=="svelte-4jkxwr"&&(Ns.innerHTML=ed),Tl=l(s),Vs=p(s,"P",{"data-svelte-h":!0}),o(Vs)!=="svelte-i58rjm"&&(Vs.textContent=nd),jl=l(s),Rs=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(Rs)!=="svelte-z4szg9"&&(Rs.innerHTML=ld),_l=l(s),qs=p(s,"P",{"data-svelte-h":!0}),o(qs)!=="svelte-28orre"&&(qs.textContent=id),Jl=l(s),As=p(s,"P",{"data-svelte-h":!0}),o(As)!=="svelte-1jmd8j1"&&(As.textContent=pd),Ul=l(s),Es=p(s,"UL",{"data-svelte-h":!0}),o(Es)!=="svelte-1th1pdf"&&(Es.innerHTML=md),Cl=l(s),Ys=p(s,"P",{"data-svelte-h":!0}),o(Ys)!=="svelte-28fp1w"&&(Ys.innerHTML=od),zl=l(s),Ss=p(s,"P",{"data-svelte-h":!0}),o(Ss)!=="svelte-1blw6tt"&&(Ss.innerHTML=rd),Ll=l(s),Fs=p(s,"P",{"data-svelte-h":!0}),o(Fs)!=="svelte-6j2ucx"&&(Fs.innerHTML=hd),Gl=l(s),Ps=p(s,"P",{"data-svelte-h":!0}),o(Ps)!=="svelte-g7zrz4"&&(Ps.textContent=cd),Bl=l(s),g(Ks.$$.fragment,s),$l=l(s),g(Ds.$$.fragment,s),Zl=l(s),Os=p(s,"P",{"data-svelte-h":!0}),o(Os)!=="svelte-z9gs9x"&&(Os.innerHTML=ud),Il=l(s),st=p(s,"P",{"data-svelte-h":!0}),o(st)!=="svelte-19462o4"&&(st.innerHTML=dd),Wl=l(s),tt=p(s,"P",{"data-svelte-h":!0}),o(tt)!=="svelte-ppeb89"&&(tt.innerHTML=gd),Ql=l(s),at=p(s,"P",{"data-svelte-h":!0}),o(at)!=="svelte-81gnnp"&&(at.innerHTML=yd),Hl=l(s),g(et.$$.fragment,s),Xl=l(s),g(nt.$$.fragment,s),Nl=l(s),lt=p(s,"P",{"data-svelte-h":!0}),o(lt)!=="svelte-jl1fz0"&&(lt.innerHTML=fd),Vl=l(s),g(it.$$.fragment,s),Rl=l(s),pt=p(s,"P",{"data-svelte-h":!0}),o(pt)!=="svelte-174suzs"&&(pt.textContent=wd),ql=l(s),g(mt.$$.fragment,s),Al=l(s),ot=p(s,"P",{"data-svelte-h":!0}),o(ot)!=="svelte-138fn6l"&&(ot.innerHTML=bd),El=l(s),g(rt.$$.fragment,s),Yl=l(s),ht=p(s,"P",{"data-svelte-h":!0}),o(ht)!=="svelte-jl1fz0"&&(ht.innerHTML=vd),Sl=l(s),g(ct.$$.fragment,s),Fl=l(s),ut=p(s,"P",{"data-svelte-h":!0}),o(ut)!=="svelte-14j7eyl"&&(ut.textContent=Md),Pl=l(s),dt=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(dt)!=="svelte-1gvcuxd"&&(dt.innerHTML=xd),Kl=l(s),gt=p(s,"P",{"data-svelte-h":!0}),o(gt)!=="svelte-1wqa7qp"&&(gt.innerHTML=kd),Dl=l(s),yt=p(s,"P",{"data-svelte-h":!0}),o(yt)!=="svelte-1uylvrd"&&(yt.innerHTML=Td),Ol=l(s),g(ft.$$.fragment,s),si=l(s),wt=p(s,"P",{"data-svelte-h":!0}),o(wt)!=="svelte-zicod6"&&(wt.textContent=jd),ti=l(s),g(bt.$$.fragment,s),ai=l(s),vt=p(s,"P",{"data-svelte-h":!0}),o(vt)!=="svelte-ptycsu"&&(vt.innerHTML=_d),ei=l(s),g(Mt.$$.fragment,s),ni=l(s),xt=p(s,"P",{"data-svelte-h":!0}),o(xt)!=="svelte-sz5glc"&&(xt.innerHTML=Jd),li=l(s),kt=p(s,"P",{"data-svelte-h":!0}),o(kt)!=="svelte-xkk23p"&&(kt.innerHTML=Ud),ii=l(s),Tt=p(s,"P",{"data-svelte-h":!0}),o(Tt)!=="svelte-171gpq5"&&(Tt.textContent=Cd),pi=l(s),jt=p(s,"UL",{"data-svelte-h":!0}),o(jt)!=="svelte-11ifrvi"&&(jt.innerHTML=zd),mi=l(s),B=p(s,"P",{});var I=M(B);zc=h(I,"In a nutshell, this means that "),wn=p(I,"EM",{"data-svelte-h":!0}),o(wn)!=="svelte-1hxrv2n"&&(wn.textContent=Ld),Lc=h(I," multiplications, with"),oi=u(I,!1),ri=h(I," being the "),bn=p(I,"EM",{"data-svelte-h":!0}),o(bn)!=="svelte-9j42in"&&(bn.textContent=Gd),Gc=h(I,","),hi=u(I,!1),ci=h(I," being a weight matrix and"),ui=u(I,!1),di=h(I,` being the output:
`),gi=u(I,!1),I.forEach(a),yi=l(s),_t=p(s,"P",{});var Su=M(_t);Bc=h(Su,`are changed to
`),fi=u(Su,!1),Su.forEach(a),wi=l(s),Jt=p(s,"P",{"data-svelte-h":!0}),o(Jt)!=="svelte-12wl41q"&&(Jt.textContent=Bd),bi=l(s),Ut=p(s,"P",{"data-svelte-h":!0}),o(Ut)!=="svelte-pja6xk"&&(Ut.innerHTML=$d),vi=l(s),g(Ct.$$.fragment,s),Mi=l(s),zt=p(s,"P",{"data-svelte-h":!0}),o(zt)!=="svelte-6n5flp"&&(zt.innerHTML=Zd),xi=l(s),g(Lt.$$.fragment,s),ki=l(s),Gt=p(s,"P",{"data-svelte-h":!0}),o(Gt)!=="svelte-1yy92n4"&&(Gt.textContent=Id),Ti=l(s),g(Bt.$$.fragment,s),ji=l(s),$t=p(s,"P",{"data-svelte-h":!0}),o($t)!=="svelte-jl1fz0"&&($t.innerHTML=Wd),_i=l(s),g(Zt.$$.fragment,s),Ji=l(s),It=p(s,"P",{"data-svelte-h":!0}),o(It)!=="svelte-12urs7j"&&(It.textContent=Qd),Ui=l(s),g(Wt.$$.fragment,s),Ci=l(s),Qt=p(s,"P",{"data-svelte-h":!0}),o(Qt)!=="svelte-jl1fz0"&&(Qt.innerHTML=Hd),zi=l(s),g(Ht.$$.fragment,s),Li=l(s),Xt=p(s,"P",{"data-svelte-h":!0}),o(Xt)!=="svelte-x8wynt"&&(Xt.textContent=Xd),Gi=l(s),Nt=p(s,"P",{"data-svelte-h":!0}),o(Nt)!=="svelte-3uld8v"&&(Nt.textContent=Nd),Bi=l(s),g(Vt.$$.fragment,s),$i=l(s),g(Rt.$$.fragment,s),Zi=l(s),qt=p(s,"P",{"data-svelte-h":!0}),o(qt)!=="svelte-1sadlnz"&&(qt.innerHTML=Vd),Ii=l(s),g(At.$$.fragment,s),Wi=l(s),Et=p(s,"P",{"data-svelte-h":!0}),o(Et)!=="svelte-jl1fz0"&&(Et.innerHTML=Rd),Qi=l(s),g(Yt.$$.fragment,s),Hi=l(s),St=p(s,"P",{"data-svelte-h":!0}),o(St)!=="svelte-1scve2"&&(St.innerHTML=qd),Xi=l(s),g(Ft.$$.fragment,s),Ni=l(s),Pt=p(s,"P",{"data-svelte-h":!0}),o(Pt)!=="svelte-jl1fz0"&&(Pt.innerHTML=Ad),Vi=l(s),g(Kt.$$.fragment,s),Ri=l(s),Dt=p(s,"P",{"data-svelte-h":!0}),o(Dt)!=="svelte-1mhmf5k"&&(Dt.textContent=Ed),qi=l(s),Ot=p(s,"P",{"data-svelte-h":!0}),o(Ot)!=="svelte-sad4qj"&&(Ot.innerHTML=Yd),Ai=l(s),K=p(s,"P",{});var tl=M(K);$c=h(tl,"Also note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to"),Ei=u(tl,!1),Yi=h(tl," and"),Si=u(tl,!1),Fi=h(tl," taking longer during inference."),tl.forEach(a),Pi=l(s),g(sa.$$.fragment,s),Ki=l(s),g(ta.$$.fragment,s),Di=l(s),aa=p(s,"P",{"data-svelte-h":!0}),o(aa)!=="svelte-9e7sx8"&&(aa.textContent=Sd),Oi=l(s),ea=p(s,"P",{"data-svelte-h":!0}),o(ea)!=="svelte-pphiln"&&(ea.textContent=Fd),sp=l(s),na=p(s,"P",{"data-svelte-h":!0}),o(na)!=="svelte-6h5u0r"&&(na.innerHTML=Pd),tp=l(s),la=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(la)!=="svelte-i1sgud"&&(la.innerHTML=Kd),ap=l(s),ia=p(s,"P",{"data-svelte-h":!0}),o(ia)!=="svelte-68w1lt"&&(ia.textContent=Dd),ep=l(s),pa=p(s,"P",{"data-svelte-h":!0}),o(pa)!=="svelte-ei9u00"&&(pa.innerHTML=Od),np=l(s),g(ma.$$.fragment,s),lp=l(s),oa=p(s,"P",{"data-svelte-h":!0}),o(oa)!=="svelte-649tnu"&&(oa.textContent=sg),ip=l(s),q=p(s,"P",{});var Ts=M(q);Zc=h(Ts,`Self-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens.
However, the peak GPU memory consumption for self-attention layers grows `),vn=p(Ts,"EM",{"data-svelte-h":!0}),o(vn)!=="svelte-1jeo576"&&(vn.textContent=tg),Ic=h(Ts," both in compute and memory complexity with number of input tokens (also called "),Mn=p(Ts,"EM",{"data-svelte-h":!0}),o(Mn)!=="svelte-1xe7eav"&&(Mn.textContent=ag),Wc=h(Ts,") that we denote in the following by"),pp=u(Ts,!1),mp=h(Ts,` .
While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens).`),Ts.forEach(a),op=l(s),T=p(s,"P",{});var J=M(T);Qc=h(J,"Let‚Äôs take a closer look. The formula to compute the output"),rp=u(J,!1),hp=h(J," of a self-attention layer for an input"),cp=u(J,!1),up=h(J," of length"),dp=u(J,!1),gp=h(J,` is:
`),yp=u(J,!1),fp=l(J),wp=u(J,!1),bp=h(J," is thereby the input sequence to the attention layer. The projections"),vp=u(J,!1),Mp=h(J," and"),xp=u(J,!1),kp=h(J," will each consist of"),Tp=u(J,!1),jp=h(J," vectors resulting in the"),_p=u(J,!1),Jp=h(J," being of size"),Up=u(J,!1),Cp=h(J," ."),J.forEach(a),zp=l(s),U=p(s,"P",{});var Q=M(U);Hc=h(Q,`LLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.
Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the`),Lp=u(Q,!1),Gp=h(Q," matrices to be"),Bp=u(Q,!1),$p=h(Q," bytes. For"),Zp=u(Q,!1),Ip=h(Q," only around 50 MB of VRAM are needed, however, for"),Wp=u(Q,!1),Qp=h(Q," we would need 19 GB of VRAM, and for"),Hp=u(Q,!1),Xp=h(Q," we would need almost 1TB just to store the"),Np=u(Q,!1),Vp=h(Q," matrices."),Q.forEach(a),Rp=l(s),ra=p(s,"P",{"data-svelte-h":!0}),o(ra)!=="svelte-bptbn2"&&(ra.textContent=eg),qp=l(s),ha=p(s,"P",{"data-svelte-h":!0}),o(ha)!=="svelte-opmd8n"&&(ha.textContent=ng),Ap=l(s),A=p(s,"P",{});var js=M(A);Xc=h(js,"How can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the"),Ep=u(js,!1),Yp=h(js," matrix. "),hs=p(js,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(hs)!=="svelte-wf1aa"&&(hs.textContent=lg),Nc=h(js," developed exactly such a new algorithm and called it "),xn=p(js,"STRONG",{"data-svelte-h":!0}),o(xn)!=="svelte-eh0j4m"&&(xn.textContent=ig),Vc=h(js,"."),js.forEach(a),Sp=l(s),ps=p(s,"P",{});var hl=M(ps);Rc=h(hl,"In a nutshell, Flash Attention breaks the "),Fp=u(hl,!1),Pp=h(hl,`) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:
`),Kp=u(hl,!1),hl.forEach(a),Dp=l(s),W=p(s,"P",{});var es=M(W);qc=h(es,"with"),Op=u(es,!1),sm=h(es," and"),tm=u(es,!1),am=h(es," being some softmax normalization statistics that need to be recomputed for every"),em=u(es,!1),nm=h(es," and"),lm=u(es,!1),im=h(es," ."),es.forEach(a),pm=l(s),ca=p(s,"P",{"data-svelte-h":!0}),o(ca)!=="svelte-1q23ia7"&&(ca.innerHTML=pg),mm=l(s),ua=p(s,"P",{"data-svelte-h":!0}),o(ua)!=="svelte-1ufr4xd"&&(ua.textContent=mg),om=l(s),kn=p(s,"BLOCKQUOTE",{});var uf=M(kn);ms=p(uf,"P",{});var al=M(ms);Ac=h(al,"By keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives "),Tn=p(al,"STRONG",{"data-svelte-h":!0}),o(Tn)!=="svelte-1fh6qbl"&&(Tn.textContent=og),Ec=h(al," outputs compared to the default self-attention layer at a memory cost that only increases linearly with"),rm=u(al,!1),hm=h(al," ."),al.forEach(a),uf.forEach(a),cm=l(s),da=p(s,"P",{"data-svelte-h":!0}),o(da)!=="svelte-1drjn6s"&&(da.innerHTML=rg),um=l(s),ga=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(ga)!=="svelte-m261q0"&&(ga.innerHTML=hg),dm=l(s),D=p(s,"P",{});var el=M(D);Yc=h(el,"Essentially, Flash Attention makes sure that all intermediate write and read operations can be done using the fast "),jn=p(el,"EM",{"data-svelte-h":!0}),o(jn)!=="svelte-1g1d7vc"&&(jn.textContent=cg),Sc=h(el," SRAM memory instead of having to access the slower VRAM memory to compute the output vector"),gm=u(el,!1),ym=h(el," ."),el.forEach(a),fm=l(s),ya=p(s,"P",{"data-svelte-h":!0}),o(ya)!=="svelte-1ew53wd"&&(ya.innerHTML=ug),wm=l(s),fa=p(s,"P",{"data-svelte-h":!0}),o(fa)!=="svelte-1uhed9l"&&(fa.textContent=dg),bm=l(s),wa=p(s,"P",{"data-svelte-h":!0}),o(wa)!=="svelte-66kx9u"&&(wa.innerHTML=gg),vm=l(s),g(ba.$$.fragment,s),Mm=l(s),va=p(s,"P",{"data-svelte-h":!0}),o(va)!=="svelte-1qzrmrh"&&(va.innerHTML=yg),xm=l(s),g(Ma.$$.fragment,s),km=l(s),xa=p(s,"P",{"data-svelte-h":!0}),o(xa)!=="svelte-1lfqnh1"&&(xa.textContent=fg),Tm=l(s),g(ka.$$.fragment,s),jm=l(s),Ta=p(s,"P",{"data-svelte-h":!0}),o(Ta)!=="svelte-tc5wkn"&&(Ta.innerHTML=wg),_m=l(s),g(ja.$$.fragment,s),Jm=l(s),_a=p(s,"P",{"data-svelte-h":!0}),o(_a)!=="svelte-jl1fz0"&&(_a.innerHTML=bg),Um=l(s),g(Ja.$$.fragment,s),Cm=l(s),Ua=p(s,"P",{"data-svelte-h":!0}),o(Ua)!=="svelte-sz1cee"&&(Ua.textContent=vg),zm=l(s),Ca=p(s,"P",{"data-svelte-h":!0}),o(Ca)!=="svelte-d2g4iy"&&(Ca.innerHTML=Mg),Lm=l(s),za=p(s,"P",{"data-svelte-h":!0}),o(za)!=="svelte-2il51v"&&(za.textContent=xg),Gm=l(s),g(La.$$.fragment,s),Bm=l(s),Ga=p(s,"P",{"data-svelte-h":!0}),o(Ga)!=="svelte-jl1fz0"&&(Ga.innerHTML=kg),$m=l(s),g(Ba.$$.fragment,s),Zm=l(s),$a=p(s,"P",{"data-svelte-h":!0}),o($a)!=="svelte-1uumitt"&&($a.textContent=Tg),Im=l(s),Za=p(s,"P",{"data-svelte-h":!0}),o(Za)!=="svelte-bk1lpp"&&(Za.innerHTML=jg),Wm=l(s),g(Ia.$$.fragment,s),Qm=l(s),Wa=p(s,"P",{"data-svelte-h":!0}),o(Wa)!=="svelte-wh831x"&&(Wa.innerHTML=_g),Hm=l(s),g(Qa.$$.fragment,s),Xm=l(s),Ha=p(s,"P",{"data-svelte-h":!0}),o(Ha)!=="svelte-ncw8pe"&&(Ha.textContent=Jg),Nm=l(s),g(Xa.$$.fragment,s),Vm=l(s),Na=p(s,"P",{"data-svelte-h":!0}),o(Na)!=="svelte-jl1fz0"&&(Na.innerHTML=Ug),Rm=l(s),g(Va.$$.fragment,s),qm=l(s),Ra=p(s,"P",{"data-svelte-h":!0}),o(Ra)!=="svelte-1onj0qe"&&(Ra.textContent=Cg),Am=l(s),qa=p(s,"P",{"data-svelte-h":!0}),o(qa)!=="svelte-1b8rrkf"&&(qa.textContent=zg),Em=l(s),g(Aa.$$.fragment,s),Ym=l(s),Ea=p(s,"P",{"data-svelte-h":!0}),o(Ea)!=="svelte-jl1fz0"&&(Ea.innerHTML=Lg),Sm=l(s),g(Ya.$$.fragment,s),Fm=l(s),Sa=p(s,"P",{"data-svelte-h":!0}),o(Sa)!=="svelte-gk4woy"&&(Sa.textContent=Gg),Pm=l(s),Fa=p(s,"P",{"data-svelte-h":!0}),o(Fa)!=="svelte-1lt2th7"&&(Fa.textContent=Bg),Km=l(s),g(Pa.$$.fragment,s),Dm=l(s),Ka=p(s,"P",{"data-svelte-h":!0}),o(Ka)!=="svelte-fkwi5h"&&(Ka.innerHTML=$g),Om=l(s),g(Da.$$.fragment,s),so=l(s),Oa=p(s,"P",{"data-svelte-h":!0}),o(Oa)!=="svelte-1i10a94"&&(Oa.textContent=Zg),to=l(s),se=p(s,"UL",{"data-svelte-h":!0}),o(se)!=="svelte-1ti1vbk"&&(se.innerHTML=Ig),ao=l(s),te=p(s,"P",{"data-svelte-h":!0}),o(te)!=="svelte-jqprou"&&(te.innerHTML=Wg),eo=l(s),ae=p(s,"UL",{"data-svelte-h":!0}),o(ae)!=="svelte-ig4ec7"&&(ae.innerHTML=Qg),no=l(s),ee=p(s,"P",{"data-svelte-h":!0}),o(ee)!=="svelte-awpy0j"&&(ee.innerHTML=Hg),lo=l(s),ne=p(s,"P",{"data-svelte-h":!0}),o(ne)!=="svelte-152e8kl"&&(ne.textContent=Xg),io=l(s),le=p(s,"UL",{"data-svelte-h":!0}),o(le)!=="svelte-k4f0dn"&&(le.innerHTML=Ng),po=l(s),ie=p(s,"P",{"data-svelte-h":!0}),o(ie)!=="svelte-wxdkvm"&&(ie.textContent=Vg),mo=l(s),g(pe.$$.fragment,s),oo=l(s),O=p(s,"P",{});var nl=M(O);Fc=h(nl,`Self-attention puts each token in relation to each other‚Äôs tokens.
As an example, the`),ro=u(nl,!1),ho=h(nl," matrix of the text input sequence "),_n=p(nl,"EM",{"data-svelte-h":!0}),o(_n)!=="svelte-13ogmf8"&&(_n.textContent=Rg),Pc=h(nl," could look as follows:"),nl.forEach(a),co=l(s),me=p(s,"P",{"data-svelte-h":!0}),o(me)!=="svelte-7ssrcl"&&(me.innerHTML=qg),uo=l(s),oe=p(s,"P",{"data-svelte-h":!0}),o(oe)!=="svelte-1xg4o0y"&&(oe.innerHTML=Ag),go=l(s),Z=p(s,"P",{});var F=M(Z);Kc=h(F,`A LLM based on self-attention, but without position embeddings would have great difficulties in understanding the positions of the text inputs to each other.
This is because the probability score computed by`),yo=u(F,!1),fo=h(F," relates each word token to each other word token in"),wo=u(F,!1),bo=h(F,` computations regardless of their relative positional distance to each other.
Therefore, for the LLM without position embeddings each token appears to have the same distance to all other tokens, `),Jn=p(F,"EM",{"data-svelte-h":!0}),o(Jn)!=="svelte-2dbwd4"&&(Jn.textContent=Eg),Dc=h(F," differentiating between "),Un=p(F,"EM",{"data-svelte-h":!0}),o(Un)!=="svelte-1u1601v"&&(Un.textContent=Yg),Oc=h(F," and "),Cn=p(F,"EM",{"data-svelte-h":!0}),o(Cn)!=="svelte-16pe0y3"&&(Cn.textContent=Sg),su=h(F," would be very challenging."),F.forEach(a),vo=l(s),re=p(s,"P",{"data-svelte-h":!0}),o(re)!=="svelte-10v2cm"&&(re.innerHTML=Fg),Mo=l(s),C=p(s,"P",{});var H=M(C);tu=h(H,"The authors of the "),cs=p(H,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(cs)!=="svelte-c939r5"&&(cs.innerHTML=Pg),au=h(H," paper introduced sinusoidal positional embeddings"),xo=u(H,!1),ko=h(H,` .
where each vector`),To=u(H,!1),jo=h(H," is computed as a sinusoidal function of its position"),_o=u(H,!1),Jo=h(H,` .
The positional encodings are then simply added to the input sequence vectors`),Uo=u(H,!1),Co=h(H," ="),zo=u(H,!1),Lo=h(H," thereby cueing the model to better learn sentence order."),H.forEach(a),Go=l(s),ss=p(s,"P",{});var ll=M(ss);eu=h(ll,"Instead of using fixed position embeddings, others (such as "),us=p(ll,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(us)!=="svelte-1fx2dh3"&&(us.textContent=Kg),nu=h(ll,") used learned positional encodings for which the positional embeddings"),Bo=u(ll,!1),$o=h(ll," are learned during training."),ll.forEach(a),Zo=l(s),he=p(s,"P",{"data-svelte-h":!0}),o(he)!=="svelte-gfh3nk"&&(he.textContent=Dg),Io=l(s),ds=p(s,"OL",{});var kc=M(ds);V=p(kc,"LI",{});var ns=M(V);lu=h(ns,"Sinusoidal and learned position embeddings are both absolute positional embeddings, "),zn=p(ns,"EM",{"data-svelte-h":!0}),o(zn)!=="svelte-1636wt6"&&(zn.textContent=Og),iu=h(ns," encoding a unique embedding for each position id:"),Wo=u(ns,!1),Qo=h(ns," . As shown by "),gs=p(ns,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(gs)!=="svelte-hsiyr1"&&(gs.textContent=sy),pu=h(ns," and "),ys=p(ns,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(ys)!=="svelte-586uf6"&&(ys.textContent=ty),mu=h(ns,", absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position."),ns.forEach(a),ou=l(kc),ce=p(kc,"LI",{});var Tc=M(ce);ru=h(Tc,"When using learned position embeddings, the LLM has to be trained on a fixed input length"),Ho=u(Tc,!1),Xo=h(Tc,", which makes it difficult to extrapolate to an input length longer than what it was trained on."),Tc.forEach(a),kc.forEach(a),No=l(s),ue=p(s,"P",{"data-svelte-h":!0}),o(ue)!=="svelte-pzlnat"&&(ue.textContent=ay),Vo=l(s),de=p(s,"UL",{"data-svelte-h":!0}),o(de)!=="svelte-psbdcy"&&(de.innerHTML=ey),Ro=l(s),E=p(s,"P",{});var _s=M(E);hu=h(_s,"Both "),Ln=p(_s,"EM",{"data-svelte-h":!0}),o(Ln)!=="svelte-1koqbni"&&(Ln.textContent=ny),cu=h(_s," and "),Gn=p(_s,"EM",{"data-svelte-h":!0}),o(Gn)!=="svelte-4g6i8t"&&(Gn.textContent=ly),uu=h(_s," argue that it‚Äôs best to cue the LLM about sentence order directly in the self-attention algorithm as it‚Äôs there that word tokens are put into relation with each other. More specifically, sentence order should be cued by modifying the"),qo=u(_s,!1),Ao=h(_s," computation."),_s.forEach(a),Eo=l(s),k=p(s,"P",{});var _=M(k);du=h(_,"Without going into too many details, "),Bn=p(_,"EM",{"data-svelte-h":!0}),o(Bn)!=="svelte-1koqbni"&&(Bn.textContent=iy),gu=h(_," notes that positional information can be encoded into query-key pairs, "),$n=p(_,"EM",{"data-svelte-h":!0}),o($n)!=="svelte-2dbwd4"&&($n.textContent=py),Yo=u(_,!1),So=h(_," and"),Fo=u(_,!1),Po=h(_," by rotating each vector by an angle"),Ko=u(_,!1),Do=h(_," and"),Oo=u(_,!1),sr=h(_," respectively with"),tr=u(_,!1),ar=h(_,` describing each vectors sentence position:
`),er=u(_,!1),nr=l(_),lr=u(_,!1),ir=h(_," thereby represents a rotational matrix."),pr=u(_,!1),mr=h(_," is "),Zn=p(_,"EM",{"data-svelte-h":!0}),o(Zn)!=="svelte-r5iszh"&&(Zn.textContent=my),yu=h(_," learned during training, but instead set to a pre-defined value that depends on the maximum input sequence length during training."),_.forEach(a),or=l(s),In=p(s,"BLOCKQUOTE",{});var df=M(In);$=p(df,"P",{});var X=M($);fu=h(X,"By doing so, the probability score between"),rr=u(X,!1),hr=h(X," and"),cr=u(X,!1),ur=h(X," is only affected if"),dr=u(X,!1),gr=h(X," and solely depends on the relative distance"),yr=u(X,!1),fr=h(X," regardless of each vector‚Äôs specific positions"),wr=u(X,!1),br=h(X," and"),vr=u(X,!1),Mr=h(X," ."),X.forEach(a),df.forEach(a),xr=l(s),ge=p(s,"P",{"data-svelte-h":!0}),o(ge)!=="svelte-1ct7c61"&&(ge.innerHTML=oy),kr=l(s),ye=p(s,"UL",{"data-svelte-h":!0}),o(ye)!=="svelte-5jgayy"&&(ye.innerHTML=ry),Tr=l(s),Y=p(s,"P",{});var Js=M(Y);wu=h(Js,"As an alternative, "),Wn=p(Js,"EM",{"data-svelte-h":!0}),o(Wn)!=="svelte-4g6i8t"&&(Wn.textContent=hy),bu=h(Js," proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value "),Qn=p(Js,"CODE",{"data-svelte-h":!0}),o(Qn)!=="svelte-1gskrin"&&(Qn.textContent=cy),vu=h(Js," to each query-key entry of the"),jr=u(Js,!1),_r=h(Js," matrix right before the softmax computation."),Js.forEach(a),Jr=l(s),fe=p(s,"P",{"data-svelte-h":!0}),o(fe)!=="svelte-1sb0gvf"&&(fe.innerHTML=uy),Ur=l(s),we=p(s,"P",{"data-svelte-h":!0}),o(we)!=="svelte-1pz1yps"&&(we.innerHTML=dy),Cr=l(s),be=p(s,"P",{"data-svelte-h":!0}),o(be)!=="svelte-3uyjm"&&(be.innerHTML=gy),zr=l(s),ve=p(s,"UL",{"data-svelte-h":!0}),o(ve)!=="svelte-l3k8fx"&&(ve.innerHTML=yy),Lr=l(s),x=p(s,"P",{});var j=M(x);Mu=h(j,"Both "),Hn=p(j,"EM",{"data-svelte-h":!0}),o(Hn)!=="svelte-1koqbni"&&(Hn.textContent=fy),xu=h(j," and "),Xn=p(j,"EM",{"data-svelte-h":!0}),o(Xn)!=="svelte-4g6i8t"&&(Xn.textContent=wy),ku=h(j," position encodings can extrapolate to input lengths not seen during training whereas it has been shown that extrapolation works much better out-of-the-box for "),Nn=p(j,"EM",{"data-svelte-h":!0}),o(Nn)!=="svelte-4g6i8t"&&(Nn.textContent=by),Tu=h(j," as compared to "),Vn=p(j,"EM",{"data-svelte-h":!0}),o(Vn)!=="svelte-1koqbni"&&(Vn.textContent=vy),ju=h(j,`.
For ALiBi, one simply increases the values of the lower triangular position matrix to match the length of the input sequence.
For `),Rn=p(j,"EM",{"data-svelte-h":!0}),o(Rn)!=="svelte-1koqbni"&&(Rn.textContent=My),_u=h(j,", keeping the same"),Gr=u(j,!1),Br=h(j," that was used during training leads to poor results when passing text inputs much longer than those seen during training, "),qn=p(j,"EM",{"data-svelte-h":!0}),o(qn)!=="svelte-6ye6px"&&(qn.textContent=xy),Ju=l(j),fs=p(j,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(fs)!=="svelte-dl1rlq"&&(fs.textContent=ky),Uu=h(j,". However, the community has found a couple of effective tricks that adapt"),$r=u(j,!1),Zr=h(j,", thereby allowing "),An=p(j,"EM",{"data-svelte-h":!0}),o(An)!=="svelte-1koqbni"&&(An.textContent=Ty),Cu=h(j," position embeddings to work well for extrapolated text input sequences (see "),ws=p(j,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(ws)!=="svelte-ax8fr8"&&(ws.textContent=jy),zu=h(j,")."),j.forEach(a),Ir=l(s),bs=p(s,"BLOCKQUOTE",{});var jc=M(bs);En=p(jc,"P",{"data-svelte-h":!0}),o(En)!=="svelte-1bw79ap"&&(En.innerHTML=_y),Lu=l(jc),os=p(jc,"UL",{});var il=M(os);Me=p(il,"LI",{});var _c=M(Me);Gu=h(_c,"Positional cues about the text inputs should be given directly to the"),Wr=u(_c,!1),Qr=h(_c," matrix of the self-attention layer"),_c.forEach(a),Bu=l(il),Yn=p(il,"LI",{"data-svelte-h":!0}),o(Yn)!=="svelte-lwsxvy"&&(Yn.innerHTML=Jy),$u=l(il),Sn=p(il,"LI",{"data-svelte-h":!0}),o(Sn)!=="svelte-as1kp0"&&(Sn.textContent=Uy),il.forEach(a),jc.forEach(a),Hr=l(s),S=p(s,"P",{});var Us=M(S);Zu=h(Us,"In conclusion, LLMs that are intended to be deployed in tasks that require handling large text inputs are better trained with relative positional embeddings, such as RoPE and ALiBi. Also note that even if an LLM with RoPE and ALiBi has been trained only on a fixed length of say"),Xr=u(Us,!1),Nr=h(Us," it can still be used in practice with text inputs much larger than"),Vr=u(Us,!1),Rr=h(Us,", like"),qr=u(Us,!1),Ar=h(Us," by extrapolating the positional embeddings."),Us.forEach(a),Er=l(s),g(xe.$$.fragment,s),Yr=l(s),ke=p(s,"P",{"data-svelte-h":!0}),o(ke)!=="svelte-ph4j6j"&&(ke.textContent=Cy),Sr=l(s),Te=p(s,"P",{"data-svelte-h":!0}),o(Te)!=="svelte-583sqe"&&(Te.innerHTML=zy),Fr=l(s),je=p(s,"P",{"data-svelte-h":!0}),o(je)!=="svelte-809dal"&&(je.innerHTML=Ly),Pr=l(s),g(_e.$$.fragment,s),Kr=l(s),Je=p(s,"P",{"data-svelte-h":!0}),o(Je)!=="svelte-jl1fz0"&&(Je.innerHTML=Gy),Dr=l(s),g(Ue.$$.fragment,s),Or=l(s),Ce=p(s,"P",{"data-svelte-h":!0}),o(Ce)!=="svelte-1hvw6ou"&&(Ce.textContent=By),sh=l(s),ze=p(s,"P",{"data-svelte-h":!0}),o(ze)!=="svelte-cclyh"&&(ze.innerHTML=$y),th=l(s),z=p(s,"P",{});var N=M(z);Iu=h(N,"As a consequence, tokens "),Fn=p(N,"EM",{"data-svelte-h":!0}),o(Fn)!=="svelte-1pjm1n8"&&(Fn.textContent=Zy),Wu=h(N," depend on previous tokens, more specifically the"),ah=u(N,!1),eh=h(N," vector is never put in relation with any key, values vectors"),nh=u(N,!1),lh=h(N," if"),ih=u(N,!1),ph=h(N," . Instead"),mh=u(N,!1),oh=h(N," only attends to previous key-value vectors"),rh=u(N,!1),hh=h(N,". In order to reduce unnecessary computation, one can therefore cache each layer‚Äôs key-value vectors for all previous timesteps."),N.forEach(a),ch=l(s),Le=p(s,"P",{"data-svelte-h":!0}),o(Le)!=="svelte-1f21m0a"&&(Le.innerHTML=Iy),uh=l(s),g(Ge.$$.fragment,s),dh=l(s),Be=p(s,"P",{"data-svelte-h":!0}),o(Be)!=="svelte-jl1fz0"&&(Be.innerHTML=Wy),gh=l(s),g($e.$$.fragment,s),yh=l(s),Ze=p(s,"P",{"data-svelte-h":!0}),o(Ze)!=="svelte-sjvt58"&&(Ze.innerHTML=Qy),fh=l(s),Pn=p(s,"BLOCKQUOTE",{});var gf=M(Pn);R=p(gf,"P",{});var ls=M(R);Qu=h(ls,"Making use of the key-value cache means that the"),wh=u(ls,!1),bh=h(ls," is essentially reduced to"),vh=u(ls,!1),Mh=h(ls," with"),xh=u(ls,!1),kh=h(ls," being the query projection of the currently passed input token which is "),Kn=p(ls,"EM",{"data-svelte-h":!0}),o(Kn)!=="svelte-x3mkar"&&(Kn.textContent=Hy),Hu=h(ls," just a single vector."),ls.forEach(a),gf.forEach(a),Th=l(s),Ie=p(s,"P",{"data-svelte-h":!0}),o(Ie)!=="svelte-1xsdpw0"&&(Ie.textContent=Xy),jh=l(s),vs=p(s,"UL",{});var Jc=M(vs);We=p(Jc,"LI",{});var Uc=M(We);Xu=h(Uc,"Significant increase in computational efficiency as less computations are performed compared to computing the full"),_h=u(Uc,!1),Jh=h(Uc," matrix. This leads to an increase in inference speed"),Uc.forEach(a),Nu=l(Jc),Dn=p(Jc,"LI",{"data-svelte-h":!0}),o(Dn)!=="svelte-1uy8i0e"&&(Dn.textContent=Ny),Jc.forEach(a),Uh=l(s),Qe=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(Qe)!=="svelte-1o98ifb"&&(Qe.innerHTML=Vy),Ch=l(s),g(Ms.$$.fragment,s),zh=l(s),g(He.$$.fragment,s),Lh=l(s),Xe=p(s,"P",{"data-svelte-h":!0}),o(Xe)!=="svelte-15et702"&&(Xe.textContent=Ry),Gh=l(s),g(Ne.$$.fragment,s),Bh=l(s),Ve=p(s,"P",{"data-svelte-h":!0}),o(Ve)!=="svelte-1nmendz"&&(Ve.textContent=qy),$h=l(s),Re=p(s,"OL",{"data-svelte-h":!0}),o(Re)!=="svelte-2wclx5"&&(Re.innerHTML=Ay),Zh=l(s),qe=p(s,"P",{"data-svelte-h":!0}),o(qe)!=="svelte-cqzbwt"&&(qe.textContent=Ey),Ih=l(s),Ae=p(s,"OL",{"data-svelte-h":!0}),o(Ae)!=="svelte-1bgwi27"&&(Ae.innerHTML=Yy),Wh=l(s),Ee=p(s,"P",{"data-svelte-h":!0}),o(Ee)!=="svelte-naw54a"&&(Ee.innerHTML=Sy),Qh=l(s),g(Ye.$$.fragment,s),Hh=l(s),Se=p(s,"P",{"data-svelte-h":!0}),o(Se)!=="svelte-jl1fz0"&&(Se.innerHTML=Fy),Xh=l(s),g(Fe.$$.fragment,s),Nh=l(s),ts=p(s,"P",{});var pl=M(ts);Vu=h(pl,"Great, no additional time is spent recomputing the same key and values for the attention layer! There is however one catch. While the required peak memory for the"),Vh=u(pl,!1),Rh=h(pl," matrix is significantly reduced, holding the key-value cache in memory can become very memory expensive for long input sequences or multi-turn chat. Remember that the key-value cache needs to store the key-value vectors for all previous input vectors"),qh=u(pl,!1),Ah=h(pl," for all self-attention layers and for all attention heads."),pl.forEach(a),Eh=l(s),Pe=p(s,"P",{"data-svelte-h":!0}),o(Pe)!=="svelte-hoekyy"&&(Pe.innerHTML=Py),Yh=l(s),g(Ke.$$.fragment,s),Sh=l(s),De=p(s,"P",{"data-svelte-h":!0}),o(De)!=="svelte-jl1fz0"&&(De.innerHTML=Ky),Fh=l(s),g(Oe.$$.fragment,s),Ph=l(s),sn=p(s,"P",{"data-svelte-h":!0}),o(sn)!=="svelte-yntyqd"&&(sn.innerHTML=Dy),Kh=l(s),g(tn.$$.fragment,s),Dh=l(s),an=p(s,"P",{"data-svelte-h":!0}),o(an)!=="svelte-1gvmr62"&&(an.innerHTML=Oy),Oh=l(s),On=p(s,"BLOCKQUOTE",{});var yf=M(On);rs=p(yf,"P",{});var ml=M(rs);Ru=h(ml,"By using a single head-value projection weight pair, the key value vectors"),sc=u(ml,!1),tc=h(ml," have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of "),sl=p(ml,"CODE",{"data-svelte-h":!0}),o(sl)!=="svelte-6j0g1x"&&(sl.textContent=sf),qu=h(ml," ones."),ml.forEach(a),yf.forEach(a),ac=l(s),en=p(s,"P",{"data-svelte-h":!0}),o(en)!=="svelte-16lijvq"&&(en.textContent=tf),ec=l(s),as=p(s,"P",{});var ol=M(as);Au=h(ol,`In addition to memory savings, MQA also leads to improved computational efficiency as explained in the following.
In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the`),nc=u(ol,!1),lc=h(ol," computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at "),xs=p(ol,"A",{href:!0,rel:!0,"data-svelte-h":!0}),o(xs)!=="svelte-1bddqff"&&(xs.textContent=af),Eu=h(ol,"."),ol.forEach(a),ic=l(s),ks=p(s,"P",{});var Cc=M(ks);Yu=h(Cc,"The important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different"),pc=u(Cc,!1),mc=h(Cc," matrix."),Cc.forEach(a),oc=l(s),nn=p(s,"P",{"data-svelte-h":!0}),o(nn)!=="svelte-1q4ym7y"&&(nn.textContent=ef),rc=l(s),ln=p(s,"UL",{"data-svelte-h":!0}),o(ln)!=="svelte-hm64nr"&&(ln.innerHTML=nf),hc=l(s),pn=p(s,"P",{"data-svelte-h":!0}),o(pn)!=="svelte-fzvfyw"&&(pn.innerHTML=lf),cc=l(s),g(mn.$$.fragment,s),uc=l(s),on=p(s,"P",{"data-svelte-h":!0}),o(on)!=="svelte-8w21e6"&&(on.innerHTML=pf),dc=l(s),rn=p(s,"P",{"data-svelte-h":!0}),o(rn)!=="svelte-1rn3bde"&&(rn.innerHTML=mf),gc=l(s),hn=p(s,"P",{"data-svelte-h":!0}),o(hn)!=="svelte-c4mo8y"&&(hn.innerHTML=of),yc=l(s),cn=p(s,"BLOCKQUOTE",{"data-svelte-h":!0}),o(cn)!=="svelte-xhly52"&&(cn.innerHTML=rf),fc=l(s),g(un.$$.fragment,s),wc=l(s),dn=p(s,"P",{"data-svelte-h":!0}),o(dn)!=="svelte-18qaar1"&&(dn.innerHTML=hf),bc=l(s),gn=p(s,"P",{"data-svelte-h":!0}),o(gn)!=="svelte-2w30uj"&&(gn.innerHTML=cf),vc=l(s),g(yn.$$.fragment,s),Mc=l(s),rl=p(s,"P",{}),M(rl).forEach(a),this.h()},h(){L(G,"name","hf:doc:metadata"),L(G,"content",sb),oi.a=ri,hi.a=ci,ui.a=di,gi.a=null,fi.a=null,Ei.a=Yi,Si.a=Fi,pp.a=mp,rp.a=hp,cp.a=up,dp.a=gp,yp.a=fp,wp.a=bp,vp.a=Mp,xp.a=kp,Tp.a=jp,_p.a=Jp,Up.a=Cp,Lp.a=Gp,Bp.a=$p,Zp.a=Ip,Wp.a=Qp,Hp.a=Xp,Np.a=Vp,Ep.a=Yp,L(hs,"href","https://huggingface.co/papers/2205.14135"),L(hs,"rel","nofollow"),Fp.a=Pp,Kp.a=null,Op.a=sm,tm.a=am,em.a=nm,lm.a=im,rm.a=hm,gm.a=ym,ro.a=ho,yo.a=fo,wo.a=bo,L(cs,"href","https://huggingface.co/papers/1706.03762"),L(cs,"rel","nofollow"),xo.a=ko,To.a=jo,_o.a=Jo,Uo.a=Co,zo.a=Lo,L(us,"href","https://huggingface.co/papers/1810.04805"),L(us,"rel","nofollow"),Bo.a=$o,Wo.a=Qo,L(gs,"href","https://huggingface.co/papers/2009.13658"),L(gs,"rel","nofollow"),L(ys,"href","https://huggingface.co/papers/2104.09864"),L(ys,"rel","nofollow"),Ho.a=Xo,qo.a=Ao,Yo.a=So,Fo.a=Po,Ko.a=Do,Oo.a=sr,tr.a=ar,er.a=nr,lr.a=ir,pr.a=mr,rr.a=hr,cr.a=ur,dr.a=gr,yr.a=fr,wr.a=br,vr.a=Mr,jr.a=_r,Gr.a=Br,L(fs,"href","https://huggingface.co/papers/2108.12409"),L(fs,"rel","nofollow"),$r.a=Zr,L(ws,"href","https://github.com/huggingface/transformers/pull/24653"),L(ws,"rel","nofollow"),Wr.a=Qr,Xr.a=Nr,Vr.a=Rr,qr.a=Ar,ah.a=eh,nh.a=lh,ih.a=ph,mh.a=oh,rh.a=hh,wh.a=bh,vh.a=Mh,xh.a=kh,_h.a=Jh,Vh.a=Rh,qh.a=Ah,sc.a=tc,nc.a=lc,L(xs,"href","https://huggingface.co/papers/1911.02150"),L(xs,"rel","nofollow"),pc.a=mc},m(s,t){m(document.head,G),e(s,Cs,t),e(s,P,t),e(s,fn,t),y(zs,s,t),e(s,ul,t),y(Ls,s,t),e(s,dl,t),e(s,Gs,t),e(s,gl,t),e(s,Bs,t),e(s,yl,t),e(s,$s,t),e(s,fl,t),e(s,Zs,t),e(s,wl,t),e(s,Is,t),e(s,bl,t),e(s,Ws,t),e(s,vl,t),y(Qs,s,t),e(s,Ml,t),e(s,Hs,t),e(s,xl,t),e(s,Xs,t),e(s,kl,t),e(s,Ns,t),e(s,Tl,t),e(s,Vs,t),e(s,jl,t),e(s,Rs,t),e(s,_l,t),e(s,qs,t),e(s,Jl,t),e(s,As,t),e(s,Ul,t),e(s,Es,t),e(s,Cl,t),e(s,Ys,t),e(s,zl,t),e(s,Ss,t),e(s,Ll,t),e(s,Fs,t),e(s,Gl,t),e(s,Ps,t),e(s,Bl,t),y(Ks,s,t),e(s,$l,t),y(Ds,s,t),e(s,Zl,t),e(s,Os,t),e(s,Il,t),e(s,st,t),e(s,Wl,t),e(s,tt,t),e(s,Ql,t),e(s,at,t),e(s,Hl,t),y(et,s,t),e(s,Xl,t),y(nt,s,t),e(s,Nl,t),e(s,lt,t),e(s,Vl,t),y(it,s,t),e(s,Rl,t),e(s,pt,t),e(s,ql,t),y(mt,s,t),e(s,Al,t),e(s,ot,t),e(s,El,t),y(rt,s,t),e(s,Yl,t),e(s,ht,t),e(s,Sl,t),y(ct,s,t),e(s,Fl,t),e(s,ut,t),e(s,Pl,t),e(s,dt,t),e(s,Kl,t),e(s,gt,t),e(s,Dl,t),e(s,yt,t),e(s,Ol,t),y(ft,s,t),e(s,si,t),e(s,wt,t),e(s,ti,t),y(bt,s,t),e(s,ai,t),e(s,vt,t),e(s,ei,t),y(Mt,s,t),e(s,ni,t),e(s,xt,t),e(s,li,t),e(s,kt,t),e(s,ii,t),e(s,Tt,t),e(s,pi,t),e(s,jt,t),e(s,mi,t),e(s,B,t),m(B,zc),m(B,wn),m(B,Lc),oi.m(ff,B),m(B,ri),m(B,bn),m(B,Gc),hi.m(wf,B),m(B,ci),ui.m(bf,B),m(B,di),gi.m(vf,B),e(s,yi,t),e(s,_t,t),m(_t,Bc),fi.m(Mf,_t),e(s,wi,t),e(s,Jt,t),e(s,bi,t),e(s,Ut,t),e(s,vi,t),y(Ct,s,t),e(s,Mi,t),e(s,zt,t),e(s,xi,t),y(Lt,s,t),e(s,ki,t),e(s,Gt,t),e(s,Ti,t),y(Bt,s,t),e(s,ji,t),e(s,$t,t),e(s,_i,t),y(Zt,s,t),e(s,Ji,t),e(s,It,t),e(s,Ui,t),y(Wt,s,t),e(s,Ci,t),e(s,Qt,t),e(s,zi,t),y(Ht,s,t),e(s,Li,t),e(s,Xt,t),e(s,Gi,t),e(s,Nt,t),e(s,Bi,t),y(Vt,s,t),e(s,$i,t),y(Rt,s,t),e(s,Zi,t),e(s,qt,t),e(s,Ii,t),y(At,s,t),e(s,Wi,t),e(s,Et,t),e(s,Qi,t),y(Yt,s,t),e(s,Hi,t),e(s,St,t),e(s,Xi,t),y(Ft,s,t),e(s,Ni,t),e(s,Pt,t),e(s,Vi,t),y(Kt,s,t),e(s,Ri,t),e(s,Dt,t),e(s,qi,t),e(s,Ot,t),e(s,Ai,t),e(s,K,t),m(K,$c),Ei.m(xf,K),m(K,Yi),Si.m(kf,K),m(K,Fi),e(s,Pi,t),y(sa,s,t),e(s,Ki,t),y(ta,s,t),e(s,Di,t),e(s,aa,t),e(s,Oi,t),e(s,ea,t),e(s,sp,t),e(s,na,t),e(s,tp,t),e(s,la,t),e(s,ap,t),e(s,ia,t),e(s,ep,t),e(s,pa,t),e(s,np,t),y(ma,s,t),e(s,lp,t),e(s,oa,t),e(s,ip,t),e(s,q,t),m(q,Zc),m(q,vn),m(q,Ic),m(q,Mn),m(q,Wc),pp.m(Tf,q),m(q,mp),e(s,op,t),e(s,T,t),m(T,Qc),rp.m(jf,T),m(T,hp),cp.m(_f,T),m(T,up),dp.m(Jf,T),m(T,gp),yp.m(Uf,T),m(T,fp),wp.m(Cf,T),m(T,bp),vp.m(zf,T),m(T,Mp),xp.m(Lf,T),m(T,kp),Tp.m(Gf,T),m(T,jp),_p.m(Bf,T),m(T,Jp),Up.m($f,T),m(T,Cp),e(s,zp,t),e(s,U,t),m(U,Hc),Lp.m(Zf,U),m(U,Gp),Bp.m(If,U),m(U,$p),Zp.m(Wf,U),m(U,Ip),Wp.m(Qf,U),m(U,Qp),Hp.m(Hf,U),m(U,Xp),Np.m(Xf,U),m(U,Vp),e(s,Rp,t),e(s,ra,t),e(s,qp,t),e(s,ha,t),e(s,Ap,t),e(s,A,t),m(A,Xc),Ep.m(Nf,A),m(A,Yp),m(A,hs),m(A,Nc),m(A,xn),m(A,Vc),e(s,Sp,t),e(s,ps,t),m(ps,Rc),Fp.m(Vf,ps),m(ps,Pp),Kp.m(Rf,ps),e(s,Dp,t),e(s,W,t),m(W,qc),Op.m(qf,W),m(W,sm),tm.m(Af,W),m(W,am),em.m(Ef,W),m(W,nm),lm.m(Yf,W),m(W,im),e(s,pm,t),e(s,ca,t),e(s,mm,t),e(s,ua,t),e(s,om,t),e(s,kn,t),m(kn,ms),m(ms,Ac),m(ms,Tn),m(ms,Ec),rm.m(Sf,ms),m(ms,hm),e(s,cm,t),e(s,da,t),e(s,um,t),e(s,ga,t),e(s,dm,t),e(s,D,t),m(D,Yc),m(D,jn),m(D,Sc),gm.m(Ff,D),m(D,ym),e(s,fm,t),e(s,ya,t),e(s,wm,t),e(s,fa,t),e(s,bm,t),e(s,wa,t),e(s,vm,t),y(ba,s,t),e(s,Mm,t),e(s,va,t),e(s,xm,t),y(Ma,s,t),e(s,km,t),e(s,xa,t),e(s,Tm,t),y(ka,s,t),e(s,jm,t),e(s,Ta,t),e(s,_m,t),y(ja,s,t),e(s,Jm,t),e(s,_a,t),e(s,Um,t),y(Ja,s,t),e(s,Cm,t),e(s,Ua,t),e(s,zm,t),e(s,Ca,t),e(s,Lm,t),e(s,za,t),e(s,Gm,t),y(La,s,t),e(s,Bm,t),e(s,Ga,t),e(s,$m,t),y(Ba,s,t),e(s,Zm,t),e(s,$a,t),e(s,Im,t),e(s,Za,t),e(s,Wm,t),y(Ia,s,t),e(s,Qm,t),e(s,Wa,t),e(s,Hm,t),y(Qa,s,t),e(s,Xm,t),e(s,Ha,t),e(s,Nm,t),y(Xa,s,t),e(s,Vm,t),e(s,Na,t),e(s,Rm,t),y(Va,s,t),e(s,qm,t),e(s,Ra,t),e(s,Am,t),e(s,qa,t),e(s,Em,t),y(Aa,s,t),e(s,Ym,t),e(s,Ea,t),e(s,Sm,t),y(Ya,s,t),e(s,Fm,t),e(s,Sa,t),e(s,Pm,t),e(s,Fa,t),e(s,Km,t),y(Pa,s,t),e(s,Dm,t),e(s,Ka,t),e(s,Om,t),y(Da,s,t),e(s,so,t),e(s,Oa,t),e(s,to,t),e(s,se,t),e(s,ao,t),e(s,te,t),e(s,eo,t),e(s,ae,t),e(s,no,t),e(s,ee,t),e(s,lo,t),e(s,ne,t),e(s,io,t),e(s,le,t),e(s,po,t),e(s,ie,t),e(s,mo,t),y(pe,s,t),e(s,oo,t),e(s,O,t),m(O,Fc),ro.m(Pf,O),m(O,ho),m(O,_n),m(O,Pc),e(s,co,t),e(s,me,t),e(s,uo,t),e(s,oe,t),e(s,go,t),e(s,Z,t),m(Z,Kc),yo.m(Kf,Z),m(Z,fo),wo.m(Df,Z),m(Z,bo),m(Z,Jn),m(Z,Dc),m(Z,Un),m(Z,Oc),m(Z,Cn),m(Z,su),e(s,vo,t),e(s,re,t),e(s,Mo,t),e(s,C,t),m(C,tu),m(C,cs),m(C,au),xo.m(Of,C),m(C,ko),To.m(sw,C),m(C,jo),_o.m(tw,C),m(C,Jo),Uo.m(aw,C),m(C,Co),zo.m(ew,C),m(C,Lo),e(s,Go,t),e(s,ss,t),m(ss,eu),m(ss,us),m(ss,nu),Bo.m(nw,ss),m(ss,$o),e(s,Zo,t),e(s,he,t),e(s,Io,t),e(s,ds,t),m(ds,V),m(V,lu),m(V,zn),m(V,iu),Wo.m(lw,V),m(V,Qo),m(V,gs),m(V,pu),m(V,ys),m(V,mu),m(ds,ou),m(ds,ce),m(ce,ru),Ho.m(iw,ce),m(ce,Xo),e(s,No,t),e(s,ue,t),e(s,Vo,t),e(s,de,t),e(s,Ro,t),e(s,E,t),m(E,hu),m(E,Ln),m(E,cu),m(E,Gn),m(E,uu),qo.m(pw,E),m(E,Ao),e(s,Eo,t),e(s,k,t),m(k,du),m(k,Bn),m(k,gu),m(k,$n),Yo.m(mw,k),m(k,So),Fo.m(ow,k),m(k,Po),Ko.m(rw,k),m(k,Do),Oo.m(hw,k),m(k,sr),tr.m(cw,k),m(k,ar),er.m(uw,k),m(k,nr),lr.m(dw,k),m(k,ir),pr.m(gw,k),m(k,mr),m(k,Zn),m(k,yu),e(s,or,t),e(s,In,t),m(In,$),m($,fu),rr.m(yw,$),m($,hr),cr.m(fw,$),m($,ur),dr.m(ww,$),m($,gr),yr.m(bw,$),m($,fr),wr.m(vw,$),m($,br),vr.m(Mw,$),m($,Mr),e(s,xr,t),e(s,ge,t),e(s,kr,t),e(s,ye,t),e(s,Tr,t),e(s,Y,t),m(Y,wu),m(Y,Wn),m(Y,bu),m(Y,Qn),m(Y,vu),jr.m(xw,Y),m(Y,_r),e(s,Jr,t),e(s,fe,t),e(s,Ur,t),e(s,we,t),e(s,Cr,t),e(s,be,t),e(s,zr,t),e(s,ve,t),e(s,Lr,t),e(s,x,t),m(x,Mu),m(x,Hn),m(x,xu),m(x,Xn),m(x,ku),m(x,Nn),m(x,Tu),m(x,Vn),m(x,ju),m(x,Rn),m(x,_u),Gr.m(kw,x),m(x,Br),m(x,qn),m(x,Ju),m(x,fs),m(x,Uu),$r.m(Tw,x),m(x,Zr),m(x,An),m(x,Cu),m(x,ws),m(x,zu),e(s,Ir,t),e(s,bs,t),m(bs,En),m(bs,Lu),m(bs,os),m(os,Me),m(Me,Gu),Wr.m(jw,Me),m(Me,Qr),m(os,Bu),m(os,Yn),m(os,$u),m(os,Sn),e(s,Hr,t),e(s,S,t),m(S,Zu),Xr.m(_w,S),m(S,Nr),Vr.m(Jw,S),m(S,Rr),qr.m(Uw,S),m(S,Ar),e(s,Er,t),y(xe,s,t),e(s,Yr,t),e(s,ke,t),e(s,Sr,t),e(s,Te,t),e(s,Fr,t),e(s,je,t),e(s,Pr,t),y(_e,s,t),e(s,Kr,t),e(s,Je,t),e(s,Dr,t),y(Ue,s,t),e(s,Or,t),e(s,Ce,t),e(s,sh,t),e(s,ze,t),e(s,th,t),e(s,z,t),m(z,Iu),m(z,Fn),m(z,Wu),ah.m(Cw,z),m(z,eh),nh.m(zw,z),m(z,lh),ih.m(Lw,z),m(z,ph),mh.m(Gw,z),m(z,oh),rh.m(Bw,z),m(z,hh),e(s,ch,t),e(s,Le,t),e(s,uh,t),y(Ge,s,t),e(s,dh,t),e(s,Be,t),e(s,gh,t),y($e,s,t),e(s,yh,t),e(s,Ze,t),e(s,fh,t),e(s,Pn,t),m(Pn,R),m(R,Qu),wh.m($w,R),m(R,bh),vh.m(Zw,R),m(R,Mh),xh.m(Iw,R),m(R,kh),m(R,Kn),m(R,Hu),e(s,Th,t),e(s,Ie,t),e(s,jh,t),e(s,vs,t),m(vs,We),m(We,Xu),_h.m(Ww,We),m(We,Jh),m(vs,Nu),m(vs,Dn),e(s,Uh,t),e(s,Qe,t),e(s,Ch,t),y(Ms,s,t),e(s,zh,t),y(He,s,t),e(s,Lh,t),e(s,Xe,t),e(s,Gh,t),y(Ne,s,t),e(s,Bh,t),e(s,Ve,t),e(s,$h,t),e(s,Re,t),e(s,Zh,t),e(s,qe,t),e(s,Ih,t),e(s,Ae,t),e(s,Wh,t),e(s,Ee,t),e(s,Qh,t),y(Ye,s,t),e(s,Hh,t),e(s,Se,t),e(s,Xh,t),y(Fe,s,t),e(s,Nh,t),e(s,ts,t),m(ts,Vu),Vh.m(Qw,ts),m(ts,Rh),qh.m(Hw,ts),m(ts,Ah),e(s,Eh,t),e(s,Pe,t),e(s,Yh,t),y(Ke,s,t),e(s,Sh,t),e(s,De,t),e(s,Fh,t),y(Oe,s,t),e(s,Ph,t),e(s,sn,t),e(s,Kh,t),y(tn,s,t),e(s,Dh,t),e(s,an,t),e(s,Oh,t),e(s,On,t),m(On,rs),m(rs,Ru),sc.m(Xw,rs),m(rs,tc),m(rs,sl),m(rs,qu),e(s,ac,t),e(s,en,t),e(s,ec,t),e(s,as,t),m(as,Au),nc.m(Nw,as),m(as,lc),m(as,xs),m(as,Eu),e(s,ic,t),e(s,ks,t),m(ks,Yu),pc.m(Vw,ks),m(ks,mc),e(s,oc,t),e(s,nn,t),e(s,rc,t),e(s,ln,t),e(s,hc,t),e(s,pn,t),e(s,cc,t),y(mn,s,t),e(s,uc,t),e(s,on,t),e(s,dc,t),e(s,rn,t),e(s,gc,t),e(s,hn,t),e(s,yc,t),e(s,cn,t),e(s,fc,t),y(un,s,t),e(s,wc,t),e(s,dn,t),e(s,bc,t),e(s,gn,t),e(s,vc,t),y(yn,s,t),e(s,Mc,t),e(s,rl,t),xc=!0},p(s,[t]){const I={};t&2&&(I.$$scope={dirty:t,ctx:s}),Ms.$set(I)},i(s){xc||(f(zs.$$.fragment,s),f(Ls.$$.fragment,s),f(Qs.$$.fragment,s),f(Ks.$$.fragment,s),f(Ds.$$.fragment,s),f(et.$$.fragment,s),f(nt.$$.fragment,s),f(it.$$.fragment,s),f(mt.$$.fragment,s),f(rt.$$.fragment,s),f(ct.$$.fragment,s),f(ft.$$.fragment,s),f(bt.$$.fragment,s),f(Mt.$$.fragment,s),f(Ct.$$.fragment,s),f(Lt.$$.fragment,s),f(Bt.$$.fragment,s),f(Zt.$$.fragment,s),f(Wt.$$.fragment,s),f(Ht.$$.fragment,s),f(Vt.$$.fragment,s),f(Rt.$$.fragment,s),f(At.$$.fragment,s),f(Yt.$$.fragment,s),f(Ft.$$.fragment,s),f(Kt.$$.fragment,s),f(sa.$$.fragment,s),f(ta.$$.fragment,s),f(ma.$$.fragment,s),f(ba.$$.fragment,s),f(Ma.$$.fragment,s),f(ka.$$.fragment,s),f(ja.$$.fragment,s),f(Ja.$$.fragment,s),f(La.$$.fragment,s),f(Ba.$$.fragment,s),f(Ia.$$.fragment,s),f(Qa.$$.fragment,s),f(Xa.$$.fragment,s),f(Va.$$.fragment,s),f(Aa.$$.fragment,s),f(Ya.$$.fragment,s),f(Pa.$$.fragment,s),f(Da.$$.fragment,s),f(pe.$$.fragment,s),f(xe.$$.fragment,s),f(_e.$$.fragment,s),f(Ue.$$.fragment,s),f(Ge.$$.fragment,s),f($e.$$.fragment,s),f(Ms.$$.fragment,s),f(He.$$.fragment,s),f(Ne.$$.fragment,s),f(Ye.$$.fragment,s),f(Fe.$$.fragment,s),f(Ke.$$.fragment,s),f(Oe.$$.fragment,s),f(tn.$$.fragment,s),f(mn.$$.fragment,s),f(un.$$.fragment,s),f(yn.$$.fragment,s),xc=!0)},o(s){w(zs.$$.fragment,s),w(Ls.$$.fragment,s),w(Qs.$$.fragment,s),w(Ks.$$.fragment,s),w(Ds.$$.fragment,s),w(et.$$.fragment,s),w(nt.$$.fragment,s),w(it.$$.fragment,s),w(mt.$$.fragment,s),w(rt.$$.fragment,s),w(ct.$$.fragment,s),w(ft.$$.fragment,s),w(bt.$$.fragment,s),w(Mt.$$.fragment,s),w(Ct.$$.fragment,s),w(Lt.$$.fragment,s),w(Bt.$$.fragment,s),w(Zt.$$.fragment,s),w(Wt.$$.fragment,s),w(Ht.$$.fragment,s),w(Vt.$$.fragment,s),w(Rt.$$.fragment,s),w(At.$$.fragment,s),w(Yt.$$.fragment,s),w(Ft.$$.fragment,s),w(Kt.$$.fragment,s),w(sa.$$.fragment,s),w(ta.$$.fragment,s),w(ma.$$.fragment,s),w(ba.$$.fragment,s),w(Ma.$$.fragment,s),w(ka.$$.fragment,s),w(ja.$$.fragment,s),w(Ja.$$.fragment,s),w(La.$$.fragment,s),w(Ba.$$.fragment,s),w(Ia.$$.fragment,s),w(Qa.$$.fragment,s),w(Xa.$$.fragment,s),w(Va.$$.fragment,s),w(Aa.$$.fragment,s),w(Ya.$$.fragment,s),w(Pa.$$.fragment,s),w(Da.$$.fragment,s),w(pe.$$.fragment,s),w(xe.$$.fragment,s),w(_e.$$.fragment,s),w(Ue.$$.fragment,s),w(Ge.$$.fragment,s),w($e.$$.fragment,s),w(Ms.$$.fragment,s),w(He.$$.fragment,s),w(Ne.$$.fragment,s),w(Ye.$$.fragment,s),w(Fe.$$.fragment,s),w(Ke.$$.fragment,s),w(Oe.$$.fragment,s),w(tn.$$.fragment,s),w(mn.$$.fragment,s),w(un.$$.fragment,s),w(yn.$$.fragment,s),xc=!1},d(s){s&&(a(Cs),a(P),a(fn),a(ul),a(dl),a(Gs),a(gl),a(Bs),a(yl),a($s),a(fl),a(Zs),a(wl),a(Is),a(bl),a(Ws),a(vl),a(Ml),a(Hs),a(xl),a(Xs),a(kl),a(Ns),a(Tl),a(Vs),a(jl),a(Rs),a(_l),a(qs),a(Jl),a(As),a(Ul),a(Es),a(Cl),a(Ys),a(zl),a(Ss),a(Ll),a(Fs),a(Gl),a(Ps),a(Bl),a($l),a(Zl),a(Os),a(Il),a(st),a(Wl),a(tt),a(Ql),a(at),a(Hl),a(Xl),a(Nl),a(lt),a(Vl),a(Rl),a(pt),a(ql),a(Al),a(ot),a(El),a(Yl),a(ht),a(Sl),a(Fl),a(ut),a(Pl),a(dt),a(Kl),a(gt),a(Dl),a(yt),a(Ol),a(si),a(wt),a(ti),a(ai),a(vt),a(ei),a(ni),a(xt),a(li),a(kt),a(ii),a(Tt),a(pi),a(jt),a(mi),a(B),a(yi),a(_t),a(wi),a(Jt),a(bi),a(Ut),a(vi),a(Mi),a(zt),a(xi),a(ki),a(Gt),a(Ti),a(ji),a($t),a(_i),a(Ji),a(It),a(Ui),a(Ci),a(Qt),a(zi),a(Li),a(Xt),a(Gi),a(Nt),a(Bi),a($i),a(Zi),a(qt),a(Ii),a(Wi),a(Et),a(Qi),a(Hi),a(St),a(Xi),a(Ni),a(Pt),a(Vi),a(Ri),a(Dt),a(qi),a(Ot),a(Ai),a(K),a(Pi),a(Ki),a(Di),a(aa),a(Oi),a(ea),a(sp),a(na),a(tp),a(la),a(ap),a(ia),a(ep),a(pa),a(np),a(lp),a(oa),a(ip),a(q),a(op),a(T),a(zp),a(U),a(Rp),a(ra),a(qp),a(ha),a(Ap),a(A),a(Sp),a(ps),a(Dp),a(W),a(pm),a(ca),a(mm),a(ua),a(om),a(kn),a(cm),a(da),a(um),a(ga),a(dm),a(D),a(fm),a(ya),a(wm),a(fa),a(bm),a(wa),a(vm),a(Mm),a(va),a(xm),a(km),a(xa),a(Tm),a(jm),a(Ta),a(_m),a(Jm),a(_a),a(Um),a(Cm),a(Ua),a(zm),a(Ca),a(Lm),a(za),a(Gm),a(Bm),a(Ga),a($m),a(Zm),a($a),a(Im),a(Za),a(Wm),a(Qm),a(Wa),a(Hm),a(Xm),a(Ha),a(Nm),a(Vm),a(Na),a(Rm),a(qm),a(Ra),a(Am),a(qa),a(Em),a(Ym),a(Ea),a(Sm),a(Fm),a(Sa),a(Pm),a(Fa),a(Km),a(Dm),a(Ka),a(Om),a(so),a(Oa),a(to),a(se),a(ao),a(te),a(eo),a(ae),a(no),a(ee),a(lo),a(ne),a(io),a(le),a(po),a(ie),a(mo),a(oo),a(O),a(co),a(me),a(uo),a(oe),a(go),a(Z),a(vo),a(re),a(Mo),a(C),a(Go),a(ss),a(Zo),a(he),a(Io),a(ds),a(No),a(ue),a(Vo),a(de),a(Ro),a(E),a(Eo),a(k),a(or),a(In),a(xr),a(ge),a(kr),a(ye),a(Tr),a(Y),a(Jr),a(fe),a(Ur),a(we),a(Cr),a(be),a(zr),a(ve),a(Lr),a(x),a(Ir),a(bs),a(Hr),a(S),a(Er),a(Yr),a(ke),a(Sr),a(Te),a(Fr),a(je),a(Pr),a(Kr),a(Je),a(Dr),a(Or),a(Ce),a(sh),a(ze),a(th),a(z),a(ch),a(Le),a(uh),a(dh),a(Be),a(gh),a(yh),a(Ze),a(fh),a(Pn),a(Th),a(Ie),a(jh),a(vs),a(Uh),a(Qe),a(Ch),a(zh),a(Lh),a(Xe),a(Gh),a(Bh),a(Ve),a($h),a(Re),a(Zh),a(qe),a(Ih),a(Ae),a(Wh),a(Ee),a(Qh),a(Hh),a(Se),a(Xh),a(Nh),a(ts),a(Eh),a(Pe),a(Yh),a(Sh),a(De),a(Fh),a(Ph),a(sn),a(Kh),a(Dh),a(an),a(Oh),a(On),a(ac),a(en),a(ec),a(as),a(ic),a(ks),a(oc),a(nn),a(rc),a(ln),a(hc),a(pn),a(cc),a(uc),a(on),a(dc),a(rn),a(gc),a(hn),a(yc),a(cn),a(fc),a(wc),a(dn),a(bc),a(gn),a(vc),a(Mc),a(rl)),a(G),b(zs,s),b(Ls,s),b(Qs,s),b(Ks,s),b(Ds,s),b(et,s),b(nt,s),b(it,s),b(mt,s),b(rt,s),b(ct,s),b(ft,s),b(bt,s),b(Mt,s),b(Ct,s),b(Lt,s),b(Bt,s),b(Zt,s),b(Wt,s),b(Ht,s),b(Vt,s),b(Rt,s),b(At,s),b(Yt,s),b(Ft,s),b(Kt,s),b(sa,s),b(ta,s),b(ma,s),b(ba,s),b(Ma,s),b(ka,s),b(ja,s),b(Ja,s),b(La,s),b(Ba,s),b(Ia,s),b(Qa,s),b(Xa,s),b(Va,s),b(Aa,s),b(Ya,s),b(Pa,s),b(Da,s),b(pe,s),b(xe,s),b(_e,s),b(Ue,s),b(Ge,s),b($e,s),b(Ms,s),b(He,s),b(Ne,s),b(Ye,s),b(Fe,s),b(Ke,s),b(Oe,s),b(tn,s),b(mn,s),b(un,s),b(yn,s)}}}const sb='{"title":"Optimizing LLMs for Speed and Memory","local":"optimizing-llms-for-speed-and-memory","sections":[{"title":"1. Lower Precision","local":"1-lower-precision","sections":[],"depth":2},{"title":"2. Flash Attention","local":"2-flash-attention","sections":[],"depth":2},{"title":"3. Architectural Innovations","local":"3-architectural-innovations","sections":[{"title":"3.1 Improving positional embeddings of LLMs","local":"31-improving-positional-embeddings-of-llms","sections":[],"depth":3},{"title":"3.2 The key-value cache","local":"32-the-key-value-cache","sections":[{"title":"3.2.1 Multi-round conversation","local":"321-multi-round-conversation","sections":[],"depth":4},{"title":"3.2.2 Multi-Query-Attention (MQA)","local":"322-multi-query-attention-mqa","sections":[],"depth":4},{"title":"3.2.3 Grouped-Query-Attention (GQA)","local":"323-grouped-query-attention-gqa","sections":[],"depth":4}],"depth":3}],"depth":2},{"title":"Conclusion","local":"conclusion","sections":[],"depth":2}],"depth":1}';function tb(cl){return qw(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mb extends Ew{constructor(G){super(),Yw(this,G,tb,Ow,Rw,{})}}export{mb as component};
