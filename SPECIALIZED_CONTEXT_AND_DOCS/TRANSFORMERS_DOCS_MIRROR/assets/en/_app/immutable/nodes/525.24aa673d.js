import{s as la,o as ra,n as Ue}from"../chunks/scheduler.18a86fab.js";import{S as ia,i as oa,g as c,s as r,r as y,A as pa,h as u,f as a,c as i,j as aa,u as M,x as h,k as vt,y as ma,a as s,v as $,d as g,t as b,w as T}from"../chunks/index.98837b22.js";import{T as sa}from"../chunks/Tip.77304350.js";import{C as Z}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as fa}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as ke,E as da}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as na,a as Ze}from"../chunks/HfOption.6641485e.js";function ca(_){let n,w="Paste your User Access Token into <code>notebook_login</code> when prompted to log in.",m,f,p;return f=new Z({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`,wrap:!1}}),{c(){n=c("p"),n.innerHTML=w,m=r(),y(f.$$.fragment)},l(l){n=u(l,"P",{"data-svelte-h":!0}),h(n)!=="svelte-rmk42d"&&(n.innerHTML=w),m=i(l),M(f.$$.fragment,l)},m(l,d){s(l,n,d),s(l,m,d),$(f,l,d),p=!0},p:Ue,i(l){p||(g(f.$$.fragment,l),p=!0)},o(l){b(f.$$.fragment,l),p=!1},d(l){l&&(a(n),a(m)),T(f,l)}}}function ua(_){let n,w='Make sure the <a href="https://huggingface.co/docs/huggingface_hub/guides/cli#getting-started" rel="nofollow">huggingface_hub[cli]</a> package is installed and run the command below. Paste your User Access Token when prompted to log in.',m,f,p;return f=new Z({props:{code:"aGYlMjBhdXRoJTIwbG9naW4=",highlighted:"hf auth login",wrap:!1}}),{c(){n=c("p"),n.innerHTML=w,m=r(),y(f.$$.fragment)},l(l){n=u(l,"P",{"data-svelte-h":!0}),h(n)!=="svelte-k4i6on"&&(n.innerHTML=w),m=i(l),M(f.$$.fragment,l)},m(l,d){s(l,n,d),s(l,m,d),$(f,l,d),p=!0},p:Ue,i(l){p||(g(f.$$.fragment,l),p=!0)},o(l){b(f.$$.fragment,l),p=!1},d(l){l&&(a(n),a(m)),T(f,l)}}}function ha(_){let n,w,m,f;return n=new Ze({props:{id:"authenticate",option:"notebook",$$slots:{default:[ca]},$$scope:{ctx:_}}}),m=new Ze({props:{id:"authenticate",option:"CLI",$$slots:{default:[ua]},$$scope:{ctx:_}}}),{c(){y(n.$$.fragment),w=r(),y(m.$$.fragment)},l(p){M(n.$$.fragment,p),w=i(p),M(m.$$.fragment,p)},m(p,l){$(n,p,l),s(p,w,l),$(m,p,l),f=!0},p(p,l){const d={};l&2&&(d.$$scope={dirty:l,ctx:p}),n.$set(d);const J={};l&2&&(J.$$scope={dirty:l,ctx:p}),m.$set(J)},i(p){f||(g(n.$$.fragment,p),g(m.$$.fragment,p),f=!0)},o(p){b(n.$$.fragment,p),b(m.$$.fragment,p),f=!1},d(p){p&&a(w),T(n,p),T(m,p)}}}function ya(_){let n,w='Skip ahead to the <a href="#trainer-api">Trainer</a> section to learn how to fine-tune a model.';return{c(){n=c("p"),n.innerHTML=w},l(m){n=u(m,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1alb80h"&&(n.innerHTML=w)},m(m,f){s(m,n,f)},p:Ue,d(m){m&&a(n)}}}function Ma(_){let n,w='Refer to the <a href="./main_classes/pipelines">Pipeline</a> API reference for a complete list of available tasks.';return{c(){n=c("p"),n.innerHTML=w},l(m){n=u(m,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1dafwgm"&&(n.innerHTML=w)},m(m,f){s(m,n,f)},p:Ue,d(m){m&&a(n)}}}function $a(_){let n,w='Use <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.infer_device">~infer_device()</a> to automatically detect an available accelerator for inference.',m,f,p,l,d='Prompt <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> with some initial text to generate more text.',J,v,j;return f=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnRleHQtZ2VuZXJhdGlvbiUyMiUyQyUyMG1vZGVsJTNEJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItaGYlMjIlMkMlMjBkZXZpY2UlM0RkZXZpY2Up",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline, infer_device

device = infer_device()

pipeline = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, device=device)`,wrap:!1}}),v=new Z({props:{code:"cGlwZWxpbmUoJTIyVGhlJTIwc2VjcmV0JTIwdG8lMjBiYWtpbmclMjBhJTIwZ29vZCUyMGNha2UlMjBpcyUyMCUyMiUyQyUyMG1heF9sZW5ndGglM0Q1MCklMEElNUIlN0InZ2VuZXJhdGVkX3RleHQnJTNBJTIwJ1RoZSUyMHNlY3JldCUyMHRvJTIwYmFraW5nJTIwYSUyMGdvb2QlMjBjYWtlJTIwaXMlMjAxMDAlMjUlMjBpbiUyMHRoZSUyMGJhdHRlci4lMjBUaGUlMjBzZWNyZXQlMjB0byUyMGElMjBncmVhdCUyMGNha2UlMjBpcyUyMHRoZSUyMGljaW5nLiU1Q25UaGlzJTIwaXMlMjB3aHklMjB3ZSVFMiU4MCU5OXZlJTIwY3JlYXRlZCUyMHRoZSUyMGJlc3QlMjBidXR0ZXJjcmVhbSUyMGZyb3N0aW5nJTIwcmVjaSclN0QlNUQ=",highlighted:`pipeline(<span class="hljs-string">&quot;The secret to baking a good cake is &quot;</span>, max_length=<span class="hljs-number">50</span>)
[{<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;The secret to baking a good cake is 100% in the batter. The secret to a great cake is the icing.\\nThis is why we’ve created the best buttercream frosting reci&#x27;</span>}]`,wrap:!1}}),{c(){n=c("p"),n.innerHTML=w,m=r(),y(f.$$.fragment),p=r(),l=c("p"),l.innerHTML=d,J=r(),y(v.$$.fragment)},l(o){n=u(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1gulkt8"&&(n.innerHTML=w),m=i(o),M(f.$$.fragment,o),p=i(o),l=u(o,"P",{"data-svelte-h":!0}),h(l)!=="svelte-1s05hxr"&&(l.innerHTML=d),J=i(o),M(v.$$.fragment,o)},m(o,k){s(o,n,k),s(o,m,k),$(f,o,k),s(o,p,k),s(o,l,k),s(o,J,k),$(v,o,k),j=!0},p:Ue,i(o){j||(g(f.$$.fragment,o),g(v.$$.fragment,o),j=!0)},o(o){b(f.$$.fragment,o),b(v.$$.fragment,o),j=!1},d(o){o&&(a(n),a(m),a(p),a(l),a(J)),T(f,o),T(v,o)}}}function ga(_){let n,w='Use <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.infer_device">~infer_device()</a> to automatically detect an available accelerator for inference.',m,f,p,l,d='Pass an image - a URL or local path to the image - to <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>.',J,v,j='<img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"/>',o,k,R;return f=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUyMmltYWdlLXNlZ21lbnRhdGlvbiUyMiUyQyUyMG1vZGVsJTNEJTIyZmFjZWJvb2slMkZkZXRyLXJlc25ldC01MC1wYW5vcHRpYyUyMiUyQyUyMGRldmljZSUzRGRldmljZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline, infer_device

device = infer_device()

pipeline = pipeline(<span class="hljs-string">&quot;image-segmentation&quot;</span>, model=<span class="hljs-string">&quot;facebook/detr-resnet-50-panoptic&quot;</span>, device=device)`,wrap:!1}}),k=new Z({props:{code:"c2VnbWVudHMlMjAlM0QlMjBwaXBlbGluZSglMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZOYXJzaWwlMkZpbWFnZV9kdW1teSUyRnJhdyUyRm1haW4lMkZwYXJyb3RzLnBuZyUyMiklMEFzZWdtZW50cyU1QjAlNUQlNUIlMjJsYWJlbCUyMiU1RCUwQSdiaXJkJyUwQXNlZ21lbnRzJTVCMSU1RCU1QiUyMmxhYmVsJTIyJTVEJTBBJ2JpcmQn",highlighted:`segments = pipeline(<span class="hljs-string">&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;</span>)
segments[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;label&quot;</span>]
<span class="hljs-string">&#x27;bird&#x27;</span>
segments[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;label&quot;</span>]
<span class="hljs-string">&#x27;bird&#x27;</span>`,wrap:!1}}),{c(){n=c("p"),n.innerHTML=w,m=r(),y(f.$$.fragment),p=r(),l=c("p"),l.innerHTML=d,J=r(),v=c("div"),v.innerHTML=j,o=r(),y(k.$$.fragment),this.h()},l(U){n=u(U,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1gulkt8"&&(n.innerHTML=w),m=i(U),M(f.$$.fragment,U),p=i(U),l=u(U,"P",{"data-svelte-h":!0}),h(l)!=="svelte-lrs8zk"&&(l.innerHTML=d),J=i(U),v=u(U,"DIV",{class:!0,"data-svelte-h":!0}),h(v)!=="svelte-1p43dn0"&&(v.innerHTML=j),o=i(U),M(k.$$.fragment,U),this.h()},h(){vt(v,"class","flex justify-center")},m(U,G){s(U,n,G),s(U,m,G),$(f,U,G),s(U,p,G),s(U,l,G),s(U,J,G),s(U,v,G),s(U,o,G),$(k,U,G),R=!0},p:Ue,i(U){R||(g(f.$$.fragment,U),g(k.$$.fragment,U),R=!0)},o(U){b(f.$$.fragment,U),b(k.$$.fragment,U),R=!1},d(U){U&&(a(n),a(m),a(p),a(l),a(J),a(v),a(o)),T(f,U),T(k,U)}}}function ba(_){let n,w='Use <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.infer_device">~infer_device()</a> to automatically detect an available accelerator for inference.',m,f,p,l,d='Pass an audio file to <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>.',J,v,j;return f=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1dG9tYXRpYy1zcGVlY2gtcmVjb2duaXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMm9wZW5haSUyRndoaXNwZXItbGFyZ2UtdjMlMjIlMkMlMjBkZXZpY2UlM0RkZXZpY2Up",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline, infer_device

device = infer_device()

pipeline = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;openai/whisper-large-v3&quot;</span>, device=device)`,wrap:!1}}),v=new Z({props:{code:"cGlwZWxpbmUoJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGTmFyc2lsJTJGYXNyX2R1bW15JTJGcmVzb2x2ZSUyRm1haW4lMkYxLmZsYWMlMjIpJTBBJTdCJ3RleHQnJTNBJTIwJyUyMEhlJTIwaG9wZWQlMjB0aGVyZSUyMHdvdWxkJTIwYmUlMjBzdGV3JTIwZm9yJTIwZGlubmVyJTJDJTIwdHVybmlwcyUyMGFuZCUyMGNhcnJvdHMlMjBhbmQlMjBicnVpc2VkJTIwcG90YXRvZXMlMjBhbmQlMjBmYXQlMjBtdXR0b24lMjBwaWVjZXMlMjB0byUyMGJlJTIwbGFkbGVkJTIwb3V0JTIwaW4lMjB0aGljayUyQyUyMHBlcHBlcmVkJTIwZmxvdXItZmF0dGVuJTIwc2F1Y2UuJyU3RA==",highlighted:`pipeline(<span class="hljs-string">&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac&quot;</span>)
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27; He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.&#x27;</span>}`,wrap:!1}}),{c(){n=c("p"),n.innerHTML=w,m=r(),y(f.$$.fragment),p=r(),l=c("p"),l.innerHTML=d,J=r(),y(v.$$.fragment)},l(o){n=u(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1gulkt8"&&(n.innerHTML=w),m=i(o),M(f.$$.fragment,o),p=i(o),l=u(o,"P",{"data-svelte-h":!0}),h(l)!=="svelte-rsqh3x"&&(l.innerHTML=d),J=i(o),M(v.$$.fragment,o)},m(o,k){s(o,n,k),s(o,m,k),$(f,o,k),s(o,p,k),s(o,l,k),s(o,J,k),$(v,o,k),j=!0},p:Ue,i(o){j||(g(f.$$.fragment,o),g(v.$$.fragment,o),j=!0)},o(o){b(f.$$.fragment,o),b(v.$$.fragment,o),j=!1},d(o){o&&(a(n),a(m),a(p),a(l),a(J)),T(f,o),T(v,o)}}}function Ta(_){let n,w,m,f,p,l;return n=new Ze({props:{id:"pipeline-tasks",option:"text generation",$$slots:{default:[$a]},$$scope:{ctx:_}}}),m=new Ze({props:{id:"pipeline-tasks",option:"image segmentation",$$slots:{default:[ga]},$$scope:{ctx:_}}}),p=new Ze({props:{id:"pipeline-tasks",option:"automatic speech recognition",$$slots:{default:[ba]},$$scope:{ctx:_}}}),{c(){y(n.$$.fragment),w=r(),y(m.$$.fragment),f=r(),y(p.$$.fragment)},l(d){M(n.$$.fragment,d),w=i(d),M(m.$$.fragment,d),f=i(d),M(p.$$.fragment,d)},m(d,J){$(n,d,J),s(d,w,J),$(m,d,J),s(d,f,J),$(p,d,J),l=!0},p(d,J){const v={};J&2&&(v.$$scope={dirty:J,ctx:d}),n.$set(v);const j={};J&2&&(j.$$scope={dirty:J,ctx:d}),m.$set(j);const o={};J&2&&(o.$$scope={dirty:J,ctx:d}),p.$set(o)},i(d){l||(g(n.$$.fragment,d),g(m.$$.fragment,d),g(p.$$.fragment,d),l=!0)},o(d){b(n.$$.fragment,d),b(m.$$.fragment,d),b(p.$$.fragment,d),l=!1},d(d){d&&(a(w),a(f)),T(n,d),T(m,d),T(p,d)}}}function wa(_){let n,w,m,f,p,l,d,J,v,j="Transformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.",o,k,R="The number of user-facing abstractions is limited to only three classes for instantiating a model, and two APIs for inference or training. This quickstart introduces you to Transformers’ key features and shows you how to:",U,G,Ut='<li>load a pretrained model</li> <li>run inference with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a></li> <li>fine-tune a model with <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a></li>',je,I,Ge,V,kt='To start, we recommend creating a Hugging Face <a href="https://hf.co/join" rel="nofollow">account</a>. An account lets you host and access version controlled models, datasets, and <a href="https://hf.co/spaces" rel="nofollow">Spaces</a> on the Hugging Face <a href="https://hf.co/docs/hub/index" rel="nofollow">Hub</a>, a collaborative platform for discovery and building.',Ce,B,_t='Create a <a href="https://hf.co/docs/hub/security-tokens#user-access-tokens" rel="nofollow">User Access Token</a> and log in to your account.',xe,C,Xe,H,Zt="Install Pytorch.",We,Y,Re,z,jt="Then install an up-to-date version of Transformers and some additional libraries from the Hugging Face ecosystem for accessing datasets and vision models, evaluating training, and optimizing training for large models.",Ie,L,Ve,P,Be,N,Gt="Each pretrained model inherits from three base classes.",He,F,Ct='<thead><tr><th><strong>Class</strong></th> <th><strong>Description</strong></th></tr></thead> <tbody><tr><td><a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a></td> <td>A file that specifies a models attributes such as the number of attention heads or vocabulary size.</td></tr> <tr><td><a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a></td> <td>A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaModel">LlamaModel</a> versus <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForCausalLM">LlamaForCausalLM</a>).</td></tr> <tr><td>Preprocessor</td> <td>A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> converts text into tensors and <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> converts pixels into tensors.</td></tr></tbody>',Ye,Q,xt='We recommend using the <a href="./model_doc/auto">AutoClass</a> API to load models and preprocessors because it automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file.',ze,q,Xt='Use <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> to load the weights and configuration file from the Hub into the model and preprocessor class.',Le,E,Wt="When you load a model, configure the following parameters to ensure the model is optimally loaded.",Pe,S,Rt="<li><code>device_map=&quot;auto&quot;</code> automatically allocates the model weights to your fastest device first.</li> <li><code>dtype=&quot;auto&quot;</code> directly initializes the model weights in the data type they’re stored in, which can help avoid loading the weights twice (PyTorch loads weights in <code>torch.float32</code> by default).</li>",Ne,A,Fe,D,It="Tokenize the text and return PyTorch tensors with the tokenizer. Move the model to an accelerator if it’s available to accelerate inference.",Qe,K,qe,O,Vt="The model is now ready for inference or training.",Ee,ee,Bt='For inference, pass the tokenized inputs to <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> to generate text. Decode the token ids back into text with <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode">batch_decode()</a>.',Se,te,Ae,x,De,ae,Ke,se,Ht='The <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.',Oe,X,et,ne,Yt='Create a <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> object and select a task. By default, <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> downloads and caches a default pretrained model for a given task. Pass the model name to the <code>model</code> parameter to choose a specific model.',tt,W,at,le,st,re,zt='<a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> is a complete training and evaluation loop for PyTorch models. It abstracts away a lot of the boilerplate usually involved in manually writing a training loop, so you can start training faster and focus on training design choices. You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.',nt,ie,Lt='Use the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> class to customize the training process. It provides many options for training, evaluation, and more. Experiment with training hyperparameters and features like batch size, learning rate, mixed precision, torch.compile, and more to meet your training needs. You could also use the default training parameters to quickly produce a baseline.',lt,oe,Pt="Load a model, tokenizer, and dataset for training.",rt,pe,it,me,Nt='Create a function to tokenize the text and convert it into PyTorch tensors. Apply this function to the whole dataset with the <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> method.',ot,fe,pt,de,Ft="Load a data collator to create batches of data and pass the tokenizer to it.",mt,ce,ft,ue,Qt='Next, set up <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> with the training features and hyperparameters.',dt,he,ct,ye,qt='Finally, pass all these separate components to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> and call <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.train">train()</a> to start.',ut,Me,ht,$e,Et='Share your model and tokenizer to the Hub with <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a>.',yt,ge,Mt,be,St="Congratulations, you just trained your first model with Transformers!",$t,Te,gt,we,At="Now that you have a better understanding of Transformers and what it offers, it’s time to keep exploring and learning what interests you the most.",bt,Je,Dt='<li><strong>Base classes</strong>: Learn more about the configuration, model and processor classes. This will help you understand how to create and customize models, preprocess different types of inputs (audio, images, multimodal), and how to share your model.</li> <li><strong>Inference</strong>: Explore the <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> further, inference and chatting with LLMs, agents, and how to optimize inference with your machine learning framework and hardware.</li> <li><strong>Training</strong>: Study the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> in more detail, as well as distributed training and optimizing training on specific hardware.</li> <li><strong>Quantization</strong>: Reduce memory and storage requirements with quantization and speed up inference by representing weights with fewer bits.</li> <li><strong>Resources</strong>: Looking for end-to-end recipes for how to train and inference with a model for a specific task? Check out the task recipes!</li>',Tt,ve,wt,_e,Jt;return p=new ke({props:{title:"Quickstart",local:"quickstart",headingTag:"h1"}}),d=new fa({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),I=new ke({props:{title:"Set up",local:"set-up",headingTag:"h2"}}),C=new na({props:{id:"authenticate",options:["notebook","CLI"],$$slots:{default:[ha]},$$scope:{ctx:_}}}),Y=new Z({props:{code:"IXBpcCUyMGluc3RhbGwlMjB0b3JjaA==",highlighted:"!pip install torch",wrap:!1}}),L=new Z({props:{code:"IXBpcCUyMGluc3RhbGwlMjAtVSUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBhY2NlbGVyYXRlJTIwdGltbQ==",highlighted:"!pip install -U transformers datasets evaluate accelerate timm",wrap:!1}}),P=new ke({props:{title:"Pretrained models",local:"pretrained-models",headingTag:"h2"}}),A=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWhmJTIyJTJDJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItaGYlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, dtype=<span class="hljs-string">&quot;auto&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)`,wrap:!1}}),K=new Z({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMlRoZSUyMHNlY3JldCUyMHRvJTIwYmFraW5nJTIwYSUyMGdvb2QlMjBjYWtlJTIwaXMlMjAlMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2Up",highlighted:'model_inputs = tokenizer([<span class="hljs-string">&quot;The secret to baking a good cake is &quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)',wrap:!1}}),te=new Z({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwbWF4X2xlbmd0aCUzRDMwKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyklNUIwJTVEJTBBJyUzQ3MlM0UlMjBUaGUlMjBzZWNyZXQlMjB0byUyMGJha2luZyUyMGElMjBnb29kJTIwY2FrZSUyMGlzJTIwMTAwJTI1JTIwaW4lMjB0aGUlMjBwcmVwYXJhdGlvbi4lMjBUaGVyZSUyMGFyZSUyMHNvJTIwbWFueSUyMHJlY2lwZXMlMjBvdXQlMjB0aGVyZSUyQyc=",highlighted:`generated_ids = model.generate(**model_inputs, max_length=<span class="hljs-number">30</span>)
tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;&lt;s&gt; The secret to baking a good cake is 100% in the preparation. There are so many recipes out there,&#x27;</span>`,wrap:!1}}),x=new sa({props:{warning:!1,$$slots:{default:[ya]},$$scope:{ctx:_}}}),ae=new ke({props:{title:"Pipeline",local:"pipeline",headingTag:"h2"}}),X=new sa({props:{warning:!1,$$slots:{default:[Ma]},$$scope:{ctx:_}}}),W=new na({props:{id:"pipeline-tasks",options:["text generation","image segmentation","automatic speech recognition"],$$slots:{default:[Ta]},$$scope:{ctx:_}}}),le=new ke({props:{title:"Trainer",local:"trainer",headingTag:"h2"}}),pe=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMkMlMjBBdXRvVG9rZW5pemVyJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIycm90dGVuX3RvbWF0b2VzJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
dataset = load_dataset(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)`,wrap:!1}}),fe=new Z({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhc2V0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIydGV4dCUyMiU1RCklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5tYXAodG9rZW5pemVfZGF0YXNldCUyQyUyMGJhdGNoZWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">dataset</span>):
    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>])
dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset, batched=<span class="hljs-literal">True</span>)`,wrap:!1}}),ce=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),he=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJkaXN0aWxiZXJ0LXJvdHRlbi10b21hdG9lcyUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMG51bV90cmFpbl9lcG9jaHMlM0QyJTJDJTBBJTIwJTIwJTIwJTIwcHVzaF90b19odWIlM0RUcnVlJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">&quot;distilbert-rotten-tomatoes&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">8</span>,
    per_device_eval_batch_size=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">2</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)`,wrap:!1}}),Me=new Z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluZXIlMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=dataset[<span class="hljs-string">&quot;test&quot;</span>],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,wrap:!1}}),ge=new Z({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:"trainer.push_to_hub()",wrap:!1}}),Te=new ke({props:{title:"Next steps",local:"next-steps",headingTag:"h2"}}),ve=new da({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md"}}),{c(){n=c("meta"),w=r(),m=c("p"),f=r(),y(p.$$.fragment),l=r(),y(d.$$.fragment),J=r(),v=c("p"),v.textContent=j,o=r(),k=c("p"),k.textContent=R,U=r(),G=c("ul"),G.innerHTML=Ut,je=r(),y(I.$$.fragment),Ge=r(),V=c("p"),V.innerHTML=kt,Ce=r(),B=c("p"),B.innerHTML=_t,xe=r(),y(C.$$.fragment),Xe=r(),H=c("p"),H.textContent=Zt,We=r(),y(Y.$$.fragment),Re=r(),z=c("p"),z.textContent=jt,Ie=r(),y(L.$$.fragment),Ve=r(),y(P.$$.fragment),Be=r(),N=c("p"),N.textContent=Gt,He=r(),F=c("table"),F.innerHTML=Ct,Ye=r(),Q=c("p"),Q.innerHTML=xt,ze=r(),q=c("p"),q.innerHTML=Xt,Le=r(),E=c("p"),E.textContent=Wt,Pe=r(),S=c("ul"),S.innerHTML=Rt,Ne=r(),y(A.$$.fragment),Fe=r(),D=c("p"),D.textContent=It,Qe=r(),y(K.$$.fragment),qe=r(),O=c("p"),O.textContent=Vt,Ee=r(),ee=c("p"),ee.innerHTML=Bt,Se=r(),y(te.$$.fragment),Ae=r(),y(x.$$.fragment),De=r(),y(ae.$$.fragment),Ke=r(),se=c("p"),se.innerHTML=Ht,Oe=r(),y(X.$$.fragment),et=r(),ne=c("p"),ne.innerHTML=Yt,tt=r(),y(W.$$.fragment),at=r(),y(le.$$.fragment),st=r(),re=c("p"),re.innerHTML=zt,nt=r(),ie=c("p"),ie.innerHTML=Lt,lt=r(),oe=c("p"),oe.textContent=Pt,rt=r(),y(pe.$$.fragment),it=r(),me=c("p"),me.innerHTML=Nt,ot=r(),y(fe.$$.fragment),pt=r(),de=c("p"),de.textContent=Ft,mt=r(),y(ce.$$.fragment),ft=r(),ue=c("p"),ue.innerHTML=Qt,dt=r(),y(he.$$.fragment),ct=r(),ye=c("p"),ye.innerHTML=qt,ut=r(),y(Me.$$.fragment),ht=r(),$e=c("p"),$e.innerHTML=Et,yt=r(),y(ge.$$.fragment),Mt=r(),be=c("p"),be.textContent=St,$t=r(),y(Te.$$.fragment),gt=r(),we=c("p"),we.textContent=At,bt=r(),Je=c("ul"),Je.innerHTML=Dt,Tt=r(),y(ve.$$.fragment),wt=r(),_e=c("p"),this.h()},l(e){const t=pa("svelte-u9bgzb",document.head);n=u(t,"META",{name:!0,content:!0}),t.forEach(a),w=i(e),m=u(e,"P",{}),aa(m).forEach(a),f=i(e),M(p.$$.fragment,e),l=i(e),M(d.$$.fragment,e),J=i(e),v=u(e,"P",{"data-svelte-h":!0}),h(v)!=="svelte-661vj6"&&(v.textContent=j),o=i(e),k=u(e,"P",{"data-svelte-h":!0}),h(k)!=="svelte-5htbgh"&&(k.textContent=R),U=i(e),G=u(e,"UL",{"data-svelte-h":!0}),h(G)!=="svelte-1193vfk"&&(G.innerHTML=Ut),je=i(e),M(I.$$.fragment,e),Ge=i(e),V=u(e,"P",{"data-svelte-h":!0}),h(V)!=="svelte-2xzset"&&(V.innerHTML=kt),Ce=i(e),B=u(e,"P",{"data-svelte-h":!0}),h(B)!=="svelte-1hzybaw"&&(B.innerHTML=_t),xe=i(e),M(C.$$.fragment,e),Xe=i(e),H=u(e,"P",{"data-svelte-h":!0}),h(H)!=="svelte-ax388u"&&(H.textContent=Zt),We=i(e),M(Y.$$.fragment,e),Re=i(e),z=u(e,"P",{"data-svelte-h":!0}),h(z)!=="svelte-lgf98o"&&(z.textContent=jt),Ie=i(e),M(L.$$.fragment,e),Ve=i(e),M(P.$$.fragment,e),Be=i(e),N=u(e,"P",{"data-svelte-h":!0}),h(N)!=="svelte-1qfgkiz"&&(N.textContent=Gt),He=i(e),F=u(e,"TABLE",{"data-svelte-h":!0}),h(F)!=="svelte-19m9bw6"&&(F.innerHTML=Ct),Ye=i(e),Q=u(e,"P",{"data-svelte-h":!0}),h(Q)!=="svelte-1cw7zup"&&(Q.innerHTML=xt),ze=i(e),q=u(e,"P",{"data-svelte-h":!0}),h(q)!=="svelte-mdtevt"&&(q.innerHTML=Xt),Le=i(e),E=u(e,"P",{"data-svelte-h":!0}),h(E)!=="svelte-fw3y6g"&&(E.textContent=Wt),Pe=i(e),S=u(e,"UL",{"data-svelte-h":!0}),h(S)!=="svelte-vfrbb3"&&(S.innerHTML=Rt),Ne=i(e),M(A.$$.fragment,e),Fe=i(e),D=u(e,"P",{"data-svelte-h":!0}),h(D)!=="svelte-17gvvx9"&&(D.textContent=It),Qe=i(e),M(K.$$.fragment,e),qe=i(e),O=u(e,"P",{"data-svelte-h":!0}),h(O)!=="svelte-std1x6"&&(O.textContent=Vt),Ee=i(e),ee=u(e,"P",{"data-svelte-h":!0}),h(ee)!=="svelte-1c0kivf"&&(ee.innerHTML=Bt),Se=i(e),M(te.$$.fragment,e),Ae=i(e),M(x.$$.fragment,e),De=i(e),M(ae.$$.fragment,e),Ke=i(e),se=u(e,"P",{"data-svelte-h":!0}),h(se)!=="svelte-11qcmul"&&(se.innerHTML=Ht),Oe=i(e),M(X.$$.fragment,e),et=i(e),ne=u(e,"P",{"data-svelte-h":!0}),h(ne)!=="svelte-94050m"&&(ne.innerHTML=Yt),tt=i(e),M(W.$$.fragment,e),at=i(e),M(le.$$.fragment,e),st=i(e),re=u(e,"P",{"data-svelte-h":!0}),h(re)!=="svelte-wu412v"&&(re.innerHTML=zt),nt=i(e),ie=u(e,"P",{"data-svelte-h":!0}),h(ie)!=="svelte-1alxrh9"&&(ie.innerHTML=Lt),lt=i(e),oe=u(e,"P",{"data-svelte-h":!0}),h(oe)!=="svelte-2aamqd"&&(oe.textContent=Pt),rt=i(e),M(pe.$$.fragment,e),it=i(e),me=u(e,"P",{"data-svelte-h":!0}),h(me)!=="svelte-263wr0"&&(me.innerHTML=Nt),ot=i(e),M(fe.$$.fragment,e),pt=i(e),de=u(e,"P",{"data-svelte-h":!0}),h(de)!=="svelte-bbg97b"&&(de.textContent=Ft),mt=i(e),M(ce.$$.fragment,e),ft=i(e),ue=u(e,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-1manq9x"&&(ue.innerHTML=Qt),dt=i(e),M(he.$$.fragment,e),ct=i(e),ye=u(e,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-1xfr2gm"&&(ye.innerHTML=qt),ut=i(e),M(Me.$$.fragment,e),ht=i(e),$e=u(e,"P",{"data-svelte-h":!0}),h($e)!=="svelte-ydj9qq"&&($e.innerHTML=Et),yt=i(e),M(ge.$$.fragment,e),Mt=i(e),be=u(e,"P",{"data-svelte-h":!0}),h(be)!=="svelte-1205d2s"&&(be.textContent=St),$t=i(e),M(Te.$$.fragment,e),gt=i(e),we=u(e,"P",{"data-svelte-h":!0}),h(we)!=="svelte-ruteo0"&&(we.textContent=At),bt=i(e),Je=u(e,"UL",{"data-svelte-h":!0}),h(Je)!=="svelte-1512zn4"&&(Je.innerHTML=Dt),Tt=i(e),M(ve.$$.fragment,e),wt=i(e),_e=u(e,"P",{}),aa(_e).forEach(a),this.h()},h(){vt(n,"name","hf:doc:metadata"),vt(n,"content",Ja)},m(e,t){ma(document.head,n),s(e,w,t),s(e,m,t),s(e,f,t),$(p,e,t),s(e,l,t),$(d,e,t),s(e,J,t),s(e,v,t),s(e,o,t),s(e,k,t),s(e,U,t),s(e,G,t),s(e,je,t),$(I,e,t),s(e,Ge,t),s(e,V,t),s(e,Ce,t),s(e,B,t),s(e,xe,t),$(C,e,t),s(e,Xe,t),s(e,H,t),s(e,We,t),$(Y,e,t),s(e,Re,t),s(e,z,t),s(e,Ie,t),$(L,e,t),s(e,Ve,t),$(P,e,t),s(e,Be,t),s(e,N,t),s(e,He,t),s(e,F,t),s(e,Ye,t),s(e,Q,t),s(e,ze,t),s(e,q,t),s(e,Le,t),s(e,E,t),s(e,Pe,t),s(e,S,t),s(e,Ne,t),$(A,e,t),s(e,Fe,t),s(e,D,t),s(e,Qe,t),$(K,e,t),s(e,qe,t),s(e,O,t),s(e,Ee,t),s(e,ee,t),s(e,Se,t),$(te,e,t),s(e,Ae,t),$(x,e,t),s(e,De,t),$(ae,e,t),s(e,Ke,t),s(e,se,t),s(e,Oe,t),$(X,e,t),s(e,et,t),s(e,ne,t),s(e,tt,t),$(W,e,t),s(e,at,t),$(le,e,t),s(e,st,t),s(e,re,t),s(e,nt,t),s(e,ie,t),s(e,lt,t),s(e,oe,t),s(e,rt,t),$(pe,e,t),s(e,it,t),s(e,me,t),s(e,ot,t),$(fe,e,t),s(e,pt,t),s(e,de,t),s(e,mt,t),$(ce,e,t),s(e,ft,t),s(e,ue,t),s(e,dt,t),$(he,e,t),s(e,ct,t),s(e,ye,t),s(e,ut,t),$(Me,e,t),s(e,ht,t),s(e,$e,t),s(e,yt,t),$(ge,e,t),s(e,Mt,t),s(e,be,t),s(e,$t,t),$(Te,e,t),s(e,gt,t),s(e,we,t),s(e,bt,t),s(e,Je,t),s(e,Tt,t),$(ve,e,t),s(e,wt,t),s(e,_e,t),Jt=!0},p(e,[t]){const Kt={};t&2&&(Kt.$$scope={dirty:t,ctx:e}),C.$set(Kt);const Ot={};t&2&&(Ot.$$scope={dirty:t,ctx:e}),x.$set(Ot);const ea={};t&2&&(ea.$$scope={dirty:t,ctx:e}),X.$set(ea);const ta={};t&2&&(ta.$$scope={dirty:t,ctx:e}),W.$set(ta)},i(e){Jt||(g(p.$$.fragment,e),g(d.$$.fragment,e),g(I.$$.fragment,e),g(C.$$.fragment,e),g(Y.$$.fragment,e),g(L.$$.fragment,e),g(P.$$.fragment,e),g(A.$$.fragment,e),g(K.$$.fragment,e),g(te.$$.fragment,e),g(x.$$.fragment,e),g(ae.$$.fragment,e),g(X.$$.fragment,e),g(W.$$.fragment,e),g(le.$$.fragment,e),g(pe.$$.fragment,e),g(fe.$$.fragment,e),g(ce.$$.fragment,e),g(he.$$.fragment,e),g(Me.$$.fragment,e),g(ge.$$.fragment,e),g(Te.$$.fragment,e),g(ve.$$.fragment,e),Jt=!0)},o(e){b(p.$$.fragment,e),b(d.$$.fragment,e),b(I.$$.fragment,e),b(C.$$.fragment,e),b(Y.$$.fragment,e),b(L.$$.fragment,e),b(P.$$.fragment,e),b(A.$$.fragment,e),b(K.$$.fragment,e),b(te.$$.fragment,e),b(x.$$.fragment,e),b(ae.$$.fragment,e),b(X.$$.fragment,e),b(W.$$.fragment,e),b(le.$$.fragment,e),b(pe.$$.fragment,e),b(fe.$$.fragment,e),b(ce.$$.fragment,e),b(he.$$.fragment,e),b(Me.$$.fragment,e),b(ge.$$.fragment,e),b(Te.$$.fragment,e),b(ve.$$.fragment,e),Jt=!1},d(e){e&&(a(w),a(m),a(f),a(l),a(J),a(v),a(o),a(k),a(U),a(G),a(je),a(Ge),a(V),a(Ce),a(B),a(xe),a(Xe),a(H),a(We),a(Re),a(z),a(Ie),a(Ve),a(Be),a(N),a(He),a(F),a(Ye),a(Q),a(ze),a(q),a(Le),a(E),a(Pe),a(S),a(Ne),a(Fe),a(D),a(Qe),a(qe),a(O),a(Ee),a(ee),a(Se),a(Ae),a(De),a(Ke),a(se),a(Oe),a(et),a(ne),a(tt),a(at),a(st),a(re),a(nt),a(ie),a(lt),a(oe),a(rt),a(it),a(me),a(ot),a(pt),a(de),a(mt),a(ft),a(ue),a(dt),a(ct),a(ye),a(ut),a(ht),a($e),a(yt),a(Mt),a(be),a($t),a(gt),a(we),a(bt),a(Je),a(Tt),a(wt),a(_e)),a(n),T(p,e),T(d,e),T(I,e),T(C,e),T(Y,e),T(L,e),T(P,e),T(A,e),T(K,e),T(te,e),T(x,e),T(ae,e),T(X,e),T(W,e),T(le,e),T(pe,e),T(fe,e),T(ce,e),T(he,e),T(Me,e),T(ge,e),T(Te,e),T(ve,e)}}}const Ja='{"title":"Quickstart","local":"quickstart","sections":[{"title":"Set up","local":"set-up","sections":[],"depth":2},{"title":"Pretrained models","local":"pretrained-models","sections":[],"depth":2},{"title":"Pipeline","local":"pipeline","sections":[],"depth":2},{"title":"Trainer","local":"trainer","sections":[],"depth":2},{"title":"Next steps","local":"next-steps","sections":[],"depth":2}],"depth":1}';function va(_){return ra(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xa extends ia{constructor(n){super(),oa(this,n,va,wa,la,{})}}export{xa as component};
