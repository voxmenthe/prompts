import{s as Mt,o as zt,n as Ht}from"../chunks/scheduler.18a86fab.js";import{S as Lt,i as Jt,g as o,s as r,r as u,A as Ct,h as i,f as s,c as a,j as x,x as d,u as g,k as y,y as n,a as l,v as h,d as _,t as k,w as b}from"../chunks/index.98837b22.js";import{T as Et}from"../chunks/Tip.77304350.js";import{D as S}from"../chunks/Docstring.a1ef7999.js";import{C as It}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as _e,E as jt}from"../chunks/getInferenceSnippets.06c2775f.js";function Vt(ke){let f,C=`Herbert implementation is the same as <code>BERT</code> except for the tokenization method. Refer to <a href="bert">BERT documentation</a>
for API reference and examples.`;return{c(){f=o("p"),f.innerHTML=C},l(T){f=i(T,"P",{"data-svelte-h":!0}),d(f)!=="svelte-wj8wc4"&&(f.innerHTML=C)},m(T,O){l(T,f,O)},p:Ht,d(T){T&&s(f)}}}function Rt(ke){let f,C,T,O,E,at="<em>This model was released on 2020-05-01 and added to Hugging Face Transformers on 2020-11-16.</em>",be,I,ve,M,ot='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Te,j,$e,V,it=`The HerBERT model was proposed in <a href="https://huggingface.co/papers/2005.00630" rel="nofollow">KLEJ: Comprehensive Benchmark for Polish Language Understanding</a> by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and
Ireneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamic
masking of whole words.`,we,R,lt="The abstract from the paper is the following:",ye,U,dt=`<em>In recent years, a series of Transformer-based models unlocked major improvements in general natural language
understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which
allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of
languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language
understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing
datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new
sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.</em>`,xe,q,mt=`This model was contributed by <a href="https://huggingface.co/rmroczkowski" rel="nofollow">rmroczkowski</a>. The original code can be found
<a href="https://github.com/allegro/HerBERT" rel="nofollow">here</a>.`,Me,P,ze,Z,He,z,Le,F,Je,m,B,Pe,K,pt="Construct a BPE tokenizer for HerBERT.",Ze,ee,ct="Peculiarities:",Fe,te,ft=`<li><p>uses BERT’s pre-tokenizer: BaseTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of a
punctuation character will be treated separately.</p></li> <li><p>Such pretokenized input is BPE subtokenized</p></li>`,Be,ne,ut=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMTokenizer">XLMTokenizer</a> which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`,Xe,$,X,De,se,gt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLM sequence has the following format:`,Ge,re,ht="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt; B &lt;/s&gt;</code></li>",We,H,D,Ne,ae,_t="Converts a sequence of tokens (string) in a single string.",Ae,L,G,Ye,oe,kt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Ce,W,Ee,p,N,Qe,ie,bt="Construct a “Fast” BPE tokenizer for HerBERT (backed by HuggingFace’s <em>tokenizers</em> library).",Se,le,vt="Peculiarities:",Oe,de,Tt=`<li>uses BERT’s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.</li>`,Ke,me,$t=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`,et,w,A,tt,pe,wt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An HerBERT, like BERT sequence has the following format:`,nt,ce,yt="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt; B &lt;/s&gt;</code></li>",st,J,Y,rt,fe,xt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Ie,Q,je,he,Ve;return I=new _e({props:{title:"HerBERT",local:"herbert",headingTag:"h1"}}),j=new _e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),P=new _e({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),Z=new It({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEhlcmJlcnRUb2tlbml6ZXIlMkMlMjBSb2JlcnRhTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBIZXJiZXJ0VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJhbGxlZ3JvJTJGaGVyYmVydC1rbGVqLWNhc2VkLXRva2VuaXplci12MSUyMiklMEFtb2RlbCUyMCUzRCUyMFJvYmVydGFNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyYWxsZWdybyUyRmhlcmJlcnQta2xlai1jYXNlZC12MSUyMiklMEElMEFlbmNvZGVkX2lucHV0JTIwJTNEJTIwdG9rZW5pemVyLmVuY29kZSglMjJLdG8lMjBtYSUyMGxlcHN6JUM0JTg1JTIwc3p0dWslQzQlOTklMkMlMjBtYSUyMGxlcHN6eSUyMHJ6JUM0JTg1ZCUyMCVFMiU4MCU5MyUyMHRvJTIwamFzbmUuJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoZW5jb2RlZF9pbnB1dCklMEElMEElMjMlMjBIZXJCRVJUJTIwY2FuJTIwYWxzbyUyMGJlJTIwbG9hZGVkJTIwdXNpbmclMjBBdXRvVG9rZW5pemVyJTIwYW5kJTIwQXV0b01vZGVsJTNBJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFsbGVncm8lMkZoZXJiZXJ0LWtsZWotY2FzZWQtdG9rZW5pemVyLXYxJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJhbGxlZ3JvJTJGaGVyYmVydC1rbGVqLWNhc2VkLXYxJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> HerbertTokenizer, RobertaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = HerbertTokenizer.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-tokenizer-v1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaModel.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-v1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer.encode(<span class="hljs-string">&quot;Kto ma lepszą sztukę, ma lepszy rząd – to jasne.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_input)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># HerBERT can also be loaded using AutoTokenizer and AutoModel:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-tokenizer-v1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-v1&quot;</span>)`,wrap:!1}}),z=new Et({props:{$$slots:{default:[Vt]},$$scope:{ctx:ke}}}),F=new _e({props:{title:"HerbertTokenizer",local:"transformers.HerbertTokenizer",headingTag:"h2"}}),B=new S({props:{name:"class transformers.HerbertTokenizer",anchor:"transformers.HerbertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"tokenizer_file",val:" = None"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sep_token",val:" = '</s>'"},{name:"bos_token",val:" = '<s>'"},{name:"do_lowercase_and_remove_accent",val:" = False"},{name:"additional_special_tokens",val:" = ['<special0>', '<special1>', '<special2>', '<special3>', '<special4>', '<special5>', '<special6>', '<special7>', '<special8>', '<special9>']"},{name:"lang2id",val:" = None"},{name:"id2lang",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert.py#L277"}}),X=new S({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert.py#L511",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),D=new S({props:{name:"convert_tokens_to_string",anchor:"transformers.HerbertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert.py#L505"}}),G=new S({props:{name:"get_special_tokens_mask",anchor:"transformers.HerbertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert.py#L539",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),W=new _e({props:{title:"HerbertTokenizerFast",local:"transformers.HerbertTokenizerFast",headingTag:"h2"}}),N=new S({props:{name:"class transformers.HerbertTokenizerFast",anchor:"transformers.HerbertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sep_token",val:" = '</s>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.HerbertTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert_fast.py#L28"}}),A=new S({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert_fast.py#L74",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Y=new S({props:{name:"get_special_tokens_mask",anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/herbert/tokenization_herbert_fast.py#L101",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Q=new jt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/herbert.md"}}),{c(){f=o("meta"),C=r(),T=o("p"),O=r(),E=o("p"),E.innerHTML=at,be=r(),u(I.$$.fragment),ve=r(),M=o("div"),M.innerHTML=ot,Te=r(),u(j.$$.fragment),$e=r(),V=o("p"),V.innerHTML=it,we=r(),R=o("p"),R.textContent=lt,ye=r(),U=o("p"),U.innerHTML=dt,xe=r(),q=o("p"),q.innerHTML=mt,Me=r(),u(P.$$.fragment),ze=r(),u(Z.$$.fragment),He=r(),u(z.$$.fragment),Le=r(),u(F.$$.fragment),Je=r(),m=o("div"),u(B.$$.fragment),Pe=r(),K=o("p"),K.textContent=pt,Ze=r(),ee=o("p"),ee.textContent=ct,Fe=r(),te=o("ul"),te.innerHTML=ft,Be=r(),ne=o("p"),ne.innerHTML=ut,Xe=r(),$=o("div"),u(X.$$.fragment),De=r(),se=o("p"),se.textContent=gt,Ge=r(),re=o("ul"),re.innerHTML=ht,We=r(),H=o("div"),u(D.$$.fragment),Ne=r(),ae=o("p"),ae.textContent=_t,Ae=r(),L=o("div"),u(G.$$.fragment),Ye=r(),oe=o("p"),oe.innerHTML=kt,Ce=r(),u(W.$$.fragment),Ee=r(),p=o("div"),u(N.$$.fragment),Qe=r(),ie=o("p"),ie.innerHTML=bt,Se=r(),le=o("p"),le.textContent=vt,Oe=r(),de=o("ul"),de.innerHTML=Tt,Ke=r(),me=o("p"),me.innerHTML=$t,et=r(),w=o("div"),u(A.$$.fragment),tt=r(),pe=o("p"),pe.textContent=wt,nt=r(),ce=o("ul"),ce.innerHTML=yt,st=r(),J=o("div"),u(Y.$$.fragment),rt=r(),fe=o("p"),fe.innerHTML=xt,Ie=r(),u(Q.$$.fragment),je=r(),he=o("p"),this.h()},l(e){const t=Ct("svelte-u9bgzb",document.head);f=i(t,"META",{name:!0,content:!0}),t.forEach(s),C=a(e),T=i(e,"P",{}),x(T).forEach(s),O=a(e),E=i(e,"P",{"data-svelte-h":!0}),d(E)!=="svelte-1upgzb"&&(E.innerHTML=at),be=a(e),g(I.$$.fragment,e),ve=a(e),M=i(e,"DIV",{class:!0,"data-svelte-h":!0}),d(M)!=="svelte-13t8s2t"&&(M.innerHTML=ot),Te=a(e),g(j.$$.fragment,e),$e=a(e),V=i(e,"P",{"data-svelte-h":!0}),d(V)!=="svelte-wvhqov"&&(V.innerHTML=it),we=a(e),R=i(e,"P",{"data-svelte-h":!0}),d(R)!=="svelte-vfdo9a"&&(R.textContent=lt),ye=a(e),U=i(e,"P",{"data-svelte-h":!0}),d(U)!=="svelte-1frp4qn"&&(U.innerHTML=dt),xe=a(e),q=i(e,"P",{"data-svelte-h":!0}),d(q)!=="svelte-1jq061t"&&(q.innerHTML=mt),Me=a(e),g(P.$$.fragment,e),ze=a(e),g(Z.$$.fragment,e),He=a(e),g(z.$$.fragment,e),Le=a(e),g(F.$$.fragment,e),Je=a(e),m=i(e,"DIV",{class:!0});var c=x(m);g(B.$$.fragment,c),Pe=a(c),K=i(c,"P",{"data-svelte-h":!0}),d(K)!=="svelte-irtuqb"&&(K.textContent=pt),Ze=a(c),ee=i(c,"P",{"data-svelte-h":!0}),d(ee)!=="svelte-r7777v"&&(ee.textContent=ct),Fe=a(c),te=i(c,"UL",{"data-svelte-h":!0}),d(te)!=="svelte-15v80xk"&&(te.innerHTML=ft),Be=a(c),ne=i(c,"P",{"data-svelte-h":!0}),d(ne)!=="svelte-o8t1sl"&&(ne.innerHTML=ut),Xe=a(c),$=i(c,"DIV",{class:!0});var ue=x($);g(X.$$.fragment,ue),De=a(ue),se=i(ue,"P",{"data-svelte-h":!0}),d(se)!=="svelte-1xo6smc"&&(se.textContent=gt),Ge=a(ue),re=i(ue,"UL",{"data-svelte-h":!0}),d(re)!=="svelte-1w73b42"&&(re.innerHTML=ht),ue.forEach(s),We=a(c),H=i(c,"DIV",{class:!0});var Re=x(H);g(D.$$.fragment,Re),Ne=a(Re),ae=i(Re,"P",{"data-svelte-h":!0}),d(ae)!=="svelte-b3k2yi"&&(ae.textContent=_t),Re.forEach(s),Ae=a(c),L=i(c,"DIV",{class:!0});var Ue=x(L);g(G.$$.fragment,Ue),Ye=a(Ue),oe=i(Ue,"P",{"data-svelte-h":!0}),d(oe)!=="svelte-1f4f5kp"&&(oe.innerHTML=kt),Ue.forEach(s),c.forEach(s),Ce=a(e),g(W.$$.fragment,e),Ee=a(e),p=i(e,"DIV",{class:!0});var v=x(p);g(N.$$.fragment,v),Qe=a(v),ie=i(v,"P",{"data-svelte-h":!0}),d(ie)!=="svelte-1n4shqg"&&(ie.innerHTML=bt),Se=a(v),le=i(v,"P",{"data-svelte-h":!0}),d(le)!=="svelte-r7777v"&&(le.textContent=vt),Oe=a(v),de=i(v,"UL",{"data-svelte-h":!0}),d(de)!=="svelte-lbio7x"&&(de.innerHTML=Tt),Ke=a(v),me=i(v,"P",{"data-svelte-h":!0}),d(me)!=="svelte-unx247"&&(me.innerHTML=$t),et=a(v),w=i(v,"DIV",{class:!0});var ge=x(w);g(A.$$.fragment,ge),tt=a(ge),pe=i(ge,"P",{"data-svelte-h":!0}),d(pe)!=="svelte-11s9gqx"&&(pe.textContent=wt),nt=a(ge),ce=i(ge,"UL",{"data-svelte-h":!0}),d(ce)!=="svelte-1w73b42"&&(ce.innerHTML=yt),ge.forEach(s),st=a(v),J=i(v,"DIV",{class:!0});var qe=x(J);g(Y.$$.fragment,qe),rt=a(qe),fe=i(qe,"P",{"data-svelte-h":!0}),d(fe)!=="svelte-1f4f5kp"&&(fe.innerHTML=xt),qe.forEach(s),v.forEach(s),Ie=a(e),g(Q.$$.fragment,e),je=a(e),he=i(e,"P",{}),x(he).forEach(s),this.h()},h(){y(f,"name","hf:doc:metadata"),y(f,"content",Ut),y(M,"class","flex flex-wrap space-x-1"),y($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){n(document.head,f),l(e,C,t),l(e,T,t),l(e,O,t),l(e,E,t),l(e,be,t),h(I,e,t),l(e,ve,t),l(e,M,t),l(e,Te,t),h(j,e,t),l(e,$e,t),l(e,V,t),l(e,we,t),l(e,R,t),l(e,ye,t),l(e,U,t),l(e,xe,t),l(e,q,t),l(e,Me,t),h(P,e,t),l(e,ze,t),h(Z,e,t),l(e,He,t),h(z,e,t),l(e,Le,t),h(F,e,t),l(e,Je,t),l(e,m,t),h(B,m,null),n(m,Pe),n(m,K),n(m,Ze),n(m,ee),n(m,Fe),n(m,te),n(m,Be),n(m,ne),n(m,Xe),n(m,$),h(X,$,null),n($,De),n($,se),n($,Ge),n($,re),n(m,We),n(m,H),h(D,H,null),n(H,Ne),n(H,ae),n(m,Ae),n(m,L),h(G,L,null),n(L,Ye),n(L,oe),l(e,Ce,t),h(W,e,t),l(e,Ee,t),l(e,p,t),h(N,p,null),n(p,Qe),n(p,ie),n(p,Se),n(p,le),n(p,Oe),n(p,de),n(p,Ke),n(p,me),n(p,et),n(p,w),h(A,w,null),n(w,tt),n(w,pe),n(w,nt),n(w,ce),n(p,st),n(p,J),h(Y,J,null),n(J,rt),n(J,fe),l(e,Ie,t),h(Q,e,t),l(e,je,t),l(e,he,t),Ve=!0},p(e,[t]){const c={};t&2&&(c.$$scope={dirty:t,ctx:e}),z.$set(c)},i(e){Ve||(_(I.$$.fragment,e),_(j.$$.fragment,e),_(P.$$.fragment,e),_(Z.$$.fragment,e),_(z.$$.fragment,e),_(F.$$.fragment,e),_(B.$$.fragment,e),_(X.$$.fragment,e),_(D.$$.fragment,e),_(G.$$.fragment,e),_(W.$$.fragment,e),_(N.$$.fragment,e),_(A.$$.fragment,e),_(Y.$$.fragment,e),_(Q.$$.fragment,e),Ve=!0)},o(e){k(I.$$.fragment,e),k(j.$$.fragment,e),k(P.$$.fragment,e),k(Z.$$.fragment,e),k(z.$$.fragment,e),k(F.$$.fragment,e),k(B.$$.fragment,e),k(X.$$.fragment,e),k(D.$$.fragment,e),k(G.$$.fragment,e),k(W.$$.fragment,e),k(N.$$.fragment,e),k(A.$$.fragment,e),k(Y.$$.fragment,e),k(Q.$$.fragment,e),Ve=!1},d(e){e&&(s(C),s(T),s(O),s(E),s(be),s(ve),s(M),s(Te),s($e),s(V),s(we),s(R),s(ye),s(U),s(xe),s(q),s(Me),s(ze),s(He),s(Le),s(Je),s(m),s(Ce),s(Ee),s(p),s(Ie),s(je),s(he)),s(f),b(I,e),b(j,e),b(P,e),b(Z,e),b(z,e),b(F,e),b(B),b(X),b(D),b(G),b(W,e),b(N),b(A),b(Y),b(Q,e)}}}const Ut='{"title":"HerBERT","local":"herbert","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"HerbertTokenizer","local":"transformers.HerbertTokenizer","sections":[],"depth":2},{"title":"HerbertTokenizerFast","local":"transformers.HerbertTokenizerFast","sections":[],"depth":2}],"depth":1}';function qt(ke){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gt extends Lt{constructor(f){super(),Jt(this,f,qt,Rt,Mt,{})}}export{Gt as component};
