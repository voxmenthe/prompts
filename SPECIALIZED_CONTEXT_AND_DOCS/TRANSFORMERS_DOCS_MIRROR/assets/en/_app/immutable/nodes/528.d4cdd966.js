import{s as Ue,o as Ze,n as ve}from"../chunks/scheduler.18a86fab.js";import{S as Ne,i as We,g as r,s as n,r as u,A as Ge,h as m,f as l,c as a,j as _e,u as c,x as f,k as Xe,y as ke,a as s,v as h,d as g,t as b,w as M}from"../chunks/index.98837b22.js";import{T as Be}from"../chunks/Tip.77304350.js";import{C as z}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ce,E as Re}from"../chunks/getInferenceSnippets.06c2775f.js";function Ve(Y){let i,y='Refer to the <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli" rel="nofollow">Export a model to ONNX with optimum.exporters.onnx</a> guide for all available arguments or with the command below.',J,p,d;return p=new z({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1oZWxw",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --<span class="hljs-built_in">help</span>',wrap:!1}}),{c(){i=r("p"),i.innerHTML=y,J=n(),u(p.$$.fragment)},l(o){i=m(o,"P",{"data-svelte-h":!0}),f(i)!=="svelte-eldane"&&(i.innerHTML=y),J=a(o),c(p.$$.fragment,o)},m(o,T){s(o,i,T),s(o,J,T),h(p,o,T),d=!0},p:ve,i(o){d||(g(p.$$.fragment,o),d=!0)},o(o){b(p.$$.fragment,o),d=!1},d(o){o&&(l(i),l(J)),M(p,o)}}}function Ce(Y){let i,y,J,p,d,o,T,fe='<a href="http://onnx.ai" rel="nofollow">ONNX</a> is an open standard that defines a common set of operators and a file format to represent deep learning models in different frameworks, including PyTorch and TensorFlow. When a model is exported to ONNX, the operators construct a computational graph (or <em>intermediate representation</em>) which represents the flow of data through the model. Standardized operators and data types makes it easy to switch between frameworks.',O,$,he='The <a href="https://huggingface.co/docs/optimum/index" rel="nofollow">Optimum</a> library exports a model to ONNX with configuration objects which are supported for <a href="https://huggingface.co/docs/optimum/exporters/onnx/overview" rel="nofollow">many architectures</a> and can be easily extended. If a model isn’t supported, feel free to make a <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute" rel="nofollow">contribution</a> to Optimum.',Q,x,ge="The benefits of exporting to ONNX include the following.",S,j,be='<li><a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization" rel="nofollow">Graph optimization</a> and <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization" rel="nofollow">quantization</a> for improving inference.</li> <li>Use the <a href="https://huggingface.co/docs/optimum/v1.27.0/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel" rel="nofollow">ORTModel</a> API to run a model with <a href="https://onnxruntime.ai/" rel="nofollow">ONNX Runtime</a>.</li> <li>Use <a href="https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines" rel="nofollow">optimized inference pipelines</a> for ONNX models.</li>',q,_,Me="Export a Transformers model to ONNX with the Optimum CLI or the <code>optimum.onnxruntime</code> module.",A,X,E,U,Te='Run the command below to install Optimum and the <a href="https://huggingface.co/docs/optimum/exporters/overview" rel="nofollow">exporters</a> module.',P,Z,D,w,K,v,Je="Set the <code>--model</code> argument to export a PyTorch model from the Hub.",ee,N,te,W,we="You should see logs indicating the progress and showing where the resulting <code>model.onnx</code> is saved.",le,G,se,k,ye='For local models, make sure the model weights and tokenizer files are saved in the same directory, for example <code>local_path</code>. Pass the directory to the <code>--model</code> argument and use <code>--task</code> to indicate the <a href="https://huggingface.co/docs/optimum/exporters/task_manager" rel="nofollow">task</a> a model can perform. If <code>--task</code> isn’t provided, the model architecture without a task-specific head is used.',ne,B,ae,R,$e='The <code>model.onnx</code> file can be deployed with any <a href="https://onnx.ai/supported-tools.html#deployModel" rel="nofollow">accelerator</a> that supports ONNX. The example below demonstrates loading and running a model with ONNX Runtime.',oe,V,ie,C,re,F,xe='The <code>optimum.onnxruntime</code> module supports programmatically exporting a Transformers model. Instantiate a <a href="https://huggingface.co/docs/optimum/v1.27.0/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel" rel="nofollow">ORTModel</a> for a task and set <code>export=True</code>. Use <code>~OptimizedModel.save_pretrained</code> to save the ONNX model.',me,I,pe,H,de,L,ue;return d=new ce({props:{title:"ONNX",local:"onnx",headingTag:"h1"}}),X=new ce({props:{title:"Optimum CLI",local:"optimum-cli",headingTag:"h2"}}),Z=new z({props:{code:"cGlwJTIwaW5zdGFsbCUyMG9wdGltdW0lNUJleHBvcnRlcnMlNUQ=",highlighted:"pip install optimum[exporters]",wrap:!1}}),w=new Be({props:{warning:!1,$$slots:{default:[Ve]},$$scope:{ctx:Y}}}),N=new z({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1kaXN0aWxsZWQtc3F1YWQlMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/',wrap:!1}}),G=new z({props:{code:"VmFsaWRhdGluZyUyME9OTlglMjBtb2RlbCUyMGRpc3RpbGJlcnRfYmFzZV91bmNhc2VkX3NxdWFkX29ubnglMkZtb2RlbC5vbm54Li4uJTBBJTA5LSU1QiVFMiU5QyU5MyU1RCUyME9OTlglMjBtb2RlbCUyMG91dHB1dCUyMG5hbWVzJTIwbWF0Y2glMjByZWZlcmVuY2UlMjBtb2RlbCUyMChzdGFydF9sb2dpdHMlMkMlMjBlbmRfbG9naXRzKSUwQSUwOS0lMjBWYWxpZGF0aW5nJTIwT05OWCUyME1vZGVsJTIwb3V0cHV0JTIwJTIyc3RhcnRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBJTA5LSUyMFZhbGlkYXRpbmclMjBPTk5YJTIwTW9kZWwlMjBvdXRwdXQlMjAlMjJlbmRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBVGhlJTIwT05OWCUyMGV4cG9ydCUyMHN1Y2NlZWRlZCUyMGFuZCUyMHRoZSUyMGV4cG9ydGVkJTIwbW9kZWwlMjB3YXMlMjBzYXZlZCUyMGF0JTNBJTIwZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueA==",highlighted:`Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[✓] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output <span class="hljs-string">&quot;start_logits&quot;</span>:
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
	- Validating ONNX Model output <span class="hljs-string">&quot;end_logits&quot;</span>:
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
The ONNX <span class="hljs-built_in">export</span> succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx`,wrap:!1}}),B=new z({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGxvY2FsX3BhdGglMjAtLXRhc2slMjBxdWVzdGlvbi1hbnN3ZXJpbmclMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/',wrap:!1}}),V=new z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5vbm54cnVudGltZSUyMGltcG9ydCUyME9SVE1vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTIyKSUwQW1vZGVsJTIwJTNEJTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueCUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2hhdCUyMGFtJTIwSSUyMHVzaW5nJTNGJTIyJTJDJTIwJTIyVXNpbmclMjBEaXN0aWxCRVJUJTIwd2l0aCUyME9OTlglMjBSdW50aW1lISUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;What am I using?&quot;</span>, <span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),C=new ce({props:{title:"optimum.onnxruntime",local:"optimumonnxruntime",headingTag:"h2"}}),I=new z({props:{code:"ZnJvbSUyMG9wdGltdW0ub25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBPUlRNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2NoZWNrcG9pbnQlMjAlM0QlMjAlMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQtZGlzdGlsbGVkLXNxdWFkJTIyJTBBc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjJvbm54JTJGJTIyJTBBJTBBb3J0X21vZGVsJTIwJTNEJTIwT1JUTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9jaGVja3BvaW50JTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9jaGVja3BvaW50KSUwQSUwQW9ydF9tb2RlbC5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXJlY3RvcnkpJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChzYXZlX2RpcmVjdG9yeSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;distilbert/distilbert-base-uncased-distilled-squad&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>save_directory = <span class="hljs-string">&quot;onnx/&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model.save_pretrained(save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(save_directory)`,wrap:!1}}),H=new Re({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md"}}),{c(){i=r("meta"),y=n(),J=r("p"),p=n(),u(d.$$.fragment),o=n(),T=r("p"),T.innerHTML=fe,O=n(),$=r("p"),$.innerHTML=he,Q=n(),x=r("p"),x.textContent=ge,S=n(),j=r("ul"),j.innerHTML=be,q=n(),_=r("p"),_.innerHTML=Me,A=n(),u(X.$$.fragment),E=n(),U=r("p"),U.innerHTML=Te,P=n(),u(Z.$$.fragment),D=n(),u(w.$$.fragment),K=n(),v=r("p"),v.innerHTML=Je,ee=n(),u(N.$$.fragment),te=n(),W=r("p"),W.innerHTML=we,le=n(),u(G.$$.fragment),se=n(),k=r("p"),k.innerHTML=ye,ne=n(),u(B.$$.fragment),ae=n(),R=r("p"),R.innerHTML=$e,oe=n(),u(V.$$.fragment),ie=n(),u(C.$$.fragment),re=n(),F=r("p"),F.innerHTML=xe,me=n(),u(I.$$.fragment),pe=n(),u(H.$$.fragment),de=n(),L=r("p"),this.h()},l(e){const t=Ge("svelte-u9bgzb",document.head);i=m(t,"META",{name:!0,content:!0}),t.forEach(l),y=a(e),J=m(e,"P",{}),_e(J).forEach(l),p=a(e),c(d.$$.fragment,e),o=a(e),T=m(e,"P",{"data-svelte-h":!0}),f(T)!=="svelte-1bwvbs0"&&(T.innerHTML=fe),O=a(e),$=m(e,"P",{"data-svelte-h":!0}),f($)!=="svelte-i06kks"&&($.innerHTML=he),Q=a(e),x=m(e,"P",{"data-svelte-h":!0}),f(x)!=="svelte-1yf846y"&&(x.textContent=ge),S=a(e),j=m(e,"UL",{"data-svelte-h":!0}),f(j)!=="svelte-1aehpad"&&(j.innerHTML=be),q=a(e),_=m(e,"P",{"data-svelte-h":!0}),f(_)!=="svelte-19toeil"&&(_.innerHTML=Me),A=a(e),c(X.$$.fragment,e),E=a(e),U=m(e,"P",{"data-svelte-h":!0}),f(U)!=="svelte-r42uec"&&(U.innerHTML=Te),P=a(e),c(Z.$$.fragment,e),D=a(e),c(w.$$.fragment,e),K=a(e),v=m(e,"P",{"data-svelte-h":!0}),f(v)!=="svelte-1xjd79s"&&(v.innerHTML=Je),ee=a(e),c(N.$$.fragment,e),te=a(e),W=m(e,"P",{"data-svelte-h":!0}),f(W)!=="svelte-16mbwg7"&&(W.innerHTML=we),le=a(e),c(G.$$.fragment,e),se=a(e),k=m(e,"P",{"data-svelte-h":!0}),f(k)!=="svelte-611ab4"&&(k.innerHTML=ye),ne=a(e),c(B.$$.fragment,e),ae=a(e),R=m(e,"P",{"data-svelte-h":!0}),f(R)!=="svelte-17rgr3s"&&(R.innerHTML=$e),oe=a(e),c(V.$$.fragment,e),ie=a(e),c(C.$$.fragment,e),re=a(e),F=m(e,"P",{"data-svelte-h":!0}),f(F)!=="svelte-cb0fmz"&&(F.innerHTML=xe),me=a(e),c(I.$$.fragment,e),pe=a(e),c(H.$$.fragment,e),de=a(e),L=m(e,"P",{}),_e(L).forEach(l),this.h()},h(){Xe(i,"name","hf:doc:metadata"),Xe(i,"content",Fe)},m(e,t){ke(document.head,i),s(e,y,t),s(e,J,t),s(e,p,t),h(d,e,t),s(e,o,t),s(e,T,t),s(e,O,t),s(e,$,t),s(e,Q,t),s(e,x,t),s(e,S,t),s(e,j,t),s(e,q,t),s(e,_,t),s(e,A,t),h(X,e,t),s(e,E,t),s(e,U,t),s(e,P,t),h(Z,e,t),s(e,D,t),h(w,e,t),s(e,K,t),s(e,v,t),s(e,ee,t),h(N,e,t),s(e,te,t),s(e,W,t),s(e,le,t),h(G,e,t),s(e,se,t),s(e,k,t),s(e,ne,t),h(B,e,t),s(e,ae,t),s(e,R,t),s(e,oe,t),h(V,e,t),s(e,ie,t),h(C,e,t),s(e,re,t),s(e,F,t),s(e,me,t),h(I,e,t),s(e,pe,t),h(H,e,t),s(e,de,t),s(e,L,t),ue=!0},p(e,[t]){const je={};t&2&&(je.$$scope={dirty:t,ctx:e}),w.$set(je)},i(e){ue||(g(d.$$.fragment,e),g(X.$$.fragment,e),g(Z.$$.fragment,e),g(w.$$.fragment,e),g(N.$$.fragment,e),g(G.$$.fragment,e),g(B.$$.fragment,e),g(V.$$.fragment,e),g(C.$$.fragment,e),g(I.$$.fragment,e),g(H.$$.fragment,e),ue=!0)},o(e){b(d.$$.fragment,e),b(X.$$.fragment,e),b(Z.$$.fragment,e),b(w.$$.fragment,e),b(N.$$.fragment,e),b(G.$$.fragment,e),b(B.$$.fragment,e),b(V.$$.fragment,e),b(C.$$.fragment,e),b(I.$$.fragment,e),b(H.$$.fragment,e),ue=!1},d(e){e&&(l(y),l(J),l(p),l(o),l(T),l(O),l($),l(Q),l(x),l(S),l(j),l(q),l(_),l(A),l(E),l(U),l(P),l(D),l(K),l(v),l(ee),l(te),l(W),l(le),l(se),l(k),l(ne),l(ae),l(R),l(oe),l(ie),l(re),l(F),l(me),l(pe),l(de),l(L)),l(i),M(d,e),M(X,e),M(Z,e),M(w,e),M(N,e),M(G,e),M(B,e),M(V,e),M(C,e),M(I,e),M(H,e)}}}const Fe='{"title":"ONNX","local":"onnx","sections":[{"title":"Optimum CLI","local":"optimum-cli","sections":[],"depth":2},{"title":"optimum.onnxruntime","local":"optimumonnxruntime","sections":[],"depth":2}],"depth":1}';function Ie(Y){return Ze(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qe extends Ne{constructor(i){super(),We(this,i,Ie,Ce,Ue,{})}}export{Qe as component};
