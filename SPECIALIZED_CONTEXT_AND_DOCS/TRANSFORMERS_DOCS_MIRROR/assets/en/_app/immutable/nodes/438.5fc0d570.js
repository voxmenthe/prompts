import{s as Ke,o as et,n as Be}from"../chunks/scheduler.18a86fab.js";import{S as tt,i as ot,g as d,s as a,r as u,A as nt,h as c,f as o,c as r,j as O,x as y,u as f,k as z,y as l,a as i,v as h,d as T,t as _,w as M}from"../chunks/index.98837b22.js";import{T as st}from"../chunks/Tip.77304350.js";import{D as de}from"../chunks/Docstring.a1ef7999.js";import{C as Oe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ae}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ce,E as at}from"../chunks/getInferenceSnippets.06c2775f.js";function rt($){let n,x="Examples:",p,m,g;return m=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpVENvbmZpZyUyQyUyMEJlcnRDb25maWclMkMlMjBWaXNpb25UZXh0RHVhbEVuY29kZXJDb25maWclMkMlMjBWaXNpb25UZXh0RHVhbEVuY29kZXJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCRVJUJTIwYW5kJTIwVmlUJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ192aXNpb24lMjAlM0QlMjBWaVRDb25maWcoKSUwQWNvbmZpZ190ZXh0JTIwJTNEJTIwQmVydENvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwVmlzaW9uVGV4dER1YWxFbmNvZGVyQ29uZmlnLmZyb21fdmlzaW9uX3RleHRfY29uZmlncyhjb25maWdfdmlzaW9uJTJDJTIwY29uZmlnX3RleHQlMkMlMjBwcm9qZWN0aW9uX2RpbSUzRDUxMiklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQkVSVCUyMGFuZCUyMFZpVCUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUwQW1vZGVsJTIwJTNEJTIwVmlzaW9uVGV4dER1YWxFbmNvZGVyTW9kZWwoY29uZmlnJTNEY29uZmlnKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ192aXNpb24lMjAlM0QlMjBtb2RlbC5jb25maWcudmlzaW9uX2NvbmZpZyUwQWNvbmZpZ190ZXh0JTIwJTNEJTIwbW9kZWwuY29uZmlnLnRleHRfY29uZmlnJTBBJTBBJTIzJTIwU2F2aW5nJTIwdGhlJTIwbW9kZWwlMkMlMjBpbmNsdWRpbmclMjBpdHMlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMnZpdC1iZXJ0JTIyKSUwQSUwQSUyMyUyMGxvYWRpbmclMjBtb2RlbCUyMGFuZCUyMGNvbmZpZyUyMGZyb20lMjBwcmV0cmFpbmVkJTIwZm9sZGVyJTBBdmlzaW9uX3RleHRfY29uZmlnJTIwJTNEJTIwVmlzaW9uVGV4dER1YWxFbmNvZGVyQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJ2aXQtYmVydCUyMiklMEFtb2RlbCUyMCUzRCUyMFZpc2lvblRleHREdWFsRW5jb2Rlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ2aXQtYmVydCUyMiUyQyUyMGNvbmZpZyUzRHZpc2lvbl90ZXh0X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BERT and ViT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = ViTConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VisionTextDualEncoderConfig.from_vision_text_configs(config_vision, config_text, projection_dim=<span class="hljs-number">512</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BERT and ViT model (with random weights)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionTextDualEncoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = model.config.vision_config
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = model.config.text_config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;vit-bert&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_text_config = VisionTextDualEncoderConfig.from_pretrained(<span class="hljs-string">&quot;vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionTextDualEncoderModel.from_pretrained(<span class="hljs-string">&quot;vit-bert&quot;</span>, config=vision_text_config)`,wrap:!1}}),{c(){n=d("p"),n.textContent=x,p=a(),u(m.$$.fragment)},l(s){n=c(s,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=x),p=r(s),f(m.$$.fragment,s)},m(s,J){i(s,n,J),i(s,p,J),h(m,s,J),g=!0},p:Be,i(s){g||(T(m.$$.fragment,s),g=!0)},o(s){_(m.$$.fragment,s),g=!1},d(s){s&&(o(n),o(p)),M(m,s)}}}function it($){let n,x=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=x},l(p){n=c(p,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=x)},m(p,m){i(p,n,m)},p:Be,d(p){p&&o(n)}}}function lt($){let n,x="Examples:",p,m,g;return m=new Oe({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    VisionTextDualEncoderModel,
<span class="hljs-meta">... </span>    VisionTextDualEncoderProcessor,
<span class="hljs-meta">... </span>    AutoImageProcessor,
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionTextDualEncoderModel.from_vision_text_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>, <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># contrastive training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>urls = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>images = [Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    input_ids=inputs.input_ids,
<span class="hljs-meta">... </span>    attention_mask=inputs.attention_mask,
<span class="hljs-meta">... </span>    pixel_values=inputs.pixel_values,
<span class="hljs-meta">... </span>    return_loss=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits_per_image = outputs.loss, outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionTextDualEncoderModel.from_pretrained(<span class="hljs-string">&quot;vit-bert&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inference</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){n=d("p"),n.textContent=x,p=a(),u(m.$$.fragment)},l(s){n=c(s,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=x),p=r(s),f(m.$$.fragment,s)},m(s,J){i(s,n,J),i(s,p,J),h(m,s,J),g=!0},p:Be,i(s){g||(T(m.$$.fragment,s),g=!0)},o(s){_(m.$$.fragment,s),g=!1},d(s){s&&(o(n),o(p)),M(m,s)}}}function dt($){let n,x,p,m,g,s="<em>This model was released on 2021-11-15 and added to Hugging Face Transformers on 2021-11-30.</em>",J,B,me,C,Re='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',pe,R,ge,H,He=`The <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> can be used to initialize a vision-text dual encoder model with
any pretrained vision autoencoding model as the vision encoder (<em>e.g.</em> <a href="vit">ViT</a>, <a href="beit">BEiT</a>, <a href="deit">DeiT</a>) and any pretrained text autoencoding model as the text encoder (<em>e.g.</em> <a href="roberta">RoBERTa</a>, <a href="bert">BERT</a>). Two projection layers are added on top of both the vision and text encoder to project the output embeddings
to a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a
downstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text
training and then can be used for zero-shot vision tasks such image-classification or retrieval.`,ue,G,Ge=`In <a href="https://huggingface.co/papers/2111.07991" rel="nofollow">LiT: Zero-Shot Transfer with Locked-image Text Tuning</a> it is shown how
leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on
new zero-shot vision tasks such as image classification or retrieval.`,fe,N,he,v,P,we,K,Ne=`<a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> is the configuration class to store the configuration of a
<a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a>. It is used to instantiate <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> model according to the
specified arguments, defining the text model and vision model configs.`,Je,ee,Pe=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ve,I,je,Z,X,Ee,te,Xe=`Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> (or a derived class) from text model configuration and vision
model configuration.`,Te,Y,_e,j,F,Ue,oe,Ye=`Constructs a VisionTextDualEncoder processor which wraps an image processor and a tokenizer into a single
processor.`,$e,ne,Fe=`<a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor">VisionTextDualEncoderProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>.
See the <code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more
information.`,Me,L,ye,b,Q,Ce,se,Le="The bare Vision Text Dual Encoder Model outputting raw hidden-states without any specific head on top.",Ie,ae,Qe=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ze,re,Se=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ke,V,S,We,ie,qe='The <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> forward method, overrides the <code>__call__</code> special method.',De,k,ze,W,ve,q,be,le,xe;return B=new ce({props:{title:"VisionTextDualEncoder",local:"visiontextdualencoder",headingTag:"h1"}}),R=new ce({props:{title:"Overview",local:"overview",headingTag:"h2"}}),N=new ce({props:{title:"VisionTextDualEncoderConfig",local:"transformers.VisionTextDualEncoderConfig",headingTag:"h2"}}),P=new de({props:{name:"class transformers.VisionTextDualEncoderConfig",anchor:"transformers.VisionTextDualEncoderConfig",parameters:[{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VisionTextDualEncoderConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.VisionTextDualEncoderConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The initial value of the <em>logit_scale</em> parameter. Default is used as per the original CLIP implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.VisionTextDualEncoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L34"}}),I=new Ae({props:{anchor:"transformers.VisionTextDualEncoderConfig.example",$$slots:{default:[rt]},$$scope:{ctx:$}}}),X=new de({props:{name:"from_vision_text_configs",anchor:"transformers.VisionTextDualEncoderConfig.from_vision_text_configs",parameters:[{name:"vision_config",val:": PretrainedConfig"},{name:"text_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L109",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"
>VisionTextDualEncoderConfig</a></p>
`}}),Y=new ce({props:{title:"VisionTextDualEncoderProcessor",local:"transformers.VisionTextDualEncoderProcessor",headingTag:"h2"}}),F=new de({props:{name:"class transformers.VisionTextDualEncoderProcessor",anchor:"transformers.VisionTextDualEncoderProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VisionTextDualEncoderProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.VisionTextDualEncoderProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L31"}}),L=new ce({props:{title:"VisionTextDualEncoderModel",local:"transformers.VisionTextDualEncoderModel",headingTag:"h2"}}),Q=new de({props:{name:"class transformers.VisionTextDualEncoderModel",anchor:"transformers.VisionTextDualEncoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.models.vision_text_dual_encoder.configuration_vision_text_dual_encoder.VisionTextDualEncoderConfig] = None"},{name:"vision_model",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"text_model",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],parametersDescription:[{anchor:"transformers.VisionTextDualEncoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a>, <em>optional</em>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.VisionTextDualEncoderModel.vision_model",description:`<strong>vision_model</strong> (<code>~modeling_utils.PreTrainedModel</code>, <em>optional</em>) &#x2014;
The vision model to use.`,name:"vision_model"},{anchor:"transformers.VisionTextDualEncoderModel.text_model",description:`<strong>text_model</strong> (<code>~modeling_utils.PreTrainedModel</code>, <em>optional</em>) &#x2014;
The text model to use.`,name:"text_model"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L46"}}),S=new de({props:{name:"forward",anchor:"transformers.VisionTextDualEncoderModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VisionTextDualEncoderModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.VisionTextDualEncoderModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor">VisionTextDualEncoderProcessor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.VisionTextDualEncoderModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.VisionTextDualEncoderModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.VisionTextDualEncoderModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.VisionTextDualEncoderModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.VisionTextDualEncoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VisionTextDualEncoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VisionTextDualEncoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L187",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_clip.CLIPOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"
>VisionTextDualEncoderConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextModel"
>CLIPTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionModel"
>CLIPVisionModel</a>.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextModel"
>CLIPTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionModel"
>CLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_clip.CLIPOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),k=new st({props:{$$slots:{default:[it]},$$scope:{ctx:$}}}),W=new Ae({props:{anchor:"transformers.VisionTextDualEncoderModel.forward.example",$$slots:{default:[lt]},$$scope:{ctx:$}}}),q=new at({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md"}}),{c(){n=d("meta"),x=a(),p=d("p"),m=a(),g=d("p"),g.innerHTML=s,J=a(),u(B.$$.fragment),me=a(),C=d("div"),C.innerHTML=Re,pe=a(),u(R.$$.fragment),ge=a(),H=d("p"),H.innerHTML=He,ue=a(),G=d("p"),G.innerHTML=Ge,fe=a(),u(N.$$.fragment),he=a(),v=d("div"),u(P.$$.fragment),we=a(),K=d("p"),K.innerHTML=Ne,Je=a(),ee=d("p"),ee.innerHTML=Pe,Ve=a(),u(I.$$.fragment),je=a(),Z=d("div"),u(X.$$.fragment),Ee=a(),te=d("p"),te.innerHTML=Xe,Te=a(),u(Y.$$.fragment),_e=a(),j=d("div"),u(F.$$.fragment),Ue=a(),oe=d("p"),oe.textContent=Ye,$e=a(),ne=d("p"),ne.innerHTML=Fe,Me=a(),u(L.$$.fragment),ye=a(),b=d("div"),u(Q.$$.fragment),Ce=a(),se=d("p"),se.textContent=Le,Ie=a(),ae=d("p"),ae.innerHTML=Qe,Ze=a(),re=d("p"),re.innerHTML=Se,ke=a(),V=d("div"),u(S.$$.fragment),We=a(),ie=d("p"),ie.innerHTML=qe,De=a(),u(k.$$.fragment),ze=a(),u(W.$$.fragment),ve=a(),u(q.$$.fragment),be=a(),le=d("p"),this.h()},l(e){const t=nt("svelte-u9bgzb",document.head);n=c(t,"META",{name:!0,content:!0}),t.forEach(o),x=r(e),p=c(e,"P",{}),O(p).forEach(o),m=r(e),g=c(e,"P",{"data-svelte-h":!0}),y(g)!=="svelte-gaebdn"&&(g.innerHTML=s),J=r(e),f(B.$$.fragment,e),me=r(e),C=c(e,"DIV",{class:!0,"data-svelte-h":!0}),y(C)!=="svelte-b95w5j"&&(C.innerHTML=Re),pe=r(e),f(R.$$.fragment,e),ge=r(e),H=c(e,"P",{"data-svelte-h":!0}),y(H)!=="svelte-1q7ef0o"&&(H.innerHTML=He),ue=r(e),G=c(e,"P",{"data-svelte-h":!0}),y(G)!=="svelte-x6s8p2"&&(G.innerHTML=Ge),fe=r(e),f(N.$$.fragment,e),he=r(e),v=c(e,"DIV",{class:!0});var w=O(v);f(P.$$.fragment,w),we=r(w),K=c(w,"P",{"data-svelte-h":!0}),y(K)!=="svelte-29dqzn"&&(K.innerHTML=Ne),Je=r(w),ee=c(w,"P",{"data-svelte-h":!0}),y(ee)!=="svelte-1ek1ss9"&&(ee.innerHTML=Pe),Ve=r(w),f(I.$$.fragment,w),je=r(w),Z=c(w,"DIV",{class:!0});var A=O(Z);f(X.$$.fragment,A),Ee=r(A),te=c(A,"P",{"data-svelte-h":!0}),y(te)!=="svelte-gwmm38"&&(te.innerHTML=Xe),A.forEach(o),w.forEach(o),Te=r(e),f(Y.$$.fragment,e),_e=r(e),j=c(e,"DIV",{class:!0});var U=O(j);f(F.$$.fragment,U),Ue=r(U),oe=c(U,"P",{"data-svelte-h":!0}),y(oe)!=="svelte-c5g5w0"&&(oe.textContent=Ye),$e=r(U),ne=c(U,"P",{"data-svelte-h":!0}),y(ne)!=="svelte-199hcam"&&(ne.innerHTML=Fe),U.forEach(o),Me=r(e),f(L.$$.fragment,e),ye=r(e),b=c(e,"DIV",{class:!0});var E=O(b);f(Q.$$.fragment,E),Ce=r(E),se=c(E,"P",{"data-svelte-h":!0}),y(se)!=="svelte-1bevpiv"&&(se.textContent=Le),Ie=r(E),ae=c(E,"P",{"data-svelte-h":!0}),y(ae)!=="svelte-q52n56"&&(ae.innerHTML=Qe),Ze=r(E),re=c(E,"P",{"data-svelte-h":!0}),y(re)!=="svelte-hswkmf"&&(re.innerHTML=Se),ke=r(E),V=c(E,"DIV",{class:!0});var D=O(V);f(S.$$.fragment,D),We=r(D),ie=c(D,"P",{"data-svelte-h":!0}),y(ie)!=="svelte-1l6rod7"&&(ie.innerHTML=qe),De=r(D),f(k.$$.fragment,D),ze=r(D),f(W.$$.fragment,D),D.forEach(o),E.forEach(o),ve=r(e),f(q.$$.fragment,e),be=r(e),le=c(e,"P",{}),O(le).forEach(o),this.h()},h(){z(n,"name","hf:doc:metadata"),z(n,"content",ct),z(C,"class","flex flex-wrap space-x-1"),z(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),z(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){l(document.head,n),i(e,x,t),i(e,p,t),i(e,m,t),i(e,g,t),i(e,J,t),h(B,e,t),i(e,me,t),i(e,C,t),i(e,pe,t),h(R,e,t),i(e,ge,t),i(e,H,t),i(e,ue,t),i(e,G,t),i(e,fe,t),h(N,e,t),i(e,he,t),i(e,v,t),h(P,v,null),l(v,we),l(v,K),l(v,Je),l(v,ee),l(v,Ve),h(I,v,null),l(v,je),l(v,Z),h(X,Z,null),l(Z,Ee),l(Z,te),i(e,Te,t),h(Y,e,t),i(e,_e,t),i(e,j,t),h(F,j,null),l(j,Ue),l(j,oe),l(j,$e),l(j,ne),i(e,Me,t),h(L,e,t),i(e,ye,t),i(e,b,t),h(Q,b,null),l(b,Ce),l(b,se),l(b,Ie),l(b,ae),l(b,Ze),l(b,re),l(b,ke),l(b,V),h(S,V,null),l(V,We),l(V,ie),l(V,De),h(k,V,null),l(V,ze),h(W,V,null),i(e,ve,t),h(q,e,t),i(e,be,t),i(e,le,t),xe=!0},p(e,[t]){const w={};t&2&&(w.$$scope={dirty:t,ctx:e}),I.$set(w);const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),k.$set(A);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),W.$set(U)},i(e){xe||(T(B.$$.fragment,e),T(R.$$.fragment,e),T(N.$$.fragment,e),T(P.$$.fragment,e),T(I.$$.fragment,e),T(X.$$.fragment,e),T(Y.$$.fragment,e),T(F.$$.fragment,e),T(L.$$.fragment,e),T(Q.$$.fragment,e),T(S.$$.fragment,e),T(k.$$.fragment,e),T(W.$$.fragment,e),T(q.$$.fragment,e),xe=!0)},o(e){_(B.$$.fragment,e),_(R.$$.fragment,e),_(N.$$.fragment,e),_(P.$$.fragment,e),_(I.$$.fragment,e),_(X.$$.fragment,e),_(Y.$$.fragment,e),_(F.$$.fragment,e),_(L.$$.fragment,e),_(Q.$$.fragment,e),_(S.$$.fragment,e),_(k.$$.fragment,e),_(W.$$.fragment,e),_(q.$$.fragment,e),xe=!1},d(e){e&&(o(x),o(p),o(m),o(g),o(J),o(me),o(C),o(pe),o(ge),o(H),o(ue),o(G),o(fe),o(he),o(v),o(Te),o(_e),o(j),o(Me),o(ye),o(b),o(ve),o(be),o(le)),o(n),M(B,e),M(R,e),M(N,e),M(P),M(I),M(X),M(Y,e),M(F),M(L,e),M(Q),M(S),M(k),M(W),M(q,e)}}}const ct='{"title":"VisionTextDualEncoder","local":"visiontextdualencoder","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"VisionTextDualEncoderConfig","local":"transformers.VisionTextDualEncoderConfig","sections":[],"depth":2},{"title":"VisionTextDualEncoderProcessor","local":"transformers.VisionTextDualEncoderProcessor","sections":[],"depth":2},{"title":"VisionTextDualEncoderModel","local":"transformers.VisionTextDualEncoderModel","sections":[],"depth":2}],"depth":1}';function mt($){return et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mt extends tt{constructor(n){super(),ot(this,n,mt,dt,Ke,{})}}export{Mt as component};
