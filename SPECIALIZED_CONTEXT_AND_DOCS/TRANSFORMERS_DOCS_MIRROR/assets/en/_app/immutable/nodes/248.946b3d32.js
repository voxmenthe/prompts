import{s as ko,z as Uo,o as vo,n as Ve}from"../chunks/scheduler.18a86fab.js";import{S as zo,i as xo,g as c,s as n,r as g,A as Zo,h as d,f as o,c as a,j as k,x as w,u,k as b,l as Bo,y as i,a as l,v as f,d as _,t as M,w as y}from"../chunks/index.98837b22.js";import{T as jo}from"../chunks/Tip.77304350.js";import{D as x}from"../chunks/Docstring.a1ef7999.js";import{C as Ye}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Io}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as O,E as Wo}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as $o,a as Co}from"../chunks/HfOption.6641485e.js";function Ro(J){let t,T;return t=new Ye({props:{code:"aW1wb3J0JTIwcmUlMEFpbXBvcnQlMjB0b3JjaCUwQWltcG9ydCUyMHJlcXVlc3RzJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTJDJTIwSW1hZ2VEcmF3JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBLb3Ntb3MyXzVGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMjBpbmZlcl9kZXZpY2UlMEElMEFyZXBvJTIwJTNEJTIwJTIybWljcm9zb2Z0JTJGa29zbW9zLTIuNSUyMiUwQWRldmljZSUyMCUzRCUyMCUyMmN1ZGElM0EwJTIyJTBBZHR5cGUlMjAlM0QlMjB0b3JjaC5iZmxvYXQxNiUwQW1vZGVsJTIwJTNEJTIwS29zbW9zMl81Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uLmZyb21fcHJldHJhaW5lZChyZXBvJTJDJTIwZGV2aWNlX21hcCUzRGRldmljZSUyQyUyMGR0eXBlJTNEZHR5cGUpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQocmVwbyklMEElMEElMjMlMjBzYW1wbGUlMjBpbWFnZSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZtaWNyb3NvZnQlMkZrb3Ntb3MtMi41JTJGcmVzb2x2ZSUyRm1haW4lMkZyZWNlaXB0XzAwMDA4LnBuZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMiUzQ21kJTNFJTIyJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0Rwcm9tcHQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBaGVpZ2h0JTJDJTIwd2lkdGglMjAlM0QlMjBpbnB1dHMucG9wKCUyMmhlaWdodCUyMiklMkMlMjBpbnB1dHMucG9wKCUyMndpZHRoJTIyKSUwQXJhd193aWR0aCUyQyUyMHJhd19oZWlnaHQlMjAlM0QlMjBpbWFnZS5zaXplJTBBc2NhbGVfaGVpZ2h0JTIwJTNEJTIwcmF3X2hlaWdodCUyMCUyRiUyMGhlaWdodCUwQXNjYWxlX3dpZHRoJTIwJTNEJTIwcmF3X3dpZHRoJTIwJTJGJTIwd2lkdGglMEElMEFpbnB1dHMlMjAlM0QlMjAlN0JrJTNBJTIwdi50byhkZXZpY2UpJTIwaWYlMjB2JTIwaXMlMjBub3QlMjBOb25lJTIwZWxzZSUyME5vbmUlMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwaW5wdXRzLml0ZW1zKCklN0QlMEFpbnB1dHMlNUIlMjJmbGF0dGVuZWRfcGF0Y2hlcyUyMiU1RCUyMCUzRCUyMGlucHV0cyU1QiUyMmZsYXR0ZW5lZF9wYXRjaGVzJTIyJTVELnRvKGR0eXBlKSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSglMEElMjAlMjAlMjAlMjAqKmlucHV0cyUyQyUwQSUyMCUyMCUyMCUyMG1heF9uZXdfdG9rZW5zJTNEMTAyNCUyQyUwQSklMEElMEFnZW5lcmF0ZWRfdGV4dCUyMCUzRCUyMHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQXByaW50KGdlbmVyYXRlZF90ZXh0JTVCMCU1RCk=",highlighted:`<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageDraw
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device

repo = <span class="hljs-string">&quot;microsoft/kosmos-2.5&quot;</span>
device = <span class="hljs-string">&quot;cuda:0&quot;</span>
dtype = torch.bfloat16
model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)
processor = AutoProcessor.from_pretrained(repo)

<span class="hljs-comment"># sample image</span>
url = <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

prompt = <span class="hljs-string">&quot;&lt;md&gt;&quot;</span>
inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

height, width = inputs.pop(<span class="hljs-string">&quot;height&quot;</span>), inputs.pop(<span class="hljs-string">&quot;width&quot;</span>)
raw_width, raw_height = image.size
scale_height = raw_height / height
scale_width = raw_width / width

inputs = {k: v.to(device) <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}
inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>] = inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>].to(dtype)
generated_ids = model.generate(
    **inputs,
    max_new_tokens=<span class="hljs-number">1024</span>,
)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])`,wrap:!1}}),{c(){g(t.$$.fragment)},l(r){u(t.$$.fragment,r)},m(r,h){f(t,r,h),T=!0},p:Ve,i(r){T||(_(t.$$.fragment,r),T=!0)},o(r){M(t.$$.fragment,r),T=!1},d(r){y(t,r)}}}function Fo(J){let t,T;return t=new Ye({props:{code:"aW1wb3J0JTIwcmUlMEFpbXBvcnQlMjB0b3JjaCUwQWltcG9ydCUyMHJlcXVlc3RzJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTJDJTIwSW1hZ2VEcmF3JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBLb3Ntb3MyXzVGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMjBpbmZlcl9kZXZpY2UlMEElMEFyZXBvJTIwJTNEJTIwJTIybWljcm9zb2Z0JTJGa29zbW9zLTIuNSUyMiUwQWRldmljZSUyMCUzRCUyMCUyMmN1ZGElM0EwJTIyJTBBZHR5cGUlMjAlM0QlMjB0b3JjaC5iZmxvYXQxNiUwQW1vZGVsJTIwJTNEJTIwS29zbW9zMl81Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uLmZyb21fcHJldHJhaW5lZChyZXBvJTJDJTIwZGV2aWNlX21hcCUzRGRldmljZSUyQyUyMGR0eXBlJTNEZHR5cGUpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQocmVwbyklMEElMEElMjMlMjBzYW1wbGUlMjBpbWFnZSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZtaWNyb3NvZnQlMkZrb3Ntb3MtMi41JTJGcmVzb2x2ZSUyRm1haW4lMkZyZWNlaXB0XzAwMDA4LnBuZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQSUyMyUyMGJzJTIwJTNEJTIwMSUwQXByb21wdCUyMCUzRCUyMCUyMiUzQ29jciUzRSUyMiUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcHJvbXB0JTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQWhlaWdodCUyQyUyMHdpZHRoJTIwJTNEJTIwaW5wdXRzLnBvcCglMjJoZWlnaHQlMjIpJTJDJTIwaW5wdXRzLnBvcCglMjJ3aWR0aCUyMiklMEFyYXdfd2lkdGglMkMlMjByYXdfaGVpZ2h0JTIwJTNEJTIwaW1hZ2Uuc2l6ZSUwQXNjYWxlX2hlaWdodCUyMCUzRCUyMHJhd19oZWlnaHQlMjAlMkYlMjBoZWlnaHQlMEFzY2FsZV93aWR0aCUyMCUzRCUyMHJhd193aWR0aCUyMCUyRiUyMHdpZHRoJTBBJTBBJTIzJTIwYnMlMjAlM0UlMjAxJTJDJTIwYmF0Y2glMjBnZW5lcmF0aW9uJTBBJTIzJTIwaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0QlNUJwcm9tcHQlMkMlMjBwcm9tcHQlNUQlMkMlMjBpbWFnZXMlM0QlNUJpbWFnZSUyQ2ltYWdlJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjMlMjBoZWlnaHQlMkMlMjB3aWR0aCUyMCUzRCUyMGlucHV0cy5wb3AoJTIyaGVpZ2h0JTIyKSUyQyUyMGlucHV0cy5wb3AoJTIyd2lkdGglMjIpJTBBJTIzJTIwcmF3X3dpZHRoJTJDJTIwcmF3X2hlaWdodCUyMCUzRCUyMGltYWdlLnNpemUlMEElMjMlMjBzY2FsZV9oZWlnaHQlMjAlM0QlMjByYXdfaGVpZ2h0JTIwJTJGJTIwaGVpZ2h0JTVCMCU1RCUwQSUyMyUyMHNjYWxlX3dpZHRoJTIwJTNEJTIwcmF3X3dpZHRoJTIwJTJGJTIwd2lkdGglNUIwJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwJTdCayUzQSUyMHYudG8oZGV2aWNlKSUyMGlmJTIwdiUyMGlzJTIwbm90JTIwTm9uZSUyMGVsc2UlMjBOb25lJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMGlucHV0cy5pdGVtcygpJTdEJTBBaW5wdXRzJTVCJTIyZmxhdHRlbmVkX3BhdGNoZXMlMjIlNUQlMjAlM0QlMjBpbnB1dHMlNUIlMjJmbGF0dGVuZWRfcGF0Y2hlcyUyMiU1RC50byhkdHlwZSklMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoJTBBJTIwJTIwJTIwJTIwKippbnB1dHMlMkMlMEElMjAlMjAlMjAlMjBtYXhfbmV3X3Rva2VucyUzRDEwMjQlMkMlMEEpJTBBJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFkZWYlMjBwb3N0X3Byb2Nlc3MoeSUyQyUyMHNjYWxlX2hlaWdodCUyQyUyMHNjYWxlX3dpZHRoKSUzQSUwQSUyMCUyMCUyMCUyMHklMjAlM0QlMjB5LnJlcGxhY2UocHJvbXB0JTJDJTIwJTIyJTIyKSUwQSUyMCUyMCUyMCUyMGlmJTIwJTIyJTNDbWQlM0UlMjIlMjBpbiUyMHByb21wdCUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHJldHVybiUyMHklMEElMjAlMjAlMjAlMjBwYXR0ZXJuJTIwJTNEJTIwciUyMiUzQ2Jib3glM0UlM0N4XyU1Q2QlMkIlM0UlM0N5XyU1Q2QlMkIlM0UlM0N4XyU1Q2QlMkIlM0UlM0N5XyU1Q2QlMkIlM0UlM0MlMkZiYm94JTNFJTIyJTBBJTIwJTIwJTIwJTIwYmJveHNfcmF3JTIwJTNEJTIwcmUuZmluZGFsbChwYXR0ZXJuJTJDJTIweSklMEElMjAlMjAlMjAlMjBsaW5lcyUyMCUzRCUyMHJlLnNwbGl0KHBhdHRlcm4lMkMlMjB5KSU1QjElM0ElNUQlMEElMjAlMjAlMjAlMjBiYm94cyUyMCUzRCUyMCU1QnJlLmZpbmRhbGwociUyMiU1Q2QlMkIlMjIlMkMlMjBpKSUyMGZvciUyMGklMjBpbiUyMGJib3hzX3JhdyU1RCUwQSUyMCUyMCUyMCUyMGJib3hzJTIwJTNEJTIwJTVCJTVCaW50KGopJTIwZm9yJTIwaiUyMGluJTIwaSU1RCUyMGZvciUyMGklMjBpbiUyMGJib3hzJTVEJTBBJTIwJTIwJTIwJTIwaW5mbyUyMCUzRCUyMCUyMiUyMiUwQSUyMCUyMCUyMCUyMGZvciUyMGklMjBpbiUyMHJhbmdlKGxlbihsaW5lcykpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYm94JTIwJTNEJTIwYmJveHMlNUJpJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIweDAlMkMlMjB5MCUyQyUyMHgxJTJDJTIweTElMjAlM0QlMjBib3glMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpZiUyMG5vdCUyMCh4MCUyMCUzRSUzRCUyMHgxJTIwb3IlMjB5MCUyMCUzRSUzRCUyMHkxKSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHgwJTIwJTNEJTIwaW50KHgwJTIwKiUyMHNjYWxlX3dpZHRoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHkwJTIwJTNEJTIwaW50KHkwJTIwKiUyMHNjYWxlX2hlaWdodCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB4MSUyMCUzRCUyMGludCh4MSUyMColMjBzY2FsZV93aWR0aCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB5MSUyMCUzRCUyMGludCh5MSUyMColMjBzY2FsZV9oZWlnaHQpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaW5mbyUyMCUyQiUzRCUyMGYlMjIlN0J4MCU3RCUyQyU3QnkwJTdEJTJDJTdCeDElN0QlMkMlN0J5MCU3RCUyQyU3QngxJTdEJTJDJTdCeTElN0QlMkMlN0J4MCU3RCUyQyU3QnkxJTdEJTJDJTdCbGluZXMlNUJpJTVEJTdEJTIyJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5mbyUwQSUwQW91dHB1dF90ZXh0JTIwJTNEJTIwcG9zdF9wcm9jZXNzKGdlbmVyYXRlZF90ZXh0JTVCMCU1RCUyQyUyMHNjYWxlX2hlaWdodCUyQyUyMHNjYWxlX3dpZHRoKSUwQXByaW50KG91dHB1dF90ZXh0KSUwQSUwQWRyYXclMjAlM0QlMjBJbWFnZURyYXcuRHJhdyhpbWFnZSklMEFsaW5lcyUyMCUzRCUyMG91dHB1dF90ZXh0LnNwbGl0KCUyMiU1Q24lMjIpJTBBZm9yJTIwbGluZSUyMGluJTIwbGluZXMlM0ElMEElMjAlMjAlMjAlMjAlMjMlMjBkcmF3JTIwdGhlJTIwYm91bmRpbmclMjBib3glMEElMjAlMjAlMjAlMjBsaW5lJTIwJTNEJTIwbGlzdChsaW5lLnNwbGl0KCUyMiUyQyUyMikpJTBBJTIwJTIwJTIwJTIwaWYlMjBsZW4obGluZSklMjAlM0MlMjA4JTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwY29udGludWUlMEElMjAlMjAlMjAlMjBsaW5lJTIwJTNEJTIwbGlzdChtYXAoaW50JTJDJTIwbGluZSU1QiUzQTglNUQpKSUwQSUyMCUyMCUyMCUyMGRyYXcucG9seWdvbihsaW5lJTJDJTIwb3V0bGluZSUzRCUyMnJlZCUyMiklMEFpbWFnZS5zYXZlKCUyMm91dHB1dC5wbmclMjIp",highlighted:`<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageDraw
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device

repo = <span class="hljs-string">&quot;microsoft/kosmos-2.5&quot;</span>
device = <span class="hljs-string">&quot;cuda:0&quot;</span>
dtype = torch.bfloat16
model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)
processor = AutoProcessor.from_pretrained(repo)

<span class="hljs-comment"># sample image</span>
url = <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-comment"># bs = 1</span>
prompt = <span class="hljs-string">&quot;&lt;ocr&gt;&quot;</span>
inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
height, width = inputs.pop(<span class="hljs-string">&quot;height&quot;</span>), inputs.pop(<span class="hljs-string">&quot;width&quot;</span>)
raw_width, raw_height = image.size
scale_height = raw_height / height
scale_width = raw_width / width

<span class="hljs-comment"># bs &gt; 1, batch generation</span>
<span class="hljs-comment"># inputs = processor(text=[prompt, prompt], images=[image,image], return_tensors=&quot;pt&quot;)</span>
<span class="hljs-comment"># height, width = inputs.pop(&quot;height&quot;), inputs.pop(&quot;width&quot;)</span>
<span class="hljs-comment"># raw_width, raw_height = image.size</span>
<span class="hljs-comment"># scale_height = raw_height / height[0]</span>
<span class="hljs-comment"># scale_width = raw_width / width[0]</span>

inputs = {k: v.to(device) <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}
inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>] = inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>].to(dtype)
generated_ids = model.generate(
    **inputs,
    max_new_tokens=<span class="hljs-number">1024</span>,
)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title function_">post_process</span>(<span class="hljs-params">y, scale_height, scale_width</span>):
    y = y.replace(prompt, <span class="hljs-string">&quot;&quot;</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;&lt;md&gt;&quot;</span> <span class="hljs-keyword">in</span> prompt:
        <span class="hljs-keyword">return</span> y
    pattern = <span class="hljs-string">r&quot;&lt;bbox&gt;&lt;x_\\d+&gt;&lt;y_\\d+&gt;&lt;x_\\d+&gt;&lt;y_\\d+&gt;&lt;/bbox&gt;&quot;</span>
    bboxs_raw = re.findall(pattern, y)
    lines = re.split(pattern, y)[<span class="hljs-number">1</span>:]
    bboxs = [re.findall(<span class="hljs-string">r&quot;\\d+&quot;</span>, i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> bboxs_raw]
    bboxs = [[<span class="hljs-built_in">int</span>(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> bboxs]
    info = <span class="hljs-string">&quot;&quot;</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lines)):
        box = bboxs[i]
        x0, y0, x1, y1 = box
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (x0 &gt;= x1 <span class="hljs-keyword">or</span> y0 &gt;= y1):
            x0 = <span class="hljs-built_in">int</span>(x0 * scale_width)
            y0 = <span class="hljs-built_in">int</span>(y0 * scale_height)
            x1 = <span class="hljs-built_in">int</span>(x1 * scale_width)
            y1 = <span class="hljs-built_in">int</span>(y1 * scale_height)
            info += <span class="hljs-string">f&quot;<span class="hljs-subst">{x0}</span>,<span class="hljs-subst">{y0}</span>,<span class="hljs-subst">{x1}</span>,<span class="hljs-subst">{y0}</span>,<span class="hljs-subst">{x1}</span>,<span class="hljs-subst">{y1}</span>,<span class="hljs-subst">{x0}</span>,<span class="hljs-subst">{y1}</span>,<span class="hljs-subst">{lines[i]}</span>&quot;</span>
    <span class="hljs-keyword">return</span> info

output_text = post_process(generated_text[<span class="hljs-number">0</span>], scale_height, scale_width)
<span class="hljs-built_in">print</span>(output_text)

draw = ImageDraw.Draw(image)
lines = output_text.split(<span class="hljs-string">&quot;\\n&quot;</span>)
<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:
    <span class="hljs-comment"># draw the bounding box</span>
    line = <span class="hljs-built_in">list</span>(line.split(<span class="hljs-string">&quot;,&quot;</span>))
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(line) &lt; <span class="hljs-number">8</span>:
        <span class="hljs-keyword">continue</span>
    line = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>, line[:<span class="hljs-number">8</span>]))
    draw.polygon(line, outline=<span class="hljs-string">&quot;red&quot;</span>)
image.save(<span class="hljs-string">&quot;output.png&quot;</span>)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(r){u(t.$$.fragment,r)},m(r,h){f(t,r,h),T=!0},p:Ve,i(r){T||(_(t.$$.fragment,r),T=!0)},o(r){M(t.$$.fragment,r),T=!1},d(r){y(t,r)}}}function Go(J){let t,T,r,h;return t=new Co({props:{id:"usage",option:"AutoModel - Markdown Task",$$slots:{default:[Ro]},$$scope:{ctx:J}}}),r=new Co({props:{id:"usage",option:"AutoModel - OCR Task",$$slots:{default:[Fo]},$$scope:{ctx:J}}}),{c(){g(t.$$.fragment),T=n(),g(r.$$.fragment)},l(p){u(t.$$.fragment,p),T=a(p),u(r.$$.fragment,p)},m(p,m){f(t,p,m),l(p,T,m),f(r,p,m),h=!0},p(p,m){const U={};m&2&&(U.$$scope={dirty:m,ctx:p}),t.$set(U);const Z={};m&2&&(Z.$$scope={dirty:m,ctx:p}),r.$set(Z)},i(p){h||(_(t.$$.fragment,p),_(r.$$.fragment,p),h=!0)},o(p){M(t.$$.fragment,p),M(r.$$.fragment,p),h=!1},d(p){p&&o(T),y(t,p),y(r,p)}}}function No(J){let t,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=T},l(r){t=d(r,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=T)},m(r,h){l(r,t,h)},p:Ve,d(r){r&&o(t)}}}function Xo(J){let t,T="Examples:",r,h,p;return h=new Ye({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEtvc21vczJfNU1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBLb3Ntb3MyXzVNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGa29zbW9zMi41JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmtvc21vczIuNSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGbWljcm9zb2Z0JTJGa29zbW9zMi41JTJGcmVzb2x2ZSUyRm1haW4lMkZzbm93bWFuLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQXRleHQlMjAlM0QlMjAoJTBBJTIwJTIwJTIwJTIwJTIyJTNDZ3JvdW5kaW5nJTNFJTIwQW4lMjBpbWFnZSUyMG9mJTNDcGhyYXNlJTNFJTIwYSUyMHNub3dtYW4lM0MlMkZwaHJhc2UlM0UlM0NvYmplY3QlM0UlM0NwYXRjaF9pbmRleF8wMDQ0JTNFJTNDcGF0Y2hfaW5kZXhfMDg2MyUzRSUyMiUwQSUyMCUyMCUyMCUyMCUyMiUzQyUyRm9iamVjdCUzRSUyMHdhcm1pbmclMjBoaW1zZWxmJTIwYnklM0NwaHJhc2UlM0UlMjBhJTIwZmlyZSUzQyUyRnBocmFzZSUzRSUzQ29iamVjdCUzRSUzQ3BhdGNoX2luZGV4XzAwMDUlM0UlM0NwYXRjaF9pbmRleF8wOTExJTNFJTIyJTBBJTIwJTIwJTIwJTIwJTIyJTNDJTJGb2JqZWN0JTNFJTIyJTBBKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEdGV4dCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMGFkZF9lb3NfdG9rZW4lM0RUcnVlKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwbW9kZWwoJTBBJTIwJTIwJTIwJTIwcGl4ZWxfdmFsdWVzJTNEaW5wdXRzJTVCJTIycGl4ZWxfdmFsdWVzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwaW5wdXRfaWRzJTNEaW5wdXRzJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwYXR0ZW50aW9uX21hc2slM0RpbnB1dHMlNUIlMjJhdHRlbnRpb25fbWFzayUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGltYWdlX2VtYmVkc19wb3NpdGlvbl9tYXNrJTNEaW5wdXRzJTVCJTIyaW1hZ2VfZW1iZWRzX3Bvc2l0aW9uX21hc2slMjIlNUQlMkMlMEEpLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZS5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Kosmos2_5Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Kosmos2_5Model.from_pretrained(<span class="hljs-string">&quot;microsoft/kosmos2.5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/kosmos2.5&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos2.5/resolve/main/snowman.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>text = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&lt;grounding&gt; An image of&lt;phrase&gt; a snowman&lt;/phrase&gt;&lt;object&gt;&lt;patch_index_0044&gt;&lt;patch_index_0863&gt;&quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&lt;/object&gt; warming himself by&lt;phrase&gt; a fire&lt;/phrase&gt;&lt;object&gt;&lt;patch_index_0005&gt;&lt;patch_index_0911&gt;&quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&lt;/object&gt;&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=text, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, add_eos_token=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = model(
<span class="hljs-meta">... </span>    pixel_values=inputs[<span class="hljs-string">&quot;pixel_values&quot;</span>],
<span class="hljs-meta">... </span>    input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>],
<span class="hljs-meta">... </span>    attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>],
<span class="hljs-meta">... </span>    image_embeds_position_mask=inputs[<span class="hljs-string">&quot;image_embeds_position_mask&quot;</span>],
<span class="hljs-meta">... </span>).last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_state.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">91</span>, <span class="hljs-number">2048</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=T,r=n(),g(h.$$.fragment)},l(m){t=d(m,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=T),r=a(m),u(h.$$.fragment,m)},m(m,U){l(m,t,U),l(m,r,U),f(h,m,U),p=!0},p:Ve,i(m){p||(_(h.$$.fragment,m),p=!0)},o(m){M(h.$$.fragment,m),p=!1},d(m){m&&(o(t),o(r)),y(h,m)}}}function Ko(J){let t,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=T},l(r){t=d(r,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=T)},m(r,h){l(r,t,h)},p:Ve,d(r){r&&o(t)}}}function Qo(J){let t,T="Examples:",r,h,p;return h=new Ye({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwS29zbW9zMl81Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBcmVwbyUyMCUzRCUyMCUyMm1pY3Jvc29mdCUyRmtvc21vcy0yLjUlMjIlMEFkZXZpY2UlMjAlM0QlMjAlMjJjdWRhJTNBMCUyMiUwQWR0eXBlJTIwJTNEJTIwdG9yY2guYmZsb2F0MTYlMjAlMjMlMjB0b3JjaC5mbG9hdDE2JTBBbW9kZWwlMjAlM0QlMjBLb3Ntb3MyXzVGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKHJlcG8lMkMlMjBkZXZpY2VfbWFwJTNEZGV2aWNlJTJDJTIwZHR5cGUlM0RkdHlwZSklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChyZXBvKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZtaWNyb3NvZnQlMkZrb3Ntb3MtMi41JTJGcmVzb2x2ZSUyRm1haW4lMkZyZWNlaXB0XzAwMDA4LnBuZyUyMiUwQSUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMiUzQ29jciUzRSUyMiUyMCUyMyUyMCUzQ21kJTNFJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0Rwcm9tcHQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBaGVpZ2h0JTJDJTIwd2lkdGglMjAlM0QlMjBpbnB1dHMucG9wKCUyMmhlaWdodCUyMiklMkMlMjBpbnB1dHMucG9wKCUyMndpZHRoJTIyKSUwQWlucHV0cyUyMCUzRCUyMCU3QmslM0ElMjB2LnRvKGRldmljZSklMjBpZiUyMHYlMjBpcyUyMG5vdCUyME5vbmUlMjBlbHNlJTIwTm9uZSUyMGZvciUyMGslMkMlMjB2JTIwaW4lMjBpbnB1dHMuaXRlbXMoKSU3RCUwQWlucHV0cyU1QiUyMmZsYXR0ZW5lZF9wYXRjaGVzJTIyJTVEJTIwJTNEJTIwaW5wdXRzJTVCJTIyZmxhdHRlbmVkX3BhdGNoZXMlMjIlNUQudG8oZHR5cGUpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDbWF4X25ld190b2tlbnMlM0QxMDI0KSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQWdlbmVyYXRlZF90ZXh0",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Kosmos2_5ForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>repo = <span class="hljs-string">&quot;microsoft/kosmos-2.5&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda:0&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dtype = torch.bfloat16 <span class="hljs-comment"># torch.float16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, dtype=dtype)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(repo)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;&lt;ocr&gt;&quot;</span> <span class="hljs-comment"># &lt;md&gt;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>height, width = inputs.pop(<span class="hljs-string">&quot;height&quot;</span>), inputs.pop(<span class="hljs-string">&quot;width&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: v.to(device) <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>] = inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>].to(dtype)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs,max_new_tokens=<span class="hljs-number">1024</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text
<span class="hljs-string">&#x27;&lt;ocr&gt;&lt;bbox&gt;&lt;x_53&gt;&lt;y_573&gt;&lt;x_69&gt;&lt;y_606&gt;&lt;/bbox&gt;1\\n&lt;bbox&gt;&lt;x_79&gt;&lt;y_573&gt;&lt;x_464&gt;&lt;y_612&gt;&lt;/bbox&gt;[REG] BLACK SAKURA\\n&lt;bbox&gt;&lt;x_690&gt;&lt;y_569&gt;&lt;x_810&gt;&lt;y_606&gt;&lt;/bbox&gt;45,455\\n&lt;bbox&gt;&lt;x_53&gt;&lt;y_614&gt;&lt;x_69&gt;&lt;y_648&gt;&lt;/bbox&gt;1\\n&lt;bbox&gt;&lt;x_79&gt;&lt;y_614&gt;&lt;x_468&gt;&lt;y_650&gt;&lt;/bbox&gt;COOKIE DOH SAUCES\\n&lt;bbox&gt;&lt;x_788&gt;&lt;y_609&gt;&lt;x_812&gt;&lt;y_644&gt;&lt;/bbox&gt;0\\n&lt;bbox&gt;&lt;x_50&gt;&lt;y_658&gt;&lt;x_69&gt;&lt;y_693&gt;&lt;/bbox&gt;1\\n&lt;bbox&gt;&lt;x_79&gt;&lt;y_658&gt;&lt;x_358&gt;&lt;y_693&gt;&lt;/bbox&gt;NATA DE COCO\\n&lt;bbox&gt;&lt;x_790&gt;&lt;y_652&gt;&lt;x_814&gt;&lt;y_687&gt;&lt;/bbox&gt;0\\n&lt;bbox&gt;&lt;x_31&gt;&lt;y_742&gt;&lt;x_820&gt;&lt;y_781&gt;&lt;/bbox&gt;Sub Total 45,455\\n&lt;bbox&gt;&lt;x_27&gt;&lt;y_781&gt;&lt;x_822&gt;&lt;y_827&gt;&lt;/bbox&gt;PB1 (10%) 4,545\\n&lt;bbox&gt;&lt;x_27&gt;&lt;y_826&gt;&lt;x_824&gt;&lt;y_872&gt;&lt;/bbox&gt;Rounding 0\\n&lt;bbox&gt;&lt;x_24&gt;&lt;y_872&gt;&lt;x_827&gt;&lt;y_921&gt;&lt;/bbox&gt;Total 50,000\\n&lt;bbox&gt;&lt;x_17&gt;&lt;y_1056&gt;&lt;x_836&gt;&lt;y_1108&gt;&lt;/bbox&gt;Card Payment 50,000\\n&#x27;</span>`,wrap:!1}}),{c(){t=c("p"),t.textContent=T,r=n(),g(h.$$.fragment)},l(m){t=d(m,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=T),r=a(m),u(h.$$.fragment,m)},m(m,U){l(m,t,U),l(m,r,U),f(h,m,U),p=!0},p:Ve,i(m){p||(_(h.$$.fragment,m),p=!0)},o(m){M(h.$$.fragment,m),p=!1},d(m){m&&(o(t),o(r)),y(h,m)}}}function Ho(J){let t,T,r,h,p,m="<em>This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-19.</em>",U,Z,Ls='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',De,ee,Le,se,Os='The Kosmos-2.5 model was proposed in <a href="https://huggingface.co/papers/2309.11419/" rel="nofollow">KOSMOS-2.5: A Multimodal Literate Model</a> by Microsoft.',Oe,oe,eo="The abstract from the paper is the following:",es,te,so="<em>We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.</em>",ss,K,oo,os,Q,to,ts,ne,no='Overview of tasks that KOSMOS-2.5 can handle. Taken from the <a href="https://huggingface.co/papers/2309.11419">original paper</a>.',ns,ae,ao='The examples below demonstrates how to generate with <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, for both Markdown and OCR tasks.',as,H,rs,re,ls,le,ro="The authors also released Kosmos-2.5 Chat, which is a chat version optimized for document understanding. You can use it like so:",is,ie,cs,ce,ds,G,de,Is,ze,lo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model">Kosmos2_5Model</a>. It is used to instantiate a
KOSMOS-2.5 model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the KOSMOS-2.5
<a href="https://huggingface.co/microsoft/kosmos-2.5" rel="nofollow">microsoft/kosmos-2.5</a> architecture.`,ms,me,ps,B,pe,Cs,xe,io="Constructs a Kosmos2_5 image processor.",ks,A,he,vs,Ze,co=`Preprocess an image or batch of images. The processor first computes the maximum possible number of
aspect-ratio preserving patches of size <code>patch_size</code> that can be extracted from the image. It then pads the
image with zeros to make the image respect the constraint of <code>max_patches</code>. Before extracting the patches the
images are standardized following the tensorflow implementation of <code>per_image_standardization</code>
(<a href="https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization" rel="nofollow">https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization</a>).`,hs,ge,gs,W,ue,zs,Be,mo="Constructs a fast Kosmos2 5 image processor.",xs,We,fe,us,_e,fs,j,Me,Zs,$e,po=`Constructs a Kosmos2_5 processor which wraps a PreTrainedTokenizerFast and Kosmos2_5 image processor into a single
processor.`,Bs,Re,ho=`<a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Processor">Kosmos2_5Processor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessor">Kosmos2_5ImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>. See
the docstring of <code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Processor.decode">decode()</a> for more information.`,Ws,q,ye,$s,Fe,go=`This method forwards all its arguments to Kosmos2_5TokenizerFast’s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode">batch_decode()</a>.
Please refer to the docstring of this method for more information.`,Rs,E,Te,Fs,Ge,uo=`This method forwards all its arguments to Kosmos2_5TokenizerFast’s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode">decode()</a>. Please
refer to the docstring of this method for more information.`,_s,we,Ms,I,be,Gs,Ne,fo="KOSMOS-2.5 Model for generating text and image features. The model consists of a vision encoder and a language model.",Ns,Xe,_o=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Xs,Ke,Mo=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ks,v,Je,Qs,Qe,yo='The <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model">Kosmos2_5Model</a> forward method, overrides the <code>__call__</code> special method.',Hs,V,As,S,ys,Ue,Ts,C,je,qs,He,To=`KOSMOS-2.5 Model for generating text and bounding boxes given an image. The model consists of a vision encoder and a
language model.`,Es,Ae,wo=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Vs,qe,bo=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ss,z,Ie,Ps,Ee,Jo='The <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration">Kosmos2_5ForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Ys,P,Ds,Y,ws,Ce,bs,Se,Js;return ee=new O({props:{title:"KOSMOS-2.5",local:"kosmos-25",headingTag:"h1"}}),H=new $o({props:{id:"usage",options:["AutoModel - Markdown Task","AutoModel - OCR Task"],$$slots:{default:[Go]},$$scope:{ctx:J}}}),re=new O({props:{title:"Chat version",local:"chat-version",headingTag:"h2"}}),ie=new Ye({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageDraw
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Kosmos2_5ForConditionalGeneration

repo = <span class="hljs-string">&quot;microsoft/kosmos-2.5-chat&quot;</span>
device = <span class="hljs-string">&quot;cuda:0&quot;</span>
dtype = torch.bfloat16

model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo,
                                                          device_map=device,
                                                          torch_dtype=dtype,
                                                          attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>)
processor = AutoProcessor.from_pretrained(repo)

<span class="hljs-comment"># sample image</span>
url = <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png&quot;</span>

image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

question = <span class="hljs-string">&quot;What is the sub total of the receipt?&quot;</span>
template = <span class="hljs-string">&quot;&lt;md&gt;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#x27;s questions. USER: {} ASSISTANT:&quot;</span>
prompt = template.<span class="hljs-built_in">format</span>(question)
inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

height, width = inputs.pop(<span class="hljs-string">&quot;height&quot;</span>), inputs.pop(<span class="hljs-string">&quot;width&quot;</span>)
raw_width, raw_height = image.size
scale_height = raw_height / height
scale_width = raw_width / width

inputs = {k: v.to(device) <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}
inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>] = inputs[<span class="hljs-string">&quot;flattened_patches&quot;</span>].to(dtype)
generated_ids = model.generate(
    **inputs,
    max_new_tokens=<span class="hljs-number">1024</span>,
)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])`,wrap:!1}}),ce=new O({props:{title:"Kosmos2_5Config",local:"transformers.Kosmos2_5Config",headingTag:"h2"}}),de=new x({props:{name:"class transformers.Kosmos2_5Config",anchor:"transformers.Kosmos2_5Config",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"latent_query_num",val:" = 2048"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Kosmos2_5Config.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <code>Kosmos2_5TextConfig</code>.`,name:"text_config"},{anchor:"transformers.Kosmos2_5Config.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <code>Kosmos2_5VisionConfig</code>.`,name:"vision_config"},{anchor:"transformers.Kosmos2_5Config.latent_query_num",description:`<strong>latent_query_num</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The number of latent query tokens that represent the image features used in the text decoder component.`,name:"latent_query_num"},{anchor:"transformers.Kosmos2_5Config.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/configuration_kosmos2_5.py#L212"}}),me=new O({props:{title:"Kosmos2_5ImageProcessor",local:"transformers.Kosmos2_5ImageProcessor",headingTag:"h2"}}),pe=new x({props:{name:"class transformers.Kosmos2_5ImageProcessor",anchor:"transformers.Kosmos2_5ImageProcessor",parameters:[{name:"do_convert_rgb",val:": bool = True"},{name:"do_normalize",val:": bool = True"},{name:"patch_size",val:": typing.Optional[dict[str, int]] = None"},{name:"max_patches",val:": int = 4096"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Kosmos2_5ImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.Kosmos2_5ImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. According to Kosmos2_5 paper and code, the image is normalized with its own mean and standard
deviation.`,name:"do_normalize"},{anchor:"transformers.Kosmos2_5ImageProcessor.patch_size",description:`<strong>patch_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 16, &quot;width&quot;: 16}</code>):
The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.`,name:"patch_size"},{anchor:"transformers.Kosmos2_5ImageProcessor.max_patches",description:`<strong>max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
The maximum number of patches to extract from the image as per the
<a href="https://huggingface.co/papers/2309.11419" rel="nofollow">KOSMOS 2.5 paper</a>.`,name:"max_patches"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py#L76"}}),he=new x({props:{name:"preprocess",anchor:"transformers.Kosmos2_5ImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"max_patches",val:": typing.Optional[int] = None"},{name:"patch_size",val:": typing.Optional[dict[str, int]] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images.`,name:"images"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.max_patches",description:`<strong>max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.max_patches</code>) &#x2014;
Maximum number of patches to extract.`,name:"max_patches"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.patch_size",description:`<strong>patch_size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>self.patch_size</code>) &#x2014;
Dictionary containing the patch height and width.`,name:"patch_size"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.Kosmos2_5ImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py#L241"}}),ge=new O({props:{title:"Kosmos2_5ImageProcessorFast",local:"transformers.Kosmos2_5ImageProcessorFast",headingTag:"h2"}}),ue=new x({props:{name:"class transformers.Kosmos2_5ImageProcessorFast",anchor:"transformers.Kosmos2_5ImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.kosmos2_5.image_processing_kosmos2_5_fast.Kosmos2_5FastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py#L76"}}),fe=new x({props:{name:"preprocess",anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.kosmos2_5.image_processing_kosmos2_5_fast.Kosmos2_5FastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.patch_size",description:`<strong>patch_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 16, &quot;width&quot;: 16}</code>):
The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.`,name:"patch_size"},{anchor:"transformers.Kosmos2_5ImageProcessorFast.preprocess.max_patches",description:`<strong>max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
The maximum number of patches to extract from the image as per the
<a href="https://huggingface.co/papers/2309.11419" rel="nofollow">KOSMOS 2.5 paper</a>.`,name:"max_patches"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py#L89",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),_e=new O({props:{title:"Kosmos2_5Processor",local:"transformers.Kosmos2_5Processor",headingTag:"h2"}}),Me=new x({props:{name:"class transformers.Kosmos2_5Processor",anchor:"transformers.Kosmos2_5Processor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.Kosmos2_5Processor.image_processor",description:`<strong>image_processor</strong> (<code>Kosmos2_5ImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessor">Kosmos2_5ImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.Kosmos2_5Processor.tokenizer",description:"<strong>tokenizer</strong> (Union[<code>T5TokenizerFast</code>, <code>T5Tokenizer</code>]) &#x2014;\nAn instance of [&#x2018;T5TokenizerFast`] or [&#x2018;T5Tokenizer`]. The tokenizer is a required input.",name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/processing_kosmos2_5.py#L55"}}),ye=new x({props:{name:"batch_decode",anchor:"transformers.Kosmos2_5Processor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/processing_kosmos2_5.py#L143"}}),Te=new x({props:{name:"decode",anchor:"transformers.Kosmos2_5Processor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/processing_kosmos2_5.py#L150"}}),we=new O({props:{title:"Kosmos2_5Model",local:"transformers.Kosmos2_5Model",headingTag:"h2"}}),be=new x({props:{name:"class transformers.Kosmos2_5Model",anchor:"transformers.Kosmos2_5Model",parameters:[{name:"config",val:": Kosmos2_5Config"}],parametersDescription:[{anchor:"transformers.Kosmos2_5Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config">Kosmos2_5Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/modeling_kosmos2_5.py#L1390"}}),Je=new x({props:{name:"forward",anchor:"transformers.Kosmos2_5Model.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"flattened_patches",val:": typing.Optional[torch.Tensor] = None"},{name:"width",val:": typing.Optional[torch.Tensor] = None"},{name:"height",val:": typing.Optional[torch.Tensor] = None"},{name:"image_embeds_position_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Union[transformers.cache_utils.Cache, list[torch.FloatTensor], NoneType] = None"},{name:"image_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Kosmos2_5Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Kosmos2_5Model.forward.flattened_patches",description:`<strong>flattened_patches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_patches, 2 + patch_height * patch_width * image_channels)</code>) &#x2014;
Flattened patches of the images. <code>flattened_patches</code> can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"flattened_patches"},{anchor:"transformers.Kosmos2_5Model.forward.width",description:`<strong>width</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
The original width (before resizing) of each image in the batch. This can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"width"},{anchor:"transformers.Kosmos2_5Model.forward.height",description:`<strong>height</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
The original height (before resizing) of each image in the batch. This can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"height"},{anchor:"transformers.Kosmos2_5Model.forward.image_embeds_position_mask",description:`<strong>image_embeds_position_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to indicate the location in a sequence to insert the image features . Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for places where to put the image features,</li>
<li>0 for places that are not for image features (i.e. for text tokens).</li>
</ul>`,name:"image_embeds_position_mask"},{anchor:"transformers.Kosmos2_5Model.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Kosmos2_5Model.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Kosmos2_5Model.forward.image_embeds",description:`<strong>image_embeds</strong> &#x2014; (<code>torch.FloatTensor</code> of shape <code>(batch_size, latent_query_num, hidden_size)</code>, <em>optional</em>):
Sequence of hidden-states at the output of <code>Kosmos2ImageToTextProjection</code>.`,name:"image_embeds"},{anchor:"transformers.Kosmos2_5Model.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Kosmos2_5Model.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Kosmos2_5Model.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Kosmos2_5Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Kosmos2_5Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/modeling_kosmos2_5.py#L1409",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.kosmos2_5.modeling_kosmos2_5.Kosmos2_5ModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.kosmos2_5.configuration_kosmos2_5.Kosmos2_5Config'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>width</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) — The original width (before resizing) of each image in the batch.</p>
</li>
<li>
<p><strong>height</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) — The original height (before resizing) of each image in the batch.</p>
</li>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, latent_query_num, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of <code>Kosmos2ImageToTextProjection</code>.</p>
</li>
<li>
<p><strong>projection_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights given by <code>Kosmos2ImageToTextProjection</code>, after the attention softmax, used to compute
the weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>) — The output of the <code>Kosmos2VisionModel</code>.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.kosmos2_5.modeling_kosmos2_5.Kosmos2_5ModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),V=new jo({props:{$$slots:{default:[No]},$$scope:{ctx:J}}}),S=new Io({props:{anchor:"transformers.Kosmos2_5Model.forward.example",$$slots:{default:[Xo]},$$scope:{ctx:J}}}),Ue=new O({props:{title:"Kosmos2_5ForConditionalGeneration",local:"transformers.Kosmos2_5ForConditionalGeneration",headingTag:"h2"}}),je=new x({props:{name:"class transformers.Kosmos2_5ForConditionalGeneration",anchor:"transformers.Kosmos2_5ForConditionalGeneration",parameters:[{name:"config",val:": Kosmos2_5Config"}],parametersDescription:[{anchor:"transformers.Kosmos2_5ForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config">Kosmos2_5Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/modeling_kosmos2_5.py#L1669"}}),Ie=new x({props:{name:"forward",anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"flattened_patches",val:": typing.Optional[torch.Tensor] = None"},{name:"width",val:": typing.Optional[torch.Tensor] = None"},{name:"height",val:": typing.Optional[torch.Tensor] = None"},{name:"image_embeds_position_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Union[transformers.cache_utils.Cache, list[torch.FloatTensor], NoneType] = None"},{name:"image_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.flattened_patches",description:`<strong>flattened_patches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_patches, 2 + patch_height * patch_width * image_channels)</code>) &#x2014;
Flattened patches of the images. <code>flattened_patches</code> can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"flattened_patches"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.width",description:`<strong>width</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
The original width (before resizing) of each image in the batch. This can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"width"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.height",description:`<strong>height</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) &#x2014;
The original height (before resizing) of each image in the batch. This can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">Kosmos2_5ImageProcessor.<strong>call</strong>()</a> for details.`,name:"height"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.image_embeds_position_mask",description:`<strong>image_embeds_position_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to indicate the location in a sequence to insert the image features . Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for places where to put the image features,</li>
<li>0 for places that are not for image features (i.e. for text tokens).</li>
</ul>`,name:"image_embeds_position_mask"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.image_embeds",description:`<strong>image_embeds</strong> &#x2014; (<code>torch.FloatTensor</code> of shape <code>(batch_size, latent_query_num, hidden_size)</code>, <em>optional</em>):
Sequence of hidden-states at the output of <code>Kosmos2ImageToTextProjection</code>.`,name:"image_embeds"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/kosmos2_5/modeling_kosmos2_5.py#L1693",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.kosmos2_5.modeling_kosmos2_5.Kosmos2_5ForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.kosmos2_5.configuration_kosmos2_5.Kosmos2_5Config'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>width</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) — The original width (before resizing) of each image in the batch.</p>
</li>
<li>
<p><strong>height</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) — The original height (before resizing) of each image in the batch.</p>
</li>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, latent_query_num, hidden_size)</code>, <em>optional</em>) — Sequence of hidden-states at the output of <code>Kosmos2ImageToTextProjection</code>.</p>
</li>
<li>
<p><strong>projection_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights given by <code>Kosmos2ImageToTextProjection</code>, after the attention softmax, used to compute
the weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>) — The output of the <code>Kosmos2VisionModel</code>.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.kosmos2_5.modeling_kosmos2_5.Kosmos2_5ForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),P=new jo({props:{$$slots:{default:[Ko]},$$scope:{ctx:J}}}),Y=new Io({props:{anchor:"transformers.Kosmos2_5ForConditionalGeneration.forward.example",$$slots:{default:[Qo]},$$scope:{ctx:J}}}),Ce=new Wo({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/kosmos2_5.md"}}),{c(){t=c("meta"),T=n(),r=c("p"),h=n(),p=c("p"),p.innerHTML=m,U=n(),Z=c("div"),Z.innerHTML=Ls,De=n(),g(ee.$$.fragment),Le=n(),se=c("p"),se.innerHTML=Os,Oe=n(),oe=c("p"),oe.textContent=eo,es=n(),te=c("p"),te.innerHTML=so,ss=n(),K=c("img"),os=n(),Q=c("img"),ts=n(),ne=c("small"),ne.innerHTML=no,ns=n(),ae=c("p"),ae.innerHTML=ao,as=n(),g(H.$$.fragment),rs=n(),g(re.$$.fragment),ls=n(),le=c("p"),le.textContent=ro,is=n(),g(ie.$$.fragment),cs=n(),g(ce.$$.fragment),ds=n(),G=c("div"),g(de.$$.fragment),Is=n(),ze=c("p"),ze.innerHTML=lo,ms=n(),g(me.$$.fragment),ps=n(),B=c("div"),g(pe.$$.fragment),Cs=n(),xe=c("p"),xe.textContent=io,ks=n(),A=c("div"),g(he.$$.fragment),vs=n(),Ze=c("p"),Ze.innerHTML=co,hs=n(),g(ge.$$.fragment),gs=n(),W=c("div"),g(ue.$$.fragment),zs=n(),Be=c("p"),Be.textContent=mo,xs=n(),We=c("div"),g(fe.$$.fragment),us=n(),g(_e.$$.fragment),fs=n(),j=c("div"),g(Me.$$.fragment),Zs=n(),$e=c("p"),$e.textContent=po,Bs=n(),Re=c("p"),Re.innerHTML=ho,Ws=n(),q=c("div"),g(ye.$$.fragment),$s=n(),Fe=c("p"),Fe.innerHTML=go,Rs=n(),E=c("div"),g(Te.$$.fragment),Fs=n(),Ge=c("p"),Ge.innerHTML=uo,_s=n(),g(we.$$.fragment),Ms=n(),I=c("div"),g(be.$$.fragment),Gs=n(),Ne=c("p"),Ne.textContent=fo,Ns=n(),Xe=c("p"),Xe.innerHTML=_o,Xs=n(),Ke=c("p"),Ke.innerHTML=Mo,Ks=n(),v=c("div"),g(Je.$$.fragment),Qs=n(),Qe=c("p"),Qe.innerHTML=yo,Hs=n(),g(V.$$.fragment),As=n(),g(S.$$.fragment),ys=n(),g(Ue.$$.fragment),Ts=n(),C=c("div"),g(je.$$.fragment),qs=n(),He=c("p"),He.textContent=To,Es=n(),Ae=c("p"),Ae.innerHTML=wo,Vs=n(),qe=c("p"),qe.innerHTML=bo,Ss=n(),z=c("div"),g(Ie.$$.fragment),Ps=n(),Ee=c("p"),Ee.innerHTML=Jo,Ys=n(),g(P.$$.fragment),Ds=n(),g(Y.$$.fragment),ws=n(),g(Ce.$$.fragment),bs=n(),Se=c("p"),this.h()},l(e){const s=Zo("svelte-u9bgzb",document.head);t=d(s,"META",{name:!0,content:!0}),s.forEach(o),T=a(e),r=d(e,"P",{}),k(r).forEach(o),h=a(e),p=d(e,"P",{"data-svelte-h":!0}),w(p)!=="svelte-dozu3k"&&(p.innerHTML=m),U=a(e),Z=d(e,"DIV",{style:!0,"data-svelte-h":!0}),w(Z)!=="svelte-2m0t7r"&&(Z.innerHTML=Ls),De=a(e),u(ee.$$.fragment,e),Le=a(e),se=d(e,"P",{"data-svelte-h":!0}),w(se)!=="svelte-1obv8s7"&&(se.innerHTML=Os),Oe=a(e),oe=d(e,"P",{"data-svelte-h":!0}),w(oe)!=="svelte-vfdo9a"&&(oe.textContent=eo),es=a(e),te=d(e,"P",{"data-svelte-h":!0}),w(te)!=="svelte-1owgb0z"&&(te.innerHTML=so),ss=a(e),K=d(e,"IMG",{src:!0,alt:!0,width:!0}),os=a(e),Q=d(e,"IMG",{src:!0,alt:!0,width:!0}),ts=a(e),ne=d(e,"SMALL",{"data-svelte-h":!0}),w(ne)!=="svelte-1bb2eip"&&(ne.innerHTML=no),ns=a(e),ae=d(e,"P",{"data-svelte-h":!0}),w(ae)!=="svelte-vywy70"&&(ae.innerHTML=ao),as=a(e),u(H.$$.fragment,e),rs=a(e),u(re.$$.fragment,e),ls=a(e),le=d(e,"P",{"data-svelte-h":!0}),w(le)!=="svelte-eqrel2"&&(le.textContent=ro),is=a(e),u(ie.$$.fragment,e),cs=a(e),u(ce.$$.fragment,e),ds=a(e),G=d(e,"DIV",{class:!0});var ke=k(G);u(de.$$.fragment,ke),Is=a(ke),ze=d(ke,"P",{"data-svelte-h":!0}),w(ze)!=="svelte-dh8u7x"&&(ze.innerHTML=lo),ke.forEach(o),ms=a(e),u(me.$$.fragment,e),ps=a(e),B=d(e,"DIV",{class:!0});var N=k(B);u(pe.$$.fragment,N),Cs=a(N),xe=d(N,"P",{"data-svelte-h":!0}),w(xe)!=="svelte-cfi3eg"&&(xe.textContent=io),ks=a(N),A=d(N,"DIV",{class:!0});var ve=k(A);u(he.$$.fragment,ve),vs=a(ve),Ze=d(ve,"P",{"data-svelte-h":!0}),w(Ze)!=="svelte-14ynafp"&&(Ze.innerHTML=co),ve.forEach(o),N.forEach(o),hs=a(e),u(ge.$$.fragment,e),gs=a(e),W=d(e,"DIV",{class:!0});var X=k(W);u(ue.$$.fragment,X),zs=a(X),Be=d(X,"P",{"data-svelte-h":!0}),w(Be)!=="svelte-173df1z"&&(Be.textContent=mo),xs=a(X),We=d(X,"DIV",{class:!0});var Pe=k(We);u(fe.$$.fragment,Pe),Pe.forEach(o),X.forEach(o),us=a(e),u(_e.$$.fragment,e),fs=a(e),j=d(e,"DIV",{class:!0});var $=k(j);u(Me.$$.fragment,$),Zs=a($),$e=d($,"P",{"data-svelte-h":!0}),w($e)!=="svelte-1irj0e2"&&($e.textContent=po),Bs=a($),Re=d($,"P",{"data-svelte-h":!0}),w(Re)!=="svelte-8js9cv"&&(Re.innerHTML=ho),Ws=a($),q=d($,"DIV",{class:!0});var Us=k(q);u(ye.$$.fragment,Us),$s=a(Us),Fe=d(Us,"P",{"data-svelte-h":!0}),w(Fe)!=="svelte-1mblc37"&&(Fe.innerHTML=go),Us.forEach(o),Rs=a($),E=d($,"DIV",{class:!0});var js=k(E);u(Te.$$.fragment,js),Fs=a(js),Ge=d(js,"P",{"data-svelte-h":!0}),w(Ge)!=="svelte-tndspt"&&(Ge.innerHTML=uo),js.forEach(o),$.forEach(o),_s=a(e),u(we.$$.fragment,e),Ms=a(e),I=d(e,"DIV",{class:!0});var R=k(I);u(be.$$.fragment,R),Gs=a(R),Ne=d(R,"P",{"data-svelte-h":!0}),w(Ne)!=="svelte-1xww0l9"&&(Ne.textContent=fo),Ns=a(R),Xe=d(R,"P",{"data-svelte-h":!0}),w(Xe)!=="svelte-q52n56"&&(Xe.innerHTML=_o),Xs=a(R),Ke=d(R,"P",{"data-svelte-h":!0}),w(Ke)!=="svelte-hswkmf"&&(Ke.innerHTML=Mo),Ks=a(R),v=d(R,"DIV",{class:!0});var D=k(v);u(Je.$$.fragment,D),Qs=a(D),Qe=d(D,"P",{"data-svelte-h":!0}),w(Qe)!=="svelte-1a7hk4f"&&(Qe.innerHTML=yo),Hs=a(D),u(V.$$.fragment,D),As=a(D),u(S.$$.fragment,D),D.forEach(o),R.forEach(o),ys=a(e),u(Ue.$$.fragment,e),Ts=a(e),C=d(e,"DIV",{class:!0});var F=k(C);u(je.$$.fragment,F),qs=a(F),He=d(F,"P",{"data-svelte-h":!0}),w(He)!=="svelte-i2qzaz"&&(He.textContent=To),Es=a(F),Ae=d(F,"P",{"data-svelte-h":!0}),w(Ae)!=="svelte-q52n56"&&(Ae.innerHTML=wo),Vs=a(F),qe=d(F,"P",{"data-svelte-h":!0}),w(qe)!=="svelte-hswkmf"&&(qe.innerHTML=bo),Ss=a(F),z=d(F,"DIV",{class:!0});var L=k(z);u(Ie.$$.fragment,L),Ps=a(L),Ee=d(L,"P",{"data-svelte-h":!0}),w(Ee)!=="svelte-odvnd5"&&(Ee.innerHTML=Jo),Ys=a(L),u(P.$$.fragment,L),Ds=a(L),u(Y.$$.fragment,L),L.forEach(o),F.forEach(o),ws=a(e),u(Ce.$$.fragment,e),bs=a(e),Se=d(e,"P",{}),k(Se).forEach(o),this.h()},h(){b(t,"name","hf:doc:metadata"),b(t,"content",Ao),Bo(Z,"float","right"),Uo(K.src,oo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/kosmos2_5_ocr.png")||b(K,"src",oo),b(K,"alt","drawing"),b(K,"width","600"),Uo(Q.src,to="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/kosmos2_5_md.png")||b(Q,"src",to),b(Q,"alt","drawing"),b(Q,"width","600"),b(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){i(document.head,t),l(e,T,s),l(e,r,s),l(e,h,s),l(e,p,s),l(e,U,s),l(e,Z,s),l(e,De,s),f(ee,e,s),l(e,Le,s),l(e,se,s),l(e,Oe,s),l(e,oe,s),l(e,es,s),l(e,te,s),l(e,ss,s),l(e,K,s),l(e,os,s),l(e,Q,s),l(e,ts,s),l(e,ne,s),l(e,ns,s),l(e,ae,s),l(e,as,s),f(H,e,s),l(e,rs,s),f(re,e,s),l(e,ls,s),l(e,le,s),l(e,is,s),f(ie,e,s),l(e,cs,s),f(ce,e,s),l(e,ds,s),l(e,G,s),f(de,G,null),i(G,Is),i(G,ze),l(e,ms,s),f(me,e,s),l(e,ps,s),l(e,B,s),f(pe,B,null),i(B,Cs),i(B,xe),i(B,ks),i(B,A),f(he,A,null),i(A,vs),i(A,Ze),l(e,hs,s),f(ge,e,s),l(e,gs,s),l(e,W,s),f(ue,W,null),i(W,zs),i(W,Be),i(W,xs),i(W,We),f(fe,We,null),l(e,us,s),f(_e,e,s),l(e,fs,s),l(e,j,s),f(Me,j,null),i(j,Zs),i(j,$e),i(j,Bs),i(j,Re),i(j,Ws),i(j,q),f(ye,q,null),i(q,$s),i(q,Fe),i(j,Rs),i(j,E),f(Te,E,null),i(E,Fs),i(E,Ge),l(e,_s,s),f(we,e,s),l(e,Ms,s),l(e,I,s),f(be,I,null),i(I,Gs),i(I,Ne),i(I,Ns),i(I,Xe),i(I,Xs),i(I,Ke),i(I,Ks),i(I,v),f(Je,v,null),i(v,Qs),i(v,Qe),i(v,Hs),f(V,v,null),i(v,As),f(S,v,null),l(e,ys,s),f(Ue,e,s),l(e,Ts,s),l(e,C,s),f(je,C,null),i(C,qs),i(C,He),i(C,Es),i(C,Ae),i(C,Vs),i(C,qe),i(C,Ss),i(C,z),f(Ie,z,null),i(z,Ps),i(z,Ee),i(z,Ys),f(P,z,null),i(z,Ds),f(Y,z,null),l(e,ws,s),f(Ce,e,s),l(e,bs,s),l(e,Se,s),Js=!0},p(e,[s]){const ke={};s&2&&(ke.$$scope={dirty:s,ctx:e}),H.$set(ke);const N={};s&2&&(N.$$scope={dirty:s,ctx:e}),V.$set(N);const ve={};s&2&&(ve.$$scope={dirty:s,ctx:e}),S.$set(ve);const X={};s&2&&(X.$$scope={dirty:s,ctx:e}),P.$set(X);const Pe={};s&2&&(Pe.$$scope={dirty:s,ctx:e}),Y.$set(Pe)},i(e){Js||(_(ee.$$.fragment,e),_(H.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(ge.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(_e.$$.fragment,e),_(Me.$$.fragment,e),_(ye.$$.fragment,e),_(Te.$$.fragment,e),_(we.$$.fragment,e),_(be.$$.fragment,e),_(Je.$$.fragment,e),_(V.$$.fragment,e),_(S.$$.fragment,e),_(Ue.$$.fragment,e),_(je.$$.fragment,e),_(Ie.$$.fragment,e),_(P.$$.fragment,e),_(Y.$$.fragment,e),_(Ce.$$.fragment,e),Js=!0)},o(e){M(ee.$$.fragment,e),M(H.$$.fragment,e),M(re.$$.fragment,e),M(ie.$$.fragment,e),M(ce.$$.fragment,e),M(de.$$.fragment,e),M(me.$$.fragment,e),M(pe.$$.fragment,e),M(he.$$.fragment,e),M(ge.$$.fragment,e),M(ue.$$.fragment,e),M(fe.$$.fragment,e),M(_e.$$.fragment,e),M(Me.$$.fragment,e),M(ye.$$.fragment,e),M(Te.$$.fragment,e),M(we.$$.fragment,e),M(be.$$.fragment,e),M(Je.$$.fragment,e),M(V.$$.fragment,e),M(S.$$.fragment,e),M(Ue.$$.fragment,e),M(je.$$.fragment,e),M(Ie.$$.fragment,e),M(P.$$.fragment,e),M(Y.$$.fragment,e),M(Ce.$$.fragment,e),Js=!1},d(e){e&&(o(T),o(r),o(h),o(p),o(U),o(Z),o(De),o(Le),o(se),o(Oe),o(oe),o(es),o(te),o(ss),o(K),o(os),o(Q),o(ts),o(ne),o(ns),o(ae),o(as),o(rs),o(ls),o(le),o(is),o(cs),o(ds),o(G),o(ms),o(ps),o(B),o(hs),o(gs),o(W),o(us),o(fs),o(j),o(_s),o(Ms),o(I),o(ys),o(Ts),o(C),o(ws),o(bs),o(Se)),o(t),y(ee,e),y(H,e),y(re,e),y(ie,e),y(ce,e),y(de),y(me,e),y(pe),y(he),y(ge,e),y(ue),y(fe),y(_e,e),y(Me),y(ye),y(Te),y(we,e),y(be),y(Je),y(V),y(S),y(Ue,e),y(je),y(Ie),y(P),y(Y),y(Ce,e)}}}const Ao='{"title":"KOSMOS-2.5","local":"kosmos-25","sections":[{"title":"Chat version","local":"chat-version","sections":[],"depth":2},{"title":"Kosmos2_5Config","local":"transformers.Kosmos2_5Config","sections":[],"depth":2},{"title":"Kosmos2_5ImageProcessor","local":"transformers.Kosmos2_5ImageProcessor","sections":[],"depth":2},{"title":"Kosmos2_5ImageProcessorFast","local":"transformers.Kosmos2_5ImageProcessorFast","sections":[],"depth":2},{"title":"Kosmos2_5Processor","local":"transformers.Kosmos2_5Processor","sections":[],"depth":2},{"title":"Kosmos2_5Model","local":"transformers.Kosmos2_5Model","sections":[],"depth":2},{"title":"Kosmos2_5ForConditionalGeneration","local":"transformers.Kosmos2_5ForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function qo(J){return vo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class et extends zo{constructor(t){super(),xo(this,t,qo,Ho,ko,{})}}export{et as component};
