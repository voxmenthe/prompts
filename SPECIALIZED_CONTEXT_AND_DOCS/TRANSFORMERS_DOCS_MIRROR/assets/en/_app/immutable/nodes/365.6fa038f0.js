import{s as Mt,o as yt,n as Ze}from"../chunks/scheduler.18a86fab.js";import{S as Tt,i as wt,g as d,s as r,r as f,A as $t,h as c,f as n,c as a,j as oe,x as m,u as g,k as N,y as l,a as s,v as _,d as b,t as v,w as M}from"../chunks/index.98837b22.js";import{T as _t}from"../chunks/Tip.77304350.js";import{D as ge}from"../chunks/Docstring.a1ef7999.js";import{C as vt}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as bt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as _e,E as kt}from"../chunks/getInferenceSnippets.06c2775f.js";function Gt(x){let o,u;return o=new vt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFJlY3VycmVudEdlbW1hTW9kZWwlMkMlMjBSZWN1cnJlbnRHZW1tYUNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBSZWN1cnJlbnRHZW1tYSUyMHJlY3VycmVudGdlbW1hLTJiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFJlY3VycmVudEdlbW1hQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwcmVjdXJyZW50Z2VtbWEtMmIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFJlY3VycmVudEdlbW1hTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RecurrentGemmaModel, RecurrentGemmaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a RecurrentGemma recurrentgemma-2b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RecurrentGemmaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the recurrentgemma-2b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RecurrentGemmaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){f(o.$$.fragment)},l(i){g(o.$$.fragment,i)},m(i,p){_(o,i,p),u=!0},p:Ze,i(i){u||(b(o.$$.fragment,i),u=!0)},o(i){v(o.$$.fragment,i),u=!1},d(i){M(o,i)}}}function Rt(x){let o,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=u},l(i){o=c(i,"P",{"data-svelte-h":!0}),m(o)!=="svelte-fincs2"&&(o.innerHTML=u)},m(i,p){s(i,o,p)},p:Ze,d(i){i&&n(o)}}}function Ct(x){let o,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=u},l(i){o=c(i,"P",{"data-svelte-h":!0}),m(o)!=="svelte-fincs2"&&(o.innerHTML=u)},m(i,p){s(i,o,p)},p:Ze,d(i){i&&n(o)}}}function xt(x){let o,u="Example:",i,p,R;return p=new vt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBSZWN1cnJlbnRHZW1tYUZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBSZWN1cnJlbnRHZW1tYUZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZyZWN1cnJlbnRnZW1tYS0yYiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZyZWN1cnJlbnRnZW1tYS0yYiUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJXaGF0JTIwaXMlMjB5b3VyJTIwZmF2b3JpdGUlMjBjb25kaW1lbnQlM0YlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIocHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBHZW5lcmF0ZSUwQWdlbmVyYXRlX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0cy5pbnB1dF9pZHMlMkMlMjBtYXhfbGVuZ3RoJTNEMzApJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZV9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSUyQyUyMGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMlM0RGYWxzZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, RecurrentGemmaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = RecurrentGemmaForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/recurrentgemma-2b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/recurrentgemma-2b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;What is your favorite condiment?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;What is your favorite condiment?&quot;</span>`,wrap:!1}}),{c(){o=d("p"),o.textContent=u,i=r(),f(p.$$.fragment)},l(h){o=c(h,"P",{"data-svelte-h":!0}),m(o)!=="svelte-11lpom8"&&(o.textContent=u),i=a(h),g(p.$$.fragment,h)},m(h,j){s(h,o,j),s(h,i,j),_(p,h,j),R=!0},p:Ze,i(h){R||(b(p.$$.fragment,h),R=!0)},o(h){v(p.$$.fragment,h),R=!1},d(h){h&&(n(o),n(i)),M(p,h)}}}function Lt(x){let o,u,i,p,R,h="<em>This model was released on 2024-04-11 and added to Hugging Face Transformers on 2024-04-10.</em>",j,q,be,F,Ke='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ve,J,Me,I,et='The Recurrent Gemma model was proposed in <a href="https://huggingface.co/papers/2404.07839" rel="nofollow">RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</a> by the Griffin, RLHF and Gemma Teams of Google.',ye,B,tt="The abstract from the paper is the following:",Te,U,nt="<em>We introduce RecurrentGemma, an open language model which uses Google’s novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.</em>",we,V,ot="Tips:",$e,O,rt='<li>The original checkpoints can be converted using the conversion script <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py" rel="nofollow"><code>src/transformers/models/recurrent_gemma/convert_recurrent_gemma_weights_to_hf.py</code></a>.</li>',ke,X,at='This model was contributed by <a href="https://huggingface.co/ArthurZ" rel="nofollow">Arthur Zucker</a>. The original code can be found <a href="https://github.com/google-deepmind/recurrentgemma" rel="nofollow">here</a>.',Ge,Y,Re,y,S,He,re,st=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel">RecurrentGemmaModel</a>. It is used to instantiate a RecurrentGemma
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the RecurrentGemma-7B.`,Pe,ae,it='e.g. <a href="https://huggingface.co/google/recurrentgemma-2b" rel="nofollow">google/recurrentgemma-2b</a>',Ee,se,lt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ne,W,Ce,A,xe,T,D,qe,ie,dt="The bare Recurrent Gemma Model outputting raw hidden-states without any specific head on top.",Je,le,ct=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ie,de,mt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Be,L,Q,Ue,ce,ut='The <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel">RecurrentGemmaModel</a> forward method, overrides the <code>__call__</code> special method.',Ve,Z,Le,K,ze,w,ee,Oe,me,pt="The Recurrent Gemma Model for causal language modeling.",Xe,ue,ht=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ye,pe,ft=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Se,C,te,Ae,he,gt='The <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM">RecurrentGemmaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',De,H,Qe,P,je,ne,Fe,fe,We;return q=new _e({props:{title:"RecurrentGemma",local:"recurrentgemma",headingTag:"h1"}}),J=new _e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Y=new _e({props:{title:"RecurrentGemmaConfig",local:"transformers.RecurrentGemmaConfig",headingTag:"h2"}}),S=new ge({props:{name:"class transformers.RecurrentGemmaConfig",anchor:"transformers.RecurrentGemmaConfig",parameters:[{name:"num_hidden_layers",val:" = 26"},{name:"vocab_size",val:" = 256000"},{name:"hidden_size",val:" = 2560"},{name:"intermediate_size",val:" = 7680"},{name:"num_attention_heads",val:" = 10"},{name:"lru_width",val:" = None"},{name:"attention_window_size",val:" = 2048"},{name:"conv1d_width",val:" = 4"},{name:"logits_soft_cap",val:" = 30.0"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"bos_token_id",val:" = 2"},{name:"hidden_activation",val:" = 'gelu_pytorch_tanh'"},{name:"partial_rotary_factor",val:" = 0.5"},{name:"rope_theta",val:" = 10000.0"},{name:"block_types",val:" = ('recurrent', 'recurrent', 'attention')"},{name:"attention_dropout",val:" = 0.0"},{name:"num_key_value_heads",val:" = None"},{name:"attention_bias",val:" = False"},{name:"w_init_variance_scale",val:" = 0.01"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RecurrentGemmaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 26) &#x2014;
The number of hidden layers in the model.`,name:"num_hidden_layers"},{anchor:"transformers.RecurrentGemmaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256000) &#x2014;
Vocabulary size of the RecurrentGemma model. Defines the number of
different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel">RecurrentGemmaModel</a>`,name:"vocab_size"},{anchor:"transformers.RecurrentGemmaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2560) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.RecurrentGemmaConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7680) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.RecurrentGemmaConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The number of heads for the attention block and the number of
heads/blocks for the block-diagonal layers used in the RG-LRU gates.
This number must divide <code>hidden_size</code> and <code>lru_width</code>.`,name:"num_attention_heads"},{anchor:"transformers.RecurrentGemmaConfig.lru_width",description:`<strong>lru_width</strong> (<code>int</code> or <code>None</code>, <em>optional</em>) &#x2014;
Dimension of the hidden representations of the RG-LRU. If <code>None</code>
this will be set to <code>hidden_size</code>.
Whether to scale the output of the embeddings by <code>sqrt(hidden_size)</code>.`,name:"lru_width"},{anchor:"transformers.RecurrentGemmaConfig.attention_window_size",description:`<strong>attention_window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The size of the attention window used in the attention block.`,name:"attention_window_size"},{anchor:"transformers.RecurrentGemmaConfig.conv1d_width",description:`<strong>conv1d_width</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The kernel size of conv1d layers used in the recurrent blocks.`,name:"conv1d_width"},{anchor:"transformers.RecurrentGemmaConfig.logits_soft_cap",description:`<strong>logits_soft_cap</strong> (<code>float</code>, <em>optional</em>, defaults to 30.0) &#x2014;
The value at which the logits should be soft-capped to after the transformer and LM-head computation in the Causal LM architecture.`,name:"logits_soft_cap"},{anchor:"transformers.RecurrentGemmaConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.RecurrentGemmaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the model should return the last key/values
attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.RecurrentGemmaConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.RecurrentGemmaConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.RecurrentGemmaConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.RecurrentGemmaConfig.hidden_activation",description:"<strong>hidden_activation</strong> (<code>str` or `function</code>, <em>optional</em>, defaults to <code>&quot;gelu_pytorch_tanh&quot;</code>) &#x2014;\nThe hidden activation used in the recurrent block as well as the MLP layer of the decoder layers.",name:"hidden_activation"},{anchor:"transformers.RecurrentGemmaConfig.partial_rotary_factor",description:`<strong>partial_rotary_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The partial rotary factor used in the initialization of the rotary embeddings.`,name:"partial_rotary_factor"},{anchor:"transformers.RecurrentGemmaConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.RecurrentGemmaConfig.block_types",description:`<strong>block_types</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>(&apos;recurrent&apos;, &apos;recurrent&apos;, &apos;attention&apos;)</code>) &#x2014;
List of aleternating blocks that will be repeated to initialize the <code>temporal_block</code> layer.`,name:"block_types"},{anchor:"transformers.RecurrentGemmaConfig.attention_dropout",description:"<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014; dropout value to use after the attention softmax.",name:"attention_dropout"},{anchor:"transformers.RecurrentGemmaConfig.num_key_value_heads",description:"<strong>num_key_value_heads</strong> (<code>16</code>, <em>optional</em>, defaults to 16) &#x2014; Number of key value heads to use GQA.",name:"num_key_value_heads"},{anchor:"transformers.RecurrentGemmaConfig.attention_bias",description:"<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014; whether or not the linear q,k,v of the Attention layer should have bias",name:"attention_bias"},{anchor:"transformers.RecurrentGemmaConfig.w_init_variance_scale",description:"<strong>w_init_variance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 0.01) &#x2014; weight initialization variance.",name:"w_init_variance_scale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py#L24"}}),W=new bt({props:{anchor:"transformers.RecurrentGemmaConfig.example",$$slots:{default:[Gt]},$$scope:{ctx:x}}}),A=new _e({props:{title:"RecurrentGemmaModel",local:"transformers.RecurrentGemmaModel",headingTag:"h2"}}),D=new ge({props:{name:"class transformers.RecurrentGemmaModel",anchor:"transformers.RecurrentGemmaModel",parameters:[{name:"config",val:": RecurrentGemmaConfig"}],parametersDescription:[{anchor:"transformers.RecurrentGemmaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig">RecurrentGemmaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py#L573"}}),Q=new ge({props:{name:"forward",anchor:"transformers.RecurrentGemmaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RecurrentGemmaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RecurrentGemmaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RecurrentGemmaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RecurrentGemmaModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.RecurrentGemmaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RecurrentGemmaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.RecurrentGemmaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RecurrentGemmaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py#L592",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig"
>RecurrentGemmaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new _t({props:{$$slots:{default:[Rt]},$$scope:{ctx:x}}}),K=new _e({props:{title:"RecurrentGemmaForCausalLM",local:"transformers.RecurrentGemmaForCausalLM",headingTag:"h2"}}),ee=new ge({props:{name:"class transformers.RecurrentGemmaForCausalLM",anchor:"transformers.RecurrentGemmaForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RecurrentGemmaForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM">RecurrentGemmaForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py#L690"}}),te=new ge({props:{name:"forward",anchor:"transformers.RecurrentGemmaForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RecurrentGemmaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RecurrentGemmaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py#L702",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig"
>RecurrentGemmaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new _t({props:{$$slots:{default:[Ct]},$$scope:{ctx:x}}}),P=new bt({props:{anchor:"transformers.RecurrentGemmaForCausalLM.forward.example",$$slots:{default:[xt]},$$scope:{ctx:x}}}),ne=new kt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/recurrent_gemma.md"}}),{c(){o=d("meta"),u=r(),i=d("p"),p=r(),R=d("p"),R.innerHTML=h,j=r(),f(q.$$.fragment),be=r(),F=d("div"),F.innerHTML=Ke,ve=r(),f(J.$$.fragment),Me=r(),I=d("p"),I.innerHTML=et,ye=r(),B=d("p"),B.textContent=tt,Te=r(),U=d("p"),U.innerHTML=nt,we=r(),V=d("p"),V.textContent=ot,$e=r(),O=d("ul"),O.innerHTML=rt,ke=r(),X=d("p"),X.innerHTML=at,Ge=r(),f(Y.$$.fragment),Re=r(),y=d("div"),f(S.$$.fragment),He=r(),re=d("p"),re.innerHTML=st,Pe=r(),ae=d("p"),ae.innerHTML=it,Ee=r(),se=d("p"),se.innerHTML=lt,Ne=r(),f(W.$$.fragment),Ce=r(),f(A.$$.fragment),xe=r(),T=d("div"),f(D.$$.fragment),qe=r(),ie=d("p"),ie.textContent=dt,Je=r(),le=d("p"),le.innerHTML=ct,Ie=r(),de=d("p"),de.innerHTML=mt,Be=r(),L=d("div"),f(Q.$$.fragment),Ue=r(),ce=d("p"),ce.innerHTML=ut,Ve=r(),f(Z.$$.fragment),Le=r(),f(K.$$.fragment),ze=r(),w=d("div"),f(ee.$$.fragment),Oe=r(),me=d("p"),me.textContent=pt,Xe=r(),ue=d("p"),ue.innerHTML=ht,Ye=r(),pe=d("p"),pe.innerHTML=ft,Se=r(),C=d("div"),f(te.$$.fragment),Ae=r(),he=d("p"),he.innerHTML=gt,De=r(),f(H.$$.fragment),Qe=r(),f(P.$$.fragment),je=r(),f(ne.$$.fragment),Fe=r(),fe=d("p"),this.h()},l(e){const t=$t("svelte-u9bgzb",document.head);o=c(t,"META",{name:!0,content:!0}),t.forEach(n),u=a(e),i=c(e,"P",{}),oe(i).forEach(n),p=a(e),R=c(e,"P",{"data-svelte-h":!0}),m(R)!=="svelte-135b21r"&&(R.innerHTML=h),j=a(e),g(q.$$.fragment,e),be=a(e),F=c(e,"DIV",{class:!0,"data-svelte-h":!0}),m(F)!=="svelte-13t8s2t"&&(F.innerHTML=Ke),ve=a(e),g(J.$$.fragment,e),Me=a(e),I=c(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-22ml2p"&&(I.innerHTML=et),ye=a(e),B=c(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-vfdo9a"&&(B.textContent=tt),Te=a(e),U=c(e,"P",{"data-svelte-h":!0}),m(U)!=="svelte-1d1khq2"&&(U.innerHTML=nt),we=a(e),V=c(e,"P",{"data-svelte-h":!0}),m(V)!=="svelte-axv494"&&(V.textContent=ot),$e=a(e),O=c(e,"UL",{"data-svelte-h":!0}),m(O)!=="svelte-eiq2pr"&&(O.innerHTML=rt),ke=a(e),X=c(e,"P",{"data-svelte-h":!0}),m(X)!=="svelte-17fjyye"&&(X.innerHTML=at),Ge=a(e),g(Y.$$.fragment,e),Re=a(e),y=c(e,"DIV",{class:!0});var $=oe(y);g(S.$$.fragment,$),He=a($),re=c($,"P",{"data-svelte-h":!0}),m(re)!=="svelte-1g9ccyj"&&(re.innerHTML=st),Pe=a($),ae=c($,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-qt1jrx"&&(ae.innerHTML=it),Ee=a($),se=c($,"P",{"data-svelte-h":!0}),m(se)!=="svelte-1ek1ss9"&&(se.innerHTML=lt),Ne=a($),g(W.$$.fragment,$),$.forEach(n),Ce=a(e),g(A.$$.fragment,e),xe=a(e),T=c(e,"DIV",{class:!0});var k=oe(T);g(D.$$.fragment,k),qe=a(k),ie=c(k,"P",{"data-svelte-h":!0}),m(ie)!=="svelte-1o8mpm3"&&(ie.textContent=dt),Je=a(k),le=c(k,"P",{"data-svelte-h":!0}),m(le)!=="svelte-q52n56"&&(le.innerHTML=ct),Ie=a(k),de=c(k,"P",{"data-svelte-h":!0}),m(de)!=="svelte-hswkmf"&&(de.innerHTML=mt),Be=a(k),L=c(k,"DIV",{class:!0});var z=oe(L);g(Q.$$.fragment,z),Ue=a(z),ce=c(z,"P",{"data-svelte-h":!0}),m(ce)!=="svelte-rq0v0b"&&(ce.innerHTML=ut),Ve=a(z),g(Z.$$.fragment,z),z.forEach(n),k.forEach(n),Le=a(e),g(K.$$.fragment,e),ze=a(e),w=c(e,"DIV",{class:!0});var G=oe(w);g(ee.$$.fragment,G),Oe=a(G),me=c(G,"P",{"data-svelte-h":!0}),m(me)!=="svelte-596qmo"&&(me.textContent=pt),Xe=a(G),ue=c(G,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-q52n56"&&(ue.innerHTML=ht),Ye=a(G),pe=c(G,"P",{"data-svelte-h":!0}),m(pe)!=="svelte-hswkmf"&&(pe.innerHTML=ft),Se=a(G),C=c(G,"DIV",{class:!0});var E=oe(C);g(te.$$.fragment,E),Ae=a(E),he=c(E,"P",{"data-svelte-h":!0}),m(he)!=="svelte-jik957"&&(he.innerHTML=gt),De=a(E),g(H.$$.fragment,E),Qe=a(E),g(P.$$.fragment,E),E.forEach(n),G.forEach(n),je=a(e),g(ne.$$.fragment,e),Fe=a(e),fe=c(e,"P",{}),oe(fe).forEach(n),this.h()},h(){N(o,"name","hf:doc:metadata"),N(o,"content",zt),N(F,"class","flex flex-wrap space-x-1"),N(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){l(document.head,o),s(e,u,t),s(e,i,t),s(e,p,t),s(e,R,t),s(e,j,t),_(q,e,t),s(e,be,t),s(e,F,t),s(e,ve,t),_(J,e,t),s(e,Me,t),s(e,I,t),s(e,ye,t),s(e,B,t),s(e,Te,t),s(e,U,t),s(e,we,t),s(e,V,t),s(e,$e,t),s(e,O,t),s(e,ke,t),s(e,X,t),s(e,Ge,t),_(Y,e,t),s(e,Re,t),s(e,y,t),_(S,y,null),l(y,He),l(y,re),l(y,Pe),l(y,ae),l(y,Ee),l(y,se),l(y,Ne),_(W,y,null),s(e,Ce,t),_(A,e,t),s(e,xe,t),s(e,T,t),_(D,T,null),l(T,qe),l(T,ie),l(T,Je),l(T,le),l(T,Ie),l(T,de),l(T,Be),l(T,L),_(Q,L,null),l(L,Ue),l(L,ce),l(L,Ve),_(Z,L,null),s(e,Le,t),_(K,e,t),s(e,ze,t),s(e,w,t),_(ee,w,null),l(w,Oe),l(w,me),l(w,Xe),l(w,ue),l(w,Ye),l(w,pe),l(w,Se),l(w,C),_(te,C,null),l(C,Ae),l(C,he),l(C,De),_(H,C,null),l(C,Qe),_(P,C,null),s(e,je,t),_(ne,e,t),s(e,Fe,t),s(e,fe,t),We=!0},p(e,[t]){const $={};t&2&&($.$$scope={dirty:t,ctx:e}),W.$set($);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),Z.$set(k);const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),H.$set(z);const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),P.$set(G)},i(e){We||(b(q.$$.fragment,e),b(J.$$.fragment,e),b(Y.$$.fragment,e),b(S.$$.fragment,e),b(W.$$.fragment,e),b(A.$$.fragment,e),b(D.$$.fragment,e),b(Q.$$.fragment,e),b(Z.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(H.$$.fragment,e),b(P.$$.fragment,e),b(ne.$$.fragment,e),We=!0)},o(e){v(q.$$.fragment,e),v(J.$$.fragment,e),v(Y.$$.fragment,e),v(S.$$.fragment,e),v(W.$$.fragment,e),v(A.$$.fragment,e),v(D.$$.fragment,e),v(Q.$$.fragment,e),v(Z.$$.fragment,e),v(K.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(H.$$.fragment,e),v(P.$$.fragment,e),v(ne.$$.fragment,e),We=!1},d(e){e&&(n(u),n(i),n(p),n(R),n(j),n(be),n(F),n(ve),n(Me),n(I),n(ye),n(B),n(Te),n(U),n(we),n(V),n($e),n(O),n(ke),n(X),n(Ge),n(Re),n(y),n(Ce),n(xe),n(T),n(Le),n(ze),n(w),n(je),n(Fe),n(fe)),n(o),M(q,e),M(J,e),M(Y,e),M(S),M(W),M(A,e),M(D),M(Q),M(Z),M(K,e),M(ee),M(te),M(H),M(P),M(ne,e)}}}const zt='{"title":"RecurrentGemma","local":"recurrentgemma","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"RecurrentGemmaConfig","local":"transformers.RecurrentGemmaConfig","sections":[],"depth":2},{"title":"RecurrentGemmaModel","local":"transformers.RecurrentGemmaModel","sections":[],"depth":2},{"title":"RecurrentGemmaForCausalLM","local":"transformers.RecurrentGemmaForCausalLM","sections":[],"depth":2}],"depth":1}';function jt(x){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qt extends Tt{constructor(o){super(),wt(this,o,jt,Lt,Mt,{})}}export{qt as component};
