import{s as Pn,o as On,n as I}from"../chunks/scheduler.18a86fab.js";import{S as ta,i as la,g as w,s as c,r as d,m as Pl,A as na,h as T,f as s,c as u,j as pn,u as m,x as f,n as Ol,k as Kn,y as G,a as o,v as M,d as y,t as h,w as b}from"../chunks/index.98837b22.js";import{T as cn}from"../chunks/Tip.77304350.js";import{C as j}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as X,E as aa}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Yt,a as R}from"../chunks/HfOption.6641485e.js";function ea(J){let n,i='Refer to the torchao <a href="https://github.com/pytorch/ao#torchao-pytorch-architecture-optimization" rel="nofollow">README.md</a> for more details about the library.';return{c(){n=w("p"),n.innerHTML=i},l(l){n=T(l,"P",{"data-svelte-h":!0}),f(n)!=="svelte-1e1uvh6"&&(n.innerHTML=i)},m(l,r){o(l,n,r)},p:I,d(l){l&&s(n)}}}function sa(J){let n,i;return n=new j({props:{code:"JTIzJTIwVXBkYXRpbmclMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjB0byUyMHRoZSUyMGxhdGVzdCUyMHZlcnNpb24lMkMlMjBhcyUyMHRoZSUyMGV4YW1wbGUlMjBzY3JpcHQlMjBiZWxvdyUyMHVzZXMlMjB0aGUlMjBuZXclMjBhdXRvJTIwY29tcGlsYXRpb24lMEElMjMlMjBTdGFibGUlMjByZWxlYXNlJTIwZnJvbSUyMFB5cGklMjB3aGljaCUyMHdpbGwlMjBkZWZhdWx0JTIwdG8lMjBDVURBJTIwMTIuNiUwQXBpcCUyMGluc3RhbGwlMjAtLXVwZ3JhZGUlMjB0b3JjaGFvJTIwdHJhbnNmb3JtZXJz",highlighted:`<span class="hljs-comment"># Updating ðŸ¤— Transformers to the latest version, as the example script below uses the new auto compilation</span>
<span class="hljs-comment"># Stable release from Pypi which will default to CUDA 12.6</span>
pip install --upgrade torchao transformers`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function oa(J){let n,i="Stable Release from the PyTorch index",l,r,a;return r=new j({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRvcmNoYW8lMjAtLWluZGV4LXVybCUyMGh0dHBzJTNBJTJGJTJGZG93bmxvYWQucHl0b3JjaC5vcmclMkZ3aGwlMkZjdTEyNiUyMCUyMyUyMG9wdGlvbnMlMjBhcmUlMjBjcHUlMkZjdTExOCUyRmN1MTI2JTJGY3UxMjg=",highlighted:'pip install torchao --index-url https://download.pytorch.org/whl/cu126 <span class="hljs-comment"># options are cpu/cu118/cu126/cu128</span>',wrap:!1}}),{c(){n=w("p"),n.textContent=i,l=c(),d(r.$$.fragment)},l(p){n=T(p,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15ppzc6"&&(n.textContent=i),l=u(p),m(r.$$.fragment,p)},m(p,U){o(p,n,U),o(p,l,U),M(r,p,U),a=!0},p:I,i(p){a||(y(r.$$.fragment,p),a=!0)},o(p){h(r.$$.fragment,p),a=!1},d(p){p&&(s(n),s(l)),b(r,p)}}}function ia(J){let n,i,l,r;return n=new R({props:{id:"install torchao",option:"PyPi",$$slots:{default:[sa]},$$scope:{ctx:J}}}),l=new R({props:{id:"install torchao",option:"PyTorch Index",$$slots:{default:[oa]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function pa(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEZsb2F0OER5bmFtaWNBY3RpdmF0aW9uRmxvYXQ4V2VpZ2h0Q29uZmlnJTJDJTIwRmxvYXQ4V2VpZ2h0T25seUNvbmZpZyUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEZsb2F0OER5bmFtaWNBY3RpdmF0aW9uRmxvYXQ4V2VpZ2h0Q29uZmlnKCklMEElMjMlMjBvciUyMGZsb2F0OCUyMHdlaWdodCUyMG9ubHklMjBxdWFudGl6YXRpb24lMEElMjMlMjBxdWFudF9jb25maWclMjAlM0QlMjBGbG9hdDhXZWlnaHRPbmx5Q29uZmlnKCklMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwVG9yY2hBb0NvbmZpZyhxdWFudF90eXBlJTNEcXVhbnRfY29uZmlnKSUwQSUwQSUyMyUyMExvYWQlMjBhbmQlMjBxdWFudGl6ZSUyMHRoZSUyMG1vZGVsJTBBcXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTBBKSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQSUyMyUyMGF1dG8tY29tcGlsZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2l0aCUyMCU2MGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTYwJTIwdG8lMjBnZXQlMjBzcGVlZCUyMHVwJTBBb3V0cHV0JTIwJTNEJTIwcXVhbnRpemVkX21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Float8DynamicActivationFloat8WeightConfig, Float8WeightOnlyConfig

quant_config = Float8DynamicActivationFloat8WeightConfig()
<span class="hljs-comment"># or float8 weight only quantization</span>
<span class="hljs-comment"># quant_config = Float8WeightOnlyConfig()</span>
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ca(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEdlbWxpdGVVSW50WFdlaWdodE9ubHlDb25maWclMEElMEElMjMlMjBXZSUyMGludGVncmF0ZWQlMjB3aXRoJTIwZ2VtbGl0ZSUyQyUyMHdoaWNoJTIwb3B0aW1pemVzJTIwZm9yJTIwYmF0Y2glMjBzaXplJTIwTiUyMG9uJTIwQTEwMCUyMGFuZCUyMEgxMDAlMEFxdWFudF9jb25maWclMjAlM0QlMjBHZW1saXRlVUludFhXZWlnaHRPbmx5Q29uZmlnKGdyb3VwX3NpemUlM0QxMjgpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUwQSklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyKSUwQWlucHV0X3RleHQlMjAlM0QlMjAlMjJXaGF0JTIwYXJlJTIwd2UlMjBoYXZpbmclMjBmb3IlMjBkaW5uZXIlM0YlMjIlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEElMjMlMjBhdXRvLWNvbXBpbGUlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUyMHdpdGglMjAlNjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiU2MCUyMHRvJTIwZ2V0JTIwc3BlZWQlMjB1cCUwQW91dHB1dCUyMCUzRCUyMHF1YW50aXplZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAlMkMlMjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> GemliteUIntXWeightOnlyConfig

<span class="hljs-comment"># We integrated with gemlite, which optimizes for batch size N on A100 and H100</span>
quant_config = GemliteUIntXWeightOnlyConfig(group_size=<span class="hljs-number">128</span>)
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ua(J){let n,i,l,r;return n=new R({props:{id:"examples-H100-GPU",option:"float8-dynamic-and-weight-only",$$slots:{default:[pa]},$$scope:{ctx:J}}}),l=new R({props:{id:"examples-H100-GPU",option:"int4-weight-only",$$slots:{default:[ca]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function ra(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWclMkMlMjBJbnQ4V2VpZ2h0T25seUNvbmZpZyUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWcoKSUwQSUyMyUyMG9yJTIwaW50OCUyMHdlaWdodCUyMG9ubHklMjBxdWFudGl6YXRpb24lMEElMjMlMjBxdWFudF9jb25maWclMjAlM0QlMjBJbnQ4V2VpZ2h0T25seUNvbmZpZygpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUwQSklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyKSUwQWlucHV0X3RleHQlMjAlM0QlMjAlMjJXaGF0JTIwYXJlJTIwd2UlMjBoYXZpbmclMjBmb3IlMjBkaW5uZXIlM0YlMjIlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEElMjMlMjBhdXRvLWNvbXBpbGUlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUyMHdpdGglMjAlNjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiU2MCUyMHRvJTIwZ2V0JTIwc3BlZWQlMjB1cCUwQW91dHB1dCUyMCUzRCUyMHF1YW50aXplZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAlMkMlMjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

quant_config = Int8DynamicActivationInt8WeightConfig()
<span class="hljs-comment"># or int8 weight only quantization</span>
<span class="hljs-comment"># quant_config = Int8WeightOnlyConfig()</span>
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function da(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEdlbWxpdGVVSW50WFdlaWdodE9ubHlDb25maWclMkMlMjBJbnQ0V2VpZ2h0T25seUNvbmZpZyUwQSUwQSUyMyUyMEZvciUyMGJhdGNoJTIwc2l6ZSUyME4lMkMlMjB3ZSUyMHJlY29tbWVuZCUyMGdlbWxpdGUlMkMlMjB3aGljaCUyMG1heSUyMHJlcXVpcmUlMjBhdXRvdHVuaW5nJTBBJTIzJTIwZGVmYXVsdCUyMGlzJTIwNCUyMGJpdCUyQyUyMDglMjBiaXQlMjBpcyUyMGFsc28lMjBzdXBwb3J0ZWQlMjBieSUyMHBhc3NpbmclMjAlNjBiaXRfd2lkdGglM0Q4JTYwJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwR2VtbGl0ZVVJbnRYV2VpZ2h0T25seUNvbmZpZyhncm91cF9zaXplJTNEMTI4KSUwQSUwQSUyMyUyMEZvciUyMGJhdGNoJTIwc2l6ZSUyMDElMkMlMjB3ZSUyMGFsc28lMjBoYXZlJTIwY3VzdG9tJTIwdGlueWdlbW0lMjBrZXJuZWwlMjB0aGF0J3MlMjBvbmx5JTIwb3B0aW1pemVkJTIwZm9yJTIwdGhpcyUwQSUyMyUyMFdlJTIwY2FuJTIwc2V0JTIwJTYwdXNlX2hxcSU2MCUyMHRvJTIwJTYwVHJ1ZSU2MCUyMGZvciUyMGJldHRlciUyMGFjY3VyYWN5JTBBJTIzJTIwcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50NFdlaWdodE9ubHlDb25maWcoZ3JvdXBfc2l6ZSUzRDEyOCUyQyUyMHVzZV9ocXElM0RUcnVlKSUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBUb3JjaEFvQ29uZmlnKHF1YW50X3R5cGUlM0RxdWFudF9jb25maWcpJTBBJTBBJTIzJTIwTG9hZCUyMGFuZCUyMHF1YW50aXplJTIwdGhlJTIwbW9kZWwlMEFxdWFudGl6ZWRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiklMEFpbnB1dF90ZXh0JTIwJTNEJTIwJTIyV2hhdCUyMGFyZSUyMHdlJTIwaGF2aW5nJTIwZm9yJTIwZGlubmVyJTNGJTIyJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKGlucHV0X3RleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBJTIzJTIwYXV0by1jb21waWxlJTIwdGhlJTIwcXVhbnRpemVkJTIwbW9kZWwlMjB3aXRoJTIwJTYwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIlNjAlMjB0byUyMGdldCUyMHNwZWVkJTIwdXAlMEFvdXRwdXQlMjAlM0QlMjBxdWFudGl6ZWRfbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIpJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> GemliteUIntXWeightOnlyConfig, Int4WeightOnlyConfig

<span class="hljs-comment"># For batch size N, we recommend gemlite, which may require autotuning</span>
<span class="hljs-comment"># default is 4 bit, 8 bit is also supported by passing \`bit_width=8\`</span>
quant_config = GemliteUIntXWeightOnlyConfig(group_size=<span class="hljs-number">128</span>)

<span class="hljs-comment"># For batch size 1, we also have custom tinygemm kernel that&#x27;s only optimized for this</span>
<span class="hljs-comment"># We can set \`use_hqq\` to \`True\` for better accuracy</span>
<span class="hljs-comment"># quant_config = Int4WeightOnlyConfig(group_size=128, use_hqq=True)</span>

quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ma(J){let n,i,l,r;return n=new R({props:{id:"examples-A100-GPU",option:"int8-dynamic-and-weight-only",$$slots:{default:[ra]},$$scope:{ctx:J}}}),l=new R({props:{id:"examples-A100-GPU",option:"int4-weight-only",$$slots:{default:[da]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function Ma(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWclMkMlMjBJbnQ4V2VpZ2h0T25seUNvbmZpZyUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWcoKSUwQSUyMyUyMG9yJTIwaW50OCUyMHdlaWdodCUyMG9ubHklMjBxdWFudGl6YXRpb24lMEElMjMlMjBxdWFudF9jb25maWclMjAlM0QlMjBJbnQ4V2VpZ2h0T25seUNvbmZpZygpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUwQSklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyKSUwQWlucHV0X3RleHQlMjAlM0QlMjAlMjJXaGF0JTIwYXJlJTIwd2UlMjBoYXZpbmclMjBmb3IlMjBkaW5uZXIlM0YlMjIlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEElMjMlMjBhdXRvLWNvbXBpbGUlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUyMHdpdGglMjAlNjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiU2MCUyMHRvJTIwZ2V0JTIwc3BlZWQlMjB1cCUwQW91dHB1dCUyMCUzRCUyMHF1YW50aXplZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAlMkMlMjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

quant_config = Int8DynamicActivationInt8WeightConfig()
<span class="hljs-comment"># or int8 weight only quantization</span>
<span class="hljs-comment"># quant_config = Int8WeightOnlyConfig()</span>
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ya(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMHRvcmNoYW8uZHR5cGVzJTIwaW1wb3J0JTIwSW50NFhQVUxheW91dCUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbi5xdWFudF9wcmltaXRpdmVzJTIwaW1wb3J0JTIwWmVyb1BvaW50RG9tYWluJTBBJTBBJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50NFdlaWdodE9ubHlDb25maWcoZ3JvdXBfc2l6ZSUzRDEyOCUyQyUyMGxheW91dCUzREludDRYUFVMYXlvdXQoKSUyQyUyMHplcm9fcG9pbnRfZG9tYWluJTNEWmVyb1BvaW50RG9tYWluLklOVCklMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwVG9yY2hBb0NvbmZpZyhxdWFudF90eXBlJTNEcXVhbnRfY29uZmlnKSUwQSUwQSUyMyUyMExvYWQlMjBhbmQlMjBxdWFudGl6ZSUyMHRoZSUyMG1vZGVsJTBBcXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTBBKSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQSUyMyUyMGF1dG8tY29tcGlsZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2l0aCUyMCU2MGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTYwJTIwdG8lMjBnZXQlMjBzcGVlZCUyMHVwJTBBb3V0cHV0JTIwJTNEJTIwcXVhbnRpemVkX21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> torchao.dtypes <span class="hljs-keyword">import</span> Int4XPULayout
<span class="hljs-keyword">from</span> torchao.quantization.quant_primitives <span class="hljs-keyword">import</span> ZeroPointDomain


quant_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>, layout=Int4XPULayout(), zero_point_domain=ZeroPointDomain.INT)
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ha(J){let n,i,l,r;return n=new R({props:{id:"examples-Intel-XPU",option:"int8-dynamic-and-weight-only",$$slots:{default:[Ma]},$$scope:{ctx:J}}}),l=new R({props:{id:"examples-Intel-XPU",option:"int4-weight-only",$$slots:{default:[ya]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function ba(J){let n,i;return n=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWclMkMlMjBJbnQ4V2VpZ2h0T25seUNvbmZpZyUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEludDhEeW5hbWljQWN0aXZhdGlvbkludDhXZWlnaHRDb25maWcoKSUwQSUyMyUyMHF1YW50X2NvbmZpZyUyMCUzRCUyMEludDhXZWlnaHRPbmx5Q29uZmlnKCklMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwVG9yY2hBb0NvbmZpZyhxdWFudF90eXBlJTNEcXVhbnRfY29uZmlnKSUwQSUwQSUyMyUyMExvYWQlMjBhbmQlMjBxdWFudGl6ZSUyMHRoZSUyMG1vZGVsJTBBcXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyY3B1JTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiklMEFpbnB1dF90ZXh0JTIwJTNEJTIwJTIyV2hhdCUyMGFyZSUyMHdlJTIwaGF2aW5nJTIwZm9yJTIwZGlubmVyJTNGJTIyJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKGlucHV0X3RleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMGF1dG8tY29tcGlsZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2l0aCUyMCU2MGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTYwJTIwdG8lMjBnZXQlMjBzcGVlZCUyMHVwJTBBb3V0cHV0JTIwJTNEJTIwcXVhbnRpemVkX21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

quant_config = Int8DynamicActivationInt8WeightConfig()
<span class="hljs-comment"># quant_config = Int8WeightOnlyConfig()</span>
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;cpu&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function Ja(J){let n,i="Run the quantized model on a CPU by changing <code>device_map</code> to <code>&quot;cpu&quot;</code> and <code>layout</code> to <code>Int4CPULayout()</code>.";return{c(){n=w("p"),n.innerHTML=i},l(l){n=T(l,"P",{"data-svelte-h":!0}),f(n)!=="svelte-10s5k0"&&(n.innerHTML=i)},m(l,r){o(l,n,r)},p:I,d(l){l&&s(n)}}}function wa(J){let n,i,l,r;return n=new cn({props:{warning:!1,$$slots:{default:[Ja]},$$scope:{ctx:J}}}),l=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMHRvcmNoYW8uZHR5cGVzJTIwaW1wb3J0JTIwSW50NENQVUxheW91dCUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEludDRXZWlnaHRPbmx5Q29uZmlnKGdyb3VwX3NpemUlM0QxMjglMkMlMjBsYXlvdXQlM0RJbnQ0Q1BVTGF5b3V0KCkpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmNwdSUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTBBKSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBhdXRvLWNvbXBpbGUlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUyMHdpdGglMjAlNjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiU2MCUyMHRvJTIwZ2V0JTIwc3BlZWQlMjB1cCUwQW91dHB1dCUyMCUzRCUyMHF1YW50aXplZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAlMkMlMjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> torchao.dtypes <span class="hljs-keyword">import</span> Int4CPULayout

quant_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>, layout=Int4CPULayout())
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;cpu&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function Ta(J){let n,i,l,r;return n=new R({props:{id:"examples-CPU",option:"int8-dynamic-and-weight-only",$$slots:{default:[ba]},$$scope:{ctx:J}}}),l=new R({props:{id:"examples-CPU",option:"int4-weight-only",$$slots:{default:[wa]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function fa(J){let n,i;return n=new j({props:{code:"JTIzJTIwZG9uJ3QlMjBzZXJpYWxpemUlMjBtb2RlbCUyMHdpdGglMjBTYWZldGVuc29ycyUwQW91dHB1dF9kaXIlMjAlM0QlMjAlMjJsbGFtYTMtOGItaW50NHdvLTEyOCUyMiUwQXF1YW50aXplZF9tb2RlbC5zYXZlX3ByZXRyYWluZWQoJTIybGxhbWEzLThiLWludDR3by0xMjglMjIlMkMlMjBzYWZlX3NlcmlhbGl6YXRpb24lM0RGYWxzZSk=",highlighted:`<span class="hljs-comment"># don&#x27;t serialize model with Safetensors</span>
output_dir = <span class="hljs-string">&quot;llama3-8b-int4wo-128&quot;</span>
quantized_model.save_pretrained(<span class="hljs-string">&quot;llama3-8b-int4wo-128&quot;</span>, safe_serialization=<span class="hljs-literal">False</span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function Ua(J){let n,i;return n=new j({props:{code:"JTIzJTIwZG9uJ3QlMjBzZXJpYWxpemUlMjBtb2RlbCUyMHdpdGglMjBTYWZldGVuc29ycyUwQVVTRVJfSUQlMjAlM0QlMjAlMjJ5b3VyX2h1Z2dpbmdmYWNlX3VzZXJfaWQlMjIlMEFSRVBPX0lEJTIwJTNEJTIwJTIybGxhbWEzLThiLWludDR3by0xMjglMjIlMEFxdWFudGl6ZWRfbW9kZWwucHVzaF90b19odWIoZiUyMiU3QlVTRVJfSUQlN0QlMkZsbGFtYTMtOGItaW50NHdvLTEyOCUyMiUyQyUyMHNhZmVfc2VyaWFsaXphdGlvbiUzREZhbHNlKSUwQXRva2VuaXplci5wdXNoX3RvX2h1YihmJTIyJTdCVVNFUl9JRCU3RCUyRmxsYW1hMy04Yi1pbnQ0d28tMTI4JTIyKQ==",highlighted:`<span class="hljs-comment"># don&#x27;t serialize model with Safetensors</span>
USER_ID = <span class="hljs-string">&quot;your_huggingface_user_id&quot;</span>
REPO_ID = <span class="hljs-string">&quot;llama3-8b-int4wo-128&quot;</span>
quantized_model.push_to_hub(<span class="hljs-string">f&quot;<span class="hljs-subst">{USER_ID}</span>/llama3-8b-int4wo-128&quot;</span>, safe_serialization=<span class="hljs-literal">False</span>)
tokenizer.push_to_hub(<span class="hljs-string">f&quot;<span class="hljs-subst">{USER_ID}</span>/llama3-8b-int4wo-128&quot;</span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(l){m(n.$$.fragment,l)},m(l,r){M(n,l,r),i=!0},p:I,i(l){i||(y(n.$$.fragment,l),i=!0)},o(l){h(n.$$.fragment,l),i=!1},d(l){b(n,l)}}}function ja(J){let n,i,l,r;return n=new R({props:{id:"serialization-examples",option:"save-locally",$$slots:{default:[fa]},$$scope:{ctx:J}}}),l=new R({props:{id:"serialization-examples",option:"push-to-huggingface-hub",$$slots:{default:[Ua]},$$scope:{ctx:J}}}),{c(){d(n.$$.fragment),i=c(),d(l.$$.fragment)},l(a){m(n.$$.fragment,a),i=u(a),m(l.$$.fragment,a)},m(a,p){M(n,a,p),o(a,i,p),M(l,a,p),r=!0},p(a,p){const U={};p&2&&(U.$$scope={dirty:p,ctx:a}),n.$set(U);const W={};p&2&&(W.$$scope={dirty:p,ctx:a}),l.$set(W)},i(a){r||(y(n.$$.fragment,a),y(l.$$.fragment,a),r=!0)},o(a){h(n.$$.fragment,a),h(l.$$.fragment,a),r=!1},d(a){a&&s(i),b(n,a),b(l,a)}}}function Wa(J){let n,i="For best performance, you can use recommended settings by calling <code>torchao.quantization.utils.recommended_inductor_config_setter()</code>";return{c(){n=w("p"),n.innerHTML=i},l(l){n=T(l,"P",{"data-svelte-h":!0}),f(n)!=="svelte-dcid7j"&&(n.innerHTML=i)},m(l,r){o(l,n,r)},p:I,d(l){l&&s(n)}}}function Za(J){let n,i,l,r,a,p,U,W='<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quantization/torchao.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: Torchao Demo"/></a>',Ht,q,un='<a href="https://github.com/pytorch/ao" rel="nofollow">torchao</a> is a PyTorch architecture optimization library with support for custom high performance data types, quantization, and sparsity. It is composable with native PyTorch features such as <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a> for even faster inference and training.',xt,v,rn="See the table below for additional torchao features.",St,Q,dn='<thead><tr><th>Feature</th> <th>Description</th></tr></thead> <tbody><tr><td><strong>Quantization Aware Training (QAT)</strong></td> <td>Train quantized models with minimal accuracy loss (see <a href="https://github.com/pytorch/ao/blob/main/torchao/quantization/qat/README.md" rel="nofollow">QAT README</a>)</td></tr> <tr><td><strong>Float8 Training</strong></td> <td>High-throughput training with float8 formats (see <a href="https://github.com/pytorch/torchtitan/blob/main/docs/float8.md" rel="nofollow">torchtitan</a> and <a href="https://huggingface.co/docs/accelerate/usage_guides/low_precision_training#configuring-torchao" rel="nofollow">Accelerate</a> docs)</td></tr> <tr><td><strong>Sparsity Support</strong></td> <td>Semi-structured (2:4) sparsity for faster inference (see <a href="https://pytorch.org/blog/accelerating-neural-network-training/" rel="nofollow">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity</a> blog post)</td></tr> <tr><td><strong>Optimizer Quantization</strong></td> <td>Reduce optimizer state memory with 4 and 8-bit variants of Adam</td></tr> <tr><td><strong>KV Cache Quantization</strong></td> <td>Enables long context inference with lower memory (see <a href="https://github.com/pytorch/ao/blob/main/torchao/_models/llama/README.md" rel="nofollow">KV Cache Quantization</a>)</td></tr> <tr><td><strong>Custom Kernels Support</strong></td> <td>use your own <code>torch.compile</code> compatible ops</td></tr> <tr><td><strong>FSDP2</strong></td> <td>Composable with FSDP2 for training</td></tr></tbody>',At,B,Lt,E,mn='torchao supports the <a href="https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md" rel="nofollow">quantization techniques</a> below.',Dt,Y,Mn="<li>A16W8 Float8 Dynamic Quantization</li> <li>A16W8 Float8 WeightOnly Quantization</li> <li>A8W8 Int8 Dynamic Quantization</li> <li>A16W8 Int8 Weight Only Quantization</li> <li>A16W4 Int4 Weight Only Quantization</li> <li>A16W4 Int4 Weight Only Quantization + 2:4 Sparsity</li> <li>Autoquantization</li>",Kt,N,yn="torchao also supports module level configuration by specifying a dictionary from fully qualified name of module and its corresponding quantization config. This allows skip quantizing certain layers and using different quantization config for different modules.",Pt,H,hn="Check the table below to see if your hardware is compatible.",Ot,x,bn="<thead><tr><th>Component</th> <th>Compatibility</th></tr></thead> <tbody><tr><td>CUDA Versions</td> <td>âœ… cu118, cu126, cu128</td></tr> <tr><td>XPU Versions</td> <td>âœ… pytorch2.8</td></tr> <tr><td>CPU</td> <td>âœ… change <code>device_map=&quot;cpu&quot;</code> (see examples below)</td></tr></tbody>",tl,S,Jn="Install torchao from PyPi or the PyTorch index with the following commands.",ll,C,nl,A,wn='If your torchao version is below 0.10.0, you need to upgrade it, please refer to the <a href="#deprecation-notice">deprecation notice</a> for more details.',al,L,el,D,Tn="TorchAO provides a variety of quantization configurations. Each configuration can be further customized with parameters such as <code>group_size</code>, <code>scheme</code>, and <code>layout</code> to optimize for specific hardware and model architectures.",sl,K,fn='For a complete list of available configurations, see the <a href="https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py" rel="nofollow">quantization API documentation</a>.',ol,P,Un="You can manually choose the quantization types and settings or automatically select the quantization types.",il,O,jn='Create a <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.TorchAoConfig">TorchAoConfig</a> and specify the quantization type and <code>group_size</code> of the weights to quantize (for int8 weight only and int4 weight only). Set the <code>cache_implementation</code> to <code>&quot;static&quot;</code> to automatically <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a> the forward method.',pl,tt,Wn="Weâ€™ll show examples for recommended quantization methods based on hardwares, e.g. A100 GPU, H100 GPU, CPU.",cl,lt,ul,V,rl,nt,dl,at,ml,_,Ml,et,yl,st,hl,F,bl,ot,Jl,z,wl,it,Tl,pt,fl,ct,Zn="With <code>ModuleFqnToConfig</code> we can specify a default configuration for all layers while skipping quantization for certain layers.",Ul,ut,jl,rt,Wl,dt,Zl,mt,gl,Mt,gn='If you want to automatically choose a quantization type for quantizable layers (<code>nn.Linear</code>) you can use the <a href="https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant" rel="nofollow">autoquant</a> API.',Gl,yt,Gn="The <code>autoquant</code> API automatically chooses a quantization type by micro-benchmarking on input type and shape and compiling a single linear layer.",Xl,ht,Xn="Note: autoquant is for GPU only right now.",Il,bt,In='Create a <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.TorchAoConfig">TorchAoConfig</a> and set to <code>&quot;autoquant&quot;</code>. Set the <code>cache_implementation</code> to <code>&quot;static&quot;</code> to automatically <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a> the forward method. Finally, call <code>finalize_autoquant</code> on the quantized model to finalize the quantization and log the input shapes.',Rl,Jt,Bl,wt,Cl,Tt,Rn='torchao implements <a href="https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor" rel="nofollow">torch.Tensor subclasses</a> for maximum flexibility in supporting new quantized torch.Tensor formats. <a href="https://huggingface.co/docs/safetensors/en/index" rel="nofollow">Safetensors</a> serialization and deserialization does not work with torchao.',Vl,ft,Bn='To avoid arbitrary user code execution, torchao sets <code>weights_only=True</code> in <a href="https://pytorch.org/docs/stable/generated/torch.load.html" rel="nofollow">torch.load</a> to ensure only tensors are loaded. Any known user functions can be whitelisted with <a href="https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals" rel="nofollow">add_safe_globals</a>.',_l,k,Fl,Ut,zl,jt,Cn="Loading a quantized model depends on the quantization scheme. For quantization schemes, like int8 and float8, you can quantize the model on any device and also load it on any device. The example below demonstrates quantizing a model on the CPU and then loading it on CUDA or XPU.",kl,Wt,$l,Zt,Vn="For int4, the model can only be loaded on the same device it was quantized on because the layout is specific to the device. The example below demonstrates quantizing and loading a model on the CPU.",ql,gt,vl,Gt,Ql,Z,zt,_n="Starting with version 0.10.0, the string-based API for quantization configuration (e.g., <code>TorchAoConfig(&quot;int4_weight_only&quot;, group_size=128)</code>) is <strong>deprecated</strong> and will be removed in a future release.",tn,kt,Fn="Please use the new <code>AOBaseConfig</code>-based approach instead:",ln,Xt,nn,$t,zn="The new API offers greater flexibility, better type safety, and access to the full range of features available in torchao.",an,qt,kn='<a href="#migration-guide">Migration Guide</a>',en,vt,$n="Hereâ€™s how to migrate from common string identifiers to their <code>AOBaseConfig</code> equivalents:",sn,Qt,qn="<thead><tr><th>Old String API</th> <th>New <code>AOBaseConfig</code> API</th></tr></thead> <tbody><tr><td><code>&quot;int4_weight_only&quot;</code></td> <td><code>Int4WeightOnlyConfig()</code></td></tr> <tr><td><code>&quot;int8_weight_only&quot;</code></td> <td><code>Int8WeightOnlyConfig()</code></td></tr> <tr><td><code>&quot;int8_dynamic_activation_int8_weight&quot;</code></td> <td><code>Int8DynamicActivationInt8WeightConfig()</code></td></tr></tbody>",on,Et,vn="All configuration objects accept parameters for customization (e.g., <code>group_size</code>, <code>scheme</code>, <code>layout</code>).",El,It,Yl,Rt,Qn='For a better sense of expected performance, view the <a href="https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks" rel="nofollow">benchmarks</a> for various models with CUDA and XPU backends. You can also run the code below to benchmark a model yourself.',Nl,Bt,Hl,$,xl,Ct,En='Refer to <a href="https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques" rel="nofollow">Other Available Quantization Techniques</a> for more examples and documentation.',Sl,Vt,Al,_t,Yn='If you encounter any issues with the Transformers integration, please open an issue on the <a href="https://github.com/huggingface/transformers/issues" rel="nofollow">Transformers</a> repository. For issues directly related to torchao, please open an issue on the <a href="https://github.com/pytorch/ao/issues" rel="nofollow">torchao</a> repository.',Ll,Ft,Dl,Nt,Kl;return a=new X({props:{title:"torchao",local:"torchao",headingTag:"h1"}}),B=new cn({props:{warning:!1,$$slots:{default:[ea]},$$scope:{ctx:J}}}),C=new Yt({props:{id:"install torchao",options:["PyPi","PyTorch Index"],$$slots:{default:[ia]},$$scope:{ctx:J}}}),L=new X({props:{title:"Quantization examples",local:"quantization-examples",headingTag:"h2"}}),lt=new X({props:{title:"H100 GPU",local:"h100-gpu",headingTag:"h3"}}),V=new Yt({props:{id:"examples-H100-GPU",options:["float8-dynamic-and-weight-only","int4-weight-only"],$$slots:{default:[ua]},$$scope:{ctx:J}}}),nt=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMHRvcmNoYW8uZHR5cGVzJTIwaW1wb3J0JTIwTWFybGluU3BhcnNlTGF5b3V0JTBBJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50NFdlaWdodE9ubHlDb25maWcobGF5b3V0JTNETWFybGluU3BhcnNlTGF5b3V0KCkpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUyMHdpdGglMjBzcGFyc2l0eS4lMjBBJTIwc3BhcnNlJTIwY2hlY2twb2ludCUyMGlzJTIwbmVlZGVkJTIwdG8lMjBhY2NlbGVyYXRlJTIwd2l0aG91dCUyMGFjY3VyYWN5JTIwbG9zcyUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJSZWRIYXRBSSUyRlNwYXJzZS1MbGFtYS0zLjEtOEItMm9mNCUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyUmVkSGF0QUklMkZTcGFyc2UtTGxhbWEtMy4xLThCLTJvZjQlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQSUyMyUyMGF1dG8tY29tcGlsZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2l0aCUyMCU2MGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTYwJTIwdG8lMjBnZXQlMjBzcGVlZCUyMHVwJTBBb3V0cHV0JTIwJTNEJTIwcXVhbnRpemVkX21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> torchao.dtypes <span class="hljs-keyword">import</span> MarlinSparseLayout

quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;RedHatAI/Sparse-Llama-3.1-8B-2of4&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;RedHatAI/Sparse-Llama-3.1-8B-2of4&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),at=new X({props:{title:"A100 GPU",local:"a100-gpu",headingTag:"h3"}}),_=new Yt({props:{id:"examples-A100-GPU",options:["int8-dynamic-and-weight-only","int4-weight-only"],$$slots:{default:[ma]},$$scope:{ctx:J}}}),et=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMHRvcmNoYW8uZHR5cGVzJTIwaW1wb3J0JTIwTWFybGluU3BhcnNlTGF5b3V0JTBBJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50NFdlaWdodE9ubHlDb25maWcobGF5b3V0JTNETWFybGluU3BhcnNlTGF5b3V0KCkpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUyMHdpdGglMjBzcGFyc2l0eS4lMjBBJTIwc3BhcnNlJTIwY2hlY2twb2ludCUyMGlzJTIwbmVlZGVkJTIwdG8lMjBhY2NlbGVyYXRlJTIwd2l0aG91dCUyMGFjY3VyYWN5JTIwbG9zcyUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJSZWRIYXRBSSUyRlNwYXJzZS1MbGFtYS0zLjEtOEItMm9mNCUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyUmVkSGF0QUklMkZTcGFyc2UtTGxhbWEtMy4xLThCLTJvZjQlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQSUyMyUyMGF1dG8tY29tcGlsZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2l0aCUyMCU2MGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTYwJTIwdG8lMjBnZXQlMjBzcGVlZCUyMHVwJTBBb3V0cHV0JTIwJTNEJTIwcXVhbnRpemVkX21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> torchao.dtypes <span class="hljs-keyword">import</span> MarlinSparseLayout

quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;RedHatAI/Sparse-Llama-3.1-8B-2of4&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;RedHatAI/Sparse-Llama-3.1-8B-2of4&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),st=new X({props:{title:"Intel XPU",local:"intel-xpu",headingTag:"h3"}}),F=new Yt({props:{id:"examples-Intel-XPU",options:["int8-dynamic-and-weight-only","int4-weight-only"],$$slots:{default:[ha]},$$scope:{ctx:J}}}),ot=new X({props:{title:"CPU",local:"cpu",headingTag:"h3"}}),z=new Yt({props:{id:"examples-CPU",options:["int8-dynamic-and-weight-only","int4-weight-only"],$$slots:{default:[Ta]},$$scope:{ctx:J}}}),it=new X({props:{title:"Per Module Quantization",local:"per-module-quantization",headingTag:"h3"}}),pt=new X({props:{title:"1. Skip quantization for certain layers",local:"1-skip-quantization-for-certain-layers",headingTag:"h4"}}),ut=new j({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

model_id = <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>

<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig, ModuleFqnToConfig
config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>)

<span class="hljs-comment"># set default to int4 (for linears), and skip quantizing \`model.layers.0.self_attn.q_proj\`</span>
quant_config = ModuleFqnToConfig({<span class="hljs-string">&quot;_default&quot;</span>: config, <span class="hljs-string">&quot;model.layers.0.self_attn.q_proj&quot;</span>: <span class="hljs-literal">None</span>})
quantization_config = TorchAoConfig(quant_type=quant_config)
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, dtype=torch.bfloat16, quantization_config=quantization_config)
<span class="hljs-comment"># lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;quantized model:&quot;</span>, quantized_model)
tokenizer = AutoTokenizer.from_pretrained(model_id)

<span class="hljs-comment"># Manual Testing</span>
prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(quantized_model.device.<span class="hljs-built_in">type</span>)
generated_ids = quantized_model.generate(**inputs, max_new_tokens=<span class="hljs-number">128</span>)
output_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>
)
<span class="hljs-built_in">print</span>(output_text)`,wrap:!1}}),rt=new X({props:{title:"2. Quantizing different layers with different quantization configs",local:"2-quantizing-different-layers-with-different-quantization-configs",headingTag:"h4"}}),dt=new j({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

model_id = <span class="hljs-string">&quot;facebook/opt-125m&quot;</span>

<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig, ModuleFqnToConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType

weight_dtype = torch.int8
granularity = PerAxis(<span class="hljs-number">0</span>)
mapping_type = MappingType.ASYMMETRIC
embedding_config = IntxWeightOnlyConfig(
    weight_dtype=weight_dtype,
    granularity=granularity,
    mapping_type=mapping_type,
)
linear_config = Int8DynamicActivationInt4WeightConfig(group_size=<span class="hljs-number">128</span>)
quant_config = ModuleFqnToConfig({<span class="hljs-string">&quot;_default&quot;</span>: linear_config, <span class="hljs-string">&quot;model.decoder.embed_tokens&quot;</span>: embedding_config, <span class="hljs-string">&quot;model.decoder.embed_positions&quot;</span>: <span class="hljs-literal">None</span>})
<span class="hljs-comment"># set \`include_embedding\` to True in order to include embedding in quantization</span>
<span class="hljs-comment"># when \`include_embedding\` is True, we&#x27;ll remove input embedding from \`modules_not_to_convert\` as well</span>
quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=<span class="hljs-literal">True</span>)
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;cpu&quot;</span>, dtype=torch.bfloat16, quantization_config=quantization_config)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;quantized model:&quot;</span>, quantized_model)
<span class="hljs-comment"># make sure embedding is quantized</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;embed_tokens weight:&quot;</span>, quantized_model.model.decoder.embed_tokens.weight)
tokenizer = AutoTokenizer.from_pretrained(model_id)

<span class="hljs-comment"># Manual Testing</span>
prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cpu&quot;</span>)
generated_ids = quantized_model.generate(**inputs, max_new_tokens=<span class="hljs-number">128</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
output_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>
)
<span class="hljs-built_in">print</span>(output_text)`,wrap:!1}}),mt=new X({props:{title:"Autoquant",local:"autoquant",headingTag:"h3"}}),Jt=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBUb3JjaEFvQ29uZmlnKCUyMmF1dG9xdWFudCUyMiUyQyUyMG1pbl9zcW5yJTNETm9uZSklMEFxdWFudGl6ZWRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiklMEFpbnB1dF90ZXh0JTIwJTNEJTIwJTIyV2hhdCUyMGFyZSUyMHdlJTIwaGF2aW5nJTIwZm9yJTIwZGlubmVyJTNGJTIyJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKGlucHV0X3RleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhxdWFudGl6ZWRfbW9kZWwuZGV2aWNlLnR5cGUpJTBBJTBBJTIzJTIwYXV0by1jb21waWxlJTIwdGhlJTIwcXVhbnRpemVkJTIwbW9kZWwlMjB3aXRoJTIwJTYwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIlNjAlMjB0byUyMGdldCUyMHNwZWVkJTIwdXAlMEFvdXRwdXQlMjAlM0QlMjBxdWFudGl6ZWRfbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIpJTBBJTIzJTIwZXhwbGljaXRseSUyMGNhbGwlMjAlNjBmaW5hbGl6ZV9hdXRvcXVhbnQlNjAlMjAobWF5JTIwYmUlMjByZWZhY3RvcmVkJTIwYW5kJTIwcmVtb3ZlZCUyMGluJTIwdGhlJTIwZnV0dXJlKSUwQXF1YW50aXplZF9tb2RlbC5maW5hbGl6ZV9hdXRvcXVhbnQoKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer

quantization_config = TorchAoConfig(<span class="hljs-string">&quot;autoquant&quot;</span>, min_sqnr=<span class="hljs-literal">None</span>)
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(quantized_model.device.<span class="hljs-built_in">type</span>)

<span class="hljs-comment"># auto-compile the quantized model with \`cache_implementation=&quot;static&quot;\` to get speed up</span>
output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
<span class="hljs-comment"># explicitly call \`finalize_autoquant\` (may be refactored and removed in the future)</span>
quantized_model.finalize_autoquant()
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),wt=new X({props:{title:"Serialization",local:"serialization",headingTag:"h2"}}),k=new Yt({props:{id:"serialization-examples",options:["save-locally","push-to-huggingface-hub"],$$slots:{default:[ja]},$$scope:{ctx:J}}}),Ut=new X({props:{title:"Loading quantized models",local:"loading-quantized-models",headingTag:"h2"}}),Wt=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDhXZWlnaHRPbmx5Q29uZmlnJTBBJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50OFdlaWdodE9ubHlDb25maWcoZ3JvdXBfc2l6ZSUzRDEyOCklMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwVG9yY2hBb0NvbmZpZyhxdWFudF90eXBlJTNEcXVhbnRfY29uZmlnKSUwQSUwQSUyMyUyMExvYWQlMjBhbmQlMjBxdWFudGl6ZSUyMHRoZSUyMG1vZGVsJTBBcXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyY3B1JTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTIzJTIwc2F2ZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTBBb3V0cHV0X2RpciUyMCUzRCUyMCUyMmxsYW1hLTMuMS04Yi10b3JjaGFvLWludDglMjIlMEFxdWFudGl6ZWRfbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKG91dHB1dF9kaXIlMkMlMjBzYWZlX3NlcmlhbGl6YXRpb24lM0RGYWxzZSklMEElMEElMjMlMjByZWxvYWQlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUwQXJlbG9hZGVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMEEpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04Qi1JbnN0cnVjdCUyMiklMEFpbnB1dF90ZXh0JTIwJTNEJTIwJTIyV2hhdCUyMGFyZSUyMHdlJTIwaGF2aW5nJTIwZm9yJTIwZGlubmVyJTNGJTIyJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKGlucHV0X3RleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhyZWxvYWRlZF9tb2RlbC5kZXZpY2UudHlwZSklMEElMEFvdXRwdXQlMjAlM0QlMjByZWxvYWRlZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTApJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKSUwQQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int8WeightOnlyConfig

quant_config = Int8WeightOnlyConfig(group_size=<span class="hljs-number">128</span>)
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;cpu&quot;</span>,
    quantization_config=quantization_config
)
<span class="hljs-comment"># save the quantized model</span>
output_dir = <span class="hljs-string">&quot;llama-3.1-8b-torchao-int8&quot;</span>
quantized_model.save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># reload the quantized model</span>
reloaded_model = AutoModelForCausalLM.from_pretrained(
    output_dir,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(reloaded_model.device.<span class="hljs-built_in">type</span>)

output = reloaded_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
`,wrap:!1}}),gt=new j({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVG9yY2hBb0NvbmZpZyUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBZnJvbSUyMHRvcmNoYW8uZHR5cGVzJTIwaW1wb3J0JTIwSW50NENQVUxheW91dCUwQSUwQXF1YW50X2NvbmZpZyUyMCUzRCUyMEludDRXZWlnaHRPbmx5Q29uZmlnKGdyb3VwX3NpemUlM0QxMjglMkMlMjBsYXlvdXQlM0RJbnQ0Q1BVTGF5b3V0KCkpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEElMEElMjMlMjBMb2FkJTIwYW5kJTIwcXVhbnRpemUlMjB0aGUlMjBtb2RlbCUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCLUluc3RydWN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmNwdSUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTBBKSUwQSUyMyUyMHNhdmUlMjB0aGUlMjBxdWFudGl6ZWQlMjBtb2RlbCUwQW91dHB1dF9kaXIlMjAlM0QlMjAlMjJsbGFtYS0zLjEtOGItdG9yY2hhby1pbnQ0LWNwdSUyMiUwQXF1YW50aXplZF9tb2RlbC5zYXZlX3ByZXRyYWluZWQob3V0cHV0X2RpciUyQyUyMHNhZmVfc2VyaWFsaXphdGlvbiUzREZhbHNlKSUwQSUwQSUyMyUyMHJlbG9hZCUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTBBcmVsb2FkZWRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJjcHUlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTBBKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjEtOEItSW5zdHJ1Y3QlMjIpJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMldoYXQlMjBhcmUlMjB3ZSUyMGhhdmluZyUyMGZvciUyMGRpbm5lciUzRiUyMiUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplcihpbnB1dF90ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXQlMjAlM0QlMjByZWxvYWRlZF9tb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTApJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKSUwQQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
<span class="hljs-keyword">from</span> torchao.dtypes <span class="hljs-keyword">import</span> Int4CPULayout

quant_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>, layout=Int4CPULayout())
quantization_config = TorchAoConfig(quant_type=quant_config)

<span class="hljs-comment"># Load and quantize the model</span>
quantized_model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
    dtype=<span class="hljs-string">&quot;auto&quot;</span>,
    device_map=<span class="hljs-string">&quot;cpu&quot;</span>,
    quantization_config=quantization_config
)
<span class="hljs-comment"># save the quantized model</span>
output_dir = <span class="hljs-string">&quot;llama-3.1-8b-torchao-int4-cpu&quot;</span>
quantized_model.save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># reload the quantized model</span>
reloaded_model = AutoModelForCausalLM.from_pretrained(
    output_dir,
    device_map=<span class="hljs-string">&quot;cpu&quot;</span>,
    dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

output = reloaded_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))
`,wrap:!1}}),Gt=new X({props:{title:"âš ï¸ Deprecation Notice",local:"-deprecation-notice",headingTag:"h2"}}),Xt=new j({props:{code:"JTIzJTIwT2xkJTIwd2F5JTIwKGRlcHJlY2F0ZWQpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcoJTIyaW50NF93ZWlnaHRfb25seSUyMiUyQyUyMGdyb3VwX3NpemUlM0QxMjgpJTBBJTBBJTIzJTIwTmV3JTIwd2F5JTIwKHJlY29tbWVuZGVkKSUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSW50NFdlaWdodE9ubHlDb25maWcoZ3JvdXBfc2l6ZSUzRDEyOCklMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwVG9yY2hBb0NvbmZpZyhxdWFudF90eXBlJTNEcXVhbnRfY29uZmlnKQ==",highlighted:`<span class="hljs-comment"># Old way (deprecated)</span>
quantization_config = TorchAoConfig(<span class="hljs-string">&quot;int4_weight_only&quot;</span>, group_size=<span class="hljs-number">128</span>)

<span class="hljs-comment"># New way (recommended)</span>
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig
quant_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>)
quantization_config = TorchAoConfig(quant_type=quant_config)`,wrap:!1}}),It=new X({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Bt=new j({props:{code:"ZnJvbSUyMHRvcmNoLl9pbmR1Y3Rvci51dGlscyUyMGltcG9ydCUyMGRvX2JlbmNoX3VzaW5nX3Byb2ZpbGluZyUwQWZyb20lMjB0eXBpbmclMjBpbXBvcnQlMjBDYWxsYWJsZSUwQSUwQWRlZiUyMGJlbmNobWFya19mbihmdW5jJTNBJTIwQ2FsbGFibGUlMkMlMjAqYXJncyUyQyUyMCoqa3dhcmdzKSUyMC0lM0UlMjBmbG9hdCUzQSUwQSUyMCUyMCUyMCUyMCUyMiUyMiUyMlRoaW4lMjB3cmFwcGVyJTIwYXJvdW5kJTIwZG9fYmVuY2hfdXNpbmdfcHJvZmlsaW5nJTIyJTIyJTIyJTBBJTIwJTIwJTIwJTIwbm9fYXJncyUyMCUzRCUyMGxhbWJkYSUzQSUyMGZ1bmMoKmFyZ3MlMkMlMjAqKmt3YXJncyklMEElMjAlMjAlMjAlMjB0aW1lJTIwJTNEJTIwZG9fYmVuY2hfdXNpbmdfcHJvZmlsaW5nKG5vX2FyZ3MpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwdGltZSUyMColMjAxZTMlMEElMEFNQVhfTkVXX1RPS0VOUyUyMCUzRCUyMDEwMDAlMEFwcmludCglMjJpbnQ0d28tMTI4JTIwbW9kZWwlM0ElMjIlMkMlMjBiZW5jaG1hcmtfZm4ocXVhbnRpemVkX21vZGVsLmdlbmVyYXRlJTJDJTIwKippbnB1dF9pZHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRE1BWF9ORVdfVE9LRU5TJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIpKSUwQSUwQWJmMTZfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiklMEFvdXRwdXQlMjAlM0QlMjBiZjE2X21vZGVsLmdlbmVyYXRlKCoqaW5wdXRfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyKSUyMCUyMyUyMGF1dG8tY29tcGlsZSUwQXByaW50KCUyMmJmMTYlMjBtb2RlbCUzQSUyMiUyQyUyMGJlbmNobWFya19mbihiZjE2X21vZGVsLmdlbmVyYXRlJTJDJTIwKippbnB1dF9pZHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRE1BWF9ORVdfVE9LRU5TJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJzdGF0aWMlMjIpKQ==",highlighted:`<span class="hljs-keyword">from</span> torch._inductor.utils <span class="hljs-keyword">import</span> do_bench_using_profiling
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Callable</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">benchmark_fn</span>(<span class="hljs-params">func: <span class="hljs-type">Callable</span>, *args, **kwargs</span>) -&gt; <span class="hljs-built_in">float</span>:
    <span class="hljs-string">&quot;&quot;&quot;Thin wrapper around do_bench_using_profiling&quot;&quot;&quot;</span>
    no_args = <span class="hljs-keyword">lambda</span>: func(*args, **kwargs)
    time = do_bench_using_profiling(no_args)
    <span class="hljs-keyword">return</span> time * <span class="hljs-number">1e3</span>

MAX_NEW_TOKENS = <span class="hljs-number">1000</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;int4wo-128 model:&quot;</span>, benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>))

bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, dtype=torch.bfloat16)
output = bf16_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>) <span class="hljs-comment"># auto-compile</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;bf16 model:&quot;</span>, benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>))`,wrap:!1}}),$=new cn({props:{warning:!1,$$slots:{default:[Wa]},$$scope:{ctx:J}}}),Vt=new X({props:{title:"Issues",local:"issues",headingTag:"h2"}}),Ft=new aa({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md"}}),{c(){n=w("meta"),i=c(),l=w("p"),r=c(),d(a.$$.fragment),p=c(),U=w("p"),U.innerHTML=W,Ht=c(),q=w("p"),q.innerHTML=un,xt=c(),v=w("p"),v.textContent=rn,St=c(),Q=w("table"),Q.innerHTML=dn,At=c(),d(B.$$.fragment),Lt=c(),E=w("p"),E.innerHTML=mn,Dt=c(),Y=w("ul"),Y.innerHTML=Mn,Kt=c(),N=w("p"),N.textContent=yn,Pt=c(),H=w("p"),H.textContent=hn,Ot=c(),x=w("table"),x.innerHTML=bn,tl=c(),S=w("p"),S.textContent=Jn,ll=c(),d(C.$$.fragment),nl=c(),A=w("p"),A.innerHTML=wn,al=c(),d(L.$$.fragment),el=c(),D=w("p"),D.innerHTML=Tn,sl=c(),K=w("p"),K.innerHTML=fn,ol=c(),P=w("p"),P.textContent=Un,il=c(),O=w("p"),O.innerHTML=jn,pl=c(),tt=w("p"),tt.textContent=Wn,cl=c(),d(lt.$$.fragment),ul=c(),d(V.$$.fragment),rl=Pl(`
</hfoption>
<hfoption id="int4-weight-only-24sparse">

	`),d(nt.$$.fragment),dl=Pl(`
</hfoption>
</hfoptions>
`),d(at.$$.fragment),ml=c(),d(_.$$.fragment),Ml=Pl(`
</hfoption>
<hfoption id="int4-weight-only-24sparse">

	`),d(et.$$.fragment),yl=Pl(`
</hfoption>
</hfoptions>
`),d(st.$$.fragment),hl=c(),d(F.$$.fragment),bl=c(),d(ot.$$.fragment),Jl=c(),d(z.$$.fragment),wl=c(),d(it.$$.fragment),Tl=c(),d(pt.$$.fragment),fl=c(),ct=w("p"),ct.innerHTML=Zn,Ul=c(),d(ut.$$.fragment),jl=c(),d(rt.$$.fragment),Wl=c(),d(dt.$$.fragment),Zl=c(),d(mt.$$.fragment),gl=c(),Mt=w("p"),Mt.innerHTML=gn,Gl=c(),yt=w("p"),yt.innerHTML=Gn,Xl=c(),ht=w("p"),ht.textContent=Xn,Il=c(),bt=w("p"),bt.innerHTML=In,Rl=c(),d(Jt.$$.fragment),Bl=c(),d(wt.$$.fragment),Cl=c(),Tt=w("p"),Tt.innerHTML=Rn,Vl=c(),ft=w("p"),ft.innerHTML=Bn,_l=c(),d(k.$$.fragment),Fl=c(),d(Ut.$$.fragment),zl=c(),jt=w("p"),jt.textContent=Cn,kl=c(),d(Wt.$$.fragment),$l=c(),Zt=w("p"),Zt.textContent=Vn,ql=c(),d(gt.$$.fragment),vl=c(),d(Gt.$$.fragment),Ql=c(),Z=w("blockquote"),zt=w("p"),zt.innerHTML=_n,tn=c(),kt=w("p"),kt.innerHTML=Fn,ln=c(),d(Xt.$$.fragment),nn=c(),$t=w("p"),$t.textContent=zn,an=c(),qt=w("p"),qt.innerHTML=kn,en=c(),vt=w("p"),vt.innerHTML=$n,sn=c(),Qt=w("table"),Qt.innerHTML=qn,on=c(),Et=w("p"),Et.innerHTML=vn,El=c(),d(It.$$.fragment),Yl=c(),Rt=w("p"),Rt.innerHTML=Qn,Nl=c(),d(Bt.$$.fragment),Hl=c(),d($.$$.fragment),xl=c(),Ct=w("p"),Ct.innerHTML=En,Sl=c(),d(Vt.$$.fragment),Al=c(),_t=w("p"),_t.innerHTML=Yn,Ll=c(),d(Ft.$$.fragment),Dl=c(),Nt=w("p"),this.h()},l(t){const e=na("svelte-u9bgzb",document.head);n=T(e,"META",{name:!0,content:!0}),e.forEach(s),i=u(t),l=T(t,"P",{}),pn(l).forEach(s),r=u(t),m(a.$$.fragment,t),p=u(t),U=T(t,"P",{"data-svelte-h":!0}),f(U)!=="svelte-8yu6d7"&&(U.innerHTML=W),Ht=u(t),q=T(t,"P",{"data-svelte-h":!0}),f(q)!=="svelte-hptymg"&&(q.innerHTML=un),xt=u(t),v=T(t,"P",{"data-svelte-h":!0}),f(v)!=="svelte-k1mb66"&&(v.textContent=rn),St=u(t),Q=T(t,"TABLE",{"data-svelte-h":!0}),f(Q)!=="svelte-1x8g49v"&&(Q.innerHTML=dn),At=u(t),m(B.$$.fragment,t),Lt=u(t),E=T(t,"P",{"data-svelte-h":!0}),f(E)!=="svelte-bctle4"&&(E.innerHTML=mn),Dt=u(t),Y=T(t,"UL",{"data-svelte-h":!0}),f(Y)!=="svelte-1c7onxq"&&(Y.innerHTML=Mn),Kt=u(t),N=T(t,"P",{"data-svelte-h":!0}),f(N)!=="svelte-1u913mc"&&(N.textContent=yn),Pt=u(t),H=T(t,"P",{"data-svelte-h":!0}),f(H)!=="svelte-1o9cwfq"&&(H.textContent=hn),Ot=u(t),x=T(t,"TABLE",{"data-svelte-h":!0}),f(x)!=="svelte-k3clyq"&&(x.innerHTML=bn),tl=u(t),S=T(t,"P",{"data-svelte-h":!0}),f(S)!=="svelte-1wnb12y"&&(S.textContent=Jn),ll=u(t),m(C.$$.fragment,t),nl=u(t),A=T(t,"P",{"data-svelte-h":!0}),f(A)!=="svelte-1v1t5ji"&&(A.innerHTML=wn),al=u(t),m(L.$$.fragment,t),el=u(t),D=T(t,"P",{"data-svelte-h":!0}),f(D)!=="svelte-mfxtr3"&&(D.innerHTML=Tn),sl=u(t),K=T(t,"P",{"data-svelte-h":!0}),f(K)!=="svelte-1omxb57"&&(K.innerHTML=fn),ol=u(t),P=T(t,"P",{"data-svelte-h":!0}),f(P)!=="svelte-hxxsaz"&&(P.textContent=Un),il=u(t),O=T(t,"P",{"data-svelte-h":!0}),f(O)!=="svelte-14t2jih"&&(O.innerHTML=jn),pl=u(t),tt=T(t,"P",{"data-svelte-h":!0}),f(tt)!=="svelte-yb6i3u"&&(tt.textContent=Wn),cl=u(t),m(lt.$$.fragment,t),ul=u(t),m(V.$$.fragment,t),rl=Ol(t,`
</hfoption>
<hfoption id="int4-weight-only-24sparse">

	`),m(nt.$$.fragment,t),dl=Ol(t,`
</hfoption>
</hfoptions>
`),m(at.$$.fragment,t),ml=u(t),m(_.$$.fragment,t),Ml=Ol(t,`
</hfoption>
<hfoption id="int4-weight-only-24sparse">

	`),m(et.$$.fragment,t),yl=Ol(t,`
</hfoption>
</hfoptions>
`),m(st.$$.fragment,t),hl=u(t),m(F.$$.fragment,t),bl=u(t),m(ot.$$.fragment,t),Jl=u(t),m(z.$$.fragment,t),wl=u(t),m(it.$$.fragment,t),Tl=u(t),m(pt.$$.fragment,t),fl=u(t),ct=T(t,"P",{"data-svelte-h":!0}),f(ct)!=="svelte-60csp8"&&(ct.innerHTML=Zn),Ul=u(t),m(ut.$$.fragment,t),jl=u(t),m(rt.$$.fragment,t),Wl=u(t),m(dt.$$.fragment,t),Zl=u(t),m(mt.$$.fragment,t),gl=u(t),Mt=T(t,"P",{"data-svelte-h":!0}),f(Mt)!=="svelte-1fr0k2r"&&(Mt.innerHTML=gn),Gl=u(t),yt=T(t,"P",{"data-svelte-h":!0}),f(yt)!=="svelte-axxx0x"&&(yt.innerHTML=Gn),Xl=u(t),ht=T(t,"P",{"data-svelte-h":!0}),f(ht)!=="svelte-19318ol"&&(ht.textContent=Xn),Il=u(t),bt=T(t,"P",{"data-svelte-h":!0}),f(bt)!=="svelte-180nclx"&&(bt.innerHTML=In),Rl=u(t),m(Jt.$$.fragment,t),Bl=u(t),m(wt.$$.fragment,t),Cl=u(t),Tt=T(t,"P",{"data-svelte-h":!0}),f(Tt)!=="svelte-1g3oift"&&(Tt.innerHTML=Rn),Vl=u(t),ft=T(t,"P",{"data-svelte-h":!0}),f(ft)!=="svelte-5ma9bd"&&(ft.innerHTML=Bn),_l=u(t),m(k.$$.fragment,t),Fl=u(t),m(Ut.$$.fragment,t),zl=u(t),jt=T(t,"P",{"data-svelte-h":!0}),f(jt)!=="svelte-9pklwu"&&(jt.textContent=Cn),kl=u(t),m(Wt.$$.fragment,t),$l=u(t),Zt=T(t,"P",{"data-svelte-h":!0}),f(Zt)!=="svelte-1oj3qy1"&&(Zt.textContent=Vn),ql=u(t),m(gt.$$.fragment,t),vl=u(t),m(Gt.$$.fragment,t),Ql=u(t),Z=T(t,"BLOCKQUOTE",{});var g=pn(Z);zt=T(g,"P",{"data-svelte-h":!0}),f(zt)!=="svelte-jmxllg"&&(zt.innerHTML=_n),tn=u(g),kt=T(g,"P",{"data-svelte-h":!0}),f(kt)!=="svelte-1qak5au"&&(kt.innerHTML=Fn),ln=u(g),m(Xt.$$.fragment,g),nn=u(g),$t=T(g,"P",{"data-svelte-h":!0}),f($t)!=="svelte-8tiw44"&&($t.textContent=zn),an=u(g),qt=T(g,"P",{"data-svelte-h":!0}),f(qt)!=="svelte-1wt1icj"&&(qt.innerHTML=kn),en=u(g),vt=T(g,"P",{"data-svelte-h":!0}),f(vt)!=="svelte-1kyext4"&&(vt.innerHTML=$n),sn=u(g),Qt=T(g,"TABLE",{"data-svelte-h":!0}),f(Qt)!=="svelte-1w9xui3"&&(Qt.innerHTML=qn),on=u(g),Et=T(g,"P",{"data-svelte-h":!0}),f(Et)!=="svelte-9z9ctj"&&(Et.innerHTML=vn),g.forEach(s),El=u(t),m(It.$$.fragment,t),Yl=u(t),Rt=T(t,"P",{"data-svelte-h":!0}),f(Rt)!=="svelte-1cgj2bs"&&(Rt.innerHTML=Qn),Nl=u(t),m(Bt.$$.fragment,t),Hl=u(t),m($.$$.fragment,t),xl=u(t),Ct=T(t,"P",{"data-svelte-h":!0}),f(Ct)!=="svelte-fj0t1q"&&(Ct.innerHTML=En),Sl=u(t),m(Vt.$$.fragment,t),Al=u(t),_t=T(t,"P",{"data-svelte-h":!0}),f(_t)!=="svelte-1auvp72"&&(_t.innerHTML=Yn),Ll=u(t),m(Ft.$$.fragment,t),Dl=u(t),Nt=T(t,"P",{}),pn(Nt).forEach(s),this.h()},h(){Kn(n,"name","hf:doc:metadata"),Kn(n,"content",ga)},m(t,e){G(document.head,n),o(t,i,e),o(t,l,e),o(t,r,e),M(a,t,e),o(t,p,e),o(t,U,e),o(t,Ht,e),o(t,q,e),o(t,xt,e),o(t,v,e),o(t,St,e),o(t,Q,e),o(t,At,e),M(B,t,e),o(t,Lt,e),o(t,E,e),o(t,Dt,e),o(t,Y,e),o(t,Kt,e),o(t,N,e),o(t,Pt,e),o(t,H,e),o(t,Ot,e),o(t,x,e),o(t,tl,e),o(t,S,e),o(t,ll,e),M(C,t,e),o(t,nl,e),o(t,A,e),o(t,al,e),M(L,t,e),o(t,el,e),o(t,D,e),o(t,sl,e),o(t,K,e),o(t,ol,e),o(t,P,e),o(t,il,e),o(t,O,e),o(t,pl,e),o(t,tt,e),o(t,cl,e),M(lt,t,e),o(t,ul,e),M(V,t,e),o(t,rl,e),M(nt,t,e),o(t,dl,e),M(at,t,e),o(t,ml,e),M(_,t,e),o(t,Ml,e),M(et,t,e),o(t,yl,e),M(st,t,e),o(t,hl,e),M(F,t,e),o(t,bl,e),M(ot,t,e),o(t,Jl,e),M(z,t,e),o(t,wl,e),M(it,t,e),o(t,Tl,e),M(pt,t,e),o(t,fl,e),o(t,ct,e),o(t,Ul,e),M(ut,t,e),o(t,jl,e),M(rt,t,e),o(t,Wl,e),M(dt,t,e),o(t,Zl,e),M(mt,t,e),o(t,gl,e),o(t,Mt,e),o(t,Gl,e),o(t,yt,e),o(t,Xl,e),o(t,ht,e),o(t,Il,e),o(t,bt,e),o(t,Rl,e),M(Jt,t,e),o(t,Bl,e),M(wt,t,e),o(t,Cl,e),o(t,Tt,e),o(t,Vl,e),o(t,ft,e),o(t,_l,e),M(k,t,e),o(t,Fl,e),M(Ut,t,e),o(t,zl,e),o(t,jt,e),o(t,kl,e),M(Wt,t,e),o(t,$l,e),o(t,Zt,e),o(t,ql,e),M(gt,t,e),o(t,vl,e),M(Gt,t,e),o(t,Ql,e),o(t,Z,e),G(Z,zt),G(Z,tn),G(Z,kt),G(Z,ln),M(Xt,Z,null),G(Z,nn),G(Z,$t),G(Z,an),G(Z,qt),G(Z,en),G(Z,vt),G(Z,sn),G(Z,Qt),G(Z,on),G(Z,Et),o(t,El,e),M(It,t,e),o(t,Yl,e),o(t,Rt,e),o(t,Nl,e),M(Bt,t,e),o(t,Hl,e),M($,t,e),o(t,xl,e),o(t,Ct,e),o(t,Sl,e),M(Vt,t,e),o(t,Al,e),o(t,_t,e),o(t,Ll,e),M(Ft,t,e),o(t,Dl,e),o(t,Nt,e),Kl=!0},p(t,[e]){const g={};e&2&&(g.$$scope={dirty:e,ctx:t}),B.$set(g);const Nn={};e&2&&(Nn.$$scope={dirty:e,ctx:t}),C.$set(Nn);const Hn={};e&2&&(Hn.$$scope={dirty:e,ctx:t}),V.$set(Hn);const xn={};e&2&&(xn.$$scope={dirty:e,ctx:t}),_.$set(xn);const Sn={};e&2&&(Sn.$$scope={dirty:e,ctx:t}),F.$set(Sn);const An={};e&2&&(An.$$scope={dirty:e,ctx:t}),z.$set(An);const Ln={};e&2&&(Ln.$$scope={dirty:e,ctx:t}),k.$set(Ln);const Dn={};e&2&&(Dn.$$scope={dirty:e,ctx:t}),$.$set(Dn)},i(t){Kl||(y(a.$$.fragment,t),y(B.$$.fragment,t),y(C.$$.fragment,t),y(L.$$.fragment,t),y(lt.$$.fragment,t),y(V.$$.fragment,t),y(nt.$$.fragment,t),y(at.$$.fragment,t),y(_.$$.fragment,t),y(et.$$.fragment,t),y(st.$$.fragment,t),y(F.$$.fragment,t),y(ot.$$.fragment,t),y(z.$$.fragment,t),y(it.$$.fragment,t),y(pt.$$.fragment,t),y(ut.$$.fragment,t),y(rt.$$.fragment,t),y(dt.$$.fragment,t),y(mt.$$.fragment,t),y(Jt.$$.fragment,t),y(wt.$$.fragment,t),y(k.$$.fragment,t),y(Ut.$$.fragment,t),y(Wt.$$.fragment,t),y(gt.$$.fragment,t),y(Gt.$$.fragment,t),y(Xt.$$.fragment,t),y(It.$$.fragment,t),y(Bt.$$.fragment,t),y($.$$.fragment,t),y(Vt.$$.fragment,t),y(Ft.$$.fragment,t),Kl=!0)},o(t){h(a.$$.fragment,t),h(B.$$.fragment,t),h(C.$$.fragment,t),h(L.$$.fragment,t),h(lt.$$.fragment,t),h(V.$$.fragment,t),h(nt.$$.fragment,t),h(at.$$.fragment,t),h(_.$$.fragment,t),h(et.$$.fragment,t),h(st.$$.fragment,t),h(F.$$.fragment,t),h(ot.$$.fragment,t),h(z.$$.fragment,t),h(it.$$.fragment,t),h(pt.$$.fragment,t),h(ut.$$.fragment,t),h(rt.$$.fragment,t),h(dt.$$.fragment,t),h(mt.$$.fragment,t),h(Jt.$$.fragment,t),h(wt.$$.fragment,t),h(k.$$.fragment,t),h(Ut.$$.fragment,t),h(Wt.$$.fragment,t),h(gt.$$.fragment,t),h(Gt.$$.fragment,t),h(Xt.$$.fragment,t),h(It.$$.fragment,t),h(Bt.$$.fragment,t),h($.$$.fragment,t),h(Vt.$$.fragment,t),h(Ft.$$.fragment,t),Kl=!1},d(t){t&&(s(i),s(l),s(r),s(p),s(U),s(Ht),s(q),s(xt),s(v),s(St),s(Q),s(At),s(Lt),s(E),s(Dt),s(Y),s(Kt),s(N),s(Pt),s(H),s(Ot),s(x),s(tl),s(S),s(ll),s(nl),s(A),s(al),s(el),s(D),s(sl),s(K),s(ol),s(P),s(il),s(O),s(pl),s(tt),s(cl),s(ul),s(rl),s(dl),s(ml),s(Ml),s(yl),s(hl),s(bl),s(Jl),s(wl),s(Tl),s(fl),s(ct),s(Ul),s(jl),s(Wl),s(Zl),s(gl),s(Mt),s(Gl),s(yt),s(Xl),s(ht),s(Il),s(bt),s(Rl),s(Bl),s(Cl),s(Tt),s(Vl),s(ft),s(_l),s(Fl),s(zl),s(jt),s(kl),s($l),s(Zt),s(ql),s(vl),s(Ql),s(Z),s(El),s(Yl),s(Rt),s(Nl),s(Hl),s(xl),s(Ct),s(Sl),s(Al),s(_t),s(Ll),s(Dl),s(Nt)),s(n),b(a,t),b(B,t),b(C,t),b(L,t),b(lt,t),b(V,t),b(nt,t),b(at,t),b(_,t),b(et,t),b(st,t),b(F,t),b(ot,t),b(z,t),b(it,t),b(pt,t),b(ut,t),b(rt,t),b(dt,t),b(mt,t),b(Jt,t),b(wt,t),b(k,t),b(Ut,t),b(Wt,t),b(gt,t),b(Gt,t),b(Xt),b(It,t),b(Bt,t),b($,t),b(Vt,t),b(Ft,t)}}}const ga='{"title":"torchao","local":"torchao","sections":[{"title":"Quantization examples","local":"quantization-examples","sections":[{"title":"H100 GPU","local":"h100-gpu","sections":[],"depth":3},{"title":"A100 GPU","local":"a100-gpu","sections":[],"depth":3},{"title":"Intel XPU","local":"intel-xpu","sections":[],"depth":3},{"title":"CPU","local":"cpu","sections":[],"depth":3},{"title":"Per Module Quantization","local":"per-module-quantization","sections":[{"title":"1. Skip quantization for certain layers","local":"1-skip-quantization-for-certain-layers","sections":[],"depth":4},{"title":"2. Quantizing different layers with different quantization configs","local":"2-quantizing-different-layers-with-different-quantization-configs","sections":[],"depth":4}],"depth":3},{"title":"Autoquant","local":"autoquant","sections":[],"depth":3}],"depth":2},{"title":"Serialization","local":"serialization","sections":[],"depth":2},{"title":"Loading quantized models","local":"loading-quantized-models","sections":[],"depth":2},{"title":"âš ï¸ Deprecation Notice","local":"-deprecation-notice","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Issues","local":"issues","sections":[],"depth":2}],"depth":1}';function Ga(J){return On(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _a extends ta{constructor(n){super(),la(this,n,Ga,Za,Pn,{})}}export{_a as component};
