import{s as vt,o as yt,n as et}from"../chunks/scheduler.18a86fab.js";import{S as Tt,i as Xt,g as l,s as a,r as h,A as xt,h as c,f as o,c as s,j as se,x as p,u as g,k as G,y as m,a as n,v as b,d as w,t as _,w as M}from"../chunks/index.98837b22.js";import{T as $t}from"../chunks/Tip.77304350.js";import{D as ge}from"../chunks/Docstring.a1ef7999.js";import{C as Le}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Mt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as be,E as Jt}from"../chunks/getInferenceSnippets.06c2775f.js";function Zt(Z){let r,T="Example:",u,d,v;return d=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFhjb2RlY01vZGVsJTJDJTIwWGNvZGVjQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBYY29kZWNDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwWGNvZGVjTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XcodecModel, XcodecConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = XcodecConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XcodecModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=l("p"),r.textContent=T,u=a(),h(d.$$.fragment)},l(i){r=c(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-11lpom8"&&(r.textContent=T),u=s(i),g(d.$$.fragment,i)},m(i,x){n(i,r,x),n(i,u,x),b(d,i,x),v=!0},p:et,i(i){v||(w(d.$$.fragment,i),v=!0)},o(i){_(d.$$.fragment,i),v=!1},d(i){i&&(o(r),o(u)),M(d,i)}}}function Wt(Z){let r,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=l("p"),r.innerHTML=T},l(u){r=c(u,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=T)},m(u,d){n(u,r,d)},p:et,d(u){u&&o(r)}}}function jt(Z){let r,T="Example:",u,d,v;return d=new Le({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwWGNvZGVjTW9kZWwlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMmhmLWF1ZGlvJTJGeGNvZGVjLWh1YmVydC1saWJyaXNwZWVjaCUyMiUwQW1vZGVsJTIwJTNEJTIwWGNvZGVjTW9kZWwuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRGZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUpKSUwQWF1ZGlvX3NhbXBsZSUyMCUzRCUyMGRhdGFzZXQlNUIwJTVEJTVCJ2F1ZGlvJyU1RCU1QidhcnJheSclNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihyYXdfYXVkaW8lM0RhdWRpb19zYW1wbGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFhdWRpb19jb2RlcyUyMCUzRCUyMG91dHB1dHMuYXVkaW9fY29kZXMlMEFhdWRpb192YWx1ZXMlMjAlM0QlMjBvdXRwdXRzLmF1ZGlvX3ZhbHVlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, XcodecModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;hf-audio/xcodec-hubert-librispeech&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XcodecModel.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=feature_extractor.sampling_rate))
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_sample = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;audio&#x27;</span>][<span class="hljs-string">&#x27;array&#x27;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(raw_audio=audio_sample, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_codes = outputs.audio_codes
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = outputs.audio_values`,wrap:!1}}),{c(){r=l("p"),r.textContent=T,u=a(),h(d.$$.fragment)},l(i){r=c(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-11lpom8"&&(r.textContent=T),u=s(i),g(d.$$.fragment,i)},m(i,x){n(i,r,x),n(i,u,x),b(d,i,x),v=!0},p:et,i(i){v||(w(d.$$.fragment,i),v=!0)},o(i){_(d.$$.fragment,i),v=!1},d(i){i&&(o(r),o(u)),M(d,i)}}}function Ct(Z){let r,T,u,d,v,i="<em>This model was released on 2024-08-30 and added to Hugging Face Transformers on 2025-08-15.</em>",x,R,we,W,tt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',_e,V,Me,z,ot='The X-Codec model was proposed in <a href="https://huggingface.co/papers/2408.17175" rel="nofollow">Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model</a> by Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, Yike Guo, Wei Xue.',ve,F,nt="The X-Codec model is a neural audio codec that integrates semantic information from self-supervised models (e.g., HuBERT) alongside traditional acoustic information. This enables:",ye,I,at="<li><strong>Music continuation</strong>: Better modeling of musical semantics yields more coherent continuations.</li> <li><strong>Text-to-Sound Synthesis</strong>: X-Codec captures semantic alignment between text prompts and generated audio.</li> <li><strong>Semantic aware audio tokenization</strong>: X-Codec is used as an audio tokenizer in the YuE lyrics to song generation model.</li>",Te,Y,st="The abstract of the paper states the following:",Xe,L,rt="<em>Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation.</em>",xe,H,it="Model cards:",$e,E,lt='<li><a href="https://huggingface.co/hf-audio/xcodec-hubert-librispeech" rel="nofollow">xcodec-hubert-librispeech</a> (for speech)</li> <li><a href="https://huggingface.co/hf-audio/xcodec-wavlm-mls" rel="nofollow">xcodec-wavlm-mls</a> (for speech)</li> <li><a href="https://huggingface.co/hf-audio/xcodec-wavlm-more-data" rel="nofollow">xcodec-wavlm-more-data</a> (for speech)</li> <li><a href="https://huggingface.co/hf-audio/xcodec-hubert-general" rel="nofollow">xcodec-hubert-general</a> (for general audio)</li> <li><a href="https://huggingface.co/hf-audio/xcodec-hubert-general-balanced" rel="nofollow">xcodec-hubert-general-balanced</a> (for general audio)</li>',Je,Q,ct='This model was contributed by <a href="https://huggingface.co/Manel" rel="nofollow">Manal El Aidouni</a>. The original code can be found <a href="https://github.com/zhenye234/xcodec" rel="nofollow">here</a> and original checkpoints for the five different models <a href="https://github.com/zhenye234/xcodec?tab=readme-ov-file#available-models" rel="nofollow">here</a>.',Ze,N,dt='Demos can be found on this <a href="https://x-codec-audio.github.io/" rel="nofollow">page</a>.',We,B,je,S,mt="Here is a quick example of how to encode and decode an audio using this model:",Ce,q,ke,D,pt="To listen to the original and reconstructed audio, run the snippet below and then open the generated <code>original.wav</code> and <code>reconstruction.wav</code> files in your music player to compare.",Ue,P,Ge,A,Re,X,K,He,re,ut=`This is the configuration class to store the configuration of an <a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecModel">XcodecModel</a>. It is used to instantiate a
Xcodec model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/Manel/X-Codec" rel="nofollow">Manel/X-Codec</a> architecture.`,Ee,ie,ft=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Qe,j,Ve,O,ze,f,ee,Ne,le,ht="The Xcodec neural audio codec model.",Be,ce,gt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Se,de,bt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qe,me,te,De,pe,oe,Pe,$,ne,Ae,ue,wt='The <a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecModel">XcodecModel</a> forward method, overrides the <code>__call__</code> special method.',Ke,C,Oe,k,Fe,ae,Ie,fe,Ye;return R=new be({props:{title:"X-Codec",local:"x-codec",headingTag:"h1"}}),V=new be({props:{title:"Overview",local:"overview",headingTag:"h2"}}),B=new be({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),q=new Le({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwWGNvZGVjTW9kZWwlMkMlMjBBdXRvRmVhdHVyZUV4dHJhY3RvciUwQWR1bW15X2RhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQSUwQSUyMyUyMGxvYWQlMjBtb2RlbCUyMGFuZCUyMGZlYXR1cmUlMjBleHRyYWN0b3IlMEFtb2RlbF9pZCUyMCUzRCUyMCUyMmhmLWF1ZGlvJTJGeGNvZGVjLWh1YmVydC1saWJyaXNwZWVjaCUyMiUwQW1vZGVsJTIwJTNEJTIwWGNvZGVjTW9kZWwuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQSUwQSUyMyUyMGxvYWQlMjBhdWRpbyUyMHNhbXBsZSUwQWR1bW15X2RhdGFzZXQlMjAlM0QlMjBkdW1teV9kYXRhc2V0LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRGZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUpKSUwQWF1ZGlvX3NhbXBsZSUyMCUzRCUyMGR1bW15X2RhdGFzZXQlNUItMSU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihyYXdfYXVkaW8lM0RhdWRpb19zYW1wbGUlMkMlMjBzYW1wbGluZ19yYXRlJTNEZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwZW5jb2RlJTIwYW5kJTIwZGVjb2RlJTBBZW5jb2Rlcl9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZW5jb2RlKGlucHV0cyU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCklMEFkZWNvZGVyX291dHB1dHMlMjAlM0QlMjBtb2RlbC5kZWNvZGUoZW5jb2Rlcl9vdXRwdXRzLmF1ZGlvX2NvZGVzKSUwQWF1ZGlvX3ZhbHVlcyUyMCUzRCUyMGRlY29kZXJfb3V0cHV0cy5hdWRpb192YWx1ZXMlMEElMEElMjMlMjBvciUyMHRoZSUyMGVxdWl2YWxlbnQlMjB3aXRoJTIwYSUyMGZvcndhcmQlMjBwYXNzJTBBYXVkaW9fdmFsdWVzJTIwJTNEJTIwbW9kZWwoaW5wdXRzJTVCJTIyaW5wdXRfdmFsdWVzJTIyJTVEKS5hdWRpb192YWx1ZXMlMEE=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XcodecModel, AutoFeatureExtractor
dummy_dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

<span class="hljs-comment"># load model and feature extractor</span>
model_id = <span class="hljs-string">&quot;hf-audio/xcodec-hubert-librispeech&quot;</span>
model = XcodecModel.from_pretrained(model_id)
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

<span class="hljs-comment"># load audio sample</span>
dummy_dataset = dummy_dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=feature_extractor.sampling_rate))
audio_sample = dummy_dataset[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
inputs = feature_extractor(raw_audio=audio_sample, sampling_rate=feature_extractor.sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># encode and decode</span>
encoder_outputs = model.encode(inputs[<span class="hljs-string">&quot;input_values&quot;</span>])
decoder_outputs = model.decode(encoder_outputs.audio_codes)
audio_values = decoder_outputs.audio_values

<span class="hljs-comment"># or the equivalent with a forward pass</span>
audio_values = model(inputs[<span class="hljs-string">&quot;input_values&quot;</span>]).audio_values
`,wrap:!1}}),P=new Le({props:{code:"aW1wb3J0JTIwc291bmRmaWxlJTIwYXMlMjBzZiUwQSUwQW9yaWdpbmFsJTIwJTNEJTIwYXVkaW9fc2FtcGxlJTBBcmVjb25zdHJ1Y3Rpb24lMjAlM0QlMjBhdWRpb192YWx1ZXMlNUIwJTVELmNwdSgpLmRldGFjaCgpLm51bXB5KCklMEFzYW1wbGluZ19yYXRlJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSUwQSUwQXNmLndyaXRlKCUyMm9yaWdpbmFsLndhdiUyMiUyQyUyMG9yaWdpbmFsJTJDJTIwc2FtcGxpbmdfcmF0ZSklMEFzZi53cml0ZSglMjJyZWNvbnN0cnVjdGlvbi53YXYlMjIlMkMlMjByZWNvbnN0cnVjdGlvbi5UJTJDJTIwc2FtcGxpbmdfcmF0ZSk=",highlighted:`<span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

original = audio_sample
reconstruction = audio_values[<span class="hljs-number">0</span>].cpu().detach().numpy()
sampling_rate = feature_extractor.sampling_rate

sf.write(<span class="hljs-string">&quot;original.wav&quot;</span>, original, sampling_rate)
sf.write(<span class="hljs-string">&quot;reconstruction.wav&quot;</span>, reconstruction.T, sampling_rate)`,wrap:!1}}),A=new be({props:{title:"XcodecConfig",local:"transformers.XcodecConfig",headingTag:"h2"}}),K=new ge({props:{name:"class transformers.XcodecConfig",anchor:"transformers.XcodecConfig",parameters:[{name:"target_bandwidths",val:": typing.Optional[list[float]] = None"},{name:"sample_rate",val:": int = 16000"},{name:"kernel_size",val:": int = 3"},{name:"channel_ratios",val:": list = [1, 1]"},{name:"strides",val:": list = [1, 1]"},{name:"block_dilations",val:": list = [1, 1]"},{name:"unit_kernel_size",val:": int = 3"},{name:"codebook_size",val:": int = 1024"},{name:"codebook_dim",val:": typing.Optional[int] = None"},{name:"initializer_range",val:": float = 0.02"},{name:"acoustic_model_config",val:": typing.Union[dict, transformers.models.dac.configuration_dac.DacConfig] = None"},{name:"semantic_model_config",val:": typing.Union[dict, transformers.models.hubert.configuration_hubert.HubertConfig] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.XcodecConfig.target_bandwidths",description:`<strong>target_bandwidths</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.5, 1, 1.5, 2, 4]</code>) &#x2014;
The range of different bandwidths (in kbps) the model can encode audio with.`,name:"target_bandwidths"},{anchor:"transformers.XcodecConfig.sample_rate",description:`<strong>sample_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 16000) &#x2014;
The sampling rate at which the audio waveform should be digitalized, in hertz (Hz).`,name:"sample_rate"},{anchor:"transformers.XcodecConfig.kernel_size",description:`<strong>kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size for the initial semantic convolution.`,name:"kernel_size"},{anchor:"transformers.XcodecConfig.channel_ratios",description:`<strong>channel_ratios</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[1, 1]</code>) &#x2014;
Expansion factors for the number of output channels in each semantic block.`,name:"channel_ratios"},{anchor:"transformers.XcodecConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 1]</code>) &#x2014;
Strides for each semantic encoder block.`,name:"strides"},{anchor:"transformers.XcodecConfig.block_dilations",description:`<strong>block_dilations</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 1]</code>) &#x2014;
Dilation factors for the residual units in semantic blocks.`,name:"block_dilations"},{anchor:"transformers.XcodecConfig.unit_kernel_size",description:`<strong>unit_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size inside each ResidualUnit in semantic blocks.`,name:"unit_kernel_size"},{anchor:"transformers.XcodecConfig.codebook_size",description:`<strong>codebook_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Number of entries in each residual quantizer&#x2019;s codebook.`,name:"codebook_size"},{anchor:"transformers.XcodecConfig.codebook_dim",description:`<strong>codebook_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dimensionality of each codebook vector. Defaults to sum of hidden size of acoustic and semantic models.`,name:"codebook_dim"},{anchor:"transformers.XcodecConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
Standard deviation of the truncated normal initializer for all weight matrices.`,name:"initializer_range"},{anchor:"transformers.XcodecConfig.acoustic_model_config",description:`<strong>acoustic_model_config</strong> (<code>Union[Dict, DacConfig]</code>, <em>optional</em>) &#x2014;
An instance of the configuration for the acoustic (DAC) model.`,name:"acoustic_model_config"},{anchor:"transformers.XcodecConfig.semantic_model_config",description:`<strong>semantic_model_config</strong> (<code>Union[Dict, HubertConfig, WavLMConfig]</code>, <em>optional</em>) &#x2014;
An instance of the configuration object for the semantic (HuBERT) model.`,name:"semantic_model_config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/xcodec/configuration_xcodec.py#L31"}}),j=new Mt({props:{anchor:"transformers.XcodecConfig.example",$$slots:{default:[Zt]},$$scope:{ctx:Z}}}),O=new be({props:{title:"XcodecModel",local:"transformers.XcodecModel",headingTag:"h2"}}),ee=new ge({props:{name:"class transformers.XcodecModel",anchor:"transformers.XcodecModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.XcodecModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecModel">XcodecModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/xcodec/modeling_xcodec.py#L382"}}),te=new ge({props:{name:"decode",anchor:"transformers.XcodecModel.decode",parameters:[{name:"audio_codes",val:": Tensor"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.XcodecModel.decode.audio_codes",description:`<strong>audio_codes</strong> (<code>torch.LongTensor</code>  of shape <code>(batch_size, num_quantizers, codes_length)</code>) &#x2014;
Discrete code indices computed using <code>model.encode</code>.`,name:"audio_codes"},{anchor:"transformers.XcodecModel.decode.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a>`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/xcodec/modeling_xcodec.py#L473",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Decoded audio values of shape <code>(batch_size, channels, num_samples)</code> obtained using the decoder part of
Xcodec.</p>
`}}),oe=new ge({props:{name:"encode",anchor:"transformers.XcodecModel.encode",parameters:[{name:"input_values",val:": Tensor"},{name:"bandwidth",val:": typing.Optional[float] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.XcodecModel.encode.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, channels, num_samples)</code>) &#x2014;
Float values of the input audio waveform.`,name:"input_values"},{anchor:"transformers.XcodecModel.encode.bandwidth",description:`<strong>bandwidth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The target bandwidth in (kbps) supports only values in <code>config.target_bandwidths</code>.
Defaults to the highest available bandwidth <code>4.0</code> kbps.`,name:"bandwidth"},{anchor:"transformers.XcodecModel.encode.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a>.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/xcodec/modeling_xcodec.py#L423",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.LongTensor</code> of shape <code>(batch_size, num_quantizers, codes_length)</code> containing the discrete encoded audio codes.</p>
`}}),ne=new ge({props:{name:"forward",anchor:"transformers.XcodecModel.forward",parameters:[{name:"input_values",val:": Tensor"},{name:"audio_codes",val:": typing.Optional[torch.Tensor] = None"},{name:"bandwidth",val:": typing.Optional[float] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.XcodecModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, channels, num_samples)</code>) &#x2014;
The raw float values of the input audio waveform.`,name:"input_values"},{anchor:"transformers.XcodecModel.forward.audio_codes",description:`<strong>audio_codes</strong> (<code>torch.LongTensor</code>  of shape <code>(batch_size, num_quantizers, codes_length)</code> &#x2014;
Discrete code indices computed using <code>model.encode</code>.`,name:"audio_codes"},{anchor:"transformers.XcodecModel.forward.bandwidth",description:`<strong>bandwidth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Target bandwidth in kbps. Must be one of <code>config.target_bandwidths</code>. Defaults to the highest available bandwidth.`,name:"bandwidth"},{anchor:"transformers.XcodecModel.forward.bandwidth",description:`<strong>bandwidth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Target bandwidth in kbps. Must be one of <code>config.target_bandwidths</code>. Defaults to the highest available bandwidth.`,name:"bandwidth"},{anchor:"transformers.XcodecModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return a <code>XcodecOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/xcodec/modeling_xcodec.py#L501",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><code>audio_codes</code> of shape <code>(batch_size, num_quantizers, codes_length)</code>: the quantized discrete codes.</li>
<li><code>audio_values</code> of shape <code>(batch_size, channels, num_samples)</code>: the reconstructed audio waveform given the codes.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>XcodecOutput</code> or tuple <code>(audio_codes, audio_values)</code></p>
`}}),C=new $t({props:{$$slots:{default:[Wt]},$$scope:{ctx:Z}}}),k=new Mt({props:{anchor:"transformers.XcodecModel.forward.example",$$slots:{default:[jt]},$$scope:{ctx:Z}}}),ae=new Jt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/xcodec.md"}}),{c(){r=l("meta"),T=a(),u=l("p"),d=a(),v=l("p"),v.innerHTML=i,x=a(),h(R.$$.fragment),we=a(),W=l("div"),W.innerHTML=tt,_e=a(),h(V.$$.fragment),Me=a(),z=l("p"),z.innerHTML=ot,ve=a(),F=l("p"),F.textContent=nt,ye=a(),I=l("ul"),I.innerHTML=at,Te=a(),Y=l("p"),Y.textContent=st,Xe=a(),L=l("p"),L.innerHTML=rt,xe=a(),H=l("p"),H.textContent=it,$e=a(),E=l("ul"),E.innerHTML=lt,Je=a(),Q=l("p"),Q.innerHTML=ct,Ze=a(),N=l("p"),N.innerHTML=dt,We=a(),h(B.$$.fragment),je=a(),S=l("p"),S.textContent=mt,Ce=a(),h(q.$$.fragment),ke=a(),D=l("p"),D.innerHTML=pt,Ue=a(),h(P.$$.fragment),Ge=a(),h(A.$$.fragment),Re=a(),X=l("div"),h(K.$$.fragment),He=a(),re=l("p"),re.innerHTML=ut,Ee=a(),ie=l("p"),ie.innerHTML=ft,Qe=a(),h(j.$$.fragment),Ve=a(),h(O.$$.fragment),ze=a(),f=l("div"),h(ee.$$.fragment),Ne=a(),le=l("p"),le.textContent=ht,Be=a(),ce=l("p"),ce.innerHTML=gt,Se=a(),de=l("p"),de.innerHTML=bt,qe=a(),me=l("div"),h(te.$$.fragment),De=a(),pe=l("div"),h(oe.$$.fragment),Pe=a(),$=l("div"),h(ne.$$.fragment),Ae=a(),ue=l("p"),ue.innerHTML=wt,Ke=a(),h(C.$$.fragment),Oe=a(),h(k.$$.fragment),Fe=a(),h(ae.$$.fragment),Ie=a(),fe=l("p"),this.h()},l(e){const t=xt("svelte-u9bgzb",document.head);r=c(t,"META",{name:!0,content:!0}),t.forEach(o),T=s(e),u=c(e,"P",{}),se(u).forEach(o),d=s(e),v=c(e,"P",{"data-svelte-h":!0}),p(v)!=="svelte-1mp8dqe"&&(v.innerHTML=i),x=s(e),g(R.$$.fragment,e),we=s(e),W=c(e,"DIV",{class:!0,"data-svelte-h":!0}),p(W)!=="svelte-13t8s2t"&&(W.innerHTML=tt),_e=s(e),g(V.$$.fragment,e),Me=s(e),z=c(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-16c1xa7"&&(z.innerHTML=ot),ve=s(e),F=c(e,"P",{"data-svelte-h":!0}),p(F)!=="svelte-b5h3h2"&&(F.textContent=nt),ye=s(e),I=c(e,"UL",{"data-svelte-h":!0}),p(I)!=="svelte-ic0kvt"&&(I.innerHTML=at),Te=s(e),Y=c(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-1pvthah"&&(Y.textContent=st),Xe=s(e),L=c(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-1awxpqa"&&(L.innerHTML=rt),xe=s(e),H=c(e,"P",{"data-svelte-h":!0}),p(H)!=="svelte-1qqx5k0"&&(H.textContent=it),$e=s(e),E=c(e,"UL",{"data-svelte-h":!0}),p(E)!=="svelte-6ip5mu"&&(E.innerHTML=lt),Je=s(e),Q=c(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-bdlk8q"&&(Q.innerHTML=ct),Ze=s(e),N=c(e,"P",{"data-svelte-h":!0}),p(N)!=="svelte-1l5mzep"&&(N.innerHTML=dt),We=s(e),g(B.$$.fragment,e),je=s(e),S=c(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-b4kgu7"&&(S.textContent=mt),Ce=s(e),g(q.$$.fragment,e),ke=s(e),D=c(e,"P",{"data-svelte-h":!0}),p(D)!=="svelte-1uztmmu"&&(D.innerHTML=pt),Ue=s(e),g(P.$$.fragment,e),Ge=s(e),g(A.$$.fragment,e),Re=s(e),X=c(e,"DIV",{class:!0});var J=se(X);g(K.$$.fragment,J),He=s(J),re=c(J,"P",{"data-svelte-h":!0}),p(re)!=="svelte-1xz3afp"&&(re.innerHTML=ut),Ee=s(J),ie=c(J,"P",{"data-svelte-h":!0}),p(ie)!=="svelte-1ek1ss9"&&(ie.innerHTML=ft),Qe=s(J),g(j.$$.fragment,J),J.forEach(o),Ve=s(e),g(O.$$.fragment,e),ze=s(e),f=c(e,"DIV",{class:!0});var y=se(f);g(ee.$$.fragment,y),Ne=s(y),le=c(y,"P",{"data-svelte-h":!0}),p(le)!=="svelte-1em9lch"&&(le.textContent=ht),Be=s(y),ce=c(y,"P",{"data-svelte-h":!0}),p(ce)!=="svelte-q52n56"&&(ce.innerHTML=gt),Se=s(y),de=c(y,"P",{"data-svelte-h":!0}),p(de)!=="svelte-hswkmf"&&(de.innerHTML=bt),qe=s(y),me=c(y,"DIV",{class:!0});var he=se(me);g(te.$$.fragment,he),he.forEach(o),De=s(y),pe=c(y,"DIV",{class:!0});var _t=se(pe);g(oe.$$.fragment,_t),_t.forEach(o),Pe=s(y),$=c(y,"DIV",{class:!0});var U=se($);g(ne.$$.fragment,U),Ae=s(U),ue=c(U,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-11789ax"&&(ue.innerHTML=wt),Ke=s(U),g(C.$$.fragment,U),Oe=s(U),g(k.$$.fragment,U),U.forEach(o),y.forEach(o),Fe=s(e),g(ae.$$.fragment,e),Ie=s(e),fe=c(e,"P",{}),se(fe).forEach(o),this.h()},h(){G(r,"name","hf:doc:metadata"),G(r,"content",kt),G(W,"class","flex flex-wrap space-x-1"),G(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){m(document.head,r),n(e,T,t),n(e,u,t),n(e,d,t),n(e,v,t),n(e,x,t),b(R,e,t),n(e,we,t),n(e,W,t),n(e,_e,t),b(V,e,t),n(e,Me,t),n(e,z,t),n(e,ve,t),n(e,F,t),n(e,ye,t),n(e,I,t),n(e,Te,t),n(e,Y,t),n(e,Xe,t),n(e,L,t),n(e,xe,t),n(e,H,t),n(e,$e,t),n(e,E,t),n(e,Je,t),n(e,Q,t),n(e,Ze,t),n(e,N,t),n(e,We,t),b(B,e,t),n(e,je,t),n(e,S,t),n(e,Ce,t),b(q,e,t),n(e,ke,t),n(e,D,t),n(e,Ue,t),b(P,e,t),n(e,Ge,t),b(A,e,t),n(e,Re,t),n(e,X,t),b(K,X,null),m(X,He),m(X,re),m(X,Ee),m(X,ie),m(X,Qe),b(j,X,null),n(e,Ve,t),b(O,e,t),n(e,ze,t),n(e,f,t),b(ee,f,null),m(f,Ne),m(f,le),m(f,Be),m(f,ce),m(f,Se),m(f,de),m(f,qe),m(f,me),b(te,me,null),m(f,De),m(f,pe),b(oe,pe,null),m(f,Pe),m(f,$),b(ne,$,null),m($,Ae),m($,ue),m($,Ke),b(C,$,null),m($,Oe),b(k,$,null),n(e,Fe,t),b(ae,e,t),n(e,Ie,t),n(e,fe,t),Ye=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),j.$set(J);const y={};t&2&&(y.$$scope={dirty:t,ctx:e}),C.$set(y);const he={};t&2&&(he.$$scope={dirty:t,ctx:e}),k.$set(he)},i(e){Ye||(w(R.$$.fragment,e),w(V.$$.fragment,e),w(B.$$.fragment,e),w(q.$$.fragment,e),w(P.$$.fragment,e),w(A.$$.fragment,e),w(K.$$.fragment,e),w(j.$$.fragment,e),w(O.$$.fragment,e),w(ee.$$.fragment,e),w(te.$$.fragment,e),w(oe.$$.fragment,e),w(ne.$$.fragment,e),w(C.$$.fragment,e),w(k.$$.fragment,e),w(ae.$$.fragment,e),Ye=!0)},o(e){_(R.$$.fragment,e),_(V.$$.fragment,e),_(B.$$.fragment,e),_(q.$$.fragment,e),_(P.$$.fragment,e),_(A.$$.fragment,e),_(K.$$.fragment,e),_(j.$$.fragment,e),_(O.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(C.$$.fragment,e),_(k.$$.fragment,e),_(ae.$$.fragment,e),Ye=!1},d(e){e&&(o(T),o(u),o(d),o(v),o(x),o(we),o(W),o(_e),o(Me),o(z),o(ve),o(F),o(ye),o(I),o(Te),o(Y),o(Xe),o(L),o(xe),o(H),o($e),o(E),o(Je),o(Q),o(Ze),o(N),o(We),o(je),o(S),o(Ce),o(ke),o(D),o(Ue),o(Ge),o(Re),o(X),o(Ve),o(ze),o(f),o(Fe),o(Ie),o(fe)),o(r),M(R,e),M(V,e),M(B,e),M(q,e),M(P,e),M(A,e),M(K),M(j),M(O,e),M(ee),M(te),M(oe),M(ne),M(C),M(k),M(ae,e)}}}const kt='{"title":"X-Codec","local":"x-codec","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"XcodecConfig","local":"transformers.XcodecConfig","sections":[],"depth":2},{"title":"XcodecModel","local":"transformers.XcodecModel","sections":[],"depth":2}],"depth":1}';function Ut(Z){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lt extends Tt{constructor(r){super(),Xt(this,r,Ut,Ct,vt,{})}}export{Lt as component};
