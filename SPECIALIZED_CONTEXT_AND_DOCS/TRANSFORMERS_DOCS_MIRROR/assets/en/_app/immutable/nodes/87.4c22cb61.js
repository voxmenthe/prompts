import{s as kt,o as _t,n as je}from"../chunks/scheduler.18a86fab.js";import{S as bt,i as Tt,g as d,s as a,r as h,A as zt,h as c,f as s,c as i,j as U,x as z,u,k as I,l as wt,y as r,a as m,v as g,d as k,t as _,w as b}from"../chunks/index.98837b22.js";import{T as $t}from"../chunks/Tip.77304350.js";import{D as A}from"../chunks/Docstring.a1ef7999.js";import{C as tt}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as Ke,E as yt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as vt,a as et}from"../chunks/HfOption.6641485e.js";function Mt(M){let t,p=`This model was contributed by <a href="https://huggingface.co/moussakam" rel="nofollow">moussakam</a>.
Refer to the <a href="./bart">BART</a> docs for more usage examples.`;return{c(){t=d("p"),t.innerHTML=p},l(n){t=c(n,"P",{"data-svelte-h":!0}),z(t)!=="svelte-dxa1qq"&&(t.innerHTML=p)},m(n,w){m(n,t,w)},p:je,d(n){n&&s(t)}}}function qt(M){let t,p;return t=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJmaWxsLW1hc2slMjIlMkMlMEElMjAlMjAlMjAlMjBtb2RlbCUzRCUyMm1vdXNzYUthbSUyRmJhcnRoZXolMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2UlM0QwJTBBKSUwQXBpcGVsaW5lKCUyMkxlcyUyMHBsYW50ZXMlMjBwcm9kdWlzZW50JTIwJTNDbWFzayUzRSUyMGdyJUMzJUEyY2UlMjAlQzMlQTAlMjB1biUyMHByb2Nlc3N1cyUyMGFwcGVsJUMzJUE5JTIwcGhvdG9zeW50aCVDMyVBOHNlLiUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;fill-mask&quot;</span>,
    model=<span class="hljs-string">&quot;moussaKam/barthez&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;Les plantes produisent &lt;mask&gt; grâce à un processus appelé photosynthèse.&quot;</span>)`,wrap:!1}}),{c(){h(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,w){g(t,n,w),p=!0},p:je,i(n){p||(k(t.$$.fragment,n),p=!0)},o(n){_(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function Bt(M){let t,p;return t=new tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yTWFza2VkTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIybW91c3NhS2FtJTJGYmFydGhleiUyMiUyQyUwQSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtb3Vzc2FLYW0lMkZiYXJ0aGV6JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEEpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkxlcyUyMHBsYW50ZXMlMjBwcm9kdWlzZW50JTIwJTNDbWFzayUzRSUyMGdyJUMzJUEyY2UlMjAlQzMlQTAlMjB1biUyMHByb2Nlc3N1cyUyMGFwcGVsJUMzJUE5JTIwcGhvdG9zeW50aCVDMyVBOHNlLiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMEFtYXNrZWRfaW5kZXglMjAlM0QlMjB0b3JjaC53aGVyZShpbnB1dHMlNUInaW5wdXRfaWRzJyU1RCUyMCUzRCUzRCUyMHRva2VuaXplci5tYXNrX3Rva2VuX2lkKSU1QjElNUQlMEFwcmVkaWN0ZWRfdG9rZW5faWQlMjAlM0QlMjBwcmVkaWN0aW9ucyU1QjAlMkMlMjBtYXNrZWRfaW5kZXglNUQuYXJnbWF4KGRpbSUzRC0xKSUwQXByZWRpY3RlZF90b2tlbiUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUocHJlZGljdGVkX3Rva2VuX2lkKSUwQSUwQXByaW50KGYlMjJUaGUlMjBwcmVkaWN0ZWQlMjB0b2tlbiUyMGlzJTNBJTIwJTdCcHJlZGljdGVkX3Rva2VuJTdEJTIyKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    <span class="hljs-string">&quot;moussaKam/barthez&quot;</span>,
)
model = AutoModelForMaskedLM.from_pretrained(
    <span class="hljs-string">&quot;moussaKam/barthez&quot;</span>,
    dtype=torch.float16,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
)
inputs = tokenizer(<span class="hljs-string">&quot;Les plantes produisent &lt;mask&gt; grâce à un processus appelé photosynthèse.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>] == tokenizer.mask_token_id)[<span class="hljs-number">1</span>]
predicted_token_id = predictions[<span class="hljs-number">0</span>, masked_index].argmax(dim=-<span class="hljs-number">1</span>)
predicted_token = tokenizer.decode(predicted_token_id)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted token is: <span class="hljs-subst">{predicted_token}</span>&quot;</span>)`,wrap:!1}}),{c(){h(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,w){g(t,n,w),p=!0},p:je,i(n){p||(k(t.$$.fragment,n),p=!0)},o(n){_(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function xt(M){let t,p;return t=new tt({props:{code:"ZWNobyUyMC1lJTIwJTIyTGVzJTIwcGxhbnRlcyUyMHByb2R1aXNlbnQlMjAlM0NtYXNrJTNFJTIwZ3IlQzMlQTJjZSUyMCVDMyVBMCUyMHVuJTIwcHJvY2Vzc3VzJTIwYXBwZWwlQzMlQTklMjBwaG90b3N5bnRoJUMzJUE4c2UuJTIyJTIwJTdDJTIwdHJhbnNmb3JtZXJzJTIwcnVuJTIwLS10YXNrJTIwZmlsbC1tYXNrJTIwLS1tb2RlbCUyMG1vdXNzYUthbSUyRmJhcnRoZXolMjAtLWRldmljZSUyMDA=",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Les plantes produisent &lt;mask&gt; grâce à un processus appelé photosynthèse.&quot;</span> | transformers run --task fill-mask --model moussaKam/barthez --device 0',wrap:!1}}),{c(){h(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,w){g(t,n,w),p=!0},p:je,i(n){p||(k(t.$$.fragment,n),p=!0)},o(n){_(t.$$.fragment,n),p=!1},d(n){b(t,n)}}}function Jt(M){let t,p,n,w,y,Z;return t=new et({props:{id:"usage",option:"Pipeline",$$slots:{default:[qt]},$$scope:{ctx:M}}}),n=new et({props:{id:"usage",option:"AutoModel",$$slots:{default:[Bt]},$$scope:{ctx:M}}}),y=new et({props:{id:"usage",option:"transformers CLI",$$slots:{default:[xt]},$$scope:{ctx:M}}}),{c(){h(t.$$.fragment),p=a(),h(n.$$.fragment),w=a(),h(y.$$.fragment)},l(l){u(t.$$.fragment,l),p=i(l),u(n.$$.fragment,l),w=i(l),u(y.$$.fragment,l)},m(l,f){g(t,l,f),m(l,p,f),g(n,l,f),m(l,w,f),g(y,l,f),Z=!0},p(l,f){const ge={};f&2&&(ge.$$scope={dirty:f,ctx:l}),t.$set(ge);const V={};f&2&&(V.$$scope={dirty:f,ctx:l}),n.$set(V);const q={};f&2&&(q.$$scope={dirty:f,ctx:l}),y.$set(q)},i(l){Z||(k(t.$$.fragment,l),k(n.$$.fragment,l),k(y.$$.fragment,l),Z=!0)},o(l){_(t.$$.fragment,l),_(n.$$.fragment,l),_(y.$$.fragment,l),Z=!1},d(l){l&&(s(p),s(w)),b(t,l),b(n,l),b(y,l)}}}function Ut(M){let t,p,n,w,y,Z="<em>This model was released on 2020-10-23 and added to Hugging Face Transformers on 2020-11-27.</em>",l,f,ge='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',V,q,_e,N,nt='<a href="https://huggingface.co/papers/2010.12321" rel="nofollow">BARThez</a> is a <a href="./bart">BART</a> model designed for French language tasks. Unlike existing French BERT models, BARThez includes a pretrained encoder-decoder, allowing it to generate text as well. This model is also available as a multilingual variant, mBARThez, by continuing pretraining multilingual BART on a French corpus.',be,D,st='You can find all of the original BARThez checkpoints under the <a href="https://huggingface.co/collections/dascim/barthez-670920b569a07aa53e3b6887" rel="nofollow">BARThez</a> collection.',Te,j,ze,F,ot='The example below demonstrates how to predict the <code>&lt;mask&gt;</code> token with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',we,R,$e,G,ye,T,X,Re,se,rt=`Adapted from <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertTokenizer">CamembertTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartTokenizer">BartTokenizer</a>. Construct a BARThez tokenizer. Based on
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.`,Le,oe,at=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,He,B,P,Ee,re,it=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARThez sequence has the following format:`,We,ae,lt="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Ae,L,S,Ze,ie,dt="Converts a sequence of tokens (string) in a single string.",Ve,H,Q,Ne,le,ct="Create a mask from the two sequences passed to be used in a sequence-pair classification task.",De,E,Y,Fe,de,pt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,ve,O,Me,v,K,Ge,ce,mt=`Adapted from <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertTokenizer">CamembertTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartTokenizer">BartTokenizer</a>. Construct a “fast” BARThez tokenizer. Based on
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.`,Xe,pe,ft=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Pe,x,ee,Se,me,ht=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BARThez sequence has the following format:`,Qe,fe,ut="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Ye,W,te,Oe,he,gt="Create a mask from the two sequences passed to be used in a sequence-pair classification task.",qe,ne,Be,ke,xe;return q=new Ke({props:{title:"BARThez",local:"barthez",headingTag:"h1"}}),j=new $t({props:{warning:!1,$$slots:{default:[Mt]},$$scope:{ctx:M}}}),R=new vt({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Jt]},$$scope:{ctx:M}}}),G=new Ke({props:{title:"BarthezTokenizer",local:"transformers.BarthezTokenizer",headingTag:"h2"}}),X=new A({props:{name:"class transformers.BarthezTokenizer",anchor:"transformers.BarthezTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sp_model_kwargs",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarthezTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.BarthezTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BarthezTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BarthezTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BarthezTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BarthezTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BarthezTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BarthezTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BarthezTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.BarthezTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez.py#L39"}}),P=new A({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BarthezTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BarthezTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BarthezTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez.py#L143",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),S=new A({props:{name:"convert_tokens_to_string",anchor:"transformers.BarthezTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez.py#L239"}}),Q=new A({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BarthezTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BarthezTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BarthezTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez.py#L196",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Y=new A({props:{name:"get_special_tokens_mask",anchor:"transformers.BarthezTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BarthezTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BarthezTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BarthezTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez.py#L169",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),O=new Ke({props:{title:"BarthezTokenizerFast",local:"transformers.BarthezTokenizerFast",headingTag:"h2"}}),K=new A({props:{name:"class transformers.BarthezTokenizerFast",anchor:"transformers.BarthezTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarthezTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.BarthezTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BarthezTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BarthezTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BarthezTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BarthezTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BarthezTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BarthezTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.BarthezTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L39"}}),ee=new A({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BarthezTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BarthezTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BarthezTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L125",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),te=new A({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/barthez/tokenization_barthez_fast.py#L151",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),ne=new yt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/barthez.md"}}),{c(){t=d("meta"),p=a(),n=d("p"),w=a(),y=d("p"),y.innerHTML=Z,l=a(),f=d("div"),f.innerHTML=ge,V=a(),h(q.$$.fragment),_e=a(),N=d("p"),N.innerHTML=nt,be=a(),D=d("p"),D.innerHTML=st,Te=a(),h(j.$$.fragment),ze=a(),F=d("p"),F.innerHTML=ot,we=a(),h(R.$$.fragment),$e=a(),h(G.$$.fragment),ye=a(),T=d("div"),h(X.$$.fragment),Re=a(),se=d("p"),se.innerHTML=rt,Le=a(),oe=d("p"),oe.innerHTML=at,He=a(),B=d("div"),h(P.$$.fragment),Ee=a(),re=d("p"),re.textContent=it,We=a(),ae=d("ul"),ae.innerHTML=lt,Ae=a(),L=d("div"),h(S.$$.fragment),Ze=a(),ie=d("p"),ie.textContent=dt,Ve=a(),H=d("div"),h(Q.$$.fragment),Ne=a(),le=d("p"),le.textContent=ct,De=a(),E=d("div"),h(Y.$$.fragment),Fe=a(),de=d("p"),de.innerHTML=pt,ve=a(),h(O.$$.fragment),Me=a(),v=d("div"),h(K.$$.fragment),Ge=a(),ce=d("p"),ce.innerHTML=mt,Xe=a(),pe=d("p"),pe.innerHTML=ft,Pe=a(),x=d("div"),h(ee.$$.fragment),Se=a(),me=d("p"),me.textContent=ht,Qe=a(),fe=d("ul"),fe.innerHTML=ut,Ye=a(),W=d("div"),h(te.$$.fragment),Oe=a(),he=d("p"),he.textContent=gt,qe=a(),h(ne.$$.fragment),Be=a(),ke=d("p"),this.h()},l(e){const o=zt("svelte-u9bgzb",document.head);t=c(o,"META",{name:!0,content:!0}),o.forEach(s),p=i(e),n=c(e,"P",{}),U(n).forEach(s),w=i(e),y=c(e,"P",{"data-svelte-h":!0}),z(y)!=="svelte-9s6fo1"&&(y.innerHTML=Z),l=i(e),f=c(e,"DIV",{style:!0,"data-svelte-h":!0}),z(f)!=="svelte-wa5t4p"&&(f.innerHTML=ge),V=i(e),u(q.$$.fragment,e),_e=i(e),N=c(e,"P",{"data-svelte-h":!0}),z(N)!=="svelte-ol1ygk"&&(N.innerHTML=nt),be=i(e),D=c(e,"P",{"data-svelte-h":!0}),z(D)!=="svelte-sdbkch"&&(D.innerHTML=st),Te=i(e),u(j.$$.fragment,e),ze=i(e),F=c(e,"P",{"data-svelte-h":!0}),z(F)!=="svelte-10lshn2"&&(F.innerHTML=ot),we=i(e),u(R.$$.fragment,e),$e=i(e),u(G.$$.fragment,e),ye=i(e),T=c(e,"DIV",{class:!0});var $=U(T);u(X.$$.fragment,$),Re=i($),se=c($,"P",{"data-svelte-h":!0}),z(se)!=="svelte-d69zxd"&&(se.innerHTML=rt),Le=i($),oe=c($,"P",{"data-svelte-h":!0}),z(oe)!=="svelte-ntrhio"&&(oe.innerHTML=at),He=i($),B=c($,"DIV",{class:!0});var C=U(B);u(P.$$.fragment,C),Ee=i(C),re=c(C,"P",{"data-svelte-h":!0}),z(re)!=="svelte-js8h2n"&&(re.textContent=it),We=i(C),ae=c(C,"UL",{"data-svelte-h":!0}),z(ae)!=="svelte-rq8uot"&&(ae.innerHTML=lt),C.forEach(s),Ae=i($),L=c($,"DIV",{class:!0});var Je=U(L);u(S.$$.fragment,Je),Ze=i(Je),ie=c(Je,"P",{"data-svelte-h":!0}),z(ie)!=="svelte-b3k2yi"&&(ie.textContent=dt),Je.forEach(s),Ve=i($),H=c($,"DIV",{class:!0});var Ue=U(H);u(Q.$$.fragment,Ue),Ne=i(Ue),le=c(Ue,"P",{"data-svelte-h":!0}),z(le)!=="svelte-sqjw56"&&(le.textContent=ct),Ue.forEach(s),De=i($),E=c($,"DIV",{class:!0});var Ie=U(E);u(Y.$$.fragment,Ie),Fe=i(Ie),de=c(Ie,"P",{"data-svelte-h":!0}),z(de)!=="svelte-1f4f5kp"&&(de.innerHTML=pt),Ie.forEach(s),$.forEach(s),ve=i(e),u(O.$$.fragment,e),Me=i(e),v=c(e,"DIV",{class:!0});var J=U(v);u(K.$$.fragment,J),Ge=i(J),ce=c(J,"P",{"data-svelte-h":!0}),z(ce)!=="svelte-15k672g"&&(ce.innerHTML=mt),Xe=i(J),pe=c(J,"P",{"data-svelte-h":!0}),z(pe)!=="svelte-gxzj9w"&&(pe.innerHTML=ft),Pe=i(J),x=c(J,"DIV",{class:!0});var ue=U(x);u(ee.$$.fragment,ue),Se=i(ue),me=c(ue,"P",{"data-svelte-h":!0}),z(me)!=="svelte-js8h2n"&&(me.textContent=ht),Qe=i(ue),fe=c(ue,"UL",{"data-svelte-h":!0}),z(fe)!=="svelte-rq8uot"&&(fe.innerHTML=ut),ue.forEach(s),Ye=i(J),W=c(J,"DIV",{class:!0});var Ce=U(W);u(te.$$.fragment,Ce),Oe=i(Ce),he=c(Ce,"P",{"data-svelte-h":!0}),z(he)!=="svelte-sqjw56"&&(he.textContent=gt),Ce.forEach(s),J.forEach(s),qe=i(e),u(ne.$$.fragment,e),Be=i(e),ke=c(e,"P",{}),U(ke).forEach(s),this.h()},h(){I(t,"name","hf:doc:metadata"),I(t,"content",It),wt(f,"float","right"),I(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){r(document.head,t),m(e,p,o),m(e,n,o),m(e,w,o),m(e,y,o),m(e,l,o),m(e,f,o),m(e,V,o),g(q,e,o),m(e,_e,o),m(e,N,o),m(e,be,o),m(e,D,o),m(e,Te,o),g(j,e,o),m(e,ze,o),m(e,F,o),m(e,we,o),g(R,e,o),m(e,$e,o),g(G,e,o),m(e,ye,o),m(e,T,o),g(X,T,null),r(T,Re),r(T,se),r(T,Le),r(T,oe),r(T,He),r(T,B),g(P,B,null),r(B,Ee),r(B,re),r(B,We),r(B,ae),r(T,Ae),r(T,L),g(S,L,null),r(L,Ze),r(L,ie),r(T,Ve),r(T,H),g(Q,H,null),r(H,Ne),r(H,le),r(T,De),r(T,E),g(Y,E,null),r(E,Fe),r(E,de),m(e,ve,o),g(O,e,o),m(e,Me,o),m(e,v,o),g(K,v,null),r(v,Ge),r(v,ce),r(v,Xe),r(v,pe),r(v,Pe),r(v,x),g(ee,x,null),r(x,Se),r(x,me),r(x,Qe),r(x,fe),r(v,Ye),r(v,W),g(te,W,null),r(W,Oe),r(W,he),m(e,qe,o),g(ne,e,o),m(e,Be,o),m(e,ke,o),xe=!0},p(e,[o]){const $={};o&2&&($.$$scope={dirty:o,ctx:e}),j.$set($);const C={};o&2&&(C.$$scope={dirty:o,ctx:e}),R.$set(C)},i(e){xe||(k(q.$$.fragment,e),k(j.$$.fragment,e),k(R.$$.fragment,e),k(G.$$.fragment,e),k(X.$$.fragment,e),k(P.$$.fragment,e),k(S.$$.fragment,e),k(Q.$$.fragment,e),k(Y.$$.fragment,e),k(O.$$.fragment,e),k(K.$$.fragment,e),k(ee.$$.fragment,e),k(te.$$.fragment,e),k(ne.$$.fragment,e),xe=!0)},o(e){_(q.$$.fragment,e),_(j.$$.fragment,e),_(R.$$.fragment,e),_(G.$$.fragment,e),_(X.$$.fragment,e),_(P.$$.fragment,e),_(S.$$.fragment,e),_(Q.$$.fragment,e),_(Y.$$.fragment,e),_(O.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),xe=!1},d(e){e&&(s(p),s(n),s(w),s(y),s(l),s(f),s(V),s(_e),s(N),s(be),s(D),s(Te),s(ze),s(F),s(we),s($e),s(ye),s(T),s(ve),s(Me),s(v),s(qe),s(Be),s(ke)),s(t),b(q,e),b(j,e),b(R,e),b(G,e),b(X),b(P),b(S),b(Q),b(Y),b(O,e),b(K),b(ee),b(te),b(ne,e)}}}const It='{"title":"BARThez","local":"barthez","sections":[{"title":"BarthezTokenizer","local":"transformers.BarthezTokenizer","sections":[],"depth":2},{"title":"BarthezTokenizerFast","local":"transformers.BarthezTokenizerFast","sections":[],"depth":2}],"depth":1}';function Ct(M){return _t(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zt extends bt{constructor(t){super(),Tt(this,t,Ct,Ut,kt,{})}}export{Zt as component};
