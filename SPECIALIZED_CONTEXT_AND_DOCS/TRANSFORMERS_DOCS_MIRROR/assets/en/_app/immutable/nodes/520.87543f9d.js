import{s as gt,n as wt,o as Tt}from"../chunks/scheduler.18a86fab.js";import{S as jt,i as Ut,g as o,s as n,r as G,A as kt,h as r,f as a,c as s,j as ft,u as I,x as i,k as yt,y as Jt,a as l,v as B,d as _,t as v,w as Q}from"../chunks/index.98837b22.js";import{C as bt}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as lt,E as Wt}from"../chunks/getInferenceSnippets.06c2775f.js";function Zt(nt){let p,C,$,X,d,F,m,st='<a href="https://quark.docs.amd.com/latest/" rel="nofollow">Quark</a> is a deep learning quantization toolkit designed to be agnostic to specific data types, algorithms, and hardware. Different pre-processing strategies, algorithms and data-types can be combined in Quark.',z,u,ot='The PyTorch support integrated through ðŸ¤— Transformers primarily targets AMD CPUs and GPUs, and is primarily meant to be used for evaluation purposes. For example, it is possible to use <a href="https://github.com/EleutherAI/lm-evaluation-harness" rel="nofollow">lm-evaluation-harness</a> with ðŸ¤— Transformers backend and evaluate a wide range of models quantized through Quark seamlessly.',q,c,rt='Users interested in Quark can refer to its <a href="https://quark.docs.amd.com/latest/" rel="nofollow">documentation</a> to get started quantizing models and using them in supported open-source libraries!',R,h,it='Although Quark has its own checkpoint / <a href="https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV-Quark-test/blob/main/config.json#L26" rel="nofollow">configuration format</a>, the library also supports producing models with a serialization layout compliant with other quantization/runtime implementations (<a href="https://huggingface.co/docs/transformers/quantization/awq" rel="nofollow">AutoAWQ</a>, <a href="https://huggingface.co/docs/transformers/quantization/finegrained_fp8" rel="nofollow">native fp8 in ðŸ¤— Transformers</a>).',Y,M,pt="To be able to load Quark quantized models in Transformers, the library first needs to be installed:",E,f,S,y,H,b,dt="Models quantized through Quark support a large range of features, that can be combined together. All quantized models independently of their configuration can seamlessly be reloaded through <code>PretrainedModel.from_pretrained</code>.",V,g,mt="The table below shows a few features supported by Quark:",L,w,ut="<thead><tr><th><strong>Feature</strong></th> <th><strong>Supported subset in Quark</strong></th> <th></th></tr></thead> <tbody><tr><td>Data types</td> <td>int8, int4, int2, bfloat16, float16, fp8_e5m2, fp8_e4m3, fp6_e3m2, fp6_e2m3, fp4, OCP MX, MX6, MX9, bfp16</td> <td></td></tr> <tr><td>Pre-quantization transformation</td> <td>SmoothQuant, QuaRot, SpinQuant, AWQ</td> <td></td></tr> <tr><td>Quantization algorithm</td> <td>GPTQ</td> <td></td></tr> <tr><td>Supported operators</td> <td><code>nn.Linear</code>, <code>nn.Conv2d</code>, <code>nn.ConvTranspose2d</code>, <code>nn.Embedding</code>, <code>nn.EmbeddingBag</code></td> <td></td></tr> <tr><td>Granularity</td> <td>per-tensor, per-channel, per-block, per-layer, per-layer type</td> <td></td></tr> <tr><td>KV cache</td> <td>fp8</td> <td></td></tr> <tr><td>Activation calibration</td> <td>MinMax / Percentile / MSE</td> <td></td></tr> <tr><td>Quantization strategy</td> <td>weight-only, static, dynamic, with or without output quantization</td> <td></td></tr></tbody>",A,T,P,j,ct='Public models using Quark native serialization can be found at <a href="https://huggingface.co/models?other=quark" rel="nofollow">https://huggingface.co/models?other=quark</a>.',N,U,ht='Although Quark also supports <a href="https://huggingface.co/models?other=fp8" rel="nofollow">models using <code>quant_method=&quot;fp8&quot;</code></a> and <a href="https://huggingface.co/models?other=awq" rel="nofollow">models using <code>quant_method=&quot;awq&quot;</code></a>, Transformers loads these models rather through <a href="https://huggingface.co/docs/transformers/quantization/awq" rel="nofollow">AutoAWQ</a> or uses the <a href="https://huggingface.co/docs/transformers/quantization/finegrained_fp8" rel="nofollow">native fp8 support in ðŸ¤— Transformers</a>.',D,k,K,J,Mt="Here is an example of how one can load a Quark model in Transformers:",O,W,tt,Z,et,x,at;return d=new lt({props:{title:"Quark",local:"quark",headingTag:"h1"}}),f=new bt({props:{code:"cGlwJTIwaW5zdGFsbCUyMGFtZC1xdWFyaw==",highlighted:"pip install amd-quark",wrap:!1}}),y=new lt({props:{title:"Support matrix",local:"support-matrix",headingTag:"h2"}}),T=new lt({props:{title:"Models on Hugging Face Hub",local:"models-on-hugging-face-hub",headingTag:"h2"}}),k=new lt({props:{title:"Using Quark models in Transformers",local:"using-quark-models-in-transformers",headingTag:"h2"}}),W=new bt({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

<span class="hljs-built_in">print</span>(model.model.layers[<span class="hljs-number">0</span>].self_attn.q_proj)
<span class="hljs-comment"># QParamsLinear(</span>
<span class="hljs-comment">#   (weight_quantizer): ScaledRealQuantizer()</span>
<span class="hljs-comment">#   (input_quantizer): ScaledRealQuantizer()</span>
<span class="hljs-comment">#   (output_quantizer): ScaledRealQuantizer()</span>
<span class="hljs-comment"># )</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
inp = tokenizer(<span class="hljs-string">&quot;Where is a good place to cycle around Tokyo?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
inp = inp.to(model.device)

res = model.generate(**inp, min_new_tokens=<span class="hljs-number">50</span>, max_new_tokens=<span class="hljs-number">100</span>)

<span class="hljs-built_in">print</span>(tokenizer.batch_decode(res)[<span class="hljs-number">0</span>])
<span class="hljs-comment"># &lt;|begin_of_text|&gt;Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:</span>
<span class="hljs-comment"># 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that&#x27;s perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.</span>
<span class="hljs-comment"># 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that&#x27;s perfect for cycling. You can also visit the</span>`,wrap:!1}}),Z=new Wt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/quark.md"}}),{c(){p=o("meta"),C=n(),$=o("p"),X=n(),G(d.$$.fragment),F=n(),m=o("p"),m.innerHTML=st,z=n(),u=o("p"),u.innerHTML=ot,q=n(),c=o("p"),c.innerHTML=rt,R=n(),h=o("p"),h.innerHTML=it,Y=n(),M=o("p"),M.textContent=pt,E=n(),G(f.$$.fragment),S=n(),G(y.$$.fragment),H=n(),b=o("p"),b.innerHTML=dt,V=n(),g=o("p"),g.textContent=mt,L=n(),w=o("table"),w.innerHTML=ut,A=n(),G(T.$$.fragment),P=n(),j=o("p"),j.innerHTML=ct,N=n(),U=o("p"),U.innerHTML=ht,D=n(),G(k.$$.fragment),K=n(),J=o("p"),J.textContent=Mt,O=n(),G(W.$$.fragment),tt=n(),G(Z.$$.fragment),et=n(),x=o("p"),this.h()},l(t){const e=kt("svelte-u9bgzb",document.head);p=r(e,"META",{name:!0,content:!0}),e.forEach(a),C=s(t),$=r(t,"P",{}),ft($).forEach(a),X=s(t),I(d.$$.fragment,t),F=s(t),m=r(t,"P",{"data-svelte-h":!0}),i(m)!=="svelte-f91k68"&&(m.innerHTML=st),z=s(t),u=r(t,"P",{"data-svelte-h":!0}),i(u)!=="svelte-1bghkp0"&&(u.innerHTML=ot),q=s(t),c=r(t,"P",{"data-svelte-h":!0}),i(c)!=="svelte-5ndvt6"&&(c.innerHTML=rt),R=s(t),h=r(t,"P",{"data-svelte-h":!0}),i(h)!=="svelte-8fh62i"&&(h.innerHTML=it),Y=s(t),M=r(t,"P",{"data-svelte-h":!0}),i(M)!=="svelte-14oxujq"&&(M.textContent=pt),E=s(t),I(f.$$.fragment,t),S=s(t),I(y.$$.fragment,t),H=s(t),b=r(t,"P",{"data-svelte-h":!0}),i(b)!=="svelte-1vpj1hi"&&(b.innerHTML=dt),V=s(t),g=r(t,"P",{"data-svelte-h":!0}),i(g)!=="svelte-3upp27"&&(g.textContent=mt),L=s(t),w=r(t,"TABLE",{"data-svelte-h":!0}),i(w)!=="svelte-vm9870"&&(w.innerHTML=ut),A=s(t),I(T.$$.fragment,t),P=s(t),j=r(t,"P",{"data-svelte-h":!0}),i(j)!=="svelte-4nfu69"&&(j.innerHTML=ct),N=s(t),U=r(t,"P",{"data-svelte-h":!0}),i(U)!=="svelte-1m8qmmj"&&(U.innerHTML=ht),D=s(t),I(k.$$.fragment,t),K=s(t),J=r(t,"P",{"data-svelte-h":!0}),i(J)!=="svelte-sp1dmt"&&(J.textContent=Mt),O=s(t),I(W.$$.fragment,t),tt=s(t),I(Z.$$.fragment,t),et=s(t),x=r(t,"P",{}),ft(x).forEach(a),this.h()},h(){yt(p,"name","hf:doc:metadata"),yt(p,"content",Gt)},m(t,e){Jt(document.head,p),l(t,C,e),l(t,$,e),l(t,X,e),B(d,t,e),l(t,F,e),l(t,m,e),l(t,z,e),l(t,u,e),l(t,q,e),l(t,c,e),l(t,R,e),l(t,h,e),l(t,Y,e),l(t,M,e),l(t,E,e),B(f,t,e),l(t,S,e),B(y,t,e),l(t,H,e),l(t,b,e),l(t,V,e),l(t,g,e),l(t,L,e),l(t,w,e),l(t,A,e),B(T,t,e),l(t,P,e),l(t,j,e),l(t,N,e),l(t,U,e),l(t,D,e),B(k,t,e),l(t,K,e),l(t,J,e),l(t,O,e),B(W,t,e),l(t,tt,e),B(Z,t,e),l(t,et,e),l(t,x,e),at=!0},p:wt,i(t){at||(_(d.$$.fragment,t),_(f.$$.fragment,t),_(y.$$.fragment,t),_(T.$$.fragment,t),_(k.$$.fragment,t),_(W.$$.fragment,t),_(Z.$$.fragment,t),at=!0)},o(t){v(d.$$.fragment,t),v(f.$$.fragment,t),v(y.$$.fragment,t),v(T.$$.fragment,t),v(k.$$.fragment,t),v(W.$$.fragment,t),v(Z.$$.fragment,t),at=!1},d(t){t&&(a(C),a($),a(X),a(F),a(m),a(z),a(u),a(q),a(c),a(R),a(h),a(Y),a(M),a(E),a(S),a(H),a(b),a(V),a(g),a(L),a(w),a(A),a(P),a(j),a(N),a(U),a(D),a(K),a(J),a(O),a(tt),a(et),a(x)),a(p),Q(d,t),Q(f,t),Q(y,t),Q(T,t),Q(k,t),Q(W,t),Q(Z,t)}}}const Gt='{"title":"Quark","local":"quark","sections":[{"title":"Support matrix","local":"support-matrix","sections":[],"depth":2},{"title":"Models on Hugging Face Hub","local":"models-on-hugging-face-hub","sections":[],"depth":2},{"title":"Using Quark models in Transformers","local":"using-quark-models-in-transformers","sections":[],"depth":2}],"depth":1}';function It(nt){return Tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $t extends jt{constructor(p){super(),Ut(this,p,It,Zt,gt,{})}}export{$t as component};
