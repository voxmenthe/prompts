import{s as de,o as Me,n as he}from"../chunks/scheduler.18a86fab.js";import{S as $e,i as ge,g as u,s as p,r as d,A as Je,h,f as a,c as f,j as ce,u as M,x as b,k as me,y as we,a as i,v as $,d as g,t as J,w}from"../chunks/index.98837b22.js";import{C as R}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ne,E as be}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as ye,a as ue}from"../chunks/HfOption.6641485e.js";function Te(y){let l,m='Quantize a model by creating a <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.HqqConfig">HqqConfig</a> and specifying the <code>nbits</code> and <code>group_size</code> to replace for all the linear layers (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="nofollow">torch.nn.Linear</a>) of the model.',r,o,s;return o=new R({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwSHFxQ29uZmlnJTBBJTBBcXVhbnRfY29uZmlnJTIwJTNEJTIwSHFxQ29uZmlnKG5iaXRzJTNEOCUyQyUyMGdyb3VwX3NpemUlM0Q2NCklMEFtb2RlbCUyMCUzRCUyMHRyYW5zZm9ybWVycy5BdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMS04QiUyMiUyQyUyMCUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMCUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50X2NvbmZpZyUwQSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, HqqConfig

quant_config = HqqConfig(nbits=<span class="hljs-number">8</span>, group_size=<span class="hljs-number">64</span>)
model = transformers.AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B&quot;</span>, 
    dtype=torch.float16, 
    device_map=<span class="hljs-string">&quot;auto&quot;</span>, 
    quantization_config=quant_config
)`,wrap:!1}}),{c(){l=u("p"),l.innerHTML=m,r=p(),d(o.$$.fragment)},l(t){l=h(t,"P",{"data-svelte-h":!0}),b(l)!=="svelte-18vcdwd"&&(l.innerHTML=m),r=f(t),M(o.$$.fragment,t)},m(t,c){i(t,l,c),i(t,r,c),$(o,t,c),s=!0},p:he,i(t){s||(g(o.$$.fragment,t),s=!0)},o(t){J(o.$$.fragment,t),s=!1},d(t){t&&(a(l),a(r)),w(o,t)}}}function je(y){let l,m='Quantize a model by creating a dictionary specifying the <code>nbits</code> and <code>group_size</code> for the linear layers to quantize. Pass them to <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.HqqConfig">HqqConfig</a> and set which layers to quantize with the config. This approach is especially useful for quantizing mixture-of-experts (MoEs) because they are less affected ly lower quantization settings.',r,o,s;return o=new R({props:{code:"cTRfY29uZmlnJTIwJTNEJTIwJTdCJ25iaXRzJyUzQTQlMkMlMjAnZ3JvdXBfc2l6ZSclM0E2NCU3RCUwQXEzX2NvbmZpZyUyMCUzRCUyMCU3QiduYml0cyclM0EzJTJDJTIwJ2dyb3VwX3NpemUnJTNBMzIlN0QlMEFxdWFudF9jb25maWclMjAlMjAlM0QlMjBIcXFDb25maWcoZHluYW1pY19jb25maWclM0QlN0IlMEElMjAlMjAnc2VsZl9hdHRuLnFfcHJvaiclM0FxNF9jb25maWclMkMlMEElMjAlMjAnc2VsZl9hdHRuLmtfcHJvaiclM0FxNF9jb25maWclMkMlMEElMjAlMjAnc2VsZl9hdHRuLnZfcHJvaiclM0FxNF9jb25maWclMkMlMEElMjAlMjAnc2VsZl9hdHRuLm9fcHJvaiclM0FxNF9jb25maWclMkMlMEElMEElMjAlMjAnbWxwLmdhdGVfcHJvaiclM0FxM19jb25maWclMkMlMEElMjAlMjAnbWxwLnVwX3Byb2onJTIwJTIwJTNBcTNfY29uZmlnJTJDJTBBJTIwJTIwJ21scC5kb3duX3Byb2onJTNBcTNfY29uZmlnJTJDJTBBJTdEKSUwQSUwQW1vZGVsJTIwJTNEJTIwdHJhbnNmb3JtZXJzLkF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCJTIyJTJDJTIwJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjAlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRfY29uZmlnJTBBKQ==",highlighted:`q4_config = {<span class="hljs-string">&#x27;nbits&#x27;</span>:<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;group_size&#x27;</span>:<span class="hljs-number">64</span>}
q3_config = {<span class="hljs-string">&#x27;nbits&#x27;</span>:<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;group_size&#x27;</span>:<span class="hljs-number">32</span>}
quant_config  = HqqConfig(dynamic_config={
  <span class="hljs-string">&#x27;self_attn.q_proj&#x27;</span>:q4_config,
  <span class="hljs-string">&#x27;self_attn.k_proj&#x27;</span>:q4_config,
  <span class="hljs-string">&#x27;self_attn.v_proj&#x27;</span>:q4_config,
  <span class="hljs-string">&#x27;self_attn.o_proj&#x27;</span>:q4_config,

  <span class="hljs-string">&#x27;mlp.gate_proj&#x27;</span>:q3_config,
  <span class="hljs-string">&#x27;mlp.up_proj&#x27;</span>  :q3_config,
  <span class="hljs-string">&#x27;mlp.down_proj&#x27;</span>:q3_config,
})

model = transformers.AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;meta-llama/Llama-3.1-8B&quot;</span>, 
    dtype=torch.float16, 
    device_map=<span class="hljs-string">&quot;auto&quot;</span>, 
    quantization_config=quant_config
)`,wrap:!1}}),{c(){l=u("p"),l.innerHTML=m,r=p(),d(o.$$.fragment)},l(t){l=h(t,"P",{"data-svelte-h":!0}),b(l)!=="svelte-1ls33v2"&&(l.innerHTML=m),r=f(t),M(o.$$.fragment,t)},m(t,c){i(t,l,c),i(t,r,c),$(o,t,c),s=!0},p:he,i(t){s||(g(o.$$.fragment,t),s=!0)},o(t){J(o.$$.fragment,t),s=!1},d(t){t&&(a(l),a(r)),w(o,t)}}}function _e(y){let l,m,r,o;return l=new ue({props:{id:"hqq",option:"replace all layers",$$slots:{default:[Te]},$$scope:{ctx:y}}}),r=new ue({props:{id:"hqq",option:"specific layers only",$$slots:{default:[je]},$$scope:{ctx:y}}}),{c(){d(l.$$.fragment),m=p(),d(r.$$.fragment)},l(s){M(l.$$.fragment,s),m=f(s),M(r.$$.fragment,s)},m(s,t){$(l,s,t),i(s,m,t),$(r,s,t),o=!0},p(s,t){const c={};t&2&&(c.$$scope={dirty:t,ctx:s}),l.$set(c);const z={};t&2&&(z.$$scope={dirty:t,ctx:s}),r.$set(z)},i(s){o||(g(l.$$.fragment,s),g(r.$$.fragment,s),o=!0)},o(s){J(l.$$.fragment,s),J(r.$$.fragment,s),o=!1},d(s){s&&a(m),w(l,s),w(r,s)}}}function qe(y){let l,m,r,o,s,t,c,z='<a href="https://github.com/mobiusml/hqq/" rel="nofollow">Half-Quadratic Quantization (HQQ)</a> supports fast on-the-fly quantization for 8, 4, 3, 2, and even 1-bits. It doesnâ€™t require calibration data, and it is compatible with any model modality (LLMs, vision, etc.).',X,j,se='HQQ further supports fine-tuning with <a href="https://huggingface.co/docs/peft" rel="nofollow">PEFT</a> and is fully compatible with <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a> for even faster inference and training.',W,_,ae="Install HQQ with the following command to get the latest version and to build its corresponding CUDA kernels if you are using a cuda device. It also support Intel XPU with pure pytorch implementation.",L,q,F,x,le="You can choose to either replace all the linear layers in a model with the same quantization config or dedicate a specific quantization config for specific linear layers.",A,T,G,C,N,Q,ie="HQQ supports various backends, including pure PyTorch and custom dequantization CUDA kernels. These backends are suitable for older GPUs and PEFT/QLoRA training.",Y,U,V,H,oe="For faster inference, HQQ supports 4-bit fused kernels (torchao and Marlin) after a model is quantized. These can reach up to 200 tokens/sec on a single 4090. The example below demonstrates enabling the torchao_int4 backend.",S,v,P,Z,re='Refer to the <a href="https://github.com/mobiusml/hqq/#backend" rel="nofollow">Backend</a> guide for more details.',D,I,O,k,pe='Read the <a href="https://mobiusml.github.io/hqq_blog/" rel="nofollow">Half-Quadratic Quantization of Large Machine Learning Models</a> blog post for more details about HQQ.',K,E,ee,B,te;return s=new ne({props:{title:"HQQ",local:"hqq",headingTag:"h1"}}),q=new R({props:{code:"cGlwJTIwaW5zdGFsbCUyMGhxcQ==",highlighted:"pip install hqq",wrap:!1}}),T=new ye({props:{id:"hqq",options:["replace all layers","specific layers only"],$$slots:{default:[_e]},$$scope:{ctx:y}}}),C=new ne({props:{title:"Backends",local:"backends",headingTag:"h2"}}),U=new R({props:{code:"ZnJvbSUyMGhxcS5jb3JlLnF1YW50aXplJTIwaW1wb3J0JTIwKiUwQSUwQUhRUUxpbmVhci5zZXRfYmFja2VuZChIUVFCYWNrZW5kLlBZVE9SQ0gp",highlighted:`<span class="hljs-keyword">from</span> hqq.core.quantize <span class="hljs-keyword">import</span> *

HQQLinear.set_backend(HQQBackend.PYTORCH)`,wrap:!1}}),v=new R({props:{code:"ZnJvbSUyMGhxcS51dGlscy5wYXRjaGluZyUyMGltcG9ydCUyMHByZXBhcmVfZm9yX2luZmVyZW5jZSUwQSUwQXByZXBhcmVfZm9yX2luZmVyZW5jZSglMjJtb2RlbCUyMiUyQyUyMGJhY2tlbmQlM0QlMjJ0b3JjaGFvX2ludDQlMjIp",highlighted:`<span class="hljs-keyword">from</span> hqq.utils.patching <span class="hljs-keyword">import</span> prepare_for_inference

prepare_for_inference(<span class="hljs-string">&quot;model&quot;</span>, backend=<span class="hljs-string">&quot;torchao_int4&quot;</span>)`,wrap:!1}}),I=new ne({props:{title:"Resources",local:"resources",headingTag:"h2"}}),E=new be({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/hqq.md"}}),{c(){l=u("meta"),m=p(),r=u("p"),o=p(),d(s.$$.fragment),t=p(),c=u("p"),c.innerHTML=z,X=p(),j=u("p"),j.innerHTML=se,W=p(),_=u("p"),_.textContent=ae,L=p(),d(q.$$.fragment),F=p(),x=u("p"),x.textContent=le,A=p(),d(T.$$.fragment),G=p(),d(C.$$.fragment),N=p(),Q=u("p"),Q.textContent=ie,Y=p(),d(U.$$.fragment),V=p(),H=u("p"),H.textContent=oe,S=p(),d(v.$$.fragment),P=p(),Z=u("p"),Z.innerHTML=re,D=p(),d(I.$$.fragment),O=p(),k=u("p"),k.innerHTML=pe,K=p(),d(E.$$.fragment),ee=p(),B=u("p"),this.h()},l(e){const n=Je("svelte-u9bgzb",document.head);l=h(n,"META",{name:!0,content:!0}),n.forEach(a),m=f(e),r=h(e,"P",{}),ce(r).forEach(a),o=f(e),M(s.$$.fragment,e),t=f(e),c=h(e,"P",{"data-svelte-h":!0}),b(c)!=="svelte-fwh8ox"&&(c.innerHTML=z),X=f(e),j=h(e,"P",{"data-svelte-h":!0}),b(j)!=="svelte-14m3csp"&&(j.innerHTML=se),W=f(e),_=h(e,"P",{"data-svelte-h":!0}),b(_)!=="svelte-1nal6be"&&(_.textContent=ae),L=f(e),M(q.$$.fragment,e),F=f(e),x=h(e,"P",{"data-svelte-h":!0}),b(x)!=="svelte-1n7dug2"&&(x.textContent=le),A=f(e),M(T.$$.fragment,e),G=f(e),M(C.$$.fragment,e),N=f(e),Q=h(e,"P",{"data-svelte-h":!0}),b(Q)!=="svelte-1fe8tjm"&&(Q.textContent=ie),Y=f(e),M(U.$$.fragment,e),V=f(e),H=h(e,"P",{"data-svelte-h":!0}),b(H)!=="svelte-4w4k3t"&&(H.textContent=oe),S=f(e),M(v.$$.fragment,e),P=f(e),Z=h(e,"P",{"data-svelte-h":!0}),b(Z)!=="svelte-h25g7y"&&(Z.innerHTML=re),D=f(e),M(I.$$.fragment,e),O=f(e),k=h(e,"P",{"data-svelte-h":!0}),b(k)!=="svelte-ubx7z0"&&(k.innerHTML=pe),K=f(e),M(E.$$.fragment,e),ee=f(e),B=h(e,"P",{}),ce(B).forEach(a),this.h()},h(){me(l,"name","hf:doc:metadata"),me(l,"content",xe)},m(e,n){we(document.head,l),i(e,m,n),i(e,r,n),i(e,o,n),$(s,e,n),i(e,t,n),i(e,c,n),i(e,X,n),i(e,j,n),i(e,W,n),i(e,_,n),i(e,L,n),$(q,e,n),i(e,F,n),i(e,x,n),i(e,A,n),$(T,e,n),i(e,G,n),$(C,e,n),i(e,N,n),i(e,Q,n),i(e,Y,n),$(U,e,n),i(e,V,n),i(e,H,n),i(e,S,n),$(v,e,n),i(e,P,n),i(e,Z,n),i(e,D,n),$(I,e,n),i(e,O,n),i(e,k,n),i(e,K,n),$(E,e,n),i(e,ee,n),i(e,B,n),te=!0},p(e,[n]){const fe={};n&2&&(fe.$$scope={dirty:n,ctx:e}),T.$set(fe)},i(e){te||(g(s.$$.fragment,e),g(q.$$.fragment,e),g(T.$$.fragment,e),g(C.$$.fragment,e),g(U.$$.fragment,e),g(v.$$.fragment,e),g(I.$$.fragment,e),g(E.$$.fragment,e),te=!0)},o(e){J(s.$$.fragment,e),J(q.$$.fragment,e),J(T.$$.fragment,e),J(C.$$.fragment,e),J(U.$$.fragment,e),J(v.$$.fragment,e),J(I.$$.fragment,e),J(E.$$.fragment,e),te=!1},d(e){e&&(a(m),a(r),a(o),a(t),a(c),a(X),a(j),a(W),a(_),a(L),a(F),a(x),a(A),a(G),a(N),a(Q),a(Y),a(V),a(H),a(S),a(P),a(Z),a(D),a(O),a(k),a(K),a(ee),a(B)),a(l),w(s,e),w(q,e),w(T,e),w(C,e),w(U,e),w(v,e),w(I,e),w(E,e)}}}const xe='{"title":"HQQ","local":"hqq","sections":[{"title":"Backends","local":"backends","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2}],"depth":1}';function Ce(y){return Me(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ie extends $e{constructor(l){super(),ge(this,l,Ce,qe,de,{})}}export{Ie as component};
