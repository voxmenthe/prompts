import{s as bt,o as yt,n as Oe}from"../chunks/scheduler.18a86fab.js";import{S as wt,i as Tt,g as l,s as a,r as f,A as Ut,h as d,f as n,c as r,j as ae,x as p,u as g,k as G,y as c,a as o,v as _,d as v,t as b,w as y}from"../chunks/index.98837b22.js";import{T as Mt}from"../chunks/Tip.77304350.js";import{D as ge}from"../chunks/Docstring.a1ef7999.js";import{C as Ke}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as vt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as _e,E as xt}from"../chunks/getInferenceSnippets.06c2775f.js";function Nt(j){let s,U="Example:",h,m,u;return m=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFVuaXZOZXRNb2RlbCUyQyUyMFVuaXZOZXRDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwVG9ydG9pc2UlMjBUVFMlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwVW5pdk5ldENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBUb3J0b2lzZSUyMFRUUyUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwVW5pdk5ldE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UnivNetModel, UnivNetConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Tortoise TTS style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UnivNetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the Tortoise TTS style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UnivNetModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=l("p"),s.textContent=U,h=a(),f(m.$$.fragment)},l(i){s=d(i,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=U),h=r(i),g(m.$$.fragment,i)},m(i,N){o(i,s,N),o(i,h,N),_(m,i,N),u=!0},p:Oe,i(i){u||(v(m.$$.fragment,i),u=!0)},o(i){b(m.$$.fragment,i),u=!1},d(i){i&&(n(s),n(h)),y(m,i)}}}function kt(j){let s,U=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=l("p"),s.innerHTML=U},l(h){s=d(h,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=U)},m(h,m){o(h,s,m)},p:Oe,d(h){h&&n(s)}}}function Jt(j){let s,U="Example:",h,m,u;return m=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFVuaXZOZXRGZWF0dXJlRXh0cmFjdG9yJTJDJTIwVW5pdk5ldE1vZGVsJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFtb2RlbCUyMCUzRCUyMFVuaXZOZXRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZGc4NDUlMkZ1bml2bmV0LWRldiUyMiklMEFmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMFVuaXZOZXRGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjJkZzg0NSUyRnVuaXZuZXQtZGV2JTIyKSUwQSUwQWRzJTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmhmLWludGVybmFsLXRlc3RpbmclMkZsaWJyaXNwZWVjaF9hc3JfZHVtbXklMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEElMjMlMjBSZXNhbXBsZSUyMHRoZSUyMGF1ZGlvJTIwdG8lMjB0aGUlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJ3MlMjBzYW1wbGluZyUyMHJhdGUuJTBBZHMlMjAlM0QlMjBkcy5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0RmZWF0dXJlX2V4dHJhY3Rvci5zYW1wbGluZ19yYXRlKSklMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvciglMEElMjAlMjAlMjAlMjBkcyU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVEJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRGRzJTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyc2FtcGxpbmdfcmF0ZSUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpJTBBYXVkaW8lMjAlM0QlMjBtb2RlbCgqKmlucHV0cykud2F2ZWZvcm1zJTBBbGlzdChhdWRpby5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UnivNetFeatureExtractor, UnivNetModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>model = UnivNetModel.from_pretrained(<span class="hljs-string">&quot;dg845/univnet-dev&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = UnivNetFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;dg845/univnet-dev&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Resample the audio to the feature extractor&#x27;s sampling rate.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=feature_extractor.sampling_rate))
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(
<span class="hljs-meta">... </span>    ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;sampling_rate&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio = model(**inputs).waveforms
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(audio.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">140288</span>]`,wrap:!1}}),{c(){s=l("p"),s.textContent=U,h=a(),f(m.$$.fragment)},l(i){s=d(i,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=U),h=r(i),g(m.$$.fragment,i)},m(i,N){o(i,s,N),o(i,h,N),_(m,i,N),u=!0},p:Oe,i(i){u||(v(m.$$.fragment,i),u=!0)},o(i){b(m.$$.fragment,i),u=!1},d(i){i&&(n(s),n(h)),y(m,i)}}}function Ft(j){let s,U,h,m,u,i="<em>This model was released on 2021-06-15 and added to Hugging Face Transformers on 2023-11-22.</em>",N,W,ve,$,et='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',be,B,ye,X,tt=`The UnivNet model was proposed in <a href="https://huggingface.co/papers/2106.07889" rel="nofollow">UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation</a> by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kin, and Juntae Kim.
The UnivNet model is a generative adversarial network (GAN) trained to synthesize high fidelity speech waveforms. The UnivNet model shared in <code>transformers</code> is the <em>generator</em>, which maps a conditioning log-mel spectrogram and optional noise sequence to a speech waveform (e.g. a vocoder). Only the generator is required for inference. The <em>discriminator</em> used to train the <code>generator</code> is not implemented.`,we,R,nt="The abstract from the paper is the following:",Te,H,ot="<em>Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.</em>",Ue,V,at="Tips:",Me,q,rt='<li>The <code>noise_sequence</code> argument for <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel.forward">UnivNetModel.forward()</a> should be standard Gaussian noise (such as from <code>torch.randn</code>) of shape <code>([batch_size], noise_length, model.config.model_in_channels)</code>, where <code>noise_length</code> should match the length dimension (dimension 1) of the <code>input_features</code> argument. If not supplied, it will be randomly generated; a <code>torch.Generator</code> can be supplied to the <code>generator</code> argument so that the forward pass can be reproduced. (Note that <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> will return generated noise by default, so it shouldn’t be necessary to generate <code>noise_sequence</code> manually.)</li> <li>Padding added by <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> can be removed from the <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> output through the <code>UnivNetFeatureExtractor.batch_decode()</code> method, as shown in the usage example below.</li> <li>Padding the end of each waveform with silence can reduce artifacts at the end of the generated audio sample. This can be done by supplying <code>pad_end = True</code> to <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__">UnivNetFeatureExtractor.<strong>call</strong>()</a>. See <a href="https://github.com/seungwonpark/melgan/issues/8" rel="nofollow">this issue</a> for more details.</li>',xe,S,st="Usage Example:",Ne,Y,ke,P,it=`This model was contributed by <a href="https://huggingface.co/dg845" rel="nofollow">dg845</a>.
To the best of my knowledge, there is no official code release, but an unofficial implementation can be found at <a href="https://github.com/maum-ai/univnet" rel="nofollow">maum-ai/univnet</a> with pretrained checkpoints <a href="https://github.com/maum-ai/univnet#pre-trained-model" rel="nofollow">here</a>.`,Je,Q,Fe,M,L,Ge,re,lt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a>. It is used to instantiate a
UnivNet vocoder model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UnivNet
<a href="https://huggingface.co/dg845/univnet-dev" rel="nofollow">dg845/univnet-dev</a> architecture, which corresponds to the ‘c32’
architecture in <a href="https://github.com/maum-ai/univnet/blob/master/config/default_c32.yaml" rel="nofollow">maum-ai/univnet</a>.`,We,se,dt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Be,E,je,A,$e,w,D,Xe,ie,ct="Constructs a UnivNet feature extractor.",Re,le,mt=`This class extracts log-mel-filter bank features from raw speech using the short time Fourier Transform (STFT). The
STFT implementation follows that of TacoTron 2 and Hifi-GAN.`,He,de,pt=`This feature extractor inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods.`,Ve,Z,O,qe,ce,ht="Main method to featurize and prepare for the model one or several sequence(s).",Ee,K,Ze,T,ee,Se,me,ut="The bare Univnet Model outputting raw hidden-states without any specific head on top.",Ye,pe,ft=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Pe,he,gt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Qe,k,te,Le,ue,_t='The <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> forward method, overrides the <code>__call__</code> special method.',Ae,z,De,I,ze,ne,Ie,fe,Ce;return W=new _e({props:{title:"UnivNet",local:"univnet",headingTag:"h1"}}),B=new _e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Y=new Ke({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwc2NpcHkuaW8ud2F2ZmlsZSUyMGltcG9ydCUyMHdyaXRlJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwQXVkaW8lMkMlMjBsb2FkX2RhdGFzZXQlMEElMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVW5pdk5ldEZlYXR1cmVFeHRyYWN0b3IlMkMlMjBVbml2TmV0TW9kZWwlMEElMEFtb2RlbF9pZF9vcl9wYXRoJTIwJTNEJTIwJTIyZGc4NDUlMkZ1bml2bmV0LWRldiUyMiUwQW1vZGVsJTIwJTNEJTIwVW5pdk5ldE1vZGVsLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZF9vcl9wYXRoKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwVW5pdk5ldEZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkX29yX3BhdGgpJTBBJTBBZHMlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQSUyMyUyMFJlc2FtcGxlJTIwdGhlJTIwYXVkaW8lMjB0byUyMHRoZSUyMG1vZGVsJTIwYW5kJTIwZmVhdHVyZSUyMGV4dHJhY3RvcidzJTIwc2FtcGxpbmclMjByYXRlLiUwQWRzJTIwJTNEJTIwZHMuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSkpJTBBJTIzJTIwUGFkJTIwdGhlJTIwZW5kJTIwb2YlMjB0aGUlMjBjb252ZXJ0ZWQlMjB3YXZlZm9ybXMlMjB0byUyMHJlZHVjZSUyMGFydGlmYWN0cyUyMGF0JTIwdGhlJTIwZW5kJTIwb2YlMjB0aGUlMjBvdXRwdXQlMjBhdWRpbyUyMHNhbXBsZXMuJTBBaW5wdXRzJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3IoJTBBJTIwJTIwJTIwJTIwZHMlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RkcyU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMnNhbXBsaW5nX3JhdGUlMjIlNUQlMkMlMjBwYWRfZW5kJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGF1ZGlvJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwUmVtb3ZlJTIwdGhlJTIwZXh0cmElMjBwYWRkaW5nJTIwYXQlMjB0aGUlMjBlbmQlMjBvZiUyMHRoZSUyMG91dHB1dC4lMEFhdWRpbyUyMCUzRCUyMGZlYXR1cmVfZXh0cmFjdG9yLmJhdGNoX2RlY29kZSgqKmF1ZGlvKSU1QjAlNUQlMEElMjMlMjBDb252ZXJ0JTIwdG8lMjB3YXYlMjBmaWxlJTBBd3JpdGUoJTIyc2FtcGxlX2F1ZGlvLndhdiUyMiUyQyUyMGZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUlMkMlMjBhdWRpbyk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> scipy.io.wavfile <span class="hljs-keyword">import</span> write
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Audio, load_dataset

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UnivNetFeatureExtractor, UnivNetModel

model_id_or_path = <span class="hljs-string">&quot;dg845/univnet-dev&quot;</span>
model = UnivNetModel.from_pretrained(model_id_or_path)
feature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)

ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-comment"># Resample the audio to the model and feature extractor&#x27;s sampling rate.</span>
ds = ds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=feature_extractor.sampling_rate))
<span class="hljs-comment"># Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.</span>
inputs = feature_extractor(
    ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;sampling_rate&quot;</span>], pad_end=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
)

<span class="hljs-keyword">with</span> torch.no_grad():
    audio = model(**inputs)

<span class="hljs-comment"># Remove the extra padding at the end of the output.</span>
audio = feature_extractor.batch_decode(**audio)[<span class="hljs-number">0</span>]
<span class="hljs-comment"># Convert to wav file</span>
write(<span class="hljs-string">&quot;sample_audio.wav&quot;</span>, feature_extractor.sampling_rate, audio)`,wrap:!1}}),Q=new _e({props:{title:"UnivNetConfig",local:"transformers.UnivNetConfig",headingTag:"h2"}}),L=new ge({props:{name:"class transformers.UnivNetConfig",anchor:"transformers.UnivNetConfig",parameters:[{name:"model_in_channels",val:" = 64"},{name:"model_hidden_channels",val:" = 32"},{name:"num_mel_bins",val:" = 100"},{name:"resblock_kernel_sizes",val:" = [3, 3, 3]"},{name:"resblock_stride_sizes",val:" = [8, 8, 4]"},{name:"resblock_dilation_sizes",val:" = [[1, 3, 9, 27], [1, 3, 9, 27], [1, 3, 9, 27]]"},{name:"kernel_predictor_num_blocks",val:" = 3"},{name:"kernel_predictor_hidden_channels",val:" = 64"},{name:"kernel_predictor_conv_size",val:" = 3"},{name:"kernel_predictor_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.01"},{name:"leaky_relu_slope",val:" = 0.2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UnivNetConfig.model_in_channels",description:`<strong>model_in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of input channels for the UnivNet residual network. This should correspond to
<code>noise_sequence.shape[1]</code> and the value used in the <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> class.`,name:"model_in_channels"},{anchor:"transformers.UnivNetConfig.model_hidden_channels",description:`<strong>model_hidden_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of hidden channels of each residual block in the UnivNet residual network.`,name:"model_hidden_channels"},{anchor:"transformers.UnivNetConfig.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The number of frequency bins in the conditioning log-mel spectrogram. This should correspond to the value
used in the <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> class.`,name:"num_mel_bins"},{anchor:"transformers.UnivNetConfig.resblock_kernel_sizes",description:`<strong>resblock_kernel_sizes</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 3]</code>) &#x2014;
A tuple of integers defining the kernel sizes of the 1D convolutional layers in the UnivNet residual
network. The length of <code>resblock_kernel_sizes</code> defines the number of resnet blocks and should match that of
<code>resblock_stride_sizes</code> and <code>resblock_dilation_sizes</code>.`,name:"resblock_kernel_sizes"},{anchor:"transformers.UnivNetConfig.resblock_stride_sizes",description:`<strong>resblock_stride_sizes</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4]</code>) &#x2014;
A tuple of integers defining the stride sizes of the 1D convolutional layers in the UnivNet residual
network. The length of <code>resblock_stride_sizes</code> should match that of <code>resblock_kernel_sizes</code> and
<code>resblock_dilation_sizes</code>.`,name:"resblock_stride_sizes"},{anchor:"transformers.UnivNetConfig.resblock_dilation_sizes",description:`<strong>resblock_dilation_sizes</strong> (<code>tuple[tuple[int]]</code> or <code>list[list[int]]</code>, <em>optional</em>, defaults to <code>[[1, 3, 9, 27], [1, 3, 9, 27], [1, 3, 9, 27]]</code>) &#x2014;
A nested tuple of integers defining the dilation rates of the dilated 1D convolutional layers in the
UnivNet residual network. The length of <code>resblock_dilation_sizes</code> should match that of
<code>resblock_kernel_sizes</code> and <code>resblock_stride_sizes</code>. The length of each nested list in
<code>resblock_dilation_sizes</code> defines the number of convolutional layers per resnet block.`,name:"resblock_dilation_sizes"},{anchor:"transformers.UnivNetConfig.kernel_predictor_num_blocks",description:`<strong>kernel_predictor_num_blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of residual blocks in the kernel predictor network, which calculates the kernel and bias for
each location variable convolution layer in the UnivNet residual network.`,name:"kernel_predictor_num_blocks"},{anchor:"transformers.UnivNetConfig.kernel_predictor_hidden_channels",description:`<strong>kernel_predictor_hidden_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of hidden channels for each residual block in the kernel predictor network.`,name:"kernel_predictor_hidden_channels"},{anchor:"transformers.UnivNetConfig.kernel_predictor_conv_size",description:`<strong>kernel_predictor_conv_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The kernel size of each 1D convolutional layer in the kernel predictor network.`,name:"kernel_predictor_conv_size"},{anchor:"transformers.UnivNetConfig.kernel_predictor_dropout",description:`<strong>kernel_predictor_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for each residual block in the kernel predictor network.`,name:"kernel_predictor_dropout"},{anchor:"transformers.UnivNetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.01) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UnivNetConfig.leaky_relu_slope",description:`<strong>leaky_relu_slope</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The angle of the negative slope used by the leaky ReLU activation.`,name:"leaky_relu_slope"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/univnet/configuration_univnet.py#L23"}}),E=new vt({props:{anchor:"transformers.UnivNetConfig.example",$$slots:{default:[Nt]},$$scope:{ctx:j}}}),A=new _e({props:{title:"UnivNetFeatureExtractor",local:"transformers.UnivNetFeatureExtractor",headingTag:"h2"}}),D=new ge({props:{name:"class transformers.UnivNetFeatureExtractor",anchor:"transformers.UnivNetFeatureExtractor",parameters:[{name:"feature_size",val:": int = 1"},{name:"sampling_rate",val:": int = 24000"},{name:"padding_value",val:": float = 0.0"},{name:"do_normalize",val:": bool = False"},{name:"num_mel_bins",val:": int = 100"},{name:"hop_length",val:": int = 256"},{name:"win_length",val:": int = 1024"},{name:"win_function",val:": str = 'hann_window'"},{name:"filter_length",val:": typing.Optional[int] = 1024"},{name:"max_length_s",val:": int = 10"},{name:"fmin",val:": float = 0.0"},{name:"fmax",val:": typing.Optional[float] = None"},{name:"mel_floor",val:": float = 1e-09"},{name:"center",val:": bool = False"},{name:"compression_factor",val:": float = 1.0"},{name:"compression_clip_val",val:": float = 1e-05"},{name:"normalize_min",val:": float = -11.512925148010254"},{name:"normalize_max",val:": float = 2.3143386840820312"},{name:"model_in_channels",val:": int = 64"},{name:"pad_end_length",val:": int = 10"},{name:"return_attention_mask",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UnivNetFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.UnivNetFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 24000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.UnivNetFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The value to pad with when applying the padding strategy defined by the <code>padding</code> argument to
<a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__">UnivNetFeatureExtractor.<strong>call</strong>()</a>. Should correspond to audio silence. The <code>pad_end</code> argument to
<code>__call__</code> will also use this padding value.`,name:"padding_value"},{anchor:"transformers.UnivNetFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to perform Tacotron 2 normalization on the input. Normalizing can help to significantly improve the
performance for some models.`,name:"do_normalize"},{anchor:"transformers.UnivNetFeatureExtractor.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The number of mel-frequency bins in the extracted spectrogram features. This should match
<code>UnivNetModel.config.num_mel_bins</code>.`,name:"num_mel_bins"},{anchor:"transformers.UnivNetFeatureExtractor.hop_length",description:`<strong>hop_length</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The direct number of samples between sliding windows. Otherwise referred to as &#x201C;shift&#x201D; in many papers. Note
that this is different from other audio feature extractors such as <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor">SpeechT5FeatureExtractor</a> which take
the <code>hop_length</code> in ms.`,name:"hop_length"},{anchor:"transformers.UnivNetFeatureExtractor.win_length",description:`<strong>win_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The direct number of samples for each sliding window. Note that this is different from other audio feature
extractors such as <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor">SpeechT5FeatureExtractor</a> which take the <code>win_length</code> in ms.`,name:"win_length"},{anchor:"transformers.UnivNetFeatureExtractor.win_function",description:`<strong>win_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;hann_window&quot;</code>) &#x2014;
Name for the window function used for windowing, must be accessible via <code>torch.{win_function}</code>`,name:"win_function"},{anchor:"transformers.UnivNetFeatureExtractor.filter_length",description:`<strong>filter_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The number of FFT components to use. If <code>None</code>, this is determined using
<code>transformers.audio_utils.optimal_fft_length</code>.`,name:"filter_length"},{anchor:"transformers.UnivNetFeatureExtractor.max_length_s",description:`<strong>max_length_s</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The maximum input length of the model in seconds. This is used to pad the audio.`,name:"max_length_s"},{anchor:"transformers.UnivNetFeatureExtractor.fmin",description:`<strong>fmin</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Minimum mel frequency in Hz.`,name:"fmin"},{anchor:"transformers.UnivNetFeatureExtractor.fmax",description:`<strong>fmax</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum mel frequency in Hz. If not set, defaults to <code>sampling_rate / 2</code>.`,name:"fmax"},{anchor:"transformers.UnivNetFeatureExtractor.mel_floor",description:`<strong>mel_floor</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-09) &#x2014;
Minimum value of mel frequency banks. Note that the way <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> uses <code>mel_floor</code> is
different than in <a href="/docs/transformers/v4.56.2/en/internal/audio_utils#transformers.audio_utils.spectrogram">transformers.audio_utils.spectrogram()</a>.`,name:"mel_floor"},{anchor:"transformers.UnivNetFeatureExtractor.center",description:`<strong>center</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to pad the waveform so that frame <code>t</code> is centered around time <code>t * hop_length</code>. If <code>False</code>, frame
<code>t</code> will start at time <code>t * hop_length</code>.`,name:"center"},{anchor:"transformers.UnivNetFeatureExtractor.compression_factor",description:`<strong>compression_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The multiplicative compression factor for dynamic range compression during spectral normalization.`,name:"compression_factor"},{anchor:"transformers.UnivNetFeatureExtractor.compression_clip_val",description:`<strong>compression_clip_val</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The clip value applied to the waveform before applying dynamic range compression during spectral
normalization.`,name:"compression_clip_val"},{anchor:"transformers.UnivNetFeatureExtractor.normalize_min",description:`<strong>normalize_min</strong> (<code>float</code>, <em>optional</em>, defaults to -11.512925148010254) &#x2014;
The min value used for Tacotron 2-style linear normalization. The default is the original value from the
Tacotron 2 implementation.`,name:"normalize_min"},{anchor:"transformers.UnivNetFeatureExtractor.normalize_max",description:`<strong>normalize_max</strong> (<code>float</code>, <em>optional</em>, defaults to 2.3143386840820312) &#x2014;
The max value used for Tacotron 2-style linear normalization. The default is the original value from the
Tacotron 2 implementation.`,name:"normalize_max"},{anchor:"transformers.UnivNetFeatureExtractor.model_in_channels",description:`<strong>model_in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of input channels to the <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> model. This should match
<code>UnivNetModel.config.model_in_channels</code>.`,name:"model_in_channels"},{anchor:"transformers.UnivNetFeatureExtractor.pad_end_length",description:`<strong>pad_end_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
If padding the end of each waveform, the number of spectrogram frames worth of samples to append. The
number of appended samples will be <code>pad_end_length * hop_length</code>.`,name:"pad_end_length"},{anchor:"transformers.UnivNetFeatureExtractor.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__"><strong>call</strong>()</a> should return <code>attention_mask</code>.`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/univnet/feature_extraction_univnet.py#L29"}}),O=new ge({props:{name:"__call__",anchor:"transformers.UnivNetFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": typing.Union[numpy.ndarray, list[float], list[numpy.ndarray], list[list[float]]]"},{name:"sampling_rate",val:": typing.Optional[int] = None"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = True"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_noise",val:": bool = True"},{name:"generator",val:": typing.Optional[numpy.random._generator.Generator] = None"},{name:"pad_end",val:": bool = False"},{name:"pad_length",val:": typing.Optional[int] = None"},{name:"do_normalize",val:": typing.Optional[str] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.UnivNetFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>np.ndarray</code>, <code>list[float]</code>, <code>list[np.ndarray]</code>, <code>list[list[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not
stereo, i.e. single float per timestep.`,name:"raw_speech"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors and allow automatic speech recognition
pipeline.`,name:"sampling_rate"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the input <code>raw_speech</code> waveforms (according to the model&#x2019;s padding side and
padding index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>
<p>If <code>pad_end = True</code>, that padding will occur before the <code>padding</code> strategy is applied.`,name:"padding"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.`,name:"pad_to_multiple_of"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.return_noise",description:`<strong>return_noise</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to generate and return a noise waveform for use in <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel.forward">UnivNetModel.forward()</a>.`,name:"return_noise"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.generator",description:`<strong>generator</strong> (<code>numpy.random.Generator</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
An optional <code>numpy.random.Generator</code> random number generator to use when generating noise.`,name:"generator"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.pad_end",description:`<strong>pad_end</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to pad the end of each waveform with silence. This can help reduce artifacts at the end of the
generated audio sample; see <a href="https://github.com/seungwonpark/melgan/issues/8" rel="nofollow">https://github.com/seungwonpark/melgan/issues/8</a> for more details. This
padding will be done before the padding strategy specified in <code>padding</code> is performed.`,name:"pad_end"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.pad_length",description:`<strong>pad_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
If padding the end of each waveform, the length of the padding in spectrogram frames. If not set, this
will default to <code>self.config.pad_end_length</code>.`,name:"pad_length"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to perform Tacotron 2 normalization on the input. Normalizing can help to significantly improve
the performance for some models. If not set, this will default to <code>self.config.do_normalize</code>.`,name:"do_normalize"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.UnivNetFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.np.array</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/univnet/feature_extraction_univnet.py#L286"}}),K=new _e({props:{title:"UnivNetModel",local:"transformers.UnivNetModel",headingTag:"h2"}}),ee=new ge({props:{name:"class transformers.UnivNetModel",anchor:"transformers.UnivNetModel",parameters:[{name:"config",val:": UnivNetConfig"}],parametersDescription:[{anchor:"transformers.UnivNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetConfig">UnivNetConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/univnet/modeling_univnet.py#L428"}}),te=new ge({props:{name:"forward",anchor:"transformers.UnivNetModel.forward",parameters:[{name:"input_features",val:": FloatTensor"},{name:"noise_sequence",val:": typing.Optional[torch.FloatTensor] = None"},{name:"padding_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.UnivNetModel.forward.input_features",description:`<strong>input_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, feature_dim)</code>) &#x2014;
The tensors corresponding to the input audio features. Audio features can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__">UnivNetFeatureExtractor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> for processing audios).`,name:"input_features"},{anchor:"transformers.UnivNetModel.forward.noise_sequence",description:`<strong>noise_sequence</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Tensor containing a noise sequence of standard Gaussian noise. Can be batched and of shape <code>(batch_size, sequence_length, config.model_in_channels)</code>, or un-batched and of shape (sequence_length,
config.model_in_channels)\`. If not supplied, will be randomly generated.`,name:"noise_sequence"},{anchor:"transformers.UnivNetModel.forward.padding_mask",description:`<strong>padding_mask</strong> (<code>torch.BoolTensor</code>, <em>optional</em>) &#x2014;
Mask indicating which parts of each sequence are padded. Mask values are selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong></li>
<li>0 for tokens that are <strong>masked</strong></li>
</ul>
<p>The mask can be batched and of shape <code>(batch_size, sequence_length)</code> or un-batched and of shape
<code>(sequence_length,)</code>.`,name:"padding_mask"},{anchor:"transformers.UnivNetModel.forward.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.
return_dict:
Whether to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> subclass instead of a plain tuple.`,name:"generator"},{anchor:"transformers.UnivNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/univnet/modeling_univnet.py#L471",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.univnet.modeling_univnet.UnivNetModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetConfig"
>UnivNetConfig</a>) and inputs.</p>
<ul>
<li><strong>waveforms</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) — Batched 1D (mono-channel) output audio waveforms.</li>
<li><strong>waveform_lengths</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>) — The batched length in samples of each unpadded waveform in <code>waveforms</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.univnet.modeling_univnet.UnivNetModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),z=new Mt({props:{$$slots:{default:[kt]},$$scope:{ctx:j}}}),I=new vt({props:{anchor:"transformers.UnivNetModel.forward.example",$$slots:{default:[Jt]},$$scope:{ctx:j}}}),ne=new xt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/univnet.md"}}),{c(){s=l("meta"),U=a(),h=l("p"),m=a(),u=l("p"),u.innerHTML=i,N=a(),f(W.$$.fragment),ve=a(),$=l("div"),$.innerHTML=et,be=a(),f(B.$$.fragment),ye=a(),X=l("p"),X.innerHTML=tt,we=a(),R=l("p"),R.textContent=nt,Te=a(),H=l("p"),H.innerHTML=ot,Ue=a(),V=l("p"),V.textContent=at,Me=a(),q=l("ul"),q.innerHTML=rt,xe=a(),S=l("p"),S.textContent=st,Ne=a(),f(Y.$$.fragment),ke=a(),P=l("p"),P.innerHTML=it,Je=a(),f(Q.$$.fragment),Fe=a(),M=l("div"),f(L.$$.fragment),Ge=a(),re=l("p"),re.innerHTML=lt,We=a(),se=l("p"),se.innerHTML=dt,Be=a(),f(E.$$.fragment),je=a(),f(A.$$.fragment),$e=a(),w=l("div"),f(D.$$.fragment),Xe=a(),ie=l("p"),ie.textContent=ct,Re=a(),le=l("p"),le.textContent=mt,He=a(),de=l("p"),de.innerHTML=pt,Ve=a(),Z=l("div"),f(O.$$.fragment),qe=a(),ce=l("p"),ce.textContent=ht,Ee=a(),f(K.$$.fragment),Ze=a(),T=l("div"),f(ee.$$.fragment),Se=a(),me=l("p"),me.textContent=ut,Ye=a(),pe=l("p"),pe.innerHTML=ft,Pe=a(),he=l("p"),he.innerHTML=gt,Qe=a(),k=l("div"),f(te.$$.fragment),Le=a(),ue=l("p"),ue.innerHTML=_t,Ae=a(),f(z.$$.fragment),De=a(),f(I.$$.fragment),ze=a(),f(ne.$$.fragment),Ie=a(),fe=l("p"),this.h()},l(e){const t=Ut("svelte-u9bgzb",document.head);s=d(t,"META",{name:!0,content:!0}),t.forEach(n),U=r(e),h=d(e,"P",{}),ae(h).forEach(n),m=r(e),u=d(e,"P",{"data-svelte-h":!0}),p(u)!=="svelte-nhz7rc"&&(u.innerHTML=i),N=r(e),g(W.$$.fragment,e),ve=r(e),$=d(e,"DIV",{class:!0,"data-svelte-h":!0}),p($)!=="svelte-13t8s2t"&&($.innerHTML=et),be=r(e),g(B.$$.fragment,e),ye=r(e),X=d(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-qk0u7e"&&(X.innerHTML=tt),we=r(e),R=d(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-vfdo9a"&&(R.textContent=nt),Te=r(e),H=d(e,"P",{"data-svelte-h":!0}),p(H)!=="svelte-1uiu21e"&&(H.innerHTML=ot),Ue=r(e),V=d(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-axv494"&&(V.textContent=at),Me=r(e),q=d(e,"UL",{"data-svelte-h":!0}),p(q)!=="svelte-15qkn4t"&&(q.innerHTML=rt),xe=r(e),S=d(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-knisyv"&&(S.textContent=st),Ne=r(e),g(Y.$$.fragment,e),ke=r(e),P=d(e,"P",{"data-svelte-h":!0}),p(P)!=="svelte-vb7l6k"&&(P.innerHTML=it),Je=r(e),g(Q.$$.fragment,e),Fe=r(e),M=d(e,"DIV",{class:!0});var J=ae(M);g(L.$$.fragment,J),Ge=r(J),re=d(J,"P",{"data-svelte-h":!0}),p(re)!=="svelte-g9ttph"&&(re.innerHTML=lt),We=r(J),se=d(J,"P",{"data-svelte-h":!0}),p(se)!=="svelte-1ek1ss9"&&(se.innerHTML=dt),Be=r(J),g(E.$$.fragment,J),J.forEach(n),je=r(e),g(A.$$.fragment,e),$e=r(e),w=d(e,"DIV",{class:!0});var x=ae(w);g(D.$$.fragment,x),Xe=r(x),ie=d(x,"P",{"data-svelte-h":!0}),p(ie)!=="svelte-a7avru"&&(ie.textContent=ct),Re=r(x),le=d(x,"P",{"data-svelte-h":!0}),p(le)!=="svelte-v863cv"&&(le.textContent=mt),He=r(x),de=d(x,"P",{"data-svelte-h":!0}),p(de)!=="svelte-ue5gbv"&&(de.innerHTML=pt),Ve=r(x),Z=d(x,"DIV",{class:!0});var oe=ae(Z);g(O.$$.fragment,oe),qe=r(oe),ce=d(oe,"P",{"data-svelte-h":!0}),p(ce)!=="svelte-1a6wgfx"&&(ce.textContent=ht),oe.forEach(n),x.forEach(n),Ee=r(e),g(K.$$.fragment,e),Ze=r(e),T=d(e,"DIV",{class:!0});var F=ae(T);g(ee.$$.fragment,F),Se=r(F),me=d(F,"P",{"data-svelte-h":!0}),p(me)!=="svelte-zfonm7"&&(me.textContent=ut),Ye=r(F),pe=d(F,"P",{"data-svelte-h":!0}),p(pe)!=="svelte-q52n56"&&(pe.innerHTML=ft),Pe=r(F),he=d(F,"P",{"data-svelte-h":!0}),p(he)!=="svelte-hswkmf"&&(he.innerHTML=gt),Qe=r(F),k=d(F,"DIV",{class:!0});var C=ae(k);g(te.$$.fragment,C),Le=r(C),ue=d(C,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-1gkuzdi"&&(ue.innerHTML=_t),Ae=r(C),g(z.$$.fragment,C),De=r(C),g(I.$$.fragment,C),C.forEach(n),F.forEach(n),ze=r(e),g(ne.$$.fragment,e),Ie=r(e),fe=d(e,"P",{}),ae(fe).forEach(n),this.h()},h(){G(s,"name","hf:doc:metadata"),G(s,"content",jt),G($,"class","flex flex-wrap space-x-1"),G(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,s),o(e,U,t),o(e,h,t),o(e,m,t),o(e,u,t),o(e,N,t),_(W,e,t),o(e,ve,t),o(e,$,t),o(e,be,t),_(B,e,t),o(e,ye,t),o(e,X,t),o(e,we,t),o(e,R,t),o(e,Te,t),o(e,H,t),o(e,Ue,t),o(e,V,t),o(e,Me,t),o(e,q,t),o(e,xe,t),o(e,S,t),o(e,Ne,t),_(Y,e,t),o(e,ke,t),o(e,P,t),o(e,Je,t),_(Q,e,t),o(e,Fe,t),o(e,M,t),_(L,M,null),c(M,Ge),c(M,re),c(M,We),c(M,se),c(M,Be),_(E,M,null),o(e,je,t),_(A,e,t),o(e,$e,t),o(e,w,t),_(D,w,null),c(w,Xe),c(w,ie),c(w,Re),c(w,le),c(w,He),c(w,de),c(w,Ve),c(w,Z),_(O,Z,null),c(Z,qe),c(Z,ce),o(e,Ee,t),_(K,e,t),o(e,Ze,t),o(e,T,t),_(ee,T,null),c(T,Se),c(T,me),c(T,Ye),c(T,pe),c(T,Pe),c(T,he),c(T,Qe),c(T,k),_(te,k,null),c(k,Le),c(k,ue),c(k,Ae),_(z,k,null),c(k,De),_(I,k,null),o(e,ze,t),_(ne,e,t),o(e,Ie,t),o(e,fe,t),Ce=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),E.$set(J);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),z.$set(x);const oe={};t&2&&(oe.$$scope={dirty:t,ctx:e}),I.$set(oe)},i(e){Ce||(v(W.$$.fragment,e),v(B.$$.fragment,e),v(Y.$$.fragment,e),v(Q.$$.fragment,e),v(L.$$.fragment,e),v(E.$$.fragment,e),v(A.$$.fragment,e),v(D.$$.fragment,e),v(O.$$.fragment,e),v(K.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(z.$$.fragment,e),v(I.$$.fragment,e),v(ne.$$.fragment,e),Ce=!0)},o(e){b(W.$$.fragment,e),b(B.$$.fragment,e),b(Y.$$.fragment,e),b(Q.$$.fragment,e),b(L.$$.fragment,e),b(E.$$.fragment,e),b(A.$$.fragment,e),b(D.$$.fragment,e),b(O.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(z.$$.fragment,e),b(I.$$.fragment,e),b(ne.$$.fragment,e),Ce=!1},d(e){e&&(n(U),n(h),n(m),n(u),n(N),n(ve),n($),n(be),n(ye),n(X),n(we),n(R),n(Te),n(H),n(Ue),n(V),n(Me),n(q),n(xe),n(S),n(Ne),n(ke),n(P),n(Je),n(Fe),n(M),n(je),n($e),n(w),n(Ee),n(Ze),n(T),n(ze),n(Ie),n(fe)),n(s),y(W,e),y(B,e),y(Y,e),y(Q,e),y(L),y(E),y(A,e),y(D),y(O),y(K,e),y(ee),y(te),y(z),y(I),y(ne,e)}}}const jt='{"title":"UnivNet","local":"univnet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"UnivNetConfig","local":"transformers.UnivNetConfig","sections":[],"depth":2},{"title":"UnivNetFeatureExtractor","local":"transformers.UnivNetFeatureExtractor","sections":[],"depth":2},{"title":"UnivNetModel","local":"transformers.UnivNetModel","sections":[],"depth":2}],"depth":1}';function $t(j){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bt extends wt{constructor(s){super(),Tt(this,s,$t,Ft,bt,{})}}export{Bt as component};
