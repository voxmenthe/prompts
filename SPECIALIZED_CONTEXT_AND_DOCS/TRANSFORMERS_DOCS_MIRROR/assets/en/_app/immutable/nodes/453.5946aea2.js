import{s as Zn,o as xn,n as N}from"../chunks/scheduler.18a86fab.js";import{S as zn,i as Xn,g as m,s,r as u,A as Gn,h as p,f as a,c as r,j as W,x as f,u as h,k as T,y as l,a as d,v as g,d as _,t as v,w as b}from"../chunks/index.98837b22.js";import{T as jo}from"../chunks/Tip.77304350.js";import{D as Z}from"../chunks/Docstring.a1ef7999.js";import{C as Oe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as De}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as E,E as Rn}from"../chunks/getInferenceSnippets.06c2775f.js";function Yn(C){let o,y="Example:",i,c,M;return c=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFdhdjJWZWMyQ29uZm9ybWVyQ29uZmlnJTJDJTIwV2F2MlZlYzJDb25mb3JtZXJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBXYXYyVmVjMkNvbmZvcm1lciUyMGZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwV2F2MlZlYzJDb25mb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwZmFjZWJvb2slMkZ3YXYydmVjMi1jb25mb3JtZXItcmVsLXBvcy1sYXJnZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwV2F2MlZlYzJDb25mb3JtZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Wav2Vec2Conformer facebook/wav2vec2-conformer-rel-pos-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Wav2Vec2ConformerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the facebook/wav2vec2-conformer-rel-pos-large style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function In(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function qn(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function Nn(C){let o,y="Example:",i,c,M;return c=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBXYXYyVmVjMkNvbmZvcm1lckZvckNUQyUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBXYXYyVmVjMkNvbmZvcm1lckZvckNUQy5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1jb25mb3JtZXItcmVsLXBvcy1sYXJnZSUyMiklMEElMEElMjMlMjBhdWRpbyUyMGZpbGUlMjBpcyUyMGRlY29kZWQlMjBvbiUyMHRoZSUyMGZseSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEFwcmVkaWN0ZWRfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQSUwQSUyMyUyMHRyYW5zY3JpYmUlMjBzcGVlY2glMEF0cmFuc2NyaXB0aW9uJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShwcmVkaWN0ZWRfaWRzKSUwQXRyYW5zY3JpcHRpb24lNUIwJTVEJTBBJTBBaW5wdXRzJTVCJTIybGFiZWxzJTIyJTVEJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0RkYXRhc2V0JTVCMCU1RCU1QiUyMnRleHQlMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5pbnB1dF9pZHMlMEElMEElMjMlMjBjb21wdXRlJTIwbG9zcyUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Wav2Vec2ConformerForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForCTC.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription[<span class="hljs-number">0</span>]
...

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(text=dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function Bn(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function Sn(C){let o,y="Example of single-label classification:",i,c,M;return c=new Oe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMFdhdjJWZWMyQ29uZm9ybWVyRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBXYXYyVmVjMkNvbmZvcm1lckZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBcHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heCgpLml0ZW0oKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RCUwQSUwQSUyMyUyMFRvJTIwdHJhaW4lMjBhJTIwbW9kZWwlMjBvbiUyMCU2MG51bV9sYWJlbHMlNjAlMjBjbGFzc2VzJTJDJTIweW91JTIwY2FuJTIwcGFzcyUyMCU2MG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTYwJTIwdG8lMjAlNjAuZnJvbV9wcmV0cmFpbmVkKC4uLiklNjAlMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKG1vZGVsLmNvbmZpZy5pZDJsYWJlbCklMEFtb2RlbCUyMCUzRCUyMFdhdjJWZWMyQ29uZm9ybWVyRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1jb25mb3JtZXItcmVsLXBvcy1sYXJnZSUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzKSUwQSUwQWxhYmVscyUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIxJTVEKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, Wav2Vec2ConformerForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
...

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-ykxpe4"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function Hn(C){let o,y="Example of multi-label classification:",i,c,M;return c=new Oe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMFdhdjJWZWMyQ29uZm9ybWVyRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBXYXYyVmVjMkNvbmZvcm1lckZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIlMkMlMjBwcm9ibGVtX3R5cGUlM0QlMjJtdWx0aV9sYWJlbF9jbGFzc2lmaWNhdGlvbiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJhbmdlKDAlMkMlMjBsb2dpdHMuc2hhcGUlNUItMSU1RCklNUJ0b3JjaC5zaWdtb2lkKGxvZ2l0cykuc3F1ZWV6ZShkaW0lM0QwKSUyMCUzRSUyMDAuNSU1RCUwQSUwQSUyMyUyMFRvJTIwdHJhaW4lMjBhJTIwbW9kZWwlMjBvbiUyMCU2MG51bV9sYWJlbHMlNjAlMjBjbGFzc2VzJTJDJTIweW91JTIwY2FuJTIwcGFzcyUyMCU2MG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTYwJTIwdG8lMjAlNjAuZnJvbV9wcmV0cmFpbmVkKC4uLiklNjAlMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKG1vZGVsLmNvbmZpZy5pZDJsYWJlbCklMEFtb2RlbCUyMCUzRCUyMFdhdjJWZWMyQ29uZm9ybWVyRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1jb25mb3JtZXItcmVsLXBvcy1sYXJnZSUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTJDJTIwcHJvYmxlbV90eXBlJTNEJTIybXVsdGlfbGFiZWxfY2xhc3NpZmljYXRpb24lMjIlMEEpJTBBJTBBbGFiZWxzJTIwJTNEJTIwdG9yY2guc3VtKCUwQSUyMCUyMCUyMCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwub25lX2hvdChwcmVkaWN0ZWRfY2xhc3NfaWRzJTVCTm9uZSUyQyUyMCUzQSU1RC5jbG9uZSgpJTJDJTIwbnVtX2NsYXNzZXMlM0RudW1fbGFiZWxzKSUyQyUyMGRpbSUzRDElMEEpLnRvKHRvcmNoLmZsb2F0KSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, Wav2Vec2ConformerForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.arange(<span class="hljs-number">0</span>, logits.shape[-<span class="hljs-number">1</span>])[torch.sigmoid(logits).squeeze(dim=<span class="hljs-number">0</span>) &gt; <span class="hljs-number">0.5</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.<span class="hljs-built_in">sum</span>(
<span class="hljs-meta">... </span>    torch.nn.functional.one_hot(predicted_class_ids[<span class="hljs-literal">None</span>, :].clone(), num_classes=num_labels), dim=<span class="hljs-number">1</span>
<span class="hljs-meta">... </span>).to(torch.<span class="hljs-built_in">float</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-1l8e32d"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function Qn(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function En(C){let o,y="Example:",i,c,M;return c=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwV2F2MlZlYzJDb25mb3JtZXJGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kZW1vJTIyJTJDJTIwJTIyY2xlYW4lMjIlMkMlMjBzcGxpdCUzRCUyMnZhbGlkYXRpb24lMjIpJTBBZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuc29ydCglMjJpZCUyMiklMEFzYW1wbGluZ19yYXRlJTIwJTNEJTIwZGF0YXNldC5mZWF0dXJlcyU1QiUyMmF1ZGlvJTIyJTVELnNhbXBsaW5nX3JhdGUlMEElMEFmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWNvbmZvcm1lci1yZWwtcG9zLWxhcmdlJTIyKSUwQW1vZGVsJTIwJTNEJTIwV2F2MlZlYzJDb25mb3JtZXJGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItY29uZm9ybWVyLXJlbC1wb3MtbGFyZ2UlMjIpJTBBJTBBJTIzJTIwYXVkaW8lMjBmaWxlJTIwaXMlMjBkZWNvZGVkJTIwb24lMjB0aGUlMjBmbHklMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRHNhbXBsaW5nX3JhdGUpJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcm9iYWJpbGl0aWVzJTIwJTNEJTIwdG9yY2guc2lnbW9pZChsb2dpdHMlNUIwJTVEKSUwQSUyMyUyMGxhYmVscyUyMGlzJTIwYSUyMG9uZS1ob3QlMjBhcnJheSUyMG9mJTIwc2hhcGUlMjAobnVtX2ZyYW1lcyUyQyUyMG51bV9zcGVha2VycyklMEFsYWJlbHMlMjAlM0QlMjAocHJvYmFiaWxpdGllcyUyMCUzRSUyMDAuNSkubG9uZygpJTBBbGFiZWxzJTVCMCU1RC50b2xpc3QoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, Wav2Vec2ConformerForAudioFrameClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, sampling_rate=sampling_rate)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.sigmoid(logits[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># labels is a one-hot array of shape (num_frames, num_speakers)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = (probabilities &gt; <span class="hljs-number">0.5</span>).long()
<span class="hljs-meta">&gt;&gt;&gt; </span>labels[<span class="hljs-number">0</span>].tolist()
...`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function Ln(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function Pn(C){let o,y="Example:",i,c,M;return c=new Oe({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, Wav2Vec2ConformerForXVector
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForXVector.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-conformer-rel-pos-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(
<span class="hljs-meta">... </span>    [d[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dataset[:<span class="hljs-number">2</span>][<span class="hljs-string">&quot;audio&quot;</span>]], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    embeddings = model(**inputs).embeddings

<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = torch.nn.functional.normalize(embeddings, dim=-<span class="hljs-number">1</span>).cpu()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the resulting embeddings can be used for cosine similarity-based retrieval</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim = torch.nn.CosineSimilarity(dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>similarity = cosine_sim(embeddings[<span class="hljs-number">0</span>], embeddings[<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>threshold = <span class="hljs-number">0.7</span>  <span class="hljs-comment"># the optimal threshold is dataset-dependent</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> similarity &lt; threshold:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Speakers are not the same!&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(similarity.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function An(C){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(i){o=p(i,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(i,c){d(i,o,c)},p:N,d(i){i&&a(o)}}}function Dn(C){let o,y="Example:",i,c,M;return c=new Oe({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, Wav2Vec2ConformerForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer <span class="hljs-keyword">import</span> _compute_mask_indices, _sample_negative_indices
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2_conformer-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ConformerForPreTraining.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2_conformer-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = feature_extractor(ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute masked indices</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size, raw_sequence_length = input_values.shape
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_time_indices = _compute_mask_indices(
<span class="hljs-meta">... </span>    shape=(batch_size, sequence_length), mask_prob=<span class="hljs-number">0.2</span>, mask_length=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampled_negative_indices = _sample_negative_indices(
<span class="hljs-meta">... </span>    features_shape=(batch_size, sequence_length),
<span class="hljs-meta">... </span>    num_negatives=model.config.num_negatives,
<span class="hljs-meta">... </span>    mask_time_indices=mask_time_indices,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampled_negative_indices = torch.tensor(
<span class="hljs-meta">... </span>    data=sampled_negative_indices, device=input_values.device, dtype=torch.long
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(input_values, mask_time_indices=mask_time_indices)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># show that cosine similarity is much higher than random</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim[mask_time_indices.to(torch.<span class="hljs-built_in">bool</span>)].mean() &gt; <span class="hljs-number">0.5</span>
tensor(<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># for contrastive loss training model should be put into train mode</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(
<span class="hljs-meta">... </span>    input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices
<span class="hljs-meta">... </span>).loss`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,i=s(),u(c.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-11lpom8"&&(o.textContent=y),i=r(t),h(c.$$.fragment,t)},m(t,w){d(t,o,w),d(t,i,w),g(c,t,w),M=!0},p:N,i(t){M||(_(c.$$.fragment,t),M=!0)},o(t){v(c.$$.fragment,t),M=!1},d(t){t&&(a(o),a(i)),b(c,t)}}}function On(C){let o,y,i,c,M,t="<em>This model was released on 2020-10-11 and added to Hugging Face Transformers on 2022-05-17.</em>",w,ge,Jo,oe,Dt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Fo,_e,$o,ve,Ot='The Wav2Vec2-Conformer was added to an updated version of <a href="https://huggingface.co/papers/2010.05171" rel="nofollow">fairseq S2T: Fast Speech-to-Text Modeling with fairseq</a> by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.',Uo,be,Kt="The official results of the model can be found in Table 3 and Table 4 of the paper.",Zo,ye,en='The Wav2Vec2-Conformer weights were released by the Meta AI team within the <a href="https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models" rel="nofollow">Fairseq library</a>.',xo,Me,on=`This model was contributed by <a href="https://huggingface.co/patrickvonplaten" rel="nofollow">patrickvonplaten</a>.
The original code can be found <a href="https://github.com/pytorch/fairseq/tree/main/examples/wav2vec" rel="nofollow">here</a>.`,zo,we,tn='Note: Meta (FAIR) released a new version of <a href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert" rel="nofollow">Wav2Vec2-BERT 2.0</a> - itâ€™s pretrained on 4.5M hours of audio. We especially recommend using it for fine-tuning tasks, e.g. as per <a href="https://huggingface.co/blog/fine-tune-w2v2-bert" rel="nofollow">this guide</a>.',Xo,Ce,Go,Te,nn=`<li>Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the <em>Attention</em>-block with a <em>Conformer</em>-block
as introduced in <a href="https://huggingface.co/papers/2005.08100" rel="nofollow">Conformer: Convolution-augmented Transformer for Speech Recognition</a>.</li> <li>For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields
an improved word error rate.</li> <li>Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.</li> <li>Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or
rotary position embeddings by setting the correct <code>config.position_embeddings_type</code>.</li>`,Ro,We,Yo,Ve,an='<li><a href="../tasks/audio_classification">Audio classification task guide</a></li> <li><a href="../tasks/asr">Automatic speech recognition task guide</a></li>',Io,je,qo,x,ke,st,Ke,sn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a>. It is used to
instantiate an Wav2Vec2Conformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Wav2Vec2Conformer
<a href="https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large" rel="nofollow">facebook/wav2vec2-conformer-rel-pos-large</a>
architecture.`,rt,eo,rn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,lt,te,No,Je,Bo,K,Fe,it,oo,ln='Output type of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a>, with potential hidden states and attentions.',So,$e,Ho,V,Ue,ct,to,cn="The bare Wav2Vec2 Conformer Model outputting raw hidden-states without any specific head on top.",dt,no,dn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mt,ao,mn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pt,O,Ze,ft,so,pn='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a> forward method, overrides the <code>__call__</code> special method.',ut,ne,Qo,xe,Eo,j,ze,ht,ro,fn="Wav2Vec2Conformer Model with a <code>language modeling</code> head on top for Connectionist Temporal Classification (CTC).",gt,lo,un=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,_t,io,hn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,vt,B,Xe,bt,co,gn='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a> forward method, overrides the <code>__call__</code> special method.',yt,ae,Mt,se,Lo,Ge,Po,k,Re,wt,mo,_n=`Wav2Vec2Conformer Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`,Ct,po,vn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Tt,fo,bn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Wt,U,Ye,Vt,uo,yn='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',jt,re,kt,le,Jt,ie,Ao,Ie,Do,J,qe,Ft,ho,Mn="The Wav2Vec2 Conformer Model with a frame classification head on top for tasks like Speaker Diarization.",$t,go,wn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ut,_o,Cn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Zt,S,Ne,xt,vo,Tn='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a> forward method, overrides the <code>__call__</code> special method.',zt,ce,Xt,de,Oo,Be,Ko,F,Se,Gt,bo,Wn="Wav2Vec2Conformer Model with an XVector feature extraction head on top for tasks like Speaker Verification.",Rt,yo,Vn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Yt,Mo,jn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,It,H,He,qt,wo,kn='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a> forward method, overrides the <code>__call__</code> special method.',Nt,me,Bt,pe,et,Qe,ot,$,Ee,St,Co,Jn="Wav2Vec2Conformer Model with a quantizer and <code>VQ</code> head on top.",Ht,To,Fn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Qt,Wo,$n=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Et,Q,Le,Lt,Vo,Un='The <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a> forward method, overrides the <code>__call__</code> special method.',Pt,fe,At,ue,tt,Pe,nt,ko,at;return ge=new E({props:{title:"Wav2Vec2-Conformer",local:"wav2vec2-conformer",headingTag:"h1"}}),_e=new E({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ce=new E({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),We=new E({props:{title:"Resources",local:"resources",headingTag:"h2"}}),je=new E({props:{title:"Wav2Vec2ConformerConfig",local:"transformers.Wav2Vec2ConformerConfig",headingTag:"h2"}}),ke=new Z({props:{name:"class transformers.Wav2Vec2ConformerConfig",anchor:"transformers.Wav2Vec2ConformerConfig",parameters:[{name:"vocab_size",val:" = None"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"feat_proj_dropout",val:" = 0.0"},{name:"feat_quantizer_dropout",val:" = 0.0"},{name:"final_dropout",val:" = 0.1"},{name:"layerdrop",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"feat_extract_norm",val:" = 'group'"},{name:"feat_extract_activation",val:" = 'gelu'"},{name:"conv_dim",val:" = (512, 512, 512, 512, 512, 512, 512)"},{name:"conv_stride",val:" = (5, 2, 2, 2, 2, 2, 2)"},{name:"conv_kernel",val:" = (10, 3, 3, 3, 3, 2, 2)"},{name:"conv_bias",val:" = False"},{name:"num_conv_pos_embeddings",val:" = 128"},{name:"num_conv_pos_embedding_groups",val:" = 16"},{name:"apply_spec_augment",val:" = True"},{name:"mask_time_prob",val:" = 0.05"},{name:"mask_time_length",val:" = 10"},{name:"mask_time_min_masks",val:" = 2"},{name:"mask_feature_prob",val:" = 0.0"},{name:"mask_feature_length",val:" = 10"},{name:"mask_feature_min_masks",val:" = 0"},{name:"num_codevectors_per_group",val:" = 320"},{name:"num_codevector_groups",val:" = 2"},{name:"contrastive_logits_temperature",val:" = 0.1"},{name:"num_negatives",val:" = 100"},{name:"codevector_dim",val:" = 256"},{name:"proj_codevector_dim",val:" = 256"},{name:"diversity_loss_weight",val:" = 0.1"},{name:"ctc_loss_reduction",val:" = 'sum'"},{name:"ctc_zero_infinity",val:" = False"},{name:"use_weighted_layer_sum",val:" = False"},{name:"classifier_proj_size",val:" = 256"},{name:"tdnn_dim",val:" = (512, 512, 512, 512, 1500)"},{name:"tdnn_kernel",val:" = (5, 3, 3, 1, 1)"},{name:"tdnn_dilation",val:" = (1, 2, 3, 1, 1)"},{name:"xvector_output_dim",val:" = 512"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"add_adapter",val:" = False"},{name:"adapter_kernel_size",val:" = 3"},{name:"adapter_stride",val:" = 2"},{name:"num_adapter_layers",val:" = 3"},{name:"output_hidden_size",val:" = None"},{name:"position_embeddings_type",val:" = 'relative'"},{name:"rotary_embedding_base",val:" = 10000"},{name:"max_source_positions",val:" = 5000"},{name:"conv_depthwise_kernel_size",val:" = 31"},{name:"conformer_conv_dropout",val:" = 0.1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Vocabulary size of the Wav2Vec2Conformer model. Defines the number of different tokens that can be
represented by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a>. Vocabulary size of the
model. Defines the different tokens that can be represented by the <em>inputs_ids</em> passed to the forward
method of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a>.`,name:"vocab_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Wav2Vec2ConformerConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Wav2Vec2ConformerConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.final_dropout",description:`<strong>final_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the final projection layer of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a>.`,name:"final_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.layerdrop",description:`<strong>layerdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The LayerDrop probability. See the [LayerDrop paper](see <a href="https://huggingface.co/papers/1909.11556" rel="nofollow">https://huggingface.co/papers/1909.11556</a>) for more
details.`,name:"layerdrop"},{anchor:"transformers.Wav2Vec2ConformerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Wav2Vec2ConformerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Wav2Vec2ConformerConfig.feat_extract_norm",description:`<strong>feat_extract_norm</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;group&quot;</code>) &#x2014;
The norm to be applied to 1D convolutional layers in feature encoder. One of <code>&quot;group&quot;</code> for group
normalization of only the first 1D convolutional layer or <code>&quot;layer&quot;</code> for layer normalization of all 1D
convolutional layers.`,name:"feat_extract_norm"},{anchor:"transformers.Wav2Vec2ConformerConfig.feat_proj_dropout",description:`<strong>feat_proj_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for output of the feature encoder.`,name:"feat_proj_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.feat_extract_activation",description:"<strong>feat_extract_activation</strong> (<code>str, </code>optional<code>, defaults to </code>&#x201C;gelu&#x201D;<code>) -- The non-linear activation function (function or string) in the 1D convolutional layers of the feature extractor. If string, </code>&#x201C;gelu&#x201D;<code>, </code>&#x201C;relu&#x201D;<code>, </code>&#x201C;selu&#x201D;<code>and</code>&#x201C;gelu_new&#x201D;` are supported.",name:"feat_extract_activation"},{anchor:"transformers.Wav2Vec2ConformerConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for quantized feature encoder states.`,name:"feat_quantizer_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.conv_dim",description:`<strong>conv_dim</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(512, 512, 512, 512, 512, 512, 512)</code>) &#x2014;
A tuple of integers defining the number of input and output channels of each 1D convolutional layer in the
feature encoder. The length of <em>conv_dim</em> defines the number of 1D convolutional layers.`,name:"conv_dim"},{anchor:"transformers.Wav2Vec2ConformerConfig.conv_stride",description:`<strong>conv_stride</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(5, 2, 2, 2, 2, 2, 2)</code>) &#x2014;
A tuple of integers defining the stride of each 1D convolutional layer in the feature encoder. The length
of <em>conv_stride</em> defines the number of convolutional layers and has to match the length of <em>conv_dim</em>.`,name:"conv_stride"},{anchor:"transformers.Wav2Vec2ConformerConfig.conv_kernel",description:`<strong>conv_kernel</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(10, 3, 3, 3, 3, 3, 3)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the feature encoder. The
length of <em>conv_kernel</em> defines the number of convolutional layers and has to match the length of
<em>conv_dim</em>.`,name:"conv_kernel"},{anchor:"transformers.Wav2Vec2ConformerConfig.conv_bias",description:`<strong>conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the 1D convolutional layers have a bias.`,name:"conv_bias"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_conv_pos_embeddings",description:`<strong>num_conv_pos_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional
embeddings layer.`,name:"num_conv_pos_embeddings"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_conv_pos_embedding_groups",description:`<strong>num_conv_pos_embedding_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of groups of 1D convolutional positional embeddings layer.`,name:"num_conv_pos_embedding_groups"},{anchor:"transformers.Wav2Vec2ConformerConfig.apply_spec_augment",description:`<strong>apply_spec_augment</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply <em>SpecAugment</em> data augmentation to the outputs of the feature encoder. For reference see
<a href="https://huggingface.co/papers/1904.08779" rel="nofollow">SpecAugment: A Simple Data Augmentation Method for Automatic Speech
Recognition</a>.`,name:"apply_spec_augment"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_time_prob",description:`<strong>mask_time_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking
procedure generates &#x201D;mask_time_prob<em>len(time_axis)/mask_time_length&#x201D; independent masks over the axis. If
reasoning from the probability of each feature vector to be chosen as the start of the vector span to be
masked, </em>mask_time_prob<em> should be \`prob_vector_start</em>mask_time_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is True\`.`,name:"mask_time_prob"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_time_length",description:`<strong>mask_time_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the time axis.`,name:"mask_time_length"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_time_min_masks",description:`<strong>mask_time_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 2), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the time axis, each time step,
irrespectively of <code>mask_feature_prob</code>. Only relevant if &#x201D;mask_time_prob*len(time_axis)/mask_time_length &lt;
mask_time_min_masks&#x201D;`,name:"mask_time_min_masks"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_feature_prob",description:`<strong>mask_feature_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The
masking procedure generates &#x201D;mask_feature_prob<em>len(feature_axis)/mask_time_length&#x201D; independent masks over
the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector
span to be masked, </em>mask_feature_prob<em> should be \`prob_vector_start</em>mask_feature_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is
True\`.`,name:"mask_feature_prob"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_feature_length",description:`<strong>mask_feature_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the feature axis.`,name:"mask_feature_length"},{anchor:"transformers.Wav2Vec2ConformerConfig.mask_feature_min_masks",description:`<strong>mask_feature_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 0), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the feature axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_feature_prob*len(feature_axis)/mask_feature_length &lt; mask_feature_min_masks&#x201D;`,name:"mask_feature_min_masks"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_codevectors_per_group",description:`<strong>num_codevectors_per_group</strong> (<code>int</code>, <em>optional</em>, defaults to 320) &#x2014;
Number of entries in each quantization codebook (group).`,name:"num_codevectors_per_group"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_codevector_groups",description:`<strong>num_codevector_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of codevector groups for product codevector quantization.`,name:"num_codevector_groups"},{anchor:"transformers.Wav2Vec2ConformerConfig.contrastive_logits_temperature",description:`<strong>contrastive_logits_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The temperature <em>kappa</em> in the contrastive loss.`,name:"contrastive_logits_temperature"},{anchor:"transformers.Wav2Vec2ConformerConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for the output of the feature encoder that&#x2019;s used by the quantizer.`,name:"feat_quantizer_dropout"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_negatives",description:`<strong>num_negatives</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Number of negative samples for the contrastive loss.`,name:"num_negatives"},{anchor:"transformers.Wav2Vec2ConformerConfig.codevector_dim",description:`<strong>codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the quantized feature vectors.`,name:"codevector_dim"},{anchor:"transformers.Wav2Vec2ConformerConfig.proj_codevector_dim",description:`<strong>proj_codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the final projection of both the quantized and the transformer features.`,name:"proj_codevector_dim"},{anchor:"transformers.Wav2Vec2ConformerConfig.diversity_loss_weight",description:`<strong>diversity_loss_weight</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The weight of the codebook diversity loss component.`,name:"diversity_loss_weight"},{anchor:"transformers.Wav2Vec2ConformerConfig.ctc_loss_reduction",description:`<strong>ctc_loss_reduction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;sum&quot;</code>) &#x2014;
Specifies the reduction to apply to the output of <code>torch.nn.CTCLoss</code>. Only relevant when training an
instance of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a>.`,name:"ctc_loss_reduction"},{anchor:"transformers.Wav2Vec2ConformerConfig.ctc_zero_infinity",description:`<strong>ctc_zero_infinity</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to zero infinite losses and the associated gradients of <code>torch.nn.CTCLoss</code>. Infinite losses mainly
occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance
of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a>.`,name:"ctc_zero_infinity"},{anchor:"transformers.Wav2Vec2ConformerConfig.use_weighted_layer_sum",description:`<strong>use_weighted_layer_sum</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an
instance of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a>.`,name:"use_weighted_layer_sum"},{anchor:"transformers.Wav2Vec2ConformerConfig.classifier_proj_size",description:`<strong>classifier_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the projection before token mean-pooling for classification.`,name:"classifier_proj_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.tdnn_dim",description:`<strong>tdnn_dim</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(512, 512, 512, 512, 1500)</code>) &#x2014;
A tuple of integers defining the number of output channels of each 1D convolutional layer in the <em>TDNN</em>
module of the <em>XVector</em> model. The length of <em>tdnn_dim</em> defines the number of <em>TDNN</em> layers.`,name:"tdnn_dim"},{anchor:"transformers.Wav2Vec2ConformerConfig.tdnn_kernel",description:`<strong>tdnn_kernel</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(5, 3, 3, 1, 1)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the <em>TDNN</em> module of the
<em>XVector</em> model. The length of <em>tdnn_kernel</em> has to match the length of <em>tdnn_dim</em>.`,name:"tdnn_kernel"},{anchor:"transformers.Wav2Vec2ConformerConfig.tdnn_dilation",description:`<strong>tdnn_dilation</strong> (<code>tuple[int]</code> or <code>list[int]</code>, <em>optional</em>, defaults to <code>(1, 2, 3, 1, 1)</code>) &#x2014;
A tuple of integers defining the dilation factor of each 1D convolutional layer in <em>TDNN</em> module of the
<em>XVector</em> model. The length of <em>tdnn_dilation</em> has to match the length of <em>tdnn_dim</em>.`,name:"tdnn_dilation"},{anchor:"transformers.Wav2Vec2ConformerConfig.xvector_output_dim",description:`<strong>xvector_output_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the <em>XVector</em> embedding vectors.`,name:"xvector_output_dim"},{anchor:"transformers.Wav2Vec2ConformerConfig.add_adapter",description:`<strong>add_adapter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether a convolutional network should be stacked on top of the Wav2Vec2Conformer Encoder. Can be very
useful for warm-starting Wav2Vec2Conformer for SpeechEncoderDecoder models.`,name:"add_adapter"},{anchor:"transformers.Wav2Vec2ConformerConfig.adapter_kernel_size",description:`<strong>adapter_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.`,name:"adapter_kernel_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.adapter_stride",description:`<strong>adapter_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Stride of the convolutional layers in the adapter network. Only relevant if <code>add_adapter is True</code>.`,name:"adapter_stride"},{anchor:"transformers.Wav2Vec2ConformerConfig.num_adapter_layers",description:`<strong>num_adapter_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of convolutional layers that should be used in the adapter network. Only relevant if <code>add_adapter is True</code>.`,name:"num_adapter_layers"},{anchor:"transformers.Wav2Vec2ConformerConfig.output_hidden_size",description:`<strong>output_hidden_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dimensionality of the encoder output layer. If not defined, this defaults to <em>hidden-size</em>. Only relevant
if <code>add_adapter is True</code>.`,name:"output_hidden_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.position_embeddings_type",description:`<strong>position_embeddings_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative&quot;</code>) &#x2014;
Can be specified to <code>relative</code> or <code>rotary</code> for relative or rotary position embeddings respectively. If left
<code>None</code> no relative position embedding is applied.`,name:"position_embeddings_type"},{anchor:"transformers.Wav2Vec2ConformerConfig.rotary_embedding_base",description:`<strong>rotary_embedding_base</strong> (<code>int</code>, <em>optional</em>, defaults to 10000) &#x2014;
If <code>&quot;rotary&quot;</code> position embeddings are used, defines the size of the embedding base.`,name:"rotary_embedding_base"},{anchor:"transformers.Wav2Vec2ConformerConfig.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
if <code>&quot;relative&quot;</code> position embeddings are used, defines the maximum source input positions.`,name:"max_source_positions"},{anchor:"transformers.Wav2Vec2ConformerConfig.conv_depthwise_kernel_size",description:`<strong>conv_depthwise_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 31) &#x2014;
Kernel size of convolutional depthwise 1D layer in Conformer blocks.`,name:"conv_depthwise_kernel_size"},{anchor:"transformers.Wav2Vec2ConformerConfig.conformer_conv_dropout",description:`<strong>conformer_conv_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all convolutional layers in Conformer blocks.`,name:"conformer_conv_dropout"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/configuration_wav2vec2_conformer.py#L27"}}),te=new De({props:{anchor:"transformers.Wav2Vec2ConformerConfig.example",$$slots:{default:[Yn]},$$scope:{ctx:C}}}),Je=new E({props:{title:"Wav2Vec2Conformer specific outputs",local:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput",headingTag:"h2"}}),Fe=new Z({props:{name:"class transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput",anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_quantized_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"codevector_perplexity",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"contrastive_loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"diversity_loss",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.loss",description:`<strong>loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a href="https://huggingface.co/papers/2006.11477" rel="nofollow">official
paper</a>.`,name:"loss"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.projected_states",description:`<strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.`,name:"projected_states"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.projected_quantized_states",description:`<strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.`,name:"projected_quantized_states"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.codevector_perplexity",description:`<strong>codevector_perplexity</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
The perplexity of the codevector distribution, used to measure the diversity of the codebook.`,name:"codevector_perplexity"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.contrastive_loss",description:`<strong>contrastive_loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
The contrastive loss (L_m) as stated in the <a href="https://huggingface.co/papers/2006.11477" rel="nofollow">official paper</a>.`,name:"contrastive_loss"},{anchor:"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput.diversity_loss",description:`<strong>diversity_loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
The diversity loss (L_d) as stated in the <a href="https://huggingface.co/papers/2006.11477" rel="nofollow">official paper</a>.`,name:"diversity_loss"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L44"}}),$e=new E({props:{title:"Wav2Vec2ConformerModel",local:"transformers.Wav2Vec2ConformerModel",headingTag:"h2"}}),Ue=new Z({props:{name:"class transformers.Wav2Vec2ConformerModel",anchor:"transformers.Wav2Vec2ConformerModel",parameters:[{name:"config",val:": Wav2Vec2ConformerConfig"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1070"}}),Ze=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerModel.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"mask_time_indices",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__">Wav2Vec2Processor.<strong>call</strong>()</a> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerModel.forward.mask_time_indices",description:`<strong>mask_time_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
masked extracted features in <em>config.proj_codevector_dim</em> space.`,name:"mask_time_indices"},{anchor:"transformers.Wav2Vec2ConformerModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1141",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.modeling_outputs.Wav2Vec2BaseModelOutput"
>transformers.modeling_outputs.Wav2Vec2BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) â€” Sequence of extracted feature vectors of the last convolutional layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.modeling_outputs.Wav2Vec2BaseModelOutput"
>transformers.modeling_outputs.Wav2Vec2BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ne=new jo({props:{$$slots:{default:[In]},$$scope:{ctx:C}}}),xe=new E({props:{title:"Wav2Vec2ConformerForCTC",local:"transformers.Wav2Vec2ConformerForCTC",headingTag:"h2"}}),ze=new Z({props:{name:"class transformers.Wav2Vec2ConformerForCTC",anchor:"transformers.Wav2Vec2ConformerForCTC",parameters:[{name:"config",val:""},{name:"target_lang",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForCTC.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.Wav2Vec2ConformerForCTC.target_lang",description:`<strong>target_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or
adapter.<lang>.bin. Only relevant when using an instance of <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> with adapters. Uses &#x2018;eng&#x2019; by
default.</lang></lang>`,name:"target_lang"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1421"}}),Xe=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerForCTC.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__">Wav2Vec2Processor.<strong>call</strong>()</a> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_length)</code>, <em>optional</em>) &#x2014;
Labels for connectionist temporal classification. Note that <code>target_length</code> has to be smaller or equal to
the sequence length of the output logits. Indices are selected in <code>[-100, 0, ..., config.vocab_size - 1]</code>.
All labels set to <code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1458",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ae=new jo({props:{$$slots:{default:[qn]},$$scope:{ctx:C}}}),se=new De({props:{anchor:"transformers.Wav2Vec2ConformerForCTC.forward.example",$$slots:{default:[Nn]},$$scope:{ctx:C}}}),Ge=new E({props:{title:"Wav2Vec2ConformerForSequenceClassification",local:"transformers.Wav2Vec2ConformerForSequenceClassification",headingTag:"h2"}}),Re=new Z({props:{name:"class transformers.Wav2Vec2ConformerForSequenceClassification",anchor:"transformers.Wav2Vec2ConformerForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1536"}}),Ye=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <code>Wav2Vec2ConformerProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1569",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new jo({props:{$$slots:{default:[Bn]},$$scope:{ctx:C}}}),le=new De({props:{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.example",$$slots:{default:[Sn]},$$scope:{ctx:C}}}),ie=new De({props:{anchor:"transformers.Wav2Vec2ConformerForSequenceClassification.forward.example-2",$$slots:{default:[Hn]},$$scope:{ctx:C}}}),Ie=new E({props:{title:"Wav2Vec2ConformerForAudioFrameClassification",local:"transformers.Wav2Vec2ConformerForAudioFrameClassification",headingTag:"h2"}}),qe=new Z({props:{name:"class transformers.Wav2Vec2ConformerForAudioFrameClassification",anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1640"}}),Ne=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <code>Wav2Vec2ConformerProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1672",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  â€” Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) â€” Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ce=new jo({props:{$$slots:{default:[Qn]},$$scope:{ctx:C}}}),de=new De({props:{anchor:"transformers.Wav2Vec2ConformerForAudioFrameClassification.forward.example",$$slots:{default:[En]},$$scope:{ctx:C}}}),Be=new E({props:{title:"Wav2Vec2ConformerForXVector",local:"transformers.Wav2Vec2ConformerForXVector",headingTag:"h2"}}),Se=new Z({props:{name:"class transformers.Wav2Vec2ConformerForXVector",anchor:"transformers.Wav2Vec2ConformerForXVector",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForXVector.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1793"}}),He=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerForXVector.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <code>Wav2Vec2ConformerProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1843",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput"
>transformers.modeling_outputs.XVectorOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) â€” Classification hidden states before AMSoftmax.</p>
</li>
<li>
<p><strong>embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) â€” Utterance embeddings used for vector similarity-based retrieval.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput"
>transformers.modeling_outputs.XVectorOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),me=new jo({props:{$$slots:{default:[Ln]},$$scope:{ctx:C}}}),pe=new De({props:{anchor:"transformers.Wav2Vec2ConformerForXVector.forward.example",$$slots:{default:[Pn]},$$scope:{ctx:C}}}),Qe=new E({props:{title:"Wav2Vec2ConformerForPreTraining",local:"transformers.Wav2Vec2ConformerForPreTraining",headingTag:"h2"}}),Ee=new Z({props:{name:"class transformers.Wav2Vec2ConformerForPreTraining",anchor:"transformers.Wav2Vec2ConformerForPreTraining",parameters:[{name:"config",val:": Wav2Vec2ConformerConfig"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1205"}}),Le=new Z({props:{name:"forward",anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor]"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"mask_time_indices",val:": typing.Optional[torch.BoolTensor] = None"},{name:"sampled_negative_indices",val:": typing.Optional[torch.BoolTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file
into an array of type <code>list[float]</code>, a <code>numpy.ndarray</code> or a <code>torch.Tensor</code>, <em>e.g.</em> via the torchcodec library
(<code>pip install torchcodec</code>) or the soundfile library (<code>pip install soundfile</code>).
To prepare the array into <code>input_values</code>, the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> should be used for padding and conversion
into a tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__">Wav2Vec2Processor.<strong>call</strong>()</a> for details.`,name:"input_values"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.mask_time_indices",description:`<strong>mask_time_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
masked extracted features in <em>config.proj_codevector_dim</em> space.`,name:"mask_time_indices"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.sampled_negative_indices",description:`<strong>sampled_negative_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length, num_negatives)</code>, <em>optional</em>) &#x2014;
Indices indicating which quantized target vectors are used as negative sampled vectors in contrastive loss.
Required input for pre-training.`,name:"sampled_negative_indices"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py#L1253",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput"
>transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig"
>Wav2Vec2ConformerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a
  href="https://huggingface.co/papers/2006.11477"
  rel="nofollow"
>official
paper</a>.</p>
</li>
<li>
<p><strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) â€” Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.</p>
</li>
<li>
<p><strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) â€” Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.</p>
</li>
<li>
<p><strong>codevector_perplexity</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” The perplexity of the codevector distribution, used to measure the diversity of the codebook.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>contrastive_loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” The contrastive loss (L_m) as stated in the <a
  href="https://huggingface.co/papers/2006.11477"
  rel="nofollow"
>official paper</a>.</p>
</li>
<li>
<p><strong>diversity_loss</strong> (<code>*optional*</code>, returned when <code>sample_negative_indices</code> are passed, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” The diversity loss (L_d) as stated in the <a
  href="https://huggingface.co/papers/2006.11477"
  rel="nofollow"
>official paper</a>.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput"
>transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new jo({props:{$$slots:{default:[An]},$$scope:{ctx:C}}}),ue=new De({props:{anchor:"transformers.Wav2Vec2ConformerForPreTraining.forward.example",$$slots:{default:[Dn]},$$scope:{ctx:C}}}),Pe=new Rn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2-conformer.md"}}),{c(){o=m("meta"),y=s(),i=m("p"),c=s(),M=m("p"),M.innerHTML=t,w=s(),u(ge.$$.fragment),Jo=s(),oe=m("div"),oe.innerHTML=Dt,Fo=s(),u(_e.$$.fragment),$o=s(),ve=m("p"),ve.innerHTML=Ot,Uo=s(),be=m("p"),be.textContent=Kt,Zo=s(),ye=m("p"),ye.innerHTML=en,xo=s(),Me=m("p"),Me.innerHTML=on,zo=s(),we=m("p"),we.innerHTML=tn,Xo=s(),u(Ce.$$.fragment),Go=s(),Te=m("ul"),Te.innerHTML=nn,Ro=s(),u(We.$$.fragment),Yo=s(),Ve=m("ul"),Ve.innerHTML=an,Io=s(),u(je.$$.fragment),qo=s(),x=m("div"),u(ke.$$.fragment),st=s(),Ke=m("p"),Ke.innerHTML=sn,rt=s(),eo=m("p"),eo.innerHTML=rn,lt=s(),u(te.$$.fragment),No=s(),u(Je.$$.fragment),Bo=s(),K=m("div"),u(Fe.$$.fragment),it=s(),oo=m("p"),oo.innerHTML=ln,So=s(),u($e.$$.fragment),Ho=s(),V=m("div"),u(Ue.$$.fragment),ct=s(),to=m("p"),to.textContent=cn,dt=s(),no=m("p"),no.innerHTML=dn,mt=s(),ao=m("p"),ao.innerHTML=mn,pt=s(),O=m("div"),u(Ze.$$.fragment),ft=s(),so=m("p"),so.innerHTML=pn,ut=s(),u(ne.$$.fragment),Qo=s(),u(xe.$$.fragment),Eo=s(),j=m("div"),u(ze.$$.fragment),ht=s(),ro=m("p"),ro.innerHTML=fn,gt=s(),lo=m("p"),lo.innerHTML=un,_t=s(),io=m("p"),io.innerHTML=hn,vt=s(),B=m("div"),u(Xe.$$.fragment),bt=s(),co=m("p"),co.innerHTML=gn,yt=s(),u(ae.$$.fragment),Mt=s(),u(se.$$.fragment),Lo=s(),u(Ge.$$.fragment),Po=s(),k=m("div"),u(Re.$$.fragment),wt=s(),mo=m("p"),mo.textContent=_n,Ct=s(),po=m("p"),po.innerHTML=vn,Tt=s(),fo=m("p"),fo.innerHTML=bn,Wt=s(),U=m("div"),u(Ye.$$.fragment),Vt=s(),uo=m("p"),uo.innerHTML=yn,jt=s(),u(re.$$.fragment),kt=s(),u(le.$$.fragment),Jt=s(),u(ie.$$.fragment),Ao=s(),u(Ie.$$.fragment),Do=s(),J=m("div"),u(qe.$$.fragment),Ft=s(),ho=m("p"),ho.textContent=Mn,$t=s(),go=m("p"),go.innerHTML=wn,Ut=s(),_o=m("p"),_o.innerHTML=Cn,Zt=s(),S=m("div"),u(Ne.$$.fragment),xt=s(),vo=m("p"),vo.innerHTML=Tn,zt=s(),u(ce.$$.fragment),Xt=s(),u(de.$$.fragment),Oo=s(),u(Be.$$.fragment),Ko=s(),F=m("div"),u(Se.$$.fragment),Gt=s(),bo=m("p"),bo.textContent=Wn,Rt=s(),yo=m("p"),yo.innerHTML=Vn,Yt=s(),Mo=m("p"),Mo.innerHTML=jn,It=s(),H=m("div"),u(He.$$.fragment),qt=s(),wo=m("p"),wo.innerHTML=kn,Nt=s(),u(me.$$.fragment),Bt=s(),u(pe.$$.fragment),et=s(),u(Qe.$$.fragment),ot=s(),$=m("div"),u(Ee.$$.fragment),St=s(),Co=m("p"),Co.innerHTML=Jn,Ht=s(),To=m("p"),To.innerHTML=Fn,Qt=s(),Wo=m("p"),Wo.innerHTML=$n,Et=s(),Q=m("div"),u(Le.$$.fragment),Lt=s(),Vo=m("p"),Vo.innerHTML=Un,Pt=s(),u(fe.$$.fragment),At=s(),u(ue.$$.fragment),tt=s(),u(Pe.$$.fragment),nt=s(),ko=m("p"),this.h()},l(e){const n=Gn("svelte-u9bgzb",document.head);o=p(n,"META",{name:!0,content:!0}),n.forEach(a),y=r(e),i=p(e,"P",{}),W(i).forEach(a),c=r(e),M=p(e,"P",{"data-svelte-h":!0}),f(M)!=="svelte-6ubmay"&&(M.innerHTML=t),w=r(e),h(ge.$$.fragment,e),Jo=r(e),oe=p(e,"DIV",{class:!0,"data-svelte-h":!0}),f(oe)!=="svelte-13t8s2t"&&(oe.innerHTML=Dt),Fo=r(e),h(_e.$$.fragment,e),$o=r(e),ve=p(e,"P",{"data-svelte-h":!0}),f(ve)!=="svelte-17akjle"&&(ve.innerHTML=Ot),Uo=r(e),be=p(e,"P",{"data-svelte-h":!0}),f(be)!=="svelte-5fkfc1"&&(be.textContent=Kt),Zo=r(e),ye=p(e,"P",{"data-svelte-h":!0}),f(ye)!=="svelte-y9fpue"&&(ye.innerHTML=en),xo=r(e),Me=p(e,"P",{"data-svelte-h":!0}),f(Me)!=="svelte-1l6txl9"&&(Me.innerHTML=on),zo=r(e),we=p(e,"P",{"data-svelte-h":!0}),f(we)!=="svelte-1bqdtpc"&&(we.innerHTML=tn),Xo=r(e),h(Ce.$$.fragment,e),Go=r(e),Te=p(e,"UL",{"data-svelte-h":!0}),f(Te)!=="svelte-m3vhjl"&&(Te.innerHTML=nn),Ro=r(e),h(We.$$.fragment,e),Yo=r(e),Ve=p(e,"UL",{"data-svelte-h":!0}),f(Ve)!=="svelte-11qmliz"&&(Ve.innerHTML=an),Io=r(e),h(je.$$.fragment,e),qo=r(e),x=p(e,"DIV",{class:!0});var L=W(x);h(ke.$$.fragment,L),st=r(L),Ke=p(L,"P",{"data-svelte-h":!0}),f(Ke)!=="svelte-sqzzox"&&(Ke.innerHTML=sn),rt=r(L),eo=p(L,"P",{"data-svelte-h":!0}),f(eo)!=="svelte-1ek1ss9"&&(eo.innerHTML=rn),lt=r(L),h(te.$$.fragment,L),L.forEach(a),No=r(e),h(Je.$$.fragment,e),Bo=r(e),K=p(e,"DIV",{class:!0});var Ae=W(K);h(Fe.$$.fragment,Ae),it=r(Ae),oo=p(Ae,"P",{"data-svelte-h":!0}),f(oo)!=="svelte-ce0e39"&&(oo.innerHTML=ln),Ae.forEach(a),So=r(e),h($e.$$.fragment,e),Ho=r(e),V=p(e,"DIV",{class:!0});var z=W(V);h(Ue.$$.fragment,z),ct=r(z),to=p(z,"P",{"data-svelte-h":!0}),f(to)!=="svelte-2n3iz5"&&(to.textContent=cn),dt=r(z),no=p(z,"P",{"data-svelte-h":!0}),f(no)!=="svelte-q52n56"&&(no.innerHTML=dn),mt=r(z),ao=p(z,"P",{"data-svelte-h":!0}),f(ao)!=="svelte-hswkmf"&&(ao.innerHTML=mn),pt=r(z),O=p(z,"DIV",{class:!0});var ee=W(O);h(Ze.$$.fragment,ee),ft=r(ee),so=p(ee,"P",{"data-svelte-h":!0}),f(so)!=="svelte-1kp4cjn"&&(so.innerHTML=pn),ut=r(ee),h(ne.$$.fragment,ee),ee.forEach(a),z.forEach(a),Qo=r(e),h(xe.$$.fragment,e),Eo=r(e),j=p(e,"DIV",{class:!0});var X=W(j);h(ze.$$.fragment,X),ht=r(X),ro=p(X,"P",{"data-svelte-h":!0}),f(ro)!=="svelte-fpzdqh"&&(ro.innerHTML=fn),gt=r(X),lo=p(X,"P",{"data-svelte-h":!0}),f(lo)!=="svelte-q52n56"&&(lo.innerHTML=un),_t=r(X),io=p(X,"P",{"data-svelte-h":!0}),f(io)!=="svelte-hswkmf"&&(io.innerHTML=hn),vt=r(X),B=p(X,"DIV",{class:!0});var P=W(B);h(Xe.$$.fragment,P),bt=r(P),co=p(P,"P",{"data-svelte-h":!0}),f(co)!=="svelte-dhke33"&&(co.innerHTML=gn),yt=r(P),h(ae.$$.fragment,P),Mt=r(P),h(se.$$.fragment,P),P.forEach(a),X.forEach(a),Lo=r(e),h(Ge.$$.fragment,e),Po=r(e),k=p(e,"DIV",{class:!0});var G=W(k);h(Re.$$.fragment,G),wt=r(G),mo=p(G,"P",{"data-svelte-h":!0}),f(mo)!=="svelte-1y0tt3z"&&(mo.textContent=_n),Ct=r(G),po=p(G,"P",{"data-svelte-h":!0}),f(po)!=="svelte-q52n56"&&(po.innerHTML=vn),Tt=r(G),fo=p(G,"P",{"data-svelte-h":!0}),f(fo)!=="svelte-hswkmf"&&(fo.innerHTML=bn),Wt=r(G),U=p(G,"DIV",{class:!0});var R=W(U);h(Ye.$$.fragment,R),Vt=r(R),uo=p(R,"P",{"data-svelte-h":!0}),f(uo)!=="svelte-17lww0l"&&(uo.innerHTML=yn),jt=r(R),h(re.$$.fragment,R),kt=r(R),h(le.$$.fragment,R),Jt=r(R),h(ie.$$.fragment,R),R.forEach(a),G.forEach(a),Ao=r(e),h(Ie.$$.fragment,e),Do=r(e),J=p(e,"DIV",{class:!0});var Y=W(J);h(qe.$$.fragment,Y),Ft=r(Y),ho=p(Y,"P",{"data-svelte-h":!0}),f(ho)!=="svelte-ocxhfm"&&(ho.textContent=Mn),$t=r(Y),go=p(Y,"P",{"data-svelte-h":!0}),f(go)!=="svelte-q52n56"&&(go.innerHTML=wn),Ut=r(Y),_o=p(Y,"P",{"data-svelte-h":!0}),f(_o)!=="svelte-hswkmf"&&(_o.innerHTML=Cn),Zt=r(Y),S=p(Y,"DIV",{class:!0});var A=W(S);h(Ne.$$.fragment,A),xt=r(A),vo=p(A,"P",{"data-svelte-h":!0}),f(vo)!=="svelte-vi4dzx"&&(vo.innerHTML=Tn),zt=r(A),h(ce.$$.fragment,A),Xt=r(A),h(de.$$.fragment,A),A.forEach(a),Y.forEach(a),Oo=r(e),h(Be.$$.fragment,e),Ko=r(e),F=p(e,"DIV",{class:!0});var I=W(F);h(Se.$$.fragment,I),Gt=r(I),bo=p(I,"P",{"data-svelte-h":!0}),f(bo)!=="svelte-1vfzkaz"&&(bo.textContent=Wn),Rt=r(I),yo=p(I,"P",{"data-svelte-h":!0}),f(yo)!=="svelte-q52n56"&&(yo.innerHTML=Vn),Yt=r(I),Mo=p(I,"P",{"data-svelte-h":!0}),f(Mo)!=="svelte-hswkmf"&&(Mo.innerHTML=jn),It=r(I),H=p(I,"DIV",{class:!0});var D=W(H);h(He.$$.fragment,D),qt=r(D),wo=p(D,"P",{"data-svelte-h":!0}),f(wo)!=="svelte-1p4yef"&&(wo.innerHTML=kn),Nt=r(D),h(me.$$.fragment,D),Bt=r(D),h(pe.$$.fragment,D),D.forEach(a),I.forEach(a),et=r(e),h(Qe.$$.fragment,e),ot=r(e),$=p(e,"DIV",{class:!0});var q=W($);h(Ee.$$.fragment,q),St=r(q),Co=p(q,"P",{"data-svelte-h":!0}),f(Co)!=="svelte-ipd06z"&&(Co.innerHTML=Jn),Ht=r(q),To=p(q,"P",{"data-svelte-h":!0}),f(To)!=="svelte-q52n56"&&(To.innerHTML=Fn),Qt=r(q),Wo=p(q,"P",{"data-svelte-h":!0}),f(Wo)!=="svelte-hswkmf"&&(Wo.innerHTML=$n),Et=r(q),Q=p(q,"DIV",{class:!0});var he=W(Q);h(Le.$$.fragment,he),Lt=r(he),Vo=p(he,"P",{"data-svelte-h":!0}),f(Vo)!=="svelte-16j98f"&&(Vo.innerHTML=Un),Pt=r(he),h(fe.$$.fragment,he),At=r(he),h(ue.$$.fragment,he),he.forEach(a),q.forEach(a),tt=r(e),h(Pe.$$.fragment,e),nt=r(e),ko=p(e,"P",{}),W(ko).forEach(a),this.h()},h(){T(o,"name","hf:doc:metadata"),T(o,"content",Kn),T(oe,"class","flex flex-wrap space-x-1"),T(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){l(document.head,o),d(e,y,n),d(e,i,n),d(e,c,n),d(e,M,n),d(e,w,n),g(ge,e,n),d(e,Jo,n),d(e,oe,n),d(e,Fo,n),g(_e,e,n),d(e,$o,n),d(e,ve,n),d(e,Uo,n),d(e,be,n),d(e,Zo,n),d(e,ye,n),d(e,xo,n),d(e,Me,n),d(e,zo,n),d(e,we,n),d(e,Xo,n),g(Ce,e,n),d(e,Go,n),d(e,Te,n),d(e,Ro,n),g(We,e,n),d(e,Yo,n),d(e,Ve,n),d(e,Io,n),g(je,e,n),d(e,qo,n),d(e,x,n),g(ke,x,null),l(x,st),l(x,Ke),l(x,rt),l(x,eo),l(x,lt),g(te,x,null),d(e,No,n),g(Je,e,n),d(e,Bo,n),d(e,K,n),g(Fe,K,null),l(K,it),l(K,oo),d(e,So,n),g($e,e,n),d(e,Ho,n),d(e,V,n),g(Ue,V,null),l(V,ct),l(V,to),l(V,dt),l(V,no),l(V,mt),l(V,ao),l(V,pt),l(V,O),g(Ze,O,null),l(O,ft),l(O,so),l(O,ut),g(ne,O,null),d(e,Qo,n),g(xe,e,n),d(e,Eo,n),d(e,j,n),g(ze,j,null),l(j,ht),l(j,ro),l(j,gt),l(j,lo),l(j,_t),l(j,io),l(j,vt),l(j,B),g(Xe,B,null),l(B,bt),l(B,co),l(B,yt),g(ae,B,null),l(B,Mt),g(se,B,null),d(e,Lo,n),g(Ge,e,n),d(e,Po,n),d(e,k,n),g(Re,k,null),l(k,wt),l(k,mo),l(k,Ct),l(k,po),l(k,Tt),l(k,fo),l(k,Wt),l(k,U),g(Ye,U,null),l(U,Vt),l(U,uo),l(U,jt),g(re,U,null),l(U,kt),g(le,U,null),l(U,Jt),g(ie,U,null),d(e,Ao,n),g(Ie,e,n),d(e,Do,n),d(e,J,n),g(qe,J,null),l(J,Ft),l(J,ho),l(J,$t),l(J,go),l(J,Ut),l(J,_o),l(J,Zt),l(J,S),g(Ne,S,null),l(S,xt),l(S,vo),l(S,zt),g(ce,S,null),l(S,Xt),g(de,S,null),d(e,Oo,n),g(Be,e,n),d(e,Ko,n),d(e,F,n),g(Se,F,null),l(F,Gt),l(F,bo),l(F,Rt),l(F,yo),l(F,Yt),l(F,Mo),l(F,It),l(F,H),g(He,H,null),l(H,qt),l(H,wo),l(H,Nt),g(me,H,null),l(H,Bt),g(pe,H,null),d(e,et,n),g(Qe,e,n),d(e,ot,n),d(e,$,n),g(Ee,$,null),l($,St),l($,Co),l($,Ht),l($,To),l($,Qt),l($,Wo),l($,Et),l($,Q),g(Le,Q,null),l(Q,Lt),l(Q,Vo),l(Q,Pt),g(fe,Q,null),l(Q,At),g(ue,Q,null),d(e,tt,n),g(Pe,e,n),d(e,nt,n),d(e,ko,n),at=!0},p(e,[n]){const L={};n&2&&(L.$$scope={dirty:n,ctx:e}),te.$set(L);const Ae={};n&2&&(Ae.$$scope={dirty:n,ctx:e}),ne.$set(Ae);const z={};n&2&&(z.$$scope={dirty:n,ctx:e}),ae.$set(z);const ee={};n&2&&(ee.$$scope={dirty:n,ctx:e}),se.$set(ee);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),re.$set(X);const P={};n&2&&(P.$$scope={dirty:n,ctx:e}),le.$set(P);const G={};n&2&&(G.$$scope={dirty:n,ctx:e}),ie.$set(G);const R={};n&2&&(R.$$scope={dirty:n,ctx:e}),ce.$set(R);const Y={};n&2&&(Y.$$scope={dirty:n,ctx:e}),de.$set(Y);const A={};n&2&&(A.$$scope={dirty:n,ctx:e}),me.$set(A);const I={};n&2&&(I.$$scope={dirty:n,ctx:e}),pe.$set(I);const D={};n&2&&(D.$$scope={dirty:n,ctx:e}),fe.$set(D);const q={};n&2&&(q.$$scope={dirty:n,ctx:e}),ue.$set(q)},i(e){at||(_(ge.$$.fragment,e),_(_e.$$.fragment,e),_(Ce.$$.fragment,e),_(We.$$.fragment,e),_(je.$$.fragment,e),_(ke.$$.fragment,e),_(te.$$.fragment,e),_(Je.$$.fragment,e),_(Fe.$$.fragment,e),_($e.$$.fragment,e),_(Ue.$$.fragment,e),_(Ze.$$.fragment,e),_(ne.$$.fragment,e),_(xe.$$.fragment,e),_(ze.$$.fragment,e),_(Xe.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(Ge.$$.fragment,e),_(Re.$$.fragment,e),_(Ye.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(ie.$$.fragment,e),_(Ie.$$.fragment,e),_(qe.$$.fragment,e),_(Ne.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(Be.$$.fragment,e),_(Se.$$.fragment,e),_(He.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(Qe.$$.fragment,e),_(Ee.$$.fragment,e),_(Le.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(Pe.$$.fragment,e),at=!0)},o(e){v(ge.$$.fragment,e),v(_e.$$.fragment,e),v(Ce.$$.fragment,e),v(We.$$.fragment,e),v(je.$$.fragment,e),v(ke.$$.fragment,e),v(te.$$.fragment,e),v(Je.$$.fragment,e),v(Fe.$$.fragment,e),v($e.$$.fragment,e),v(Ue.$$.fragment,e),v(Ze.$$.fragment,e),v(ne.$$.fragment,e),v(xe.$$.fragment,e),v(ze.$$.fragment,e),v(Xe.$$.fragment,e),v(ae.$$.fragment,e),v(se.$$.fragment,e),v(Ge.$$.fragment,e),v(Re.$$.fragment,e),v(Ye.$$.fragment,e),v(re.$$.fragment,e),v(le.$$.fragment,e),v(ie.$$.fragment,e),v(Ie.$$.fragment,e),v(qe.$$.fragment,e),v(Ne.$$.fragment,e),v(ce.$$.fragment,e),v(de.$$.fragment,e),v(Be.$$.fragment,e),v(Se.$$.fragment,e),v(He.$$.fragment,e),v(me.$$.fragment,e),v(pe.$$.fragment,e),v(Qe.$$.fragment,e),v(Ee.$$.fragment,e),v(Le.$$.fragment,e),v(fe.$$.fragment,e),v(ue.$$.fragment,e),v(Pe.$$.fragment,e),at=!1},d(e){e&&(a(y),a(i),a(c),a(M),a(w),a(Jo),a(oe),a(Fo),a($o),a(ve),a(Uo),a(be),a(Zo),a(ye),a(xo),a(Me),a(zo),a(we),a(Xo),a(Go),a(Te),a(Ro),a(Yo),a(Ve),a(Io),a(qo),a(x),a(No),a(Bo),a(K),a(So),a(Ho),a(V),a(Qo),a(Eo),a(j),a(Lo),a(Po),a(k),a(Ao),a(Do),a(J),a(Oo),a(Ko),a(F),a(et),a(ot),a($),a(tt),a(nt),a(ko)),a(o),b(ge,e),b(_e,e),b(Ce,e),b(We,e),b(je,e),b(ke),b(te),b(Je,e),b(Fe),b($e,e),b(Ue),b(Ze),b(ne),b(xe,e),b(ze),b(Xe),b(ae),b(se),b(Ge,e),b(Re),b(Ye),b(re),b(le),b(ie),b(Ie,e),b(qe),b(Ne),b(ce),b(de),b(Be,e),b(Se),b(He),b(me),b(pe),b(Qe,e),b(Ee),b(Le),b(fe),b(ue),b(Pe,e)}}}const Kn='{"title":"Wav2Vec2-Conformer","local":"wav2vec2-conformer","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Wav2Vec2ConformerConfig","local":"transformers.Wav2Vec2ConformerConfig","sections":[],"depth":2},{"title":"Wav2Vec2Conformer specific outputs","local":"transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput","sections":[],"depth":2},{"title":"Wav2Vec2ConformerModel","local":"transformers.Wav2Vec2ConformerModel","sections":[],"depth":2},{"title":"Wav2Vec2ConformerForCTC","local":"transformers.Wav2Vec2ConformerForCTC","sections":[],"depth":2},{"title":"Wav2Vec2ConformerForSequenceClassification","local":"transformers.Wav2Vec2ConformerForSequenceClassification","sections":[],"depth":2},{"title":"Wav2Vec2ConformerForAudioFrameClassification","local":"transformers.Wav2Vec2ConformerForAudioFrameClassification","sections":[],"depth":2},{"title":"Wav2Vec2ConformerForXVector","local":"transformers.Wav2Vec2ConformerForXVector","sections":[],"depth":2},{"title":"Wav2Vec2ConformerForPreTraining","local":"transformers.Wav2Vec2ConformerForPreTraining","sections":[],"depth":2}],"depth":1}';function ea(C){return xn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ia extends zn{constructor(o){super(),Xn(this,o,ea,On,Zn,{})}}export{ia as component};
