import{s as Hr,o as Xr,n as Sr}from"../chunks/scheduler.18a86fab.js";import{S as Yr,i as Gr,g as a,s as o,r as m,A as Ar,h as i,f as r,c as n,j as v,u as l,x as h,k as u,y as t,a as c,v as d,d as p,t as g,w as f}from"../chunks/index.98837b22.js";import{D as y}from"../chunks/Docstring.a1ef7999.js";import{C as ur}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Qr}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Oe,E as Or}from"../chunks/getInferenceSnippets.06c2775f.js";function Kr(Ke){let x,K="Examples:",C,M,P;return M=new ur({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqSW1hZ2VQcm9jZXNzaW5nTWl4aW4qJTIwc28lMjBsZXQncyUyMHNob3clMjB0aGUlMjBleGFtcGxlcyUyMG9uJTIwYSUwQSUyMyUyMGRlcml2ZWQlMjBjbGFzcyUzQSUyMCpDTElQSW1hZ2VQcm9jZXNzb3IqJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMEEpJTIwJTIwJTIzJTIwRG93bmxvYWQlMjBpbWFnZV9wcm9jZXNzaW5nX2NvbmZpZyUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBDTElQSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiUyQyUyMGRvX25vcm1hbGl6ZSUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwaW1hZ2VfcHJvY2Vzc29yLmRvX25vcm1hbGl6ZSUyMGlzJTIwRmFsc2UlMEFpbWFnZV9wcm9jZXNzb3IlMkMlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMkMlMjBkb19ub3JtYWxpemUlM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGltYWdlX3Byb2Nlc3Nvci5kb19ub3JtYWxpemUlMjBpcyUyMEZhbHNlJTBBYXNzZXJ0JTIwdW51c2VkX2t3YXJncyUyMCUzRCUzRCUyMCU3QiUyMmZvbyUyMiUzQSUyMEZhbHNlJTdE",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){x=a("p"),x.textContent=K,C=o(),m(M.$$.fragment)},l(I){x=i(I,"P",{"data-svelte-h":!0}),h(x)!=="svelte-kvfsh7"&&(x.textContent=K),C=n(I),l(M.$$.fragment,I)},m(I,z){c(I,x,z),c(I,C,z),d(M,I,z),P=!0},p:Sr,i(I){P||(p(M.$$.fragment,I),P=!0)},o(I){g(M.$$.fragment,I),P=!1},d(I){I&&(r(x),r(C)),f(M,I)}}}function eo(Ke){let x,K,C,M,P,I,z,_r=`An image processor is in charge of loading images (optionally), preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.
Fast image processors are available for a few models and more will be added in the future. They are based on the <a href="https://pytorch.org/vision/stable/index.html" rel="nofollow">torchvision</a> library and provide a significant speed-up, especially when processing on GPU.
They have the same API as the base image processors and can be used as drop-in replacements.
To use a fast image processor, you need to install the <code>torchvision</code> library, and set the <code>use_fast</code> argument to <code>True</code> when instantiating the image processor:`,et,ee,tt,te,vr="Note that <code>use_fast</code> will be set to <code>True</code> by default in a future release.",rt,re,br="When using a fast image processor, you can also set the <code>device</code> argument to specify the device on which the processing should be done. By default, the processing is done on the same device as the inputs if the inputs are tensors, or on the CPU otherwise.",ot,oe,nt,ne,yr="Here are some speed comparisons between the base and fast image processors for the <code>DETR</code> and <code>RT-DETR</code> models, and how they impact overall inference time:",st,k,Ir='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png"/>',at,j,xr='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png"/>',it,J,$r='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png"/>',ct,N,Tr='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png"/>',mt,se,wr='These benchmarks were run on an <a href="https://aws.amazon.com/ec2/instance-types/g5/" rel="nofollow">AWS EC2 g5.2xlarge instance</a>, utilizing an NVIDIA A10G Tensor Core GPU.',lt,ae,dt,T,ie,Jt,Fe,Mr=`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,Nt,F,ce,Zt,Ue,Pr='Instantiate a type of <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from an image processor.',Dt,Z,Lt,D,me,qt,ke,zr=`Save an image processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin.from_pretrained">from_pretrained()</a> class method.`,pt,le,gt,$,de,Wt,je,Br='Holds the output of the <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> and feature extractor specific <code>__call__</code> methods.',Rt,Je,Cr="This class is derived from a python dictionary and can be used as a dictionary.",Vt,L,pe,Et,Ne,Fr="Convert the inner content to tensors.",Ht,q,ge,Xt,Ze,Ur=`Send all values to device by calling <code>v.to(*args, **kwargs)</code> (PyTorch only). This should support casting in
different <code>dtypes</code> and sending the <code>BatchFeature</code> to a different <code>device</code>.`,ft,fe,ht,w,he,St,W,ue,Yt,De,kr=`Center crop an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. If the input size is smaller than <code>crop_size</code> along
any edge, the image is padded with 0’s and then center cropped.`,Gt,R,_e,At,Le,jr="Normalize an image. image = (image - image_mean) / image_std.",Qt,V,ve,Ot,qe,Jr="Rescale an image by a scale factor. image = image * scale.",ut,be,_t,_,ye,Kt,E,Ie,er,We,Nr=`Center crop an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. If the input size is smaller than <code>crop_size</code> along
any edge, the image is padded with 0’s and then center cropped.`,tr,H,xe,rr,Re,Zr="A wrapper around <code>F.resize</code> so that it is compatible with torch.compile when the image is a uint8 tensor.",or,X,$e,nr,Ve,Dr=`Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image
as is.`,sr,S,Te,ar,Ee,Lr="Filter out the unused kwargs from the kwargs dictionary.",ir,Y,we,cr,He,qr="Normalize an image. image = (image - image_mean) / image_std.",mr,Xe,Me,lr,G,Pe,dr,Se,Wr="Rescale an image by a scale factor. image = image * scale.",pr,A,ze,gr,Ye,Rr="Rescale and normalize images.",fr,Q,Be,hr,Ge,Vr="Resize an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>.",vt,Ce,bt,Qe,yt;return P=new Oe({props:{title:"Image Processor",local:"image-processor",headingTag:"h1"}}),ee=new ur({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZXRyLXJlc25ldC01MCUyMiUyQyUyMHVzZV9mYXN0JTNEVHJ1ZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/detr-resnet-50&quot;</span>, use_fast=<span class="hljs-literal">True</span>)`,wrap:!1}}),oe=new ur({props:{code:"ZnJvbSUyMHRvcmNodmlzaW9uLmlvJTIwaW1wb3J0JTIwcmVhZF9pbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBEZXRySW1hZ2VQcm9jZXNzb3JGYXN0JTBBJTBBaW1hZ2VzJTIwJTNEJTIwcmVhZF9pbWFnZSglMjJpbWFnZS5qcGclMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwRGV0ckltYWdlUHJvY2Vzc29yRmFzdC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZXRyLXJlc25ldC01MCUyMiklMEFpbWFnZXNfcHJvY2Vzc2VkJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBkZXZpY2UlM0QlMjJjdWRhJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DetrImageProcessorFast

images = read_image(<span class="hljs-string">&quot;image.jpg&quot;</span>)
processor = DetrImageProcessorFast.from_pretrained(<span class="hljs-string">&quot;facebook/detr-resnet-50&quot;</span>)
images_processed = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),ae=new Oe({props:{title:"ImageProcessingMixin",local:"transformers.ImageProcessingMixin",headingTag:"h2"}}),ie=new y({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L64"}}),ce=new y({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[str, bool, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L91",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),Z=new Qr({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[Kr]},$$scope:{ctx:Ke}}}),me=new y({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L205"}}),le=new Oe({props:{title:"BatchFeature",local:"transformers.BatchFeature",headingTag:"h2"}}),de=new y({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L63"}}),pe=new y({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L172"}}),ge=new y({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BatchFeature.to.args",description:`<strong>args</strong> (<code>Tuple</code>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"args"},{anchor:"transformers.BatchFeature.to.kwargs",description:`<strong>kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.
To enable asynchronous data transfer, set the <code>non_blocking</code> flag in <code>kwargs</code> (defaults to <code>False</code>).`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L203",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The same instance after modification.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),fe=new Oe({props:{title:"BaseImageProcessor",local:"transformers.BaseImageProcessor",headingTag:"h2"}}),he=new y({props:{name:"class transformers.BaseImageProcessor",anchor:"transformers.BaseImageProcessor",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils.py#L38"}}),ue=new y({props:{name:"center_crop",anchor:"transformers.BaseImageProcessor.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessor.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.BaseImageProcessor.center_crop.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.BaseImageProcessor.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BaseImageProcessor.center_crop.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils.py#L125"}}),_e=new y({props:{name:"normalize",anchor:"transformers.BaseImageProcessor.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"std",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessor.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.BaseImageProcessor.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image mean to use for normalization.`,name:"mean"},{anchor:"transformers.BaseImageProcessor.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.BaseImageProcessor.normalize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BaseImageProcessor.normalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils.py#L88",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The normalized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),ve=new y({props:{name:"rescale",anchor:"transformers.BaseImageProcessor.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessor.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.BaseImageProcessor.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scaling factor to rescale pixel values by.`,name:"scale"},{anchor:"transformers.BaseImageProcessor.rescale.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BaseImageProcessor.rescale.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils.py#L56",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),be=new Oe({props:{title:"BaseImageProcessorFast",local:"transformers.BaseImageProcessorFast",headingTag:"h2"}}),ye=new y({props:{name:"class transformers.BaseImageProcessorFast",anchor:"transformers.BaseImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.image_processing_utils_fast.DefaultFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L193"}}),Ie=new y({props:{name:"center_crop",anchor:"transformers.BaseImageProcessorFast.center_crop",parameters:[{name:"image",val:": torch.Tensor"},{name:"size",val:": dict"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.center_crop.image",description:`<strong>image</strong> (<code>&quot;torch.Tensor&quot;</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.BaseImageProcessorFast.center_crop.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L405",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The center cropped image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),xe=new y({props:{name:"compile_friendly_resize",anchor:"transformers.BaseImageProcessorFast.compile_friendly_resize",parameters:[{name:"image",val:": torch.Tensor"},{name:"new_size",val:": tuple"},{name:"interpolation",val:": typing.Optional[ForwardRef('F.InterpolationMode')] = None"},{name:"antialias",val:": bool = True"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L296"}}),$e=new y({props:{name:"convert_to_rgb",anchor:"transformers.BaseImageProcessorFast.convert_to_rgb",parameters:[{name:"image",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.convert_to_rgb.image",description:`<strong>image</strong> (ImageInput) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L428",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The converted image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>ImageInput</p>
`}}),Te=new y({props:{name:"filter_out_unused_kwargs",anchor:"transformers.BaseImageProcessorFast.filter_out_unused_kwargs",parameters:[{name:"kwargs",val:": dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L444"}}),we=new y({props:{name:"normalize",anchor:"transformers.BaseImageProcessorFast.normalize",parameters:[{name:"image",val:": torch.Tensor"},{name:"mean",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"std",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.normalize.image",description:`<strong>image</strong> (<code>torch.Tensor</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.BaseImageProcessorFast.normalize.mean",description:`<strong>mean</strong> (<code>torch.Tensor</code>, <code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image mean to use for normalization.`,name:"mean"},{anchor:"transformers.BaseImageProcessorFast.normalize.std",description:`<strong>std</strong> (<code>torch.Tensor</code>, <code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image standard deviation to use for normalization.`,name:"std"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L337",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The normalized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),Me=new y({props:{name:"preprocess",anchor:"transformers.BaseImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"*args",val:""},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.image_processing_utils_fast.DefaultFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.BaseImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.BaseImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.BaseImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.BaseImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.BaseImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.BaseImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.BaseImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.BaseImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.BaseImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.BaseImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.BaseImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.BaseImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.BaseImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.BaseImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.BaseImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.BaseImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.BaseImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L639",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),Pe=new y({props:{name:"rescale",anchor:"transformers.BaseImageProcessorFast.rescale",parameters:[{name:"image",val:": torch.Tensor"},{name:"scale",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.rescale.image",description:`<strong>image</strong> (<code>torch.Tensor</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.BaseImageProcessorFast.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scaling factor to rescale pixel values by.`,name:"scale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L317",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),ze=new y({props:{name:"rescale_and_normalize",anchor:"transformers.BaseImageProcessorFast.rescale_and_normalize",parameters:[{name:"images",val:": torch.Tensor"},{name:"do_rescale",val:": bool"},{name:"rescale_factor",val:": float"},{name:"do_normalize",val:": bool"},{name:"image_mean",val:": typing.Union[float, list[float]]"},{name:"image_std",val:": typing.Union[float, list[float]]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L377"}}),Be=new y({props:{name:"resize",anchor:"transformers.BaseImageProcessorFast.resize",parameters:[{name:"image",val:": torch.Tensor"},{name:"size",val:": SizeDict"},{name:"interpolation",val:": F.InterpolationMode = None"},{name:"antialias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BaseImageProcessorFast.resize.image",description:`<strong>image</strong> (<code>torch.Tensor</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.BaseImageProcessorFast.resize.size",description:`<strong>size</strong> (<code>SizeDict</code>) &#x2014;
Dictionary in the format <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code> specifying the size of the output image.`,name:"size"},{anchor:"transformers.BaseImageProcessorFast.resize.interpolation",description:`<strong>interpolation</strong> (<code>InterpolationMode</code>, <em>optional</em>, defaults to <code>InterpolationMode.BILINEAR</code>) &#x2014;
<code>InterpolationMode</code> filter to use when resizing the image e.g. <code>InterpolationMode.BICUBIC</code>.`,name:"interpolation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L242",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),Ce=new Or({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/image_processor.md"}}),{c(){x=a("meta"),K=o(),C=a("p"),M=o(),m(P.$$.fragment),I=o(),z=a("p"),z.innerHTML=_r,et=o(),m(ee.$$.fragment),tt=o(),te=a("p"),te.innerHTML=vr,rt=o(),re=a("p"),re.innerHTML=br,ot=o(),m(oe.$$.fragment),nt=o(),ne=a("p"),ne.innerHTML=yr,st=o(),k=a("div"),k.innerHTML=Ir,at=o(),j=a("div"),j.innerHTML=xr,it=o(),J=a("div"),J.innerHTML=$r,ct=o(),N=a("div"),N.innerHTML=Tr,mt=o(),se=a("p"),se.innerHTML=wr,lt=o(),m(ae.$$.fragment),dt=o(),T=a("div"),m(ie.$$.fragment),Jt=o(),Fe=a("p"),Fe.textContent=Mr,Nt=o(),F=a("div"),m(ce.$$.fragment),Zt=o(),Ue=a("p"),Ue.innerHTML=Pr,Dt=o(),m(Z.$$.fragment),Lt=o(),D=a("div"),m(me.$$.fragment),qt=o(),ke=a("p"),ke.innerHTML=zr,pt=o(),m(le.$$.fragment),gt=o(),$=a("div"),m(de.$$.fragment),Wt=o(),je=a("p"),je.innerHTML=Br,Rt=o(),Je=a("p"),Je.textContent=Cr,Vt=o(),L=a("div"),m(pe.$$.fragment),Et=o(),Ne=a("p"),Ne.textContent=Fr,Ht=o(),q=a("div"),m(ge.$$.fragment),Xt=o(),Ze=a("p"),Ze.innerHTML=Ur,ft=o(),m(fe.$$.fragment),ht=o(),w=a("div"),m(he.$$.fragment),St=o(),W=a("div"),m(ue.$$.fragment),Yt=o(),De=a("p"),De.innerHTML=kr,Gt=o(),R=a("div"),m(_e.$$.fragment),At=o(),Le=a("p"),Le.textContent=jr,Qt=o(),V=a("div"),m(ve.$$.fragment),Ot=o(),qe=a("p"),qe.textContent=Jr,ut=o(),m(be.$$.fragment),_t=o(),_=a("div"),m(ye.$$.fragment),Kt=o(),E=a("div"),m(Ie.$$.fragment),er=o(),We=a("p"),We.innerHTML=Nr,tr=o(),H=a("div"),m(xe.$$.fragment),rr=o(),Re=a("p"),Re.innerHTML=Zr,or=o(),X=a("div"),m($e.$$.fragment),nr=o(),Ve=a("p"),Ve.textContent=Dr,sr=o(),S=a("div"),m(Te.$$.fragment),ar=o(),Ee=a("p"),Ee.textContent=Lr,ir=o(),Y=a("div"),m(we.$$.fragment),cr=o(),He=a("p"),He.textContent=qr,mr=o(),Xe=a("div"),m(Me.$$.fragment),lr=o(),G=a("div"),m(Pe.$$.fragment),dr=o(),Se=a("p"),Se.textContent=Wr,pr=o(),A=a("div"),m(ze.$$.fragment),gr=o(),Ye=a("p"),Ye.textContent=Rr,fr=o(),Q=a("div"),m(Be.$$.fragment),hr=o(),Ge=a("p"),Ge.innerHTML=Vr,vt=o(),m(Ce.$$.fragment),bt=o(),Qe=a("p"),this.h()},l(e){const s=Ar("svelte-u9bgzb",document.head);x=i(s,"META",{name:!0,content:!0}),s.forEach(r),K=n(e),C=i(e,"P",{}),v(C).forEach(r),M=n(e),l(P.$$.fragment,e),I=n(e),z=i(e,"P",{"data-svelte-h":!0}),h(z)!=="svelte-mbluwt"&&(z.innerHTML=_r),et=n(e),l(ee.$$.fragment,e),tt=n(e),te=i(e,"P",{"data-svelte-h":!0}),h(te)!=="svelte-ge56gn"&&(te.innerHTML=vr),rt=n(e),re=i(e,"P",{"data-svelte-h":!0}),h(re)!=="svelte-1gp7zdk"&&(re.innerHTML=br),ot=n(e),l(oe.$$.fragment,e),nt=n(e),ne=i(e,"P",{"data-svelte-h":!0}),h(ne)!=="svelte-f7fjym"&&(ne.innerHTML=yr),st=n(e),k=i(e,"DIV",{class:!0,"data-svelte-h":!0}),h(k)!=="svelte-xcs38v"&&(k.innerHTML=Ir),at=n(e),j=i(e,"DIV",{class:!0,"data-svelte-h":!0}),h(j)!=="svelte-c80zcw"&&(j.innerHTML=xr),it=n(e),J=i(e,"DIV",{class:!0,"data-svelte-h":!0}),h(J)!=="svelte-19h8gfy"&&(J.innerHTML=$r),ct=n(e),N=i(e,"DIV",{class:!0,"data-svelte-h":!0}),h(N)!=="svelte-1tv8qfr"&&(N.innerHTML=Tr),mt=n(e),se=i(e,"P",{"data-svelte-h":!0}),h(se)!=="svelte-1tjod1j"&&(se.innerHTML=wr),lt=n(e),l(ae.$$.fragment,e),dt=n(e),T=i(e,"DIV",{class:!0});var B=v(T);l(ie.$$.fragment,B),Jt=n(B),Fe=i(B,"P",{"data-svelte-h":!0}),h(Fe)!=="svelte-16ht4m3"&&(Fe.textContent=Mr),Nt=n(B),F=i(B,"DIV",{class:!0});var Ae=v(F);l(ce.$$.fragment,Ae),Zt=n(Ae),Ue=i(Ae,"P",{"data-svelte-h":!0}),h(Ue)!=="svelte-lkptaz"&&(Ue.innerHTML=Pr),Dt=n(Ae),l(Z.$$.fragment,Ae),Ae.forEach(r),Lt=n(B),D=i(B,"DIV",{class:!0});var It=v(D);l(me.$$.fragment,It),qt=n(It),ke=i(It,"P",{"data-svelte-h":!0}),h(ke)!=="svelte-1gn5bqe"&&(ke.innerHTML=zr),It.forEach(r),B.forEach(r),pt=n(e),l(le.$$.fragment,e),gt=n(e),$=i(e,"DIV",{class:!0});var U=v($);l(de.$$.fragment,U),Wt=n(U),je=i(U,"P",{"data-svelte-h":!0}),h(je)!=="svelte-lzs6g2"&&(je.innerHTML=Br),Rt=n(U),Je=i(U,"P",{"data-svelte-h":!0}),h(Je)!=="svelte-saqdtk"&&(Je.textContent=Cr),Vt=n(U),L=i(U,"DIV",{class:!0});var xt=v(L);l(pe.$$.fragment,xt),Et=n(xt),Ne=i(xt,"P",{"data-svelte-h":!0}),h(Ne)!=="svelte-pxfh9u"&&(Ne.textContent=Fr),xt.forEach(r),Ht=n(U),q=i(U,"DIV",{class:!0});var $t=v(q);l(ge.$$.fragment,$t),Xt=n($t),Ze=i($t,"P",{"data-svelte-h":!0}),h(Ze)!=="svelte-d0cfhs"&&(Ze.innerHTML=Ur),$t.forEach(r),U.forEach(r),ft=n(e),l(fe.$$.fragment,e),ht=n(e),w=i(e,"DIV",{class:!0});var O=v(w);l(he.$$.fragment,O),St=n(O),W=i(O,"DIV",{class:!0});var Tt=v(W);l(ue.$$.fragment,Tt),Yt=n(Tt),De=i(Tt,"P",{"data-svelte-h":!0}),h(De)!=="svelte-193kiu8"&&(De.innerHTML=kr),Tt.forEach(r),Gt=n(O),R=i(O,"DIV",{class:!0});var wt=v(R);l(_e.$$.fragment,wt),At=n(wt),Le=i(wt,"P",{"data-svelte-h":!0}),h(Le)!=="svelte-1e5okex"&&(Le.textContent=jr),wt.forEach(r),Qt=n(O),V=i(O,"DIV",{class:!0});var Mt=v(V);l(ve.$$.fragment,Mt),Ot=n(Mt),qe=i(Mt,"P",{"data-svelte-h":!0}),h(qe)!=="svelte-qun0mt"&&(qe.textContent=Jr),Mt.forEach(r),O.forEach(r),ut=n(e),l(be.$$.fragment,e),_t=n(e),_=i(e,"DIV",{class:!0});var b=v(_);l(ye.$$.fragment,b),Kt=n(b),E=i(b,"DIV",{class:!0});var Pt=v(E);l(Ie.$$.fragment,Pt),er=n(Pt),We=i(Pt,"P",{"data-svelte-h":!0}),h(We)!=="svelte-193kiu8"&&(We.innerHTML=Nr),Pt.forEach(r),tr=n(b),H=i(b,"DIV",{class:!0});var zt=v(H);l(xe.$$.fragment,zt),rr=n(zt),Re=i(zt,"P",{"data-svelte-h":!0}),h(Re)!=="svelte-1o9v9sx"&&(Re.innerHTML=Zr),zt.forEach(r),or=n(b),X=i(b,"DIV",{class:!0});var Bt=v(X);l($e.$$.fragment,Bt),nr=n(Bt),Ve=i(Bt,"P",{"data-svelte-h":!0}),h(Ve)!=="svelte-ryflh3"&&(Ve.textContent=Dr),Bt.forEach(r),sr=n(b),S=i(b,"DIV",{class:!0});var Ct=v(S);l(Te.$$.fragment,Ct),ar=n(Ct),Ee=i(Ct,"P",{"data-svelte-h":!0}),h(Ee)!=="svelte-3gmw3g"&&(Ee.textContent=Lr),Ct.forEach(r),ir=n(b),Y=i(b,"DIV",{class:!0});var Ft=v(Y);l(we.$$.fragment,Ft),cr=n(Ft),He=i(Ft,"P",{"data-svelte-h":!0}),h(He)!=="svelte-1e5okex"&&(He.textContent=qr),Ft.forEach(r),mr=n(b),Xe=i(b,"DIV",{class:!0});var Er=v(Xe);l(Me.$$.fragment,Er),Er.forEach(r),lr=n(b),G=i(b,"DIV",{class:!0});var Ut=v(G);l(Pe.$$.fragment,Ut),dr=n(Ut),Se=i(Ut,"P",{"data-svelte-h":!0}),h(Se)!=="svelte-qun0mt"&&(Se.textContent=Wr),Ut.forEach(r),pr=n(b),A=i(b,"DIV",{class:!0});var kt=v(A);l(ze.$$.fragment,kt),gr=n(kt),Ye=i(kt,"P",{"data-svelte-h":!0}),h(Ye)!=="svelte-1hhhif3"&&(Ye.textContent=Rr),kt.forEach(r),fr=n(b),Q=i(b,"DIV",{class:!0});var jt=v(Q);l(Be.$$.fragment,jt),hr=n(jt),Ge=i(jt,"P",{"data-svelte-h":!0}),h(Ge)!=="svelte-1oee9wu"&&(Ge.innerHTML=Vr),jt.forEach(r),b.forEach(r),vt=n(e),l(Ce.$$.fragment,e),bt=n(e),Qe=i(e,"P",{}),v(Qe).forEach(r),this.h()},h(){u(x,"name","hf:doc:metadata"),u(x,"content",to),u(k,"class","flex"),u(j,"class","flex"),u(J,"class","flex"),u(N,"class","flex"),u(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){t(document.head,x),c(e,K,s),c(e,C,s),c(e,M,s),d(P,e,s),c(e,I,s),c(e,z,s),c(e,et,s),d(ee,e,s),c(e,tt,s),c(e,te,s),c(e,rt,s),c(e,re,s),c(e,ot,s),d(oe,e,s),c(e,nt,s),c(e,ne,s),c(e,st,s),c(e,k,s),c(e,at,s),c(e,j,s),c(e,it,s),c(e,J,s),c(e,ct,s),c(e,N,s),c(e,mt,s),c(e,se,s),c(e,lt,s),d(ae,e,s),c(e,dt,s),c(e,T,s),d(ie,T,null),t(T,Jt),t(T,Fe),t(T,Nt),t(T,F),d(ce,F,null),t(F,Zt),t(F,Ue),t(F,Dt),d(Z,F,null),t(T,Lt),t(T,D),d(me,D,null),t(D,qt),t(D,ke),c(e,pt,s),d(le,e,s),c(e,gt,s),c(e,$,s),d(de,$,null),t($,Wt),t($,je),t($,Rt),t($,Je),t($,Vt),t($,L),d(pe,L,null),t(L,Et),t(L,Ne),t($,Ht),t($,q),d(ge,q,null),t(q,Xt),t(q,Ze),c(e,ft,s),d(fe,e,s),c(e,ht,s),c(e,w,s),d(he,w,null),t(w,St),t(w,W),d(ue,W,null),t(W,Yt),t(W,De),t(w,Gt),t(w,R),d(_e,R,null),t(R,At),t(R,Le),t(w,Qt),t(w,V),d(ve,V,null),t(V,Ot),t(V,qe),c(e,ut,s),d(be,e,s),c(e,_t,s),c(e,_,s),d(ye,_,null),t(_,Kt),t(_,E),d(Ie,E,null),t(E,er),t(E,We),t(_,tr),t(_,H),d(xe,H,null),t(H,rr),t(H,Re),t(_,or),t(_,X),d($e,X,null),t(X,nr),t(X,Ve),t(_,sr),t(_,S),d(Te,S,null),t(S,ar),t(S,Ee),t(_,ir),t(_,Y),d(we,Y,null),t(Y,cr),t(Y,He),t(_,mr),t(_,Xe),d(Me,Xe,null),t(_,lr),t(_,G),d(Pe,G,null),t(G,dr),t(G,Se),t(_,pr),t(_,A),d(ze,A,null),t(A,gr),t(A,Ye),t(_,fr),t(_,Q),d(Be,Q,null),t(Q,hr),t(Q,Ge),c(e,vt,s),d(Ce,e,s),c(e,bt,s),c(e,Qe,s),yt=!0},p(e,[s]){const B={};s&2&&(B.$$scope={dirty:s,ctx:e}),Z.$set(B)},i(e){yt||(p(P.$$.fragment,e),p(ee.$$.fragment,e),p(oe.$$.fragment,e),p(ae.$$.fragment,e),p(ie.$$.fragment,e),p(ce.$$.fragment,e),p(Z.$$.fragment,e),p(me.$$.fragment,e),p(le.$$.fragment,e),p(de.$$.fragment,e),p(pe.$$.fragment,e),p(ge.$$.fragment,e),p(fe.$$.fragment,e),p(he.$$.fragment,e),p(ue.$$.fragment,e),p(_e.$$.fragment,e),p(ve.$$.fragment,e),p(be.$$.fragment,e),p(ye.$$.fragment,e),p(Ie.$$.fragment,e),p(xe.$$.fragment,e),p($e.$$.fragment,e),p(Te.$$.fragment,e),p(we.$$.fragment,e),p(Me.$$.fragment,e),p(Pe.$$.fragment,e),p(ze.$$.fragment,e),p(Be.$$.fragment,e),p(Ce.$$.fragment,e),yt=!0)},o(e){g(P.$$.fragment,e),g(ee.$$.fragment,e),g(oe.$$.fragment,e),g(ae.$$.fragment,e),g(ie.$$.fragment,e),g(ce.$$.fragment,e),g(Z.$$.fragment,e),g(me.$$.fragment,e),g(le.$$.fragment,e),g(de.$$.fragment,e),g(pe.$$.fragment,e),g(ge.$$.fragment,e),g(fe.$$.fragment,e),g(he.$$.fragment,e),g(ue.$$.fragment,e),g(_e.$$.fragment,e),g(ve.$$.fragment,e),g(be.$$.fragment,e),g(ye.$$.fragment,e),g(Ie.$$.fragment,e),g(xe.$$.fragment,e),g($e.$$.fragment,e),g(Te.$$.fragment,e),g(we.$$.fragment,e),g(Me.$$.fragment,e),g(Pe.$$.fragment,e),g(ze.$$.fragment,e),g(Be.$$.fragment,e),g(Ce.$$.fragment,e),yt=!1},d(e){e&&(r(K),r(C),r(M),r(I),r(z),r(et),r(tt),r(te),r(rt),r(re),r(ot),r(nt),r(ne),r(st),r(k),r(at),r(j),r(it),r(J),r(ct),r(N),r(mt),r(se),r(lt),r(dt),r(T),r(pt),r(gt),r($),r(ft),r(ht),r(w),r(ut),r(_t),r(_),r(vt),r(bt),r(Qe)),r(x),f(P,e),f(ee,e),f(oe,e),f(ae,e),f(ie),f(ce),f(Z),f(me),f(le,e),f(de),f(pe),f(ge),f(fe,e),f(he),f(ue),f(_e),f(ve),f(be,e),f(ye),f(Ie),f(xe),f($e),f(Te),f(we),f(Me),f(Pe),f(ze),f(Be),f(Ce,e)}}}const to='{"title":"Image Processor","local":"image-processor","sections":[{"title":"ImageProcessingMixin","local":"transformers.ImageProcessingMixin","sections":[],"depth":2},{"title":"BatchFeature","local":"transformers.BatchFeature","sections":[],"depth":2},{"title":"BaseImageProcessor","local":"transformers.BaseImageProcessor","sections":[],"depth":2},{"title":"BaseImageProcessorFast","local":"transformers.BaseImageProcessorFast","sections":[],"depth":2}],"depth":1}';function ro(Ke){return Xr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mo extends Yr{constructor(x){super(),Gr(this,x,ro,eo,Hr,{})}}export{mo as component};
