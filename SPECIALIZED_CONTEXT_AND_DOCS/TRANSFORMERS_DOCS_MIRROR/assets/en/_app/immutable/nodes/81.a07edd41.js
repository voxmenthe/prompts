import{s as LL,o as kL,n as k}from"../chunks/scheduler.18a86fab.js";import{S as xL,i as PL,g as i,s as n,r as u,A as GL,h as l,f as d,c as a,j as y,u as p,x as f,k as w,y as r,a as h,v as _,d as v,t as M,w as b}from"../chunks/index.98837b22.js";import{T as Fh}from"../chunks/Tip.77304350.js";import{D as L}from"../chunks/Docstring.a1ef7999.js";import{C as x}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as P}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as G,E as VL}from"../chunks/getInferenceSnippets.06c2775f.js";function AL(F){let s,T=`If your <code>NewModelConfig</code> is a subclass of <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, make sure its
<code>model_type</code> attribute is set to the same key you use when registering the config (here <code>&quot;new-model&quot;</code>).`,c,t,g=`Likewise, if your <code>NewModel</code> is a subclass of <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>, make sure its
<code>config_class</code> attribute is set to the same class you use when registering the model (here
<code>NewModelConfig</code>).`;return{c(){s=i("p"),s.innerHTML=T,c=n(),t=i("p"),t.innerHTML=g},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1kuwgyg"&&(s.innerHTML=T),c=a(o),t=l(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1wy0bqg"&&(t.innerHTML=g)},m(o,C){h(o,s,C),h(o,c,C),h(o,t,C)},p:k,d(o){o&&(d(s),d(c),d(t))}}}function SL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlJTIwaXMlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyQyUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRmJlcnRfc2F2ZWRfbW9kZWwlMkYlMjIpJTBBJTBBJTIzJTIwTG9hZCUyMGElMjBzcGVjaWZpYyUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGYmVydF9zYXZlZF9tb2RlbCUyRm15X2NvbmZpZ3VyYXRpb24uanNvbiUyMiklMEElMEElMjMlMjBDaGFuZ2UlMjBzb21lJTIwY29uZmlnJTIwYXR0cmlidXRlcyUyMHdoZW4lMjBsb2FkaW5nJTIwYSUyMHByZXRyYWluZWQlMjBjb25maWcuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSUyQyUyMGZvbyUzREZhbHNlKSUwQWNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQWNvbmZpZyUyQyUyMHVudXNlZF9rd2FyZ3MlMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlJTJDJTIwZm9vJTNERmFsc2UlMkMlMjByZXR1cm5fdW51c2VkX2t3YXJncyUzRFRydWUlMEEpJTBBY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBdW51c2VkX2t3YXJncw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function BL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEElMjMlMjBEb3dubG9hZCUyMHZvY2FidWxhcnklMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMHZvY2FidWxhcnklMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwdG9rZW5pemVyJTIwd2FzJTIwc2F2ZWQlMjB1c2luZyUyMCpzYXZlX3ByZXRyYWluZWQoJy4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYnKSopJTBBJTIzJTIwdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZiZXJ0X3NhdmVkX21vZGVsJTJGJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGRlZmluZSUyMG1vZGVsLXNwZWNpZmljJTIwYXJndW1lbnRzJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyRmFjZWJvb2tBSSUyRnJvYmVydGEtYmFzZSUyMiUyQyUyMGFkZF9wcmVmaXhfc3BhY2UlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;./test/bert_saved_model/&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and define model-specific arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;FacebookAI/roberta-base&quot;</span>, add_prefix_space=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function $L(F){let s,T="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){s=i("p"),s.innerHTML=T},l(c){s=l(c,"P",{"data-svelte-h":!0}),f(s)!=="svelte-15auxyb"&&(s.innerHTML=T)},m(c,t){h(c,s,t)},p:k,d(c){c&&d(s)}}}function IL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIpJTBBJTBBJTIzJTIwSWYlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMGZlYXR1cmUlMjBleHRyYWN0b3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># feature_extractor = AutoFeatureExtractor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function jL(F){let s,T="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){s=i("p"),s.innerHTML=T},l(c){s=l(c,"P",{"data-svelte-h":!0}),f(s)!=="svelte-15auxyb"&&(s.innerHTML=T)},m(c,t){h(c,s,t)},p:k,d(c){c&&d(s)}}}function EL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQSUyMyUyMERvd25sb2FkJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0LWluMjFrJTIyKSUwQSUwQSUyMyUyMElmJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download image processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If image processor files are in a directory (e.g. image processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># image_processor = AutoImageProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ZL(F){let s,T="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){s=i("p"),s.innerHTML=T},l(c){s=l(c,"P",{"data-svelte-h":!0}),f(s)!=="svelte-15auxyb"&&(s.innerHTML=T)},m(c,t){h(c,s,t)},p:k,d(c){c&&d(s)}}}function RL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9WaWRlb1Byb2Nlc3NvciUwQSUwQSUyMyUyMERvd25sb2FkJTIwdmlkZW8lMjBwcm9jZXNzb3IlMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEF2aWRlb19wcm9jZXNzb3IlMjAlM0QlMjBBdXRvVmlkZW9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmxsYXZhLWhmJTJGbGxhdmEtb25ldmlzaW9uLXF3ZW4yLTAuNWItb3YtaGYlMjIpJTBBJTBBJTIzJTIwSWYlMjB2aWRlbyUyMHByb2Nlc3NvciUyMGZpbGVzJTIwYXJlJTIwaW4lMjBhJTIwZGlyZWN0b3J5JTIwKGUuZy4lMjB2aWRlbyUyMHByb2Nlc3NvciUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKSUwQSUyMyUyMHZpZGVvX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9WaWRlb1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoVideoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download video processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_processor = AutoVideoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-onevision-qwen2-0.5b-ov-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If video processor files are in a directory (e.g. video processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># video_processor = AutoVideoProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function WL(F){let s,T="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){s=i("p"),s.innerHTML=T},l(c){s=l(c,"P",{"data-svelte-h":!0}),f(s)!=="svelte-15auxyb"&&(s.innerHTML=T)},m(c,t){h(c,s,t)},p:k,d(c){c&&d(s)}}}function NL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEElMjMlMjBEb3dubG9hZCUyMHByb2Nlc3NvciUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKSUwQSUwQSUyMyUyMElmJTIwcHJvY2Vzc29yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMHByb2Nlc3NvciUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKSUwQSUyMyUyMHByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># processor = AutoProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function JL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function DL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function UL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUHJlVHJhaW5pbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function qL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function zL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function XL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function QL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function HL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function YL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function OL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwVEYlMjBjaGVja3BvaW50JTIwZmlsZSUyMGluc3RlYWQlMjBvZiUyMGElMjBQeVRvcmNoJTIwbW9kZWwlMjAoc2xvd2VyKSUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9tb2RlbCUyRnQ1X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZ0NV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function KL(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ek(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ok(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function rk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function nk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ak(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function sk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function tk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ik(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function lk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function dk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function mk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function ck(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function fk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function gk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function hk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function uk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function pk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function _k(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function vk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Mk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function bk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Ck(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Tk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Fk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW5zdGFuY2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function yk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function wk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Lk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function kk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function xk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Pk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RPYmplY3REZXRlY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Gk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Vk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Ak(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Sk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Bk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function $k(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Ik(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function jk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Ek(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Zk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Rk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Wk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1Rva2VuaXphdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9Ub2tlbml6YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioTokenization

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioTokenization.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Nk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1Rva2VuaXphdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb1Rva2VuaXphdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvVG9rZW5pemF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvVG9rZW5pemF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioTokenization

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioTokenization.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioTokenization.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioTokenization.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Jk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Dk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnRhcGFzLWJhc2UtZmluZXR1bmVkLXd0cSUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ0YXBhc190Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnRhcGFzX3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Uk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyaW1waXJhJTJGbGF5b3V0bG0tZG9jdW1lbnQtcWElMjIlMkMlMjByZXZpc2lvbiUzRCUyMjUyZTAxYjMlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function qk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmltcGlyYSUyRmxheW91dGxtLWRvY3VtZW50LXFhJTIyJTJDJTIwcmV2aXNpb24lM0QlMjI1MmUwMWIzJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGbGF5b3V0bG1fdGZfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZsYXlvdXRsbV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/layoutlm_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/layoutlm_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function zk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRhbmRlbGluJTJGdmlsdC1iMzItZmluZXR1bmVkLXZxYSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc3VhbFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Xk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ2aWx0X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVmlzdWFsUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnZpbHRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/vilt_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/vilt_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Qk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVRleHRUb1RleHQlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlVGV4dFRvVGV4dC5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageTextToText

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageTextToText.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Hk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVRleHRUb1RleHQlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW1hZ2VUZXh0VG9UZXh0LmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW1hZ2VUZXh0VG9UZXh0LmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlVGV4dFRvVGV4dC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageTextToText

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageTextToText.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageTextToText.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageTextToText.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Yk(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUaW1lU2VyaWVzUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGltZVNlcmllc1ByZWRpY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTimeSeriesPrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTimeSeriesPrediction.from_config(config)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Ok(F){let s,T="Examples:",c,t,g;return t=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUaW1lU2VyaWVzUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUaW1lU2VyaWVzUHJlZGljdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRpbWVTZXJpZXNQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRpbWVTZXJpZXNQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTimeSeriesPrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTimeSeriesPrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTimeSeriesPrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTimeSeriesPrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){s=i("p"),s.textContent=T,c=n(),u(t.$$.fragment)},l(o){s=l(o,"P",{"data-svelte-h":!0}),f(s)!=="svelte-kvfsh7"&&(s.textContent=T),c=a(o),p(t.$$.fragment,o)},m(o,C){h(o,s,C),h(o,c,C),_(t,o,C),g=!0},p:k,i(o){g||(v(t.$$.fragment,o),g=!0)},o(o){M(t.$$.fragment,o),g=!1},d(o){o&&(d(s),d(c)),b(t,o)}}}function Kk(F){let s,T,c,t,g,o,C,J5=`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the <code>from_pretrained()</code> method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`,yh,ts,D5=`Instantiating one of <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> will directly create a class of the relevant architecture. For instance`,wh,is,Lh,ls,U5='will create a model that is an instance of <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.',kh,ds,q5="There is one class of <code>AutoModel</code> for each task.",xh,ms,Ph,cs,z5=`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model <code>NewModel</code>, make sure you have a <code>NewModelConfig</code> then you can add those to the auto
classes like this:`,Gh,fs,Vh,gs,X5="You will then be able to use the auto classes like you would usually do!",Ah,vn,Sh,hs,Bh,ge,us,Sp,Xl,Q5=`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoConfig.from_pretrained">from_pretrained()</a> class method.`,Bp,Ql,H5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",$p,Mo,ps,Ip,Hl,Y5="Instantiate one of the configuration classes of the library from a pretrained model configuration.",jp,Yl,O5=`The configuration class to instantiate is selected based on the <code>model_type</code> property of the config object that
is loaded, or when its missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ep,Ol,K5='<li><strong>aimv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2Config">Aimv2Config</a> (AIMv2 model)</li> <li><strong>aimv2_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2VisionConfig">Aimv2VisionConfig</a> (Aimv2VisionModel model)</li> <li><strong>albert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> (AltCLIP model)</li> <li><strong>apertus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusConfig">ApertusConfig</a> (Apertus model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> (Arcee model)</li> <li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaConfig">AriaConfig</a> (Aria model)</li> <li><strong>aria_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextConfig">AriaTextConfig</a> (AriaText model)</li> <li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> (Autoformer model)</li> <li><strong>aya_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionConfig">AyaVisionConfig</a> (AyaVision model)</li> <li><strong>bamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaConfig">BambaConfig</a> (Bamba model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> (BART model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> (BEiT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> (BioGpt model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitConfig">BitConfig</a> (BiT model)</li> <li><strong>bitnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetConfig">BitNetConfig</a> (BitNet model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> (BLIP-2 model)</li> <li><strong>blip_2_qformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a> (BLIP-2 QFormer model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> (CANINE model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonConfig">ChameleonConfig</a> (Chameleon model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> (Chinese-CLIP model)</li> <li><strong>chinese_clip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionConfig">ChineseCLIPVisionConfig</a> (ChineseCLIPVisionModel model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapConfig">ClapConfig</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> (CLIP model)</li> <li><strong>clip_text_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextConfig">CLIPTextConfig</a> (CLIPTextModel model)</li> <li><strong>clip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> (CodeGen model)</li> <li><strong>cohere</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereConfig">CohereConfig</a> (Cohere model)</li> <li><strong>cohere2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2Config">Cohere2Config</a> (Cohere2 model)</li> <li><strong>cohere2_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig">Cohere2VisionConfig</a> (Cohere2Vision model)</li> <li><strong>colpali</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colpali#transformers.ColPaliConfig">ColPaliConfig</a> (ColPali model)</li> <li><strong>colqwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colqwen2#transformers.ColQwen2Config">ColQwen2Config</a> (ColQwen2 model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> (Conditional DETR model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> (ConvBERT model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> (CPM-Ant model)</li> <li><strong>csm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/csm#transformers.CsmConfig">CsmConfig</a> (CSM model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> (CTRL model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> (CvT model)</li> <li><strong>d_fine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineConfig">DFineConfig</a> (D-FINE model)</li> <li><strong>dab-detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig">DabDetrConfig</a> (DAB-DETR model)</li> <li><strong>dac</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacConfig">DacConfig</a> (DAC model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> (Data2VecVision model)</li> <li><strong>dbrx</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxConfig">DbrxConfig</a> (DBRX model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> (Decision Transformer model)</li> <li><strong>deepseek_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Config">DeepseekV2Config</a> (DeepSeek-V2 model)</li> <li><strong>deepseek_v3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Config">DeepseekV3Config</a> (DeepSeek-V3 model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig">DeepseekVLConfig</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig">DeepseekVLHybridConfig</a> (DeepseekVLHybrid model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> (DeiT model)</li> <li><strong>depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingConfig">DepthAnythingConfig</a> (Depth Anything model)</li> <li><strong>depth_pro</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProConfig">DepthProConfig</a> (DepthPro model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> (DETR model)</li> <li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaConfig">DiaConfig</a> (Dia model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> (DiffLlama model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> (DINOv2 model)</li> <li><strong>dinov2_with_registers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig">Dinov2WithRegistersConfig</a> (DINOv2 with Registers model)</li> <li><strong>dinov3_convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ConvNextConfig">DINOv3ConvNextConfig</a> (DINOv3 ConvNext model)</li> <li><strong>dinov3_vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ViTConfig">DINOv3ViTConfig</a> (DINOv3 ViT model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> (DistilBERT model)</li> <li><strong>doge</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeConfig">DogeConfig</a> (Doge model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinConfig">DonutSwinConfig</a> (DonutSwin model)</li> <li><strong>dots1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1Config">Dots1Config</a> (dots1 model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> (DPR model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> (EfficientFormer model)</li> <li><strong>efficientloftr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig">EfficientLoFTRConfig</a> (EfficientLoFTR model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> (EfficientNet model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> (ELECTRA model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Config">Emu3Config</a> (Emu3 model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecConfig">EncodecConfig</a> (EnCodec model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> (Encoder decoder model)</li> <li><strong>eomt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtConfig">EomtConfig</a> (EoMT model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> (ERNIE model)</li> <li><strong>ernie4_5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5Config">Ernie4_5Config</a> (Ernie4_5 model)</li> <li><strong>ernie4_5_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig">Ernie4_5_MoeConfig</a> (Ernie4_5_MoE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> (ESM model)</li> <li><strong>evolla</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaConfig">EvollaConfig</a> (Evolla model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> (Falcon model)</li> <li><strong>falcon_h1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1Config">FalconH1Config</a> (FalconH1 model)</li> <li><strong>falcon_mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaConfig">FalconMambaConfig</a> (FalconMamba model)</li> <li><strong>fastspeech2_conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig">FastSpeech2ConformerConfig</a> (FastSpeech2Conformer model)</li> <li><strong>fastspeech2_conformer_with_hifigan</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGanConfig">FastSpeech2ConformerWithHifiGanConfig</a> (FastSpeech2ConformerWithHifiGan model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> (FLAVA model)</li> <li><strong>florence2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Config">Florence2Config</a> (Florence2 model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> (FNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> (FocalNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> (Funnel Transformer model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> (Fuyu model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaConfig">GemmaConfig</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Config">Gemma2Config</a> (Gemma2 model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3TextConfig">Gemma3TextConfig</a> (Gemma3ForCausalLM model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nConfig">Gemma3nConfig</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>gemma3n_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nAudioConfig">Gemma3nAudioConfig</a> (Gemma3nAudioEncoder model)</li> <li><strong>gemma3n_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nTextConfig">Gemma3nTextConfig</a> (Gemma3nForCausalLM model)</li> <li><strong>gemma3n_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nVisionConfig">Gemma3nVisionConfig</a> (TimmWrapperModel model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig">GitConfig</a> (GIT model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmConfig">GlmConfig</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Config">Glm4Config</a> (GLM4 model)</li> <li><strong>glm4_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeConfig">Glm4MoeConfig</a> (Glm4MoE model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vConfig">Glm4vConfig</a> (GLM4V model)</li> <li><strong>glm4v_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig">Glm4vMoeConfig</a> (GLM4VMOE model)</li> <li><strong>glm4v_moe_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextConfig">Glm4vMoeTextConfig</a> (GLM4VMOE model)</li> <li><strong>glm4v_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vTextConfig">Glm4vTextConfig</a> (GLM4V model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> (GLPN model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Config">GotOcr2Config</a> (GOT-OCR2 model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> (GPT NeoX Japanese model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssConfig">GptOssConfig</a> (GptOss model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> (GPTSAN-japanese model)</li> <li><strong>granite</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteConfig">GraniteConfig</a> (Granite model)</li> <li><strong>granite_speech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechConfig">GraniteSpeechConfig</a> (GraniteSpeech model)</li> <li><strong>granitemoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeConfig">GraniteMoeConfig</a> (GraniteMoeMoe model)</li> <li><strong>granitemoehybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig">GraniteMoeHybridConfig</a> (GraniteMoeHybrid model)</li> <li><strong>granitemoeshared</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig">GraniteMoeSharedConfig</a> (GraniteMoeSharedMoe model)</li> <li><strong>granitevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextConfig">LlavaNextConfig</a> (LLaVA-NeXT model)</li> <li><strong>graphormer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/graphormer#transformers.GraphormerConfig">GraphormerConfig</a> (Graphormer model)</li> <li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoConfig">GroundingDinoConfig</a> (Grounding DINO model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/groupvit#transformers.GroupViTConfig">GroupViTConfig</a> (GroupViT model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumConfig">HeliumConfig</a> (Helium model)</li> <li><strong>hgnet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config">HGNetV2Config</a> (HGNet-V2 model)</li> <li><strong>hiera</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraConfig">HieraConfig</a> (Hiera model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> (Hubert model)</li> <li><strong>hunyuan_v1_dense</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config">HunYuanDenseV1Config</a> (HunYuanDenseV1 model)</li> <li><strong>hunyuan_v1_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config">HunYuanMoEV1Config</a> (HunYuanMoeV1 model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Config">Idefics2Config</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Config">Idefics3Config</a> (Idefics3 model)</li> <li><strong>idefics3_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3VisionConfig">Idefics3VisionConfig</a> (Idefics3VisionTransformer model)</li> <li><strong>ijepa</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaConfig">IJepaConfig</a> (I-JEPA model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> (ImageGPT model)</li> <li><strong>informer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/informer#transformers.InformerConfig">InformerConfig</a> (Informer model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipConfig">InstructBlipConfig</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a> (InstructBlipVideo model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLConfig">InternVLConfig</a> (InternVL model)</li> <li><strong>internvl_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLVisionConfig">InternVLVisionConfig</a> (InternVLVision model)</li> <li><strong>jamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaConfig">JambaConfig</a> (Jamba model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig">JanusConfig</a> (Janus model)</li> <li><strong>jetmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeConfig">JetMoeConfig</a> (JetMoe model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jukebox#transformers.JukeboxConfig">JukeboxConfig</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config">Kosmos2_5Config</a> (KOSMOS-2.5 model)</li> <li><strong>kyutai_speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig">KyutaiSpeechToTextConfig</a> (KyutaiSpeechToText model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> (LED model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> (LeViT model)</li> <li><strong>lfm2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2Config">Lfm2Config</a> (Lfm2 model)</li> <li><strong>lightglue</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueConfig">LightGlueConfig</a> (LightGlue model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> (LLaMA model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4Config">Llama4Config</a> (Llama4 model)</li> <li><strong>llama4_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4TextConfig">Llama4TextConfig</a> (Llama4ForCausalLM model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextConfig">LlavaNextConfig</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig">LlavaNextVideoConfig</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig">LlavaOnevisionConfig</a> (LLaVA-Onevision model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> (M2M100 model)</li> <li><strong>mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig">MambaConfig</a> (Mamba model)</li> <li><strong>mamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config">Mamba2Config</a> (mamba2 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> (Marian model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> (MarkupLM model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> (MaskFormer model)</li> <li><strong>maskformer-swin</strong>  <code>MaskFormerSwinConfig</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> (mBART model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> (M-CTC-T model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> (Megatron-BERT model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a> (MetaCLIP 2 model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrConfig">MgpstrConfig</a> (MGP-STR model)</li> <li><strong>mimi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mimi#transformers.MimiConfig">MimiConfig</a> (Mimi model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> (Mistral model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Config">Mistral3Config</a> (Mistral3 model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> (Mixtral model)</li> <li><strong>mlcd</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionConfig">MLCDVisionConfig</a> (MLCD model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaConfig">MllamaConfig</a> (Mllama model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig">MMGroundingDinoConfig</a> (MM Grounding DINO model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> (MobileViTV2 model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> (ModernBERT model)</li> <li><strong>modernbert-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig">ModernBertDecoderConfig</a> (ModernBertDecoder model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineConfig">MoonshineConfig</a> (Moonshine model)</li> <li><strong>moshi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiConfig">MoshiConfig</a> (Moshi model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> (MT5 model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenConfig">MusicgenConfig</a> (MusicGen model)</li> <li><strong>musicgen_melody</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig">MusicgenMelodyConfig</a> (MusicGen Melody model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> (MVP model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig">NatConfig</a> (NAT model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> (NLLB-MOE model)</li> <li><strong>nougat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> (Nougat model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> (Nystrmformer model)</li> <li><strong>olmo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoConfig">OlmoConfig</a> (OLMo model)</li> <li><strong>olmo2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2Config">Olmo2Config</a> (OLMo2 model)</li> <li><strong>olmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeConfig">OlmoeConfig</a> (OLMoE model)</li> <li><strong>omdet-turbo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig">OmDetTurboConfig</a> (OmDet-Turbo model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> (OneFormer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> (OPT model)</li> <li><strong>ovis2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Config">Ovis2Config</a> (Ovis2 model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> (OWL-ViT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaConfig">PaliGemmaConfig</a> (PaliGemma model)</li> <li><strong>patchtsmixer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig">PatchTSMixerConfig</a> (PatchTSMixer model)</li> <li><strong>patchtst</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/patchtst#transformers.PatchTSTConfig">PatchTSTConfig</a> (PatchTST model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> (Perceiver model)</li> <li><strong>perception_encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig">TimmWrapperConfig</a> (PerceptionEncoder model)</li> <li><strong>perception_lm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMConfig">PerceptionLMConfig</a> (PerceptionLM model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Config">Phi3Config</a> (Phi3 model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig">Phi4MultimodalConfig</a> (Phi4Multimodal model)</li> <li><strong>phimoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeConfig">PhimoeConfig</a> (Phimoe model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructConfig">Pix2StructConfig</a> (Pix2Struct model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralVisionConfig">PixtralVisionConfig</a> (Pixtral model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> (PLBart model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> (PoolFormer model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig">Pop2PianoConfig</a> (Pop2Piano model)</li> <li><strong>prompt_depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig">PromptDepthAnythingConfig</a> (PromptDepthAnything model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> (ProphetNet model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> (PVT model)</li> <li><strong>pvt_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config">PvtV2Config</a> (PVTv2 model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> (Qwen2 model)</li> <li><strong>qwen2_5_omni</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniConfig">Qwen2_5OmniConfig</a> (Qwen2_5Omni model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig">Qwen2_5_VLConfig</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_5_vl_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextConfig">Qwen2_5_VLTextConfig</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig">Qwen2AudioConfig</a> (Qwen2Audio model)</li> <li><strong>qwen2_audio_encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig">Qwen2AudioEncoderConfig</a> (Qwen2AudioEncoder model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> (Qwen2MoE model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig">Qwen2VLConfig</a> (Qwen2VL model)</li> <li><strong>qwen2_vl_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLTextConfig">Qwen2VLTextConfig</a> (Qwen2VL model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> (Qwen3MoE model)</li> <li><strong>rag</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rag#transformers.RagConfig">RagConfig</a> (RAG model)</li> <li><strong>realm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a> (REALM model)</li> <li><strong>recurrent_gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig">RecurrentGemmaConfig</a> (RecurrentGemma model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> (Reformer model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> (RegNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> (RemBERT model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> (ResNet model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> (RoFormer model)</li> <li><strong>rt_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrConfig">RTDetrConfig</a> (RT-DETR model)</li> <li><strong>rt_detr_resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrResNetConfig">RTDetrResNetConfig</a> (RT-DETR-ResNet model)</li> <li><strong>rt_detr_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config">RTDetrV2Config</a> (RT-DETRv2 model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> (RWKV model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig">SamConfig</a> (SAM model)</li> <li><strong>sam2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config">Sam2Config</a> (SAM2 model)</li> <li><strong>sam2_hiera_det_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2HieraDetConfig">Sam2HieraDetConfig</a> (Sam2HieraDetModel model)</li> <li><strong>sam2_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoConfig">Sam2VideoConfig</a> (Sam2VideoModel model)</li> <li><strong>sam2_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionConfig">Sam2VisionConfig</a> (Sam2VisionModel model)</li> <li><strong>sam_hq</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQConfig">SamHQConfig</a> (SAM-HQ model)</li> <li><strong>sam_hq_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQVisionConfig">SamHQVisionConfig</a> (SamHQVisionModel model)</li> <li><strong>sam_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionConfig">SamVisionConfig</a> (SamVisionModel model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> (SeamlessM4Tv2 model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> (SeedOss model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> (SegFormer model)</li> <li><strong>seggpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig">SegGptConfig</a> (SegGPT model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> (SEW-D model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config">ShieldGemma2Config</a> (Shieldgemma2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Config">Siglip2Config</a> (SigLIP2 model)</li> <li><strong>siglip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipVisionConfig">SiglipVisionConfig</a> (SiglipVisionModel model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> (SmolLM3 model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMConfig">SmolVLMConfig</a> (SmolVLM model)</li> <li><strong>smolvlm_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMVisionConfig">SmolVLMVisionConfig</a> (SmolVLMVisionTransformer model)</li> <li><strong>speech-encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Config">Starcoder2Config</a> (Starcoder2 model)</li> <li><strong>superglue</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueConfig">SuperGlueConfig</a> (SuperGlue model)</li> <li><strong>superpoint</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointConfig">SuperPointConfig</a> (SuperPoint model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRConfig">Swin2SRConfig</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> (T5Gemma model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> (Table Transformer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> (TAPAS model)</li> <li><strong>textnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetConfig">TextNetConfig</a> (TextNet model)</li> <li><strong>time_series_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig">TimeSeriesTransformerConfig</a> (Time Series Transformer model)</li> <li><strong>timesfm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig">TimesFmConfig</a> (TimesFm model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> (TimeSformer model)</li> <li><strong>timm_backbone</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.TimmBackboneConfig">TimmBackboneConfig</a> (TimmBackbone model)</li> <li><strong>timm_wrapper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig">TimmWrapperConfig</a> (TimmWrapperModel model)</li> <li><strong>trajectory_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig">TrajectoryTransformerConfig</a> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> (Transformer-XL model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> (TrOCR model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpConfig">TvpConfig</a> (TVP model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopConfig">UdopConfig</a> (UDOP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> (UMT5 model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetConfig">UnivNetConfig</a> (UnivNet model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetConfig">UperNetConfig</a> (UPerNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig">VanConfig</a> (VAN model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaConfig">VideoLlavaConfig</a> (VideoLlava model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> (Vision Encoder decoder model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> (VisualBERT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> (ViTMSN model)</li> <li><strong>vitdet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a> (VitDet model)</li> <li><strong>vitmatte</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vitmatte#transformers.VitMatteConfig">VitMatteConfig</a> (ViTMatte model)</li> <li><strong>vitpose</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vitpose#transformers.VitPoseConfig">VitPoseConfig</a> (ViTPose model)</li> <li><strong>vitpose_backbone</strong>  <code>VitPoseBackboneConfig</code> (ViTPoseBackbone model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vits#transformers.VitsConfig">VitsConfig</a> (VITS model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> (ViViT model)</li> <li><strong>vjepa2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config">VJEPA2Config</a> (VJEPA2Model model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralConfig">VoxtralConfig</a> (Voxtral model)</li> <li><strong>voxtral_encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralEncoderConfig">VoxtralEncoderConfig</a> (Voxtral Encoder model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xclip#transformers.XCLIPConfig">XCLIPConfig</a> (X-CLIP model)</li> <li><strong>xcodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecConfig">XcodecConfig</a> (X-CODEC model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> (XLNet model)</li> <li><strong>xlstm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMConfig">xLSTMConfig</a> (xLSTM model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> (X-MOD model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> (YOLOS model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> (YOSO model)</li> <li><strong>zamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaConfig">ZambaConfig</a> (Zamba model)</li> <li><strong>zamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Config">Zamba2Config</a> (Zamba2 model)</li> <li><strong>zoedepth</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthConfig">ZoeDepthConfig</a> (ZoeDepth model)</li>',Zp,Mn,Rp,bn,_s,Wp,Kl,e4="Register a new configuration for this class.",$h,vs,Ih,he,Ms,Np,ed,o4=`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained">AutoTokenizer.from_pretrained()</a> class method.`,Jp,od,r4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Dp,bo,bs,Up,rd,n4="Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.",qp,nd,a4=`The tokenizer class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,zp,ad,s4='<li><strong>aimv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (AIMv2 model)</li> <li><strong>albert</strong>  <code>AlbertTokenizer</code> or <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ALIGN model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Arcee model)</li> <li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Aria model)</li> <li><strong>aya_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereTokenizerFast">CohereTokenizerFast</a> (AyaVision model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartTokenizer">BartTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartTokenizerFast">BartTokenizerFast</a> (BART model)</li> <li><strong>barthez</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/barthez#transformers.BarthezTokenizer">BarthezTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/barthez#transformers.BarthezTokenizerFast">BarthezTokenizerFast</a> (BARThez model)</li> <li><strong>bartpho</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bartpho#transformers.BartphoTokenizer">BartphoTokenizer</a> (BARTpho model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationTokenizer">BertGenerationTokenizer</a> (Bert Generation model)</li> <li><strong>bert-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer">BertJapaneseTokenizer</a> (BertJapanese model)</li> <li><strong>bertweet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bertweet#transformers.BertweetTokenizer">BertweetTokenizer</a> (BERTweet model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdTokenizer">BigBirdTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdTokenizerFast">BigBirdTokenizerFast</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptTokenizer">BioGptTokenizer</a> (BioGpt model)</li> <li><strong>bitnet</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (BitNet model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotTokenizer">BlenderbotTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast">BlenderbotTokenizerFast</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer">BlenderbotSmallTokenizer</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (BLIP-2 model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomTokenizerFast">BloomTokenizerFast</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BROS model)</li> <li><strong>byt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/byt5#transformers.ByT5Tokenizer">ByT5Tokenizer</a> (ByT5 model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertTokenizer">CamembertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertTokenizerFast">CamembertTokenizerFast</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineTokenizer">CanineTokenizer</a> (CANINE model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Chameleon model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpTokenizer">ClvpTokenizer</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/code_llama#transformers.CodeLlamaTokenizer">CodeLlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/code_llama#transformers.CodeLlamaTokenizerFast">CodeLlamaTokenizerFast</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (CodeGen model)</li> <li><strong>cohere</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereTokenizerFast">CohereTokenizerFast</a> (Cohere model)</li> <li><strong>cohere2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereTokenizerFast">CohereTokenizerFast</a> (Cohere2 model)</li> <li><strong>colpali</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (ColPali model)</li> <li><strong>colqwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (ColQwen2 model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertTokenizer">ConvBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertTokenizerFast">ConvBertTokenizerFast</a> (ConvBERT model)</li> <li><strong>cpm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cpm#transformers.CpmTokenizer">CpmTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/cpm#transformers.CpmTokenizerFast">CpmTokenizerFast</a> (CPM model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntTokenizer">CpmAntTokenizer</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLTokenizer">CTRLTokenizer</a> (CTRL model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (Data2VecText model)</li> <li><strong>dbrx</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (DBRX model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaTokenizer">DebertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaTokenizerFast">DebertaTokenizerFast</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer">DebertaV2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2TokenizerFast">DebertaV2TokenizerFast</a> (DeBERTa-v2 model)</li> <li><strong>deepseek_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (DeepSeek-V2 model)</li> <li><strong>deepseek_v3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (DeepSeek-V3 model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (DeepseekVLHybrid model)</li> <li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaTokenizer">DiaTokenizer</a> (Dia model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (DiffLlama model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertTokenizerFast">DistilBertTokenizerFast</a> (DistilBERT model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer">DPRQuestionEncoderTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast">DPRQuestionEncoderTokenizerFast</a> (DPR model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraTokenizer">ElectraTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraTokenizerFast">ElectraTokenizerFast</a> (ELECTRA model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (Emu3 model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ERNIE model)</li> <li><strong>ernie4_5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Ernie4_5 model)</li> <li><strong>ernie4_5_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Ernie4_5_MoE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMTokenizer">ErnieMTokenizer</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmTokenizer">EsmTokenizer</a> (ESM model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Falcon model)</li> <li><strong>falcon_mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (FalconMamba model)</li> <li><strong>fastspeech2_conformer</strong>   (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertTokenizer">FlaubertTokenizer</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetTokenizer">FNetTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetTokenizerFast">FNetTokenizerFast</a> (FNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelTokenizerFast">FunnelTokenizerFast</a> (Funnel Transformer model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma2 model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma3ForCausalLM model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>gemma3n_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Gemma3nForCausalLM model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (GIT model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (GLM4 model)</li> <li><strong>glm4_moe</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Glm4MoE model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (GLM4V model)</li> <li><strong>glm4v_moe</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (GLM4VMOE model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt-sw3#transformers.GPTSw3Tokenizer">GPTSw3Tokenizer</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseTokenizer">GPTNeoXJapaneseTokenizer</a> (GPT NeoX Japanese model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (GptOss model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseTokenizer">GPTSanJapaneseTokenizer</a> (GPTSAN-japanese model)</li> <li><strong>granite</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> (Granite model)</li> <li><strong>granitemoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> (GraniteMoeMoe model)</li> <li><strong>granitemoehybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> (GraniteMoeHybrid model)</li> <li><strong>granitemoeshared</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> (GraniteMoeSharedMoe model)</li> <li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Grounding DINO model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (GroupViT model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Helium model)</li> <li><strong>herbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/herbert#transformers.HerbertTokenizer">HerbertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/herbert#transformers.HerbertTokenizerFast">HerbertTokenizerFast</a> (HerBERT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Hubert model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Idefics3 model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (InstructBlipVideo model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (InternVL model)</li> <li><strong>jamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Jamba model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Janus model)</li> <li><strong>jetmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (JetMoe model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jukebox#transformers.JukeboxTokenizer">JukeboxTokenizer</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (KOSMOS-2.5 model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMTokenizer">LayoutLMTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast">LayoutLMTokenizerFast</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer">LayoutLMv2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast">LayoutLMv2TokenizerFast</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer">LayoutLMv3Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast">LayoutLMv3TokenizerFast</a> (LayoutLMv3 model)</li> <li><strong>layoutxlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer">LayoutXLMTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast">LayoutXLMTokenizerFast</a> (LayoutXLM model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDTokenizer">LEDTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDTokenizerFast">LEDTokenizerFast</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer">LayoutLMv3Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast">LayoutLMv3TokenizerFast</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaMA model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Llama4 model)</li> <li><strong>llama4_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Llama4ForCausalLM model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaVA-Onevision model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerTokenizer">LongformerTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerTokenizerFast">LongformerTokenizerFast</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeTokenizer">LukeTokenizer</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertTokenizer">LxmertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertTokenizerFast">LxmertTokenizerFast</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Tokenizer">M2M100Tokenizer</a> (M2M100 model)</li> <li><strong>mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (Mamba model)</li> <li><strong>mamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (mamba2 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianTokenizer">MarianTokenizer</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartTokenizer">MBartTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartTokenizerFast">MBartTokenizerFast</a> (mBART model)</li> <li><strong>mbart50</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBart50Tokenizer">MBart50Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBart50TokenizerFast">MBart50TokenizerFast</a> (mBART-50 model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Megatron-BERT model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (MetaCLIP 2 model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrTokenizer">MgpstrTokenizer</a> (MGP-STR model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer">MistralCommonTokenizer</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer">MistralCommonTokenizer</a> (Mixtral model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Mllama model)</li> <li><strong>mluke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mluke#transformers.MLukeTokenizer">MLukeTokenizer</a> (mLUKE model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (MM Grounding DINO model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertTokenizer">MobileBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast">MobileBertTokenizerFast</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (ModernBERT model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Moonshine model)</li> <li><strong>moshi</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Moshi model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetTokenizer">MPNetTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetTokenizerFast">MPNetTokenizerFast</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Tokenizer">MT5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5TokenizerFast">MT5TokenizerFast</a> (MT5 model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (MusicGen model)</li> <li><strong>musicgen_melody</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (MusicGen Melody model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpTokenizer">MvpTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpTokenizerFast">MvpTokenizerFast</a> (MVP model)</li> <li><strong>myt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/myt5#transformers.MyT5Tokenizer">MyT5Tokenizer</a> (myt5 model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Nezha model)</li> <li><strong>nllb</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb#transformers.NllbTokenizer">NllbTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/nllb#transformers.NllbTokenizerFast">NllbTokenizerFast</a> (NLLB model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb#transformers.NllbTokenizer">NllbTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/nllb#transformers.NllbTokenizerFast">NllbTokenizerFast</a> (NLLB-MOE model)</li> <li><strong>nystromformer</strong>  <code>AlbertTokenizer</code> or <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (Nystrmformer model)</li> <li><strong>olmo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (OLMo model)</li> <li><strong>olmo2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (OLMo2 model)</li> <li><strong>olmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (OLMoE model)</li> <li><strong>omdet-turbo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OmDet-Turbo model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OneFormer model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer">OpenAIGPTTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast">OpenAIGPTTokenizerFast</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (OPT model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWL-ViT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (PaliGemma model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverTokenizer">PerceiverTokenizer</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Phi3 model)</li> <li><strong>phimoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Phimoe model)</li> <li><strong>phobert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phobert#transformers.PhobertTokenizer">PhobertTokenizer</a> (PhoBERT model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (Pix2Struct model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer">MistralCommonTokenizer</a> (Pixtral model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartTokenizer">PLBartTokenizer</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a> (ProphetNet model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2 model)</li> <li><strong>qwen2_5_omni</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2_5Omni model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2Audio model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2MoE model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2VL model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen3MoE model)</li> <li><strong>rag</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rag#transformers.RagTokenizer">RagTokenizer</a> (RAG model)</li> <li><strong>realm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/realm#transformers.RealmTokenizerFast">RealmTokenizerFast</a> (REALM model)</li> <li><strong>recurrent_gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (RecurrentGemma model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerTokenizerFast">ReformerTokenizerFast</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertTokenizer">RemBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertTokenizerFast">RemBertTokenizerFast</a> (RemBERT model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertTokenizer">RetriBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertTokenizerFast">RetriBertTokenizerFast</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertTokenizer">RoCBertTokenizer</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerTokenizer">RoFormerTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerTokenizerFast">RoFormerTokenizerFast</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (RWKV model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a> (SeamlessM4Tv2 model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (Shieldgemma2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipTokenizer">SiglipTokenizer</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (SigLIP2 model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (SmolLM3 model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer">SpeechT5Tokenizer</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterTokenizer">SplinterTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterTokenizerFast">SplinterTokenizerFast</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer">SqueezeBertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast">SqueezeBertTokenizerFast</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (Starcoder2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizer">GemmaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaTokenizerFast">GemmaTokenizerFast</a> (T5Gemma model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasTokenizer">TapasTokenizer</a> (TAPAS model)</li> <li><strong>tapex</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapex#transformers.TapexTokenizer">TapexTokenizer</a> (TAPEX model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer">TransfoXLTokenizer</a> (Transformer-XL model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (TVP model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizer">UdopTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizerFast">UdopTokenizerFast</a> (UDOP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5TokenizerFast">T5TokenizerFast</a> (UMT5 model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (VideoLlava model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (VipLlava model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (VisualBERT model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vits#transformers.VitsTokenizer">VitsTokenizer</a> (VITS model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer">MistralCommonTokenizer</a> (Voxtral model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2-Conformer model)</li> <li><strong>wav2vec2_phoneme</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer">Wav2Vec2PhonemeCTCTokenizer</a> (Wav2Vec2Phoneme model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperTokenizer">WhisperTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperTokenizerFast">WhisperTokenizerFast</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (X-CLIP model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMTokenizer">XGLMTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMTokenizerFast">XGLMTokenizerFast</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMTokenizer">XLMTokenizer</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer">XLMProphetNetTokenizer</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetTokenizer">XLNetTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetTokenizerFast">XLNetTokenizerFast</a> (XLNet model)</li> <li><strong>xlstm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (xLSTM model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (X-MOD model)</li> <li><strong>yoso</strong>  <code>AlbertTokenizer</code> or <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (YOSO model)</li> <li><strong>zamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Zamba model)</li> <li><strong>zamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Zamba2 model)</li>',Xp,Cn,Qp,Tn,Cs,Hp,sd,t4="Register a new tokenizer in this mapping.",jh,Ts,Eh,ue,Fs,Yp,td,i4=`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained">AutoFeatureExtractor.from_pretrained()</a> class method.`,Op,id,l4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Kp,V,ys,e_,ld,d4="Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary.",o_,dd,m4=`The feature extractor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,r_,md,c4='<li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor">ASTFeatureExtractor</a> (Audio Spectrogram Transformer model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (BEiT model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPFeatureExtractor">ChineseCLIPFeatureExtractor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapFeatureExtractor">ClapFeatureExtractor</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor">ClvpFeatureExtractor</a> (CLVP model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrFeatureExtractor">ConditionalDetrFeatureExtractor</a> (Conditional DETR model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ConvNeXT model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (CvT model)</li> <li><strong>dac</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacFeatureExtractor">DacFeatureExtractor</a> (DAC model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Data2VecAudio model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (Data2VecVision model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrFeatureExtractor">DeformableDetrFeatureExtractor</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTFeatureExtractor">DeiTFeatureExtractor</a> (DeiT model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (DETR model)</li> <li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaFeatureExtractor">DiaFeatureExtractor</a> (Dia model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (DiNAT model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutFeatureExtractor">DonutFeatureExtractor</a> (DonutSwin model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTFeatureExtractor">DPTFeatureExtractor</a> (DPT model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor">EncodecFeatureExtractor</a> (EnCodec model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a> (FLAVA model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nAudioFeatureExtractor">Gemma3nAudioFeatureExtractor</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor">GLPNFeatureExtractor</a> (GLPN model)</li> <li><strong>granite_speech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechFeatureExtractor">GraniteSpeechFeatureExtractor</a> (GraniteSpeech model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Hubert model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor">ImageGPTFeatureExtractor</a> (ImageGPT model)</li> <li><strong>kyutai_speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextFeatureExtractor">KyutaiSpeechToTextFeatureExtractor</a> (KyutaiSpeechToText model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor">LayoutLMv2FeatureExtractor</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3FeatureExtractor">LayoutLMv3FeatureExtractor</a> (LayoutLMv3 model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitFeatureExtractor">LevitFeatureExtractor</a> (LeViT model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor">MaskFormerFeatureExtractor</a> (MaskFormer model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTFeatureExtractor">MCTCTFeatureExtractor</a> (M-CTC-T model)</li> <li><strong>mimi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor">EncodecFeatureExtractor</a> (Mimi model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1FeatureExtractor">MobileNetV1FeatureExtractor</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2FeatureExtractor">MobileNetV2FeatureExtractor</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor">MobileViTFeatureExtractor</a> (MobileViT model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Moonshine model)</li> <li><strong>moshi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor">EncodecFeatureExtractor</a> (Moshi model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (NAT model)</li> <li><strong>owlvit</strong>  <code>OwlViTFeatureExtractor</code> (OWL-ViT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor">PerceiverFeatureExtractor</a> (Perceiver model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalFeatureExtractor">Phi4MultimodalFeatureExtractor</a> (Phi4Multimodal model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor">PoolFormerFeatureExtractor</a> (PoolFormer model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.models.pop2piano.feature_extraction_pop2piano._LazyModule.__getattr__.%3Clocals%3E.Placeholder">Pop2PianoFeatureExtractor</a> (Pop2Piano model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ResNet model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor">SeamlessM4TFeatureExtractor</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor">SeamlessM4TFeatureExtractor</a> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor">SegformerFeatureExtractor</a> (SegFormer model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (SEW-D model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor">Speech2TextFeatureExtractor</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor">SpeechT5FeatureExtractor</a> (SpeechT5 model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (Table Transformer model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEFeatureExtractor">VideoMAEFeatureExtractor</a> (TimeSformer model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltFeatureExtractor">TvltFeatureExtractor</a> (TVLT model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> (UnivNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEFeatureExtractor">VideoMAEFeatureExtractor</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltFeatureExtractor">ViltFeatureExtractor</a> (ViLT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViT model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViTMSN model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (X-CLIP model)</li> <li><strong>xcodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacFeatureExtractor">DacFeatureExtractor</a> (X-CODEC model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosFeatureExtractor">YolosFeatureExtractor</a> (YOLOS model)</li>',n_,Fn,a_,yn,s_,wn,ws,t_,cd,f4="Register a new feature extractor for this class.",Zh,Ls,Rh,pe,ks,i_,fd,g4=`This is a generic image processor class that will be instantiated as one of the image processor classes of the
library when created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained">AutoImageProcessor.from_pretrained()</a> class method.`,l_,gd,h4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",d_,A,xs,m_,hd,u4="Instantiate one of the image processor classes of the library from a pretrained model vocabulary.",c_,ud,p4=`The image processor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,f_,pd,_4='<li><strong>aimv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (AIMv2 model)</li> <li><strong>aimv2_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (Aimv2VisionModel model)</li> <li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessorFast">EfficientNetImageProcessorFast</a> (ALIGN model)</li> <li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaImageProcessor">AriaImageProcessor</a> (Aria model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitImageProcessorFast">BeitImageProcessorFast</a> (BEiT model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessorFast">BitImageProcessorFast</a> (BiT model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessorFast">BlipImageProcessorFast</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessorFast">BlipImageProcessorFast</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessorFast">BridgeTowerImageProcessorFast</a> (BridgeTower model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonImageProcessor">ChameleonImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonImageProcessorFast">ChameleonImageProcessorFast</a> (Chameleon model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessor">ChineseCLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessorFast">ChineseCLIPImageProcessorFast</a> (Chinese-CLIP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (CLIPSeg model)</li> <li><strong>cohere2_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionImageProcessorFast">Cohere2VisionImageProcessorFast</a> (Cohere2Vision model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor">ConditionalDetrImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessorFast">ConditionalDetrImageProcessorFast</a> (Conditional DETR model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitImageProcessorFast">BeitImageProcessorFast</a> (Data2VecVision model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLImageProcessor">DeepseekVLImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLImageProcessorFast">DeepseekVLImageProcessorFast</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridImageProcessor">DeepseekVLHybridImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridImageProcessorFast">DeepseekVLHybridImageProcessorFast</a> (DeepseekVLHybrid model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrImageProcessor">DeformableDetrImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrImageProcessorFast">DeformableDetrImageProcessorFast</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTImageProcessor">DeiTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTImageProcessorFast">DeiTImageProcessorFast</a> (DeiT model)</li> <li><strong>depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessorFast">DPTImageProcessorFast</a> (Depth Anything model)</li> <li><strong>depth_pro</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessor">DepthProImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessorFast">DepthProImageProcessorFast</a> (DepthPro model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaImageProcessor">DetaImageProcessor</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrImageProcessorFast">DetrImageProcessorFast</a> (DETR model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessorFast">BitImageProcessorFast</a> (DINOv2 model)</li> <li><strong>dinov3_vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ViTImageProcessorFast">DINOv3ViTImageProcessorFast</a> (DINOv3 ViT model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutImageProcessor">DonutImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutImageProcessorFast">DonutImageProcessorFast</a> (DonutSwin model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessorFast">DPTImageProcessorFast</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerImageProcessor">EfficientFormerImageProcessor</a> (EfficientFormer model)</li> <li><strong>efficientloftr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRImageProcessor">EfficientLoFTRImageProcessor</a> (EfficientLoFTR model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessorFast">EfficientNetImageProcessorFast</a> (EfficientNet model)</li> <li><strong>eomt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtImageProcessor">EomtImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtImageProcessorFast">EomtImageProcessorFast</a> (EoMT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaImageProcessorFast">FlavaImageProcessorFast</a> (FLAVA model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessorFast">BitImageProcessorFast</a> (FocalNet model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor">FuyuImageProcessor</a> (Fuyu model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ImageProcessor">Gemma3ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ImageProcessorFast">Gemma3ImageProcessorFast</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessor">SiglipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessorFast">SiglipImageProcessorFast</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (GIT model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vImageProcessor">Glm4vImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vImageProcessorFast">Glm4vImageProcessorFast</a> (GLM4V model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNImageProcessor">GLPNImageProcessor</a> (GLPN model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessor">GotOcr2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ImageProcessorFast">GotOcr2ImageProcessorFast</a> (GOT-OCR2 model)</li> <li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessorFast">GroundingDinoImageProcessorFast</a> (Grounding DINO model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (GroupViT model)</li> <li><strong>hiera</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitImageProcessorFast">BitImageProcessorFast</a> (Hiera model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsImageProcessor">IdeficsImageProcessor</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ImageProcessor">Idefics2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ImageProcessorFast">Idefics2ImageProcessorFast</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ImageProcessor">Idefics3ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ImageProcessorFast">Idefics3ImageProcessorFast</a> (Idefics3 model)</li> <li><strong>ijepa</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (I-JEPA model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor">ImageGPTImageProcessor</a> (ImageGPT model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessorFast">BlipImageProcessorFast</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a> (InstructBlipVideo model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessorFast">JanusImageProcessorFast</a> (Janus model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessor">Kosmos2_5ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ImageProcessorFast">Kosmos2_5ImageProcessorFast</a> (KOSMOS-2.5 model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor">LayoutLMv2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessorFast">LayoutLMv2ImageProcessorFast</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessorFast">LayoutLMv3ImageProcessorFast</a> (LayoutLMv3 model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitImageProcessor">LevitImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitImageProcessorFast">LevitImageProcessorFast</a> (LeViT model)</li> <li><strong>lightglue</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueImageProcessor">LightGlueImageProcessor</a> (LightGlue model)</li> <li><strong>llama4</strong>  <code>Llama4ImageProcessor</code> or <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ImageProcessorFast">Llama4ImageProcessorFast</a> (Llama4 model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaImageProcessor">LlavaImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaImageProcessorFast">LlavaImageProcessorFast</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextImageProcessor">LlavaNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next#transformers.LlavaNextImageProcessorFast">LlavaNextImageProcessorFast</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoImageProcessor">LlavaNextVideoImageProcessor</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionImageProcessor">LlavaOnevisionImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionImageProcessorFast">LlavaOnevisionImageProcessorFast</a> (LLaVA-Onevision model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerImageProcessor">Mask2FormerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerImageProcessorFast">Mask2FormerImageProcessorFast</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerImageProcessor">MaskFormerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerImageProcessorFast">MaskFormerImageProcessorFast</a> (MaskFormer model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (MetaCLIP 2 model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (MGP-STR model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralImageProcessor">PixtralImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralImageProcessorFast">PixtralImageProcessorFast</a> (Mistral3 model)</li> <li><strong>mlcd</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (MLCD model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaImageProcessor">MllamaImageProcessor</a> (Mllama model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessorFast">GroundingDinoImageProcessorFast</a> (MM Grounding DINO model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1ImageProcessor">MobileNetV1ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1ImageProcessorFast">MobileNetV1ImageProcessorFast</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessor">MobileNetV2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessorFast">MobileNetV2ImageProcessorFast</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessorFast">MobileViTImageProcessorFast</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessorFast">MobileViTImageProcessorFast</a> (MobileViTV2 model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (NAT model)</li> <li><strong>nougat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/nougat#transformers.NougatImageProcessorFast">NougatImageProcessorFast</a> (Nougat model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor">OneFormerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerImageProcessorFast">OneFormerImageProcessorFast</a> (OneFormer model)</li> <li><strong>ovis2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2ImageProcessor">Ovis2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2ImageProcessorFast">Ovis2ImageProcessorFast</a> (Ovis2 model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2ImageProcessor">Owlv2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2ImageProcessorFast">Owlv2ImageProcessorFast</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTImageProcessor">OwlViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTImageProcessorFast">OwlViTImageProcessorFast</a> (OWL-ViT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessor">SiglipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessorFast">SiglipImageProcessorFast</a> (PaliGemma model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverImageProcessor">PerceiverImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverImageProcessorFast">PerceiverImageProcessorFast</a> (Perceiver model)</li> <li><strong>perception_lm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMImageProcessorFast">PerceptionLMImageProcessorFast</a> (PerceptionLM model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalImageProcessorFast">Phi4MultimodalImageProcessorFast</a> (Phi4Multimodal model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor">Pix2StructImageProcessor</a> (Pix2Struct model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralImageProcessor">PixtralImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralImageProcessorFast">PixtralImageProcessorFast</a> (Pixtral model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerImageProcessor">PoolFormerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerImageProcessorFast">PoolFormerImageProcessorFast</a> (PoolFormer model)</li> <li><strong>prompt_depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingImageProcessor">PromptDepthAnythingImageProcessor</a> (PromptDepthAnything model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessorFast">PvtImageProcessorFast</a> (PVT model)</li> <li><strong>pvt_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtImageProcessorFast">PvtImageProcessorFast</a> (PVTv2 model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor">Qwen2VLImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast">Qwen2VLImageProcessorFast</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessor">Qwen2VLImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLImageProcessorFast">Qwen2VLImageProcessorFast</a> (Qwen2VL model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (ResNet model)</li> <li><strong>rt_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrImageProcessor">RTDetrImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrImageProcessorFast">RTDetrImageProcessorFast</a> (RT-DETR model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessorFast">SamImageProcessorFast</a> (SAM model)</li> <li><strong>sam2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2ImageProcessorFast">Sam2ImageProcessorFast</a> (SAM2 model)</li> <li><strong>sam_hq</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamImageProcessorFast">SamImageProcessorFast</a> (SAM-HQ model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessorFast">SegformerImageProcessorFast</a> (SegFormer model)</li> <li><strong>seggpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptImageProcessor">SegGptImageProcessor</a> (SegGPT model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ImageProcessor">Gemma3ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ImageProcessorFast">Gemma3ImageProcessorFast</a> (Shieldgemma2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessor">SiglipImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipImageProcessorFast">SiglipImageProcessorFast</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2ImageProcessor">Siglip2ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2ImageProcessorFast">Siglip2ImageProcessorFast</a> (SigLIP2 model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMImageProcessor">SmolVLMImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMImageProcessorFast">SmolVLMImageProcessorFast</a> (SmolVLM model)</li> <li><strong>superglue</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueImageProcessor">SuperGlueImageProcessor</a> (SuperGlue model)</li> <li><strong>superpoint</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointImageProcessor">SuperPointImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/superpoint#transformers.SuperPointImageProcessorFast">SuperPointImageProcessorFast</a> (SuperPoint model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRImageProcessor">Swin2SRImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRImageProcessorFast">Swin2SRImageProcessorFast</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrImageProcessorFast">DetrImageProcessorFast</a> (Table Transformer model)</li> <li><strong>textnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetImageProcessor">TextNetImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetImageProcessorFast">TextNetImageProcessorFast</a> (TextNet model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEImageProcessor">VideoMAEImageProcessor</a> (TimeSformer model)</li> <li><strong>timm_wrapper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor">TimmWrapperImageProcessor</a> (TimmWrapperModel model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltImageProcessor">TvltImageProcessor</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessorFast">TvpImageProcessorFast</a> (TVP model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessorFast">LayoutLMv3ImageProcessorFast</a> (UDOP model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessorFast">SegformerImageProcessorFast</a> (UPerNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextImageProcessorFast">ConvNextImageProcessorFast</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEImageProcessor">VideoMAEImageProcessor</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltImageProcessor">ViltImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltImageProcessorFast">ViltImageProcessorFast</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (VipLlava model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridImageProcessor">ViTHybridImageProcessor</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessorFast">ViTImageProcessorFast</a> (ViTMSN model)</li> <li><strong>vitmatte</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vitmatte#transformers.VitMatteImageProcessor">VitMatteImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/vitmatte#transformers.VitMatteImageProcessorFast">VitMatteImageProcessorFast</a> (ViTMatte model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessorFast">CLIPImageProcessorFast</a> (X-CLIP model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosImageProcessor">YolosImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosImageProcessorFast">YolosImageProcessorFast</a> (YOLOS model)</li> <li><strong>zoedepth</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthImageProcessor">ZoeDepthImageProcessor</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthImageProcessorFast">ZoeDepthImageProcessorFast</a> (ZoeDepth model)</li>',g_,Ln,h_,kn,u_,xn,Ps,p_,_d,v4="Register a new image processor for this class.",Wh,Gs,Nh,_e,Vs,__,vd,M4=`This is a generic video processor class that will be instantiated as one of the video processor classes of the
library when created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoVideoProcessor.from_pretrained">AutoVideoProcessor.from_pretrained()</a> class method.`,v_,Md,b4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",M_,S,As,b_,bd,C4="Instantiate one of the video processor classes of the library from a pretrained model vocabulary.",C_,Cd,T4=`The video processor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,T_,Td,F4='<li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vVideoProcessor">Glm4vVideoProcessor</a> (GLM4V model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVideoProcessor">InstructBlipVideoVideoProcessor</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVideoProcessor">InstructBlipVideoVideoProcessor</a> (InstructBlipVideo model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLVideoProcessor">InternVLVideoProcessor</a> (InternVL model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoVideoProcessor">LlavaNextVideoVideoProcessor</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionVideoProcessor">LlavaOnevisionVideoProcessor</a> (LLaVA-Onevision model)</li> <li><strong>qwen2_5_omni</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a> (Qwen2_5Omni model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLVideoProcessor">Qwen2VLVideoProcessor</a> (Qwen2VL model)</li> <li><strong>sam2_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoVideoProcessor">Sam2VideoVideoProcessor</a> (Sam2VideoModel model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMVideoProcessor">SmolVLMVideoProcessor</a> (SmolVLM model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaVideoProcessor">VideoLlavaVideoProcessor</a> (VideoLlava model)</li> <li><strong>vjepa2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2VideoProcessor">VJEPA2VideoProcessor</a> (VJEPA2Model model)</li>',F_,Pn,y_,Gn,w_,Vn,Ss,L_,Fd,y4="Register a new video processor for this class.",Jh,Bs,Dh,ve,$s,k_,yd,w4=`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained">AutoProcessor.from_pretrained()</a> class method.`,x_,wd,L4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",P_,B,Is,G_,Ld,k4="Instantiate one of the processor classes of the library from a pretrained model vocabulary.",V_,kd,x4=`The processor class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible):`,A_,xd,P4='<li><strong>aimv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (AIMv2 model)</li> <li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignProcessor">AlignProcessor</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> (AltCLIP model)</li> <li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaProcessor">AriaProcessor</a> (Aria model)</li> <li><strong>aya_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionProcessor">AyaVisionProcessor</a> (AyaVision model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> (Bark model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipProcessor">BlipProcessor</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> (BridgeTower model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonProcessor">ChameleonProcessor</a> (Chameleon model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPProcessor">ChineseCLIPProcessor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapProcessor">ClapProcessor</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpProcessor">ClvpProcessor</a> (CLVP model)</li> <li><strong>cohere2_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionProcessor">Cohere2VisionProcessor</a> (Cohere2Vision model)</li> <li><strong>colpali</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colpali#transformers.ColPaliProcessor">ColPaliProcessor</a> (ColPali model)</li> <li><strong>colqwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colqwen2#transformers.ColQwen2Processor">ColQwen2Processor</a> (ColQwen2 model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLProcessor">DeepseekVLProcessor</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridProcessor">DeepseekVLHybridProcessor</a> (DeepseekVLHybrid model)</li> <li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaProcessor">DiaProcessor</a> (Dia model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Processor">Emu3Processor</a> (Emu3 model)</li> <li><strong>evolla</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaProcessor">EvollaProcessor</a> (Evolla model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> (FLAVA model)</li> <li><strong>florence2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Processor">Florence2Processor</a> (Florence2 model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuProcessor">FuyuProcessor</a> (Fuyu model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Processor">Gemma3Processor</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nProcessor">Gemma3nProcessor</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> (GIT model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vProcessor">Glm4vProcessor</a> (GLM4V model)</li> <li><strong>glm4v_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vProcessor">Glm4vProcessor</a> (GLM4VMOE model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Processor">GotOcr2Processor</a> (GOT-OCR2 model)</li> <li><strong>granite_speech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechProcessor">GraniteSpeechProcessor</a> (GraniteSpeech model)</li> <li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor">GroundingDinoProcessor</a> (Grounding DINO model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Hubert model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsProcessor">IdeficsProcessor</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Processor">Idefics2Processor</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Processor">Idefics3Processor</a> (Idefics3 model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipProcessor">InstructBlipProcessor</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a> (InstructBlipVideo model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLProcessor">InternVLProcessor</a> (InternVL model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> (Janus model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Processor">Kosmos2Processor</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Processor">Kosmos2_5Processor</a> (KOSMOS-2.5 model)</li> <li><strong>kyutai_speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextProcessor">KyutaiSpeechToTextProcessor</a> (KyutaiSpeechToText model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor">LayoutLMv2Processor</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor">LayoutLMv3Processor</a> (LayoutLMv3 model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4Processor">Llama4Processor</a> (Llama4 model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaProcessor">LlavaProcessor</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextProcessor">LlavaNextProcessor</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoProcessor">LlavaNextVideoProcessor</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionProcessor">LlavaOnevisionProcessor</a> (LLaVA-Onevision model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMProcessor">MarkupLMProcessor</a> (MarkupLM model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTProcessor">MCTCTProcessor</a> (M-CTC-T model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (MetaCLIP 2 model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrProcessor">MgpstrProcessor</a> (MGP-STR model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralProcessor">PixtralProcessor</a> (Mistral3 model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaProcessor">MllamaProcessor</a> (Mllama model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor">GroundingDinoProcessor</a> (MM Grounding DINO model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Moonshine model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerProcessor">OneFormerProcessor</a> (OneFormer model)</li> <li><strong>ovis2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Processor">Ovis2Processor</a> (Ovis2 model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Processor">Owlv2Processor</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTProcessor">OwlViTProcessor</a> (OWL-ViT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaProcessor">PaliGemmaProcessor</a> (PaliGemma model)</li> <li><strong>perception_lm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMProcessor">PerceptionLMProcessor</a> (PerceptionLM model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalProcessor">Phi4MultimodalProcessor</a> (Phi4Multimodal model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructProcessor">Pix2StructProcessor</a> (Pix2Struct model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralProcessor">PixtralProcessor</a> (Pixtral model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.models.pop2piano.processing_pop2piano._LazyModule.__getattr__.%3Clocals%3E.Placeholder">Pop2PianoProcessor</a> (Pop2Piano model)</li> <li><strong>qwen2_5_omni</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_omni#transformers.Qwen2_5OmniProcessor">Qwen2_5OmniProcessor</a> (Qwen2_5Omni model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLProcessor">Qwen2_5_VLProcessor</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioProcessor">Qwen2AudioProcessor</a> (Qwen2Audio model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLProcessor">Qwen2VLProcessor</a> (Qwen2VL model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamProcessor">SamProcessor</a> (SAM model)</li> <li><strong>sam2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Processor">Sam2Processor</a> (SAM2 model)</li> <li><strong>sam_hq</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQProcessor">SamHQProcessor</a> (SAM-HQ model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor">SeamlessM4TProcessor</a> (SeamlessM4T model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (SEW-D model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2Processor">ShieldGemma2Processor</a> (Shieldgemma2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipProcessor">SiglipProcessor</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Processor">Siglip2Processor</a> (SigLIP2 model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMProcessor">SmolVLMProcessor</a> (SmolVLM model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor">Speech2TextProcessor</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor">Speech2Text2Processor</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Processor">SpeechT5Processor</a> (SpeechT5 model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRProcessor">TrOCRProcessor</a> (TrOCR model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltProcessor">TvltProcessor</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> (TVP model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> (UDOP model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (UniSpeechSat model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaProcessor">VideoLlavaProcessor</a> (VideoLlava model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltProcessor">ViltProcessor</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaProcessor">LlavaProcessor</a> (VipLlava model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor">VisionTextDualEncoderProcessor</a> (VisionTextDualEncoder model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralProcessor">VoxtralProcessor</a> (Voxtral model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperProcessor">WhisperProcessor</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xclip#transformers.XCLIPProcessor">XCLIPProcessor</a> (X-CLIP model)</li>',S_,An,B_,Sn,$_,Bn,js,I_,Pd,G4="Register a new processor for this class.",Uh,Es,qh,Zs,V4="The following auto classes are available for instantiating a base model class without a specific head.",zh,Rs,Xh,Me,Ws,j_,Gd,A4=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,E_,Vd,S4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Z_,zo,Ns,R_,Ad,B4="Instantiates one of the base model classes of the library from a configuration.",W_,Sd,$4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,N_,$n,J_,$,Js,D_,Bd,I4="Instantiate one of the base model classes of the library from a pretrained model.",U_,$d,j4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,q_,Id,E4='<li><strong>aimv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2Model">Aimv2Model</a> (AIMv2 model)</li> <li><strong>aimv2_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2VisionModel">Aimv2VisionModel</a> (Aimv2VisionModel model)</li> <li><strong>albert</strong>  <code>AlbertModel</code> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>apertus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusModel">ApertusModel</a> (Apertus model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeModel">ArceeModel</a> (Arcee model)</li> <li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaModel">AriaModel</a> (Aria model)</li> <li><strong>aria_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextModel">AriaTextModel</a> (AriaText model)</li> <li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li> <li><strong>aya_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionModel">AyaVisionModel</a> (AyaVision model)</li> <li><strong>bamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaModel">BambaModel</a> (Bamba model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li> <li><strong>bitnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetModel">BitNetModel</a> (BitNet model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li> <li><strong>blip_2_qformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a> (BLIP-2 QFormer model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonModel">ChameleonModel</a> (Chameleon model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>chinese_clip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionModel">ChineseCLIPVisionModel</a> (ChineseCLIPVisionModel model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clip_text_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a> (CLIPTextModel model)</li> <li><strong>clip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaModel">LlamaModel</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li> <li><strong>cohere</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereModel">CohereModel</a> (Cohere model)</li> <li><strong>cohere2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2Model">Cohere2Model</a> (Cohere2 model)</li> <li><strong>cohere2_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionModel">Cohere2VisionModel</a> (Cohere2Vision model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li> <li><strong>csm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/csm#transformers.CsmForConditionalGeneration">CsmForConditionalGeneration</a> (CSM model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li> <li><strong>d_fine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineModel">DFineModel</a> (D-FINE model)</li> <li><strong>dab-detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrModel">DabDetrModel</a> (DAB-DETR model)</li> <li><strong>dac</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a> (DAC model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li> <li><strong>dbrx</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxModel">DbrxModel</a> (DBRX model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li> <li><strong>deepseek_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Model">DeepseekV2Model</a> (DeepSeek-V2 model)</li> <li><strong>deepseek_v3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Model">DeepseekV3Model</a> (DeepSeek-V3 model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLModel">DeepseekVLModel</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridModel">DeepseekVLHybridModel</a> (DeepseekVLHybrid model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li> <li><strong>depth_pro</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProModel">DepthProModel</a> (DepthPro model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li> <li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaModel">DiaModel</a> (Dia model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaModel">DiffLlamaModel</a> (DiffLlama model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> (DINOv2 model)</li> <li><strong>dinov2_with_registers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersModel">Dinov2WithRegistersModel</a> (DINOv2 with Registers model)</li> <li><strong>dinov3_convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ConvNextModel">DINOv3ConvNextModel</a> (DINOv3 ConvNext model)</li> <li><strong>dinov3_vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ViTModel">DINOv3ViTModel</a> (DINOv3 ViT model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li> <li><strong>doge</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeModel">DogeModel</a> (Doge model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinModel">DonutSwinModel</a> (DonutSwin model)</li> <li><strong>dots1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1Model">Dots1Model</a> (dots1 model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTModel">DPTModel</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerModel">EfficientFormerModel</a> (EfficientFormer model)</li> <li><strong>efficientloftr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel">EfficientLoFTRModel</a> (EfficientLoFTR model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a> (EfficientNet model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Model">Emu3Model</a> (Emu3 model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a> (EnCodec model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieModel">ErnieModel</a> (ERNIE model)</li> <li><strong>ernie4_5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5Model">Ernie4_5Model</a> (Ernie4_5 model)</li> <li><strong>ernie4_5_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeModel">Ernie4_5_MoeModel</a> (Ernie4_5_MoE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMModel">ErnieMModel</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmModel">EsmModel</a> (ESM model)</li> <li><strong>evolla</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaModel">EvollaModel</a> (Evolla model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Model">Exaone4Model</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconModel">FalconModel</a> (Falcon model)</li> <li><strong>falcon_h1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1Model">FalconH1Model</a> (FalconH1 model)</li> <li><strong>falcon_mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaModel">FalconMambaModel</a> (FalconMamba model)</li> <li><strong>fastspeech2_conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel">FastSpeech2ConformerModel</a> (FastSpeech2Conformer model)</li> <li><strong>fastspeech2_conformer_with_hifigan</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGan">FastSpeech2ConformerWithHifiGan</a> (FastSpeech2ConformerWithHifiGan model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> (FLAVA model)</li> <li><strong>florence2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Model">Florence2Model</a> (Florence2 model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a> (FocalNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuModel">FuyuModel</a> (Fuyu model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaModel">GemmaModel</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Model">Gemma2Model</a> (Gemma2 model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Model">Gemma3Model</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3TextModel">Gemma3TextModel</a> (Gemma3ForCausalLM model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nModel">Gemma3nModel</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>gemma3n_audio</strong>  <code>Gemma3nAudioEncoder</code> (Gemma3nAudioEncoder model)</li> <li><strong>gemma3n_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nTextModel">Gemma3nTextModel</a> (Gemma3nForCausalLM model)</li> <li><strong>gemma3n_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperModel">TimmWrapperModel</a> (TimmWrapperModel model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a> (GIT model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmModel">GlmModel</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Model">Glm4Model</a> (GLM4 model)</li> <li><strong>glm4_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeModel">Glm4MoeModel</a> (Glm4MoE model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vModel">Glm4vModel</a> (GLM4V model)</li> <li><strong>glm4v_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeModel">Glm4vMoeModel</a> (GLM4VMOE model)</li> <li><strong>glm4v_moe_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextModel">Glm4vMoeTextModel</a> (GLM4VMOE model)</li> <li><strong>glm4v_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vTextModel">Glm4vTextModel</a> (GLM4V model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNModel">GLPNModel</a> (GLPN model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Model">GotOcr2Model</a> (GOT-OCR2 model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCodeModel</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXModel">GPTNeoXModel</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> (GPT NeoX Japanese model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssModel">GptOssModel</a> (GptOss model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>granite</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteModel">GraniteModel</a> (Granite model)</li> <li><strong>granitemoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeModel">GraniteMoeModel</a> (GraniteMoeMoe model)</li> <li><strong>granitemoehybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridModel">GraniteMoeHybridModel</a> (GraniteMoeHybrid model)</li> <li><strong>granitemoeshared</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedModel">GraniteMoeSharedModel</a> (GraniteMoeSharedMoe model)</li> <li><strong>graphormer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/graphormer#transformers.GraphormerModel">GraphormerModel</a> (Graphormer model)</li> <li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoModel">GroundingDinoModel</a> (Grounding DINO model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/groupvit#transformers.GroupViTModel">GroupViTModel</a> (GroupViT model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumModel">HeliumModel</a> (Helium model)</li> <li><strong>hgnet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone">HGNetV2Backbone</a> (HGNet-V2 model)</li> <li><strong>hiera</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraModel">HieraModel</a> (Hiera model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li> <li><strong>hunyuan_v1_dense</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Model">HunYuanDenseV1Model</a> (HunYuanDenseV1 model)</li> <li><strong>hunyuan_v1_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Model">HunYuanMoEV1Model</a> (HunYuanMoeV1 model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsModel">IdeficsModel</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Model">Idefics2Model</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Model">Idefics3Model</a> (Idefics3 model)</li> <li><strong>idefics3_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3VisionTransformer">Idefics3VisionTransformer</a> (Idefics3VisionTransformer model)</li> <li><strong>ijepa</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaModel">IJepaModel</a> (I-JEPA model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li> <li><strong>informer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/informer#transformers.InformerModel">InformerModel</a> (Informer model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipModel">InstructBlipModel</a> (InstructBLIP model)</li> <li><strong>instructblipvideo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel">InstructBlipVideoModel</a> (InstructBlipVideo model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLModel">InternVLModel</a> (InternVL model)</li> <li><strong>internvl_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLVisionModel">InternVLVisionModel</a> (InternVLVision model)</li> <li><strong>jamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaModel">JambaModel</a> (Jamba model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusModel">JanusModel</a> (Janus model)</li> <li><strong>jetmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeModel">JetMoeModel</a> (JetMoe model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jukebox#transformers.JukeboxModel">JukeboxModel</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Model">Kosmos2Model</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model">Kosmos2_5Model</a> (KOSMOS-2.5 model)</li> <li><strong>kyutai_speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextModel">KyutaiSpeechToTextModel</a> (KyutaiSpeechToText model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model">LayoutLMv3Model</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitModel">LevitModel</a> (LeViT model)</li> <li><strong>lfm2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2Model">Lfm2Model</a> (Lfm2 model)</li> <li><strong>lightglue</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching">LightGlueForKeypointMatching</a> (LightGlue model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltModel">LiltModel</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaModel">LlamaModel</a> (LLaMA model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration">Llama4ForConditionalGeneration</a> (Llama4 model)</li> <li><strong>llama4_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4TextModel">Llama4TextModel</a> (Llama4ForCausalLM model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaModel">LlavaModel</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next#transformers.LlavaNextModel">LlavaNextModel</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoModel">LlavaNextVideoModel</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionModel">LlavaOnevisionModel</a> (LLaVA-Onevision model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5Model">LongT5Model</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li> <li><strong>mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a> (Mamba model)</li> <li><strong>mamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a> (mamba2 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMModel">MarkupLMModel</a> (MarkupLM model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerModel">Mask2FormerModel</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li> <li><strong>maskformer-swin</strong>  <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTModel">MCTCTModel</a> (M-CTC-T model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaModel">MegaModel</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (Megatron-BERT model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a> (MetaCLIP 2 model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition">MgpstrForSceneTextRecognition</a> (MGP-STR model)</li> <li><strong>mimi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mimi#transformers.MimiModel">MimiModel</a> (Mimi model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxModel">MiniMaxModel</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralModel">MistralModel</a> (Mistral model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Model">Mistral3Model</a> (Mistral3 model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a> (Mixtral model)</li> <li><strong>mlcd</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionModel">MLCDVisionModel</a> (MLCD model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaModel">MllamaModel</a> (Mllama model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel">MMGroundingDinoModel</a> (MM Grounding DINO model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model">MobileNetV1Model</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model">MobileNetV2Model</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTModel">MobileViTModel</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> (MobileViTV2 model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertModel">ModernBertModel</a> (ModernBERT model)</li> <li><strong>modernbert-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderModel">ModernBertDecoderModel</a> (ModernBertDecoder model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineModel">MoonshineModel</a> (Moonshine model)</li> <li><strong>moshi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiModel">MoshiModel</a> (Moshi model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptModel">MptModel</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraModel">MraModel</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (MT5 model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenModel">MusicgenModel</a> (MusicGen model)</li> <li><strong>musicgen_melody</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyModel">MusicgenMelodyModel</a> (MusicGen Melody model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpModel">MvpModel</a> (MVP model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatModel">NatModel</a> (NAT model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronModel">NemotronModel</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaModel">NezhaModel</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeModel">NllbMoeModel</a> (NLLB-MOE model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystrmformer model)</li> <li><strong>olmo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoModel">OlmoModel</a> (OLMo model)</li> <li><strong>olmo2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2Model">Olmo2Model</a> (OLMo2 model)</li> <li><strong>olmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeModel">OlmoeModel</a> (OLMoE model)</li> <li><strong>omdet-turbo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection">OmDetTurboForObjectDetection</a> (OmDet-Turbo model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerModel">OneFormerModel</a> (OneFormer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTModel">OPTModel</a> (OPT model)</li> <li><strong>ovis2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Model">Ovis2Model</a> (Ovis2 model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Model">Owlv2Model</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a> (OWL-ViT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaModel">PaliGemmaModel</a> (PaliGemma model)</li> <li><strong>patchtsmixer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel">PatchTSMixerModel</a> (PatchTSMixer model)</li> <li><strong>patchtst</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/patchtst#transformers.PatchTSTModel">PatchTSTModel</a> (PatchTST model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXModel">PegasusXModel</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li> <li><strong>perception_encoder</strong>  <code>PerceptionEncoder</code> (PerceptionEncoder model)</li> <li><strong>perception_lm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMModel">PerceptionLMModel</a> (PerceptionLM model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonModel">PersimmonModel</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiModel">PhiModel</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Model">Phi3Model</a> (Phi3 model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalModel">Phi4MultimodalModel</a> (Phi4Multimodal model)</li> <li><strong>phimoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeModel">PhimoeModel</a> (Phimoe model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralVisionModel">PixtralVisionModel</a> (Pixtral model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtModel">PvtModel</a> (PVT model)</li> <li><strong>pvt_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Model">PvtV2Model</a> (PVTv2 model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Model">Qwen2Model</a> (Qwen2 model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLModel">Qwen2_5_VLModel</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_5_vl_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextModel">Qwen2_5_VLTextModel</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_audio_encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder">Qwen2AudioEncoder</a> (Qwen2AudioEncoder model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeModel">Qwen2MoeModel</a> (Qwen2MoE model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLModel">Qwen2VLModel</a> (Qwen2VL model)</li> <li><strong>qwen2_vl_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLTextModel">Qwen2VLTextModel</a> (Qwen2VL model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Model">Qwen3Model</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeModel">Qwen3MoeModel</a> (Qwen3MoE model)</li> <li><strong>recurrent_gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel">RecurrentGemmaModel</a> (RecurrentGemma model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetModel">RegNetModel</a> (RegNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (ResNet model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel">RobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertModel">RoCBertModel</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li> <li><strong>rt_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrModel">RTDetrModel</a> (RT-DETR model)</li> <li><strong>rt_detr_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model">RTDetrV2Model</a> (RT-DETRv2 model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvModel">RwkvModel</a> (RWKV model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamModel">SamModel</a> (SAM model)</li> <li><strong>sam2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Model">Sam2Model</a> (SAM2 model)</li> <li><strong>sam2_hiera_det_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2HieraDetModel">Sam2HieraDetModel</a> (Sam2HieraDetModel model)</li> <li><strong>sam2_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoModel">Sam2VideoModel</a> (Sam2VideoModel model)</li> <li><strong>sam2_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionModel">Sam2VisionModel</a> (Sam2VisionModel model)</li> <li><strong>sam_hq</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQModel">SamHQModel</a> (SAM-HQ model)</li> <li><strong>sam_hq_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQVisionModel">SamHQVisionModel</a> (SamHQVisionModel model)</li> <li><strong>sam_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionModel">SamVisionModel</a> (SamVisionModel model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel">SeamlessM4TModel</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model">SeamlessM4Tv2Model</a> (SeamlessM4Tv2 model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssModel">SeedOssModel</a> (SeedOss model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li> <li><strong>seggpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptModel">SegGptModel</a> (SegGPT model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Model">Siglip2Model</a> (SigLIP2 model)</li> <li><strong>siglip_vision_model</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipVisionModel">SiglipVisionModel</a> (SiglipVisionModel model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Model">SmolLM3Model</a> (SmolLM3 model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMModel">SmolVLMModel</a> (SmolVLM model)</li> <li><strong>smolvlm_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMVisionTransformer">SmolVLMVisionTransformer</a> (SmolVLMVisionTransformer model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Model">SpeechT5Model</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Model">Starcoder2Model</a> (Starcoder2 model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRModel">Swin2SRModel</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel">SwitchTransformersModel</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaModel">T5GemmaModel</a> (T5Gemma model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerModel">TableTransformerModel</a> (Table Transformer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li> <li><strong>textnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetModel">TextNetModel</a> (TextNet model)</li> <li><strong>time_series_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel">TimeSeriesTransformerModel</a> (Time Series Transformer model)</li> <li><strong>timesfm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModel">TimesFmModel</a> (TimesFm model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerModel">TimesformerModel</a> (TimeSformer model)</li> <li><strong>timm_backbone</strong>  <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.TimmBackbone">TimmBackbone</a> (TimmBackbone model)</li> <li><strong>timm_wrapper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperModel">TimmWrapperModel</a> (TimmWrapperModel model)</li> <li><strong>trajectory_transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel">TrajectoryTransformerModel</a> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltModel">TvltModel</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a> (TVP model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopModel">UdopModel</a> (UDOP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Model">UMT5Model</a> (UMT5 model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> (UnivNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanModel">VanModel</a> (VAN model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaModel">VideoLlavaModel</a> (VideoLlava model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEModel">VideoMAEModel</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaModel">VipLlavaModel</a> (VipLlava model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBERT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel">ViTHybridModel</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> (ViTMSN model)</li> <li><strong>vitdet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> (VitDet model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vits#transformers.VitsModel">VitsModel</a> (VITS model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitModel">VivitModel</a> (ViViT model)</li> <li><strong>vjepa2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Model">VJEPA2Model</a> (VJEPA2Model model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li> <li><strong>voxtral_encoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralEncoder">VoxtralEncoder</a> (Voxtral Encoder model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel">Wav2Vec2BertModel</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperModel">WhisperModel</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xclip#transformers.XCLIPModel">XCLIPModel</a> (X-CLIP model)</li> <li><strong>xcodec</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecModel">XcodecModel</a> (X-CODEC model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li> <li><strong>xlstm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMModel">xLSTMModel</a> (xLSTM model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodModel">XmodModel</a> (X-MOD model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosModel">YolosModel</a> (YOLOS model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li> <li><strong>zamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaModel">ZambaModel</a> (Zamba model)</li> <li><strong>zamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Model">Zamba2Model</a> (Zamba2 model)</li>',z_,jd,Z4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,X_,In,Qh,Ds,Hh,Us,R4="The following auto classes are available for instantiating a model with a pretraining head.",Yh,qs,Oh,be,zs,Q_,Ed,W4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,H_,Zd,N4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Y_,Xo,Xs,O_,Rd,J4="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",K_,Wd,D4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,ev,jn,ov,I,Qs,rv,Nd,U4="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",nv,Jd,q4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,av,Dd,z4='<li><strong>albert</strong>  <code>AlbertForPreTraining</code> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>colpali</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colpali#transformers.ColPaliForRetrieval">ColPaliForRetrieval</a> (ColPali model)</li> <li><strong>colqwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/colqwen2#transformers.ColQwen2ForRetrieval">ColQwen2ForRetrieval</a> (ColQwen2 model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForPreTraining">ErnieForPreTraining</a> (ERNIE model)</li> <li><strong>evolla</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaForProteinText2Text">EvollaForProteinText2Text</a> (Evolla model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForCausalLM">Exaone4ForCausalLM</a> (EXAONE-4.0 model)</li> <li><strong>falcon_mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM">FalconMambaForCausalLM</a> (FalconMamba model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> (FLAVA model)</li> <li><strong>florence2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration">Florence2ForConditionalGeneration</a> (Florence2 model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>hiera</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraForPreTraining">HieraForPreTraining</a> (Hiera model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration">Idefics2ForConditionalGeneration</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration">Idefics3ForConditionalGeneration</a> (Idefics3 model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusForConditionalGeneration">JanusForConditionalGeneration</a> (Janus model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextForConditionalGeneration">LlavaNextForConditionalGeneration</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration">LlavaNextVideoForConditionalGeneration</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration">LlavaOnevisionForConditionalGeneration</a> (LLaVA-Onevision model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li> <li><strong>mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a> (Mamba model)</li> <li><strong>mamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a> (mamba2 model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (Megatron-BERT model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration">Mistral3ForConditionalGeneration</a> (Mistral3 model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForConditionalGeneration">MllamaForConditionalGeneration</a> (Mllama model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForPreTraining">NezhaForPreTraining</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration">PaliGemmaForConditionalGeneration</a> (PaliGemma model)</li> <li><strong>qwen2_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a> (Qwen2Audio model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForPreTraining">RoCBertForPreTraining</a> (RoCBert model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterForPreTraining">SplinterForPreTraining</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration">T5GemmaForConditionalGeneration</a> (T5Gemma model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltForPreTraining">TvltForPreTraining</a> (TVLT model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li> <li><strong>video_llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaForConditionalGeneration">VideoLlavaForConditionalGeneration</a> (VideoLlava model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEForPreTraining">VideoMAEForPreTraining</a> (VideoMAE model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBERT model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a> (Wav2Vec2-Conformer model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li> <li><strong>xlstm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMForCausalLM">xLSTMForCausalLM</a> (xLSTM model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>',sv,Ud,X4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,tv,En,Kh,Hs,eu,Ys,Q4="The following auto classes are available for the following natural language processing tasks.",ou,Os,ru,Ce,Ks,iv,qd,H4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,lv,zd,Y4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",dv,Qo,et,mv,Xd,O4="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",cv,Qd,K4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,fv,Zn,gv,j,ot,hv,Hd,eF="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",uv,Yd,oF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pv,Od,rF='<li><strong>apertus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusForCausalLM">ApertusForCausalLM</a> (Apertus model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForCausalLM">ArceeForCausalLM</a> (Arcee model)</li> <li><strong>aria_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextForCausalLM">AriaTextForCausalLM</a> (AriaText model)</li> <li><strong>bamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaForCausalLM">BambaForCausalLM</a> (Bamba model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li> <li><strong>bitnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetForCausalLM">BitNetForCausalLM</a> (BitNet model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li> <li><strong>cohere</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereForCausalLM">CohereForCausalLM</a> (Cohere model)</li> <li><strong>cohere2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2ForCausalLM">Cohere2ForCausalLM</a> (Cohere2 model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li> <li><strong>dbrx</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxForCausalLM">DbrxForCausalLM</a> (DBRX model)</li> <li><strong>deepseek_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2ForCausalLM">DeepseekV2ForCausalLM</a> (DeepSeek-V2 model)</li> <li><strong>deepseek_v3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3ForCausalLM">DeepseekV3ForCausalLM</a> (DeepSeek-V3 model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForCausalLM">DiffLlamaForCausalLM</a> (DiffLlama model)</li> <li><strong>doge</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeForCausalLM">DogeForCausalLM</a> (Doge model)</li> <li><strong>dots1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1ForCausalLM">Dots1ForCausalLM</a> (dots1 model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3ForCausalLM">Emu3ForCausalLM</a> (Emu3 model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForCausalLM">ErnieForCausalLM</a> (ERNIE model)</li> <li><strong>ernie4_5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5ForCausalLM">Ernie4_5ForCausalLM</a> (Ernie4_5 model)</li> <li><strong>ernie4_5_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeForCausalLM">Ernie4_5_MoeForCausalLM</a> (Ernie4_5_MoE model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForCausalLM">Exaone4ForCausalLM</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForCausalLM">FalconForCausalLM</a> (Falcon model)</li> <li><strong>falcon_h1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1ForCausalLM">FalconH1ForCausalLM</a> (FalconH1 model)</li> <li><strong>falcon_mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM">FalconMambaForCausalLM</a> (FalconMamba model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForCausalLM">GemmaForCausalLM</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForCausalLM">Gemma2ForCausalLM</a> (Gemma2 model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForCausalLM">Gemma3ForCausalLM</a> (Gemma3ForCausalLM model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration">Gemma3nForConditionalGeneration</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>gemma3n_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForCausalLM">Gemma3nForCausalLM</a> (Gemma3nForCausalLM model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForCausalLM">GlmForCausalLM</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForCausalLM">Glm4ForCausalLM</a> (GLM4 model)</li> <li><strong>glm4_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeForCausalLM">Glm4MoeForCausalLM</a> (Glm4MoE model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration">GotOcr2ForConditionalGeneration</a> (GOT-OCR2 model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM">GPTNeoXForCausalLM</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> (GPT NeoX Japanese model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForCausalLM">GptOssForCausalLM</a> (GptOss model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li> <li><strong>granite</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteForCausalLM">GraniteForCausalLM</a> (Granite model)</li> <li><strong>granitemoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeForCausalLM">GraniteMoeForCausalLM</a> (GraniteMoeMoe model)</li> <li><strong>granitemoehybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridForCausalLM">GraniteMoeHybridForCausalLM</a> (GraniteMoeHybrid model)</li> <li><strong>granitemoeshared</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedForCausalLM">GraniteMoeSharedForCausalLM</a> (GraniteMoeSharedMoe model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForCausalLM">HeliumForCausalLM</a> (Helium model)</li> <li><strong>hunyuan_v1_dense</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForCausalLM">HunYuanDenseV1ForCausalLM</a> (HunYuanDenseV1 model)</li> <li><strong>hunyuan_v1_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForCausalLM">HunYuanMoEV1ForCausalLM</a> (HunYuanMoeV1 model)</li> <li><strong>jamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaForCausalLM">JambaForCausalLM</a> (Jamba model)</li> <li><strong>jetmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeForCausalLM">JetMoeForCausalLM</a> (JetMoe model)</li> <li><strong>lfm2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2ForCausalLM">Lfm2ForCausalLM</a> (Lfm2 model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (LLaMA model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForCausalLM">Llama4ForCausalLM</a> (Llama4 model)</li> <li><strong>llama4_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForCausalLM">Llama4ForCausalLM</a> (Llama4ForCausalLM model)</li> <li><strong>mamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a> (Mamba model)</li> <li><strong>mamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a> (mamba2 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForCausalLM">MegaForCausalLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (Megatron-BERT model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForCausalLM">MiniMaxForCausalLM</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForCausalLM">MistralForCausalLM</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a> (Mixtral model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForCausalLM">MllamaForCausalLM</a> (Mllama model)</li> <li><strong>modernbert-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForCausalLM">ModernBertDecoderForCausalLM</a> (ModernBertDecoder model)</li> <li><strong>moshi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiForCausalLM">MoshiForCausalLM</a> (Moshi model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM">MusicgenForCausalLM</a> (MusicGen model)</li> <li><strong>musicgen_melody</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyForCausalLM">MusicgenMelodyForCausalLM</a> (MusicGen Melody model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForCausalLM">MvpForCausalLM</a> (MVP model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForCausalLM">NemotronForCausalLM</a> (Nemotron model)</li> <li><strong>olmo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoForCausalLM">OlmoForCausalLM</a> (OLMo model)</li> <li><strong>olmo2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2ForCausalLM">Olmo2ForCausalLM</a> (OLMo2 model)</li> <li><strong>olmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeForCausalLM">OlmoeForCausalLM</a> (OLMoE model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForCausalLM">OPTForCausalLM</a> (OPT model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForCausalLM">PersimmonForCausalLM</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForCausalLM">PhiForCausalLM</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForCausalLM">Phi3ForCausalLM</a> (Phi3 model)</li> <li><strong>phi4_multimodal</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM">Phi4MultimodalForCausalLM</a> (Phi4Multimodal model)</li> <li><strong>phimoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeForCausalLM">PhimoeForCausalLM</a> (Phimoe model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForCausalLM">Qwen2ForCausalLM</a> (Qwen2 model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForCausalLM">Qwen2MoeForCausalLM</a> (Qwen2MoE model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForCausalLM">Qwen3ForCausalLM</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForCausalLM">Qwen3MoeForCausalLM</a> (Qwen3MoE model)</li> <li><strong>recurrent_gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM">RecurrentGemmaForCausalLM</a> (RecurrentGemma model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM">RobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForCausalLM">RoCBertForCausalLM</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForCausalLM">SeedOssForCausalLM</a> (SeedOss model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForCausalLM">SmolLM3ForCausalLM</a> (SmolLM3 model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForCausalLM">StableLmForCausalLM</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForCausalLM">Starcoder2ForCausalLM</a> (Starcoder2 model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForCausalLM">WhisperForCausalLM</a> (Whisper model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li> <li><strong>xlstm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMForCausalLM">xLSTMForCausalLM</a> (xLSTM model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForCausalLM">XmodForCausalLM</a> (X-MOD model)</li> <li><strong>zamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaForCausalLM">ZambaForCausalLM</a> (Zamba model)</li> <li><strong>zamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2ForCausalLM">Zamba2ForCausalLM</a> (Zamba2 model)</li>',_v,Kd,nF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,vv,Rn,nu,rt,au,Te,nt,Mv,em,aF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,bv,om,sF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Cv,Ho,at,Tv,rm,tF="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",Fv,nm,iF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,yv,Wn,wv,E,st,Lv,am,lF="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",kv,sm,dF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,xv,tm,mF='<li><strong>albert</strong>  <code>AlbertForMaskedLM</code> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForMaskedLM">ErnieForMaskedLM</a> (ERNIE model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForMaskedLM">EsmForMaskedLM</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMaskedLM">ModernBertForMaskedLM</a> (ModernBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForMaskedLM">NezhaForMaskedLM</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystrmformer model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM">RoCBertForMaskedLM</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li> <li><strong>wav2vec2</strong>  <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>',Pv,im,cF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Gv,Nn,su,tt,tu,it,lt,iu,dt,lu,Fe,mt,Vv,lm,fF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,Av,dm,gF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Sv,Yo,ct,Bv,mm,hF="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",$v,cm,uF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,Iv,Jn,jv,Z,ft,Ev,fm,pF="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",Zv,gm,_F=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Rv,hm,vF='<li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>granite_speech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration">GraniteSpeechForConditionalGeneration</a> (GraniteSpeech model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration">LongT5ForConditionalGeneration</a> (LongT5 model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration">PegasusXForConditionalGeneration</a> (PEGASUS-X model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li> <li><strong>qwen2_audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a> (Qwen2Audio model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText">SeamlessM4TForTextToText</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText">SeamlessM4Tv2ForTextToText</a> (SeamlessM4Tv2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration">T5GemmaForConditionalGeneration</a> (T5Gemma model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration">UMT5ForConditionalGeneration</a> (UMT5 model)</li> <li><strong>voxtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLM-ProphetNet model)</li>',Wv,um,MF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Nv,Dn,du,gt,mu,ye,ht,Jv,pm,bF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,Dv,_m,CF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Uv,Oo,ut,qv,vm,TF="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",zv,Mm,FF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,Xv,Un,Qv,R,pt,Hv,bm,yF="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",Yv,Cm,wF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ov,Tm,LF='<li><strong>albert</strong>  <code>AlbertForSequenceClassification</code> (ALBERT model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForSequenceClassification">ArceeForSequenceClassification</a> (Arcee model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (CodeLlama model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li> <li><strong>deepseek_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2ForSequenceClassification">DeepseekV2ForSequenceClassification</a> (DeepSeek-V2 model)</li> <li><strong>deepseek_v3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3ForSequenceClassification">DeepseekV3ForSequenceClassification</a> (DeepSeek-V3 model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForSequenceClassification">DiffLlamaForSequenceClassification</a> (DiffLlama model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li> <li><strong>doge</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeForSequenceClassification">DogeForSequenceClassification</a> (Doge model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForSequenceClassification">ErnieForSequenceClassification</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForSequenceClassification">ErnieMForSequenceClassification</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForSequenceClassification">EsmForSequenceClassification</a> (ESM model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForSequenceClassification">Exaone4ForSequenceClassification</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForSequenceClassification">FalconForSequenceClassification</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForSequenceClassification">GemmaForSequenceClassification</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForSequenceClassification">Gemma2ForSequenceClassification</a> (Gemma2 model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForSequenceClassification">Gemma3ForSequenceClassification</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForSequenceClassification">GlmForSequenceClassification</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForSequenceClassification">Glm4ForSequenceClassification</a> (GLM4 model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification">GPTBigCodeForSequenceClassification</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification">GPTNeoXForSequenceClassification</a> (GPT NeoX model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForSequenceClassification">GptOssForSequenceClassification</a> (GptOss model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForSequenceClassification">HeliumForSequenceClassification</a> (Helium model)</li> <li><strong>hunyuan_v1_dense</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForSequenceClassification">HunYuanDenseV1ForSequenceClassification</a> (HunYuanDenseV1 model)</li> <li><strong>hunyuan_v1_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForSequenceClassification">HunYuanMoEV1ForSequenceClassification</a> (HunYuanMoeV1 model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li> <li><strong>jamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaForSequenceClassification">JambaForSequenceClassification</a> (Jamba model)</li> <li><strong>jetmoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeForSequenceClassification">JetMoeForSequenceClassification</a> (JetMoe model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification">LayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForSequenceClassification">LiltForSequenceClassification</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForSequenceClassification">LukeForSequenceClassification</a> (LUKE model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification">MarkupLMForSequenceClassification</a> (MarkupLM model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForSequenceClassification">MegaForSequenceClassification</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (Megatron-BERT model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForSequenceClassification">MiniMaxForSequenceClassification</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForSequenceClassification">MixtralForSequenceClassification</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification">ModernBertForSequenceClassification</a> (ModernBERT model)</li> <li><strong>modernbert-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForSequenceClassification">ModernBertDecoderForSequenceClassification</a> (ModernBertDecoder model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForSequenceClassification">MptForSequenceClassification</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForSequenceClassification">MraForSequenceClassification</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForSequenceClassification">MT5ForSequenceClassification</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForSequenceClassification">MvpForSequenceClassification</a> (MVP model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForSequenceClassification">NemotronForSequenceClassification</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForSequenceClassification">NezhaForSequenceClassification</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystrmformer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForSequenceClassification">OPTForSequenceClassification</a> (OPT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification">PersimmonForSequenceClassification</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForSequenceClassification">PhiForSequenceClassification</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForSequenceClassification">Phi3ForSequenceClassification</a> (Phi3 model)</li> <li><strong>phimoe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeForSequenceClassification">PhimoeForSequenceClassification</a> (Phimoe model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification">Qwen2ForSequenceClassification</a> (Qwen2 model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForSequenceClassification">Qwen2MoeForSequenceClassification</a> (Qwen2MoE model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForSequenceClassification">Qwen3ForSequenceClassification</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForSequenceClassification">Qwen3MoeForSequenceClassification</a> (Qwen3MoE model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification">RobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification">RoCBertForSequenceClassification</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForSequenceClassification">SeedOssForSequenceClassification</a> (SeedOss model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForSequenceClassification">SmolLM3ForSequenceClassification</a> (SmolLM3 model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForSequenceClassification">Starcoder2ForSequenceClassification</a> (Starcoder2 model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForSequenceClassification">T5ForSequenceClassification</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForSequenceClassification">T5GemmaForSequenceClassification</a> (T5Gemma model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForSequenceClassification">UMT5ForSequenceClassification</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForSequenceClassification">XmodForSequenceClassification</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li> <li><strong>zamba</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaForSequenceClassification">ZambaForSequenceClassification</a> (Zamba model)</li> <li><strong>zamba2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2ForSequenceClassification">Zamba2ForSequenceClassification</a> (Zamba2 model)</li>',Kv,Fm,kF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,e2,qn,cu,_t,fu,we,vt,o2,ym,xF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,r2,wm,PF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",n2,Ko,Mt,a2,Lm,GF="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",s2,km,VF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,t2,zn,i2,W,bt,l2,xm,AF="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",d2,Pm,SF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,m2,Gm,BF='<li><strong>albert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForMultipleChoice">ErnieForMultipleChoice</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForMultipleChoice">ErnieMForMultipleChoice</a> (ErnieM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMultipleChoice">LukeForMultipleChoice</a> (LUKE model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMultipleChoice">MegaForMultipleChoice</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice">ModernBertForMultipleChoice</a> (ModernBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMultipleChoice">MraForMultipleChoice</a> (MRA model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForMultipleChoice">NezhaForMultipleChoice</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystrmformer model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice">RobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice">RoCBertForMultipleChoice</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMultipleChoice">XmodForMultipleChoice</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>',c2,Vm,$F=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,f2,Xn,gu,Ct,hu,Le,Tt,g2,Am,IF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,h2,Sm,jF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",u2,er,Ft,p2,Bm,EF="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",_2,$m,ZF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,v2,Qn,M2,N,yt,b2,Im,RF="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",C2,jm,WF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,T2,Em,NF='<li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction">ErnieForNextSentencePrediction</a> (ERNIE model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForNextSentencePrediction">NezhaForNextSentencePrediction</a> (Nezha model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>',F2,Zm,JF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,y2,Hn,uu,wt,pu,ke,Lt,w2,Rm,DF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,L2,Wm,UF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",k2,or,kt,x2,Nm,qF="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",P2,Jm,zF=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,G2,Yn,V2,J,xt,A2,Dm,XF="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",S2,Um,QF=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,B2,qm,HF='<li><strong>albert</strong>  <code>AlbertForTokenClassification</code> (ALBERT model)</li> <li><strong>apertus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusForTokenClassification">ApertusForTokenClassification</a> (Apertus model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForTokenClassification">ArceeForTokenClassification</a> (Arcee model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForTokenClassification">DiffLlamaForTokenClassification</a> (DiffLlama model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForTokenClassification">ErnieForTokenClassification</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForTokenClassification">ErnieMForTokenClassification</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForTokenClassification">EsmForTokenClassification</a> (ESM model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForTokenClassification">Exaone4ForTokenClassification</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForTokenClassification">FalconForTokenClassification</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li> <li><strong>gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForTokenClassification">GemmaForTokenClassification</a> (Gemma model)</li> <li><strong>gemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForTokenClassification">Gemma2ForTokenClassification</a> (Gemma2 model)</li> <li><strong>glm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForTokenClassification">GlmForTokenClassification</a> (GLM model)</li> <li><strong>glm4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForTokenClassification">Glm4ForTokenClassification</a> (GLM4 model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification">GPTBigCodeForTokenClassification</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification">GPTNeoForTokenClassification</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification">GPTNeoXForTokenClassification</a> (GPT NeoX model)</li> <li><strong>gpt_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForTokenClassification">GptOssForTokenClassification</a> (GptOss model)</li> <li><strong>helium</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForTokenClassification">HeliumForTokenClassification</a> (Helium model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification">LayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForTokenClassification">LiltForTokenClassification</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForTokenClassification">LlamaForTokenClassification</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForTokenClassification">LukeForTokenClassification</a> (LUKE model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification">MarkupLMForTokenClassification</a> (MarkupLM model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForTokenClassification">MegaForTokenClassification</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (Megatron-BERT model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForTokenClassification">MiniMaxForTokenClassification</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForTokenClassification">MistralForTokenClassification</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForTokenClassification">MixtralForTokenClassification</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForTokenClassification">ModernBertForTokenClassification</a> (ModernBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForTokenClassification">MptForTokenClassification</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForTokenClassification">MraForTokenClassification</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForTokenClassification">MT5ForTokenClassification</a> (MT5 model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForTokenClassification">NemotronForTokenClassification</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForTokenClassification">NezhaForTokenClassification</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystrmformer model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForTokenClassification">PersimmonForTokenClassification</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForTokenClassification">PhiForTokenClassification</a> (Phi model)</li> <li><strong>phi3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForTokenClassification">Phi3ForTokenClassification</a> (Phi3 model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForTokenClassification">Qwen2ForTokenClassification</a> (Qwen2 model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForTokenClassification">Qwen2MoeForTokenClassification</a> (Qwen2MoE model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForTokenClassification">Qwen3ForTokenClassification</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForTokenClassification">Qwen3MoeForTokenClassification</a> (Qwen3MoE model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification">RobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification">RoCBertForTokenClassification</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForTokenClassification">SeedOssForTokenClassification</a> (SeedOss model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForTokenClassification">SmolLM3ForTokenClassification</a> (SmolLM3 model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForTokenClassification">StableLmForTokenClassification</a> (StableLm model)</li> <li><strong>starcoder2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForTokenClassification">Starcoder2ForTokenClassification</a> (Starcoder2 model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForTokenClassification">T5ForTokenClassification</a> (T5 model)</li> <li><strong>t5gemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForTokenClassification">T5GemmaForTokenClassification</a> (T5Gemma model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForTokenClassification">UMT5ForTokenClassification</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForTokenClassification">XmodForTokenClassification</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>',$2,zm,YF=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,I2,On,_u,Pt,vu,xe,Gt,j2,Xm,OF=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,E2,Qm,KF="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Z2,rr,Vt,R2,Hm,ey="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",W2,Ym,oy=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,N2,Kn,J2,D,At,D2,Om,ry="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",U2,Km,ny=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,q2,ec,ay='<li><strong>albert</strong>  <code>AlbertForQuestionAnswering</code> (ALBERT model)</li> <li><strong>arcee</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForQuestionAnswering">ArceeForQuestionAnswering</a> (Arcee model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li> <li><strong>diffllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForQuestionAnswering">DiffLlamaForQuestionAnswering</a> (DiffLlama model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForQuestionAnswering">ErnieForQuestionAnswering</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForQuestionAnswering">ErnieMForQuestionAnswering</a> (ErnieM model)</li> <li><strong>exaone4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForQuestionAnswering">Exaone4ForQuestionAnswering</a> (EXAONE-4.0 model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForQuestionAnswering">FalconForQuestionAnswering</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering">GPT2ForQuestionAnswering</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering">GPTNeoForQuestionAnswering</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering">GPTNeoXForQuestionAnswering</a> (GPT NeoX model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForQuestionAnswering">LiltForQuestionAnswering</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForQuestionAnswering">LlamaForQuestionAnswering</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForQuestionAnswering">LukeForQuestionAnswering</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering">MarkupLMForQuestionAnswering</a> (MarkupLM model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForQuestionAnswering">MegaForQuestionAnswering</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (Megatron-BERT model)</li> <li><strong>minimax</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForQuestionAnswering">MiniMaxForQuestionAnswering</a> (MiniMax model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForQuestionAnswering">MistralForQuestionAnswering</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForQuestionAnswering">MixtralForQuestionAnswering</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li> <li><strong>modernbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering">ModernBertForQuestionAnswering</a> (ModernBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForQuestionAnswering">MptForQuestionAnswering</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForQuestionAnswering">MraForQuestionAnswering</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForQuestionAnswering">MT5ForQuestionAnswering</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForQuestionAnswering">MvpForQuestionAnswering</a> (MVP model)</li> <li><strong>nemotron</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForQuestionAnswering">NemotronForQuestionAnswering</a> (Nemotron model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForQuestionAnswering">NezhaForQuestionAnswering</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystrmformer model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForQuestionAnswering">OPTForQuestionAnswering</a> (OPT model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForQuestionAnswering">Qwen2ForQuestionAnswering</a> (Qwen2 model)</li> <li><strong>qwen2_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForQuestionAnswering">Qwen2MoeForQuestionAnswering</a> (Qwen2MoE model)</li> <li><strong>qwen3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForQuestionAnswering">Qwen3ForQuestionAnswering</a> (Qwen3 model)</li> <li><strong>qwen3_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForQuestionAnswering">Qwen3MoeForQuestionAnswering</a> (Qwen3MoE model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering">RobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering">RoCBertForQuestionAnswering</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li> <li><strong>seed_oss</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForQuestionAnswering">SeedOssForQuestionAnswering</a> (SeedOss model)</li> <li><strong>smollm3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForQuestionAnswering">SmolLM3ForQuestionAnswering</a> (SmolLM3 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForQuestionAnswering">T5ForQuestionAnswering</a> (T5 model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering">UMT5ForQuestionAnswering</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForQuestionAnswering">XmodForQuestionAnswering</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>',z2,oc,sy=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,X2,ea,Mu,St,bu,Bt,$t,Cu,It,Tu,jt,ty="The following auto classes are available for the following computer vision tasks.",Fu,Et,yu,Pe,Zt,Q2,rc,iy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a depth estimation head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,H2,nc,ly="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Y2,nr,Rt,O2,ac,dy="Instantiates one of the model classes of the library (with a depth estimation head) from a configuration.",K2,sc,my=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,eM,oa,oM,U,Wt,rM,tc,cy="Instantiate one of the model classes of the library (with a depth estimation head) from a pretrained model.",nM,ic,fy=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,aM,lc,gy='<li><strong>depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a> (Depth Anything model)</li> <li><strong>depth_pro</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation">DepthProForDepthEstimation</a> (DepthPro model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTForDepthEstimation">DPTForDepthEstimation</a> (DPT model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNForDepthEstimation">GLPNForDepthEstimation</a> (GLPN model)</li> <li><strong>prompt_depth_anything</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation">PromptDepthAnythingForDepthEstimation</a> (PromptDepthAnything model)</li> <li><strong>zoedepth</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthForDepthEstimation">ZoeDepthForDepthEstimation</a> (ZoeDepth model)</li>',sM,dc,hy=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,tM,ra,wu,Nt,Lu,Ge,Jt,iM,mc,uy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,lM,cc,py="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",dM,ar,Dt,mM,fc,_y="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",cM,gc,vy=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,fM,na,gM,q,Ut,hM,hc,My="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",uM,uc,by=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pM,pc,Cy='<li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPForImageClassification">CLIPForImageClassification</a> (CLIP model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> (DINOv2 model)</li> <li><strong>dinov2_with_registers</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersForImageClassification">Dinov2WithRegistersForImageClassification</a> (DINOv2 with Registers model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinForImageClassification">DonutSwinForImageClassification</a> (DonutSwin model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassification">EfficientFormerForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher">EfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a> (EfficientNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetForImageClassification">FocalNetForImageClassification</a> (FocalNet model)</li> <li><strong>hgnet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification">HGNetV2ForImageClassification</a> (HGNet-V2 model)</li> <li><strong>hiera</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraForImageClassification">HieraForImageClassification</a> (Hiera model)</li> <li><strong>ijepa</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaForImageClassification">IJepaForImageClassification</a> (I-JEPA model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitForImageClassification">LevitForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher">LevitForImageClassificationWithTeacher</a> (LeViT model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification">MetaClip2ForImageClassification</a> (MetaCLIP 2 model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification">MobileNetV1ForImageClassification</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification">MobileNetV2ForImageClassification</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification">MobileViTForImageClassification</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> (MobileViTV2 model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> (NAT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtForImageClassification">PvtForImageClassification</a> (PVT model)</li> <li><strong>pvt_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification">PvtV2ForImageClassification</a> (PVTv2 model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetForImageClassification">RegNetForImageClassification</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (ResNet model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2ForImageClassification">ShieldGemma2ForImageClassification</a> (Shieldgemma2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipForImageClassification">SiglipForImageClassification</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2ForImageClassification">Siglip2ForImageClassification</a> (SigLIP2 model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> (Swin Transformer V2 model)</li> <li><strong>textnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetForImageClassification">TextNetForImageClassification</a> (TextNet model)</li> <li><strong>timm_wrapper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification">TimmWrapperForImageClassification</a> (TimmWrapperModel model)</li> <li><strong>van</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> (VAN model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification">ViTHybridForImageClassification</a> (ViT Hybrid model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> (ViTMSN model)</li>',_M,_c,Ty=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,vM,aa,ku,qt,xu,Ve,zt,MM,vc,Fy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,bM,Mc,yy="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",CM,sr,Xt,TM,bc,wy="Instantiates one of the model classes of the library (with a video classification head) from a configuration.",FM,Cc,Ly=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,yM,sa,wM,z,Qt,LM,Tc,ky="Instantiate one of the model classes of the library (with a video classification head) from a pretrained model.",kM,Fc,xy=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,xM,yc,Py='<li><strong>timesformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerForVideoClassification">TimesformerForVideoClassification</a> (TimeSformer model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEForVideoClassification">VideoMAEForVideoClassification</a> (VideoMAE model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitForVideoClassification">VivitForVideoClassification</a> (ViViT model)</li> <li><strong>vjepa2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification">VJEPA2ForVideoClassification</a> (VJEPA2Model model)</li>',PM,wc,Gy=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,GM,ta,Pu,Ht,Gu,Yt,Ot,Vu,Kt,Au,ei,oi,Su,ri,Bu,Ae,ni,VM,Lc,Vy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,AM,kc,Ay="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",SM,tr,ai,BM,xc,Sy="Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.",$M,Pc,By=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,IM,ia,jM,X,si,EM,Gc,$y="Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.",ZM,Vc,Iy=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RM,Ac,jy='<li><strong>deit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling">FocalNetForMaskedImageModeling</a> (FocalNet model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> (Swin Transformer V2 model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>',WM,Sc,Ey=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,NM,la,$u,ti,Iu,Se,ii,JM,Bc,Zy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,DM,$c,Ry="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",UM,ir,li,qM,Ic,Wy="Instantiates one of the model classes of the library (with a object detection head) from a configuration.",zM,jc,Ny=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,XM,da,QM,Q,di,HM,Ec,Jy="Instantiate one of the model classes of the library (with a object detection head) from a pretrained model.",YM,Zc,Dy=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,OM,Rc,Uy='<li><strong>conditional_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li> <li><strong>d_fine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineForObjectDetection">DFineForObjectDetection</a> (D-FINE model)</li> <li><strong>dab-detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection">DabDetrForObjectDetection</a> (DAB-DETR model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li> <li><strong>rt_detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrForObjectDetection">RTDetrForObjectDetection</a> (RT-DETR model)</li> <li><strong>rt_detr_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection">RTDetrV2ForObjectDetection</a> (RT-DETRv2 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection">TableTransformerForObjectDetection</a> (Table Transformer model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosForObjectDetection">YolosForObjectDetection</a> (YOLOS model)</li>',KM,Wc,qy=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,eb,ma,ju,mi,Eu,Be,ci,ob,Nc,zy=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,rb,Jc,Xy="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",nb,lr,fi,ab,Dc,Qy="Instantiates one of the model classes of the library (with a image segmentation head) from a configuration.",sb,Uc,Hy=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,tb,ca,ib,H,gi,lb,qc,Yy="Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model.",db,zc,Oy=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,mb,Xc,Ky='<li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>',cb,Qc,e6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,fb,fa,Zu,hi,Ru,ui,pi,Wu,_i,Nu,$e,vi,gb,Hc,o6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,hb,Yc,r6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ub,dr,Mi,pb,Oc,n6="Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.",_b,Kc,a6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,vb,ga,Mb,Y,bi,bb,ef,s6="Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.",Cb,of,t6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Tb,rf,i6='<li><strong>beit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation">DPTForSemanticSegmentation</a> (DPT model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation">MobileNetV2ForSemanticSegmentation</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation">MobileViTForSemanticSegmentation</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> (MobileViTV2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> (UPerNet model)</li>',Fb,nf,l6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,yb,ha,Ju,Ci,Du,Ie,Ti,wb,af,d6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,Lb,sf,m6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",kb,mr,Fi,xb,tf,c6="Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration.",Pb,lf,f6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,Gb,ua,Vb,O,yi,Ab,df,g6="Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model.",Sb,mf,h6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Bb,cf,u6='<li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>',$b,ff,p6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Ib,pa,Uu,wi,qu,je,Li,jb,gf,_6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a universal image segmentation head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,Eb,hf,v6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Zb,cr,ki,Rb,uf,M6="Instantiates one of the model classes of the library (with a universal image segmentation head) from a configuration.",Wb,pf,b6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,Nb,_a,Jb,K,xi,Db,_f,C6="Instantiate one of the model classes of the library (with a universal image segmentation head) from a pretrained model.",Ub,vf,T6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,qb,Mf,F6='<li><strong>detr</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li> <li><strong>eomt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtForUniversalSegmentation">EomtForUniversalSegmentation</a> (EoMT model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation">Mask2FormerForUniversalSegmentation</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation">OneFormerForUniversalSegmentation</a> (OneFormer model)</li>',zb,bf,y6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Xb,va,zu,Pi,Xu,Ee,Gi,Qb,Cf,w6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,Hb,Tf,L6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Yb,fr,Vi,Ob,Ff,k6="Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.",Kb,yf,x6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,eC,Ma,oC,ee,Ai,rC,wf,P6="Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.",nC,Lf,G6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,aC,kf,V6='<li><strong>align</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval">Blip2ForImageTextRetrieval</a> (BLIP-2 model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>metaclip_2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a> (MetaCLIP 2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li> <li><strong>siglip2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Model">Siglip2Model</a> (SigLIP2 model)</li>',sC,xf,A6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,tC,ba,Qu,Si,Hu,Ze,Bi,iC,Pf,S6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot object detection head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,lC,Gf,B6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",dC,gr,$i,mC,Vf,$6="Instantiates one of the model classes of the library (with a zero-shot object detection head) from a configuration.",cC,Af,I6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,fC,Ca,gC,oe,Ii,hC,Sf,j6="Instantiate one of the model classes of the library (with a zero-shot object detection head) from a pretrained model.",uC,Bf,E6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pC,$f,Z6='<li><strong>grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoForObjectDetection">GroundingDinoForObjectDetection</a> (Grounding DINO model)</li> <li><strong>mm-grounding-dino</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection">MMGroundingDinoForObjectDetection</a> (MM Grounding DINO model)</li> <li><strong>omdet-turbo</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection">OmDetTurboForObjectDetection</a> (OmDet-Turbo model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection">Owlv2ForObjectDetection</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTForObjectDetection">OwlViTForObjectDetection</a> (OWL-ViT model)</li>',_C,If,R6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,vC,Ta,Yu,ji,Ou,Ei,W6="The following auto classes are available for the following audio tasks.",Ku,Zi,ep,Re,Ri,MC,jf,N6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,bC,Ef,J6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",CC,hr,Wi,TC,Zf,D6="Instantiates one of the model classes of the library (with a audio classification head) from a configuration.",FC,Rf,U6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,yC,Fa,wC,re,Ni,LC,Wf,q6="Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.",kC,Nf,z6=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,xC,Jf,X6='<li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification">Wav2Vec2BertForSequenceClassification</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForAudioClassification">WhisperForAudioClassification</a> (Whisper model)</li>',PC,Df,Q6=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,GC,ya,op,Ji,rp,We,Di,VC,Uf,H6=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,AC,qf,Y6="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",SC,ur,Ui,BC,zf,O6="Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration.",$C,Xf,K6=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,IC,wa,jC,ne,qi,EC,Qf,ew="Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model.",ZC,Hf,ow=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RC,Yf,rw='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification">Wav2Vec2BertForAudioFrameClassification</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>',WC,Of,nw=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,NC,La,np,zi,ap,Ne,Xi,JC,Kf,aw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,DC,eg,sw="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",UC,pr,Qi,qC,og,tw="Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration.",zC,rg,iw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,XC,ka,QC,ae,Hi,HC,ng,lw="Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model.",YC,ag,dw=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,OC,sg,mw='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTForCTC">MCTCTForCTC</a> (M-CTC-T model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC">Wav2Vec2BertForCTC</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>',KC,tg,cw=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,eT,xa,sp,Yi,tp,Je,Oi,oT,ig,fw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,rT,lg,gw="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",nT,_r,Ki,aT,dg,hw="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",sT,mg,uw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,tT,Pa,iT,se,el,lT,cg,pw="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",dT,fg,_w=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,mT,gg,vw='<li><strong>dia</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaForConditionalGeneration">DiaForConditionalGeneration</a> (Dia model)</li> <li><strong>granite_speech</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration">GraniteSpeechForConditionalGeneration</a> (GraniteSpeech model)</li> <li><strong>kyutai_speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration">KyutaiSpeechToTextForConditionalGeneration</a> (KyutaiSpeechToText model)</li> <li><strong>moonshine</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineForConditionalGeneration">MoonshineForConditionalGeneration</a> (Moonshine model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration">Pop2PianoForConditionalGeneration</a> (Pop2Piano model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText">SeamlessM4TForSpeechToText</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText">SeamlessM4Tv2ForSpeechToText</a> (SeamlessM4Tv2 model)</li> <li><strong>speech-encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText">SpeechT5ForSpeechToText</a> (SpeechT5 model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForConditionalGeneration">WhisperForConditionalGeneration</a> (Whisper model)</li>',cT,hg,Mw=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,fT,Ga,ip,ol,lp,De,rl,gT,ug,bw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,hT,pg,Cw="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",uT,vr,nl,pT,_g,Tw="Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration.",_T,vg,Fw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,vT,Va,MT,te,al,bT,Mg,yw="Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model.",CT,bg,ww=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,TT,Cg,Lw='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector">Wav2Vec2BertForXVector</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>',FT,Tg,kw=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,yT,Aa,dp,sl,mp,tl,il,cp,ll,fp,dl,ml,gp,cl,hp,Ue,fl,wT,Fg,xw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio tokenization through codebooks head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,LT,yg,Pw="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",kT,Mr,gl,xT,wg,Gw="Instantiates one of the model classes of the library (with a audio tokenization through codebooks head) from a configuration.",PT,Lg,Vw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,GT,Sa,VT,ie,hl,AT,kg,Aw="Instantiate one of the model classes of the library (with a audio tokenization through codebooks head) from a pretrained model.",ST,xg,Sw=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,BT,Pg,Bw='<li><strong>dac</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a> (DAC model)</li>',$T,Gg,$w=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,IT,Ba,up,ul,pp,pl,Iw="The following auto classes are available for the following multimodal tasks.",_p,_l,vp,qe,vl,jT,Vg,jw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,ET,Ag,Ew="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ZT,br,Ml,RT,Sg,Zw="Instantiates one of the model classes of the library (with a table question answering head) from a configuration.",WT,Bg,Rw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,NT,$a,JT,le,bl,DT,$g,Ww="Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.",UT,Ig,Nw=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,qT,jg,Jw='<li><strong>tapas</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>',zT,Eg,Dw=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,XT,Ia,Mp,Cl,bp,ze,Tl,QT,Zg,Uw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,HT,Rg,qw="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",YT,Cr,Fl,OT,Wg,zw="Instantiates one of the model classes of the library (with a document question answering head) from a configuration.",KT,Ng,Xw=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,e5,ja,o5,de,yl,r5,Jg,Qw="Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.",n5,Dg,Hw=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,a5,Ug,Yw='<li><strong>layoutlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering">LayoutLMForQuestionAnswering</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>',s5,qg,Ow=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,t5,Ea,Cp,wl,Tp,Xe,Ll,i5,zg,Kw=`This is a generic model class that will be instantiated as one of the model classes of the library (with a visual question answering head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,l5,Xg,eL="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",d5,Tr,kl,m5,Qg,oL="Instantiates one of the model classes of the library (with a visual question answering head) from a configuration.",c5,Hg,rL=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,f5,Za,g5,me,xl,h5,Yg,nL="Instantiate one of the model classes of the library (with a visual question answering head) from a pretrained model.",u5,Og,aL=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,p5,Kg,sL='<li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipForQuestionAnswering">BlipForQuestionAnswering</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering">ViltForQuestionAnswering</a> (ViLT model)</li>',_5,eh,tL=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,v5,Ra,Fp,Pl,yp,Gl,Vl,wp,Al,Lp,Qe,Sl,M5,oh,iL=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image-text-to-text modeling head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,b5,rh,lL="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",C5,Fr,Bl,T5,nh,dL="Instantiates one of the model classes of the library (with a image-text-to-text modeling head) from a configuration.",F5,ah,mL=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,y5,Wa,w5,ce,$l,L5,sh,cL="Instantiate one of the model classes of the library (with a image-text-to-text modeling head) from a pretrained model.",k5,th,fL=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,x5,ih,gL='<li><strong>aria</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaForConditionalGeneration">AriaForConditionalGeneration</a> (Aria model)</li> <li><strong>aya_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration">AyaVisionForConditionalGeneration</a> (AyaVision model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>chameleon</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration">ChameleonForConditionalGeneration</a> (Chameleon model)</li> <li><strong>cohere2_vision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration">Cohere2VisionForConditionalGeneration</a> (Cohere2Vision model)</li> <li><strong>deepseek_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration">DeepseekVLForConditionalGeneration</a> (DeepseekVL model)</li> <li><strong>deepseek_vl_hybrid</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration">DeepseekVLHybridForConditionalGeneration</a> (DeepseekVLHybrid model)</li> <li><strong>emu3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration">Emu3ForConditionalGeneration</a> (Emu3 model)</li> <li><strong>evolla</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaForProteinText2Text">EvollaForProteinText2Text</a> (Evolla model)</li> <li><strong>florence2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration">Florence2ForConditionalGeneration</a> (Florence2 model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li> <li><strong>gemma3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li> <li><strong>gemma3n</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration">Gemma3nForConditionalGeneration</a> (Gemma3nForConditionalGeneration model)</li> <li><strong>git</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li> <li><strong>glm4v</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration">Glm4vForConditionalGeneration</a> (GLM4V model)</li> <li><strong>glm4v_moe</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration">Glm4vMoeForConditionalGeneration</a> (GLM4VMOE model)</li> <li><strong>got_ocr2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration">GotOcr2ForConditionalGeneration</a> (GOT-OCR2 model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li> <li><strong>idefics2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration">Idefics2ForConditionalGeneration</a> (Idefics2 model)</li> <li><strong>idefics3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration">Idefics3ForConditionalGeneration</a> (Idefics3 model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration">InstructBlipForConditionalGeneration</a> (InstructBLIP model)</li> <li><strong>internvl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLForConditionalGeneration">InternVLForConditionalGeneration</a> (InternVL model)</li> <li><strong>janus</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusForConditionalGeneration">JanusForConditionalGeneration</a> (Janus model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration">Kosmos2ForConditionalGeneration</a> (KOSMOS-2 model)</li> <li><strong>kosmos-2.5</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration">Kosmos2_5ForConditionalGeneration</a> (KOSMOS-2.5 model)</li> <li><strong>llama4</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration">Llama4ForConditionalGeneration</a> (Llama4 model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li> <li><strong>llava_next</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextForConditionalGeneration">LlavaNextForConditionalGeneration</a> (LLaVA-NeXT model)</li> <li><strong>llava_next_video</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration">LlavaNextVideoForConditionalGeneration</a> (LLaVa-NeXT-Video model)</li> <li><strong>llava_onevision</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration">LlavaOnevisionForConditionalGeneration</a> (LLaVA-Onevision model)</li> <li><strong>mistral3</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration">Mistral3ForConditionalGeneration</a> (Mistral3 model)</li> <li><strong>mllama</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForConditionalGeneration">MllamaForConditionalGeneration</a> (Mllama model)</li> <li><strong>ovis2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration">Ovis2ForConditionalGeneration</a> (Ovis2 model)</li> <li><strong>paligemma</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration">PaliGemmaForConditionalGeneration</a> (PaliGemma model)</li> <li><strong>perception_lm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration">PerceptionLMForConditionalGeneration</a> (PerceptionLM model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration">Pix2StructForConditionalGeneration</a> (Pix2Struct model)</li> <li><strong>pixtral</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (Pixtral model)</li> <li><strong>qwen2_5_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration">Qwen2_5_VLForConditionalGeneration</a> (Qwen2_5_VL model)</li> <li><strong>qwen2_vl</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration">Qwen2VLForConditionalGeneration</a> (Qwen2VL model)</li> <li><strong>shieldgemma2</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Shieldgemma2 model)</li> <li><strong>smolvlm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration">SmolVLMForConditionalGeneration</a> (SmolVLM model)</li> <li><strong>udop</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a> (UDOP model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>',P5,lh,hL=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,G5,Na,kp,Il,xp,jl,Pp,He,El,V5,dh,uL=`This is a generic model class that will be instantiated as one of the model classes of the library (with a time-series prediction head) when created
with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_config">from_config()</a> class
method.`,A5,mh,pL="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",S5,yr,Zl,B5,ch,_L="Instantiates one of the model classes of the library (with a time-series prediction head) from a configuration.",$5,fh,vL=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel.from_pretrained">from_pretrained()</a> to load the model weights.`,I5,Ja,j5,fe,Rl,E5,gh,ML="Instantiate one of the model classes of the library (with a time-series prediction head) from a pretrained model.",Z5,hh,bL=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,R5,uh,CL='<li><strong>timesfm</strong>  <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModelForPrediction">TimesFmModelForPrediction</a> (TimesFm model)</li>',W5,ph,TL=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,N5,Da,Gp,Wl,Vp,_h,Ap;return g=new G({props:{title:"Auto Classes",local:"auto-classes",headingTag:"h1"}}),is=new x({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKQ==",highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)',wrap:!1}}),ms=new G({props:{title:"Extending the Auto Classes",local:"extending-the-auto-classes",headingTag:"h2"}}),fs=new x({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEFBdXRvQ29uZmlnLnJlZ2lzdGVyKCUyMm5ldy1tb2RlbCUyMiUyQyUyME5ld01vZGVsQ29uZmlnKSUwQUF1dG9Nb2RlbC5yZWdpc3RlcihOZXdNb2RlbENvbmZpZyUyQyUyME5ld01vZGVsKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,wrap:!1}}),vn=new Fh({props:{warning:!0,$$slots:{default:[AL]},$$scope:{ctx:F}}}),hs=new G({props:{title:"AutoConfig",local:"transformers.AutoConfig",headingTag:"h2"}}),us=new L({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/configuration_auto.py#L1163"}}),ps=new L({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike[str]]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/configuration_auto.py#L1186"}}),Mn=new P({props:{anchor:"transformers.AutoConfig.from_pretrained.example",$$slots:{default:[SL]},$$scope:{ctx:F}}}),_s=new L({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/configuration_auto.py#L1335"}}),vs=new G({props:{title:"AutoTokenizer",local:"transformers.AutoTokenizer",headingTag:"h2"}}),Ms=new L({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/tokenization_auto.py#L917"}}),bs=new L({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to determine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Use a <a href="https://huggingface.co/docs/tokenizers/index" rel="nofollow">fast Rust-based tokenizer</a> if it is supported for
a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer
is returned instead.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/tokenization_auto.py#L931"}}),Cn=new P({props:{anchor:"transformers.AutoTokenizer.from_pretrained.example",$$slots:{default:[BL]},$$scope:{ctx:F}}}),Cs=new L({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.fast_tokenizer_class",description:`<strong>fast_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"fast_tokenizer_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/tokenization_auto.py#L1159"}}),Ts=new G({props:{title:"AutoFeatureExtractor",local:"transformers.AutoFeatureExtractor",headingTag:"h2"}}),Fs=new L({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/feature_extraction_auto.py#L253"}}),ys=new L({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/feature_extraction_auto.py#L267"}}),Fn=new Fh({props:{$$slots:{default:[$L]},$$scope:{ctx:F}}}),yn=new P({props:{anchor:"transformers.AutoFeatureExtractor.from_pretrained.example",$$slots:{default:[IL]},$$scope:{ctx:F}}}),ws=new L({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/feature_extraction_auto.py#L407"}}),Ls=new G({props:{title:"AutoImageProcessor",local:"transformers.AutoImageProcessor",headingTag:"h2"}}),ks=new L({props:{name:"class transformers.AutoImageProcessor",anchor:"transformers.AutoImageProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/image_processing_auto.py#L351"}}),xs=new L({props:{name:"from_pretrained",anchor:"transformers.AutoImageProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoImageProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoImageProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoImageProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoImageProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoImageProcessor.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Use a fast torchvision-base image processor if it is supported for a given model.
If a fast image processor is not available for a given model, a normal numpy-based image processor
is returned instead.`,name:"use_fast"},{anchor:"transformers.AutoImageProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final image processor object. If <code>True</code>, then this
functions returns a <code>Tuple(image_processor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>image_processor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoImageProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoImageProcessor.from_pretrained.image_processor_filename",description:`<strong>image_processor_filename</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;config.json&quot;</code>) &#x2014;
The name of the file in the model directory to use for the image processor config.`,name:"image_processor_filename"},{anchor:"transformers.AutoImageProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are image processor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> image processor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/image_processing_auto.py#L365"}}),Ln=new Fh({props:{$$slots:{default:[jL]},$$scope:{ctx:F}}}),kn=new P({props:{anchor:"transformers.AutoImageProcessor.from_pretrained.example",$$slots:{default:[EL]},$$scope:{ctx:F}}}),Ps=new L({props:{name:"register",anchor:"transformers.AutoImageProcessor.register",parameters:[{name:"config_class",val:""},{name:"image_processor_class",val:" = None"},{name:"slow_image_processor_class",val:" = None"},{name:"fast_image_processor_class",val:" = None"},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoImageProcessor.register.image_processor_class",description:'<strong>image_processor_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a>) &#x2014; The image processor to register.',name:"image_processor_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/image_processing_auto.py#L627"}}),Gs=new G({props:{title:"AutoVideoProcessor",local:"transformers.AutoVideoProcessor",headingTag:"h2"}}),Vs=new L({props:{name:"class transformers.AutoVideoProcessor",anchor:"transformers.AutoVideoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/video_processing_auto.py#L202"}}),As=new L({props:{name:"from_pretrained",anchor:"transformers.AutoVideoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoVideoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained video_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a video processor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/video_processor#transformers.BaseVideoProcessor.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved video processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model video processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the video processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final video processor object. If <code>True</code>, then this
functions returns a <code>Tuple(video_processor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not video processor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>video_processor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoVideoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are video processor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> video processor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/video_processing_auto.py#L216"}}),Pn=new Fh({props:{$$slots:{default:[ZL]},$$scope:{ctx:F}}}),Gn=new P({props:{anchor:"transformers.AutoVideoProcessor.from_pretrained.example",$$slots:{default:[RL]},$$scope:{ctx:F}}}),Ss=new L({props:{name:"register",anchor:"transformers.AutoVideoProcessor.register",parameters:[{name:"config_class",val:""},{name:"video_processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoVideoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoVideoProcessor.register.video_processor_class",description:`<strong>video_processor_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/video_processor#transformers.BaseVideoProcessor">BaseVideoProcessor</a>) &#x2014;
The video processor to register.`,name:"video_processor_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/video_processing_auto.py#L371"}}),Bs=new G({props:{title:"AutoProcessor",local:"transformers.AutoProcessor",headingTag:"h2"}}),$s=new L({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/processing_auto.py#L183"}}),Is=new L({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/processing_auto.py#L197"}}),An=new Fh({props:{$$slots:{default:[WL]},$$scope:{ctx:F}}}),Sn=new P({props:{anchor:"transformers.AutoProcessor.from_pretrained.example",$$slots:{default:[NL]},$$scope:{ctx:F}}}),js=new L({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:'<strong>processor_class</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin">ProcessorMixin</a>) &#x2014; The processor to register.',name:"processor_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/processing_auto.py#L425"}}),Es=new G({props:{title:"Generic model classes",local:"generic-model-classes",headingTag:"h2"}}),Rs=new G({props:{title:"AutoModel",local:"transformers.AutoModel",headingTag:"h3"}}),Ws=new L({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1898"}}),Ns=new L({props:{name:"from_config",anchor:"transformers.AutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2Config">Aimv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2Model">Aimv2Model</a> (AIMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2VisionConfig">Aimv2VisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aimv2#transformers.Aimv2VisionModel">Aimv2VisionModel</a> (Aimv2VisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertModel</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusConfig">ApertusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusModel">ApertusModel</a> (Apertus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeModel">ArceeModel</a> (Arcee model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaConfig">AriaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaModel">AriaModel</a> (Aria model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextConfig">AriaTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextModel">AriaTextModel</a> (AriaText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionConfig">AyaVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionModel">AyaVisionModel</a> (AyaVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaConfig">BambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaModel">BambaModel</a> (Bamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetConfig">BitNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetModel">BitNetModel</a> (BitNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a> (BLIP-2 QFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextConfig">CLIPTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a> (CLIPTextModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonConfig">ChameleonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonModel">ChameleonModel</a> (Chameleon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionConfig">ChineseCLIPVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPVisionModel">ChineseCLIPVisionModel</a> (ChineseCLIPVisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapConfig">ClapConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2Config">Cohere2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2Model">Cohere2Model</a> (Cohere2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig">Cohere2VisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionModel">Cohere2VisionModel</a> (Cohere2Vision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereConfig">CohereConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereModel">CohereModel</a> (Cohere model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/csm#transformers.CsmConfig">CsmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/csm#transformers.CsmForConditionalGeneration">CsmForConditionalGeneration</a> (CSM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineConfig">DFineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineModel">DFineModel</a> (D-FINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ConvNextConfig">DINOv3ConvNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ConvNextModel">DINOv3ConvNextModel</a> (DINOv3 ConvNext model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ViTConfig">DINOv3ViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov3#transformers.DINOv3ViTModel">DINOv3ViTModel</a> (DINOv3 ViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTModel">DPTModel</a> (DPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig">DabDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrModel">DabDetrModel</a> (DAB-DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacConfig">DacConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a> (DAC model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxConfig">DbrxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxModel">DbrxModel</a> (DBRX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Config">DeepseekV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Model">DeepseekV2Model</a> (DeepSeek-V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Config">DeepseekV3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Model">DeepseekV3Model</a> (DeepSeek-V3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig">DeepseekVLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLModel">DeepseekVLModel</a> (DeepseekVL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig">DeepseekVLHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridModel">DeepseekVLHybridModel</a> (DeepseekVLHybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProConfig">DepthProConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProModel">DepthProModel</a> (DepthPro model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaConfig">DiaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaModel">DiaModel</a> (Dia model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaModel">DiffLlamaModel</a> (DiffLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> (DINOv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig">Dinov2WithRegistersConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersModel">Dinov2WithRegistersModel</a> (DINOv2 with Registers model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeConfig">DogeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeModel">DogeModel</a> (Doge model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinConfig">DonutSwinConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinModel">DonutSwinModel</a> (DonutSwin model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1Config">Dots1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1Model">Dots1Model</a> (dots1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerModel">EfficientFormerModel</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRConfig">EfficientLoFTRConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/efficientloftr#transformers.EfficientLoFTRModel">EfficientLoFTRModel</a> (EfficientLoFTR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a> (EfficientNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Config">Emu3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Model">Emu3Model</a> (Emu3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecConfig">EncodecConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a> (EnCodec model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5Config">Ernie4_5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5Model">Ernie4_5Model</a> (Ernie4_5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig">Ernie4_5_MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeModel">Ernie4_5_MoeModel</a> (Ernie4_5_MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieModel">ErnieModel</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMModel">ErnieMModel</a> (ErnieM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmModel">EsmModel</a> (ESM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaConfig">EvollaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaModel">EvollaModel</a> (Evolla model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Model">Exaone4Model</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconModel">FalconModel</a> (Falcon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1Config">FalconH1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1Model">FalconH1Model</a> (FalconH1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaConfig">FalconMambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaModel">FalconMambaModel</a> (FalconMamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig">FastSpeech2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel">FastSpeech2ConformerModel</a> (FastSpeech2Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGanConfig">FastSpeech2ConformerWithHifiGanConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerWithHifiGan">FastSpeech2ConformerWithHifiGan</a> (FastSpeech2ConformerWithHifiGan model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> (FLAVA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Config">Florence2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Model">Florence2Model</a> (Florence2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a> (FocalNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuModel">FuyuModel</a> (Fuyu model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNModel">GLPNModel</a> (GLPN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCodeModel</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXModel">GPTNeoXModel</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> (GPT NeoX Japanese model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Config">Gemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Model">Gemma2Model</a> (Gemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Model">Gemma3Model</a> (Gemma3ForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3TextConfig">Gemma3TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3TextModel">Gemma3TextModel</a> (Gemma3ForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nAudioConfig">Gemma3nAudioConfig</a> configuration class: <code>Gemma3nAudioEncoder</code> (Gemma3nAudioEncoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nConfig">Gemma3nConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nModel">Gemma3nModel</a> (Gemma3nForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nTextConfig">Gemma3nTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nTextModel">Gemma3nTextModel</a> (Gemma3nForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nVisionConfig">Gemma3nVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperModel">TimmWrapperModel</a> (TimmWrapperModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaConfig">GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaModel">GemmaModel</a> (Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitModel">GitModel</a> (GIT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Config">Glm4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Model">Glm4Model</a> (GLM4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeConfig">Glm4MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeModel">Glm4MoeModel</a> (Glm4MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vConfig">Glm4vConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vModel">Glm4vModel</a> (GLM4V model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig">Glm4vMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeModel">Glm4vMoeModel</a> (GLM4VMOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextConfig">Glm4vMoeTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeTextModel">Glm4vMoeTextModel</a> (GLM4VMOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vTextConfig">Glm4vTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vTextModel">Glm4vTextModel</a> (GLM4V model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmConfig">GlmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmModel">GlmModel</a> (GLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Config">GotOcr2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Model">GotOcr2Model</a> (GOT-OCR2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssConfig">GptOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssModel">GptOssModel</a> (GptOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteConfig">GraniteConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteModel">GraniteModel</a> (Granite model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeConfig">GraniteMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeModel">GraniteMoeModel</a> (GraniteMoeMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig">GraniteMoeHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridModel">GraniteMoeHybridModel</a> (GraniteMoeHybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig">GraniteMoeSharedConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedModel">GraniteMoeSharedModel</a> (GraniteMoeSharedMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/graphormer#transformers.GraphormerConfig">GraphormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/graphormer#transformers.GraphormerModel">GraphormerModel</a> (Graphormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoConfig">GroundingDinoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoModel">GroundingDinoModel</a> (Grounding DINO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/groupvit#transformers.GroupViTConfig">GroupViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/groupvit#transformers.GroupViTModel">GroupViTModel</a> (GroupViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config">HGNetV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Backbone">HGNetV2Backbone</a> (HGNet-V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumConfig">HeliumConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumModel">HeliumModel</a> (Helium model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraConfig">HieraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraModel">HieraModel</a> (Hiera model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config">HunYuanDenseV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Model">HunYuanDenseV1Model</a> (HunYuanDenseV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config">HunYuanMoEV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Model">HunYuanMoEV1Model</a> (HunYuanMoeV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaConfig">IJepaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaModel">IJepaModel</a> (I-JEPA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Config">Idefics2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Model">Idefics2Model</a> (Idefics2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Config">Idefics3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Model">Idefics3Model</a> (Idefics3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3VisionConfig">Idefics3VisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3VisionTransformer">Idefics3VisionTransformer</a> (Idefics3VisionTransformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsModel">IdeficsModel</a> (IDEFICS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/informer#transformers.InformerConfig">InformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/informer#transformers.InformerModel">InformerModel</a> (Informer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipConfig">InstructBlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipModel">InstructBlipModel</a> (InstructBLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel">InstructBlipVideoModel</a> (InstructBlipVideo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLConfig">InternVLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLModel">InternVLModel</a> (InternVL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLVisionConfig">InternVLVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLVisionModel">InternVLVisionModel</a> (InternVLVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaConfig">JambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaModel">JambaModel</a> (Jamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig">JanusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusModel">JanusModel</a> (Janus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeConfig">JetMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeModel">JetMoeModel</a> (JetMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jukebox#transformers.JukeboxConfig">JukeboxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jukebox#transformers.JukeboxModel">JukeboxModel</a> (Jukebox model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Model">Kosmos2Model</a> (KOSMOS-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config">Kosmos2_5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Model">Kosmos2_5Model</a> (KOSMOS-2.5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig">KyutaiSpeechToTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextModel">KyutaiSpeechToTextModel</a> (KyutaiSpeechToText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model">LayoutLMv3Model</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitModel">LevitModel</a> (LeViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2Config">Lfm2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2Model">Lfm2Model</a> (Lfm2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueConfig">LightGlueConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching">LightGlueForKeypointMatching</a> (LightGlue model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltModel">LiltModel</a> (LiLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4Config">Llama4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration">Llama4ForConditionalGeneration</a> (Llama4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4TextConfig">Llama4TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4TextModel">Llama4TextModel</a> (Llama4ForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaModel">LlamaModel</a> (LLaMA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaModel">LlavaModel</a> (LLaVa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextConfig">LlavaNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next#transformers.LlavaNextModel">LlavaNextModel</a> (LLaVA-NeXT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig">LlavaNextVideoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoModel">LlavaNextVideoModel</a> (LLaVa-NeXT-Video model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig">LlavaOnevisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionModel">LlavaOnevisionModel</a> (LLaVA-Onevision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5Model">LongT5Model</a> (LongT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTModel">MCTCTModel</a> (M-CTC-T model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionConfig">MLCDVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mlcd#transformers.MLCDVisionModel">MLCDVisionModel</a> (MLCD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig">MMGroundingDinoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel">MMGroundingDinoModel</a> (MM Grounding DINO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (MT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config">Mamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a> (mamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig">MambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a> (Mamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMModel">MarkupLMModel</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerModel">Mask2FormerModel</a> (Mask2Former model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><code>MaskFormerSwinConfig</code> configuration class: <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaModel">MegaModel</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a> (MetaCLIP 2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrConfig">MgpstrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition">MgpstrForSceneTextRecognition</a> (MGP-STR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mimi#transformers.MimiConfig">MimiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mimi#transformers.MimiModel">MimiModel</a> (Mimi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxModel">MiniMaxModel</a> (MiniMax model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Config">Mistral3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Model">Mistral3Model</a> (Mistral3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralModel">MistralModel</a> (Mistral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a> (Mixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaConfig">MllamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaModel">MllamaModel</a> (Mllama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model">MobileNetV1Model</a> (MobileNetV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model">MobileNetV2Model</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTModel">MobileViTModel</a> (MobileViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertModel">ModernBertModel</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig">ModernBertDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderModel">ModernBertDecoderModel</a> (ModernBertDecoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineConfig">MoonshineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineModel">MoonshineModel</a> (Moonshine model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiConfig">MoshiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiModel">MoshiModel</a> (Moshi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptModel">MptModel</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraModel">MraModel</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenConfig">MusicgenConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenModel">MusicgenModel</a> (MusicGen model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig">MusicgenMelodyConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyModel">MusicgenMelodyModel</a> (MusicGen Melody model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpModel">MvpModel</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig">NatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatModel">NatModel</a> (NAT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronModel">NemotronModel</a> (Nemotron model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaModel">NezhaModel</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeModel">NllbMoeModel</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTModel">OPTModel</a> (OPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2Config">Olmo2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2Model">Olmo2Model</a> (OLMo2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoConfig">OlmoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoModel">OlmoModel</a> (OLMo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeConfig">OlmoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeModel">OlmoeModel</a> (OLMoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig">OmDetTurboConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection">OmDetTurboForObjectDetection</a> (OmDet-Turbo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerModel">OneFormerModel</a> (OneFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Config">Ovis2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Model">Ovis2Model</a> (Ovis2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a> (OWL-ViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Model">Owlv2Model</a> (OWLv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaConfig">PaliGemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaModel">PaliGemmaModel</a> (PaliGemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig">PatchTSMixerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel">PatchTSMixerModel</a> (PatchTSMixer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/patchtst#transformers.PatchTSTConfig">PatchTSTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/patchtst#transformers.PatchTSTModel">PatchTSTModel</a> (PatchTST model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXModel">PegasusXModel</a> (PEGASUS-X model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMConfig">PerceptionLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMModel">PerceptionLMModel</a> (PerceptionLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonModel">PersimmonModel</a> (Persimmon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Config">Phi3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Model">Phi3Model</a> (Phi3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig">Phi4MultimodalConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalModel">Phi4MultimodalModel</a> (Phi4Multimodal model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiModel">PhiModel</a> (Phi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeConfig">PhimoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeModel">PhimoeModel</a> (Phimoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralVisionConfig">PixtralVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralVisionModel">PixtralVisionModel</a> (Pixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtModel">PvtModel</a> (PVT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config">PvtV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Model">PvtV2Model</a> (PVTv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoderConfig">Qwen2AudioEncoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder">Qwen2AudioEncoder</a> (Qwen2AudioEncoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Model">Qwen2Model</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeModel">Qwen2MoeModel</a> (Qwen2MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig">Qwen2VLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLModel">Qwen2VLModel</a> (Qwen2VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLTextConfig">Qwen2VLTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLTextModel">Qwen2VLTextModel</a> (Qwen2VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig">Qwen2_5_VLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLModel">Qwen2_5_VLModel</a> (Qwen2_5_VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextConfig">Qwen2_5_VLTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLTextModel">Qwen2_5_VLTextModel</a> (Qwen2_5_VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Model">Qwen3Model</a> (Qwen3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeModel">Qwen3MoeModel</a> (Qwen3MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrConfig">RTDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrModel">RTDetrModel</a> (RT-DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config">RTDetrV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model">RTDetrV2Model</a> (RT-DETRv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig">RecurrentGemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaModel">RecurrentGemmaModel</a> (RecurrentGemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetModel">RegNetModel</a> (RegNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (ResNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertModel">RoCBertModel</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel">RobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvModel">RwkvModel</a> (RWKV model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config">Sam2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Model">Sam2Model</a> (SAM2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2HieraDetConfig">Sam2HieraDetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2HieraDetModel">Sam2HieraDetModel</a> (Sam2HieraDetModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoConfig">Sam2VideoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoModel">Sam2VideoModel</a> (Sam2VideoModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionConfig">Sam2VisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionModel">Sam2VisionModel</a> (Sam2VisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamConfig">SamConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamModel">SamModel</a> (SAM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQConfig">SamHQConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQModel">SamHQModel</a> (SAM-HQ model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQVisionConfig">SamHQVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam_hq#transformers.SamHQVisionModel">SamHQVisionModel</a> (SamHQVisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionConfig">SamVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sam#transformers.SamVisionModel">SamVisionModel</a> (SamVisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel">SeamlessM4TModel</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model">SeamlessM4Tv2Model</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssModel">SeedOssModel</a> (SeedOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptConfig">SegGptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seggpt#transformers.SegGptModel">SegGptModel</a> (SegGPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Config">Siglip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Model">Siglip2Model</a> (SigLIP2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipVisionConfig">SiglipVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipVisionModel">SiglipVisionModel</a> (SiglipVisionModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Model">SmolLM3Model</a> (SmolLM3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMConfig">SmolVLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMModel">SmolVLMModel</a> (SmolVLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMVisionConfig">SmolVLMVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMVisionTransformer">SmolVLMVisionTransformer</a> (SmolVLMVisionTransformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Model">SpeechT5Model</a> (SpeechT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a> (StableLm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Config">Starcoder2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Model">Starcoder2Model</a> (Starcoder2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> (SwiftFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRConfig">Swin2SRConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swin2sr#transformers.Swin2SRModel">Swin2SRModel</a> (Swin2SR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel">SwitchTransformersModel</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaModel">T5GemmaModel</a> (T5Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerModel">TableTransformerModel</a> (Table Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetConfig">TextNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetModel">TextNetModel</a> (TextNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig">TimeSeriesTransformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel">TimeSeriesTransformerModel</a> (Time Series Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig">TimesFmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModel">TimesFmModel</a> (TimesFm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerModel">TimesformerModel</a> (TimeSformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.TimmBackboneConfig">TimmBackboneConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.TimmBackbone">TimmBackbone</a> (TimmBackbone model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig">TimmWrapperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperModel">TimmWrapperModel</a> (TimmWrapperModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig">TrajectoryTransformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel">TrajectoryTransformerModel</a> (Trajectory Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltModel">TvltModel</a> (TVLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpConfig">TvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a> (TVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Model">UMT5Model</a> (UMT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopConfig">UdopConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopModel">UdopModel</a> (UDOP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetConfig">UnivNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> (UnivNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config">VJEPA2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Model">VJEPA2Model</a> (VJEPA2Model model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanModel">VanModel</a> (VAN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel">ViTHybridModel</a> (ViT Hybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> (ViTMSN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaConfig">VideoLlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaModel">VideoLlavaModel</a> (VideoLlava model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEModel">VideoMAEModel</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaModel">VipLlavaModel</a> (VipLlava model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> (VitDet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vits#transformers.VitsConfig">VitsConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vits#transformers.VitsModel">VitsModel</a> (VITS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitModel">VivitModel</a> (ViViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralConfig">VoxtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralEncoderConfig">VoxtralEncoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralEncoder">VoxtralEncoder</a> (Voxtral Encoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel">Wav2Vec2BertModel</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperModel">WhisperModel</a> (Whisper model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xclip#transformers.XCLIPConfig">XCLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xclip#transformers.XCLIPModel">XCLIPModel</a> (X-CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLM-ProphetNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecConfig">XcodecConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xcodec#transformers.XcodecModel">XcodecModel</a> (X-CODEC model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodModel">XmodModel</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosModel">YolosModel</a> (YOLOS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Config">Zamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Model">Zamba2Model</a> (Zamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaConfig">ZambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaModel">ZambaModel</a> (Zamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMConfig">xLSTMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMModel">xLSTMModel</a> (xLSTM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModel.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),$n=new P({props:{anchor:"transformers.AutoModel.from_config.example",$$slots:{default:[JL]},$$scope:{ctx:F}}}),Js=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModel.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModel.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),In=new P({props:{anchor:"transformers.AutoModel.from_pretrained.example",$$slots:{default:[DL]},$$scope:{ctx:F}}}),Ds=new G({props:{title:"Generic pretraining classes",local:"generic-pretraining-classes",headingTag:"h2"}}),qs=new G({props:{title:"AutoModelForPreTraining",local:"transformers.AutoModelForPreTraining",headingTag:"h3"}}),zs=new L({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1905"}}),Xs=new L({props:{name:"from_config",anchor:"transformers.AutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertForPreTraining</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/colpali#transformers.ColPaliConfig">ColPaliConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/colpali#transformers.ColPaliForRetrieval">ColPaliForRetrieval</a> (ColPali model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/colqwen2#transformers.ColQwen2Config">ColQwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/colqwen2#transformers.ColQwen2ForRetrieval">ColQwen2ForRetrieval</a> (ColQwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForPreTraining">ErnieForPreTraining</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaConfig">EvollaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaForProteinText2Text">EvollaForProteinText2Text</a> (Evolla model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForCausalLM">Exaone4ForCausalLM</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaConfig">FalconMambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM">FalconMambaForCausalLM</a> (FalconMamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> (FLAVA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Config">Florence2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration">Florence2ForConditionalGeneration</a> (Florence2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraConfig">HieraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraForPreTraining">HieraForPreTraining</a> (Hiera model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Config">Idefics2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration">Idefics2ForConditionalGeneration</a> (Idefics2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Config">Idefics3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration">Idefics3ForConditionalGeneration</a> (Idefics3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig">JanusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusForConditionalGeneration">JanusForConditionalGeneration</a> (Janus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextConfig">LlavaNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextForConditionalGeneration">LlavaNextForConditionalGeneration</a> (LLaVA-NeXT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig">LlavaNextVideoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration">LlavaNextVideoForConditionalGeneration</a> (LLaVa-NeXT-Video model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig">LlavaOnevisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration">LlavaOnevisionForConditionalGeneration</a> (LLaVA-Onevision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config">Mamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a> (mamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig">MambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a> (Mamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Config">Mistral3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration">Mistral3ForConditionalGeneration</a> (Mistral3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaConfig">MllamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForConditionalGeneration">MllamaForConditionalGeneration</a> (Mllama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForPreTraining">NezhaForPreTraining</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaConfig">PaliGemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration">PaliGemmaForConditionalGeneration</a> (PaliGemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig">Qwen2AudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a> (Qwen2Audio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForPreTraining">RoCBertForPreTraining</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterForPreTraining">SplinterForPreTraining</a> (Splinter model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration">T5GemmaForConditionalGeneration</a> (T5Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tvlt#transformers.TvltForPreTraining">TvltForPreTraining</a> (TVLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaConfig">VideoLlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/video_llava#transformers.VideoLlavaForConditionalGeneration">VideoLlavaForConditionalGeneration</a> (VideoLlava model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEForPreTraining">VideoMAEForPreTraining</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralConfig">VoxtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMConfig">xLSTMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMForCausalLM">xLSTMForCausalLM</a> (xLSTM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForPreTraining.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),jn=new P({props:{anchor:"transformers.AutoModelForPreTraining.from_config.example",$$slots:{default:[UL]},$$scope:{ctx:F}}}),Qs=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),En=new P({props:{anchor:"transformers.AutoModelForPreTraining.from_pretrained.example",$$slots:{default:[qL]},$$scope:{ctx:F}}}),Hs=new G({props:{title:"Natural Language Processing",local:"natural-language-processing",headingTag:"h2"}}),Os=new G({props:{title:"AutoModelForCausalLM",local:"transformers.AutoModelForCausalLM",headingTag:"h3"}}),Ks=new L({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1920"}}),et=new L({props:{name:"from_config",anchor:"transformers.AutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusConfig">ApertusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusForCausalLM">ApertusForCausalLM</a> (Apertus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForCausalLM">ArceeForCausalLM</a> (Arcee model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextConfig">AriaTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaTextForCausalLM">AriaTextForCausalLM</a> (AriaText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaConfig">BambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bamba#transformers.BambaForCausalLM">BambaForCausalLM</a> (Bamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetConfig">BitNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bitnet#transformers.BitNetForCausalLM">BitNetForCausalLM</a> (BitNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2Config">Cohere2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2#transformers.Cohere2ForCausalLM">Cohere2ForCausalLM</a> (Cohere2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereConfig">CohereConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere#transformers.CohereForCausalLM">CohereForCausalLM</a> (Cohere model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxConfig">DbrxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dbrx#transformers.DbrxForCausalLM">DbrxForCausalLM</a> (DBRX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Config">DeepseekV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2ForCausalLM">DeepseekV2ForCausalLM</a> (DeepSeek-V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Config">DeepseekV3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3ForCausalLM">DeepseekV3ForCausalLM</a> (DeepSeek-V3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForCausalLM">DiffLlamaForCausalLM</a> (DiffLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeConfig">DogeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeForCausalLM">DogeForCausalLM</a> (Doge model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1Config">Dots1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dots1#transformers.Dots1ForCausalLM">Dots1ForCausalLM</a> (dots1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Config">Emu3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3ForCausalLM">Emu3ForCausalLM</a> (Emu3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5Config">Ernie4_5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5#transformers.Ernie4_5ForCausalLM">Ernie4_5ForCausalLM</a> (Ernie4_5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeConfig">Ernie4_5_MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie4_5_moe#transformers.Ernie4_5_MoeForCausalLM">Ernie4_5_MoeForCausalLM</a> (Ernie4_5_MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForCausalLM">ErnieForCausalLM</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForCausalLM">Exaone4ForCausalLM</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForCausalLM">FalconForCausalLM</a> (Falcon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1Config">FalconH1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_h1#transformers.FalconH1ForCausalLM">FalconH1ForCausalLM</a> (FalconH1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaConfig">FalconMambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon_mamba#transformers.FalconMambaForCausalLM">FalconMambaForCausalLM</a> (FalconMamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM">GPTNeoXForCausalLM</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> (GPT NeoX Japanese model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Config">Gemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForCausalLM">Gemma2ForCausalLM</a> (Gemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3TextConfig">Gemma3TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForCausalLM">Gemma3ForCausalLM</a> (Gemma3ForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nConfig">Gemma3nConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration">Gemma3nForConditionalGeneration</a> (Gemma3nForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nTextConfig">Gemma3nTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForCausalLM">Gemma3nForCausalLM</a> (Gemma3nForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaConfig">GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForCausalLM">GemmaForCausalLM</a> (Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Config">Glm4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForCausalLM">Glm4ForCausalLM</a> (GLM4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeConfig">Glm4MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4_moe#transformers.Glm4MoeForCausalLM">Glm4MoeForCausalLM</a> (Glm4MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmConfig">GlmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForCausalLM">GlmForCausalLM</a> (GLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Config">GotOcr2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration">GotOcr2ForConditionalGeneration</a> (GOT-OCR2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssConfig">GptOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForCausalLM">GptOssForCausalLM</a> (GptOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteConfig">GraniteConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granite#transformers.GraniteForCausalLM">GraniteForCausalLM</a> (Granite model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeConfig">GraniteMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoe#transformers.GraniteMoeForCausalLM">GraniteMoeForCausalLM</a> (GraniteMoeMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridConfig">GraniteMoeHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoehybrid#transformers.GraniteMoeHybridForCausalLM">GraniteMoeHybridForCausalLM</a> (GraniteMoeHybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedConfig">GraniteMoeSharedConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitemoeshared#transformers.GraniteMoeSharedForCausalLM">GraniteMoeSharedForCausalLM</a> (GraniteMoeSharedMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumConfig">HeliumConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForCausalLM">HeliumForCausalLM</a> (Helium model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config">HunYuanDenseV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForCausalLM">HunYuanDenseV1ForCausalLM</a> (HunYuanDenseV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config">HunYuanMoEV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForCausalLM">HunYuanMoEV1ForCausalLM</a> (HunYuanMoeV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaConfig">JambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaForCausalLM">JambaForCausalLM</a> (Jamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeConfig">JetMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeForCausalLM">JetMoeForCausalLM</a> (JetMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2Config">Lfm2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lfm2#transformers.Lfm2ForCausalLM">Lfm2ForCausalLM</a> (Lfm2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4Config">Llama4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForCausalLM">Llama4ForCausalLM</a> (Llama4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4TextConfig">Llama4TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForCausalLM">Llama4ForCausalLM</a> (Llama4ForCausalLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (LLaMA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config">Mamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a> (mamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig">MambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a> (Mamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForCausalLM">MegaForCausalLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForCausalLM">MiniMaxForCausalLM</a> (MiniMax model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForCausalLM">MistralForCausalLM</a> (Mistral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a> (Mixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaConfig">MllamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForCausalLM">MllamaForCausalLM</a> (Mllama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig">ModernBertDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForCausalLM">ModernBertDecoderForCausalLM</a> (ModernBertDecoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiConfig">MoshiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/moshi#transformers.MoshiForCausalLM">MoshiForCausalLM</a> (Moshi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenConfig">MusicgenConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM">MusicgenForCausalLM</a> (MusicGen model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyConfig">MusicgenMelodyConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/musicgen_melody#transformers.MusicgenMelodyForCausalLM">MusicgenMelodyForCausalLM</a> (MusicGen Melody model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForCausalLM">MvpForCausalLM</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForCausalLM">NemotronForCausalLM</a> (Nemotron model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForCausalLM">OPTForCausalLM</a> (OPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2Config">Olmo2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmo2#transformers.Olmo2ForCausalLM">Olmo2ForCausalLM</a> (OLMo2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoConfig">OlmoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmo#transformers.OlmoForCausalLM">OlmoForCausalLM</a> (OLMo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeConfig">OlmoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/olmoe#transformers.OlmoeForCausalLM">OlmoeForCausalLM</a> (OLMoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForCausalLM">PersimmonForCausalLM</a> (Persimmon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Config">Phi3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForCausalLM">Phi3ForCausalLM</a> (Phi3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalConfig">Phi4MultimodalConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi4_multimodal#transformers.Phi4MultimodalForCausalLM">Phi4MultimodalForCausalLM</a> (Phi4Multimodal model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForCausalLM">PhiForCausalLM</a> (Phi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeConfig">PhimoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeForCausalLM">PhimoeForCausalLM</a> (Phimoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForCausalLM">Qwen2ForCausalLM</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForCausalLM">Qwen2MoeForCausalLM</a> (Qwen2MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForCausalLM">Qwen3ForCausalLM</a> (Qwen3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForCausalLM">Qwen3MoeForCausalLM</a> (Qwen3MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaConfig">RecurrentGemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/recurrent_gemma#transformers.RecurrentGemmaForCausalLM">RecurrentGemmaForCausalLM</a> (RecurrentGemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForCausalLM">RoCBertForCausalLM</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM">RobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForCausalLM">SeedOssForCausalLM</a> (SeedOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForCausalLM">SmolLM3ForCausalLM</a> (SmolLM3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForCausalLM">StableLmForCausalLM</a> (StableLm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Config">Starcoder2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForCausalLM">Starcoder2ForCausalLM</a> (Starcoder2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForCausalLM">WhisperForCausalLM</a> (Whisper model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLM-ProphetNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForCausalLM">XmodForCausalLM</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Config">Zamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2ForCausalLM">Zamba2ForCausalLM</a> (Zamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaConfig">ZambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaForCausalLM">ZambaForCausalLM</a> (Zamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMConfig">xLSTMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlstm#transformers.xLSTMForCausalLM">xLSTMForCausalLM</a> (xLSTM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCausalLM.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Zn=new P({props:{anchor:"transformers.AutoModelForCausalLM.from_config.example",$$slots:{default:[zL]},$$scope:{ctx:F}}}),ot=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Rn=new P({props:{anchor:"transformers.AutoModelForCausalLM.from_pretrained.example",$$slots:{default:[XL]},$$scope:{ctx:F}}}),rt=new G({props:{title:"AutoModelForMaskedLM",local:"transformers.AutoModelForMaskedLM",headingTag:"h3"}}),nt=new L({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1937"}}),at=new L({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertForMaskedLM</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForMaskedLM">ErnieForMaskedLM</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForMaskedLM">EsmForMaskedLM</a> (ESM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMaskedLM">ModernBertForMaskedLM</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForMaskedLM">NezhaForMaskedLM</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM">RoCBertForMaskedLM</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedLM.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Wn=new P({props:{anchor:"transformers.AutoModelForMaskedLM.from_config.example",$$slots:{default:[QL]},$$scope:{ctx:F}}}),st=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Nn=new P({props:{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[HL]},$$scope:{ctx:F}}}),tt=new G({props:{title:"AutoModelForMaskGeneration",local:"transformers.AutoModelForMaskGeneration",headingTag:"h3"}}),lt=new L({props:{name:"class transformers.AutoModelForMaskGeneration",anchor:"transformers.AutoModelForMaskGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1878"}}),dt=new G({props:{title:"AutoModelForSeq2SeqLM",local:"transformers.AutoModelForSeq2SeqLM",headingTag:"h3"}}),mt=new L({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1944"}}),ct=new L({props:{name:"from_config",anchor:"transformers.AutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechConfig">GraniteSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration">GraniteSpeechForConditionalGeneration</a> (GraniteSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration">LongT5ForConditionalGeneration</a> (LongT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (MT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration">PegasusXForConditionalGeneration</a> (PEGASUS-X model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioConfig">Qwen2AudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_audio#transformers.Qwen2AudioForConditionalGeneration">Qwen2AudioForConditionalGeneration</a> (Qwen2Audio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText">SeamlessM4TForTextToText</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText">SeamlessM4Tv2ForTextToText</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForConditionalGeneration">T5GemmaForConditionalGeneration</a> (T5Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration">UMT5ForConditionalGeneration</a> (UMT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralConfig">VoxtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/voxtral#transformers.VoxtralForConditionalGeneration">VoxtralForConditionalGeneration</a> (Voxtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLM-ProphetNet model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Jn=new P({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[YL]},$$scope:{ctx:F}}}),ft=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Dn=new P({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[OL]},$$scope:{ctx:F}}}),gt=new G({props:{title:"AutoModelForSequenceClassification",local:"transformers.AutoModelForSequenceClassification",headingTag:"h3"}}),ht=new L({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1955"}}),ut=new L({props:{name:"from_config",anchor:"transformers.AutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertForSequenceClassification</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForSequenceClassification">ArceeForSequenceClassification</a> (Arcee model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2Config">DeepseekV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v2#transformers.DeepseekV2ForSequenceClassification">DeepseekV2ForSequenceClassification</a> (DeepSeek-V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3Config">DeepseekV3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_v3#transformers.DeepseekV3ForSequenceClassification">DeepseekV3ForSequenceClassification</a> (DeepSeek-V3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForSequenceClassification">DiffLlamaForSequenceClassification</a> (DiffLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeConfig">DogeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/doge#transformers.DogeForSequenceClassification">DogeForSequenceClassification</a> (Doge model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForSequenceClassification">ErnieForSequenceClassification</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForSequenceClassification">ErnieMForSequenceClassification</a> (ErnieM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForSequenceClassification">EsmForSequenceClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForSequenceClassification">Exaone4ForSequenceClassification</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForSequenceClassification">FalconForSequenceClassification</a> (Falcon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification">GPTBigCodeForSequenceClassification</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification">GPTNeoXForSequenceClassification</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Config">Gemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForSequenceClassification">Gemma2ForSequenceClassification</a> (Gemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForSequenceClassification">Gemma3ForSequenceClassification</a> (Gemma3ForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaConfig">GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForSequenceClassification">GemmaForSequenceClassification</a> (Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Config">Glm4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForSequenceClassification">Glm4ForSequenceClassification</a> (GLM4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmConfig">GlmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForSequenceClassification">GlmForSequenceClassification</a> (GLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssConfig">GptOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForSequenceClassification">GptOssForSequenceClassification</a> (GptOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumConfig">HeliumConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForSequenceClassification">HeliumForSequenceClassification</a> (Helium model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1Config">HunYuanDenseV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_dense#transformers.HunYuanDenseV1ForSequenceClassification">HunYuanDenseV1ForSequenceClassification</a> (HunYuanDenseV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1Config">HunYuanMoEV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hunyuan_v1_moe#transformers.HunYuanMoEV1ForSequenceClassification">HunYuanMoEV1ForSequenceClassification</a> (HunYuanMoeV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaConfig">JambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jamba#transformers.JambaForSequenceClassification">JambaForSequenceClassification</a> (Jamba model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeConfig">JetMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/jetmoe#transformers.JetMoeForSequenceClassification">JetMoeForSequenceClassification</a> (JetMoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification">LayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForSequenceClassification">LiltForSequenceClassification</a> (LiLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (LLaMA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForSequenceClassification">LukeForSequenceClassification</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForSequenceClassification">MT5ForSequenceClassification</a> (MT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification">MarkupLMForSequenceClassification</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForSequenceClassification">MegaForSequenceClassification</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForSequenceClassification">MiniMaxForSequenceClassification</a> (MiniMax model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> (Mistral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForSequenceClassification">MixtralForSequenceClassification</a> (Mixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForSequenceClassification">ModernBertForSequenceClassification</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderConfig">ModernBertDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert-decoder#transformers.ModernBertDecoderForSequenceClassification">ModernBertDecoderForSequenceClassification</a> (ModernBertDecoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForSequenceClassification">MptForSequenceClassification</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForSequenceClassification">MraForSequenceClassification</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForSequenceClassification">MvpForSequenceClassification</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForSequenceClassification">NemotronForSequenceClassification</a> (Nemotron model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForSequenceClassification">NezhaForSequenceClassification</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForSequenceClassification">OPTForSequenceClassification</a> (OPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification">PersimmonForSequenceClassification</a> (Persimmon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Config">Phi3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForSequenceClassification">Phi3ForSequenceClassification</a> (Phi3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForSequenceClassification">PhiForSequenceClassification</a> (Phi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeConfig">PhimoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phimoe#transformers.PhimoeForSequenceClassification">PhimoeForSequenceClassification</a> (Phimoe model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification">Qwen2ForSequenceClassification</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForSequenceClassification">Qwen2MoeForSequenceClassification</a> (Qwen2MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForSequenceClassification">Qwen3ForSequenceClassification</a> (Qwen3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForSequenceClassification">Qwen3MoeForSequenceClassification</a> (Qwen3MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification">RoCBertForSequenceClassification</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification">RobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForSequenceClassification">SeedOssForSequenceClassification</a> (SeedOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForSequenceClassification">SmolLM3ForSequenceClassification</a> (SmolLM3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> (StableLm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Config">Starcoder2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForSequenceClassification">Starcoder2ForSequenceClassification</a> (Starcoder2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForSequenceClassification">T5ForSequenceClassification</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForSequenceClassification">T5GemmaForSequenceClassification</a> (T5Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForSequenceClassification">UMT5ForSequenceClassification</a> (UMT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForSequenceClassification">XmodForSequenceClassification</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2Config">Zamba2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba2#transformers.Zamba2ForSequenceClassification">Zamba2ForSequenceClassification</a> (Zamba2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaConfig">ZambaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zamba#transformers.ZambaForSequenceClassification">ZambaForSequenceClassification</a> (Zamba model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSequenceClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Un=new P({props:{anchor:"transformers.AutoModelForSequenceClassification.from_config.example",$$slots:{default:[KL]},$$scope:{ctx:F}}}),pt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),qn=new P({props:{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[ek]},$$scope:{ctx:F}}}),_t=new G({props:{title:"AutoModelForMultipleChoice",local:"transformers.AutoModelForMultipleChoice",headingTag:"h3"}}),vt=new L({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2011"}}),Mt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForMultipleChoice">ErnieForMultipleChoice</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForMultipleChoice">ErnieMForMultipleChoice</a> (ErnieM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForMultipleChoice">LukeForMultipleChoice</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForMultipleChoice">MegaForMultipleChoice</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForMultipleChoice">ModernBertForMultipleChoice</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForMultipleChoice">MraForMultipleChoice</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForMultipleChoice">NezhaForMultipleChoice</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice">RoCBertForMultipleChoice</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice">RobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForMultipleChoice">XmodForMultipleChoice</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMultipleChoice.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),zn=new P({props:{anchor:"transformers.AutoModelForMultipleChoice.from_config.example",$$slots:{default:[ok]},$$scope:{ctx:F}}}),bt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Xn=new P({props:{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[rk]},$$scope:{ctx:F}}}),Ct=new G({props:{title:"AutoModelForNextSentencePrediction",local:"transformers.AutoModelForNextSentencePrediction",headingTag:"h3"}}),Tt=new L({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2018"}}),Ft=new L({props:{name:"from_config",anchor:"transformers.AutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction">ErnieForNextSentencePrediction</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForNextSentencePrediction">NezhaForNextSentencePrediction</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Qn=new P({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[nk]},$$scope:{ctx:F}}}),yt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Hn=new P({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[ak]},$$scope:{ctx:F}}}),wt=new G({props:{title:"AutoModelForTokenClassification",local:"transformers.AutoModelForTokenClassification",headingTag:"h3"}}),Lt=new L({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2004"}}),kt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertForTokenClassification</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusConfig">ApertusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/apertus#transformers.ApertusForTokenClassification">ApertusForTokenClassification</a> (Apertus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForTokenClassification">ArceeForTokenClassification</a> (Arcee model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForTokenClassification">DiffLlamaForTokenClassification</a> (DiffLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForTokenClassification">ErnieForTokenClassification</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForTokenClassification">ErnieMForTokenClassification</a> (ErnieM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/esm#transformers.EsmForTokenClassification">EsmForTokenClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForTokenClassification">Exaone4ForTokenClassification</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForTokenClassification">FalconForTokenClassification</a> (Falcon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification">GPTBigCodeForTokenClassification</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification">GPTNeoForTokenClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification">GPTNeoXForTokenClassification</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2Config">Gemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma2#transformers.Gemma2ForTokenClassification">Gemma2ForTokenClassification</a> (Gemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaConfig">GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma#transformers.GemmaForTokenClassification">GemmaForTokenClassification</a> (Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4Config">Glm4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4#transformers.Glm4ForTokenClassification">Glm4ForTokenClassification</a> (GLM4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmConfig">GlmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm#transformers.GlmForTokenClassification">GlmForTokenClassification</a> (GLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssConfig">GptOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_oss#transformers.GptOssForTokenClassification">GptOssForTokenClassification</a> (GptOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumConfig">HeliumConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/helium#transformers.HeliumForTokenClassification">HeliumForTokenClassification</a> (Helium model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification">LayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForTokenClassification">LiltForTokenClassification</a> (LiLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForTokenClassification">LlamaForTokenClassification</a> (LLaMA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForTokenClassification">LukeForTokenClassification</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForTokenClassification">MT5ForTokenClassification</a> (MT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification">MarkupLMForTokenClassification</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForTokenClassification">MegaForTokenClassification</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForTokenClassification">MiniMaxForTokenClassification</a> (MiniMax model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForTokenClassification">MistralForTokenClassification</a> (Mistral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForTokenClassification">MixtralForTokenClassification</a> (Mixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForTokenClassification">ModernBertForTokenClassification</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForTokenClassification">MptForTokenClassification</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForTokenClassification">MraForTokenClassification</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForTokenClassification">NemotronForTokenClassification</a> (Nemotron model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForTokenClassification">NezhaForTokenClassification</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/persimmon#transformers.PersimmonForTokenClassification">PersimmonForTokenClassification</a> (Persimmon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3Config">Phi3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi3#transformers.Phi3ForTokenClassification">Phi3ForTokenClassification</a> (Phi3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/phi#transformers.PhiForTokenClassification">PhiForTokenClassification</a> (Phi model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForTokenClassification">Qwen2ForTokenClassification</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForTokenClassification">Qwen2MoeForTokenClassification</a> (Qwen2MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForTokenClassification">Qwen3ForTokenClassification</a> (Qwen3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForTokenClassification">Qwen3MoeForTokenClassification</a> (Qwen3MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification">RoCBertForTokenClassification</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification">RobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForTokenClassification">SeedOssForTokenClassification</a> (SeedOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForTokenClassification">SmolLM3ForTokenClassification</a> (SmolLM3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/stablelm#transformers.StableLmForTokenClassification">StableLmForTokenClassification</a> (StableLm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2Config">Starcoder2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/starcoder2#transformers.Starcoder2ForTokenClassification">Starcoder2ForTokenClassification</a> (Starcoder2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForTokenClassification">T5ForTokenClassification</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaConfig">T5GemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5gemma#transformers.T5GemmaForTokenClassification">T5GemmaForTokenClassification</a> (T5Gemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForTokenClassification">UMT5ForTokenClassification</a> (UMT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForTokenClassification">XmodForTokenClassification</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTokenClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Yn=new P({props:{anchor:"transformers.AutoModelForTokenClassification.from_config.example",$$slots:{default:[sk]},$$scope:{ctx:F}}}),xt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),On=new P({props:{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[tk]},$$scope:{ctx:F}}}),Pt=new G({props:{title:"AutoModelForQuestionAnswering",local:"transformers.AutoModelForQuestionAnswering",headingTag:"h3"}}),Gt=new L({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1964"}}),Vt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <code>AlbertForQuestionAnswering</code> (ALBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeConfig">ArceeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/arcee#transformers.ArceeForQuestionAnswering">ArceeForQuestionAnswering</a> (Arcee model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaConfig">DiffLlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/diffllama#transformers.DiffLlamaForQuestionAnswering">DiffLlamaForQuestionAnswering</a> (DiffLlama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie#transformers.ErnieForQuestionAnswering">ErnieForQuestionAnswering</a> (ERNIE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ernie_m#transformers.ErnieMForQuestionAnswering">ErnieMForQuestionAnswering</a> (ErnieM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4Config">Exaone4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/exaone4#transformers.Exaone4ForQuestionAnswering">Exaone4ForQuestionAnswering</a> (EXAONE-4.0 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/falcon#transformers.FalconForQuestionAnswering">FalconForQuestionAnswering</a> (Falcon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering">GPT2ForQuestionAnswering</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering">GPTNeoForQuestionAnswering</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering">GPTNeoXForQuestionAnswering</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lilt#transformers.LiltForQuestionAnswering">LiltForQuestionAnswering</a> (LiLT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaForQuestionAnswering">LlamaForQuestionAnswering</a> (LLaMA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/luke#transformers.LukeForQuestionAnswering">LukeForQuestionAnswering</a> (LUKE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mt5#transformers.MT5ForQuestionAnswering">MT5ForQuestionAnswering</a> (MT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering">MarkupLMForQuestionAnswering</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mega#transformers.MegaForQuestionAnswering">MegaForQuestionAnswering</a> (MEGA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxConfig">MiniMaxConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/minimax#transformers.MiniMaxForQuestionAnswering">MiniMaxForQuestionAnswering</a> (MiniMax model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral#transformers.MistralForQuestionAnswering">MistralForQuestionAnswering</a> (Mistral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForQuestionAnswering">MixtralForQuestionAnswering</a> (Mixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertConfig">ModernBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/modernbert#transformers.ModernBertForQuestionAnswering">ModernBertForQuestionAnswering</a> (ModernBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mpt#transformers.MptForQuestionAnswering">MptForQuestionAnswering</a> (MPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mra#transformers.MraForQuestionAnswering">MraForQuestionAnswering</a> (MRA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mvp#transformers.MvpForQuestionAnswering">MvpForQuestionAnswering</a> (MVP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronConfig">NemotronConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nemotron#transformers.NemotronForQuestionAnswering">NemotronForQuestionAnswering</a> (Nemotron model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nezha#transformers.NezhaForQuestionAnswering">NezhaForQuestionAnswering</a> (Nezha model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/opt#transformers.OPTForQuestionAnswering">OPTForQuestionAnswering</a> (OPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2#transformers.Qwen2ForQuestionAnswering">Qwen2ForQuestionAnswering</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeConfig">Qwen2MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_moe#transformers.Qwen2MoeForQuestionAnswering">Qwen2MoeForQuestionAnswering</a> (Qwen2MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3Config">Qwen3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3#transformers.Qwen3ForQuestionAnswering">Qwen3ForQuestionAnswering</a> (Qwen3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeConfig">Qwen3MoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen3_moe#transformers.Qwen3MoeForQuestionAnswering">Qwen3MoeForQuestionAnswering</a> (Qwen3MoE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering">RoCBertForQuestionAnswering</a> (RoCBert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering">RobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssConfig">SeedOssConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seed_oss#transformers.SeedOssForQuestionAnswering">SeedOssForQuestionAnswering</a> (SeedOss model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3Config">SmolLM3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smollm3#transformers.SmolLM3ForQuestionAnswering">SmolLM3ForQuestionAnswering</a> (SmolLM3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForQuestionAnswering">T5ForQuestionAnswering</a> (T5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering">UMT5ForQuestionAnswering</a> (UMT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/xmod#transformers.XmodForQuestionAnswering">XmodForQuestionAnswering</a> (X-MOD model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForQuestionAnswering.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Kn=new P({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_config.example",$$slots:{default:[ik]},$$scope:{ctx:F}}}),At=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ea=new P({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[lk]},$$scope:{ctx:F}}}),St=new G({props:{title:"AutoModelForTextEncoding",local:"transformers.AutoModelForTextEncoding",headingTag:"h3"}}),$t=new L({props:{name:"class transformers.AutoModelForTextEncoding",anchor:"transformers.AutoModelForTextEncoding",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1890"}}),It=new G({props:{title:"Computer vision",local:"computer-vision",headingTag:"h2"}}),Et=new G({props:{title:"AutoModelForDepthEstimation",local:"transformers.AutoModelForDepthEstimation",headingTag:"h3"}}),Zt=new L({props:{name:"class transformers.AutoModelForDepthEstimation",anchor:"transformers.AutoModelForDepthEstimation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2102"}}),Rt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForDepthEstimation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTForDepthEstimation">DPTForDepthEstimation</a> (DPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingConfig">DepthAnythingConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a> (Depth Anything model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProConfig">DepthProConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation">DepthProForDepthEstimation</a> (DepthPro model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glpn#transformers.GLPNForDepthEstimation">GLPNForDepthEstimation</a> (GLPN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingConfig">PromptDepthAnythingConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/prompt_depth_anything#transformers.PromptDepthAnythingForDepthEstimation">PromptDepthAnythingForDepthEstimation</a> (PromptDepthAnything model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthConfig">ZoeDepthConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/zoedepth#transformers.ZoeDepthForDepthEstimation">ZoeDepthForDepthEstimation</a> (ZoeDepth model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDepthEstimation.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),oa=new P({props:{anchor:"transformers.AutoModelForDepthEstimation.from_config.example",$$slots:{default:[dk]},$$scope:{ctx:F}}}),Wt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDepthEstimation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ra=new P({props:{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.example",$$slots:{default:[mk]},$$scope:{ctx:F}}}),Nt=new G({props:{title:"AutoModelForImageClassification",local:"transformers.AutoModelForImageClassification",headingTag:"h3"}}),Jt=new L({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2027"}}),Dt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPForImageClassification">CLIPForImageClassification</a> (CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> (DINOv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersConfig">Dinov2WithRegistersConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dinov2_with_registers#transformers.Dinov2WithRegistersForImageClassification">Dinov2WithRegistersForImageClassification</a> (DINOv2 with Registers model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinConfig">DonutSwinConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/donut#transformers.DonutSwinForImageClassification">DonutSwinForImageClassification</a> (DonutSwin model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassification">EfficientFormerForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher">EfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a> (EfficientNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetForImageClassification">FocalNetForImageClassification</a> (FocalNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2Config">HGNetV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hgnet_v2#transformers.HGNetV2ForImageClassification">HGNetV2ForImageClassification</a> (HGNet-V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraConfig">HieraConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hiera#transformers.HieraForImageClassification">HieraForImageClassification</a> (Hiera model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaConfig">IJepaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ijepa#transformers.IJepaForImageClassification">IJepaForImageClassification</a> (I-JEPA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitForImageClassification">LevitForImageClassification</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher">LevitForImageClassificationWithTeacher</a> (LeViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification">MetaClip2ForImageClassification</a> (MetaCLIP 2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification">MobileNetV1ForImageClassification</a> (MobileNetV1 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification">MobileNetV2ForImageClassification</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTForImageClassification">MobileViTForImageClassification</a> (MobileViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig">NatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> (NAT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pvt#transformers.PvtForImageClassification">PvtForImageClassification</a> (PVT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2Config">PvtV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pvt_v2#transformers.PvtV2ForImageClassification">PvtV2ForImageClassification</a> (PVTv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/regnet#transformers.RegNetForImageClassification">RegNetForImageClassification</a> (RegNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (ResNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config">ShieldGemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2ForImageClassification">ShieldGemma2ForImageClassification</a> (Shieldgemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Config">Siglip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2ForImageClassification">Siglip2ForImageClassification</a> (SigLIP2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipForImageClassification">SiglipForImageClassification</a> (SigLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> (SwiftFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetConfig">TextNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/textnet#transformers.TextNetForImageClassification">TextNetForImageClassification</a> (TextNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig">TimmWrapperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification">TimmWrapperForImageClassification</a> (TimmWrapperModel model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> (VAN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification">ViTHybridForImageClassification</a> (ViT Hybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> (ViTMSN model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),na=new P({props:{anchor:"transformers.AutoModelForImageClassification.from_config.example",$$slots:{default:[ck]},$$scope:{ctx:F}}}),Ut=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),aa=new P({props:{anchor:"transformers.AutoModelForImageClassification.from_pretrained.example",$$slots:{default:[fk]},$$scope:{ctx:F}}}),qt=new G({props:{title:"AutoModelForVideoClassification",local:"transformers.AutoModelForVideoClassification",headingTag:"h3"}}),zt=new L({props:{name:"class transformers.AutoModelForVideoClassification",anchor:"transformers.AutoModelForVideoClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2109"}}),Xt=new L({props:{name:"from_config",anchor:"transformers.AutoModelForVideoClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timesformer#transformers.TimesformerForVideoClassification">TimesformerForVideoClassification</a> (TimeSformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2Config">VJEPA2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vjepa2#transformers.VJEPA2ForVideoClassification">VJEPA2ForVideoClassification</a> (VJEPA2Model model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/videomae#transformers.VideoMAEForVideoClassification">VideoMAEForVideoClassification</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vivit#transformers.VivitForVideoClassification">VivitForVideoClassification</a> (ViViT model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVideoClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),sa=new P({props:{anchor:"transformers.AutoModelForVideoClassification.from_config.example",$$slots:{default:[gk]},$$scope:{ctx:F}}}),Qt=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVideoClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ta=new P({props:{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.example",$$slots:{default:[hk]},$$scope:{ctx:F}}}),Ht=new G({props:{title:"AutoModelForKeypointDetection",local:"transformers.AutoModelForKeypointDetection",headingTag:"h3"}}),Ot=new L({props:{name:"class transformers.AutoModelForKeypointDetection",anchor:"transformers.AutoModelForKeypointDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1882"}}),Kt=new G({props:{title:"AutoModelForKeypointMatching",local:"transformers.AutoModelForKeypointMatching",headingTag:"h3"}}),oi=new L({props:{name:"class transformers.AutoModelForKeypointMatching",anchor:"transformers.AutoModelForKeypointMatching",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1886"}}),ri=new G({props:{title:"AutoModelForMaskedImageModeling",local:"transformers.AutoModelForMaskedImageModeling",headingTag:"h3"}}),ni=new L({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2192"}}),ai=new L({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedImageModeling.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling">FocalNetForMaskedImageModeling</a> (FocalNet model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ia=new P({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.example",$$slots:{default:[uk]},$$scope:{ctx:F}}}),si=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),la=new P({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.example",$$slots:{default:[pk]},$$scope:{ctx:F}}}),ti=new G({props:{title:"AutoModelForObjectDetection",local:"transformers.AutoModelForObjectDetection",headingTag:"h3"}}),ii=new L({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2086"}}),li=new L({props:{name:"from_config",anchor:"transformers.AutoModelForObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineConfig">DFineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/d_fine#transformers.DFineForObjectDetection">DFineForObjectDetection</a> (D-FINE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrConfig">DabDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dab-detr#transformers.DabDetrForObjectDetection">DabDetrForObjectDetection</a> (DAB-DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrConfig">RTDetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr#transformers.RTDetrForObjectDetection">RTDetrForObjectDetection</a> (RT-DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config">RTDetrV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection">RTDetrV2ForObjectDetection</a> (RT-DETRv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection">TableTransformerForObjectDetection</a> (Table Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/yolos#transformers.YolosForObjectDetection">YolosForObjectDetection</a> (YOLOS model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForObjectDetection.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),da=new P({props:{anchor:"transformers.AutoModelForObjectDetection.from_config.example",$$slots:{default:[_k]},$$scope:{ctx:F}}}),di=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ma=new P({props:{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.example",$$slots:{default:[vk]},$$scope:{ctx:F}}}),mi=new G({props:{title:"AutoModelForImageSegmentation",local:"transformers.AutoModelForImageSegmentation",headingTag:"h3"}}),ci=new L({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2043"}}),fi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForImageSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageSegmentation.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ca=new P({props:{anchor:"transformers.AutoModelForImageSegmentation.from_config.example",$$slots:{default:[Mk]},$$scope:{ctx:F}}}),gi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),fa=new P({props:{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.example",$$slots:{default:[bk]},$$scope:{ctx:F}}}),hi=new G({props:{title:"AutoModelForImageToImage",local:"transformers.AutoModelForImageToImage",headingTag:"h3"}}),pi=new L({props:{name:"class transformers.AutoModelForImageToImage",anchor:"transformers.AutoModelForImageToImage",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1894"}}),_i=new G({props:{title:"AutoModelForSemanticSegmentation",local:"transformers.AutoModelForSemanticSegmentation",headingTag:"h3"}}),vi=new L({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2050"}}),Mi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForSemanticSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTForSemanticSegmentation">DPTForSemanticSegmentation</a> (DPT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation">MobileNetV2ForSemanticSegmentation</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation">MobileViTForSemanticSegmentation</a> (MobileViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetConfig">UperNetConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> (UPerNet model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ga=new P({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.example",$$slots:{default:[Ck]},$$scope:{ctx:F}}}),bi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ha=new P({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.example",$$slots:{default:[Tk]},$$scope:{ctx:F}}}),Ci=new G({props:{title:"AutoModelForInstanceSegmentation",local:"transformers.AutoModelForInstanceSegmentation",headingTag:"h3"}}),Ti=new L({props:{name:"class transformers.AutoModelForInstanceSegmentation",anchor:"transformers.AutoModelForInstanceSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2077"}}),Fi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForInstanceSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ua=new P({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.example",$$slots:{default:[Fk]},$$scope:{ctx:F}}}),yi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),pa=new P({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.example",$$slots:{default:[yk]},$$scope:{ctx:F}}}),wi=new G({props:{title:"AutoModelForUniversalSegmentation",local:"transformers.AutoModelForUniversalSegmentation",headingTag:"h3"}}),Li=new L({props:{name:"class transformers.AutoModelForUniversalSegmentation",anchor:"transformers.AutoModelForUniversalSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2068"}}),ki=new L({props:{name:"from_config",anchor:"transformers.AutoModelForUniversalSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtConfig">EomtConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/eomt#transformers.EomtForUniversalSegmentation">EomtForUniversalSegmentation</a> (EoMT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation">Mask2FormerForUniversalSegmentation</a> (Mask2Former model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation">OneFormerForUniversalSegmentation</a> (OneFormer model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),_a=new P({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.example",$$slots:{default:[wk]},$$scope:{ctx:F}}}),xi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),va=new P({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.example",$$slots:{default:[Lk]},$$scope:{ctx:F}}}),Pi=new G({props:{title:"AutoModelForZeroShotImageClassification",local:"transformers.AutoModelForZeroShotImageClassification",headingTag:"h3"}}),Gi=new L({props:{name:"class transformers.AutoModelForZeroShotImageClassification",anchor:"transformers.AutoModelForZeroShotImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2034"}}),Vi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval">Blip2ForImageTextRetrieval</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a> (MetaCLIP 2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Config">Siglip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip2#transformers.Siglip2Model">Siglip2Model</a> (SigLIP2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Ma=new P({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.example",$$slots:{default:[kk]},$$scope:{ctx:F}}}),Ai=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ba=new P({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.example",$$slots:{default:[xk]},$$scope:{ctx:F}}}),Si=new G({props:{title:"AutoModelForZeroShotObjectDetection",local:"transformers.AutoModelForZeroShotObjectDetection",headingTag:"h3"}}),Bi=new L({props:{name:"class transformers.AutoModelForZeroShotObjectDetection",anchor:"transformers.AutoModelForZeroShotObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2093"}}),$i=new L({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoConfig">GroundingDinoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoForObjectDetection">GroundingDinoForObjectDetection</a> (Grounding DINO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig">MMGroundingDinoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection">MMGroundingDinoForObjectDetection</a> (MM Grounding DINO model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboConfig">OmDetTurboConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/omdet-turbo#transformers.OmDetTurboForObjectDetection">OmDetTurboForObjectDetection</a> (OmDet-Turbo model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/owlvit#transformers.OwlViTForObjectDetection">OwlViTForObjectDetection</a> (OWL-ViT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection">Owlv2ForObjectDetection</a> (OWLv2 model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Ca=new P({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.example",$$slots:{default:[Pk]},$$scope:{ctx:F}}}),Ii=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ta=new P({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.example",$$slots:{default:[Gk]},$$scope:{ctx:F}}}),ji=new G({props:{title:"Audio",local:"audio",headingTag:"h2"}}),Zi=new G({props:{title:"AutoModelForAudioClassification",local:"transformers.AutoModelForAudioClassification",headingTag:"h3"}}),Ri=new L({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2141"}}),Wi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForAudioClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification">Wav2Vec2BertForSequenceClassification</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForAudioClassification">WhisperForAudioClassification</a> (Whisper model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Fa=new P({props:{anchor:"transformers.AutoModelForAudioClassification.from_config.example",$$slots:{default:[Vk]},$$scope:{ctx:F}}}),Ni=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),ya=new P({props:{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.example",$$slots:{default:[Ak]},$$scope:{ctx:F}}}),Ji=new G({props:{title:"AutoModelForAudioFrameClassification",local:"transformers.AutoModelForAudioFrameClassification",headingTag:"h3"}}),Di=new L({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2164"}}),Ui=new L({props:{name:"from_config",anchor:"transformers.AutoModelForAudioFrameClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification">Wav2Vec2BertForAudioFrameClassification</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),wa=new P({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.example",$$slots:{default:[Sk]},$$scope:{ctx:F}}}),qi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),La=new P({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.example",$$slots:{default:[Bk]},$$scope:{ctx:F}}}),zi=new G({props:{title:"AutoModelForCTC",local:"transformers.AutoModelForCTC",headingTag:"h3"}}),Xi=new L({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2148"}}),Qi=new L({props:{name:"from_config",anchor:"transformers.AutoModelForCTC.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mctct#transformers.MCTCTForCTC">MCTCTForCTC</a> (M-CTC-T model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC">Wav2Vec2BertForCTC</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCTC.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ka=new P({props:{anchor:"transformers.AutoModelForCTC.from_config.example",$$slots:{default:[$k]},$$scope:{ctx:F}}}),Hi=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCTC.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCTC.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCTC.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCTC.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCTC.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCTC.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCTC.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCTC.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCTC.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),xa=new P({props:{anchor:"transformers.AutoModelForCTC.from_pretrained.example",$$slots:{default:[Ik]},$$scope:{ctx:F}}}),Yi=new G({props:{title:"AutoModelForSpeechSeq2Seq",local:"transformers.AutoModelForSpeechSeq2Seq",headingTag:"h3"}}),Oi=new L({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2155"}}),Ki=new L({props:{name:"from_config",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaConfig">DiaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dia#transformers.DiaForConditionalGeneration">DiaForConditionalGeneration</a> (Dia model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechConfig">GraniteSpeechConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granite_speech#transformers.GraniteSpeechForConditionalGeneration">GraniteSpeechForConditionalGeneration</a> (GraniteSpeech model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextConfig">KyutaiSpeechToTextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kyutai_speech_to_text#transformers.KyutaiSpeechToTextForConditionalGeneration">KyutaiSpeechToTextForConditionalGeneration</a> (KyutaiSpeechToText model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineConfig">MoonshineConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/moonshine#transformers.MoonshineForConditionalGeneration">MoonshineForConditionalGeneration</a> (Moonshine model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig">Pop2PianoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration">Pop2PianoForConditionalGeneration</a> (Pop2Piano model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText">SeamlessM4TForSpeechToText</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText">SeamlessM4Tv2ForSpeechToText</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText">SpeechT5ForSpeechToText</a> (SpeechT5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/whisper#transformers.WhisperForConditionalGeneration">WhisperForConditionalGeneration</a> (Whisper model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Pa=new P({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[jk]},$$scope:{ctx:F}}}),el=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ga=new P({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[Ek]},$$scope:{ctx:F}}}),ol=new G({props:{title:"AutoModelForAudioXVector",local:"transformers.AutoModelForAudioXVector",headingTag:"h3"}}),rl=new L({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2173"}}),nl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForAudioXVector.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector">Wav2Vec2BertForXVector</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioXVector.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Va=new P({props:{anchor:"transformers.AutoModelForAudioXVector.from_config.example",$$slots:{default:[Zk]},$$scope:{ctx:F}}}),al=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioXVector.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Aa=new P({props:{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.example",$$slots:{default:[Rk]},$$scope:{ctx:F}}}),sl=new G({props:{title:"AutoModelForTextToSpectrogram",local:"transformers.AutoModelForTextToSpectrogram",headingTag:"h3"}}),il=new L({props:{name:"class transformers.AutoModelForTextToSpectrogram",anchor:"transformers.AutoModelForTextToSpectrogram",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2177"}}),ll=new G({props:{title:"AutoModelForTextToWaveform",local:"transformers.AutoModelForTextToWaveform",headingTag:"h3"}}),ml=new L({props:{name:"class transformers.AutoModelForTextToWaveform",anchor:"transformers.AutoModelForTextToWaveform",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2181"}}),cl=new G({props:{title:"AutoModelForAudioTokenization",local:"transformers.AutoModelForAudioTokenization",headingTag:"h3"}}),fl=new L({props:{name:"class transformers.AutoModelForAudioTokenization",anchor:"transformers.AutoModelForAudioTokenization",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2199"}}),gl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForAudioTokenization.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioTokenization.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacConfig">DacConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a> (DAC model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioTokenization.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Sa=new P({props:{anchor:"transformers.AutoModelForAudioTokenization.from_config.example",$$slots:{default:[Wk]},$$scope:{ctx:F}}}),hl=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioTokenization.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ba=new P({props:{anchor:"transformers.AutoModelForAudioTokenization.from_pretrained.example",$$slots:{default:[Nk]},$$scope:{ctx:F}}}),ul=new G({props:{title:"Multimodal",local:"multimodal",headingTag:"h2"}}),_l=new G({props:{title:"AutoModelForTableQuestionAnswering",local:"transformers.AutoModelForTableQuestionAnswering",headingTag:"h3"}}),vl=new L({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1971"}}),Ml=new L({props:{name:"from_config",anchor:"transformers.AutoModelForTableQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),$a=new P({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.example",$$slots:{default:[Jk]},$$scope:{ctx:F}}}),bl=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ia=new P({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.example",$$slots:{default:[Dk]},$$scope:{ctx:F}}}),Cl=new G({props:{title:"AutoModelForDocumentQuestionAnswering",local:"transformers.AutoModelForDocumentQuestionAnswering",headingTag:"h3"}}),Tl=new L({props:{name:"class transformers.AutoModelForDocumentQuestionAnswering",anchor:"transformers.AutoModelForDocumentQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1993"}}),Fl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering">LayoutLMForQuestionAnswering</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),ja=new P({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.example",$$slots:{default:[Uk]},$$scope:{ctx:F}}}),yl=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ea=new P({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.example",$$slots:{default:[qk]},$$scope:{ctx:F}}}),wl=new G({props:{title:"AutoModelForVisualQuestionAnswering",local:"transformers.AutoModelForVisualQuestionAnswering",headingTag:"h3"}}),Ll=new L({props:{name:"class transformers.AutoModelForVisualQuestionAnswering",anchor:"transformers.AutoModelForVisualQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L1982"}}),kl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipForQuestionAnswering">BlipForQuestionAnswering</a> (BLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vilt#transformers.ViltForQuestionAnswering">ViltForQuestionAnswering</a> (ViLT model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Za=new P({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.example",$$slots:{default:[zk]},$$scope:{ctx:F}}}),xl=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Ra=new P({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.example",$$slots:{default:[Xk]},$$scope:{ctx:F}}}),Pl=new G({props:{title:"AutoModelForVision2Seq",local:"transformers.AutoModelForVision2Seq",headingTag:"h3"}}),Vl=new L({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2230"}}),Al=new G({props:{title:"AutoModelForImageTextToText",local:"transformers.AutoModelForImageTextToText",headingTag:"h3"}}),Sl=new L({props:{name:"class transformers.AutoModelForImageTextToText",anchor:"transformers.AutoModelForImageTextToText",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2124"}}),Bl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForImageTextToText.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageTextToText.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaConfig">AriaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aria#transformers.AriaForConditionalGeneration">AriaForConditionalGeneration</a> (Aria model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionConfig">AyaVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/aya_vision#transformers.AyaVisionForConditionalGeneration">AyaVisionForConditionalGeneration</a> (AyaVision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonConfig">ChameleonConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/chameleon#transformers.ChameleonForConditionalGeneration">ChameleonForConditionalGeneration</a> (Chameleon model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionConfig">Cohere2VisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/cohere2_vision#transformers.Cohere2VisionForConditionalGeneration">Cohere2VisionForConditionalGeneration</a> (Cohere2Vision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLConfig">DeepseekVLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl#transformers.DeepseekVLForConditionalGeneration">DeepseekVLForConditionalGeneration</a> (DeepseekVL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridConfig">DeepseekVLHybridConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/deepseek_vl_hybrid#transformers.DeepseekVLHybridForConditionalGeneration">DeepseekVLHybridForConditionalGeneration</a> (DeepseekVLHybrid model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3Config">Emu3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/emu3#transformers.Emu3ForConditionalGeneration">Emu3ForConditionalGeneration</a> (Emu3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaConfig">EvollaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/evolla#transformers.EvollaForProteinText2Text">EvollaForProteinText2Text</a> (Evolla model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2Config">Florence2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/florence2#transformers.Florence2ForConditionalGeneration">Florence2ForConditionalGeneration</a> (Florence2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3Config">Gemma3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Gemma3ForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nConfig">Gemma3nConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3n#transformers.Gemma3nForConditionalGeneration">Gemma3nForConditionalGeneration</a> (Gemma3nForConditionalGeneration model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vConfig">Glm4vConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v#transformers.Glm4vForConditionalGeneration">Glm4vForConditionalGeneration</a> (GLM4V model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeConfig">Glm4vMoeConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/glm4v_moe#transformers.Glm4vMoeForConditionalGeneration">Glm4vMoeForConditionalGeneration</a> (GLM4VMOE model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2Config">GotOcr2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/got_ocr2#transformers.GotOcr2ForConditionalGeneration">GotOcr2ForConditionalGeneration</a> (GOT-OCR2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2Config">Idefics2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics2#transformers.Idefics2ForConditionalGeneration">Idefics2ForConditionalGeneration</a> (Idefics2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3Config">Idefics3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration">Idefics3ForConditionalGeneration</a> (Idefics3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipConfig">InstructBlipConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration">InstructBlipForConditionalGeneration</a> (InstructBLIP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLConfig">InternVLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/internvl#transformers.InternVLForConditionalGeneration">InternVLForConditionalGeneration</a> (InternVL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig">JanusConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusForConditionalGeneration">JanusForConditionalGeneration</a> (Janus model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration">Kosmos2ForConditionalGeneration</a> (KOSMOS-2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5Config">Kosmos2_5Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/kosmos2_5#transformers.Kosmos2_5ForConditionalGeneration">Kosmos2_5ForConditionalGeneration</a> (KOSMOS-2.5 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4Config">Llama4Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llama4#transformers.Llama4ForConditionalGeneration">Llama4ForConditionalGeneration</a> (Llama4 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextConfig">LlavaNextConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/granitevision#transformers.LlavaNextForConditionalGeneration">LlavaNextForConditionalGeneration</a> (LLaVA-NeXT model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoConfig">LlavaNextVideoConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_next_video#transformers.LlavaNextVideoForConditionalGeneration">LlavaNextVideoForConditionalGeneration</a> (LLaVa-NeXT-Video model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionConfig">LlavaOnevisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava_onevision#transformers.LlavaOnevisionForConditionalGeneration">LlavaOnevisionForConditionalGeneration</a> (LLaVA-Onevision model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3Config">Mistral3Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mistral3#transformers.Mistral3ForConditionalGeneration">Mistral3ForConditionalGeneration</a> (Mistral3 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaConfig">MllamaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/mllama#transformers.MllamaForConditionalGeneration">MllamaForConditionalGeneration</a> (Mllama model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2Config">Ovis2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/ovis2#transformers.Ovis2ForConditionalGeneration">Ovis2ForConditionalGeneration</a> (Ovis2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaConfig">PaliGemmaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration">PaliGemmaForConditionalGeneration</a> (PaliGemma model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMConfig">PerceptionLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/perception_lm#transformers.PerceptionLMForConditionalGeneration">PerceptionLMForConditionalGeneration</a> (PerceptionLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructConfig">Pix2StructConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration">Pix2StructForConditionalGeneration</a> (Pix2Struct model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.PixtralVisionConfig">PixtralVisionConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (Pixtral model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLConfig">Qwen2VLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_vl#transformers.Qwen2VLForConditionalGeneration">Qwen2VLForConditionalGeneration</a> (Qwen2VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLConfig">Qwen2_5_VLConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/qwen2_5_vl#transformers.Qwen2_5_VLForConditionalGeneration">Qwen2_5_VLForConditionalGeneration</a> (Qwen2_5_VL model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/shieldgemma2#transformers.ShieldGemma2Config">ShieldGemma2Config</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/gemma3#transformers.Gemma3ForConditionalGeneration">Gemma3ForConditionalGeneration</a> (Shieldgemma2 model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMConfig">SmolVLMConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/smolvlm#transformers.SmolVLMForConditionalGeneration">SmolVLMForConditionalGeneration</a> (SmolVLM model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopConfig">UdopConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a> (UDOP model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageTextToText.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Wa=new P({props:{anchor:"transformers.AutoModelForImageTextToText.from_config.example",$$slots:{default:[Qk]},$$scope:{ctx:F}}}),$l=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageTextToText.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Na=new P({props:{anchor:"transformers.AutoModelForImageTextToText.from_pretrained.example",$$slots:{default:[Hk]},$$scope:{ctx:F}}}),Il=new G({props:{title:"Time Series",local:"time-series",headingTag:"h2"}}),jl=new G({props:{title:"AutoModelForTimeSeriesPrediction",local:"transformers.AutoModelForTimeSeriesPrediction",headingTag:"h3"}}),El=new L({props:{name:"class transformers.AutoModelForTimeSeriesPrediction",anchor:"transformers.AutoModelForTimeSeriesPrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/modeling_auto.py#L2059"}}),Zl=new L({props:{name:"from_config",anchor:"transformers.AutoModelForTimeSeriesPrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmConfig">TimesFmConfig</a> configuration class: <a href="/docs/transformers/v4.56.2/en/model_doc/timesfm#transformers.TimesFmModelForPrediction">TimesFmModelForPrediction</a> (TimesFm model)</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_config.attn_implementation",description:`<strong>attn_implementation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The attention implementation to use in the model (if relevant). Can be any of <code>&quot;eager&quot;</code> (manual implementation of the attention), <code>&quot;sdpa&quot;</code> (using <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow"><code>F.scaled_dot_product_attention</code></a>), or <code>&quot;flash_attention_2&quot;</code> (using <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">Dao-AILab/flash-attention</a>). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <code>&quot;eager&quot;</code> implementation.`,name:"attn_implementation"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L424"}}),Ja=new P({props:{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_config.example",$$slots:{default:[Yk]},$$scope:{ctx:F}}}),Rl=new L({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/auto/auto_factory.py#L468"}}),Da=new P({props:{anchor:"transformers.AutoModelForTimeSeriesPrediction.from_pretrained.example",$$slots:{default:[Ok]},$$scope:{ctx:F}}}),Wl=new VL({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/auto.md"}}),{c(){s=i("meta"),T=n(),c=i("p"),t=n(),u(g.$$.fragment),o=n(),C=i("p"),C.innerHTML=J5,yh=n(),ts=i("p"),ts.innerHTML=D5,wh=n(),u(is.$$.fragment),Lh=n(),ls=i("p"),ls.innerHTML=U5,kh=n(),ds=i("p"),ds.innerHTML=q5,xh=n(),u(ms.$$.fragment),Ph=n(),cs=i("p"),cs.innerHTML=z5,Gh=n(),u(fs.$$.fragment),Vh=n(),gs=i("p"),gs.textContent=X5,Ah=n(),u(vn.$$.fragment),Sh=n(),u(hs.$$.fragment),Bh=n(),ge=i("div"),u(us.$$.fragment),Sp=n(),Xl=i("p"),Xl.innerHTML=Q5,Bp=n(),Ql=i("p"),Ql.innerHTML=H5,$p=n(),Mo=i("div"),u(ps.$$.fragment),Ip=n(),Hl=i("p"),Hl.textContent=Y5,jp=n(),Yl=i("p"),Yl.innerHTML=O5,Ep=n(),Ol=i("ul"),Ol.innerHTML=K5,Zp=n(),u(Mn.$$.fragment),Rp=n(),bn=i("div"),u(_s.$$.fragment),Wp=n(),Kl=i("p"),Kl.textContent=e4,$h=n(),u(vs.$$.fragment),Ih=n(),he=i("div"),u(Ms.$$.fragment),Np=n(),ed=i("p"),ed.innerHTML=o4,Jp=n(),od=i("p"),od.innerHTML=r4,Dp=n(),bo=i("div"),u(bs.$$.fragment),Up=n(),rd=i("p"),rd.textContent=n4,qp=n(),nd=i("p"),nd.innerHTML=a4,zp=n(),ad=i("ul"),ad.innerHTML=s4,Xp=n(),u(Cn.$$.fragment),Qp=n(),Tn=i("div"),u(Cs.$$.fragment),Hp=n(),sd=i("p"),sd.textContent=t4,jh=n(),u(Ts.$$.fragment),Eh=n(),ue=i("div"),u(Fs.$$.fragment),Yp=n(),td=i("p"),td.innerHTML=i4,Op=n(),id=i("p"),id.innerHTML=l4,Kp=n(),V=i("div"),u(ys.$$.fragment),e_=n(),ld=i("p"),ld.textContent=d4,o_=n(),dd=i("p"),dd.innerHTML=m4,r_=n(),md=i("ul"),md.innerHTML=c4,n_=n(),u(Fn.$$.fragment),a_=n(),u(yn.$$.fragment),s_=n(),wn=i("div"),u(ws.$$.fragment),t_=n(),cd=i("p"),cd.textContent=f4,Zh=n(),u(Ls.$$.fragment),Rh=n(),pe=i("div"),u(ks.$$.fragment),i_=n(),fd=i("p"),fd.innerHTML=g4,l_=n(),gd=i("p"),gd.innerHTML=h4,d_=n(),A=i("div"),u(xs.$$.fragment),m_=n(),hd=i("p"),hd.textContent=u4,c_=n(),ud=i("p"),ud.innerHTML=p4,f_=n(),pd=i("ul"),pd.innerHTML=_4,g_=n(),u(Ln.$$.fragment),h_=n(),u(kn.$$.fragment),u_=n(),xn=i("div"),u(Ps.$$.fragment),p_=n(),_d=i("p"),_d.textContent=v4,Wh=n(),u(Gs.$$.fragment),Nh=n(),_e=i("div"),u(Vs.$$.fragment),__=n(),vd=i("p"),vd.innerHTML=M4,v_=n(),Md=i("p"),Md.innerHTML=b4,M_=n(),S=i("div"),u(As.$$.fragment),b_=n(),bd=i("p"),bd.textContent=C4,C_=n(),Cd=i("p"),Cd.innerHTML=T4,T_=n(),Td=i("ul"),Td.innerHTML=F4,F_=n(),u(Pn.$$.fragment),y_=n(),u(Gn.$$.fragment),w_=n(),Vn=i("div"),u(Ss.$$.fragment),L_=n(),Fd=i("p"),Fd.textContent=y4,Jh=n(),u(Bs.$$.fragment),Dh=n(),ve=i("div"),u($s.$$.fragment),k_=n(),yd=i("p"),yd.innerHTML=w4,x_=n(),wd=i("p"),wd.innerHTML=L4,P_=n(),B=i("div"),u(Is.$$.fragment),G_=n(),Ld=i("p"),Ld.textContent=k4,V_=n(),kd=i("p"),kd.innerHTML=x4,A_=n(),xd=i("ul"),xd.innerHTML=P4,S_=n(),u(An.$$.fragment),B_=n(),u(Sn.$$.fragment),$_=n(),Bn=i("div"),u(js.$$.fragment),I_=n(),Pd=i("p"),Pd.textContent=G4,Uh=n(),u(Es.$$.fragment),qh=n(),Zs=i("p"),Zs.textContent=V4,zh=n(),u(Rs.$$.fragment),Xh=n(),Me=i("div"),u(Ws.$$.fragment),j_=n(),Gd=i("p"),Gd.innerHTML=A4,E_=n(),Vd=i("p"),Vd.innerHTML=S4,Z_=n(),zo=i("div"),u(Ns.$$.fragment),R_=n(),Ad=i("p"),Ad.textContent=B4,W_=n(),Sd=i("p"),Sd.innerHTML=$4,N_=n(),u($n.$$.fragment),J_=n(),$=i("div"),u(Js.$$.fragment),D_=n(),Bd=i("p"),Bd.textContent=I4,U_=n(),$d=i("p"),$d.innerHTML=j4,q_=n(),Id=i("ul"),Id.innerHTML=E4,z_=n(),jd=i("p"),jd.innerHTML=Z4,X_=n(),u(In.$$.fragment),Qh=n(),u(Ds.$$.fragment),Hh=n(),Us=i("p"),Us.textContent=R4,Yh=n(),u(qs.$$.fragment),Oh=n(),be=i("div"),u(zs.$$.fragment),Q_=n(),Ed=i("p"),Ed.innerHTML=W4,H_=n(),Zd=i("p"),Zd.innerHTML=N4,Y_=n(),Xo=i("div"),u(Xs.$$.fragment),O_=n(),Rd=i("p"),Rd.textContent=J4,K_=n(),Wd=i("p"),Wd.innerHTML=D4,ev=n(),u(jn.$$.fragment),ov=n(),I=i("div"),u(Qs.$$.fragment),rv=n(),Nd=i("p"),Nd.textContent=U4,nv=n(),Jd=i("p"),Jd.innerHTML=q4,av=n(),Dd=i("ul"),Dd.innerHTML=z4,sv=n(),Ud=i("p"),Ud.innerHTML=X4,tv=n(),u(En.$$.fragment),Kh=n(),u(Hs.$$.fragment),eu=n(),Ys=i("p"),Ys.textContent=Q4,ou=n(),u(Os.$$.fragment),ru=n(),Ce=i("div"),u(Ks.$$.fragment),iv=n(),qd=i("p"),qd.innerHTML=H4,lv=n(),zd=i("p"),zd.innerHTML=Y4,dv=n(),Qo=i("div"),u(et.$$.fragment),mv=n(),Xd=i("p"),Xd.textContent=O4,cv=n(),Qd=i("p"),Qd.innerHTML=K4,fv=n(),u(Zn.$$.fragment),gv=n(),j=i("div"),u(ot.$$.fragment),hv=n(),Hd=i("p"),Hd.textContent=eF,uv=n(),Yd=i("p"),Yd.innerHTML=oF,pv=n(),Od=i("ul"),Od.innerHTML=rF,_v=n(),Kd=i("p"),Kd.innerHTML=nF,vv=n(),u(Rn.$$.fragment),nu=n(),u(rt.$$.fragment),au=n(),Te=i("div"),u(nt.$$.fragment),Mv=n(),em=i("p"),em.innerHTML=aF,bv=n(),om=i("p"),om.innerHTML=sF,Cv=n(),Ho=i("div"),u(at.$$.fragment),Tv=n(),rm=i("p"),rm.textContent=tF,Fv=n(),nm=i("p"),nm.innerHTML=iF,yv=n(),u(Wn.$$.fragment),wv=n(),E=i("div"),u(st.$$.fragment),Lv=n(),am=i("p"),am.textContent=lF,kv=n(),sm=i("p"),sm.innerHTML=dF,xv=n(),tm=i("ul"),tm.innerHTML=mF,Pv=n(),im=i("p"),im.innerHTML=cF,Gv=n(),u(Nn.$$.fragment),su=n(),u(tt.$$.fragment),tu=n(),it=i("div"),u(lt.$$.fragment),iu=n(),u(dt.$$.fragment),lu=n(),Fe=i("div"),u(mt.$$.fragment),Vv=n(),lm=i("p"),lm.innerHTML=fF,Av=n(),dm=i("p"),dm.innerHTML=gF,Sv=n(),Yo=i("div"),u(ct.$$.fragment),Bv=n(),mm=i("p"),mm.textContent=hF,$v=n(),cm=i("p"),cm.innerHTML=uF,Iv=n(),u(Jn.$$.fragment),jv=n(),Z=i("div"),u(ft.$$.fragment),Ev=n(),fm=i("p"),fm.textContent=pF,Zv=n(),gm=i("p"),gm.innerHTML=_F,Rv=n(),hm=i("ul"),hm.innerHTML=vF,Wv=n(),um=i("p"),um.innerHTML=MF,Nv=n(),u(Dn.$$.fragment),du=n(),u(gt.$$.fragment),mu=n(),ye=i("div"),u(ht.$$.fragment),Jv=n(),pm=i("p"),pm.innerHTML=bF,Dv=n(),_m=i("p"),_m.innerHTML=CF,Uv=n(),Oo=i("div"),u(ut.$$.fragment),qv=n(),vm=i("p"),vm.textContent=TF,zv=n(),Mm=i("p"),Mm.innerHTML=FF,Xv=n(),u(Un.$$.fragment),Qv=n(),R=i("div"),u(pt.$$.fragment),Hv=n(),bm=i("p"),bm.textContent=yF,Yv=n(),Cm=i("p"),Cm.innerHTML=wF,Ov=n(),Tm=i("ul"),Tm.innerHTML=LF,Kv=n(),Fm=i("p"),Fm.innerHTML=kF,e2=n(),u(qn.$$.fragment),cu=n(),u(_t.$$.fragment),fu=n(),we=i("div"),u(vt.$$.fragment),o2=n(),ym=i("p"),ym.innerHTML=xF,r2=n(),wm=i("p"),wm.innerHTML=PF,n2=n(),Ko=i("div"),u(Mt.$$.fragment),a2=n(),Lm=i("p"),Lm.textContent=GF,s2=n(),km=i("p"),km.innerHTML=VF,t2=n(),u(zn.$$.fragment),i2=n(),W=i("div"),u(bt.$$.fragment),l2=n(),xm=i("p"),xm.textContent=AF,d2=n(),Pm=i("p"),Pm.innerHTML=SF,m2=n(),Gm=i("ul"),Gm.innerHTML=BF,c2=n(),Vm=i("p"),Vm.innerHTML=$F,f2=n(),u(Xn.$$.fragment),gu=n(),u(Ct.$$.fragment),hu=n(),Le=i("div"),u(Tt.$$.fragment),g2=n(),Am=i("p"),Am.innerHTML=IF,h2=n(),Sm=i("p"),Sm.innerHTML=jF,u2=n(),er=i("div"),u(Ft.$$.fragment),p2=n(),Bm=i("p"),Bm.textContent=EF,_2=n(),$m=i("p"),$m.innerHTML=ZF,v2=n(),u(Qn.$$.fragment),M2=n(),N=i("div"),u(yt.$$.fragment),b2=n(),Im=i("p"),Im.textContent=RF,C2=n(),jm=i("p"),jm.innerHTML=WF,T2=n(),Em=i("ul"),Em.innerHTML=NF,F2=n(),Zm=i("p"),Zm.innerHTML=JF,y2=n(),u(Hn.$$.fragment),uu=n(),u(wt.$$.fragment),pu=n(),ke=i("div"),u(Lt.$$.fragment),w2=n(),Rm=i("p"),Rm.innerHTML=DF,L2=n(),Wm=i("p"),Wm.innerHTML=UF,k2=n(),or=i("div"),u(kt.$$.fragment),x2=n(),Nm=i("p"),Nm.textContent=qF,P2=n(),Jm=i("p"),Jm.innerHTML=zF,G2=n(),u(Yn.$$.fragment),V2=n(),J=i("div"),u(xt.$$.fragment),A2=n(),Dm=i("p"),Dm.textContent=XF,S2=n(),Um=i("p"),Um.innerHTML=QF,B2=n(),qm=i("ul"),qm.innerHTML=HF,$2=n(),zm=i("p"),zm.innerHTML=YF,I2=n(),u(On.$$.fragment),_u=n(),u(Pt.$$.fragment),vu=n(),xe=i("div"),u(Gt.$$.fragment),j2=n(),Xm=i("p"),Xm.innerHTML=OF,E2=n(),Qm=i("p"),Qm.innerHTML=KF,Z2=n(),rr=i("div"),u(Vt.$$.fragment),R2=n(),Hm=i("p"),Hm.textContent=ey,W2=n(),Ym=i("p"),Ym.innerHTML=oy,N2=n(),u(Kn.$$.fragment),J2=n(),D=i("div"),u(At.$$.fragment),D2=n(),Om=i("p"),Om.textContent=ry,U2=n(),Km=i("p"),Km.innerHTML=ny,q2=n(),ec=i("ul"),ec.innerHTML=ay,z2=n(),oc=i("p"),oc.innerHTML=sy,X2=n(),u(ea.$$.fragment),Mu=n(),u(St.$$.fragment),bu=n(),Bt=i("div"),u($t.$$.fragment),Cu=n(),u(It.$$.fragment),Tu=n(),jt=i("p"),jt.textContent=ty,Fu=n(),u(Et.$$.fragment),yu=n(),Pe=i("div"),u(Zt.$$.fragment),Q2=n(),rc=i("p"),rc.innerHTML=iy,H2=n(),nc=i("p"),nc.innerHTML=ly,Y2=n(),nr=i("div"),u(Rt.$$.fragment),O2=n(),ac=i("p"),ac.textContent=dy,K2=n(),sc=i("p"),sc.innerHTML=my,eM=n(),u(oa.$$.fragment),oM=n(),U=i("div"),u(Wt.$$.fragment),rM=n(),tc=i("p"),tc.textContent=cy,nM=n(),ic=i("p"),ic.innerHTML=fy,aM=n(),lc=i("ul"),lc.innerHTML=gy,sM=n(),dc=i("p"),dc.innerHTML=hy,tM=n(),u(ra.$$.fragment),wu=n(),u(Nt.$$.fragment),Lu=n(),Ge=i("div"),u(Jt.$$.fragment),iM=n(),mc=i("p"),mc.innerHTML=uy,lM=n(),cc=i("p"),cc.innerHTML=py,dM=n(),ar=i("div"),u(Dt.$$.fragment),mM=n(),fc=i("p"),fc.textContent=_y,cM=n(),gc=i("p"),gc.innerHTML=vy,fM=n(),u(na.$$.fragment),gM=n(),q=i("div"),u(Ut.$$.fragment),hM=n(),hc=i("p"),hc.textContent=My,uM=n(),uc=i("p"),uc.innerHTML=by,pM=n(),pc=i("ul"),pc.innerHTML=Cy,_M=n(),_c=i("p"),_c.innerHTML=Ty,vM=n(),u(aa.$$.fragment),ku=n(),u(qt.$$.fragment),xu=n(),Ve=i("div"),u(zt.$$.fragment),MM=n(),vc=i("p"),vc.innerHTML=Fy,bM=n(),Mc=i("p"),Mc.innerHTML=yy,CM=n(),sr=i("div"),u(Xt.$$.fragment),TM=n(),bc=i("p"),bc.textContent=wy,FM=n(),Cc=i("p"),Cc.innerHTML=Ly,yM=n(),u(sa.$$.fragment),wM=n(),z=i("div"),u(Qt.$$.fragment),LM=n(),Tc=i("p"),Tc.textContent=ky,kM=n(),Fc=i("p"),Fc.innerHTML=xy,xM=n(),yc=i("ul"),yc.innerHTML=Py,PM=n(),wc=i("p"),wc.innerHTML=Gy,GM=n(),u(ta.$$.fragment),Pu=n(),u(Ht.$$.fragment),Gu=n(),Yt=i("div"),u(Ot.$$.fragment),Vu=n(),u(Kt.$$.fragment),Au=n(),ei=i("div"),u(oi.$$.fragment),Su=n(),u(ri.$$.fragment),Bu=n(),Ae=i("div"),u(ni.$$.fragment),VM=n(),Lc=i("p"),Lc.innerHTML=Vy,AM=n(),kc=i("p"),kc.innerHTML=Ay,SM=n(),tr=i("div"),u(ai.$$.fragment),BM=n(),xc=i("p"),xc.textContent=Sy,$M=n(),Pc=i("p"),Pc.innerHTML=By,IM=n(),u(ia.$$.fragment),jM=n(),X=i("div"),u(si.$$.fragment),EM=n(),Gc=i("p"),Gc.textContent=$y,ZM=n(),Vc=i("p"),Vc.innerHTML=Iy,RM=n(),Ac=i("ul"),Ac.innerHTML=jy,WM=n(),Sc=i("p"),Sc.innerHTML=Ey,NM=n(),u(la.$$.fragment),$u=n(),u(ti.$$.fragment),Iu=n(),Se=i("div"),u(ii.$$.fragment),JM=n(),Bc=i("p"),Bc.innerHTML=Zy,DM=n(),$c=i("p"),$c.innerHTML=Ry,UM=n(),ir=i("div"),u(li.$$.fragment),qM=n(),Ic=i("p"),Ic.textContent=Wy,zM=n(),jc=i("p"),jc.innerHTML=Ny,XM=n(),u(da.$$.fragment),QM=n(),Q=i("div"),u(di.$$.fragment),HM=n(),Ec=i("p"),Ec.textContent=Jy,YM=n(),Zc=i("p"),Zc.innerHTML=Dy,OM=n(),Rc=i("ul"),Rc.innerHTML=Uy,KM=n(),Wc=i("p"),Wc.innerHTML=qy,eb=n(),u(ma.$$.fragment),ju=n(),u(mi.$$.fragment),Eu=n(),Be=i("div"),u(ci.$$.fragment),ob=n(),Nc=i("p"),Nc.innerHTML=zy,rb=n(),Jc=i("p"),Jc.innerHTML=Xy,nb=n(),lr=i("div"),u(fi.$$.fragment),ab=n(),Dc=i("p"),Dc.textContent=Qy,sb=n(),Uc=i("p"),Uc.innerHTML=Hy,tb=n(),u(ca.$$.fragment),ib=n(),H=i("div"),u(gi.$$.fragment),lb=n(),qc=i("p"),qc.textContent=Yy,db=n(),zc=i("p"),zc.innerHTML=Oy,mb=n(),Xc=i("ul"),Xc.innerHTML=Ky,cb=n(),Qc=i("p"),Qc.innerHTML=e6,fb=n(),u(fa.$$.fragment),Zu=n(),u(hi.$$.fragment),Ru=n(),ui=i("div"),u(pi.$$.fragment),Wu=n(),u(_i.$$.fragment),Nu=n(),$e=i("div"),u(vi.$$.fragment),gb=n(),Hc=i("p"),Hc.innerHTML=o6,hb=n(),Yc=i("p"),Yc.innerHTML=r6,ub=n(),dr=i("div"),u(Mi.$$.fragment),pb=n(),Oc=i("p"),Oc.textContent=n6,_b=n(),Kc=i("p"),Kc.innerHTML=a6,vb=n(),u(ga.$$.fragment),Mb=n(),Y=i("div"),u(bi.$$.fragment),bb=n(),ef=i("p"),ef.textContent=s6,Cb=n(),of=i("p"),of.innerHTML=t6,Tb=n(),rf=i("ul"),rf.innerHTML=i6,Fb=n(),nf=i("p"),nf.innerHTML=l6,yb=n(),u(ha.$$.fragment),Ju=n(),u(Ci.$$.fragment),Du=n(),Ie=i("div"),u(Ti.$$.fragment),wb=n(),af=i("p"),af.innerHTML=d6,Lb=n(),sf=i("p"),sf.innerHTML=m6,kb=n(),mr=i("div"),u(Fi.$$.fragment),xb=n(),tf=i("p"),tf.textContent=c6,Pb=n(),lf=i("p"),lf.innerHTML=f6,Gb=n(),u(ua.$$.fragment),Vb=n(),O=i("div"),u(yi.$$.fragment),Ab=n(),df=i("p"),df.textContent=g6,Sb=n(),mf=i("p"),mf.innerHTML=h6,Bb=n(),cf=i("ul"),cf.innerHTML=u6,$b=n(),ff=i("p"),ff.innerHTML=p6,Ib=n(),u(pa.$$.fragment),Uu=n(),u(wi.$$.fragment),qu=n(),je=i("div"),u(Li.$$.fragment),jb=n(),gf=i("p"),gf.innerHTML=_6,Eb=n(),hf=i("p"),hf.innerHTML=v6,Zb=n(),cr=i("div"),u(ki.$$.fragment),Rb=n(),uf=i("p"),uf.textContent=M6,Wb=n(),pf=i("p"),pf.innerHTML=b6,Nb=n(),u(_a.$$.fragment),Jb=n(),K=i("div"),u(xi.$$.fragment),Db=n(),_f=i("p"),_f.textContent=C6,Ub=n(),vf=i("p"),vf.innerHTML=T6,qb=n(),Mf=i("ul"),Mf.innerHTML=F6,zb=n(),bf=i("p"),bf.innerHTML=y6,Xb=n(),u(va.$$.fragment),zu=n(),u(Pi.$$.fragment),Xu=n(),Ee=i("div"),u(Gi.$$.fragment),Qb=n(),Cf=i("p"),Cf.innerHTML=w6,Hb=n(),Tf=i("p"),Tf.innerHTML=L6,Yb=n(),fr=i("div"),u(Vi.$$.fragment),Ob=n(),Ff=i("p"),Ff.textContent=k6,Kb=n(),yf=i("p"),yf.innerHTML=x6,eC=n(),u(Ma.$$.fragment),oC=n(),ee=i("div"),u(Ai.$$.fragment),rC=n(),wf=i("p"),wf.textContent=P6,nC=n(),Lf=i("p"),Lf.innerHTML=G6,aC=n(),kf=i("ul"),kf.innerHTML=V6,sC=n(),xf=i("p"),xf.innerHTML=A6,tC=n(),u(ba.$$.fragment),Qu=n(),u(Si.$$.fragment),Hu=n(),Ze=i("div"),u(Bi.$$.fragment),iC=n(),Pf=i("p"),Pf.innerHTML=S6,lC=n(),Gf=i("p"),Gf.innerHTML=B6,dC=n(),gr=i("div"),u($i.$$.fragment),mC=n(),Vf=i("p"),Vf.textContent=$6,cC=n(),Af=i("p"),Af.innerHTML=I6,fC=n(),u(Ca.$$.fragment),gC=n(),oe=i("div"),u(Ii.$$.fragment),hC=n(),Sf=i("p"),Sf.textContent=j6,uC=n(),Bf=i("p"),Bf.innerHTML=E6,pC=n(),$f=i("ul"),$f.innerHTML=Z6,_C=n(),If=i("p"),If.innerHTML=R6,vC=n(),u(Ta.$$.fragment),Yu=n(),u(ji.$$.fragment),Ou=n(),Ei=i("p"),Ei.textContent=W6,Ku=n(),u(Zi.$$.fragment),ep=n(),Re=i("div"),u(Ri.$$.fragment),MC=n(),jf=i("p"),jf.innerHTML=N6,bC=n(),Ef=i("p"),Ef.innerHTML=J6,CC=n(),hr=i("div"),u(Wi.$$.fragment),TC=n(),Zf=i("p"),Zf.textContent=D6,FC=n(),Rf=i("p"),Rf.innerHTML=U6,yC=n(),u(Fa.$$.fragment),wC=n(),re=i("div"),u(Ni.$$.fragment),LC=n(),Wf=i("p"),Wf.textContent=q6,kC=n(),Nf=i("p"),Nf.innerHTML=z6,xC=n(),Jf=i("ul"),Jf.innerHTML=X6,PC=n(),Df=i("p"),Df.innerHTML=Q6,GC=n(),u(ya.$$.fragment),op=n(),u(Ji.$$.fragment),rp=n(),We=i("div"),u(Di.$$.fragment),VC=n(),Uf=i("p"),Uf.innerHTML=H6,AC=n(),qf=i("p"),qf.innerHTML=Y6,SC=n(),ur=i("div"),u(Ui.$$.fragment),BC=n(),zf=i("p"),zf.textContent=O6,$C=n(),Xf=i("p"),Xf.innerHTML=K6,IC=n(),u(wa.$$.fragment),jC=n(),ne=i("div"),u(qi.$$.fragment),EC=n(),Qf=i("p"),Qf.textContent=ew,ZC=n(),Hf=i("p"),Hf.innerHTML=ow,RC=n(),Yf=i("ul"),Yf.innerHTML=rw,WC=n(),Of=i("p"),Of.innerHTML=nw,NC=n(),u(La.$$.fragment),np=n(),u(zi.$$.fragment),ap=n(),Ne=i("div"),u(Xi.$$.fragment),JC=n(),Kf=i("p"),Kf.innerHTML=aw,DC=n(),eg=i("p"),eg.innerHTML=sw,UC=n(),pr=i("div"),u(Qi.$$.fragment),qC=n(),og=i("p"),og.textContent=tw,zC=n(),rg=i("p"),rg.innerHTML=iw,XC=n(),u(ka.$$.fragment),QC=n(),ae=i("div"),u(Hi.$$.fragment),HC=n(),ng=i("p"),ng.textContent=lw,YC=n(),ag=i("p"),ag.innerHTML=dw,OC=n(),sg=i("ul"),sg.innerHTML=mw,KC=n(),tg=i("p"),tg.innerHTML=cw,eT=n(),u(xa.$$.fragment),sp=n(),u(Yi.$$.fragment),tp=n(),Je=i("div"),u(Oi.$$.fragment),oT=n(),ig=i("p"),ig.innerHTML=fw,rT=n(),lg=i("p"),lg.innerHTML=gw,nT=n(),_r=i("div"),u(Ki.$$.fragment),aT=n(),dg=i("p"),dg.textContent=hw,sT=n(),mg=i("p"),mg.innerHTML=uw,tT=n(),u(Pa.$$.fragment),iT=n(),se=i("div"),u(el.$$.fragment),lT=n(),cg=i("p"),cg.textContent=pw,dT=n(),fg=i("p"),fg.innerHTML=_w,mT=n(),gg=i("ul"),gg.innerHTML=vw,cT=n(),hg=i("p"),hg.innerHTML=Mw,fT=n(),u(Ga.$$.fragment),ip=n(),u(ol.$$.fragment),lp=n(),De=i("div"),u(rl.$$.fragment),gT=n(),ug=i("p"),ug.innerHTML=bw,hT=n(),pg=i("p"),pg.innerHTML=Cw,uT=n(),vr=i("div"),u(nl.$$.fragment),pT=n(),_g=i("p"),_g.textContent=Tw,_T=n(),vg=i("p"),vg.innerHTML=Fw,vT=n(),u(Va.$$.fragment),MT=n(),te=i("div"),u(al.$$.fragment),bT=n(),Mg=i("p"),Mg.textContent=yw,CT=n(),bg=i("p"),bg.innerHTML=ww,TT=n(),Cg=i("ul"),Cg.innerHTML=Lw,FT=n(),Tg=i("p"),Tg.innerHTML=kw,yT=n(),u(Aa.$$.fragment),dp=n(),u(sl.$$.fragment),mp=n(),tl=i("div"),u(il.$$.fragment),cp=n(),u(ll.$$.fragment),fp=n(),dl=i("div"),u(ml.$$.fragment),gp=n(),u(cl.$$.fragment),hp=n(),Ue=i("div"),u(fl.$$.fragment),wT=n(),Fg=i("p"),Fg.innerHTML=xw,LT=n(),yg=i("p"),yg.innerHTML=Pw,kT=n(),Mr=i("div"),u(gl.$$.fragment),xT=n(),wg=i("p"),wg.textContent=Gw,PT=n(),Lg=i("p"),Lg.innerHTML=Vw,GT=n(),u(Sa.$$.fragment),VT=n(),ie=i("div"),u(hl.$$.fragment),AT=n(),kg=i("p"),kg.textContent=Aw,ST=n(),xg=i("p"),xg.innerHTML=Sw,BT=n(),Pg=i("ul"),Pg.innerHTML=Bw,$T=n(),Gg=i("p"),Gg.innerHTML=$w,IT=n(),u(Ba.$$.fragment),up=n(),u(ul.$$.fragment),pp=n(),pl=i("p"),pl.textContent=Iw,_p=n(),u(_l.$$.fragment),vp=n(),qe=i("div"),u(vl.$$.fragment),jT=n(),Vg=i("p"),Vg.innerHTML=jw,ET=n(),Ag=i("p"),Ag.innerHTML=Ew,ZT=n(),br=i("div"),u(Ml.$$.fragment),RT=n(),Sg=i("p"),Sg.textContent=Zw,WT=n(),Bg=i("p"),Bg.innerHTML=Rw,NT=n(),u($a.$$.fragment),JT=n(),le=i("div"),u(bl.$$.fragment),DT=n(),$g=i("p"),$g.textContent=Ww,UT=n(),Ig=i("p"),Ig.innerHTML=Nw,qT=n(),jg=i("ul"),jg.innerHTML=Jw,zT=n(),Eg=i("p"),Eg.innerHTML=Dw,XT=n(),u(Ia.$$.fragment),Mp=n(),u(Cl.$$.fragment),bp=n(),ze=i("div"),u(Tl.$$.fragment),QT=n(),Zg=i("p"),Zg.innerHTML=Uw,HT=n(),Rg=i("p"),Rg.innerHTML=qw,YT=n(),Cr=i("div"),u(Fl.$$.fragment),OT=n(),Wg=i("p"),Wg.textContent=zw,KT=n(),Ng=i("p"),Ng.innerHTML=Xw,e5=n(),u(ja.$$.fragment),o5=n(),de=i("div"),u(yl.$$.fragment),r5=n(),Jg=i("p"),Jg.textContent=Qw,n5=n(),Dg=i("p"),Dg.innerHTML=Hw,a5=n(),Ug=i("ul"),Ug.innerHTML=Yw,s5=n(),qg=i("p"),qg.innerHTML=Ow,t5=n(),u(Ea.$$.fragment),Cp=n(),u(wl.$$.fragment),Tp=n(),Xe=i("div"),u(Ll.$$.fragment),i5=n(),zg=i("p"),zg.innerHTML=Kw,l5=n(),Xg=i("p"),Xg.innerHTML=eL,d5=n(),Tr=i("div"),u(kl.$$.fragment),m5=n(),Qg=i("p"),Qg.textContent=oL,c5=n(),Hg=i("p"),Hg.innerHTML=rL,f5=n(),u(Za.$$.fragment),g5=n(),me=i("div"),u(xl.$$.fragment),h5=n(),Yg=i("p"),Yg.textContent=nL,u5=n(),Og=i("p"),Og.innerHTML=aL,p5=n(),Kg=i("ul"),Kg.innerHTML=sL,_5=n(),eh=i("p"),eh.innerHTML=tL,v5=n(),u(Ra.$$.fragment),Fp=n(),u(Pl.$$.fragment),yp=n(),Gl=i("div"),u(Vl.$$.fragment),wp=n(),u(Al.$$.fragment),Lp=n(),Qe=i("div"),u(Sl.$$.fragment),M5=n(),oh=i("p"),oh.innerHTML=iL,b5=n(),rh=i("p"),rh.innerHTML=lL,C5=n(),Fr=i("div"),u(Bl.$$.fragment),T5=n(),nh=i("p"),nh.textContent=dL,F5=n(),ah=i("p"),ah.innerHTML=mL,y5=n(),u(Wa.$$.fragment),w5=n(),ce=i("div"),u($l.$$.fragment),L5=n(),sh=i("p"),sh.textContent=cL,k5=n(),th=i("p"),th.innerHTML=fL,x5=n(),ih=i("ul"),ih.innerHTML=gL,P5=n(),lh=i("p"),lh.innerHTML=hL,G5=n(),u(Na.$$.fragment),kp=n(),u(Il.$$.fragment),xp=n(),u(jl.$$.fragment),Pp=n(),He=i("div"),u(El.$$.fragment),V5=n(),dh=i("p"),dh.innerHTML=uL,A5=n(),mh=i("p"),mh.innerHTML=pL,S5=n(),yr=i("div"),u(Zl.$$.fragment),B5=n(),ch=i("p"),ch.textContent=_L,$5=n(),fh=i("p"),fh.innerHTML=vL,I5=n(),u(Ja.$$.fragment),j5=n(),fe=i("div"),u(Rl.$$.fragment),E5=n(),gh=i("p"),gh.textContent=ML,Z5=n(),hh=i("p"),hh.innerHTML=bL,R5=n(),uh=i("ul"),uh.innerHTML=CL,W5=n(),ph=i("p"),ph.innerHTML=TL,N5=n(),u(Da.$$.fragment),Gp=n(),u(Wl.$$.fragment),Vp=n(),_h=i("p"),this.h()},l(e){const m=GL("svelte-u9bgzb",document.head);s=l(m,"META",{name:!0,content:!0}),m.forEach(d),T=a(e),c=l(e,"P",{}),y(c).forEach(d),t=a(e),p(g.$$.fragment,e),o=a(e),C=l(e,"P",{"data-svelte-h":!0}),f(C)!=="svelte-1sl6sct"&&(C.innerHTML=J5),yh=a(e),ts=l(e,"P",{"data-svelte-h":!0}),f(ts)!=="svelte-1xxs048"&&(ts.innerHTML=D5),wh=a(e),p(is.$$.fragment,e),Lh=a(e),ls=l(e,"P",{"data-svelte-h":!0}),f(ls)!=="svelte-8ide28"&&(ls.innerHTML=U5),kh=a(e),ds=l(e,"P",{"data-svelte-h":!0}),f(ds)!=="svelte-1n4kkfz"&&(ds.innerHTML=q5),xh=a(e),p(ms.$$.fragment,e),Ph=a(e),cs=l(e,"P",{"data-svelte-h":!0}),f(cs)!=="svelte-11r6347"&&(cs.innerHTML=z5),Gh=a(e),p(fs.$$.fragment,e),Vh=a(e),gs=l(e,"P",{"data-svelte-h":!0}),f(gs)!=="svelte-18w8uiz"&&(gs.textContent=X5),Ah=a(e),p(vn.$$.fragment,e),Sh=a(e),p(hs.$$.fragment,e),Bh=a(e),ge=l(e,"DIV",{class:!0});var Co=y(ge);p(us.$$.fragment,Co),Sp=a(Co),Xl=l(Co,"P",{"data-svelte-h":!0}),f(Xl)!=="svelte-5lzmld"&&(Xl.innerHTML=Q5),Bp=a(Co),Ql=l(Co,"P",{"data-svelte-h":!0}),f(Ql)!=="svelte-1n2cphd"&&(Ql.innerHTML=H5),$p=a(Co),Mo=l(Co,"DIV",{class:!0});var To=y(Mo);p(ps.$$.fragment,To),Ip=a(To),Hl=l(To,"P",{"data-svelte-h":!0}),f(Hl)!=="svelte-102bmt5"&&(Hl.textContent=Y5),jp=a(To),Yl=l(To,"P",{"data-svelte-h":!0}),f(Yl)!=="svelte-dfwk4w"&&(Yl.innerHTML=O5),Ep=a(To),Ol=l(To,"UL",{"data-svelte-h":!0}),f(Ol)!=="svelte-a7813r"&&(Ol.innerHTML=K5),Zp=a(To),p(Mn.$$.fragment,To),To.forEach(d),Rp=a(Co),bn=l(Co,"DIV",{class:!0});var Nl=y(bn);p(_s.$$.fragment,Nl),Wp=a(Nl),Kl=l(Nl,"P",{"data-svelte-h":!0}),f(Kl)!=="svelte-3x5rn5"&&(Kl.textContent=e4),Nl.forEach(d),Co.forEach(d),$h=a(e),p(vs.$$.fragment,e),Ih=a(e),he=l(e,"DIV",{class:!0});var Fo=y(he);p(Ms.$$.fragment,Fo),Np=a(Fo),ed=l(Fo,"P",{"data-svelte-h":!0}),f(ed)!=="svelte-ea0t8k"&&(ed.innerHTML=o4),Jp=a(Fo),od=l(Fo,"P",{"data-svelte-h":!0}),f(od)!=="svelte-1n2cphd"&&(od.innerHTML=r4),Dp=a(Fo),bo=l(Fo,"DIV",{class:!0});var yo=y(bo);p(bs.$$.fragment,yo),Up=a(yo),rd=l(yo,"P",{"data-svelte-h":!0}),f(rd)!=="svelte-1cgdf6a"&&(rd.textContent=n4),qp=a(yo),nd=l(yo,"P",{"data-svelte-h":!0}),f(nd)!=="svelte-yjv30u"&&(nd.innerHTML=a4),zp=a(yo),ad=l(yo,"UL",{"data-svelte-h":!0}),f(ad)!=="svelte-1fju09s"&&(ad.innerHTML=s4),Xp=a(yo),p(Cn.$$.fragment,yo),yo.forEach(d),Qp=a(Fo),Tn=l(Fo,"DIV",{class:!0});var Jl=y(Tn);p(Cs.$$.fragment,Jl),Hp=a(Jl),sd=l(Jl,"P",{"data-svelte-h":!0}),f(sd)!=="svelte-1dj9ip8"&&(sd.textContent=t4),Jl.forEach(d),Fo.forEach(d),jh=a(e),p(Ts.$$.fragment,e),Eh=a(e),ue=l(e,"DIV",{class:!0});var wo=y(ue);p(Fs.$$.fragment,wo),Yp=a(wo),td=l(wo,"P",{"data-svelte-h":!0}),f(td)!=="svelte-1mon8h8"&&(td.innerHTML=i4),Op=a(wo),id=l(wo,"P",{"data-svelte-h":!0}),f(id)!=="svelte-1n2cphd"&&(id.innerHTML=l4),Kp=a(wo),V=l(wo,"DIV",{class:!0});var Ye=y(V);p(ys.$$.fragment,Ye),e_=a(Ye),ld=l(Ye,"P",{"data-svelte-h":!0}),f(ld)!=="svelte-brndlr"&&(ld.textContent=d4),o_=a(Ye),dd=l(Ye,"P",{"data-svelte-h":!0}),f(dd)!=="svelte-8hskw5"&&(dd.innerHTML=m4),r_=a(Ye),md=l(Ye,"UL",{"data-svelte-h":!0}),f(md)!=="svelte-br9yas"&&(md.innerHTML=c4),n_=a(Ye),p(Fn.$$.fragment,Ye),a_=a(Ye),p(yn.$$.fragment,Ye),Ye.forEach(d),s_=a(wo),wn=l(wo,"DIV",{class:!0});var Dl=y(wn);p(ws.$$.fragment,Dl),t_=a(Dl),cd=l(Dl,"P",{"data-svelte-h":!0}),f(cd)!=="svelte-1a46i61"&&(cd.textContent=f4),Dl.forEach(d),wo.forEach(d),Zh=a(e),p(Ls.$$.fragment,e),Rh=a(e),pe=l(e,"DIV",{class:!0});var Lo=y(pe);p(ks.$$.fragment,Lo),i_=a(Lo),fd=l(Lo,"P",{"data-svelte-h":!0}),f(fd)!=="svelte-9737k6"&&(fd.innerHTML=g4),l_=a(Lo),gd=l(Lo,"P",{"data-svelte-h":!0}),f(gd)!=="svelte-1n2cphd"&&(gd.innerHTML=h4),d_=a(Lo),A=l(Lo,"DIV",{class:!0});var Oe=y(A);p(xs.$$.fragment,Oe),m_=a(Oe),hd=l(Oe,"P",{"data-svelte-h":!0}),f(hd)!=="svelte-cyiy9y"&&(hd.textContent=u4),c_=a(Oe),ud=l(Oe,"P",{"data-svelte-h":!0}),f(ud)!=="svelte-1vjaq5m"&&(ud.innerHTML=p4),f_=a(Oe),pd=l(Oe,"UL",{"data-svelte-h":!0}),f(pd)!=="svelte-1k5k43b"&&(pd.innerHTML=_4),g_=a(Oe),p(Ln.$$.fragment,Oe),h_=a(Oe),p(kn.$$.fragment,Oe),Oe.forEach(d),u_=a(Lo),xn=l(Lo,"DIV",{class:!0});var Ul=y(xn);p(Ps.$$.fragment,Ul),p_=a(Ul),_d=l(Ul,"P",{"data-svelte-h":!0}),f(_d)!=="svelte-8qw1k0"&&(_d.textContent=v4),Ul.forEach(d),Lo.forEach(d),Wh=a(e),p(Gs.$$.fragment,e),Nh=a(e),_e=l(e,"DIV",{class:!0});var ko=y(_e);p(Vs.$$.fragment,ko),__=a(ko),vd=l(ko,"P",{"data-svelte-h":!0}),f(vd)!=="svelte-1537qnm"&&(vd.innerHTML=M4),v_=a(ko),Md=l(ko,"P",{"data-svelte-h":!0}),f(Md)!=="svelte-1n2cphd"&&(Md.innerHTML=b4),M_=a(ko),S=l(ko,"DIV",{class:!0});var Ke=y(S);p(As.$$.fragment,Ke),b_=a(Ke),bd=l(Ke,"P",{"data-svelte-h":!0}),f(bd)!=="svelte-i9bvr4"&&(bd.textContent=C4),C_=a(Ke),Cd=l(Ke,"P",{"data-svelte-h":!0}),f(Cd)!=="svelte-14spfh8"&&(Cd.innerHTML=T4),T_=a(Ke),Td=l(Ke,"UL",{"data-svelte-h":!0}),f(Td)!=="svelte-1q215st"&&(Td.innerHTML=F4),F_=a(Ke),p(Pn.$$.fragment,Ke),y_=a(Ke),p(Gn.$$.fragment,Ke),Ke.forEach(d),w_=a(ko),Vn=l(ko,"DIV",{class:!0});var ql=y(Vn);p(Ss.$$.fragment,ql),L_=a(ql),Fd=l(ql,"P",{"data-svelte-h":!0}),f(Fd)!=="svelte-w5th1i"&&(Fd.textContent=y4),ql.forEach(d),ko.forEach(d),Jh=a(e),p(Bs.$$.fragment,e),Dh=a(e),ve=l(e,"DIV",{class:!0});var xo=y(ve);p($s.$$.fragment,xo),k_=a(xo),yd=l(xo,"P",{"data-svelte-h":!0}),f(yd)!=="svelte-tbb9ag"&&(yd.innerHTML=w4),x_=a(xo),wd=l(xo,"P",{"data-svelte-h":!0}),f(wd)!=="svelte-1n2cphd"&&(wd.innerHTML=L4),P_=a(xo),B=l(xo,"DIV",{class:!0});var eo=y(B);p(Is.$$.fragment,eo),G_=a(eo),Ld=l(eo,"P",{"data-svelte-h":!0}),f(Ld)!=="svelte-ampft1"&&(Ld.textContent=k4),V_=a(eo),kd=l(eo,"P",{"data-svelte-h":!0}),f(kd)!=="svelte-1lz4kgk"&&(kd.innerHTML=x4),A_=a(eo),xd=l(eo,"UL",{"data-svelte-h":!0}),f(xd)!=="svelte-hajrwy"&&(xd.innerHTML=P4),S_=a(eo),p(An.$$.fragment,eo),B_=a(eo),p(Sn.$$.fragment,eo),eo.forEach(d),$_=a(xo),Bn=l(xo,"DIV",{class:!0});var zl=y(Bn);p(js.$$.fragment,zl),I_=a(zl),Pd=l(zl,"P",{"data-svelte-h":!0}),f(Pd)!=="svelte-1rfmb83"&&(Pd.textContent=G4),zl.forEach(d),xo.forEach(d),Uh=a(e),p(Es.$$.fragment,e),qh=a(e),Zs=l(e,"P",{"data-svelte-h":!0}),f(Zs)!=="svelte-waxqds"&&(Zs.textContent=V4),zh=a(e),p(Rs.$$.fragment,e),Xh=a(e),Me=l(e,"DIV",{class:!0});var Po=y(Me);p(Ws.$$.fragment,Po),j_=a(Po),Gd=l(Po,"P",{"data-svelte-h":!0}),f(Gd)!=="svelte-139wdyr"&&(Gd.innerHTML=A4),E_=a(Po),Vd=l(Po,"P",{"data-svelte-h":!0}),f(Vd)!=="svelte-1n2cphd"&&(Vd.innerHTML=S4),Z_=a(Po),zo=l(Po,"DIV",{class:!0});var Rr=y(zo);p(Ns.$$.fragment,Rr),R_=a(Rr),Ad=l(Rr,"P",{"data-svelte-h":!0}),f(Ad)!=="svelte-ws3pq1"&&(Ad.textContent=B4),W_=a(Rr),Sd=l(Rr,"P",{"data-svelte-h":!0}),f(Sd)!=="svelte-19no0kv"&&(Sd.innerHTML=$4),N_=a(Rr),p($n.$$.fragment,Rr),Rr.forEach(d),J_=a(Po),$=l(Po,"DIV",{class:!0});var oo=y($);p(Js.$$.fragment,oo),D_=a(oo),Bd=l(oo,"P",{"data-svelte-h":!0}),f(Bd)!=="svelte-mt76tr"&&(Bd.textContent=I4),U_=a(oo),$d=l(oo,"P",{"data-svelte-h":!0}),f($d)!=="svelte-1enn9p2"&&($d.innerHTML=j4),q_=a(oo),Id=l(oo,"UL",{"data-svelte-h":!0}),f(Id)!=="svelte-zym2x8"&&(Id.innerHTML=E4),z_=a(oo),jd=l(oo,"P",{"data-svelte-h":!0}),f(jd)!=="svelte-tebhz6"&&(jd.innerHTML=Z4),X_=a(oo),p(In.$$.fragment,oo),oo.forEach(d),Po.forEach(d),Qh=a(e),p(Ds.$$.fragment,e),Hh=a(e),Us=l(e,"P",{"data-svelte-h":!0}),f(Us)!=="svelte-1lm34wk"&&(Us.textContent=R4),Yh=a(e),p(qs.$$.fragment,e),Oh=a(e),be=l(e,"DIV",{class:!0});var Go=y(be);p(zs.$$.fragment,Go),Q_=a(Go),Ed=l(Go,"P",{"data-svelte-h":!0}),f(Ed)!=="svelte-f6dyw9"&&(Ed.innerHTML=W4),H_=a(Go),Zd=l(Go,"P",{"data-svelte-h":!0}),f(Zd)!=="svelte-1n2cphd"&&(Zd.innerHTML=N4),Y_=a(Go),Xo=l(Go,"DIV",{class:!0});var Wr=y(Xo);p(Xs.$$.fragment,Wr),O_=a(Wr),Rd=l(Wr,"P",{"data-svelte-h":!0}),f(Rd)!=="svelte-qeb48j"&&(Rd.textContent=J4),K_=a(Wr),Wd=l(Wr,"P",{"data-svelte-h":!0}),f(Wd)!=="svelte-19no0kv"&&(Wd.innerHTML=D4),ev=a(Wr),p(jn.$$.fragment,Wr),Wr.forEach(d),ov=a(Go),I=l(Go,"DIV",{class:!0});var ro=y(I);p(Qs.$$.fragment,ro),rv=a(ro),Nd=l(ro,"P",{"data-svelte-h":!0}),f(Nd)!=="svelte-141b7sb"&&(Nd.textContent=U4),nv=a(ro),Jd=l(ro,"P",{"data-svelte-h":!0}),f(Jd)!=="svelte-1enn9p2"&&(Jd.innerHTML=q4),av=a(ro),Dd=l(ro,"UL",{"data-svelte-h":!0}),f(Dd)!=="svelte-1pdyofl"&&(Dd.innerHTML=z4),sv=a(ro),Ud=l(ro,"P",{"data-svelte-h":!0}),f(Ud)!=="svelte-tebhz6"&&(Ud.innerHTML=X4),tv=a(ro),p(En.$$.fragment,ro),ro.forEach(d),Go.forEach(d),Kh=a(e),p(Hs.$$.fragment,e),eu=a(e),Ys=l(e,"P",{"data-svelte-h":!0}),f(Ys)!=="svelte-pqqo9r"&&(Ys.textContent=Q4),ou=a(e),p(Os.$$.fragment,e),ru=a(e),Ce=l(e,"DIV",{class:!0});var Vo=y(Ce);p(Ks.$$.fragment,Vo),iv=a(Vo),qd=l(Vo,"P",{"data-svelte-h":!0}),f(qd)!=="svelte-1hmmwt8"&&(qd.innerHTML=H4),lv=a(Vo),zd=l(Vo,"P",{"data-svelte-h":!0}),f(zd)!=="svelte-1n2cphd"&&(zd.innerHTML=Y4),dv=a(Vo),Qo=l(Vo,"DIV",{class:!0});var Nr=y(Qo);p(et.$$.fragment,Nr),mv=a(Nr),Xd=l(Nr,"P",{"data-svelte-h":!0}),f(Xd)!=="svelte-6spxaa"&&(Xd.textContent=O4),cv=a(Nr),Qd=l(Nr,"P",{"data-svelte-h":!0}),f(Qd)!=="svelte-19no0kv"&&(Qd.innerHTML=K4),fv=a(Nr),p(Zn.$$.fragment,Nr),Nr.forEach(d),gv=a(Vo),j=l(Vo,"DIV",{class:!0});var no=y(j);p(ot.$$.fragment,no),hv=a(no),Hd=l(no,"P",{"data-svelte-h":!0}),f(Hd)!=="svelte-1b19um"&&(Hd.textContent=eF),uv=a(no),Yd=l(no,"P",{"data-svelte-h":!0}),f(Yd)!=="svelte-1enn9p2"&&(Yd.innerHTML=oF),pv=a(no),Od=l(no,"UL",{"data-svelte-h":!0}),f(Od)!=="svelte-otxqvb"&&(Od.innerHTML=rF),_v=a(no),Kd=l(no,"P",{"data-svelte-h":!0}),f(Kd)!=="svelte-tebhz6"&&(Kd.innerHTML=nF),vv=a(no),p(Rn.$$.fragment,no),no.forEach(d),Vo.forEach(d),nu=a(e),p(rt.$$.fragment,e),au=a(e),Te=l(e,"DIV",{class:!0});var Ao=y(Te);p(nt.$$.fragment,Ao),Mv=a(Ao),em=l(Ao,"P",{"data-svelte-h":!0}),f(em)!=="svelte-15arnqk"&&(em.innerHTML=aF),bv=a(Ao),om=l(Ao,"P",{"data-svelte-h":!0}),f(om)!=="svelte-1n2cphd"&&(om.innerHTML=sF),Cv=a(Ao),Ho=l(Ao,"DIV",{class:!0});var Jr=y(Ho);p(at.$$.fragment,Jr),Tv=a(Jr),rm=l(Jr,"P",{"data-svelte-h":!0}),f(rm)!=="svelte-1l4n5m2"&&(rm.textContent=tF),Fv=a(Jr),nm=l(Jr,"P",{"data-svelte-h":!0}),f(nm)!=="svelte-19no0kv"&&(nm.innerHTML=iF),yv=a(Jr),p(Wn.$$.fragment,Jr),Jr.forEach(d),wv=a(Ao),E=l(Ao,"DIV",{class:!0});var ao=y(E);p(st.$$.fragment,ao),Lv=a(ao),am=l(ao,"P",{"data-svelte-h":!0}),f(am)!=="svelte-1hs4l6u"&&(am.textContent=lF),kv=a(ao),sm=l(ao,"P",{"data-svelte-h":!0}),f(sm)!=="svelte-1enn9p2"&&(sm.innerHTML=dF),xv=a(ao),tm=l(ao,"UL",{"data-svelte-h":!0}),f(tm)!=="svelte-vdizre"&&(tm.innerHTML=mF),Pv=a(ao),im=l(ao,"P",{"data-svelte-h":!0}),f(im)!=="svelte-tebhz6"&&(im.innerHTML=cF),Gv=a(ao),p(Nn.$$.fragment,ao),ao.forEach(d),Ao.forEach(d),su=a(e),p(tt.$$.fragment,e),tu=a(e),it=l(e,"DIV",{class:!0});var vh=y(it);p(lt.$$.fragment,vh),vh.forEach(d),iu=a(e),p(dt.$$.fragment,e),lu=a(e),Fe=l(e,"DIV",{class:!0});var So=y(Fe);p(mt.$$.fragment,So),Vv=a(So),lm=l(So,"P",{"data-svelte-h":!0}),f(lm)!=="svelte-8quf00"&&(lm.innerHTML=fF),Av=a(So),dm=l(So,"P",{"data-svelte-h":!0}),f(dm)!=="svelte-1n2cphd"&&(dm.innerHTML=gF),Sv=a(So),Yo=l(So,"DIV",{class:!0});var Dr=y(Yo);p(ct.$$.fragment,Dr),Bv=a(Dr),mm=l(Dr,"P",{"data-svelte-h":!0}),f(mm)!=="svelte-1d0i3aa"&&(mm.textContent=hF),$v=a(Dr),cm=l(Dr,"P",{"data-svelte-h":!0}),f(cm)!=="svelte-19no0kv"&&(cm.innerHTML=uF),Iv=a(Dr),p(Jn.$$.fragment,Dr),Dr.forEach(d),jv=a(So),Z=l(So,"DIV",{class:!0});var so=y(Z);p(ft.$$.fragment,so),Ev=a(so),fm=l(so,"P",{"data-svelte-h":!0}),f(fm)!=="svelte-196fe96"&&(fm.textContent=pF),Zv=a(so),gm=l(so,"P",{"data-svelte-h":!0}),f(gm)!=="svelte-1enn9p2"&&(gm.innerHTML=_F),Rv=a(so),hm=l(so,"UL",{"data-svelte-h":!0}),f(hm)!=="svelte-bw55e1"&&(hm.innerHTML=vF),Wv=a(so),um=l(so,"P",{"data-svelte-h":!0}),f(um)!=="svelte-tebhz6"&&(um.innerHTML=MF),Nv=a(so),p(Dn.$$.fragment,so),so.forEach(d),So.forEach(d),du=a(e),p(gt.$$.fragment,e),mu=a(e),ye=l(e,"DIV",{class:!0});var Bo=y(ye);p(ht.$$.fragment,Bo),Jv=a(Bo),pm=l(Bo,"P",{"data-svelte-h":!0}),f(pm)!=="svelte-16rkwkh"&&(pm.innerHTML=bF),Dv=a(Bo),_m=l(Bo,"P",{"data-svelte-h":!0}),f(_m)!=="svelte-1n2cphd"&&(_m.innerHTML=CF),Uv=a(Bo),Oo=l(Bo,"DIV",{class:!0});var Ur=y(Oo);p(ut.$$.fragment,Ur),qv=a(Ur),vm=l(Ur,"P",{"data-svelte-h":!0}),f(vm)!=="svelte-6xp6fj"&&(vm.textContent=TF),zv=a(Ur),Mm=l(Ur,"P",{"data-svelte-h":!0}),f(Mm)!=="svelte-19no0kv"&&(Mm.innerHTML=FF),Xv=a(Ur),p(Un.$$.fragment,Ur),Ur.forEach(d),Qv=a(Bo),R=l(Bo,"DIV",{class:!0});var to=y(R);p(pt.$$.fragment,to),Hv=a(to),bm=l(to,"P",{"data-svelte-h":!0}),f(bm)!=="svelte-10e8w47"&&(bm.textContent=yF),Yv=a(to),Cm=l(to,"P",{"data-svelte-h":!0}),f(Cm)!=="svelte-1enn9p2"&&(Cm.innerHTML=wF),Ov=a(to),Tm=l(to,"UL",{"data-svelte-h":!0}),f(Tm)!=="svelte-12mdbn8"&&(Tm.innerHTML=LF),Kv=a(to),Fm=l(to,"P",{"data-svelte-h":!0}),f(Fm)!=="svelte-tebhz6"&&(Fm.innerHTML=kF),e2=a(to),p(qn.$$.fragment,to),to.forEach(d),Bo.forEach(d),cu=a(e),p(_t.$$.fragment,e),fu=a(e),we=l(e,"DIV",{class:!0});var $o=y(we);p(vt.$$.fragment,$o),o2=a($o),ym=l($o,"P",{"data-svelte-h":!0}),f(ym)!=="svelte-1gr76el"&&(ym.innerHTML=xF),r2=a($o),wm=l($o,"P",{"data-svelte-h":!0}),f(wm)!=="svelte-1n2cphd"&&(wm.innerHTML=PF),n2=a($o),Ko=l($o,"DIV",{class:!0});var qr=y(Ko);p(Mt.$$.fragment,qr),a2=a(qr),Lm=l(qr,"P",{"data-svelte-h":!0}),f(Lm)!=="svelte-snlhsn"&&(Lm.textContent=GF),s2=a(qr),km=l(qr,"P",{"data-svelte-h":!0}),f(km)!=="svelte-19no0kv"&&(km.innerHTML=VF),t2=a(qr),p(zn.$$.fragment,qr),qr.forEach(d),i2=a($o),W=l($o,"DIV",{class:!0});var io=y(W);p(bt.$$.fragment,io),l2=a(io),xm=l(io,"P",{"data-svelte-h":!0}),f(xm)!=="svelte-jtqyi7"&&(xm.textContent=AF),d2=a(io),Pm=l(io,"P",{"data-svelte-h":!0}),f(Pm)!=="svelte-1enn9p2"&&(Pm.innerHTML=SF),m2=a(io),Gm=l(io,"UL",{"data-svelte-h":!0}),f(Gm)!=="svelte-h23tl1"&&(Gm.innerHTML=BF),c2=a(io),Vm=l(io,"P",{"data-svelte-h":!0}),f(Vm)!=="svelte-tebhz6"&&(Vm.innerHTML=$F),f2=a(io),p(Xn.$$.fragment,io),io.forEach(d),$o.forEach(d),gu=a(e),p(Ct.$$.fragment,e),hu=a(e),Le=l(e,"DIV",{class:!0});var Io=y(Le);p(Tt.$$.fragment,Io),g2=a(Io),Am=l(Io,"P",{"data-svelte-h":!0}),f(Am)!=="svelte-6p72h3"&&(Am.innerHTML=IF),h2=a(Io),Sm=l(Io,"P",{"data-svelte-h":!0}),f(Sm)!=="svelte-1n2cphd"&&(Sm.innerHTML=jF),u2=a(Io),er=l(Io,"DIV",{class:!0});var zr=y(er);p(Ft.$$.fragment,zr),p2=a(zr),Bm=l(zr,"P",{"data-svelte-h":!0}),f(Bm)!=="svelte-1yu679h"&&(Bm.textContent=EF),_2=a(zr),$m=l(zr,"P",{"data-svelte-h":!0}),f($m)!=="svelte-19no0kv"&&($m.innerHTML=ZF),v2=a(zr),p(Qn.$$.fragment,zr),zr.forEach(d),M2=a(Io),N=l(Io,"DIV",{class:!0});var lo=y(N);p(yt.$$.fragment,lo),b2=a(lo),Im=l(lo,"P",{"data-svelte-h":!0}),f(Im)!=="svelte-hx8awn"&&(Im.textContent=RF),C2=a(lo),jm=l(lo,"P",{"data-svelte-h":!0}),f(jm)!=="svelte-1enn9p2"&&(jm.innerHTML=WF),T2=a(lo),Em=l(lo,"UL",{"data-svelte-h":!0}),f(Em)!=="svelte-1cyuudv"&&(Em.innerHTML=NF),F2=a(lo),Zm=l(lo,"P",{"data-svelte-h":!0}),f(Zm)!=="svelte-tebhz6"&&(Zm.innerHTML=JF),y2=a(lo),p(Hn.$$.fragment,lo),lo.forEach(d),Io.forEach(d),uu=a(e),p(wt.$$.fragment,e),pu=a(e),ke=l(e,"DIV",{class:!0});var jo=y(ke);p(Lt.$$.fragment,jo),w2=a(jo),Rm=l(jo,"P",{"data-svelte-h":!0}),f(Rm)!=="svelte-fnb2g3"&&(Rm.innerHTML=DF),L2=a(jo),Wm=l(jo,"P",{"data-svelte-h":!0}),f(Wm)!=="svelte-1n2cphd"&&(Wm.innerHTML=UF),k2=a(jo),or=l(jo,"DIV",{class:!0});var Xr=y(or);p(kt.$$.fragment,Xr),x2=a(Xr),Nm=l(Xr,"P",{"data-svelte-h":!0}),f(Nm)!=="svelte-81lf85"&&(Nm.textContent=qF),P2=a(Xr),Jm=l(Xr,"P",{"data-svelte-h":!0}),f(Jm)!=="svelte-19no0kv"&&(Jm.innerHTML=zF),G2=a(Xr),p(Yn.$$.fragment,Xr),Xr.forEach(d),V2=a(jo),J=l(jo,"DIV",{class:!0});var mo=y(J);p(xt.$$.fragment,mo),A2=a(mo),Dm=l(mo,"P",{"data-svelte-h":!0}),f(Dm)!=="svelte-16vvmzn"&&(Dm.textContent=XF),S2=a(mo),Um=l(mo,"P",{"data-svelte-h":!0}),f(Um)!=="svelte-1enn9p2"&&(Um.innerHTML=QF),B2=a(mo),qm=l(mo,"UL",{"data-svelte-h":!0}),f(qm)!=="svelte-1mfbano"&&(qm.innerHTML=HF),$2=a(mo),zm=l(mo,"P",{"data-svelte-h":!0}),f(zm)!=="svelte-tebhz6"&&(zm.innerHTML=YF),I2=a(mo),p(On.$$.fragment,mo),mo.forEach(d),jo.forEach(d),_u=a(e),p(Pt.$$.fragment,e),vu=a(e),xe=l(e,"DIV",{class:!0});var Eo=y(xe);p(Gt.$$.fragment,Eo),j2=a(Eo),Xm=l(Eo,"P",{"data-svelte-h":!0}),f(Xm)!=="svelte-1iqtw42"&&(Xm.innerHTML=OF),E2=a(Eo),Qm=l(Eo,"P",{"data-svelte-h":!0}),f(Qm)!=="svelte-1n2cphd"&&(Qm.innerHTML=KF),Z2=a(Eo),rr=l(Eo,"DIV",{class:!0});var Qr=y(rr);p(Vt.$$.fragment,Qr),R2=a(Qr),Hm=l(Qr,"P",{"data-svelte-h":!0}),f(Hm)!=="svelte-n52yuc"&&(Hm.textContent=ey),W2=a(Qr),Ym=l(Qr,"P",{"data-svelte-h":!0}),f(Ym)!=="svelte-19no0kv"&&(Ym.innerHTML=oy),N2=a(Qr),p(Kn.$$.fragment,Qr),Qr.forEach(d),J2=a(Eo),D=l(Eo,"DIV",{class:!0});var co=y(D);p(At.$$.fragment,co),D2=a(co),Om=l(co,"P",{"data-svelte-h":!0}),f(Om)!=="svelte-1h7oepk"&&(Om.textContent=ry),U2=a(co),Km=l(co,"P",{"data-svelte-h":!0}),f(Km)!=="svelte-1enn9p2"&&(Km.innerHTML=ny),q2=a(co),ec=l(co,"UL",{"data-svelte-h":!0}),f(ec)!=="svelte-16usc65"&&(ec.innerHTML=ay),z2=a(co),oc=l(co,"P",{"data-svelte-h":!0}),f(oc)!=="svelte-tebhz6"&&(oc.innerHTML=sy),X2=a(co),p(ea.$$.fragment,co),co.forEach(d),Eo.forEach(d),Mu=a(e),p(St.$$.fragment,e),bu=a(e),Bt=l(e,"DIV",{class:!0});var Mh=y(Bt);p($t.$$.fragment,Mh),Mh.forEach(d),Cu=a(e),p(It.$$.fragment,e),Tu=a(e),jt=l(e,"P",{"data-svelte-h":!0}),f(jt)!=="svelte-1a2lih2"&&(jt.textContent=ty),Fu=a(e),p(Et.$$.fragment,e),yu=a(e),Pe=l(e,"DIV",{class:!0});var Zo=y(Pe);p(Zt.$$.fragment,Zo),Q2=a(Zo),rc=l(Zo,"P",{"data-svelte-h":!0}),f(rc)!=="svelte-16g23gg"&&(rc.innerHTML=iy),H2=a(Zo),nc=l(Zo,"P",{"data-svelte-h":!0}),f(nc)!=="svelte-1n2cphd"&&(nc.innerHTML=ly),Y2=a(Zo),nr=l(Zo,"DIV",{class:!0});var Hr=y(nr);p(Rt.$$.fragment,Hr),O2=a(Hr),ac=l(Hr,"P",{"data-svelte-h":!0}),f(ac)!=="svelte-1o4g80a"&&(ac.textContent=dy),K2=a(Hr),sc=l(Hr,"P",{"data-svelte-h":!0}),f(sc)!=="svelte-19no0kv"&&(sc.innerHTML=my),eM=a(Hr),p(oa.$$.fragment,Hr),Hr.forEach(d),oM=a(Zo),U=l(Zo,"DIV",{class:!0});var fo=y(U);p(Wt.$$.fragment,fo),rM=a(fo),tc=l(fo,"P",{"data-svelte-h":!0}),f(tc)!=="svelte-100to3e"&&(tc.textContent=cy),nM=a(fo),ic=l(fo,"P",{"data-svelte-h":!0}),f(ic)!=="svelte-1enn9p2"&&(ic.innerHTML=fy),aM=a(fo),lc=l(fo,"UL",{"data-svelte-h":!0}),f(lc)!=="svelte-1gnj270"&&(lc.innerHTML=gy),sM=a(fo),dc=l(fo,"P",{"data-svelte-h":!0}),f(dc)!=="svelte-tebhz6"&&(dc.innerHTML=hy),tM=a(fo),p(ra.$$.fragment,fo),fo.forEach(d),Zo.forEach(d),wu=a(e),p(Nt.$$.fragment,e),Lu=a(e),Ge=l(e,"DIV",{class:!0});var Ro=y(Ge);p(Jt.$$.fragment,Ro),iM=a(Ro),mc=l(Ro,"P",{"data-svelte-h":!0}),f(mc)!=="svelte-wgybrf"&&(mc.innerHTML=uy),lM=a(Ro),cc=l(Ro,"P",{"data-svelte-h":!0}),f(cc)!=="svelte-1n2cphd"&&(cc.innerHTML=py),dM=a(Ro),ar=l(Ro,"DIV",{class:!0});var Yr=y(ar);p(Dt.$$.fragment,Yr),mM=a(Yr),fc=l(Yr,"P",{"data-svelte-h":!0}),f(fc)!=="svelte-3mnohx"&&(fc.textContent=_y),cM=a(Yr),gc=l(Yr,"P",{"data-svelte-h":!0}),f(gc)!=="svelte-19no0kv"&&(gc.innerHTML=vy),fM=a(Yr),p(na.$$.fragment,Yr),Yr.forEach(d),gM=a(Ro),q=l(Ro,"DIV",{class:!0});var go=y(q);p(Ut.$$.fragment,go),hM=a(go),hc=l(go,"P",{"data-svelte-h":!0}),f(hc)!=="svelte-1ezjudv"&&(hc.textContent=My),uM=a(go),uc=l(go,"P",{"data-svelte-h":!0}),f(uc)!=="svelte-1enn9p2"&&(uc.innerHTML=by),pM=a(go),pc=l(go,"UL",{"data-svelte-h":!0}),f(pc)!=="svelte-1c68c4x"&&(pc.innerHTML=Cy),_M=a(go),_c=l(go,"P",{"data-svelte-h":!0}),f(_c)!=="svelte-tebhz6"&&(_c.innerHTML=Ty),vM=a(go),p(aa.$$.fragment,go),go.forEach(d),Ro.forEach(d),ku=a(e),p(qt.$$.fragment,e),xu=a(e),Ve=l(e,"DIV",{class:!0});var Wo=y(Ve);p(zt.$$.fragment,Wo),MM=a(Wo),vc=l(Wo,"P",{"data-svelte-h":!0}),f(vc)!=="svelte-1jbawh"&&(vc.innerHTML=Fy),bM=a(Wo),Mc=l(Wo,"P",{"data-svelte-h":!0}),f(Mc)!=="svelte-1n2cphd"&&(Mc.innerHTML=yy),CM=a(Wo),sr=l(Wo,"DIV",{class:!0});var Or=y(sr);p(Xt.$$.fragment,Or),TM=a(Or),bc=l(Or,"P",{"data-svelte-h":!0}),f(bc)!=="svelte-1269scv"&&(bc.textContent=wy),FM=a(Or),Cc=l(Or,"P",{"data-svelte-h":!0}),f(Cc)!=="svelte-19no0kv"&&(Cc.innerHTML=Ly),yM=a(Or),p(sa.$$.fragment,Or),Or.forEach(d),wM=a(Wo),z=l(Wo,"DIV",{class:!0});var ho=y(z);p(Qt.$$.fragment,ho),LM=a(ho),Tc=l(ho,"P",{"data-svelte-h":!0}),f(Tc)!=="svelte-xeutyd"&&(Tc.textContent=ky),kM=a(ho),Fc=l(ho,"P",{"data-svelte-h":!0}),f(Fc)!=="svelte-1enn9p2"&&(Fc.innerHTML=xy),xM=a(ho),yc=l(ho,"UL",{"data-svelte-h":!0}),f(yc)!=="svelte-ix61do"&&(yc.innerHTML=Py),PM=a(ho),wc=l(ho,"P",{"data-svelte-h":!0}),f(wc)!=="svelte-tebhz6"&&(wc.innerHTML=Gy),GM=a(ho),p(ta.$$.fragment,ho),ho.forEach(d),Wo.forEach(d),Pu=a(e),p(Ht.$$.fragment,e),Gu=a(e),Yt=l(e,"DIV",{class:!0});var bh=y(Yt);p(Ot.$$.fragment,bh),bh.forEach(d),Vu=a(e),p(Kt.$$.fragment,e),Au=a(e),ei=l(e,"DIV",{class:!0});var Ch=y(ei);p(oi.$$.fragment,Ch),Ch.forEach(d),Su=a(e),p(ri.$$.fragment,e),Bu=a(e),Ae=l(e,"DIV",{class:!0});var No=y(Ae);p(ni.$$.fragment,No),VM=a(No),Lc=l(No,"P",{"data-svelte-h":!0}),f(Lc)!=="svelte-cd0dfp"&&(Lc.innerHTML=Vy),AM=a(No),kc=l(No,"P",{"data-svelte-h":!0}),f(kc)!=="svelte-1n2cphd"&&(kc.innerHTML=Ay),SM=a(No),tr=l(No,"DIV",{class:!0});var Kr=y(tr);p(ai.$$.fragment,Kr),BM=a(Kr),xc=l(Kr,"P",{"data-svelte-h":!0}),f(xc)!=="svelte-vmkzub"&&(xc.textContent=Sy),$M=a(Kr),Pc=l(Kr,"P",{"data-svelte-h":!0}),f(Pc)!=="svelte-19no0kv"&&(Pc.innerHTML=By),IM=a(Kr),p(ia.$$.fragment,Kr),Kr.forEach(d),jM=a(No),X=l(No,"DIV",{class:!0});var uo=y(X);p(si.$$.fragment,uo),EM=a(uo),Gc=l(uo,"P",{"data-svelte-h":!0}),f(Gc)!=="svelte-up2an7"&&(Gc.textContent=$y),ZM=a(uo),Vc=l(uo,"P",{"data-svelte-h":!0}),f(Vc)!=="svelte-1enn9p2"&&(Vc.innerHTML=Iy),RM=a(uo),Ac=l(uo,"UL",{"data-svelte-h":!0}),f(Ac)!=="svelte-1y5crcw"&&(Ac.innerHTML=jy),WM=a(uo),Sc=l(uo,"P",{"data-svelte-h":!0}),f(Sc)!=="svelte-tebhz6"&&(Sc.innerHTML=Ey),NM=a(uo),p(la.$$.fragment,uo),uo.forEach(d),No.forEach(d),$u=a(e),p(ti.$$.fragment,e),Iu=a(e),Se=l(e,"DIV",{class:!0});var Jo=y(Se);p(ii.$$.fragment,Jo),JM=a(Jo),Bc=l(Jo,"P",{"data-svelte-h":!0}),f(Bc)!=="svelte-1cg29hs"&&(Bc.innerHTML=Zy),DM=a(Jo),$c=l(Jo,"P",{"data-svelte-h":!0}),f($c)!=="svelte-1n2cphd"&&($c.innerHTML=Ry),UM=a(Jo),ir=l(Jo,"DIV",{class:!0});var en=y(ir);p(li.$$.fragment,en),qM=a(en),Ic=l(en,"P",{"data-svelte-h":!0}),f(Ic)!=="svelte-1dstum2"&&(Ic.textContent=Wy),zM=a(en),jc=l(en,"P",{"data-svelte-h":!0}),f(jc)!=="svelte-19no0kv"&&(jc.innerHTML=Ny),XM=a(en),p(da.$$.fragment,en),en.forEach(d),QM=a(Jo),Q=l(Jo,"DIV",{class:!0});var po=y(Q);p(di.$$.fragment,po),HM=a(po),Ec=l(po,"P",{"data-svelte-h":!0}),f(Ec)!=="svelte-7xylam"&&(Ec.textContent=Jy),YM=a(po),Zc=l(po,"P",{"data-svelte-h":!0}),f(Zc)!=="svelte-1enn9p2"&&(Zc.innerHTML=Dy),OM=a(po),Rc=l(po,"UL",{"data-svelte-h":!0}),f(Rc)!=="svelte-1g2syd3"&&(Rc.innerHTML=Uy),KM=a(po),Wc=l(po,"P",{"data-svelte-h":!0}),f(Wc)!=="svelte-tebhz6"&&(Wc.innerHTML=qy),eb=a(po),p(ma.$$.fragment,po),po.forEach(d),Jo.forEach(d),ju=a(e),p(mi.$$.fragment,e),Eu=a(e),Be=l(e,"DIV",{class:!0});var Do=y(Be);p(ci.$$.fragment,Do),ob=a(Do),Nc=l(Do,"P",{"data-svelte-h":!0}),f(Nc)!=="svelte-9dypp9"&&(Nc.innerHTML=zy),rb=a(Do),Jc=l(Do,"P",{"data-svelte-h":!0}),f(Jc)!=="svelte-1n2cphd"&&(Jc.innerHTML=Xy),nb=a(Do),lr=l(Do,"DIV",{class:!0});var on=y(lr);p(fi.$$.fragment,on),ab=a(on),Dc=l(on,"P",{"data-svelte-h":!0}),f(Dc)!=="svelte-a638l3"&&(Dc.textContent=Qy),sb=a(on),Uc=l(on,"P",{"data-svelte-h":!0}),f(Uc)!=="svelte-19no0kv"&&(Uc.innerHTML=Hy),tb=a(on),p(ca.$$.fragment,on),on.forEach(d),ib=a(Do),H=l(Do,"DIV",{class:!0});var _o=y(H);p(gi.$$.fragment,_o),lb=a(_o),qc=l(_o,"P",{"data-svelte-h":!0}),f(qc)!=="svelte-1rj5rvl"&&(qc.textContent=Yy),db=a(_o),zc=l(_o,"P",{"data-svelte-h":!0}),f(zc)!=="svelte-1enn9p2"&&(zc.innerHTML=Oy),mb=a(_o),Xc=l(_o,"UL",{"data-svelte-h":!0}),f(Xc)!=="svelte-glvi28"&&(Xc.innerHTML=Ky),cb=a(_o),Qc=l(_o,"P",{"data-svelte-h":!0}),f(Qc)!=="svelte-tebhz6"&&(Qc.innerHTML=e6),fb=a(_o),p(fa.$$.fragment,_o),_o.forEach(d),Do.forEach(d),Zu=a(e),p(hi.$$.fragment,e),Ru=a(e),ui=l(e,"DIV",{class:!0});var Th=y(ui);p(pi.$$.fragment,Th),Th.forEach(d),Wu=a(e),p(_i.$$.fragment,e),Nu=a(e),$e=l(e,"DIV",{class:!0});var Uo=y($e);p(vi.$$.fragment,Uo),gb=a(Uo),Hc=l(Uo,"P",{"data-svelte-h":!0}),f(Hc)!=="svelte-7uqugi"&&(Hc.innerHTML=o6),hb=a(Uo),Yc=l(Uo,"P",{"data-svelte-h":!0}),f(Yc)!=="svelte-1n2cphd"&&(Yc.innerHTML=r6),ub=a(Uo),dr=l(Uo,"DIV",{class:!0});var rn=y(dr);p(Mi.$$.fragment,rn),pb=a(rn),Oc=l(rn,"P",{"data-svelte-h":!0}),f(Oc)!=="svelte-ne8wj0"&&(Oc.textContent=n6),_b=a(rn),Kc=l(rn,"P",{"data-svelte-h":!0}),f(Kc)!=="svelte-19no0kv"&&(Kc.innerHTML=a6),vb=a(rn),p(ga.$$.fragment,rn),rn.forEach(d),Mb=a(Uo),Y=l(Uo,"DIV",{class:!0});var vo=y(Y);p(bi.$$.fragment,vo),bb=a(vo),ef=l(vo,"P",{"data-svelte-h":!0}),f(ef)!=="svelte-xit1mq"&&(ef.textContent=s6),Cb=a(vo),of=l(vo,"P",{"data-svelte-h":!0}),f(of)!=="svelte-1enn9p2"&&(of.innerHTML=t6),Tb=a(vo),rf=l(vo,"UL",{"data-svelte-h":!0}),f(rf)!=="svelte-9c1wgv"&&(rf.innerHTML=i6),Fb=a(vo),nf=l(vo,"P",{"data-svelte-h":!0}),f(nf)!=="svelte-tebhz6"&&(nf.innerHTML=l6),yb=a(vo),p(ha.$$.fragment,vo),vo.forEach(d),Uo.forEach(d),Ju=a(e),p(Ci.$$.fragment,e),Du=a(e),Ie=l(e,"DIV",{class:!0});var qo=y(Ie);p(Ti.$$.fragment,qo),wb=a(qo),af=l(qo,"P",{"data-svelte-h":!0}),f(af)!=="svelte-8vdd1l"&&(af.innerHTML=d6),Lb=a(qo),sf=l(qo,"P",{"data-svelte-h":!0}),f(sf)!=="svelte-1n2cphd"&&(sf.innerHTML=m6),kb=a(qo),mr=l(qo,"DIV",{class:!0});var Ua=y(mr);p(Fi.$$.fragment,Ua),xb=a(Ua),tf=l(Ua,"P",{"data-svelte-h":!0}),f(tf)!=="svelte-15xznv"&&(tf.textContent=c6),Pb=a(Ua),lf=l(Ua,"P",{"data-svelte-h":!0}),f(lf)!=="svelte-19no0kv"&&(lf.innerHTML=f6),Gb=a(Ua),p(ua.$$.fragment,Ua),Ua.forEach(d),Vb=a(qo),O=l(qo,"DIV",{class:!0});var wr=y(O);p(yi.$$.fragment,wr),Ab=a(wr),df=l(wr,"P",{"data-svelte-h":!0}),f(df)!=="svelte-1r6a2oz"&&(df.textContent=g6),Sb=a(wr),mf=l(wr,"P",{"data-svelte-h":!0}),f(mf)!=="svelte-1enn9p2"&&(mf.innerHTML=h6),Bb=a(wr),cf=l(wr,"UL",{"data-svelte-h":!0}),f(cf)!=="svelte-d5h3mk"&&(cf.innerHTML=u6),$b=a(wr),ff=l(wr,"P",{"data-svelte-h":!0}),f(ff)!=="svelte-tebhz6"&&(ff.innerHTML=p6),Ib=a(wr),p(pa.$$.fragment,wr),wr.forEach(d),qo.forEach(d),Uu=a(e),p(wi.$$.fragment,e),qu=a(e),je=l(e,"DIV",{class:!0});var nn=y(je);p(Li.$$.fragment,nn),jb=a(nn),gf=l(nn,"P",{"data-svelte-h":!0}),f(gf)!=="svelte-ttsyf6"&&(gf.innerHTML=_6),Eb=a(nn),hf=l(nn,"P",{"data-svelte-h":!0}),f(hf)!=="svelte-1n2cphd"&&(hf.innerHTML=v6),Zb=a(nn),cr=l(nn,"DIV",{class:!0});var qa=y(cr);p(ki.$$.fragment,qa),Rb=a(qa),uf=l(qa,"P",{"data-svelte-h":!0}),f(uf)!=="svelte-zddwso"&&(uf.textContent=M6),Wb=a(qa),pf=l(qa,"P",{"data-svelte-h":!0}),f(pf)!=="svelte-19no0kv"&&(pf.innerHTML=b6),Nb=a(qa),p(_a.$$.fragment,qa),qa.forEach(d),Jb=a(nn),K=l(nn,"DIV",{class:!0});var Lr=y(K);p(xi.$$.fragment,Lr),Db=a(Lr),_f=l(Lr,"P",{"data-svelte-h":!0}),f(_f)!=="svelte-1wyb2t8"&&(_f.textContent=C6),Ub=a(Lr),vf=l(Lr,"P",{"data-svelte-h":!0}),f(vf)!=="svelte-1enn9p2"&&(vf.innerHTML=T6),qb=a(Lr),Mf=l(Lr,"UL",{"data-svelte-h":!0}),f(Mf)!=="svelte-110qnbw"&&(Mf.innerHTML=F6),zb=a(Lr),bf=l(Lr,"P",{"data-svelte-h":!0}),f(bf)!=="svelte-tebhz6"&&(bf.innerHTML=y6),Xb=a(Lr),p(va.$$.fragment,Lr),Lr.forEach(d),nn.forEach(d),zu=a(e),p(Pi.$$.fragment,e),Xu=a(e),Ee=l(e,"DIV",{class:!0});var an=y(Ee);p(Gi.$$.fragment,an),Qb=a(an),Cf=l(an,"P",{"data-svelte-h":!0}),f(Cf)!=="svelte-1uly620"&&(Cf.innerHTML=w6),Hb=a(an),Tf=l(an,"P",{"data-svelte-h":!0}),f(Tf)!=="svelte-1n2cphd"&&(Tf.innerHTML=L6),Yb=a(an),fr=l(an,"DIV",{class:!0});var za=y(fr);p(Vi.$$.fragment,za),Ob=a(za),Ff=l(za,"P",{"data-svelte-h":!0}),f(Ff)!=="svelte-1xqzdzm"&&(Ff.textContent=k6),Kb=a(za),yf=l(za,"P",{"data-svelte-h":!0}),f(yf)!=="svelte-19no0kv"&&(yf.innerHTML=x6),eC=a(za),p(Ma.$$.fragment,za),za.forEach(d),oC=a(an),ee=l(an,"DIV",{class:!0});var kr=y(ee);p(Ai.$$.fragment,kr),rC=a(kr),wf=l(kr,"P",{"data-svelte-h":!0}),f(wf)!=="svelte-1kp6e9m"&&(wf.textContent=P6),nC=a(kr),Lf=l(kr,"P",{"data-svelte-h":!0}),f(Lf)!=="svelte-1enn9p2"&&(Lf.innerHTML=G6),aC=a(kr),kf=l(kr,"UL",{"data-svelte-h":!0}),f(kf)!=="svelte-cl8fvz"&&(kf.innerHTML=V6),sC=a(kr),xf=l(kr,"P",{"data-svelte-h":!0}),f(xf)!=="svelte-tebhz6"&&(xf.innerHTML=A6),tC=a(kr),p(ba.$$.fragment,kr),kr.forEach(d),an.forEach(d),Qu=a(e),p(Si.$$.fragment,e),Hu=a(e),Ze=l(e,"DIV",{class:!0});var sn=y(Ze);p(Bi.$$.fragment,sn),iC=a(sn),Pf=l(sn,"P",{"data-svelte-h":!0}),f(Pf)!=="svelte-tgt93j"&&(Pf.innerHTML=S6),lC=a(sn),Gf=l(sn,"P",{"data-svelte-h":!0}),f(Gf)!=="svelte-1n2cphd"&&(Gf.innerHTML=B6),dC=a(sn),gr=l(sn,"DIV",{class:!0});var Xa=y(gr);p($i.$$.fragment,Xa),mC=a(Xa),Vf=l(Xa,"P",{"data-svelte-h":!0}),f(Vf)!=="svelte-1t88i0p"&&(Vf.textContent=$6),cC=a(Xa),Af=l(Xa,"P",{"data-svelte-h":!0}),f(Af)!=="svelte-19no0kv"&&(Af.innerHTML=I6),fC=a(Xa),p(Ca.$$.fragment,Xa),Xa.forEach(d),gC=a(sn),oe=l(sn,"DIV",{class:!0});var xr=y(oe);p(Ii.$$.fragment,xr),hC=a(xr),Sf=l(xr,"P",{"data-svelte-h":!0}),f(Sf)!=="svelte-xtgygr"&&(Sf.textContent=j6),uC=a(xr),Bf=l(xr,"P",{"data-svelte-h":!0}),f(Bf)!=="svelte-1enn9p2"&&(Bf.innerHTML=E6),pC=a(xr),$f=l(xr,"UL",{"data-svelte-h":!0}),f($f)!=="svelte-189y9ln"&&($f.innerHTML=Z6),_C=a(xr),If=l(xr,"P",{"data-svelte-h":!0}),f(If)!=="svelte-tebhz6"&&(If.innerHTML=R6),vC=a(xr),p(Ta.$$.fragment,xr),xr.forEach(d),sn.forEach(d),Yu=a(e),p(ji.$$.fragment,e),Ou=a(e),Ei=l(e,"P",{"data-svelte-h":!0}),f(Ei)!=="svelte-1htpfc5"&&(Ei.textContent=W6),Ku=a(e),p(Zi.$$.fragment,e),ep=a(e),Re=l(e,"DIV",{class:!0});var tn=y(Re);p(Ri.$$.fragment,tn),MC=a(tn),jf=l(tn,"P",{"data-svelte-h":!0}),f(jf)!=="svelte-13fzpe2"&&(jf.innerHTML=N6),bC=a(tn),Ef=l(tn,"P",{"data-svelte-h":!0}),f(Ef)!=="svelte-1n2cphd"&&(Ef.innerHTML=J6),CC=a(tn),hr=l(tn,"DIV",{class:!0});var Qa=y(hr);p(Wi.$$.fragment,Qa),TC=a(Qa),Zf=l(Qa,"P",{"data-svelte-h":!0}),f(Zf)!=="svelte-1jtlz68"&&(Zf.textContent=D6),FC=a(Qa),Rf=l(Qa,"P",{"data-svelte-h":!0}),f(Rf)!=="svelte-19no0kv"&&(Rf.innerHTML=U6),yC=a(Qa),p(Fa.$$.fragment,Qa),Qa.forEach(d),wC=a(tn),re=l(tn,"DIV",{class:!0});var Pr=y(re);p(Ni.$$.fragment,Pr),LC=a(Pr),Wf=l(Pr,"P",{"data-svelte-h":!0}),f(Wf)!=="svelte-mj39cs"&&(Wf.textContent=q6),kC=a(Pr),Nf=l(Pr,"P",{"data-svelte-h":!0}),f(Nf)!=="svelte-1enn9p2"&&(Nf.innerHTML=z6),xC=a(Pr),Jf=l(Pr,"UL",{"data-svelte-h":!0}),f(Jf)!=="svelte-1lpqfhx"&&(Jf.innerHTML=X6),PC=a(Pr),Df=l(Pr,"P",{"data-svelte-h":!0}),f(Df)!=="svelte-tebhz6"&&(Df.innerHTML=Q6),GC=a(Pr),p(ya.$$.fragment,Pr),Pr.forEach(d),tn.forEach(d),op=a(e),p(Ji.$$.fragment,e),rp=a(e),We=l(e,"DIV",{class:!0});var ln=y(We);p(Di.$$.fragment,ln),VC=a(ln),Uf=l(ln,"P",{"data-svelte-h":!0}),f(Uf)!=="svelte-of0xsx"&&(Uf.innerHTML=H6),AC=a(ln),qf=l(ln,"P",{"data-svelte-h":!0}),f(qf)!=="svelte-1n2cphd"&&(qf.innerHTML=Y6),SC=a(ln),ur=l(ln,"DIV",{class:!0});var Ha=y(ur);p(Ui.$$.fragment,Ha),BC=a(Ha),zf=l(Ha,"P",{"data-svelte-h":!0}),f(zf)!=="svelte-qoovm7"&&(zf.textContent=O6),$C=a(Ha),Xf=l(Ha,"P",{"data-svelte-h":!0}),f(Xf)!=="svelte-19no0kv"&&(Xf.innerHTML=K6),IC=a(Ha),p(wa.$$.fragment,Ha),Ha.forEach(d),jC=a(ln),ne=l(ln,"DIV",{class:!0});var Gr=y(ne);p(qi.$$.fragment,Gr),EC=a(Gr),Qf=l(Gr,"P",{"data-svelte-h":!0}),f(Qf)!=="svelte-1ldaoop"&&(Qf.textContent=ew),ZC=a(Gr),Hf=l(Gr,"P",{"data-svelte-h":!0}),f(Hf)!=="svelte-1enn9p2"&&(Hf.innerHTML=ow),RC=a(Gr),Yf=l(Gr,"UL",{"data-svelte-h":!0}),f(Yf)!=="svelte-1s8bodu"&&(Yf.innerHTML=rw),WC=a(Gr),Of=l(Gr,"P",{"data-svelte-h":!0}),f(Of)!=="svelte-tebhz6"&&(Of.innerHTML=nw),NC=a(Gr),p(La.$$.fragment,Gr),Gr.forEach(d),ln.forEach(d),np=a(e),p(zi.$$.fragment,e),ap=a(e),Ne=l(e,"DIV",{class:!0});var dn=y(Ne);p(Xi.$$.fragment,dn),JC=a(dn),Kf=l(dn,"P",{"data-svelte-h":!0}),f(Kf)!=="svelte-unt1nq"&&(Kf.innerHTML=aw),DC=a(dn),eg=l(dn,"P",{"data-svelte-h":!0}),f(eg)!=="svelte-1n2cphd"&&(eg.innerHTML=sw),UC=a(dn),pr=l(dn,"DIV",{class:!0});var Ya=y(pr);p(Qi.$$.fragment,Ya),qC=a(Ya),og=l(Ya,"P",{"data-svelte-h":!0}),f(og)!=="svelte-ovue0k"&&(og.textContent=tw),zC=a(Ya),rg=l(Ya,"P",{"data-svelte-h":!0}),f(rg)!=="svelte-19no0kv"&&(rg.innerHTML=iw),XC=a(Ya),p(ka.$$.fragment,Ya),Ya.forEach(d),QC=a(dn),ae=l(dn,"DIV",{class:!0});var Vr=y(ae);p(Hi.$$.fragment,Vr),HC=a(Vr),ng=l(Vr,"P",{"data-svelte-h":!0}),f(ng)!=="svelte-1jrxhfe"&&(ng.textContent=lw),YC=a(Vr),ag=l(Vr,"P",{"data-svelte-h":!0}),f(ag)!=="svelte-1enn9p2"&&(ag.innerHTML=dw),OC=a(Vr),sg=l(Vr,"UL",{"data-svelte-h":!0}),f(sg)!=="svelte-382b5z"&&(sg.innerHTML=mw),KC=a(Vr),tg=l(Vr,"P",{"data-svelte-h":!0}),f(tg)!=="svelte-tebhz6"&&(tg.innerHTML=cw),eT=a(Vr),p(xa.$$.fragment,Vr),Vr.forEach(d),dn.forEach(d),sp=a(e),p(Yi.$$.fragment,e),tp=a(e),Je=l(e,"DIV",{class:!0});var mn=y(Je);p(Oi.$$.fragment,mn),oT=a(mn),ig=l(mn,"P",{"data-svelte-h":!0}),f(ig)!=="svelte-12ksz2a"&&(ig.innerHTML=fw),rT=a(mn),lg=l(mn,"P",{"data-svelte-h":!0}),f(lg)!=="svelte-1n2cphd"&&(lg.innerHTML=gw),nT=a(mn),_r=l(mn,"DIV",{class:!0});var Oa=y(_r);p(Ki.$$.fragment,Oa),aT=a(Oa),dg=l(Oa,"P",{"data-svelte-h":!0}),f(dg)!=="svelte-1aufdt0"&&(dg.textContent=hw),sT=a(Oa),mg=l(Oa,"P",{"data-svelte-h":!0}),f(mg)!=="svelte-19no0kv"&&(mg.innerHTML=uw),tT=a(Oa),p(Pa.$$.fragment,Oa),Oa.forEach(d),iT=a(mn),se=l(mn,"DIV",{class:!0});var Ar=y(se);p(el.$$.fragment,Ar),lT=a(Ar),cg=l(Ar,"P",{"data-svelte-h":!0}),f(cg)!=="svelte-73ck9g"&&(cg.textContent=pw),dT=a(Ar),fg=l(Ar,"P",{"data-svelte-h":!0}),f(fg)!=="svelte-1enn9p2"&&(fg.innerHTML=_w),mT=a(Ar),gg=l(Ar,"UL",{"data-svelte-h":!0}),f(gg)!=="svelte-13zq50o"&&(gg.innerHTML=vw),cT=a(Ar),hg=l(Ar,"P",{"data-svelte-h":!0}),f(hg)!=="svelte-tebhz6"&&(hg.innerHTML=Mw),fT=a(Ar),p(Ga.$$.fragment,Ar),Ar.forEach(d),mn.forEach(d),ip=a(e),p(ol.$$.fragment,e),lp=a(e),De=l(e,"DIV",{class:!0});var cn=y(De);p(rl.$$.fragment,cn),gT=a(cn),ug=l(cn,"P",{"data-svelte-h":!0}),f(ug)!=="svelte-112nzim"&&(ug.innerHTML=bw),hT=a(cn),pg=l(cn,"P",{"data-svelte-h":!0}),f(pg)!=="svelte-1n2cphd"&&(pg.innerHTML=Cw),uT=a(cn),vr=l(cn,"DIV",{class:!0});var Ka=y(vr);p(nl.$$.fragment,Ka),pT=a(Ka),_g=l(Ka,"P",{"data-svelte-h":!0}),f(_g)!=="svelte-1wxna3s"&&(_g.textContent=Tw),_T=a(Ka),vg=l(Ka,"P",{"data-svelte-h":!0}),f(vg)!=="svelte-19no0kv"&&(vg.innerHTML=Fw),vT=a(Ka),p(Va.$$.fragment,Ka),Ka.forEach(d),MT=a(cn),te=l(cn,"DIV",{class:!0});var Sr=y(te);p(al.$$.fragment,Sr),bT=a(Sr),Mg=l(Sr,"P",{"data-svelte-h":!0}),f(Mg)!=="svelte-1n6mjno"&&(Mg.textContent=yw),CT=a(Sr),bg=l(Sr,"P",{"data-svelte-h":!0}),f(bg)!=="svelte-1enn9p2"&&(bg.innerHTML=ww),TT=a(Sr),Cg=l(Sr,"UL",{"data-svelte-h":!0}),f(Cg)!=="svelte-w1ed4"&&(Cg.innerHTML=Lw),FT=a(Sr),Tg=l(Sr,"P",{"data-svelte-h":!0}),f(Tg)!=="svelte-tebhz6"&&(Tg.innerHTML=kw),yT=a(Sr),p(Aa.$$.fragment,Sr),Sr.forEach(d),cn.forEach(d),dp=a(e),p(sl.$$.fragment,e),mp=a(e),tl=l(e,"DIV",{class:!0});var FL=y(tl);p(il.$$.fragment,FL),FL.forEach(d),cp=a(e),p(ll.$$.fragment,e),fp=a(e),dl=l(e,"DIV",{class:!0});var yL=y(dl);p(ml.$$.fragment,yL),yL.forEach(d),gp=a(e),p(cl.$$.fragment,e),hp=a(e),Ue=l(e,"DIV",{class:!0});var fn=y(Ue);p(fl.$$.fragment,fn),wT=a(fn),Fg=l(fn,"P",{"data-svelte-h":!0}),f(Fg)!=="svelte-12d8u8h"&&(Fg.innerHTML=xw),LT=a(fn),yg=l(fn,"P",{"data-svelte-h":!0}),f(yg)!=="svelte-1n2cphd"&&(yg.innerHTML=Pw),kT=a(fn),Mr=l(fn,"DIV",{class:!0});var es=y(Mr);p(gl.$$.fragment,es),xT=a(es),wg=l(es,"P",{"data-svelte-h":!0}),f(wg)!=="svelte-1almkcf"&&(wg.textContent=Gw),PT=a(es),Lg=l(es,"P",{"data-svelte-h":!0}),f(Lg)!=="svelte-19no0kv"&&(Lg.innerHTML=Vw),GT=a(es),p(Sa.$$.fragment,es),es.forEach(d),VT=a(fn),ie=l(fn,"DIV",{class:!0});var Br=y(ie);p(hl.$$.fragment,Br),AT=a(Br),kg=l(Br,"P",{"data-svelte-h":!0}),f(kg)!=="svelte-kkxv2h"&&(kg.textContent=Aw),ST=a(Br),xg=l(Br,"P",{"data-svelte-h":!0}),f(xg)!=="svelte-1enn9p2"&&(xg.innerHTML=Sw),BT=a(Br),Pg=l(Br,"UL",{"data-svelte-h":!0}),f(Pg)!=="svelte-1y4ch3x"&&(Pg.innerHTML=Bw),$T=a(Br),Gg=l(Br,"P",{"data-svelte-h":!0}),f(Gg)!=="svelte-tebhz6"&&(Gg.innerHTML=$w),IT=a(Br),p(Ba.$$.fragment,Br),Br.forEach(d),fn.forEach(d),up=a(e),p(ul.$$.fragment,e),pp=a(e),pl=l(e,"P",{"data-svelte-h":!0}),f(pl)!=="svelte-1rj2vi9"&&(pl.textContent=Iw),_p=a(e),p(_l.$$.fragment,e),vp=a(e),qe=l(e,"DIV",{class:!0});var gn=y(qe);p(vl.$$.fragment,gn),jT=a(gn),Vg=l(gn,"P",{"data-svelte-h":!0}),f(Vg)!=="svelte-hzv3e6"&&(Vg.innerHTML=jw),ET=a(gn),Ag=l(gn,"P",{"data-svelte-h":!0}),f(Ag)!=="svelte-1n2cphd"&&(Ag.innerHTML=Ew),ZT=a(gn),br=l(gn,"DIV",{class:!0});var os=y(br);p(Ml.$$.fragment,os),RT=a(os),Sg=l(os,"P",{"data-svelte-h":!0}),f(Sg)!=="svelte-syrugc"&&(Sg.textContent=Zw),WT=a(os),Bg=l(os,"P",{"data-svelte-h":!0}),f(Bg)!=="svelte-19no0kv"&&(Bg.innerHTML=Rw),NT=a(os),p($a.$$.fragment,os),os.forEach(d),JT=a(gn),le=l(gn,"DIV",{class:!0});var $r=y(le);p(bl.$$.fragment,$r),DT=a($r),$g=l($r,"P",{"data-svelte-h":!0}),f($g)!=="svelte-1ontymo"&&($g.textContent=Ww),UT=a($r),Ig=l($r,"P",{"data-svelte-h":!0}),f(Ig)!=="svelte-1enn9p2"&&(Ig.innerHTML=Nw),qT=a($r),jg=l($r,"UL",{"data-svelte-h":!0}),f(jg)!=="svelte-gvckba"&&(jg.innerHTML=Jw),zT=a($r),Eg=l($r,"P",{"data-svelte-h":!0}),f(Eg)!=="svelte-tebhz6"&&(Eg.innerHTML=Dw),XT=a($r),p(Ia.$$.fragment,$r),$r.forEach(d),gn.forEach(d),Mp=a(e),p(Cl.$$.fragment,e),bp=a(e),ze=l(e,"DIV",{class:!0});var hn=y(ze);p(Tl.$$.fragment,hn),QT=a(hn),Zg=l(hn,"P",{"data-svelte-h":!0}),f(Zg)!=="svelte-1k7cbzn"&&(Zg.innerHTML=Uw),HT=a(hn),Rg=l(hn,"P",{"data-svelte-h":!0}),f(Rg)!=="svelte-1n2cphd"&&(Rg.innerHTML=qw),YT=a(hn),Cr=l(hn,"DIV",{class:!0});var rs=y(Cr);p(Fl.$$.fragment,rs),OT=a(rs),Wg=l(rs,"P",{"data-svelte-h":!0}),f(Wg)!=="svelte-axumax"&&(Wg.textContent=zw),KT=a(rs),Ng=l(rs,"P",{"data-svelte-h":!0}),f(Ng)!=="svelte-19no0kv"&&(Ng.innerHTML=Xw),e5=a(rs),p(ja.$$.fragment,rs),rs.forEach(d),o5=a(hn),de=l(hn,"DIV",{class:!0});var Ir=y(de);p(yl.$$.fragment,Ir),r5=a(Ir),Jg=l(Ir,"P",{"data-svelte-h":!0}),f(Jg)!=="svelte-dbgbst"&&(Jg.textContent=Qw),n5=a(Ir),Dg=l(Ir,"P",{"data-svelte-h":!0}),f(Dg)!=="svelte-1enn9p2"&&(Dg.innerHTML=Hw),a5=a(Ir),Ug=l(Ir,"UL",{"data-svelte-h":!0}),f(Ug)!=="svelte-1ld7et"&&(Ug.innerHTML=Yw),s5=a(Ir),qg=l(Ir,"P",{"data-svelte-h":!0}),f(qg)!=="svelte-tebhz6"&&(qg.innerHTML=Ow),t5=a(Ir),p(Ea.$$.fragment,Ir),Ir.forEach(d),hn.forEach(d),Cp=a(e),p(wl.$$.fragment,e),Tp=a(e),Xe=l(e,"DIV",{class:!0});var un=y(Xe);p(Ll.$$.fragment,un),i5=a(un),zg=l(un,"P",{"data-svelte-h":!0}),f(zg)!=="svelte-sd8rrm"&&(zg.innerHTML=Kw),l5=a(un),Xg=l(un,"P",{"data-svelte-h":!0}),f(Xg)!=="svelte-1n2cphd"&&(Xg.innerHTML=eL),d5=a(un),Tr=l(un,"DIV",{class:!0});var ns=y(Tr);p(kl.$$.fragment,ns),m5=a(ns),Qg=l(ns,"P",{"data-svelte-h":!0}),f(Qg)!=="svelte-1nom7tk"&&(Qg.textContent=oL),c5=a(ns),Hg=l(ns,"P",{"data-svelte-h":!0}),f(Hg)!=="svelte-19no0kv"&&(Hg.innerHTML=rL),f5=a(ns),p(Za.$$.fragment,ns),ns.forEach(d),g5=a(un),me=l(un,"DIV",{class:!0});var jr=y(me);p(xl.$$.fragment,jr),h5=a(jr),Yg=l(jr,"P",{"data-svelte-h":!0}),f(Yg)!=="svelte-7pdp5q"&&(Yg.textContent=nL),u5=a(jr),Og=l(jr,"P",{"data-svelte-h":!0}),f(Og)!=="svelte-1enn9p2"&&(Og.innerHTML=aL),p5=a(jr),Kg=l(jr,"UL",{"data-svelte-h":!0}),f(Kg)!=="svelte-15e8pwt"&&(Kg.innerHTML=sL),_5=a(jr),eh=l(jr,"P",{"data-svelte-h":!0}),f(eh)!=="svelte-tebhz6"&&(eh.innerHTML=tL),v5=a(jr),p(Ra.$$.fragment,jr),jr.forEach(d),un.forEach(d),Fp=a(e),p(Pl.$$.fragment,e),yp=a(e),Gl=l(e,"DIV",{class:!0});var wL=y(Gl);p(Vl.$$.fragment,wL),wL.forEach(d),wp=a(e),p(Al.$$.fragment,e),Lp=a(e),Qe=l(e,"DIV",{class:!0});var pn=y(Qe);p(Sl.$$.fragment,pn),M5=a(pn),oh=l(pn,"P",{"data-svelte-h":!0}),f(oh)!=="svelte-byihdg"&&(oh.innerHTML=iL),b5=a(pn),rh=l(pn,"P",{"data-svelte-h":!0}),f(rh)!=="svelte-1n2cphd"&&(rh.innerHTML=lL),C5=a(pn),Fr=l(pn,"DIV",{class:!0});var as=y(Fr);p(Bl.$$.fragment,as),T5=a(as),nh=l(as,"P",{"data-svelte-h":!0}),f(nh)!=="svelte-sziluy"&&(nh.textContent=dL),F5=a(as),ah=l(as,"P",{"data-svelte-h":!0}),f(ah)!=="svelte-19no0kv"&&(ah.innerHTML=mL),y5=a(as),p(Wa.$$.fragment,as),as.forEach(d),w5=a(pn),ce=l(pn,"DIV",{class:!0});var Er=y(ce);p($l.$$.fragment,Er),L5=a(Er),sh=l(Er,"P",{"data-svelte-h":!0}),f(sh)!=="svelte-11vub8g"&&(sh.textContent=cL),k5=a(Er),th=l(Er,"P",{"data-svelte-h":!0}),f(th)!=="svelte-1enn9p2"&&(th.innerHTML=fL),x5=a(Er),ih=l(Er,"UL",{"data-svelte-h":!0}),f(ih)!=="svelte-olcu8o"&&(ih.innerHTML=gL),P5=a(Er),lh=l(Er,"P",{"data-svelte-h":!0}),f(lh)!=="svelte-tebhz6"&&(lh.innerHTML=hL),G5=a(Er),p(Na.$$.fragment,Er),Er.forEach(d),pn.forEach(d),kp=a(e),p(Il.$$.fragment,e),xp=a(e),p(jl.$$.fragment,e),Pp=a(e),He=l(e,"DIV",{class:!0});var _n=y(He);p(El.$$.fragment,_n),V5=a(_n),dh=l(_n,"P",{"data-svelte-h":!0}),f(dh)!=="svelte-1w6qbbu"&&(dh.innerHTML=uL),A5=a(_n),mh=l(_n,"P",{"data-svelte-h":!0}),f(mh)!=="svelte-1n2cphd"&&(mh.innerHTML=pL),S5=a(_n),yr=l(_n,"DIV",{class:!0});var ss=y(yr);p(Zl.$$.fragment,ss),B5=a(ss),ch=l(ss,"P",{"data-svelte-h":!0}),f(ch)!=="svelte-1wmj3sc"&&(ch.textContent=_L),$5=a(ss),fh=l(ss,"P",{"data-svelte-h":!0}),f(fh)!=="svelte-19no0kv"&&(fh.innerHTML=vL),I5=a(ss),p(Ja.$$.fragment,ss),ss.forEach(d),j5=a(_n),fe=l(_n,"DIV",{class:!0});var Zr=y(fe);p(Rl.$$.fragment,Zr),E5=a(Zr),gh=l(Zr,"P",{"data-svelte-h":!0}),f(gh)!=="svelte-x57ca4"&&(gh.textContent=ML),Z5=a(Zr),hh=l(Zr,"P",{"data-svelte-h":!0}),f(hh)!=="svelte-1enn9p2"&&(hh.innerHTML=bL),R5=a(Zr),uh=l(Zr,"UL",{"data-svelte-h":!0}),f(uh)!=="svelte-2esb9k"&&(uh.innerHTML=CL),W5=a(Zr),ph=l(Zr,"P",{"data-svelte-h":!0}),f(ph)!=="svelte-tebhz6"&&(ph.innerHTML=TL),N5=a(Zr),p(Da.$$.fragment,Zr),Zr.forEach(d),_n.forEach(d),Gp=a(e),p(Wl.$$.fragment,e),Vp=a(e),_h=l(e,"P",{}),y(_h).forEach(d),this.h()},h(){w(s,"name","hf:doc:metadata"),w(s,"content",ex),w(Mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(wn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Vn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Bn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ko,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ei,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ui,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tl,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dl,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Gl,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,m){r(document.head,s),h(e,T,m),h(e,c,m),h(e,t,m),_(g,e,m),h(e,o,m),h(e,C,m),h(e,yh,m),h(e,ts,m),h(e,wh,m),_(is,e,m),h(e,Lh,m),h(e,ls,m),h(e,kh,m),h(e,ds,m),h(e,xh,m),_(ms,e,m),h(e,Ph,m),h(e,cs,m),h(e,Gh,m),_(fs,e,m),h(e,Vh,m),h(e,gs,m),h(e,Ah,m),_(vn,e,m),h(e,Sh,m),_(hs,e,m),h(e,Bh,m),h(e,ge,m),_(us,ge,null),r(ge,Sp),r(ge,Xl),r(ge,Bp),r(ge,Ql),r(ge,$p),r(ge,Mo),_(ps,Mo,null),r(Mo,Ip),r(Mo,Hl),r(Mo,jp),r(Mo,Yl),r(Mo,Ep),r(Mo,Ol),r(Mo,Zp),_(Mn,Mo,null),r(ge,Rp),r(ge,bn),_(_s,bn,null),r(bn,Wp),r(bn,Kl),h(e,$h,m),_(vs,e,m),h(e,Ih,m),h(e,he,m),_(Ms,he,null),r(he,Np),r(he,ed),r(he,Jp),r(he,od),r(he,Dp),r(he,bo),_(bs,bo,null),r(bo,Up),r(bo,rd),r(bo,qp),r(bo,nd),r(bo,zp),r(bo,ad),r(bo,Xp),_(Cn,bo,null),r(he,Qp),r(he,Tn),_(Cs,Tn,null),r(Tn,Hp),r(Tn,sd),h(e,jh,m),_(Ts,e,m),h(e,Eh,m),h(e,ue,m),_(Fs,ue,null),r(ue,Yp),r(ue,td),r(ue,Op),r(ue,id),r(ue,Kp),r(ue,V),_(ys,V,null),r(V,e_),r(V,ld),r(V,o_),r(V,dd),r(V,r_),r(V,md),r(V,n_),_(Fn,V,null),r(V,a_),_(yn,V,null),r(ue,s_),r(ue,wn),_(ws,wn,null),r(wn,t_),r(wn,cd),h(e,Zh,m),_(Ls,e,m),h(e,Rh,m),h(e,pe,m),_(ks,pe,null),r(pe,i_),r(pe,fd),r(pe,l_),r(pe,gd),r(pe,d_),r(pe,A),_(xs,A,null),r(A,m_),r(A,hd),r(A,c_),r(A,ud),r(A,f_),r(A,pd),r(A,g_),_(Ln,A,null),r(A,h_),_(kn,A,null),r(pe,u_),r(pe,xn),_(Ps,xn,null),r(xn,p_),r(xn,_d),h(e,Wh,m),_(Gs,e,m),h(e,Nh,m),h(e,_e,m),_(Vs,_e,null),r(_e,__),r(_e,vd),r(_e,v_),r(_e,Md),r(_e,M_),r(_e,S),_(As,S,null),r(S,b_),r(S,bd),r(S,C_),r(S,Cd),r(S,T_),r(S,Td),r(S,F_),_(Pn,S,null),r(S,y_),_(Gn,S,null),r(_e,w_),r(_e,Vn),_(Ss,Vn,null),r(Vn,L_),r(Vn,Fd),h(e,Jh,m),_(Bs,e,m),h(e,Dh,m),h(e,ve,m),_($s,ve,null),r(ve,k_),r(ve,yd),r(ve,x_),r(ve,wd),r(ve,P_),r(ve,B),_(Is,B,null),r(B,G_),r(B,Ld),r(B,V_),r(B,kd),r(B,A_),r(B,xd),r(B,S_),_(An,B,null),r(B,B_),_(Sn,B,null),r(ve,$_),r(ve,Bn),_(js,Bn,null),r(Bn,I_),r(Bn,Pd),h(e,Uh,m),_(Es,e,m),h(e,qh,m),h(e,Zs,m),h(e,zh,m),_(Rs,e,m),h(e,Xh,m),h(e,Me,m),_(Ws,Me,null),r(Me,j_),r(Me,Gd),r(Me,E_),r(Me,Vd),r(Me,Z_),r(Me,zo),_(Ns,zo,null),r(zo,R_),r(zo,Ad),r(zo,W_),r(zo,Sd),r(zo,N_),_($n,zo,null),r(Me,J_),r(Me,$),_(Js,$,null),r($,D_),r($,Bd),r($,U_),r($,$d),r($,q_),r($,Id),r($,z_),r($,jd),r($,X_),_(In,$,null),h(e,Qh,m),_(Ds,e,m),h(e,Hh,m),h(e,Us,m),h(e,Yh,m),_(qs,e,m),h(e,Oh,m),h(e,be,m),_(zs,be,null),r(be,Q_),r(be,Ed),r(be,H_),r(be,Zd),r(be,Y_),r(be,Xo),_(Xs,Xo,null),r(Xo,O_),r(Xo,Rd),r(Xo,K_),r(Xo,Wd),r(Xo,ev),_(jn,Xo,null),r(be,ov),r(be,I),_(Qs,I,null),r(I,rv),r(I,Nd),r(I,nv),r(I,Jd),r(I,av),r(I,Dd),r(I,sv),r(I,Ud),r(I,tv),_(En,I,null),h(e,Kh,m),_(Hs,e,m),h(e,eu,m),h(e,Ys,m),h(e,ou,m),_(Os,e,m),h(e,ru,m),h(e,Ce,m),_(Ks,Ce,null),r(Ce,iv),r(Ce,qd),r(Ce,lv),r(Ce,zd),r(Ce,dv),r(Ce,Qo),_(et,Qo,null),r(Qo,mv),r(Qo,Xd),r(Qo,cv),r(Qo,Qd),r(Qo,fv),_(Zn,Qo,null),r(Ce,gv),r(Ce,j),_(ot,j,null),r(j,hv),r(j,Hd),r(j,uv),r(j,Yd),r(j,pv),r(j,Od),r(j,_v),r(j,Kd),r(j,vv),_(Rn,j,null),h(e,nu,m),_(rt,e,m),h(e,au,m),h(e,Te,m),_(nt,Te,null),r(Te,Mv),r(Te,em),r(Te,bv),r(Te,om),r(Te,Cv),r(Te,Ho),_(at,Ho,null),r(Ho,Tv),r(Ho,rm),r(Ho,Fv),r(Ho,nm),r(Ho,yv),_(Wn,Ho,null),r(Te,wv),r(Te,E),_(st,E,null),r(E,Lv),r(E,am),r(E,kv),r(E,sm),r(E,xv),r(E,tm),r(E,Pv),r(E,im),r(E,Gv),_(Nn,E,null),h(e,su,m),_(tt,e,m),h(e,tu,m),h(e,it,m),_(lt,it,null),h(e,iu,m),_(dt,e,m),h(e,lu,m),h(e,Fe,m),_(mt,Fe,null),r(Fe,Vv),r(Fe,lm),r(Fe,Av),r(Fe,dm),r(Fe,Sv),r(Fe,Yo),_(ct,Yo,null),r(Yo,Bv),r(Yo,mm),r(Yo,$v),r(Yo,cm),r(Yo,Iv),_(Jn,Yo,null),r(Fe,jv),r(Fe,Z),_(ft,Z,null),r(Z,Ev),r(Z,fm),r(Z,Zv),r(Z,gm),r(Z,Rv),r(Z,hm),r(Z,Wv),r(Z,um),r(Z,Nv),_(Dn,Z,null),h(e,du,m),_(gt,e,m),h(e,mu,m),h(e,ye,m),_(ht,ye,null),r(ye,Jv),r(ye,pm),r(ye,Dv),r(ye,_m),r(ye,Uv),r(ye,Oo),_(ut,Oo,null),r(Oo,qv),r(Oo,vm),r(Oo,zv),r(Oo,Mm),r(Oo,Xv),_(Un,Oo,null),r(ye,Qv),r(ye,R),_(pt,R,null),r(R,Hv),r(R,bm),r(R,Yv),r(R,Cm),r(R,Ov),r(R,Tm),r(R,Kv),r(R,Fm),r(R,e2),_(qn,R,null),h(e,cu,m),_(_t,e,m),h(e,fu,m),h(e,we,m),_(vt,we,null),r(we,o2),r(we,ym),r(we,r2),r(we,wm),r(we,n2),r(we,Ko),_(Mt,Ko,null),r(Ko,a2),r(Ko,Lm),r(Ko,s2),r(Ko,km),r(Ko,t2),_(zn,Ko,null),r(we,i2),r(we,W),_(bt,W,null),r(W,l2),r(W,xm),r(W,d2),r(W,Pm),r(W,m2),r(W,Gm),r(W,c2),r(W,Vm),r(W,f2),_(Xn,W,null),h(e,gu,m),_(Ct,e,m),h(e,hu,m),h(e,Le,m),_(Tt,Le,null),r(Le,g2),r(Le,Am),r(Le,h2),r(Le,Sm),r(Le,u2),r(Le,er),_(Ft,er,null),r(er,p2),r(er,Bm),r(er,_2),r(er,$m),r(er,v2),_(Qn,er,null),r(Le,M2),r(Le,N),_(yt,N,null),r(N,b2),r(N,Im),r(N,C2),r(N,jm),r(N,T2),r(N,Em),r(N,F2),r(N,Zm),r(N,y2),_(Hn,N,null),h(e,uu,m),_(wt,e,m),h(e,pu,m),h(e,ke,m),_(Lt,ke,null),r(ke,w2),r(ke,Rm),r(ke,L2),r(ke,Wm),r(ke,k2),r(ke,or),_(kt,or,null),r(or,x2),r(or,Nm),r(or,P2),r(or,Jm),r(or,G2),_(Yn,or,null),r(ke,V2),r(ke,J),_(xt,J,null),r(J,A2),r(J,Dm),r(J,S2),r(J,Um),r(J,B2),r(J,qm),r(J,$2),r(J,zm),r(J,I2),_(On,J,null),h(e,_u,m),_(Pt,e,m),h(e,vu,m),h(e,xe,m),_(Gt,xe,null),r(xe,j2),r(xe,Xm),r(xe,E2),r(xe,Qm),r(xe,Z2),r(xe,rr),_(Vt,rr,null),r(rr,R2),r(rr,Hm),r(rr,W2),r(rr,Ym),r(rr,N2),_(Kn,rr,null),r(xe,J2),r(xe,D),_(At,D,null),r(D,D2),r(D,Om),r(D,U2),r(D,Km),r(D,q2),r(D,ec),r(D,z2),r(D,oc),r(D,X2),_(ea,D,null),h(e,Mu,m),_(St,e,m),h(e,bu,m),h(e,Bt,m),_($t,Bt,null),h(e,Cu,m),_(It,e,m),h(e,Tu,m),h(e,jt,m),h(e,Fu,m),_(Et,e,m),h(e,yu,m),h(e,Pe,m),_(Zt,Pe,null),r(Pe,Q2),r(Pe,rc),r(Pe,H2),r(Pe,nc),r(Pe,Y2),r(Pe,nr),_(Rt,nr,null),r(nr,O2),r(nr,ac),r(nr,K2),r(nr,sc),r(nr,eM),_(oa,nr,null),r(Pe,oM),r(Pe,U),_(Wt,U,null),r(U,rM),r(U,tc),r(U,nM),r(U,ic),r(U,aM),r(U,lc),r(U,sM),r(U,dc),r(U,tM),_(ra,U,null),h(e,wu,m),_(Nt,e,m),h(e,Lu,m),h(e,Ge,m),_(Jt,Ge,null),r(Ge,iM),r(Ge,mc),r(Ge,lM),r(Ge,cc),r(Ge,dM),r(Ge,ar),_(Dt,ar,null),r(ar,mM),r(ar,fc),r(ar,cM),r(ar,gc),r(ar,fM),_(na,ar,null),r(Ge,gM),r(Ge,q),_(Ut,q,null),r(q,hM),r(q,hc),r(q,uM),r(q,uc),r(q,pM),r(q,pc),r(q,_M),r(q,_c),r(q,vM),_(aa,q,null),h(e,ku,m),_(qt,e,m),h(e,xu,m),h(e,Ve,m),_(zt,Ve,null),r(Ve,MM),r(Ve,vc),r(Ve,bM),r(Ve,Mc),r(Ve,CM),r(Ve,sr),_(Xt,sr,null),r(sr,TM),r(sr,bc),r(sr,FM),r(sr,Cc),r(sr,yM),_(sa,sr,null),r(Ve,wM),r(Ve,z),_(Qt,z,null),r(z,LM),r(z,Tc),r(z,kM),r(z,Fc),r(z,xM),r(z,yc),r(z,PM),r(z,wc),r(z,GM),_(ta,z,null),h(e,Pu,m),_(Ht,e,m),h(e,Gu,m),h(e,Yt,m),_(Ot,Yt,null),h(e,Vu,m),_(Kt,e,m),h(e,Au,m),h(e,ei,m),_(oi,ei,null),h(e,Su,m),_(ri,e,m),h(e,Bu,m),h(e,Ae,m),_(ni,Ae,null),r(Ae,VM),r(Ae,Lc),r(Ae,AM),r(Ae,kc),r(Ae,SM),r(Ae,tr),_(ai,tr,null),r(tr,BM),r(tr,xc),r(tr,$M),r(tr,Pc),r(tr,IM),_(ia,tr,null),r(Ae,jM),r(Ae,X),_(si,X,null),r(X,EM),r(X,Gc),r(X,ZM),r(X,Vc),r(X,RM),r(X,Ac),r(X,WM),r(X,Sc),r(X,NM),_(la,X,null),h(e,$u,m),_(ti,e,m),h(e,Iu,m),h(e,Se,m),_(ii,Se,null),r(Se,JM),r(Se,Bc),r(Se,DM),r(Se,$c),r(Se,UM),r(Se,ir),_(li,ir,null),r(ir,qM),r(ir,Ic),r(ir,zM),r(ir,jc),r(ir,XM),_(da,ir,null),r(Se,QM),r(Se,Q),_(di,Q,null),r(Q,HM),r(Q,Ec),r(Q,YM),r(Q,Zc),r(Q,OM),r(Q,Rc),r(Q,KM),r(Q,Wc),r(Q,eb),_(ma,Q,null),h(e,ju,m),_(mi,e,m),h(e,Eu,m),h(e,Be,m),_(ci,Be,null),r(Be,ob),r(Be,Nc),r(Be,rb),r(Be,Jc),r(Be,nb),r(Be,lr),_(fi,lr,null),r(lr,ab),r(lr,Dc),r(lr,sb),r(lr,Uc),r(lr,tb),_(ca,lr,null),r(Be,ib),r(Be,H),_(gi,H,null),r(H,lb),r(H,qc),r(H,db),r(H,zc),r(H,mb),r(H,Xc),r(H,cb),r(H,Qc),r(H,fb),_(fa,H,null),h(e,Zu,m),_(hi,e,m),h(e,Ru,m),h(e,ui,m),_(pi,ui,null),h(e,Wu,m),_(_i,e,m),h(e,Nu,m),h(e,$e,m),_(vi,$e,null),r($e,gb),r($e,Hc),r($e,hb),r($e,Yc),r($e,ub),r($e,dr),_(Mi,dr,null),r(dr,pb),r(dr,Oc),r(dr,_b),r(dr,Kc),r(dr,vb),_(ga,dr,null),r($e,Mb),r($e,Y),_(bi,Y,null),r(Y,bb),r(Y,ef),r(Y,Cb),r(Y,of),r(Y,Tb),r(Y,rf),r(Y,Fb),r(Y,nf),r(Y,yb),_(ha,Y,null),h(e,Ju,m),_(Ci,e,m),h(e,Du,m),h(e,Ie,m),_(Ti,Ie,null),r(Ie,wb),r(Ie,af),r(Ie,Lb),r(Ie,sf),r(Ie,kb),r(Ie,mr),_(Fi,mr,null),r(mr,xb),r(mr,tf),r(mr,Pb),r(mr,lf),r(mr,Gb),_(ua,mr,null),r(Ie,Vb),r(Ie,O),_(yi,O,null),r(O,Ab),r(O,df),r(O,Sb),r(O,mf),r(O,Bb),r(O,cf),r(O,$b),r(O,ff),r(O,Ib),_(pa,O,null),h(e,Uu,m),_(wi,e,m),h(e,qu,m),h(e,je,m),_(Li,je,null),r(je,jb),r(je,gf),r(je,Eb),r(je,hf),r(je,Zb),r(je,cr),_(ki,cr,null),r(cr,Rb),r(cr,uf),r(cr,Wb),r(cr,pf),r(cr,Nb),_(_a,cr,null),r(je,Jb),r(je,K),_(xi,K,null),r(K,Db),r(K,_f),r(K,Ub),r(K,vf),r(K,qb),r(K,Mf),r(K,zb),r(K,bf),r(K,Xb),_(va,K,null),h(e,zu,m),_(Pi,e,m),h(e,Xu,m),h(e,Ee,m),_(Gi,Ee,null),r(Ee,Qb),r(Ee,Cf),r(Ee,Hb),r(Ee,Tf),r(Ee,Yb),r(Ee,fr),_(Vi,fr,null),r(fr,Ob),r(fr,Ff),r(fr,Kb),r(fr,yf),r(fr,eC),_(Ma,fr,null),r(Ee,oC),r(Ee,ee),_(Ai,ee,null),r(ee,rC),r(ee,wf),r(ee,nC),r(ee,Lf),r(ee,aC),r(ee,kf),r(ee,sC),r(ee,xf),r(ee,tC),_(ba,ee,null),h(e,Qu,m),_(Si,e,m),h(e,Hu,m),h(e,Ze,m),_(Bi,Ze,null),r(Ze,iC),r(Ze,Pf),r(Ze,lC),r(Ze,Gf),r(Ze,dC),r(Ze,gr),_($i,gr,null),r(gr,mC),r(gr,Vf),r(gr,cC),r(gr,Af),r(gr,fC),_(Ca,gr,null),r(Ze,gC),r(Ze,oe),_(Ii,oe,null),r(oe,hC),r(oe,Sf),r(oe,uC),r(oe,Bf),r(oe,pC),r(oe,$f),r(oe,_C),r(oe,If),r(oe,vC),_(Ta,oe,null),h(e,Yu,m),_(ji,e,m),h(e,Ou,m),h(e,Ei,m),h(e,Ku,m),_(Zi,e,m),h(e,ep,m),h(e,Re,m),_(Ri,Re,null),r(Re,MC),r(Re,jf),r(Re,bC),r(Re,Ef),r(Re,CC),r(Re,hr),_(Wi,hr,null),r(hr,TC),r(hr,Zf),r(hr,FC),r(hr,Rf),r(hr,yC),_(Fa,hr,null),r(Re,wC),r(Re,re),_(Ni,re,null),r(re,LC),r(re,Wf),r(re,kC),r(re,Nf),r(re,xC),r(re,Jf),r(re,PC),r(re,Df),r(re,GC),_(ya,re,null),h(e,op,m),_(Ji,e,m),h(e,rp,m),h(e,We,m),_(Di,We,null),r(We,VC),r(We,Uf),r(We,AC),r(We,qf),r(We,SC),r(We,ur),_(Ui,ur,null),r(ur,BC),r(ur,zf),r(ur,$C),r(ur,Xf),r(ur,IC),_(wa,ur,null),r(We,jC),r(We,ne),_(qi,ne,null),r(ne,EC),r(ne,Qf),r(ne,ZC),r(ne,Hf),r(ne,RC),r(ne,Yf),r(ne,WC),r(ne,Of),r(ne,NC),_(La,ne,null),h(e,np,m),_(zi,e,m),h(e,ap,m),h(e,Ne,m),_(Xi,Ne,null),r(Ne,JC),r(Ne,Kf),r(Ne,DC),r(Ne,eg),r(Ne,UC),r(Ne,pr),_(Qi,pr,null),r(pr,qC),r(pr,og),r(pr,zC),r(pr,rg),r(pr,XC),_(ka,pr,null),r(Ne,QC),r(Ne,ae),_(Hi,ae,null),r(ae,HC),r(ae,ng),r(ae,YC),r(ae,ag),r(ae,OC),r(ae,sg),r(ae,KC),r(ae,tg),r(ae,eT),_(xa,ae,null),h(e,sp,m),_(Yi,e,m),h(e,tp,m),h(e,Je,m),_(Oi,Je,null),r(Je,oT),r(Je,ig),r(Je,rT),r(Je,lg),r(Je,nT),r(Je,_r),_(Ki,_r,null),r(_r,aT),r(_r,dg),r(_r,sT),r(_r,mg),r(_r,tT),_(Pa,_r,null),r(Je,iT),r(Je,se),_(el,se,null),r(se,lT),r(se,cg),r(se,dT),r(se,fg),r(se,mT),r(se,gg),r(se,cT),r(se,hg),r(se,fT),_(Ga,se,null),h(e,ip,m),_(ol,e,m),h(e,lp,m),h(e,De,m),_(rl,De,null),r(De,gT),r(De,ug),r(De,hT),r(De,pg),r(De,uT),r(De,vr),_(nl,vr,null),r(vr,pT),r(vr,_g),r(vr,_T),r(vr,vg),r(vr,vT),_(Va,vr,null),r(De,MT),r(De,te),_(al,te,null),r(te,bT),r(te,Mg),r(te,CT),r(te,bg),r(te,TT),r(te,Cg),r(te,FT),r(te,Tg),r(te,yT),_(Aa,te,null),h(e,dp,m),_(sl,e,m),h(e,mp,m),h(e,tl,m),_(il,tl,null),h(e,cp,m),_(ll,e,m),h(e,fp,m),h(e,dl,m),_(ml,dl,null),h(e,gp,m),_(cl,e,m),h(e,hp,m),h(e,Ue,m),_(fl,Ue,null),r(Ue,wT),r(Ue,Fg),r(Ue,LT),r(Ue,yg),r(Ue,kT),r(Ue,Mr),_(gl,Mr,null),r(Mr,xT),r(Mr,wg),r(Mr,PT),r(Mr,Lg),r(Mr,GT),_(Sa,Mr,null),r(Ue,VT),r(Ue,ie),_(hl,ie,null),r(ie,AT),r(ie,kg),r(ie,ST),r(ie,xg),r(ie,BT),r(ie,Pg),r(ie,$T),r(ie,Gg),r(ie,IT),_(Ba,ie,null),h(e,up,m),_(ul,e,m),h(e,pp,m),h(e,pl,m),h(e,_p,m),_(_l,e,m),h(e,vp,m),h(e,qe,m),_(vl,qe,null),r(qe,jT),r(qe,Vg),r(qe,ET),r(qe,Ag),r(qe,ZT),r(qe,br),_(Ml,br,null),r(br,RT),r(br,Sg),r(br,WT),r(br,Bg),r(br,NT),_($a,br,null),r(qe,JT),r(qe,le),_(bl,le,null),r(le,DT),r(le,$g),r(le,UT),r(le,Ig),r(le,qT),r(le,jg),r(le,zT),r(le,Eg),r(le,XT),_(Ia,le,null),h(e,Mp,m),_(Cl,e,m),h(e,bp,m),h(e,ze,m),_(Tl,ze,null),r(ze,QT),r(ze,Zg),r(ze,HT),r(ze,Rg),r(ze,YT),r(ze,Cr),_(Fl,Cr,null),r(Cr,OT),r(Cr,Wg),r(Cr,KT),r(Cr,Ng),r(Cr,e5),_(ja,Cr,null),r(ze,o5),r(ze,de),_(yl,de,null),r(de,r5),r(de,Jg),r(de,n5),r(de,Dg),r(de,a5),r(de,Ug),r(de,s5),r(de,qg),r(de,t5),_(Ea,de,null),h(e,Cp,m),_(wl,e,m),h(e,Tp,m),h(e,Xe,m),_(Ll,Xe,null),r(Xe,i5),r(Xe,zg),r(Xe,l5),r(Xe,Xg),r(Xe,d5),r(Xe,Tr),_(kl,Tr,null),r(Tr,m5),r(Tr,Qg),r(Tr,c5),r(Tr,Hg),r(Tr,f5),_(Za,Tr,null),r(Xe,g5),r(Xe,me),_(xl,me,null),r(me,h5),r(me,Yg),r(me,u5),r(me,Og),r(me,p5),r(me,Kg),r(me,_5),r(me,eh),r(me,v5),_(Ra,me,null),h(e,Fp,m),_(Pl,e,m),h(e,yp,m),h(e,Gl,m),_(Vl,Gl,null),h(e,wp,m),_(Al,e,m),h(e,Lp,m),h(e,Qe,m),_(Sl,Qe,null),r(Qe,M5),r(Qe,oh),r(Qe,b5),r(Qe,rh),r(Qe,C5),r(Qe,Fr),_(Bl,Fr,null),r(Fr,T5),r(Fr,nh),r(Fr,F5),r(Fr,ah),r(Fr,y5),_(Wa,Fr,null),r(Qe,w5),r(Qe,ce),_($l,ce,null),r(ce,L5),r(ce,sh),r(ce,k5),r(ce,th),r(ce,x5),r(ce,ih),r(ce,P5),r(ce,lh),r(ce,G5),_(Na,ce,null),h(e,kp,m),_(Il,e,m),h(e,xp,m),_(jl,e,m),h(e,Pp,m),h(e,He,m),_(El,He,null),r(He,V5),r(He,dh),r(He,A5),r(He,mh),r(He,S5),r(He,yr),_(Zl,yr,null),r(yr,B5),r(yr,ch),r(yr,$5),r(yr,fh),r(yr,I5),_(Ja,yr,null),r(He,j5),r(He,fe),_(Rl,fe,null),r(fe,E5),r(fe,gh),r(fe,Z5),r(fe,hh),r(fe,R5),r(fe,uh),r(fe,W5),r(fe,ph),r(fe,N5),_(Da,fe,null),h(e,Gp,m),_(Wl,e,m),h(e,Vp,m),h(e,_h,m),Ap=!0},p(e,[m]){const Co={};m&2&&(Co.$$scope={dirty:m,ctx:e}),vn.$set(Co);const To={};m&2&&(To.$$scope={dirty:m,ctx:e}),Mn.$set(To);const Nl={};m&2&&(Nl.$$scope={dirty:m,ctx:e}),Cn.$set(Nl);const Fo={};m&2&&(Fo.$$scope={dirty:m,ctx:e}),Fn.$set(Fo);const yo={};m&2&&(yo.$$scope={dirty:m,ctx:e}),yn.$set(yo);const Jl={};m&2&&(Jl.$$scope={dirty:m,ctx:e}),Ln.$set(Jl);const wo={};m&2&&(wo.$$scope={dirty:m,ctx:e}),kn.$set(wo);const Ye={};m&2&&(Ye.$$scope={dirty:m,ctx:e}),Pn.$set(Ye);const Dl={};m&2&&(Dl.$$scope={dirty:m,ctx:e}),Gn.$set(Dl);const Lo={};m&2&&(Lo.$$scope={dirty:m,ctx:e}),An.$set(Lo);const Oe={};m&2&&(Oe.$$scope={dirty:m,ctx:e}),Sn.$set(Oe);const Ul={};m&2&&(Ul.$$scope={dirty:m,ctx:e}),$n.$set(Ul);const ko={};m&2&&(ko.$$scope={dirty:m,ctx:e}),In.$set(ko);const Ke={};m&2&&(Ke.$$scope={dirty:m,ctx:e}),jn.$set(Ke);const ql={};m&2&&(ql.$$scope={dirty:m,ctx:e}),En.$set(ql);const xo={};m&2&&(xo.$$scope={dirty:m,ctx:e}),Zn.$set(xo);const eo={};m&2&&(eo.$$scope={dirty:m,ctx:e}),Rn.$set(eo);const zl={};m&2&&(zl.$$scope={dirty:m,ctx:e}),Wn.$set(zl);const Po={};m&2&&(Po.$$scope={dirty:m,ctx:e}),Nn.$set(Po);const Rr={};m&2&&(Rr.$$scope={dirty:m,ctx:e}),Jn.$set(Rr);const oo={};m&2&&(oo.$$scope={dirty:m,ctx:e}),Dn.$set(oo);const Go={};m&2&&(Go.$$scope={dirty:m,ctx:e}),Un.$set(Go);const Wr={};m&2&&(Wr.$$scope={dirty:m,ctx:e}),qn.$set(Wr);const ro={};m&2&&(ro.$$scope={dirty:m,ctx:e}),zn.$set(ro);const Vo={};m&2&&(Vo.$$scope={dirty:m,ctx:e}),Xn.$set(Vo);const Nr={};m&2&&(Nr.$$scope={dirty:m,ctx:e}),Qn.$set(Nr);const no={};m&2&&(no.$$scope={dirty:m,ctx:e}),Hn.$set(no);const Ao={};m&2&&(Ao.$$scope={dirty:m,ctx:e}),Yn.$set(Ao);const Jr={};m&2&&(Jr.$$scope={dirty:m,ctx:e}),On.$set(Jr);const ao={};m&2&&(ao.$$scope={dirty:m,ctx:e}),Kn.$set(ao);const vh={};m&2&&(vh.$$scope={dirty:m,ctx:e}),ea.$set(vh);const So={};m&2&&(So.$$scope={dirty:m,ctx:e}),oa.$set(So);const Dr={};m&2&&(Dr.$$scope={dirty:m,ctx:e}),ra.$set(Dr);const so={};m&2&&(so.$$scope={dirty:m,ctx:e}),na.$set(so);const Bo={};m&2&&(Bo.$$scope={dirty:m,ctx:e}),aa.$set(Bo);const Ur={};m&2&&(Ur.$$scope={dirty:m,ctx:e}),sa.$set(Ur);const to={};m&2&&(to.$$scope={dirty:m,ctx:e}),ta.$set(to);const $o={};m&2&&($o.$$scope={dirty:m,ctx:e}),ia.$set($o);const qr={};m&2&&(qr.$$scope={dirty:m,ctx:e}),la.$set(qr);const io={};m&2&&(io.$$scope={dirty:m,ctx:e}),da.$set(io);const Io={};m&2&&(Io.$$scope={dirty:m,ctx:e}),ma.$set(Io);const zr={};m&2&&(zr.$$scope={dirty:m,ctx:e}),ca.$set(zr);const lo={};m&2&&(lo.$$scope={dirty:m,ctx:e}),fa.$set(lo);const jo={};m&2&&(jo.$$scope={dirty:m,ctx:e}),ga.$set(jo);const Xr={};m&2&&(Xr.$$scope={dirty:m,ctx:e}),ha.$set(Xr);const mo={};m&2&&(mo.$$scope={dirty:m,ctx:e}),ua.$set(mo);const Eo={};m&2&&(Eo.$$scope={dirty:m,ctx:e}),pa.$set(Eo);const Qr={};m&2&&(Qr.$$scope={dirty:m,ctx:e}),_a.$set(Qr);const co={};m&2&&(co.$$scope={dirty:m,ctx:e}),va.$set(co);const Mh={};m&2&&(Mh.$$scope={dirty:m,ctx:e}),Ma.$set(Mh);const Zo={};m&2&&(Zo.$$scope={dirty:m,ctx:e}),ba.$set(Zo);const Hr={};m&2&&(Hr.$$scope={dirty:m,ctx:e}),Ca.$set(Hr);const fo={};m&2&&(fo.$$scope={dirty:m,ctx:e}),Ta.$set(fo);const Ro={};m&2&&(Ro.$$scope={dirty:m,ctx:e}),Fa.$set(Ro);const Yr={};m&2&&(Yr.$$scope={dirty:m,ctx:e}),ya.$set(Yr);const go={};m&2&&(go.$$scope={dirty:m,ctx:e}),wa.$set(go);const Wo={};m&2&&(Wo.$$scope={dirty:m,ctx:e}),La.$set(Wo);const Or={};m&2&&(Or.$$scope={dirty:m,ctx:e}),ka.$set(Or);const ho={};m&2&&(ho.$$scope={dirty:m,ctx:e}),xa.$set(ho);const bh={};m&2&&(bh.$$scope={dirty:m,ctx:e}),Pa.$set(bh);const Ch={};m&2&&(Ch.$$scope={dirty:m,ctx:e}),Ga.$set(Ch);const No={};m&2&&(No.$$scope={dirty:m,ctx:e}),Va.$set(No);const Kr={};m&2&&(Kr.$$scope={dirty:m,ctx:e}),Aa.$set(Kr);const uo={};m&2&&(uo.$$scope={dirty:m,ctx:e}),Sa.$set(uo);const Jo={};m&2&&(Jo.$$scope={dirty:m,ctx:e}),Ba.$set(Jo);const en={};m&2&&(en.$$scope={dirty:m,ctx:e}),$a.$set(en);const po={};m&2&&(po.$$scope={dirty:m,ctx:e}),Ia.$set(po);const Do={};m&2&&(Do.$$scope={dirty:m,ctx:e}),ja.$set(Do);const on={};m&2&&(on.$$scope={dirty:m,ctx:e}),Ea.$set(on);const _o={};m&2&&(_o.$$scope={dirty:m,ctx:e}),Za.$set(_o);const Th={};m&2&&(Th.$$scope={dirty:m,ctx:e}),Ra.$set(Th);const Uo={};m&2&&(Uo.$$scope={dirty:m,ctx:e}),Wa.$set(Uo);const rn={};m&2&&(rn.$$scope={dirty:m,ctx:e}),Na.$set(rn);const vo={};m&2&&(vo.$$scope={dirty:m,ctx:e}),Ja.$set(vo);const qo={};m&2&&(qo.$$scope={dirty:m,ctx:e}),Da.$set(qo)},i(e){Ap||(v(g.$$.fragment,e),v(is.$$.fragment,e),v(ms.$$.fragment,e),v(fs.$$.fragment,e),v(vn.$$.fragment,e),v(hs.$$.fragment,e),v(us.$$.fragment,e),v(ps.$$.fragment,e),v(Mn.$$.fragment,e),v(_s.$$.fragment,e),v(vs.$$.fragment,e),v(Ms.$$.fragment,e),v(bs.$$.fragment,e),v(Cn.$$.fragment,e),v(Cs.$$.fragment,e),v(Ts.$$.fragment,e),v(Fs.$$.fragment,e),v(ys.$$.fragment,e),v(Fn.$$.fragment,e),v(yn.$$.fragment,e),v(ws.$$.fragment,e),v(Ls.$$.fragment,e),v(ks.$$.fragment,e),v(xs.$$.fragment,e),v(Ln.$$.fragment,e),v(kn.$$.fragment,e),v(Ps.$$.fragment,e),v(Gs.$$.fragment,e),v(Vs.$$.fragment,e),v(As.$$.fragment,e),v(Pn.$$.fragment,e),v(Gn.$$.fragment,e),v(Ss.$$.fragment,e),v(Bs.$$.fragment,e),v($s.$$.fragment,e),v(Is.$$.fragment,e),v(An.$$.fragment,e),v(Sn.$$.fragment,e),v(js.$$.fragment,e),v(Es.$$.fragment,e),v(Rs.$$.fragment,e),v(Ws.$$.fragment,e),v(Ns.$$.fragment,e),v($n.$$.fragment,e),v(Js.$$.fragment,e),v(In.$$.fragment,e),v(Ds.$$.fragment,e),v(qs.$$.fragment,e),v(zs.$$.fragment,e),v(Xs.$$.fragment,e),v(jn.$$.fragment,e),v(Qs.$$.fragment,e),v(En.$$.fragment,e),v(Hs.$$.fragment,e),v(Os.$$.fragment,e),v(Ks.$$.fragment,e),v(et.$$.fragment,e),v(Zn.$$.fragment,e),v(ot.$$.fragment,e),v(Rn.$$.fragment,e),v(rt.$$.fragment,e),v(nt.$$.fragment,e),v(at.$$.fragment,e),v(Wn.$$.fragment,e),v(st.$$.fragment,e),v(Nn.$$.fragment,e),v(tt.$$.fragment,e),v(lt.$$.fragment,e),v(dt.$$.fragment,e),v(mt.$$.fragment,e),v(ct.$$.fragment,e),v(Jn.$$.fragment,e),v(ft.$$.fragment,e),v(Dn.$$.fragment,e),v(gt.$$.fragment,e),v(ht.$$.fragment,e),v(ut.$$.fragment,e),v(Un.$$.fragment,e),v(pt.$$.fragment,e),v(qn.$$.fragment,e),v(_t.$$.fragment,e),v(vt.$$.fragment,e),v(Mt.$$.fragment,e),v(zn.$$.fragment,e),v(bt.$$.fragment,e),v(Xn.$$.fragment,e),v(Ct.$$.fragment,e),v(Tt.$$.fragment,e),v(Ft.$$.fragment,e),v(Qn.$$.fragment,e),v(yt.$$.fragment,e),v(Hn.$$.fragment,e),v(wt.$$.fragment,e),v(Lt.$$.fragment,e),v(kt.$$.fragment,e),v(Yn.$$.fragment,e),v(xt.$$.fragment,e),v(On.$$.fragment,e),v(Pt.$$.fragment,e),v(Gt.$$.fragment,e),v(Vt.$$.fragment,e),v(Kn.$$.fragment,e),v(At.$$.fragment,e),v(ea.$$.fragment,e),v(St.$$.fragment,e),v($t.$$.fragment,e),v(It.$$.fragment,e),v(Et.$$.fragment,e),v(Zt.$$.fragment,e),v(Rt.$$.fragment,e),v(oa.$$.fragment,e),v(Wt.$$.fragment,e),v(ra.$$.fragment,e),v(Nt.$$.fragment,e),v(Jt.$$.fragment,e),v(Dt.$$.fragment,e),v(na.$$.fragment,e),v(Ut.$$.fragment,e),v(aa.$$.fragment,e),v(qt.$$.fragment,e),v(zt.$$.fragment,e),v(Xt.$$.fragment,e),v(sa.$$.fragment,e),v(Qt.$$.fragment,e),v(ta.$$.fragment,e),v(Ht.$$.fragment,e),v(Ot.$$.fragment,e),v(Kt.$$.fragment,e),v(oi.$$.fragment,e),v(ri.$$.fragment,e),v(ni.$$.fragment,e),v(ai.$$.fragment,e),v(ia.$$.fragment,e),v(si.$$.fragment,e),v(la.$$.fragment,e),v(ti.$$.fragment,e),v(ii.$$.fragment,e),v(li.$$.fragment,e),v(da.$$.fragment,e),v(di.$$.fragment,e),v(ma.$$.fragment,e),v(mi.$$.fragment,e),v(ci.$$.fragment,e),v(fi.$$.fragment,e),v(ca.$$.fragment,e),v(gi.$$.fragment,e),v(fa.$$.fragment,e),v(hi.$$.fragment,e),v(pi.$$.fragment,e),v(_i.$$.fragment,e),v(vi.$$.fragment,e),v(Mi.$$.fragment,e),v(ga.$$.fragment,e),v(bi.$$.fragment,e),v(ha.$$.fragment,e),v(Ci.$$.fragment,e),v(Ti.$$.fragment,e),v(Fi.$$.fragment,e),v(ua.$$.fragment,e),v(yi.$$.fragment,e),v(pa.$$.fragment,e),v(wi.$$.fragment,e),v(Li.$$.fragment,e),v(ki.$$.fragment,e),v(_a.$$.fragment,e),v(xi.$$.fragment,e),v(va.$$.fragment,e),v(Pi.$$.fragment,e),v(Gi.$$.fragment,e),v(Vi.$$.fragment,e),v(Ma.$$.fragment,e),v(Ai.$$.fragment,e),v(ba.$$.fragment,e),v(Si.$$.fragment,e),v(Bi.$$.fragment,e),v($i.$$.fragment,e),v(Ca.$$.fragment,e),v(Ii.$$.fragment,e),v(Ta.$$.fragment,e),v(ji.$$.fragment,e),v(Zi.$$.fragment,e),v(Ri.$$.fragment,e),v(Wi.$$.fragment,e),v(Fa.$$.fragment,e),v(Ni.$$.fragment,e),v(ya.$$.fragment,e),v(Ji.$$.fragment,e),v(Di.$$.fragment,e),v(Ui.$$.fragment,e),v(wa.$$.fragment,e),v(qi.$$.fragment,e),v(La.$$.fragment,e),v(zi.$$.fragment,e),v(Xi.$$.fragment,e),v(Qi.$$.fragment,e),v(ka.$$.fragment,e),v(Hi.$$.fragment,e),v(xa.$$.fragment,e),v(Yi.$$.fragment,e),v(Oi.$$.fragment,e),v(Ki.$$.fragment,e),v(Pa.$$.fragment,e),v(el.$$.fragment,e),v(Ga.$$.fragment,e),v(ol.$$.fragment,e),v(rl.$$.fragment,e),v(nl.$$.fragment,e),v(Va.$$.fragment,e),v(al.$$.fragment,e),v(Aa.$$.fragment,e),v(sl.$$.fragment,e),v(il.$$.fragment,e),v(ll.$$.fragment,e),v(ml.$$.fragment,e),v(cl.$$.fragment,e),v(fl.$$.fragment,e),v(gl.$$.fragment,e),v(Sa.$$.fragment,e),v(hl.$$.fragment,e),v(Ba.$$.fragment,e),v(ul.$$.fragment,e),v(_l.$$.fragment,e),v(vl.$$.fragment,e),v(Ml.$$.fragment,e),v($a.$$.fragment,e),v(bl.$$.fragment,e),v(Ia.$$.fragment,e),v(Cl.$$.fragment,e),v(Tl.$$.fragment,e),v(Fl.$$.fragment,e),v(ja.$$.fragment,e),v(yl.$$.fragment,e),v(Ea.$$.fragment,e),v(wl.$$.fragment,e),v(Ll.$$.fragment,e),v(kl.$$.fragment,e),v(Za.$$.fragment,e),v(xl.$$.fragment,e),v(Ra.$$.fragment,e),v(Pl.$$.fragment,e),v(Vl.$$.fragment,e),v(Al.$$.fragment,e),v(Sl.$$.fragment,e),v(Bl.$$.fragment,e),v(Wa.$$.fragment,e),v($l.$$.fragment,e),v(Na.$$.fragment,e),v(Il.$$.fragment,e),v(jl.$$.fragment,e),v(El.$$.fragment,e),v(Zl.$$.fragment,e),v(Ja.$$.fragment,e),v(Rl.$$.fragment,e),v(Da.$$.fragment,e),v(Wl.$$.fragment,e),Ap=!0)},o(e){M(g.$$.fragment,e),M(is.$$.fragment,e),M(ms.$$.fragment,e),M(fs.$$.fragment,e),M(vn.$$.fragment,e),M(hs.$$.fragment,e),M(us.$$.fragment,e),M(ps.$$.fragment,e),M(Mn.$$.fragment,e),M(_s.$$.fragment,e),M(vs.$$.fragment,e),M(Ms.$$.fragment,e),M(bs.$$.fragment,e),M(Cn.$$.fragment,e),M(Cs.$$.fragment,e),M(Ts.$$.fragment,e),M(Fs.$$.fragment,e),M(ys.$$.fragment,e),M(Fn.$$.fragment,e),M(yn.$$.fragment,e),M(ws.$$.fragment,e),M(Ls.$$.fragment,e),M(ks.$$.fragment,e),M(xs.$$.fragment,e),M(Ln.$$.fragment,e),M(kn.$$.fragment,e),M(Ps.$$.fragment,e),M(Gs.$$.fragment,e),M(Vs.$$.fragment,e),M(As.$$.fragment,e),M(Pn.$$.fragment,e),M(Gn.$$.fragment,e),M(Ss.$$.fragment,e),M(Bs.$$.fragment,e),M($s.$$.fragment,e),M(Is.$$.fragment,e),M(An.$$.fragment,e),M(Sn.$$.fragment,e),M(js.$$.fragment,e),M(Es.$$.fragment,e),M(Rs.$$.fragment,e),M(Ws.$$.fragment,e),M(Ns.$$.fragment,e),M($n.$$.fragment,e),M(Js.$$.fragment,e),M(In.$$.fragment,e),M(Ds.$$.fragment,e),M(qs.$$.fragment,e),M(zs.$$.fragment,e),M(Xs.$$.fragment,e),M(jn.$$.fragment,e),M(Qs.$$.fragment,e),M(En.$$.fragment,e),M(Hs.$$.fragment,e),M(Os.$$.fragment,e),M(Ks.$$.fragment,e),M(et.$$.fragment,e),M(Zn.$$.fragment,e),M(ot.$$.fragment,e),M(Rn.$$.fragment,e),M(rt.$$.fragment,e),M(nt.$$.fragment,e),M(at.$$.fragment,e),M(Wn.$$.fragment,e),M(st.$$.fragment,e),M(Nn.$$.fragment,e),M(tt.$$.fragment,e),M(lt.$$.fragment,e),M(dt.$$.fragment,e),M(mt.$$.fragment,e),M(ct.$$.fragment,e),M(Jn.$$.fragment,e),M(ft.$$.fragment,e),M(Dn.$$.fragment,e),M(gt.$$.fragment,e),M(ht.$$.fragment,e),M(ut.$$.fragment,e),M(Un.$$.fragment,e),M(pt.$$.fragment,e),M(qn.$$.fragment,e),M(_t.$$.fragment,e),M(vt.$$.fragment,e),M(Mt.$$.fragment,e),M(zn.$$.fragment,e),M(bt.$$.fragment,e),M(Xn.$$.fragment,e),M(Ct.$$.fragment,e),M(Tt.$$.fragment,e),M(Ft.$$.fragment,e),M(Qn.$$.fragment,e),M(yt.$$.fragment,e),M(Hn.$$.fragment,e),M(wt.$$.fragment,e),M(Lt.$$.fragment,e),M(kt.$$.fragment,e),M(Yn.$$.fragment,e),M(xt.$$.fragment,e),M(On.$$.fragment,e),M(Pt.$$.fragment,e),M(Gt.$$.fragment,e),M(Vt.$$.fragment,e),M(Kn.$$.fragment,e),M(At.$$.fragment,e),M(ea.$$.fragment,e),M(St.$$.fragment,e),M($t.$$.fragment,e),M(It.$$.fragment,e),M(Et.$$.fragment,e),M(Zt.$$.fragment,e),M(Rt.$$.fragment,e),M(oa.$$.fragment,e),M(Wt.$$.fragment,e),M(ra.$$.fragment,e),M(Nt.$$.fragment,e),M(Jt.$$.fragment,e),M(Dt.$$.fragment,e),M(na.$$.fragment,e),M(Ut.$$.fragment,e),M(aa.$$.fragment,e),M(qt.$$.fragment,e),M(zt.$$.fragment,e),M(Xt.$$.fragment,e),M(sa.$$.fragment,e),M(Qt.$$.fragment,e),M(ta.$$.fragment,e),M(Ht.$$.fragment,e),M(Ot.$$.fragment,e),M(Kt.$$.fragment,e),M(oi.$$.fragment,e),M(ri.$$.fragment,e),M(ni.$$.fragment,e),M(ai.$$.fragment,e),M(ia.$$.fragment,e),M(si.$$.fragment,e),M(la.$$.fragment,e),M(ti.$$.fragment,e),M(ii.$$.fragment,e),M(li.$$.fragment,e),M(da.$$.fragment,e),M(di.$$.fragment,e),M(ma.$$.fragment,e),M(mi.$$.fragment,e),M(ci.$$.fragment,e),M(fi.$$.fragment,e),M(ca.$$.fragment,e),M(gi.$$.fragment,e),M(fa.$$.fragment,e),M(hi.$$.fragment,e),M(pi.$$.fragment,e),M(_i.$$.fragment,e),M(vi.$$.fragment,e),M(Mi.$$.fragment,e),M(ga.$$.fragment,e),M(bi.$$.fragment,e),M(ha.$$.fragment,e),M(Ci.$$.fragment,e),M(Ti.$$.fragment,e),M(Fi.$$.fragment,e),M(ua.$$.fragment,e),M(yi.$$.fragment,e),M(pa.$$.fragment,e),M(wi.$$.fragment,e),M(Li.$$.fragment,e),M(ki.$$.fragment,e),M(_a.$$.fragment,e),M(xi.$$.fragment,e),M(va.$$.fragment,e),M(Pi.$$.fragment,e),M(Gi.$$.fragment,e),M(Vi.$$.fragment,e),M(Ma.$$.fragment,e),M(Ai.$$.fragment,e),M(ba.$$.fragment,e),M(Si.$$.fragment,e),M(Bi.$$.fragment,e),M($i.$$.fragment,e),M(Ca.$$.fragment,e),M(Ii.$$.fragment,e),M(Ta.$$.fragment,e),M(ji.$$.fragment,e),M(Zi.$$.fragment,e),M(Ri.$$.fragment,e),M(Wi.$$.fragment,e),M(Fa.$$.fragment,e),M(Ni.$$.fragment,e),M(ya.$$.fragment,e),M(Ji.$$.fragment,e),M(Di.$$.fragment,e),M(Ui.$$.fragment,e),M(wa.$$.fragment,e),M(qi.$$.fragment,e),M(La.$$.fragment,e),M(zi.$$.fragment,e),M(Xi.$$.fragment,e),M(Qi.$$.fragment,e),M(ka.$$.fragment,e),M(Hi.$$.fragment,e),M(xa.$$.fragment,e),M(Yi.$$.fragment,e),M(Oi.$$.fragment,e),M(Ki.$$.fragment,e),M(Pa.$$.fragment,e),M(el.$$.fragment,e),M(Ga.$$.fragment,e),M(ol.$$.fragment,e),M(rl.$$.fragment,e),M(nl.$$.fragment,e),M(Va.$$.fragment,e),M(al.$$.fragment,e),M(Aa.$$.fragment,e),M(sl.$$.fragment,e),M(il.$$.fragment,e),M(ll.$$.fragment,e),M(ml.$$.fragment,e),M(cl.$$.fragment,e),M(fl.$$.fragment,e),M(gl.$$.fragment,e),M(Sa.$$.fragment,e),M(hl.$$.fragment,e),M(Ba.$$.fragment,e),M(ul.$$.fragment,e),M(_l.$$.fragment,e),M(vl.$$.fragment,e),M(Ml.$$.fragment,e),M($a.$$.fragment,e),M(bl.$$.fragment,e),M(Ia.$$.fragment,e),M(Cl.$$.fragment,e),M(Tl.$$.fragment,e),M(Fl.$$.fragment,e),M(ja.$$.fragment,e),M(yl.$$.fragment,e),M(Ea.$$.fragment,e),M(wl.$$.fragment,e),M(Ll.$$.fragment,e),M(kl.$$.fragment,e),M(Za.$$.fragment,e),M(xl.$$.fragment,e),M(Ra.$$.fragment,e),M(Pl.$$.fragment,e),M(Vl.$$.fragment,e),M(Al.$$.fragment,e),M(Sl.$$.fragment,e),M(Bl.$$.fragment,e),M(Wa.$$.fragment,e),M($l.$$.fragment,e),M(Na.$$.fragment,e),M(Il.$$.fragment,e),M(jl.$$.fragment,e),M(El.$$.fragment,e),M(Zl.$$.fragment,e),M(Ja.$$.fragment,e),M(Rl.$$.fragment,e),M(Da.$$.fragment,e),M(Wl.$$.fragment,e),Ap=!1},d(e){e&&(d(T),d(c),d(t),d(o),d(C),d(yh),d(ts),d(wh),d(Lh),d(ls),d(kh),d(ds),d(xh),d(Ph),d(cs),d(Gh),d(Vh),d(gs),d(Ah),d(Sh),d(Bh),d(ge),d($h),d(Ih),d(he),d(jh),d(Eh),d(ue),d(Zh),d(Rh),d(pe),d(Wh),d(Nh),d(_e),d(Jh),d(Dh),d(ve),d(Uh),d(qh),d(Zs),d(zh),d(Xh),d(Me),d(Qh),d(Hh),d(Us),d(Yh),d(Oh),d(be),d(Kh),d(eu),d(Ys),d(ou),d(ru),d(Ce),d(nu),d(au),d(Te),d(su),d(tu),d(it),d(iu),d(lu),d(Fe),d(du),d(mu),d(ye),d(cu),d(fu),d(we),d(gu),d(hu),d(Le),d(uu),d(pu),d(ke),d(_u),d(vu),d(xe),d(Mu),d(bu),d(Bt),d(Cu),d(Tu),d(jt),d(Fu),d(yu),d(Pe),d(wu),d(Lu),d(Ge),d(ku),d(xu),d(Ve),d(Pu),d(Gu),d(Yt),d(Vu),d(Au),d(ei),d(Su),d(Bu),d(Ae),d($u),d(Iu),d(Se),d(ju),d(Eu),d(Be),d(Zu),d(Ru),d(ui),d(Wu),d(Nu),d($e),d(Ju),d(Du),d(Ie),d(Uu),d(qu),d(je),d(zu),d(Xu),d(Ee),d(Qu),d(Hu),d(Ze),d(Yu),d(Ou),d(Ei),d(Ku),d(ep),d(Re),d(op),d(rp),d(We),d(np),d(ap),d(Ne),d(sp),d(tp),d(Je),d(ip),d(lp),d(De),d(dp),d(mp),d(tl),d(cp),d(fp),d(dl),d(gp),d(hp),d(Ue),d(up),d(pp),d(pl),d(_p),d(vp),d(qe),d(Mp),d(bp),d(ze),d(Cp),d(Tp),d(Xe),d(Fp),d(yp),d(Gl),d(wp),d(Lp),d(Qe),d(kp),d(xp),d(Pp),d(He),d(Gp),d(Vp),d(_h)),d(s),b(g,e),b(is,e),b(ms,e),b(fs,e),b(vn,e),b(hs,e),b(us),b(ps),b(Mn),b(_s),b(vs,e),b(Ms),b(bs),b(Cn),b(Cs),b(Ts,e),b(Fs),b(ys),b(Fn),b(yn),b(ws),b(Ls,e),b(ks),b(xs),b(Ln),b(kn),b(Ps),b(Gs,e),b(Vs),b(As),b(Pn),b(Gn),b(Ss),b(Bs,e),b($s),b(Is),b(An),b(Sn),b(js),b(Es,e),b(Rs,e),b(Ws),b(Ns),b($n),b(Js),b(In),b(Ds,e),b(qs,e),b(zs),b(Xs),b(jn),b(Qs),b(En),b(Hs,e),b(Os,e),b(Ks),b(et),b(Zn),b(ot),b(Rn),b(rt,e),b(nt),b(at),b(Wn),b(st),b(Nn),b(tt,e),b(lt),b(dt,e),b(mt),b(ct),b(Jn),b(ft),b(Dn),b(gt,e),b(ht),b(ut),b(Un),b(pt),b(qn),b(_t,e),b(vt),b(Mt),b(zn),b(bt),b(Xn),b(Ct,e),b(Tt),b(Ft),b(Qn),b(yt),b(Hn),b(wt,e),b(Lt),b(kt),b(Yn),b(xt),b(On),b(Pt,e),b(Gt),b(Vt),b(Kn),b(At),b(ea),b(St,e),b($t),b(It,e),b(Et,e),b(Zt),b(Rt),b(oa),b(Wt),b(ra),b(Nt,e),b(Jt),b(Dt),b(na),b(Ut),b(aa),b(qt,e),b(zt),b(Xt),b(sa),b(Qt),b(ta),b(Ht,e),b(Ot),b(Kt,e),b(oi),b(ri,e),b(ni),b(ai),b(ia),b(si),b(la),b(ti,e),b(ii),b(li),b(da),b(di),b(ma),b(mi,e),b(ci),b(fi),b(ca),b(gi),b(fa),b(hi,e),b(pi),b(_i,e),b(vi),b(Mi),b(ga),b(bi),b(ha),b(Ci,e),b(Ti),b(Fi),b(ua),b(yi),b(pa),b(wi,e),b(Li),b(ki),b(_a),b(xi),b(va),b(Pi,e),b(Gi),b(Vi),b(Ma),b(Ai),b(ba),b(Si,e),b(Bi),b($i),b(Ca),b(Ii),b(Ta),b(ji,e),b(Zi,e),b(Ri),b(Wi),b(Fa),b(Ni),b(ya),b(Ji,e),b(Di),b(Ui),b(wa),b(qi),b(La),b(zi,e),b(Xi),b(Qi),b(ka),b(Hi),b(xa),b(Yi,e),b(Oi),b(Ki),b(Pa),b(el),b(Ga),b(ol,e),b(rl),b(nl),b(Va),b(al),b(Aa),b(sl,e),b(il),b(ll,e),b(ml),b(cl,e),b(fl),b(gl),b(Sa),b(hl),b(Ba),b(ul,e),b(_l,e),b(vl),b(Ml),b($a),b(bl),b(Ia),b(Cl,e),b(Tl),b(Fl),b(ja),b(yl),b(Ea),b(wl,e),b(Ll),b(kl),b(Za),b(xl),b(Ra),b(Pl,e),b(Vl),b(Al,e),b(Sl),b(Bl),b(Wa),b($l),b(Na),b(Il,e),b(jl,e),b(El),b(Zl),b(Ja),b(Rl),b(Da),b(Wl,e)}}}const ex='{"title":"Auto Classes","local":"auto-classes","sections":[{"title":"Extending the Auto Classes","local":"extending-the-auto-classes","sections":[],"depth":2},{"title":"AutoConfig","local":"transformers.AutoConfig","sections":[],"depth":2},{"title":"AutoTokenizer","local":"transformers.AutoTokenizer","sections":[],"depth":2},{"title":"AutoFeatureExtractor","local":"transformers.AutoFeatureExtractor","sections":[],"depth":2},{"title":"AutoImageProcessor","local":"transformers.AutoImageProcessor","sections":[],"depth":2},{"title":"AutoVideoProcessor","local":"transformers.AutoVideoProcessor","sections":[],"depth":2},{"title":"AutoProcessor","local":"transformers.AutoProcessor","sections":[],"depth":2},{"title":"Generic model classes","local":"generic-model-classes","sections":[{"title":"AutoModel","local":"transformers.AutoModel","sections":[],"depth":3}],"depth":2},{"title":"Generic pretraining classes","local":"generic-pretraining-classes","sections":[{"title":"AutoModelForPreTraining","local":"transformers.AutoModelForPreTraining","sections":[],"depth":3}],"depth":2},{"title":"Natural Language Processing","local":"natural-language-processing","sections":[{"title":"AutoModelForCausalLM","local":"transformers.AutoModelForCausalLM","sections":[],"depth":3},{"title":"AutoModelForMaskedLM","local":"transformers.AutoModelForMaskedLM","sections":[],"depth":3},{"title":"AutoModelForMaskGeneration","local":"transformers.AutoModelForMaskGeneration","sections":[],"depth":3},{"title":"AutoModelForSeq2SeqLM","local":"transformers.AutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"AutoModelForSequenceClassification","local":"transformers.AutoModelForSequenceClassification","sections":[],"depth":3},{"title":"AutoModelForMultipleChoice","local":"transformers.AutoModelForMultipleChoice","sections":[],"depth":3},{"title":"AutoModelForNextSentencePrediction","local":"transformers.AutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"AutoModelForTokenClassification","local":"transformers.AutoModelForTokenClassification","sections":[],"depth":3},{"title":"AutoModelForQuestionAnswering","local":"transformers.AutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForTextEncoding","local":"transformers.AutoModelForTextEncoding","sections":[],"depth":3}],"depth":2},{"title":"Computer vision","local":"computer-vision","sections":[{"title":"AutoModelForDepthEstimation","local":"transformers.AutoModelForDepthEstimation","sections":[],"depth":3},{"title":"AutoModelForImageClassification","local":"transformers.AutoModelForImageClassification","sections":[],"depth":3},{"title":"AutoModelForVideoClassification","local":"transformers.AutoModelForVideoClassification","sections":[],"depth":3},{"title":"AutoModelForKeypointDetection","local":"transformers.AutoModelForKeypointDetection","sections":[],"depth":3},{"title":"AutoModelForKeypointMatching","local":"transformers.AutoModelForKeypointMatching","sections":[],"depth":3},{"title":"AutoModelForMaskedImageModeling","local":"transformers.AutoModelForMaskedImageModeling","sections":[],"depth":3},{"title":"AutoModelForObjectDetection","local":"transformers.AutoModelForObjectDetection","sections":[],"depth":3},{"title":"AutoModelForImageSegmentation","local":"transformers.AutoModelForImageSegmentation","sections":[],"depth":3},{"title":"AutoModelForImageToImage","local":"transformers.AutoModelForImageToImage","sections":[],"depth":3},{"title":"AutoModelForSemanticSegmentation","local":"transformers.AutoModelForSemanticSegmentation","sections":[],"depth":3},{"title":"AutoModelForInstanceSegmentation","local":"transformers.AutoModelForInstanceSegmentation","sections":[],"depth":3},{"title":"AutoModelForUniversalSegmentation","local":"transformers.AutoModelForUniversalSegmentation","sections":[],"depth":3},{"title":"AutoModelForZeroShotImageClassification","local":"transformers.AutoModelForZeroShotImageClassification","sections":[],"depth":3},{"title":"AutoModelForZeroShotObjectDetection","local":"transformers.AutoModelForZeroShotObjectDetection","sections":[],"depth":3}],"depth":2},{"title":"Audio","local":"audio","sections":[{"title":"AutoModelForAudioClassification","local":"transformers.AutoModelForAudioClassification","sections":[],"depth":3},{"title":"AutoModelForAudioFrameClassification","local":"transformers.AutoModelForAudioFrameClassification","sections":[],"depth":3},{"title":"AutoModelForCTC","local":"transformers.AutoModelForCTC","sections":[],"depth":3},{"title":"AutoModelForSpeechSeq2Seq","local":"transformers.AutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"AutoModelForAudioXVector","local":"transformers.AutoModelForAudioXVector","sections":[],"depth":3},{"title":"AutoModelForTextToSpectrogram","local":"transformers.AutoModelForTextToSpectrogram","sections":[],"depth":3},{"title":"AutoModelForTextToWaveform","local":"transformers.AutoModelForTextToWaveform","sections":[],"depth":3},{"title":"AutoModelForAudioTokenization","local":"transformers.AutoModelForAudioTokenization","sections":[],"depth":3}],"depth":2},{"title":"Multimodal","local":"multimodal","sections":[{"title":"AutoModelForTableQuestionAnswering","local":"transformers.AutoModelForTableQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForDocumentQuestionAnswering","local":"transformers.AutoModelForDocumentQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVisualQuestionAnswering","local":"transformers.AutoModelForVisualQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVision2Seq","local":"transformers.AutoModelForVision2Seq","sections":[],"depth":3},{"title":"AutoModelForImageTextToText","local":"transformers.AutoModelForImageTextToText","sections":[],"depth":3}],"depth":2},{"title":"Time Series","local":"time-series","sections":[{"title":"AutoModelForTimeSeriesPrediction","local":"transformers.AutoModelForTimeSeriesPrediction","sections":[],"depth":3}],"depth":2}],"depth":1}';function ox(F){return kL(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dx extends xL{constructor(s){super(),PL(this,s,ox,Kk,LL,{})}}export{dx as component};
