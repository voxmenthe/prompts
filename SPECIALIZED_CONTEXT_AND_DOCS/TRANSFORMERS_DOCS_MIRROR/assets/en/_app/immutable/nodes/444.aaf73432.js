import{s as Ne,o as Se,n as je}from"../chunks/scheduler.18a86fab.js";import{S as Xe,i as Oe,g as l,s as i,r as v,A as Ae,h as d,f as n,c as r,j as ne,x as u,u as w,k as ee,y as p,a as o,v as T,d as $,t as M,w as y}from"../chunks/index.98837b22.js";import{T as Qe}from"../chunks/Tip.77304350.js";import{D as ke}from"../chunks/Docstring.a1ef7999.js";import{C as Be}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as qe}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as be,E as Ye}from"../chunks/getInferenceSnippets.06c2775f.js";function Ke(k){let s,g="Example:",m,c,f;return c=new Be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpdERldENvbmZpZyUyQyUyMFZpdERldE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZpdERldCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwVml0RGV0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFZpdERldE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VitDetConfig, VitDetModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VitDet configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VitDetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitDetModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=l("p"),s.textContent=g,m=i(),v(c.$$.fragment)},l(a){s=d(a,"P",{"data-svelte-h":!0}),u(s)!=="svelte-11lpom8"&&(s.textContent=g),m=r(a),w(c.$$.fragment,a)},m(a,V){o(a,s,V),o(a,m,V),T(c,a,V),f=!0},p:je,i(a){f||($(c.$$.fragment,a),f=!0)},o(a){M(c.$$.fragment,a),f=!1},d(a){a&&(n(s),n(m)),y(c,a)}}}function et(k){let s,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=l("p"),s.innerHTML=g},l(m){s=d(m,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=g)},m(m,c){o(m,s,c)},p:je,d(m){m&&n(s)}}}function tt(k){let s,g="Examples:",m,c,f;return c=new Be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpdERldENvbmZpZyUyQyUyMFZpdERldE1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEElMEFjb25maWclMjAlM0QlMjBWaXREZXRDb25maWcoKSUwQW1vZGVsJTIwJTNEJTIwVml0RGV0TW9kZWwoY29uZmlnKSUwQSUwQXBpeGVsX3ZhbHVlcyUyMCUzRCUyMHRvcmNoLnJhbmRuKDElMkMlMjAzJTJDJTIwMjI0JTJDJTIwMjI0KSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VitDetConfig, VitDetModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VitDetConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitDetModel(config)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(pixel_values)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]`,wrap:!1}}),{c(){s=l("p"),s.textContent=g,m=i(),v(c.$$.fragment)},l(a){s=d(a,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=g),m=r(a),w(c.$$.fragment,a)},m(a,V){o(a,s,V),o(a,m,V),T(c,a,V),f=!0},p:je,i(a){f||($(c.$$.fragment,a),f=!0)},o(a){M(c.$$.fragment,a),f=!1},d(a){a&&(n(s),n(m)),y(c,a)}}}function nt(k){let s,g,m,c,f,a="<em>This model was released on 2022-03-30 and added to Hugging Face Transformers on 2023-08-29.</em>",V,W,oe,j,ze='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',se,H,ae,J,Ee=`The ViTDet model was proposed in <a href="https://huggingface.co/papers/2203.16527" rel="nofollow">Exploring Plain Vision Transformer Backbones for Object Detection</a> by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.
VitDet leverages the plain <a href="vit">Vision Transformer</a> for the task of object detection.`,ie,U,Ze="The abstract from the paper is the following:",re,P,We="<em>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors.</em>",le,I,He=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet" rel="nofollow">here</a>.`,de,R,Je="Tips:",ce,L,Ue="<li>At the moment, only the backbone is available.</li>",me,G,pe,_,F,ve,X,Pe=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a>. It is used to instantiate an
VitDet model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VitDet
<a href="https://huggingface.co/google/vitdet-base-patch16-224" rel="nofollow">google/vitdet-base-patch16-224</a> architecture.`,we,O,Ie=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Te,z,fe,q,ue,h,B,$e,A,Re="The bare Vitdet Model outputting raw hidden-states without any specific head on top.",Me,Q,Le=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ye,Y,Ge=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ve,C,N,Ce,K,Fe='The <a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> forward method, overrides the <code>__call__</code> special method.',xe,E,De,Z,he,S,ge,te,_e;return W=new be({props:{title:"ViTDet",local:"vitdet",headingTag:"h1"}}),H=new be({props:{title:"Overview",local:"overview",headingTag:"h2"}}),G=new be({props:{title:"VitDetConfig",local:"transformers.VitDetConfig",headingTag:"h2"}}),F=new ke({props:{name:"class transformers.VitDetConfig",anchor:"transformers.VitDetConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"mlp_ratio",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"pretrain_image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"drop_path_rate",val:" = 0.0"},{name:"window_block_indices",val:" = []"},{name:"residual_block_indices",val:" = []"},{name:"use_absolute_position_embeddings",val:" = True"},{name:"use_relative_position_embeddings",val:" = False"},{name:"window_size",val:" = 0"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VitDetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.VitDetConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.VitDetConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.VitDetConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of mlp hidden dim to embedding dim.`,name:"mlp_ratio"},{anchor:"transformers.VitDetConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VitDetConfig.dropout_prob",description:`<strong>dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout_prob"},{anchor:"transformers.VitDetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VitDetConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VitDetConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VitDetConfig.pretrain_image_size",description:`<strong>pretrain_image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image during pretraining.`,name:"pretrain_image_size"},{anchor:"transformers.VitDetConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.VitDetConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VitDetConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.VitDetConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.VitDetConfig.window_block_indices",description:`<strong>window_block_indices</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of indices of blocks that should have window attention instead of regular global self-attention.`,name:"window_block_indices"},{anchor:"transformers.VitDetConfig.residual_block_indices",description:`<strong>residual_block_indices</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of indices of blocks that should have an extra residual block after the MLP.`,name:"residual_block_indices"},{anchor:"transformers.VitDetConfig.use_absolute_position_embeddings",description:`<strong>use_absolute_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add absolute position embeddings to the patch embeddings.`,name:"use_absolute_position_embeddings"},{anchor:"transformers.VitDetConfig.use_relative_position_embeddings",description:`<strong>use_relative_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add relative position embeddings to the attention maps.`,name:"use_relative_position_embeddings"},{anchor:"transformers.VitDetConfig.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The size of the attention window.`,name:"window_size"},{anchor:"transformers.VitDetConfig.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.VitDetConfig.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitdet/configuration_vitdet.py#L25"}}),z=new qe({props:{anchor:"transformers.VitDetConfig.example",$$slots:{default:[Ke]},$$scope:{ctx:k}}}),q=new be({props:{title:"VitDetModel",local:"transformers.VitDetModel",headingTag:"h2"}}),B=new ke({props:{name:"class transformers.VitDetModel",anchor:"transformers.VitDetModel",parameters:[{name:"config",val:": VitDetConfig"}],parametersDescription:[{anchor:"transformers.VitDetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitdet/modeling_vitdet.py#L650"}}),N=new ke({props:{name:"forward",anchor:"transformers.VitDetModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VitDetModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<code>processor_class</code> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.VitDetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.VitDetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VitDetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VitDetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vitdet/modeling_vitdet.py#L672",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vitdet#transformers.VitDetConfig"
>VitDetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new Qe({props:{$$slots:{default:[et]},$$scope:{ctx:k}}}),Z=new qe({props:{anchor:"transformers.VitDetModel.forward.example",$$slots:{default:[tt]},$$scope:{ctx:k}}}),S=new Ye({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vitdet.md"}}),{c(){s=l("meta"),g=i(),m=l("p"),c=i(),f=l("p"),f.innerHTML=a,V=i(),v(W.$$.fragment),oe=i(),j=l("div"),j.innerHTML=ze,se=i(),v(H.$$.fragment),ae=i(),J=l("p"),J.innerHTML=Ee,ie=i(),U=l("p"),U.textContent=Ze,re=i(),P=l("p"),P.innerHTML=We,le=i(),I=l("p"),I.innerHTML=He,de=i(),R=l("p"),R.textContent=Je,ce=i(),L=l("ul"),L.innerHTML=Ue,me=i(),v(G.$$.fragment),pe=i(),_=l("div"),v(F.$$.fragment),ve=i(),X=l("p"),X.innerHTML=Pe,we=i(),O=l("p"),O.innerHTML=Ie,Te=i(),v(z.$$.fragment),fe=i(),v(q.$$.fragment),ue=i(),h=l("div"),v(B.$$.fragment),$e=i(),A=l("p"),A.textContent=Re,Me=i(),Q=l("p"),Q.innerHTML=Le,ye=i(),Y=l("p"),Y.innerHTML=Ge,Ve=i(),C=l("div"),v(N.$$.fragment),Ce=i(),K=l("p"),K.innerHTML=Fe,xe=i(),v(E.$$.fragment),De=i(),v(Z.$$.fragment),he=i(),v(S.$$.fragment),ge=i(),te=l("p"),this.h()},l(e){const t=Ae("svelte-u9bgzb",document.head);s=d(t,"META",{name:!0,content:!0}),t.forEach(n),g=r(e),m=d(e,"P",{}),ne(m).forEach(n),c=r(e),f=d(e,"P",{"data-svelte-h":!0}),u(f)!=="svelte-1hb81ws"&&(f.innerHTML=a),V=r(e),w(W.$$.fragment,e),oe=r(e),j=d(e,"DIV",{class:!0,"data-svelte-h":!0}),u(j)!=="svelte-13t8s2t"&&(j.innerHTML=ze),se=r(e),w(H.$$.fragment,e),ae=r(e),J=d(e,"P",{"data-svelte-h":!0}),u(J)!=="svelte-k09rz9"&&(J.innerHTML=Ee),ie=r(e),U=d(e,"P",{"data-svelte-h":!0}),u(U)!=="svelte-vfdo9a"&&(U.textContent=Ze),re=r(e),P=d(e,"P",{"data-svelte-h":!0}),u(P)!=="svelte-dasg8s"&&(P.innerHTML=We),le=r(e),I=d(e,"P",{"data-svelte-h":!0}),u(I)!=="svelte-14aeur3"&&(I.innerHTML=He),de=r(e),R=d(e,"P",{"data-svelte-h":!0}),u(R)!=="svelte-axv494"&&(R.textContent=Je),ce=r(e),L=d(e,"UL",{"data-svelte-h":!0}),u(L)!=="svelte-1cc6n3i"&&(L.innerHTML=Ue),me=r(e),w(G.$$.fragment,e),pe=r(e),_=d(e,"DIV",{class:!0});var x=ne(_);w(F.$$.fragment,x),ve=r(x),X=d(x,"P",{"data-svelte-h":!0}),u(X)!=="svelte-st05kx"&&(X.innerHTML=Pe),we=r(x),O=d(x,"P",{"data-svelte-h":!0}),u(O)!=="svelte-1ek1ss9"&&(O.innerHTML=Ie),Te=r(x),w(z.$$.fragment,x),x.forEach(n),fe=r(e),w(q.$$.fragment,e),ue=r(e),h=d(e,"DIV",{class:!0});var b=ne(h);w(B.$$.fragment,b),$e=r(b),A=d(b,"P",{"data-svelte-h":!0}),u(A)!=="svelte-14gzsvk"&&(A.textContent=Re),Me=r(b),Q=d(b,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-q52n56"&&(Q.innerHTML=Le),ye=r(b),Y=d(b,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-hswkmf"&&(Y.innerHTML=Ge),Ve=r(b),C=d(b,"DIV",{class:!0});var D=ne(C);w(N.$$.fragment,D),Ce=r(D),K=d(D,"P",{"data-svelte-h":!0}),u(K)!=="svelte-1n6bla7"&&(K.innerHTML=Fe),xe=r(D),w(E.$$.fragment,D),De=r(D),w(Z.$$.fragment,D),D.forEach(n),b.forEach(n),he=r(e),w(S.$$.fragment,e),ge=r(e),te=d(e,"P",{}),ne(te).forEach(n),this.h()},h(){ee(s,"name","hf:doc:metadata"),ee(s,"content",ot),ee(j,"class","flex flex-wrap space-x-1"),ee(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ee(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ee(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,s),o(e,g,t),o(e,m,t),o(e,c,t),o(e,f,t),o(e,V,t),T(W,e,t),o(e,oe,t),o(e,j,t),o(e,se,t),T(H,e,t),o(e,ae,t),o(e,J,t),o(e,ie,t),o(e,U,t),o(e,re,t),o(e,P,t),o(e,le,t),o(e,I,t),o(e,de,t),o(e,R,t),o(e,ce,t),o(e,L,t),o(e,me,t),T(G,e,t),o(e,pe,t),o(e,_,t),T(F,_,null),p(_,ve),p(_,X),p(_,we),p(_,O),p(_,Te),T(z,_,null),o(e,fe,t),T(q,e,t),o(e,ue,t),o(e,h,t),T(B,h,null),p(h,$e),p(h,A),p(h,Me),p(h,Q),p(h,ye),p(h,Y),p(h,Ve),p(h,C),T(N,C,null),p(C,Ce),p(C,K),p(C,xe),T(E,C,null),p(C,De),T(Z,C,null),o(e,he,t),T(S,e,t),o(e,ge,t),o(e,te,t),_e=!0},p(e,[t]){const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),z.$set(x);const b={};t&2&&(b.$$scope={dirty:t,ctx:e}),E.$set(b);const D={};t&2&&(D.$$scope={dirty:t,ctx:e}),Z.$set(D)},i(e){_e||($(W.$$.fragment,e),$(H.$$.fragment,e),$(G.$$.fragment,e),$(F.$$.fragment,e),$(z.$$.fragment,e),$(q.$$.fragment,e),$(B.$$.fragment,e),$(N.$$.fragment,e),$(E.$$.fragment,e),$(Z.$$.fragment,e),$(S.$$.fragment,e),_e=!0)},o(e){M(W.$$.fragment,e),M(H.$$.fragment,e),M(G.$$.fragment,e),M(F.$$.fragment,e),M(z.$$.fragment,e),M(q.$$.fragment,e),M(B.$$.fragment,e),M(N.$$.fragment,e),M(E.$$.fragment,e),M(Z.$$.fragment,e),M(S.$$.fragment,e),_e=!1},d(e){e&&(n(g),n(m),n(c),n(f),n(V),n(oe),n(j),n(se),n(ae),n(J),n(ie),n(U),n(re),n(P),n(le),n(I),n(de),n(R),n(ce),n(L),n(me),n(pe),n(_),n(fe),n(ue),n(h),n(he),n(ge),n(te)),n(s),y(W,e),y(H,e),y(G,e),y(F),y(z),y(q,e),y(B),y(N),y(E),y(Z),y(S,e)}}}const ot='{"title":"ViTDet","local":"vitdet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"VitDetConfig","local":"transformers.VitDetConfig","sections":[],"depth":2},{"title":"VitDetModel","local":"transformers.VitDetModel","sections":[],"depth":2}],"depth":1}';function st(k){return Se(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pt extends Xe{constructor(s){super(),Oe(this,s,st,nt,Ne,{})}}export{pt as component};
