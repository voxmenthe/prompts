import{s as te,o as ee,n as gt}from"../chunks/scheduler.18a86fab.js";import{S as le,i as ae,g as d,s as i,r as T,A as se,h,f as e,c as o,j as Ot,u as w,x as f,k as Mt,y as ne,a,v as J,d as U,t as j,w as b}from"../chunks/index.98837b22.js";import{T as Lt}from"../chunks/Tip.77304350.js";import{C as z}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ct,E as ie}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as oe,a as Kt}from"../chunks/HfOption.6641485e.js";function re(A){let s,g="AutoAWQ downgrades Transformers to version 4.47.1. If you want to do inference with AutoAWQ, you may need to reinstall your Transformers’ version after installing AutoAWQ.";return{c(){s=d("p"),s.textContent=g},l(r){s=h(r,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1kinopw"&&(s.textContent=g)},m(r,m){a(r,s,m)},p:gt,d(r){r&&e(s)}}}function pe(A){let s,g="Fused modules cannot be combined with other optimization techniques such as FlashAttention2.";return{c(){s=d("p"),s.textContent=g},l(r){s=h(r,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1b7phxi"&&(s.textContent=g)},m(r,m){a(r,s,m)},p:gt,d(r){r&&e(s)}}}function ue(A){let s,g='Create an <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.AwqConfig">AwqConfig</a> and set the parameters <code>fuse_max_seq_len</code> and <code>do_fuse=True</code> to enable fused modules. The <code>fuse_max_seq_len</code> parameter is the total sequence length and it should include the context length and the expected generation length. Set it to a larger value to be safe.',r,m,u='The example below fuses the AWQ modules of the <a href="https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ" rel="nofollow">TheBloke/Mistral-7B-OpenOrca-AWQ</a> model.',y,M,_,C,I='The <a href="https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ" rel="nofollow">TheBloke/Mistral-7B-OpenOrca-AWQ</a> model was benchmarked with <code>batch_size=1</code> with and without fused modules.',G,$,Q="Unfused module",x,p,q='<thead><tr><th align="right">Batch Size</th> <th align="right">Prefill Length</th> <th align="right">Decode Length</th> <th align="right">Prefill tokens/s</th> <th align="right">Decode tokens/s</th> <th align="left">Memory (VRAM)</th></tr></thead> <tbody><tr><td align="right">1</td> <td align="right">32</td> <td align="right">32</td> <td align="right">60.0984</td> <td align="right">38.4537</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">64</td> <td align="right">64</td> <td align="right">1333.67</td> <td align="right">31.6604</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">128</td> <td align="right">128</td> <td align="right">2434.06</td> <td align="right">31.6272</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">256</td> <td align="right">256</td> <td align="right">3072.26</td> <td align="right">38.1731</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">512</td> <td align="right">512</td> <td align="right">3184.74</td> <td align="right">31.6819</td> <td align="left">4.59 GB (5.80%)</td></tr> <tr><td align="right">1</td> <td align="right">1024</td> <td align="right">1024</td> <td align="right">3148.18</td> <td align="right">36.8031</td> <td align="left">4.81 GB (6.07%)</td></tr> <tr><td align="right">1</td> <td align="right">2048</td> <td align="right">2048</td> <td align="right">2927.33</td> <td align="right">35.2676</td> <td align="left">5.73 GB (7.23%)</td></tr></tbody>',X,v,N="Fused module",Z,k,S='<thead><tr><th align="right">Batch Size</th> <th align="right">Prefill Length</th> <th align="right">Decode Length</th> <th align="right">Prefill tokens/s</th> <th align="right">Decode tokens/s</th> <th align="left">Memory (VRAM)</th></tr></thead> <tbody><tr><td align="right">1</td> <td align="right">32</td> <td align="right">32</td> <td align="right">81.4899</td> <td align="right">80.2569</td> <td align="left">4.00 GB (5.05%)</td></tr> <tr><td align="right">1</td> <td align="right">64</td> <td align="right">64</td> <td align="right">1756.1</td> <td align="right">106.26</td> <td align="left">4.00 GB (5.05%)</td></tr> <tr><td align="right">1</td> <td align="right">128</td> <td align="right">128</td> <td align="right">2479.32</td> <td align="right">105.631</td> <td align="left">4.00 GB (5.06%)</td></tr> <tr><td align="right">1</td> <td align="right">256</td> <td align="right">256</td> <td align="right">1813.6</td> <td align="right">85.7485</td> <td align="left">4.01 GB (5.06%)</td></tr> <tr><td align="right">1</td> <td align="right">512</td> <td align="right">512</td> <td align="right">2848.9</td> <td align="right">97.701</td> <td align="left">4.11 GB (5.19%)</td></tr> <tr><td align="right">1</td> <td align="right">1024</td> <td align="right">1024</td> <td align="right">3044.35</td> <td align="right">87.7323</td> <td align="left">4.41 GB (5.57%)</td></tr> <tr><td align="right">1</td> <td align="right">2048</td> <td align="right">2048</td> <td align="right">2715.11</td> <td align="right">89.4709</td> <td align="left">5.57 GB (7.04%)</td></tr></tbody>',W,E,L='The speed and throughput of fused and unfused modules were also tested with the <a href="https://github.com/huggingface/optimum-benchmark" rel="nofollow">optimum-benchmark</a> library.',Y,B,V='<div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_forward_memory_plot.png" alt="generate throughput per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">forward peak memory/batch size</figcaption></div> <div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_generate_throughput_plot.png" alt="forward latency per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">generate throughput/batch size</figcaption></div>',P;return M=new z({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXdxQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwQXdxQ29uZmlnKCUwQSUyMCUyMCUyMCUyMGJpdHMlM0Q0JTJDJTBBJTIwJTIwJTIwJTIwZnVzZV9tYXhfc2VxX2xlbiUzRDUxMiUyQyUwQSUyMCUyMCUyMCUyMGRvX2Z1c2UlM0RUcnVlJTJDJTBBKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMlRoZUJsb2tlJTJGTWlzdHJhbC03Qi1PcGVuT3JjYS1BV1ElMjIlMkMlMEElMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUwQSkudG8oMCk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AwqConfig, AutoModelForCausalLM

quantization_config = AwqConfig(
    bits=<span class="hljs-number">4</span>,
    fuse_max_seq_len=<span class="hljs-number">512</span>,
    do_fuse=<span class="hljs-literal">True</span>,
)
model = AutoModelForCausalLM.from_pretrained(
  <span class="hljs-string">&quot;TheBloke/Mistral-7B-OpenOrca-AWQ&quot;</span>,
  quantization_config=quantization_config
).to(<span class="hljs-number">0</span>)`,wrap:!1}}),{c(){s=d("p"),s.innerHTML=g,r=i(),m=d("p"),m.innerHTML=u,y=i(),T(M.$$.fragment),_=i(),C=d("p"),C.innerHTML=I,G=i(),$=d("figcaption"),$.textContent=Q,x=i(),p=d("table"),p.innerHTML=q,X=i(),v=d("figcaption"),v.textContent=N,Z=i(),k=d("table"),k.innerHTML=S,W=i(),E=d("p"),E.innerHTML=L,Y=i(),B=d("div"),B.innerHTML=V,this.h()},l(n){s=h(n,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1l4rg7w"&&(s.innerHTML=g),r=o(n),m=h(n,"P",{"data-svelte-h":!0}),f(m)!=="svelte-1qsxkvh"&&(m.innerHTML=u),y=o(n),w(M.$$.fragment,n),_=o(n),C=h(n,"P",{"data-svelte-h":!0}),f(C)!=="svelte-81zmsw"&&(C.innerHTML=I),G=o(n),$=h(n,"FIGCAPTION",{class:!0,"data-svelte-h":!0}),f($)!=="svelte-grefv0"&&($.textContent=Q),x=o(n),p=h(n,"TABLE",{"data-svelte-h":!0}),f(p)!=="svelte-19aactm"&&(p.innerHTML=q),X=o(n),v=h(n,"FIGCAPTION",{class:!0,"data-svelte-h":!0}),f(v)!=="svelte-1r15bg7"&&(v.textContent=N),Z=o(n),k=h(n,"TABLE",{"data-svelte-h":!0}),f(k)!=="svelte-19fczbk"&&(k.innerHTML=S),W=o(n),E=h(n,"P",{"data-svelte-h":!0}),f(E)!=="svelte-gu8e8k"&&(E.innerHTML=L),Y=o(n),B=h(n,"DIV",{class:!0,"data-svelte-h":!0}),f(B)!=="svelte-1ke50ja"&&(B.innerHTML=V),this.h()},h(){Mt($,"class","text-center text-gray-500 text-lg"),Mt(v,"class","text-center text-gray-500 text-lg"),Mt(B,"class","flex gap-4")},m(n,c){a(n,s,c),a(n,r,c),a(n,m,c),a(n,y,c),J(M,n,c),a(n,_,c),a(n,C,c),a(n,G,c),a(n,$,c),a(n,x,c),a(n,p,c),a(n,X,c),a(n,v,c),a(n,Z,c),a(n,k,c),a(n,W,c),a(n,E,c),a(n,Y,c),a(n,B,c),P=!0},p:gt,i(n){P||(U(M.$$.fragment,n),P=!0)},o(n){j(M.$$.fragment,n),P=!1},d(n){n&&(e(s),e(r),e(m),e(y),e(_),e(C),e(G),e($),e(x),e(p),e(X),e(v),e(Z),e(k),e(W),e(E),e(Y),e(B)),b(M,n)}}}function de(A){let s,g='For architectures that don’t support fused modules, create an <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.AwqConfig">AwqConfig</a> and define a custom fusing mapping in <code>modules_to_fuse</code> to determine which modules need to be fused.',r,m,u='The example below fuses the AWQ modules of the <a href="https://huggingface.co/TheBloke/Yi-34B-AWQ" rel="nofollow">TheBloke/Yi-34B-AWQ</a> model.',y,M,_,C,I="The parameter <code>modules_to_fuse</code> should include the following keys.",G,$,Q="<li><p><code>&quot;attention&quot;</code>: The names of the attention layers to fuse in the following order: query, key, value and output projection layer. If you don’t want to fuse these layers, pass an empty list.</p></li> <li><p><code>&quot;layernorm&quot;</code>: The names of all the LayerNorm layers you want to replace with a custom fused LayerNorm. If you don’t want to fuse these layers, pass an empty list.</p></li> <li><p><code>&quot;mlp&quot;</code>: The names of the MLP layers you want to fuse into a single MLP layer in the order: (gate (dense, layer, post-attention) / up / down layers).</p></li> <li><p><code>&quot;use_alibi&quot;</code>: If your model uses ALiBi positional embedding.</p></li> <li><p><code>&quot;num_attention_heads&quot;</code>: The number of attention heads.</p></li> <li><p><code>&quot;num_key_value_heads&quot;</code>: The number of key value heads that should be used to implement Grouped Query Attention (GQA).</p> <table><thead><tr><th>parameter value</th> <th>attention</th></tr></thead> <tbody><tr><td><code>num_key_value_heads=num_attention_heads</code></td> <td>Multi-Head Attention</td></tr> <tr><td><code>num_key_value_heads=1</code></td> <td>Multi-Query Attention</td></tr> <tr><td><code>num_key_value_heads=...</code></td> <td>Grouped Query Attention</td></tr></tbody></table></li> <li><p><code>&quot;hidden_size&quot;</code>: The dimension of the hidden representations.</p></li>",x;return M=new z({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXdxQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwQXdxQ29uZmlnKCUwQSUyMCUyMCUyMCUyMGJpdHMlM0Q0JTJDJTBBJTIwJTIwJTIwJTIwZnVzZV9tYXhfc2VxX2xlbiUzRDUxMiUyQyUwQSUyMCUyMCUyMCUyMG1vZHVsZXNfdG9fZnVzZSUzRCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmF0dGVudGlvbiUyMiUzQSUyMCU1QiUyMnFfcHJvaiUyMiUyQyUyMCUyMmtfcHJvaiUyMiUyQyUyMCUyMnZfcHJvaiUyMiUyQyUyMCUyMm9fcHJvaiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmxheWVybm9ybSUyMiUzQSUyMCU1QiUyMmxuMSUyMiUyQyUyMCUyMmxuMiUyMiUyQyUyMCUyMm5vcm0lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJtbHAlMjIlM0ElMjAlNUIlMjJnYXRlX3Byb2olMjIlMkMlMjAlMjJ1cF9wcm9qJTIyJTJDJTIwJTIyZG93bl9wcm9qJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIydXNlX2FsaWJpJTIyJTNBJTIwRmFsc2UlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJudW1fYXR0ZW50aW9uX2hlYWRzJTIyJTNBJTIwNTYlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJudW1fa2V5X3ZhbHVlX2hlYWRzJTIyJTNBJTIwOCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmhpZGRlbl9zaXplJTIyJTNBJTIwNzE2OCUwQSUyMCUyMCUyMCUyMCU3RCUwQSklMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjJUaGVCbG9rZSUyRllpLTM0Qi1BV1ElMjIlMkMlMEElMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUwQSkudG8oMCk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AwqConfig, AutoModelForCausalLM

quantization_config = AwqConfig(
    bits=<span class="hljs-number">4</span>,
    fuse_max_seq_len=<span class="hljs-number">512</span>,
    modules_to_fuse={
        <span class="hljs-string">&quot;attention&quot;</span>: [<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>],
        <span class="hljs-string">&quot;layernorm&quot;</span>: [<span class="hljs-string">&quot;ln1&quot;</span>, <span class="hljs-string">&quot;ln2&quot;</span>, <span class="hljs-string">&quot;norm&quot;</span>],
        <span class="hljs-string">&quot;mlp&quot;</span>: [<span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],
        <span class="hljs-string">&quot;use_alibi&quot;</span>: <span class="hljs-literal">False</span>,
        <span class="hljs-string">&quot;num_attention_heads&quot;</span>: <span class="hljs-number">56</span>,
        <span class="hljs-string">&quot;num_key_value_heads&quot;</span>: <span class="hljs-number">8</span>,
        <span class="hljs-string">&quot;hidden_size&quot;</span>: <span class="hljs-number">7168</span>
    }
)

model = AutoModelForCausalLM.from_pretrained(
  <span class="hljs-string">&quot;TheBloke/Yi-34B-AWQ&quot;</span>,
  quantization_config=quantization_config
).to(<span class="hljs-number">0</span>)`,wrap:!1}}),{c(){s=d("p"),s.innerHTML=g,r=i(),m=d("p"),m.innerHTML=u,y=i(),T(M.$$.fragment),_=i(),C=d("p"),C.innerHTML=I,G=i(),$=d("ul"),$.innerHTML=Q},l(p){s=h(p,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1s1hvyo"&&(s.innerHTML=g),r=o(p),m=h(p,"P",{"data-svelte-h":!0}),f(m)!=="svelte-1lewtu7"&&(m.innerHTML=u),y=o(p),w(M.$$.fragment,p),_=o(p),C=h(p,"P",{"data-svelte-h":!0}),f(C)!=="svelte-1ed6mmh"&&(C.innerHTML=I),G=o(p),$=h(p,"UL",{"data-svelte-h":!0}),f($)!=="svelte-1w446wg"&&($.innerHTML=Q)},m(p,q){a(p,s,q),a(p,r,q),a(p,m,q),a(p,y,q),J(M,p,q),a(p,_,q),a(p,C,q),a(p,G,q),a(p,$,q),x=!0},p:gt,i(p){x||(U(M.$$.fragment,p),x=!0)},o(p){j(M.$$.fragment,p),x=!1},d(p){p&&(e(s),e(r),e(m),e(y),e(_),e(C),e(G),e($)),b(M,p)}}}function he(A){let s,g,r,m;return s=new Kt({props:{id:"fuse",option:"supported architectures",$$slots:{default:[ue]},$$scope:{ctx:A}}}),r=new Kt({props:{id:"fuse",option:"unsupported architectures",$$slots:{default:[de]},$$scope:{ctx:A}}}),{c(){T(s.$$.fragment),g=i(),T(r.$$.fragment)},l(u){w(s.$$.fragment,u),g=o(u),w(r.$$.fragment,u)},m(u,y){J(s,u,y),a(u,g,y),J(r,u,y),m=!0},p(u,y){const M={};y&2&&(M.$$scope={dirty:y,ctx:u}),s.$set(M);const _={};y&2&&(_.$$scope={dirty:y,ctx:u}),r.$set(_)},i(u){m||(U(s.$$.fragment,u),U(r.$$.fragment,u),m=!0)},o(u){j(s.$$.fragment,u),j(r.$$.fragment,u),m=!1},d(u){u&&e(g),b(s,u),b(r,u)}}}function me(A){let s,g="ExLlamaV2 is supported on AMD GPUs.";return{c(){s=d("p"),s.textContent=g},l(r){s=h(r,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1y6gcly"&&(s.textContent=g)},m(r,m){a(r,s,m)},p:gt,d(r){r&&e(s)}}}function fe(A){let s,g,r,m,u,y,M,_='<a href="https://hf.co/papers/2306.00978" rel="nofollow">Activation-aware Weight Quantization (AWQ)</a> preserves a small fraction of the weights that are important for LLM performance to compress a model to 4-bits with minimal performance degradation.',C,I,G='There are several libraries for quantizing models with the AWQ algorithm, such as <a href="https://github.com/mit-han-lab/llm-awq" rel="nofollow">llm-awq</a>, <a href="https://github.com/casper-hansen/AutoAWQ" rel="nofollow">autoawq</a> or <a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_inc" rel="nofollow">optimum-intel</a>. Transformers supports loading models quantized with the llm-awq and autoawq libraries. This guide will show you how to load models quantized with autoawq, but the process is similar for llm-awq quantized models.',$,Q,x="Run the command below to install autoawq",p,q,X,v,N,Z,k='Identify an AWQ-quantized model by checking the <code>quant_method</code> key in the models <a href="https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json" rel="nofollow">config.json</a> file.',S,W,E,L,Y='Load the AWQ-quantized model with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a>. This automatically sets the other weights to fp16 by default for performance reasons. Use the <code>dtype</code> parameter to load these other weights in a different format.',B,V,P="If the model is loaded on the CPU, use the <code>device_map</code> parameter to move it to an accelerator.",n,c,yt,D,Vt='Use <code>attn_implementation</code> to enable <a href="../perf_infer_gpu_one#flashattention-2">FlashAttention2</a> to further accelerate inference.',Tt,O,wt,K,Jt,tt,zt='Fused modules offer improved accuracy and performance. They are supported out-of-the-box for AWQ modules for <a href="https://huggingface.co/meta-llama" rel="nofollow">Llama</a> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" rel="nofollow">Mistral</a> architectures, but you can also fuse AWQ modules for unsupported architectures.',Ut,R,jt,F,bt,et,$t,lt,Xt='<a href="https://github.com/turboderp/exllamav2" rel="nofollow">ExLlamaV2</a> kernels support faster prefill and decoding. Run the command below to install the latest version of autoawq with ExLlamaV2 support.',Ct,at,qt,st,Rt='Set <code>version=&quot;exllama&quot;</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.AwqConfig">AwqConfig</a> to enable ExLlamaV2 kernels.',vt,H,_t,nt,At,it,It,ot,Ft='<a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/" rel="nofollow">Intel Extension for PyTorch (IPEX)</a> is designed to enable performance optimizations on Intel hardware. Run the command below to install the latest version of autoawq with IPEX support.',Qt,rt,Bt,pt,Ht='Set <code>version=&quot;ipex&quot;</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.AwqConfig">AwqConfig</a> to enable ExLlamaV2 kernels.',Gt,ut,xt,dt,Zt,ht,Nt='Run the AWQ demo <a href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY#scrollTo=Wwsg6nCwoThm" rel="nofollow">notebook</a> for more examples of how to quantize a model, push a quantized model to the Hub, and more.',Wt,mt,Et,ft,kt;return u=new ct({props:{title:"AWQ",local:"awq",headingTag:"h1"}}),q=new z({props:{code:"cGlwJTIwaW5zdGFsbCUyMGF1dG9hd3E=",highlighted:"pip install autoawq",wrap:!1}}),v=new Lt({props:{warning:!0,$$slots:{default:[re]},$$scope:{ctx:A}}}),W=new z({props:{code:"JTdCJTBBJTIwJTIwJTIyX25hbWVfb3JfcGF0aCUyMiUzQSUyMCUyMiUyRndvcmtzcGFjZSUyRnByb2Nlc3MlMkZodWdnaW5nZmFjZWg0X3plcGh5ci03Yi1hbHBoYSUyRnNvdXJjZSUyMiUyQyUwQSUyMCUyMCUyMmFyY2hpdGVjdHVyZXMlMjIlM0ElMjAlNUIlMEElMjAlMjAlMjAlMjAlMjJNaXN0cmFsRm9yQ2F1c2FsTE0lMjIlMEElMjAlMjAlNUQlMkMlMEElMjAlMjAuLi4lMEElMjAlMjAuLi4lMEElMjAlMjAuLi4lMEElMjAlMjAlMjJxdWFudGl6YXRpb25fY29uZmlnJTIyJTNBJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIycXVhbnRfbWV0aG9kJTIyJTNBJTIwJTIyYXdxJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIyemVyb19wb2ludCUyMiUzQSUyMHRydWUlMkMlMEElMjAlMjAlMjAlMjAlMjJncm91cF9zaXplJTIyJTNBJTIwMTI4JTJDJTBBJTIwJTIwJTIwJTIwJTIyYml0cyUyMiUzQSUyMDQlMkMlMEElMjAlMjAlMjAlMjAlMjJ2ZXJzaW9uJTIyJTNBJTIwJTIyZ2VtbSUyMiUwQSUyMCUyMCU3RCUwQSU3RA==",highlighted:`<span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;_name_or_path&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;/workspace/process/huggingfaceh4_zephyr-7b-alpha/source&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;architectures&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-string">&quot;MistralForCausalLM&quot;</span>
  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span>
  ...
  ...
  ...
  <span class="hljs-attr">&quot;quantization_config&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;quant_method&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;awq&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;zero_point&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;group_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">128</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;bits&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;gemm&quot;</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>`,wrap:!1}}),c=new z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMGluZmVyX2RldmljZSUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwZiUyMiU3QmluZmVyX2RldmljZSgpJTdEJTNBMCUyMiUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMlRoZUJsb2tlJTJGemVwaHlyLTdCLWFscGhhLUFXUSUyMiUyQyUwQSUyMCUyMGR0eXBlJTNEdG9yY2guZmxvYXQzMiUyQyUwQSUyMCUyMGRldmljZV9tYXAlM0RkZXZpY2UlMEEp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">f&quot;<span class="hljs-subst">{infer_device()}</span>:0&quot;</span>

model = AutoModelForCausalLM.from_pretrained(
  <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>,
  dtype=torch.float32,
  device_map=device
)`,wrap:!1}}),O=new z({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMlRoZUJsb2tlJTJGemVwaHlyLTdCLWFscGhhLUFXUSUyMiUyQyUwQSUyMCUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJmbGFzaF9hdHRlbnRpb25fMiUyMiUyQyUwQSUyMCUyMGRldmljZV9tYXAlM0QlMjJjdWRhJTNBMCUyMiUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
  <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>,
  attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
  device_map=<span class="hljs-string">&quot;cuda:0&quot;</span>
)`,wrap:!1}}),K=new ct({props:{title:"Fused modules",local:"fused-modules",headingTag:"h2"}}),R=new Lt({props:{warning:!0,$$slots:{default:[pe]},$$scope:{ctx:A}}}),F=new oe({props:{id:"fuse",options:["supported architectures","unsupported architectures"],$$slots:{default:[he]},$$scope:{ctx:A}}}),et=new ct({props:{title:"ExLlamaV2",local:"exllamav2",headingTag:"h2"}}),at=new z({props:{code:"cGlwJTIwaW5zdGFsbCUyMGdpdCUyQmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmNhc3Blci1oYW5zZW4lMkZBdXRvQVdRLmdpdA==",highlighted:"pip install git+https://github.com/casper-hansen/AutoAWQ.git",wrap:!1}}),H=new Lt({props:{warning:!1,$$slots:{default:[me]},$$scope:{ctx:A}}}),nt=new z({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQXdxQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEF3cUNvbmZpZyh2ZXJzaW9uJTNEJTIyZXhsbGFtYSUyMiklMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJUaGVCbG9rZSUyRk1pc3RyYWwtN0ItSW5zdHJ1Y3QtdjAuMS1BV1ElMjIlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, AwqConfig

quantization_config = AwqConfig(version=<span class="hljs-string">&quot;exllama&quot;</span>)

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;TheBloke/Mistral-7B-Instruct-v0.1-AWQ&quot;</span>,
    quantization_config=quantization_config,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
)`,wrap:!1}}),it=new ct({props:{title:"CPU",local:"cpu",headingTag:"h2"}}),rt=new z({props:{code:"cGlwJTIwaW5zdGFsbCUyMGludGVsLWV4dGVuc2lvbi1mb3ItcHl0b3JjaCUyMCUyMyUyMGZvciUyMElQRVgtR1BVJTIwcmVmZXIlMjB0byUyMGh0dHBzJTNBJTJGJTJGaW50ZWwuZ2l0aHViLmlvJTJGaW50ZWwtZXh0ZW5zaW9uLWZvci1weXRvcmNoJTJGeHB1JTJGMi41LjEwJTJCeHB1JTJGJTIwJTBBcGlwJTIwaW5zdGFsbCUyMGdpdCUyQmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmNhc3Blci1oYW5zZW4lMkZBdXRvQVdRLmdpdA==",highlighted:`pip install intel-extension-for-pytorch <span class="hljs-comment"># for IPEX-GPU refer to https://intel.github.io/intel-extension-for-pytorch/xpu/2.5.10+xpu/ </span>
pip install git+https://github.com/casper-hansen/AutoAWQ.git`,wrap:!1}}),ut=new z({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQXdxQ29uZmlnJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3B1JTIyJTIwJTIzJTIwc2V0JTIwdG8lMjAlMjJ4cHUlMjIlMjBmb3IlMjBJbnRlbCUyMEdQVSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBBd3FDb25maWcodmVyc2lvbiUzRCUyMmlwZXglMjIpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyVGhlQmxva2UlMkZUaW55TGxhbWEtMS4xQi1DaGF0LXYwLjMtQVdRJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEZGV2aWNlJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, AwqConfig

device = <span class="hljs-string">&quot;cpu&quot;</span> <span class="hljs-comment"># set to &quot;xpu&quot; for Intel GPU</span>
quantization_config = AwqConfig(version=<span class="hljs-string">&quot;ipex&quot;</span>)

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ&quot;</span>,
    quantization_config=quantization_config,
    device_map=device,
)`,wrap:!1}}),dt=new ct({props:{title:"Resources",local:"resources",headingTag:"h2"}}),mt=new ie({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/awq.md"}}),{c(){s=d("meta"),g=i(),r=d("p"),m=i(),T(u.$$.fragment),y=i(),M=d("p"),M.innerHTML=_,C=i(),I=d("p"),I.innerHTML=G,$=i(),Q=d("p"),Q.textContent=x,p=i(),T(q.$$.fragment),X=i(),T(v.$$.fragment),N=i(),Z=d("p"),Z.innerHTML=k,S=i(),T(W.$$.fragment),E=i(),L=d("p"),L.innerHTML=Y,B=i(),V=d("p"),V.innerHTML=P,n=i(),T(c.$$.fragment),yt=i(),D=d("p"),D.innerHTML=Vt,Tt=i(),T(O.$$.fragment),wt=i(),T(K.$$.fragment),Jt=i(),tt=d("p"),tt.innerHTML=zt,Ut=i(),T(R.$$.fragment),jt=i(),T(F.$$.fragment),bt=i(),T(et.$$.fragment),$t=i(),lt=d("p"),lt.innerHTML=Xt,Ct=i(),T(at.$$.fragment),qt=i(),st=d("p"),st.innerHTML=Rt,vt=i(),T(H.$$.fragment),_t=i(),T(nt.$$.fragment),At=i(),T(it.$$.fragment),It=i(),ot=d("p"),ot.innerHTML=Ft,Qt=i(),T(rt.$$.fragment),Bt=i(),pt=d("p"),pt.innerHTML=Ht,Gt=i(),T(ut.$$.fragment),xt=i(),T(dt.$$.fragment),Zt=i(),ht=d("p"),ht.innerHTML=Nt,Wt=i(),T(mt.$$.fragment),Et=i(),ft=d("p"),this.h()},l(t){const l=se("svelte-u9bgzb",document.head);s=h(l,"META",{name:!0,content:!0}),l.forEach(e),g=o(t),r=h(t,"P",{}),Ot(r).forEach(e),m=o(t),w(u.$$.fragment,t),y=o(t),M=h(t,"P",{"data-svelte-h":!0}),f(M)!=="svelte-1b3motd"&&(M.innerHTML=_),C=o(t),I=h(t,"P",{"data-svelte-h":!0}),f(I)!=="svelte-vomabp"&&(I.innerHTML=G),$=o(t),Q=h(t,"P",{"data-svelte-h":!0}),f(Q)!=="svelte-14s6he6"&&(Q.textContent=x),p=o(t),w(q.$$.fragment,t),X=o(t),w(v.$$.fragment,t),N=o(t),Z=h(t,"P",{"data-svelte-h":!0}),f(Z)!=="svelte-og22vq"&&(Z.innerHTML=k),S=o(t),w(W.$$.fragment,t),E=o(t),L=h(t,"P",{"data-svelte-h":!0}),f(L)!=="svelte-81v4dk"&&(L.innerHTML=Y),B=o(t),V=h(t,"P",{"data-svelte-h":!0}),f(V)!=="svelte-1pq80c7"&&(V.innerHTML=P),n=o(t),w(c.$$.fragment,t),yt=o(t),D=h(t,"P",{"data-svelte-h":!0}),f(D)!=="svelte-42py21"&&(D.innerHTML=Vt),Tt=o(t),w(O.$$.fragment,t),wt=o(t),w(K.$$.fragment,t),Jt=o(t),tt=h(t,"P",{"data-svelte-h":!0}),f(tt)!=="svelte-32kkq6"&&(tt.innerHTML=zt),Ut=o(t),w(R.$$.fragment,t),jt=o(t),w(F.$$.fragment,t),bt=o(t),w(et.$$.fragment,t),$t=o(t),lt=h(t,"P",{"data-svelte-h":!0}),f(lt)!=="svelte-1frcaex"&&(lt.innerHTML=Xt),Ct=o(t),w(at.$$.fragment,t),qt=o(t),st=h(t,"P",{"data-svelte-h":!0}),f(st)!=="svelte-61sd2r"&&(st.innerHTML=Rt),vt=o(t),w(H.$$.fragment,t),_t=o(t),w(nt.$$.fragment,t),At=o(t),w(it.$$.fragment,t),It=o(t),ot=h(t,"P",{"data-svelte-h":!0}),f(ot)!=="svelte-30l4yi"&&(ot.innerHTML=Ft),Qt=o(t),w(rt.$$.fragment,t),Bt=o(t),pt=h(t,"P",{"data-svelte-h":!0}),f(pt)!=="svelte-poec0j"&&(pt.innerHTML=Ht),Gt=o(t),w(ut.$$.fragment,t),xt=o(t),w(dt.$$.fragment,t),Zt=o(t),ht=h(t,"P",{"data-svelte-h":!0}),f(ht)!=="svelte-o4txta"&&(ht.innerHTML=Nt),Wt=o(t),w(mt.$$.fragment,t),Et=o(t),ft=h(t,"P",{}),Ot(ft).forEach(e),this.h()},h(){Mt(s,"name","hf:doc:metadata"),Mt(s,"content",ce)},m(t,l){ne(document.head,s),a(t,g,l),a(t,r,l),a(t,m,l),J(u,t,l),a(t,y,l),a(t,M,l),a(t,C,l),a(t,I,l),a(t,$,l),a(t,Q,l),a(t,p,l),J(q,t,l),a(t,X,l),J(v,t,l),a(t,N,l),a(t,Z,l),a(t,S,l),J(W,t,l),a(t,E,l),a(t,L,l),a(t,B,l),a(t,V,l),a(t,n,l),J(c,t,l),a(t,yt,l),a(t,D,l),a(t,Tt,l),J(O,t,l),a(t,wt,l),J(K,t,l),a(t,Jt,l),a(t,tt,l),a(t,Ut,l),J(R,t,l),a(t,jt,l),J(F,t,l),a(t,bt,l),J(et,t,l),a(t,$t,l),a(t,lt,l),a(t,Ct,l),J(at,t,l),a(t,qt,l),a(t,st,l),a(t,vt,l),J(H,t,l),a(t,_t,l),J(nt,t,l),a(t,At,l),J(it,t,l),a(t,It,l),a(t,ot,l),a(t,Qt,l),J(rt,t,l),a(t,Bt,l),a(t,pt,l),a(t,Gt,l),J(ut,t,l),a(t,xt,l),J(dt,t,l),a(t,Zt,l),a(t,ht,l),a(t,Wt,l),J(mt,t,l),a(t,Et,l),a(t,ft,l),kt=!0},p(t,[l]){const St={};l&2&&(St.$$scope={dirty:l,ctx:t}),v.$set(St);const Yt={};l&2&&(Yt.$$scope={dirty:l,ctx:t}),R.$set(Yt);const Pt={};l&2&&(Pt.$$scope={dirty:l,ctx:t}),F.$set(Pt);const Dt={};l&2&&(Dt.$$scope={dirty:l,ctx:t}),H.$set(Dt)},i(t){kt||(U(u.$$.fragment,t),U(q.$$.fragment,t),U(v.$$.fragment,t),U(W.$$.fragment,t),U(c.$$.fragment,t),U(O.$$.fragment,t),U(K.$$.fragment,t),U(R.$$.fragment,t),U(F.$$.fragment,t),U(et.$$.fragment,t),U(at.$$.fragment,t),U(H.$$.fragment,t),U(nt.$$.fragment,t),U(it.$$.fragment,t),U(rt.$$.fragment,t),U(ut.$$.fragment,t),U(dt.$$.fragment,t),U(mt.$$.fragment,t),kt=!0)},o(t){j(u.$$.fragment,t),j(q.$$.fragment,t),j(v.$$.fragment,t),j(W.$$.fragment,t),j(c.$$.fragment,t),j(O.$$.fragment,t),j(K.$$.fragment,t),j(R.$$.fragment,t),j(F.$$.fragment,t),j(et.$$.fragment,t),j(at.$$.fragment,t),j(H.$$.fragment,t),j(nt.$$.fragment,t),j(it.$$.fragment,t),j(rt.$$.fragment,t),j(ut.$$.fragment,t),j(dt.$$.fragment,t),j(mt.$$.fragment,t),kt=!1},d(t){t&&(e(g),e(r),e(m),e(y),e(M),e(C),e(I),e($),e(Q),e(p),e(X),e(N),e(Z),e(S),e(E),e(L),e(B),e(V),e(n),e(yt),e(D),e(Tt),e(wt),e(Jt),e(tt),e(Ut),e(jt),e(bt),e($t),e(lt),e(Ct),e(qt),e(st),e(vt),e(_t),e(At),e(It),e(ot),e(Qt),e(Bt),e(pt),e(Gt),e(xt),e(Zt),e(ht),e(Wt),e(Et),e(ft)),e(s),b(u,t),b(q,t),b(v,t),b(W,t),b(c,t),b(O,t),b(K,t),b(R,t),b(F,t),b(et,t),b(at,t),b(H,t),b(nt,t),b(it,t),b(rt,t),b(ut,t),b(dt,t),b(mt,t)}}}const ce='{"title":"AWQ","local":"awq","sections":[{"title":"Fused modules","local":"fused-modules","sections":[],"depth":2},{"title":"ExLlamaV2","local":"exllamav2","sections":[],"depth":2},{"title":"CPU","local":"cpu","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2}],"depth":1}';function Me(A){return ee(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class je extends le{constructor(s){super(),ae(this,s,Me,fe,te,{})}}export{je as component};
