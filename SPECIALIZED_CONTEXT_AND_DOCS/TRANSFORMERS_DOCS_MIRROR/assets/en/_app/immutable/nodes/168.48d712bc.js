import{s as Dt,o as Vt,n as je}from"../chunks/scheduler.18a86fab.js";import{S as At,i as Ht,g as l,s as r,r as p,A as Ot,h as d,f as o,c as a,j,x as v,u as g,k as M,y as c,a as i,v as h,d as u,t as _,w as b}from"../chunks/index.98837b22.js";import{T as St}from"../chunks/Tip.77304350.js";import{D as Z}from"../chunks/Docstring.a1ef7999.js";import{C as $t}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Nt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ue,E as Bt}from"../chunks/getInferenceSnippets.06c2775f.js";function Jt(x){let n,N="Example:",f,m,y;return m=new $t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEVmZmljaWVudE5ldENvbmZpZyUyQyUyMEVmZmljaWVudE5ldE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEVmZmljaWVudE5ldCUyMGVmZmljaWVudG5ldC1iNyUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBFZmZpY2llbnROZXRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwZWZmaWNpZW50bmV0LWI3JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBFZmZpY2llbnROZXRNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EfficientNetConfig, EfficientNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a EfficientNet efficientnet-b7 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = EfficientNetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the efficientnet-b7 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EfficientNetModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=l("p"),n.textContent=N,f=r(),p(m.$$.fragment)},l(s){n=d(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=N),f=a(s),g(m.$$.fragment,s)},m(s,$){i(s,n,$),i(s,f,$),h(m,s,$),y=!0},p:je,i(s){y||(u(m.$$.fragment,s),y=!0)},o(s){_(m.$$.fragment,s),y=!1},d(s){s&&(o(n),o(f)),b(m,s)}}}function Gt(x){let n,N=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=l("p"),n.innerHTML=N},l(f){n=d(f,"P",{"data-svelte-h":!0}),v(n)!=="svelte-fincs2"&&(n.innerHTML=N)},m(f,m){i(f,n,m)},p:je,d(f){f&&o(n)}}}function Qt(x){let n,N="Example:",f,m,y;return m=new $t({props:{code:"",highlighted:"",wrap:!1}}),{c(){n=l("p"),n.textContent=N,f=r(),p(m.$$.fragment)},l(s){n=d(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=N),f=a(s),g(m.$$.fragment,s)},m(s,$){i(s,n,$),i(s,f,$),h(m,s,$),y=!0},p:je,i(s){y||(u(m.$$.fragment,s),y=!0)},o(s){_(m.$$.fragment,s),y=!1},d(s){s&&(o(n),o(f)),b(m,s)}}}function Yt(x){let n,N=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=l("p"),n.innerHTML=N},l(f){n=d(f,"P",{"data-svelte-h":!0}),v(n)!=="svelte-fincs2"&&(n.innerHTML=N)},m(f,m){i(f,n,m)},p:je,d(f){f&&o(n)}}}function Xt(x){let n,N="Example:",f,m,y;return m=new $t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEVmZmljaWVudE5ldEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmVmZmljaWVudG5ldC1iNyUyMiklMEFtb2RlbCUyMCUzRCUyMEVmZmljaWVudE5ldEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmVmZmljaWVudG5ldC1iNyUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, EfficientNetForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/efficientnet-b7&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EfficientNetForImageClassification.from_pretrained(<span class="hljs-string">&quot;google/efficientnet-b7&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){n=l("p"),n.textContent=N,f=r(),p(m.$$.fragment)},l(s){n=d(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=N),f=a(s),g(m.$$.fragment,s)},m(s,$){i(s,n,$),i(s,f,$),h(m,s,$),y=!0},p:je,i(s){y||(u(m.$$.fragment,s),y=!0)},o(s){_(m.$$.fragment,s),y=!1},d(s){s&&(o(n),o(f)),b(m,s)}}}function Kt(x){let n,N,f,m,y,s="<em>This model was released on 2019-05-28 and added to Hugging Face Transformers on 2023-02-20.</em>",$,J,Ue,L,wt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',We,G,ke,Q,Et=`The EfficientNet model was proposed in <a href="https://huggingface.co/papers/1905.11946" rel="nofollow">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a>
by Mingxing Tan and Quoc V. Le. EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.`,Re,Y,Tt="The abstract from the paper is the following:",Ze,X,xt=`<em>Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.
To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.</em>`,Le,K,It=`This model was contributed by <a href="https://huggingface.co/adirik" rel="nofollow">adirik</a>.
The original code can be found <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" rel="nofollow">here</a>.`,qe,ee,Se,T,te,Ke,_e,Ct=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a>. It is used to instantiate an
EfficientNet model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the EfficientNet
<a href="https://huggingface.co/google/efficientnet-b7" rel="nofollow">google/efficientnet-b7</a> architecture.`,et,be,Mt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,tt,q,De,oe,Ve,z,ne,ot,ve,zt="Constructs a EfficientNet image processor.",nt,S,se,st,ye,Pt="Preprocess an image or batch of images.",Ae,re,He,P,ae,rt,Ne,Ft="Constructs a fast Efficientnet image processor.",at,$e,ie,Oe,ce,Be,w,le,it,we,jt="The bare Efficientnet Model outputting raw hidden-states without any specific head on top.",ct,Ee,Ut=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,lt,Te,Wt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,dt,I,de,ft,xe,kt='The <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a> forward method, overrides the <code>__call__</code> special method.',mt,D,pt,V,Je,fe,Ge,E,me,gt,Ie,Rt=`EfficientNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g.
for ImageNet.`,ht,Ce,Zt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ut,Me,Lt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,_t,C,pe,bt,ze,qt='The <a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',vt,A,yt,H,Qe,ge,Ye,Pe,Xe;return J=new ue({props:{title:"EfficientNet",local:"efficientnet",headingTag:"h1"}}),G=new ue({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ee=new ue({props:{title:"EfficientNetConfig",local:"transformers.EfficientNetConfig",headingTag:"h2"}}),te=new Z({props:{name:"class transformers.EfficientNetConfig",anchor:"transformers.EfficientNetConfig",parameters:[{name:"num_channels",val:": int = 3"},{name:"image_size",val:": int = 600"},{name:"width_coefficient",val:": float = 2.0"},{name:"depth_coefficient",val:": float = 3.1"},{name:"depth_divisor",val:": int = 8"},{name:"kernel_sizes",val:": list = [3, 3, 5, 3, 5, 5, 3]"},{name:"in_channels",val:": list = [32, 16, 24, 40, 80, 112, 192]"},{name:"out_channels",val:": list = [16, 24, 40, 80, 112, 192, 320]"},{name:"depthwise_padding",val:": list = []"},{name:"strides",val:": list = [1, 2, 2, 2, 1, 2, 1]"},{name:"num_block_repeats",val:": list = [1, 2, 2, 3, 3, 4, 1]"},{name:"expand_ratios",val:": list = [1, 6, 6, 6, 6, 6, 6]"},{name:"squeeze_expansion_ratio",val:": float = 0.25"},{name:"hidden_act",val:": str = 'swish'"},{name:"hidden_dim",val:": int = 2560"},{name:"pooling_type",val:": str = 'mean'"},{name:"initializer_range",val:": float = 0.02"},{name:"batch_norm_eps",val:": float = 0.001"},{name:"batch_norm_momentum",val:": float = 0.99"},{name:"dropout_rate",val:": float = 0.5"},{name:"drop_connect_rate",val:": float = 0.2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientNetConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.EfficientNetConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 600) &#x2014;
The input image size.`,name:"image_size"},{anchor:"transformers.EfficientNetConfig.width_coefficient",description:`<strong>width_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Scaling coefficient for network width at each stage.`,name:"width_coefficient"},{anchor:"transformers.EfficientNetConfig.depth_coefficient",description:`<strong>depth_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 3.1) &#x2014;
Scaling coefficient for network depth at each stage.`,name:"depth_coefficient"},{anchor:"transformers.EfficientNetConfig.depth_divisor",description:`<strong>depth_divisor</strong> <code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
A unit of network width.`,name:"depth_divisor"},{anchor:"transformers.EfficientNetConfig.kernel_sizes",description:`<strong>kernel_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 5, 3, 5, 5, 3]</code>) &#x2014;
List of kernel sizes to be used in each block.`,name:"kernel_sizes"},{anchor:"transformers.EfficientNetConfig.in_channels",description:`<strong>in_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[32, 16, 24, 40, 80, 112, 192]</code>) &#x2014;
List of input channel sizes to be used in each block for convolutional layers.`,name:"in_channels"},{anchor:"transformers.EfficientNetConfig.out_channels",description:`<strong>out_channels</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[16, 24, 40, 80, 112, 192, 320]</code>) &#x2014;
List of output channel sizes to be used in each block for convolutional layers.`,name:"out_channels"},{anchor:"transformers.EfficientNetConfig.depthwise_padding",description:`<strong>depthwise_padding</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of block indices with square padding.`,name:"depthwise_padding"},{anchor:"transformers.EfficientNetConfig.strides",description:`<strong>strides</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 2, 1, 2, 1]</code>) &#x2014;
List of stride sizes to be used in each block for convolutional layers.`,name:"strides"},{anchor:"transformers.EfficientNetConfig.num_block_repeats",description:`<strong>num_block_repeats</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 3, 3, 4, 1]</code>) &#x2014;
List of the number of times each block is to repeated.`,name:"num_block_repeats"},{anchor:"transformers.EfficientNetConfig.expand_ratios",description:`<strong>expand_ratios</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 6, 6, 6, 6, 6, 6]</code>) &#x2014;
List of scaling coefficient of each block.`,name:"expand_ratios"},{anchor:"transformers.EfficientNetConfig.squeeze_expansion_ratio",description:`<strong>squeeze_expansion_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Squeeze expansion ratio.`,name:"squeeze_expansion_ratio"},{anchor:"transformers.EfficientNetConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;, </code>&#x201C;gelu_new&#x201D;<code>, </code>&#x201C;silu&#x201D;<code>and</code>&#x201C;mish&#x201D;\` are supported.`,name:"hidden_act"},{anchor:"transformers.EfficientNetConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
The hidden dimension of the layer before the classification head.`,name:"hidden_dim"},{anchor:"transformers.EfficientNetConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Type of final pooling to be applied before the dense classification head. Available options are [<code>&quot;mean&quot;</code>,
<code>&quot;max&quot;</code>]`,name:"pooling_type"},{anchor:"transformers.EfficientNetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.EfficientNetConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"},{anchor:"transformers.EfficientNetConfig.batch_norm_momentum",description:`<strong>batch_norm_momentum</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;
The momentum used by the batch normalization layers.`,name:"batch_norm_momentum"},{anchor:"transformers.EfficientNetConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout rate to be applied before final classifier layer.`,name:"dropout_rate"},{anchor:"transformers.EfficientNetConfig.drop_connect_rate",description:`<strong>drop_connect_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The drop rate for skip connections.`,name:"drop_connect_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/configuration_efficientnet.py#L30"}}),q=new Nt({props:{anchor:"transformers.EfficientNetConfig.example",$$slots:{default:[Jt]},$$scope:{ctx:x}}}),oe=new ue({props:{title:"EfficientNetImageProcessor",local:"transformers.EfficientNetImageProcessor",headingTag:"h2"}}),ne=new Z({props:{name:"class transformers.EfficientNetImageProcessor",anchor:"transformers.EfficientNetImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = 0"},{name:"do_center_crop",val:": bool = False"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"rescale_offset",val:": bool = False"},{name:"do_rescale",val:": bool = True"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"include_top",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.EfficientNetImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by
<code>do_resize</code> in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.EfficientNetImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 346, &quot;width&quot;: 346}</code>):
Size of the image after <code>resize</code>. Can be overridden by <code>size</code> in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.EfficientNetImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code> filter, <em>optional</em>, defaults to 0) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in <code>preprocess</code>.`,name:"resample"},{anchor:"transformers.EfficientNetImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to center crop the image. If the input size is smaller than <code>crop_size</code> along any edge, the image
is padded with 0&#x2019;s and then center cropped. Can be overridden by <code>do_center_crop</code> in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.EfficientNetImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 289, &quot;width&quot;: 289}</code>):
Desired output size when applying center-cropping. Can be overridden by <code>crop_size</code> in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.EfficientNetImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.EfficientNetImageProcessor.rescale_offset",description:`<strong>rescale_offset</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to rescale the image between [-scale_range, scale_range] instead of [0, scale_range]. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_offset"},{anchor:"transformers.EfficientNetImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.EfficientNetImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.EfficientNetImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.EfficientNetImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.EfficientNetImageProcessor.include_top",description:`<strong>include_top</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image again. Should be set to True if the inputs are used for image classification.`,name:"include_top"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/image_processing_efficientnet.py#L46"}}),se=new Z({props:{name:"preprocess",anchor:"transformers.EfficientNetImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:" = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"rescale_offset",val:": typing.Optional[bool] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"include_top",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.EfficientNetImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after <code>resize</code>.`,name:"size"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
PILImageResampling filter to use if resizing the image Only has an effect if <code>do_resize</code> is set to
<code>True</code>.`,name:"resample"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the image after center crop. If one edge the image is smaller than <code>crop_size</code>, it will be
padded with zeros and then cropped`,name:"crop_size"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.rescale_offset",description:`<strong>rescale_offset</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.rescale_offset</code>) &#x2014;
Whether to rescale the image between [-scale_range, scale_range] instead of [0, scale_range].`,name:"rescale_offset"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.include_top",description:`<strong>include_top</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.include_top</code>) &#x2014;
Rescales the image again for image classification if set to True.`,name:"include_top"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li><code>None</code>: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.EfficientNetImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/image_processing_efficientnet.py#L211"}}),re=new ue({props:{title:"EfficientNetImageProcessorFast",local:"transformers.EfficientNetImageProcessorFast",headingTag:"h2"}}),ae=new Z({props:{name:"class transformers.EfficientNetImageProcessorFast",anchor:"transformers.EfficientNetImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.efficientnet.image_processing_efficientnet_fast.EfficientNetFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/image_processing_efficientnet_fast.py#L57"}}),ie=new Z({props:{name:"preprocess",anchor:"transformers.EfficientNetImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.efficientnet.image_processing_efficientnet_fast.EfficientNetFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.rescale_offset",description:`<strong>rescale_offset</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.rescale_offset</code>) &#x2014;
Whether to rescale the image between [-max_range/2, scale_range/2] instead of [0, scale_range].`,name:"rescale_offset"},{anchor:"transformers.EfficientNetImageProcessorFast.preprocess.include_top",description:`<strong>include_top</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.include_top</code>) &#x2014;
Normalize the image again with the standard deviation only for image classification if set to True.`,name:"include_top"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/image_processing_efficientnet_fast.py#L207",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),ce=new ue({props:{title:"EfficientNetModel",local:"transformers.EfficientNetModel",headingTag:"h2"}}),le=new Z({props:{name:"class transformers.EfficientNetModel",anchor:"transformers.EfficientNetModel",parameters:[{name:"config",val:": EfficientNetConfig"}],parametersDescription:[{anchor:"transformers.EfficientNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/modeling_efficientnet.py#L451"}}),de=new Z({props:{name:"forward",anchor:"transformers.EfficientNetModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.EfficientNetModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.EfficientNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.EfficientNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/modeling_efficientnet.py#L469",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig"
>EfficientNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),D=new St({props:{$$slots:{default:[Gt]},$$scope:{ctx:x}}}),V=new Nt({props:{anchor:"transformers.EfficientNetModel.forward.example",$$slots:{default:[Qt]},$$scope:{ctx:x}}}),fe=new ue({props:{title:"EfficientNetForImageClassification",local:"transformers.EfficientNetForImageClassification",headingTag:"h2"}}),me=new Z({props:{name:"class transformers.EfficientNetForImageClassification",anchor:"transformers.EfficientNetForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.EfficientNetForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/modeling_efficientnet.py#L513"}}),pe=new Z({props:{name:"forward",anchor:"transformers.EfficientNetForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.EfficientNetForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.EfficientNetForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.EfficientNetForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.EfficientNetForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/efficientnet/modeling_efficientnet.py#L526",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/efficientnet#transformers.EfficientNetConfig"
>EfficientNetConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),A=new St({props:{$$slots:{default:[Yt]},$$scope:{ctx:x}}}),H=new Nt({props:{anchor:"transformers.EfficientNetForImageClassification.forward.example",$$slots:{default:[Xt]},$$scope:{ctx:x}}}),ge=new Bt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/efficientnet.md"}}),{c(){n=l("meta"),N=r(),f=l("p"),m=r(),y=l("p"),y.innerHTML=s,$=r(),p(J.$$.fragment),Ue=r(),L=l("div"),L.innerHTML=wt,We=r(),p(G.$$.fragment),ke=r(),Q=l("p"),Q.innerHTML=Et,Re=r(),Y=l("p"),Y.textContent=Tt,Ze=r(),X=l("p"),X.innerHTML=xt,Le=r(),K=l("p"),K.innerHTML=It,qe=r(),p(ee.$$.fragment),Se=r(),T=l("div"),p(te.$$.fragment),Ke=r(),_e=l("p"),_e.innerHTML=Ct,et=r(),be=l("p"),be.innerHTML=Mt,tt=r(),p(q.$$.fragment),De=r(),p(oe.$$.fragment),Ve=r(),z=l("div"),p(ne.$$.fragment),ot=r(),ve=l("p"),ve.textContent=zt,nt=r(),S=l("div"),p(se.$$.fragment),st=r(),ye=l("p"),ye.textContent=Pt,Ae=r(),p(re.$$.fragment),He=r(),P=l("div"),p(ae.$$.fragment),rt=r(),Ne=l("p"),Ne.textContent=Ft,at=r(),$e=l("div"),p(ie.$$.fragment),Oe=r(),p(ce.$$.fragment),Be=r(),w=l("div"),p(le.$$.fragment),it=r(),we=l("p"),we.textContent=jt,ct=r(),Ee=l("p"),Ee.innerHTML=Ut,lt=r(),Te=l("p"),Te.innerHTML=Wt,dt=r(),I=l("div"),p(de.$$.fragment),ft=r(),xe=l("p"),xe.innerHTML=kt,mt=r(),p(D.$$.fragment),pt=r(),p(V.$$.fragment),Je=r(),p(fe.$$.fragment),Ge=r(),E=l("div"),p(me.$$.fragment),gt=r(),Ie=l("p"),Ie.textContent=Rt,ht=r(),Ce=l("p"),Ce.innerHTML=Zt,ut=r(),Me=l("p"),Me.innerHTML=Lt,_t=r(),C=l("div"),p(pe.$$.fragment),bt=r(),ze=l("p"),ze.innerHTML=qt,vt=r(),p(A.$$.fragment),yt=r(),p(H.$$.fragment),Qe=r(),p(ge.$$.fragment),Ye=r(),Pe=l("p"),this.h()},l(e){const t=Ot("svelte-u9bgzb",document.head);n=d(t,"META",{name:!0,content:!0}),t.forEach(o),N=a(e),f=d(e,"P",{}),j(f).forEach(o),m=a(e),y=d(e,"P",{"data-svelte-h":!0}),v(y)!=="svelte-1f9k7nu"&&(y.innerHTML=s),$=a(e),g(J.$$.fragment,e),Ue=a(e),L=d(e,"DIV",{class:!0,"data-svelte-h":!0}),v(L)!=="svelte-13t8s2t"&&(L.innerHTML=wt),We=a(e),g(G.$$.fragment,e),ke=a(e),Q=d(e,"P",{"data-svelte-h":!0}),v(Q)!=="svelte-1lokf68"&&(Q.innerHTML=Et),Re=a(e),Y=d(e,"P",{"data-svelte-h":!0}),v(Y)!=="svelte-vfdo9a"&&(Y.textContent=Tt),Ze=a(e),X=d(e,"P",{"data-svelte-h":!0}),v(X)!=="svelte-1ljdnse"&&(X.innerHTML=xt),Le=a(e),K=d(e,"P",{"data-svelte-h":!0}),v(K)!=="svelte-vnjq51"&&(K.innerHTML=It),qe=a(e),g(ee.$$.fragment,e),Se=a(e),T=d(e,"DIV",{class:!0});var F=j(T);g(te.$$.fragment,F),Ke=a(F),_e=d(F,"P",{"data-svelte-h":!0}),v(_e)!=="svelte-12cw4af"&&(_e.innerHTML=Ct),et=a(F),be=d(F,"P",{"data-svelte-h":!0}),v(be)!=="svelte-1ek1ss9"&&(be.innerHTML=Mt),tt=a(F),g(q.$$.fragment,F),F.forEach(o),De=a(e),g(oe.$$.fragment,e),Ve=a(e),z=d(e,"DIV",{class:!0});var k=j(z);g(ne.$$.fragment,k),ot=a(k),ve=d(k,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-6r2o1m"&&(ve.textContent=zt),nt=a(k),S=d(k,"DIV",{class:!0});var he=j(S);g(se.$$.fragment,he),st=a(he),ye=d(he,"P",{"data-svelte-h":!0}),v(ye)!=="svelte-1x3yxsa"&&(ye.textContent=Pt),he.forEach(o),k.forEach(o),Ae=a(e),g(re.$$.fragment,e),He=a(e),P=d(e,"DIV",{class:!0});var R=j(P);g(ae.$$.fragment,R),rt=a(R),Ne=d(R,"P",{"data-svelte-h":!0}),v(Ne)!=="svelte-izf2h2"&&(Ne.textContent=Ft),at=a(R),$e=d(R,"DIV",{class:!0});var Fe=j($e);g(ie.$$.fragment,Fe),Fe.forEach(o),R.forEach(o),Oe=a(e),g(ce.$$.fragment,e),Be=a(e),w=d(e,"DIV",{class:!0});var U=j(w);g(le.$$.fragment,U),it=a(U),we=d(U,"P",{"data-svelte-h":!0}),v(we)!=="svelte-f7nez2"&&(we.textContent=jt),ct=a(U),Ee=d(U,"P",{"data-svelte-h":!0}),v(Ee)!=="svelte-q52n56"&&(Ee.innerHTML=Ut),lt=a(U),Te=d(U,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-hswkmf"&&(Te.innerHTML=Wt),dt=a(U),I=d(U,"DIV",{class:!0});var O=j(I);g(de.$$.fragment,O),ft=a(O),xe=d(O,"P",{"data-svelte-h":!0}),v(xe)!=="svelte-1eloh0h"&&(xe.innerHTML=kt),mt=a(O),g(D.$$.fragment,O),pt=a(O),g(V.$$.fragment,O),O.forEach(o),U.forEach(o),Je=a(e),g(fe.$$.fragment,e),Ge=a(e),E=d(e,"DIV",{class:!0});var W=j(E);g(me.$$.fragment,W),gt=a(W),Ie=d(W,"P",{"data-svelte-h":!0}),v(Ie)!=="svelte-im74tq"&&(Ie.textContent=Rt),ht=a(W),Ce=d(W,"P",{"data-svelte-h":!0}),v(Ce)!=="svelte-q52n56"&&(Ce.innerHTML=Zt),ut=a(W),Me=d(W,"P",{"data-svelte-h":!0}),v(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Lt),_t=a(W),C=d(W,"DIV",{class:!0});var B=j(C);g(pe.$$.fragment,B),bt=a(B),ze=d(B,"P",{"data-svelte-h":!0}),v(ze)!=="svelte-1i0m44l"&&(ze.innerHTML=qt),vt=a(B),g(A.$$.fragment,B),yt=a(B),g(H.$$.fragment,B),B.forEach(o),W.forEach(o),Qe=a(e),g(ge.$$.fragment,e),Ye=a(e),Pe=d(e,"P",{}),j(Pe).forEach(o),this.h()},h(){M(n,"name","hf:doc:metadata"),M(n,"content",eo),M(L,"class","flex flex-wrap space-x-1"),M(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,n),i(e,N,t),i(e,f,t),i(e,m,t),i(e,y,t),i(e,$,t),h(J,e,t),i(e,Ue,t),i(e,L,t),i(e,We,t),h(G,e,t),i(e,ke,t),i(e,Q,t),i(e,Re,t),i(e,Y,t),i(e,Ze,t),i(e,X,t),i(e,Le,t),i(e,K,t),i(e,qe,t),h(ee,e,t),i(e,Se,t),i(e,T,t),h(te,T,null),c(T,Ke),c(T,_e),c(T,et),c(T,be),c(T,tt),h(q,T,null),i(e,De,t),h(oe,e,t),i(e,Ve,t),i(e,z,t),h(ne,z,null),c(z,ot),c(z,ve),c(z,nt),c(z,S),h(se,S,null),c(S,st),c(S,ye),i(e,Ae,t),h(re,e,t),i(e,He,t),i(e,P,t),h(ae,P,null),c(P,rt),c(P,Ne),c(P,at),c(P,$e),h(ie,$e,null),i(e,Oe,t),h(ce,e,t),i(e,Be,t),i(e,w,t),h(le,w,null),c(w,it),c(w,we),c(w,ct),c(w,Ee),c(w,lt),c(w,Te),c(w,dt),c(w,I),h(de,I,null),c(I,ft),c(I,xe),c(I,mt),h(D,I,null),c(I,pt),h(V,I,null),i(e,Je,t),h(fe,e,t),i(e,Ge,t),i(e,E,t),h(me,E,null),c(E,gt),c(E,Ie),c(E,ht),c(E,Ce),c(E,ut),c(E,Me),c(E,_t),c(E,C),h(pe,C,null),c(C,bt),c(C,ze),c(C,vt),h(A,C,null),c(C,yt),h(H,C,null),i(e,Qe,t),h(ge,e,t),i(e,Ye,t),i(e,Pe,t),Xe=!0},p(e,[t]){const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),q.$set(F);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),D.$set(k);const he={};t&2&&(he.$$scope={dirty:t,ctx:e}),V.$set(he);const R={};t&2&&(R.$$scope={dirty:t,ctx:e}),A.$set(R);const Fe={};t&2&&(Fe.$$scope={dirty:t,ctx:e}),H.$set(Fe)},i(e){Xe||(u(J.$$.fragment,e),u(G.$$.fragment,e),u(ee.$$.fragment,e),u(te.$$.fragment,e),u(q.$$.fragment,e),u(oe.$$.fragment,e),u(ne.$$.fragment,e),u(se.$$.fragment,e),u(re.$$.fragment,e),u(ae.$$.fragment,e),u(ie.$$.fragment,e),u(ce.$$.fragment,e),u(le.$$.fragment,e),u(de.$$.fragment,e),u(D.$$.fragment,e),u(V.$$.fragment,e),u(fe.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(A.$$.fragment,e),u(H.$$.fragment,e),u(ge.$$.fragment,e),Xe=!0)},o(e){_(J.$$.fragment,e),_(G.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(q.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(ae.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(D.$$.fragment,e),_(V.$$.fragment,e),_(fe.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(A.$$.fragment,e),_(H.$$.fragment,e),_(ge.$$.fragment,e),Xe=!1},d(e){e&&(o(N),o(f),o(m),o(y),o($),o(Ue),o(L),o(We),o(ke),o(Q),o(Re),o(Y),o(Ze),o(X),o(Le),o(K),o(qe),o(Se),o(T),o(De),o(Ve),o(z),o(Ae),o(He),o(P),o(Oe),o(Be),o(w),o(Je),o(Ge),o(E),o(Qe),o(Ye),o(Pe)),o(n),b(J,e),b(G,e),b(ee,e),b(te),b(q),b(oe,e),b(ne),b(se),b(re,e),b(ae),b(ie),b(ce,e),b(le),b(de),b(D),b(V),b(fe,e),b(me),b(pe),b(A),b(H),b(ge,e)}}}const eo='{"title":"EfficientNet","local":"efficientnet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"EfficientNetConfig","local":"transformers.EfficientNetConfig","sections":[],"depth":2},{"title":"EfficientNetImageProcessor","local":"transformers.EfficientNetImageProcessor","sections":[],"depth":2},{"title":"EfficientNetImageProcessorFast","local":"transformers.EfficientNetImageProcessorFast","sections":[],"depth":2},{"title":"EfficientNetModel","local":"transformers.EfficientNetModel","sections":[],"depth":2},{"title":"EfficientNetForImageClassification","local":"transformers.EfficientNetForImageClassification","sections":[],"depth":2}],"depth":1}';function to(x){return Vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lo extends At{constructor(n){super(),Ht(this,n,to,Kt,Dt,{})}}export{lo as component};
