import{s as Or,o as Kr,n as Ar}from"../chunks/scheduler.18a86fab.js";import{S as eo,i as to,g as a,s as o,r as l,A as ro,h as s,f as r,c as n,j as y,u as p,x as g,k as x,y as t,a as m,v as f,d as h,t as u,w as _}from"../chunks/index.98837b22.js";import{D as $}from"../chunks/Docstring.a1ef7999.js";import{C as Qr}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Gr}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as yr,E as oo}from"../chunks/getInferenceSnippets.06c2775f.js";function no(Ie){let v,U="Examples:",I,w,M;return w=new Qr({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqSW1hZ2VQcm9jZXNzaW5nTWl4aW4qJTIwc28lMjBsZXQncyUyMHNob3clMjB0aGUlMjBleGFtcGxlcyUyMG9uJTIwYSUwQSUyMyUyMGRlcml2ZWQlMjBjbGFzcyUzQSUyMCpDTElQSW1hZ2VQcm9jZXNzb3IqJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMEEpJTIwJTIwJTIzJTIwRG93bmxvYWQlMjBpbWFnZV9wcm9jZXNzaW5nX2NvbmZpZyUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBDTElQSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiUyQyUyMGRvX25vcm1hbGl6ZSUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwaW1hZ2VfcHJvY2Vzc29yLmRvX25vcm1hbGl6ZSUyMGlzJTIwRmFsc2UlMEFpbWFnZV9wcm9jZXNzb3IlMkMlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMkMlMjBkb19ub3JtYWxpemUlM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGltYWdlX3Byb2Nlc3Nvci5kb19ub3JtYWxpemUlMjBpcyUyMEZhbHNlJTBBYXNzZXJ0JTIwdW51c2VkX2t3YXJncyUyMCUzRCUzRCUyMCU3QiUyMmZvbyUyMiUzQSUyMEZhbHNlJTdE",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){v=a("p"),v.textContent=U,I=o(),l(w.$$.fragment)},l(c){v=s(c,"P",{"data-svelte-h":!0}),g(v)!=="svelte-kvfsh7"&&(v.textContent=U),I=n(c),p(w.$$.fragment,c)},m(c,T){m(c,v,T),m(c,I,T),f(w,c,T),M=!0},p:Ar,i(c){M||(h(w.$$.fragment,c),M=!0)},o(c){u(w.$$.fragment,c),M=!1},d(c){c&&(r(v),r(I)),_(w,c)}}}function ao(Ie){let v,U="Examples:",I,w,M;return w=new Qr({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWltYWdlJTIwcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBQdXNoJTIwdGhlJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjB0byUyMHlvdXIlMjBuYW1lc3BhY2UlMjB3aXRoJTIwdGhlJTIwbmFtZSUyMCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyLiUwQWltYWdlJTIwcHJvY2Vzc29yLnB1c2hfdG9faHViKCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyKSUwQSUwQSUyMyUyMFB1c2glMjB0aGUlMjBpbWFnZSUyMHByb2Nlc3NvciUyMHRvJTIwYW4lMjBvcmdhbml6YXRpb24lMjB3aXRoJTIwdGhlJTIwbmFtZSUyMCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyLiUwQWltYWdlJTIwcHJvY2Vzc29yLnB1c2hfdG9faHViKCUyMmh1Z2dpbmdmYWNlJTJGbXktZmluZXR1bmVkLWJlcnQlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

image processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the image processor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the image processor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`,wrap:!1}}),{c(){v=a("p"),v.textContent=U,I=o(),l(w.$$.fragment)},l(c){v=s(c,"P",{"data-svelte-h":!0}),g(v)!=="svelte-kvfsh7"&&(v.textContent=U),I=n(c),p(w.$$.fragment,c)},m(c,T){m(c,v,T),m(c,I,T),f(w,c,T),M=!0},p:Ar,i(c){M||(h(w.$$.fragment,c),M=!0)},o(c){u(w.$$.fragment,c),M=!1},d(c){c&&(r(v),r(I)),_(w,c)}}}function so(Ie){let v,U,I,w,M,c,T,xr=`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`,nt,O,$r="Most of those are only useful if you are studying the code of the image processors in the library.",at,K,st,N,ee,kt,Te,wr=`Crops the <code>image</code> to the specified <code>size</code> using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size <code>size</code>).`,it,C,te,Zt,Ce,Mr="Converts bounding boxes from center format to corners format.",Dt,Pe,Ir=`center format: contains the coordinate for the center of the box and its width, height dimensions
(center_x, center_y, width, height)
corners format: contains the coordinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)`,mt,P,re,Wt,je,Tr="Converts bounding boxes from corners format to center format.",qt,Ue,Cr=`corners format: contains the coordinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)
center format: contains the coordinate for the center of the box and its the width, height dimensions
(center_x, center_y, width, height)`,ct,k,oe,Bt,ze,Pr="Converts unique ID to RGB color.",dt,j,ne,Vt,Le,jr="Normalizes <code>image</code> using the mean and standard deviation specified by <code>mean</code> and <code>std</code>.",Et,Je,Ur="image = (image - mean) / std",lt,Z,ae,Ht,Ne,zr="Pads the <code>image</code> with the specified (height, width) <code>padding</code> and <code>mode</code>.",pt,D,se,Rt,ke,Lr="Converts RGB color to unique ID.",gt,W,ie,Ft,Ze,Jr="Rescales <code>image</code> by <code>scale</code>.",ft,q,me,Xt,De,Nr="Resizes <code>image</code> to <code>(height, width)</code> specified by <code>size</code> using the PIL library.",ht,B,ce,St,We,kr=`Converts <code>image</code> to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`,ut,de,_t,d,le,Yt,qe,Zr=`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,Gt,z,pe,At,Be,Dr="Convert a single or a list of urls into the corresponding <code>PIL.Image</code> objects.",Qt,Ve,Wr=`If a single url is passed, the return value will be a single object. If a list is passed a list of objects is
returned.`,Ot,E,ge,Kt,Ee,qr='Instantiates a type of <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from a Python dictionary of parameters.',er,H,fe,tr,He,Br=`Instantiates a image processor of type <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from the path to a JSON
file of parameters.`,rr,L,he,or,Re,Vr='Instantiate a type of <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from an image processor.',nr,R,ar,F,ue,sr,Fe,Er=`From a <code>pretrained_model_name_or_path</code>, resolve to a dictionary of parameters, to be used for instantiating a
image processor of type <code>~image_processor_utils.ImageProcessingMixin</code> using <code>from_dict</code>.`,ir,J,_e,mr,Xe,Hr="Upload the image processor file to the ðŸ¤— Model Hub.",cr,X,dr,S,be,lr,Se,Rr=`Register this class with a given auto class. This should only be used for custom image processors as the ones
in the library are already mapped with <code>AutoImageProcessor </code>.`,pr,Y,ve,gr,Ye,Fr=`Save an image processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin.from_pretrained">from_pretrained()</a> class method.`,fr,G,ye,hr,Ge,Xr="Serializes this instance to a Python dictionary.",ur,A,xe,_r,Ae,Sr="Save this instance to a JSON file.",br,Q,$e,vr,Qe,Yr="Serializes this instance to a JSON string.",bt,we,vt,ot,yt;return M=new yr({props:{title:"Utilities for Image Processors",local:"utilities-for-image-processors",headingTag:"h1"}}),K=new yr({props:{title:"Image Transformations",local:"transformers.image_transforms.center_crop",headingTag:"h2"}}),ee=new $({props:{name:"transformers.image_transforms.center_crop",anchor:"transformers.image_transforms.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": tuple"},{name:"data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to crop.`,name:"image"},{anchor:"transformers.image_transforms.center_crop.size",description:`<strong>size</strong> (<code>tuple[int, int]</code>) &#x2014;
The target size for the cropped image.`,name:"size"},{anchor:"transformers.image_transforms.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.center_crop.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L455",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The cropped image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),te=new $({props:{name:"transformers.image_transforms.center_to_corners_format",anchor:"transformers.image_transforms.center_to_corners_format",parameters:[{name:"bboxes_center",val:": TensorType"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L570"}}),re=new $({props:{name:"transformers.image_transforms.corners_to_center_format",anchor:"transformers.image_transforms.corners_to_center_format",parameters:[{name:"bboxes_corners",val:": TensorType"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L630"}}),oe=new $({props:{name:"transformers.image_transforms.id_to_rgb",anchor:"transformers.image_transforms.id_to_rgb",parameters:[{name:"id_map",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L664"}}),ne=new $({props:{name:"transformers.image_transforms.normalize",anchor:"transformers.image_transforms.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, collections.abc.Collection[float]]"},{name:"std",val:": typing.Union[float, collections.abc.Collection[float]]"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.image_transforms.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Collection[float]</code>) &#x2014;
The mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_transforms.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Collection[float]</code>) &#x2014;
The standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_transforms.normalize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If unset, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.image_transforms.normalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L394"}}),ae=new $({props:{name:"transformers.image_transforms.pad",anchor:"transformers.image_transforms.pad",parameters:[{name:"image",val:": ndarray"},{name:"padding",val:": typing.Union[int, tuple[int, int], collections.abc.Iterable[tuple[int, int]]]"},{name:"mode",val:": PaddingMode = <PaddingMode.CONSTANT: 'constant'>"},{name:"constant_values",val:": typing.Union[float, collections.abc.Iterable[float]] = 0.0"},{name:"data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.pad.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to pad.`,name:"image"},{anchor:"transformers.image_transforms.pad.padding",description:`<strong>padding</strong> (<code>int</code> or <code>tuple[int, int]</code> or <code>Iterable[tuple[int, int]]</code>) &#x2014;
Padding to apply to the edges of the height, width axes. Can be one of three formats:<ul>
<li><code>((before_height, after_height), (before_width, after_width))</code> unique pad widths for each axis.</li>
<li><code>((before, after),)</code> yields same before and after pad for height and width.</li>
<li><code>(pad,)</code> or int is a shortcut for before = after = pad width for all axes.</li>
</ul>`,name:"padding"},{anchor:"transformers.image_transforms.pad.mode",description:`<strong>mode</strong> (<code>PaddingMode</code>) &#x2014;
The padding mode to use. Can be one of:<ul>
<li><code>&quot;constant&quot;</code>: pads with a constant value.</li>
<li><code>&quot;reflect&quot;</code>: pads with the reflection of the vector mirrored on the first and last values of the
vector along each axis.</li>
<li><code>&quot;replicate&quot;</code>: pads with the replication of the last value on the edge of the array along each axis.</li>
<li><code>&quot;symmetric&quot;</code>: pads with the reflection of the vector mirrored along the edge of the array.</li>
</ul>`,name:"mode"},{anchor:"transformers.image_transforms.pad.constant_values",description:`<strong>constant_values</strong> (<code>float</code> or <code>Iterable[float]</code>, <em>optional</em>) &#x2014;
The value to use for the padding if <code>mode</code> is <code>&quot;constant&quot;</code>.`,name:"constant_values"},{anchor:"transformers.image_transforms.pad.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use same as the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.pad.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L694",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The padded image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),se=new $({props:{name:"transformers.image_transforms.rgb_to_id",anchor:"transformers.image_transforms.rgb_to_id",parameters:[{name:"color",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L653"}}),ie=new $({props:{name:"transformers.image_transforms.rescale",anchor:"transformers.image_transforms.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:": dtype = <class 'numpy.float32'>"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.image_transforms.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.image_transforms.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.image_transforms.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"},{anchor:"transformers.image_transforms.rescale.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred from the input image.`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L97",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),me=new $({props:{name:"transformers.image_transforms.resize",anchor:"transformers.image_transforms.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": tuple"},{name:"resample",val:": PILImageResampling = None"},{name:"reducing_gap",val:": typing.Optional[int] = None"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.image_transforms.resize.size",description:`<strong>size</strong> (<code>tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.image_transforms.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.image_transforms.resize.reducing_gap",description:`<strong>reducing_gap</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Apply optimization by resizing the image in two steps. The bigger <code>reducing_gap</code>, the closer the result to
the fair resampling. See corresponding Pillow documentation for more details.`,name:"reducing_gap"},{anchor:"transformers.image_transforms.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If unset, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.image_transforms.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"},{anchor:"transformers.image_transforms.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L323",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),ce=new $({props:{name:"transformers.image_transforms.to_pil_image",anchor:"transformers.image_transforms.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, ForwardRef('PIL.Image.Image'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.ndarray')]"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"image_mode",val:": typing.Optional[str] = None"},{name:"input_data_format",val:": typing.Union[transformers.image_utils.ChannelDimension, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.image_transforms.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type and casting to <code>int</code> would result in a loss of precision,
and <code>False</code> otherwise.`,name:"do_rescale"},{anchor:"transformers.image_transforms.to_pil_image.image_mode",description:`<strong>image_mode</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The mode to use for the PIL image. If unset, will use the default mode for the input image type.`,name:"image_mode"},{anchor:"transformers.image_transforms.to_pil_image.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_transforms.py#L162",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The converted image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>PIL.Image.Image</code></p>
`}}),de=new yr({props:{title:"ImageProcessingMixin",local:"transformers.ImageProcessingMixin",headingTag:"h2"}}),le=new $({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L64"}}),pe=new $({props:{name:"fetch_images",anchor:"transformers.ImageProcessingMixin.fetch_images",parameters:[{name:"image_url_or_urls",val:": typing.Union[str, list[str], list[list[str]]]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L522"}}),ge=new $({props:{name:"from_dict",anchor:"transformers.ImageProcessingMixin.from_dict",parameters:[{name:"image_processor_dict",val:": dict"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_dict.image_processor_dict",description:`<strong>image_processor_dict</strong> (<code>dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the image processor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/v4.56.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin.to_dict">to_dict()</a> method.`,name:"image_processor_dict"},{anchor:"transformers.ImageProcessingMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the image processor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L389",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image processor object instantiated from those
parameters.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),fe=new $({props:{name:"from_json_file",anchor:"transformers.ImageProcessingMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L446",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image_processor object
instantiated from that JSON file.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),he=new $({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[str, bool, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L91",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),R=new Gr({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[no]},$$scope:{ctx:Ie}}}),ue=new $({props:{name:"get_image_processor_dict",anchor:"transformers.ImageProcessingMixin.get_image_processor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&quot;</code>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.`,name:"subfolder"},{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.image_processor_filename",description:`<strong>image_processor_filename</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;config.json&quot;</code>) &#x2014;
The name of the file in the model directory to use for the image processor config.`,name:"image_processor_filename"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L266",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The dictionary(ies) that will be used to instantiate the image processor object.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[Dict, Dict]</code></p>
`}}),_e=new $({props:{name:"push_to_hub",anchor:"transformers.ImageProcessingMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[str, int, NoneType] = '5GB'"},{name:"create_pr",val:": bool = False"},{name:"safe_serialization",val:": bool = True"},{name:"revision",val:": typing.Optional[str] = None"},{name:"commit_description",val:": typing.Optional[str] = None"},{name:"tags",val:": typing.Optional[list[str]] = None"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your image processor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload image processor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to make the repo private. If <code>None</code> (default), the repo will be public unless the organization&#x2019;s default is private. This value is ignored if the repo already exists.`,name:"private"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.token",description:`<strong>token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"token"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;5GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>). We default it to <code>&quot;5GB&quot;</code> so that users can easily load models on free-tier
Google Colab instances without any CPU OOM issues.`,name:"max_shard_size"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.safe_serialization",description:`<strong>safe_serialization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to convert the model weights in safetensors format for safer serialization.`,name:"safe_serialization"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Branch to push the uploaded files to.`,name:"revision"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.commit_description",description:`<strong>commit_description</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The description of the commit that will be created`,name:"commit_description"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.tags",description:`<strong>tags</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
List of tags to push on the Hub.`,name:"tags"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/utils/hub.py#L847"}}),X=new Gr({props:{anchor:"transformers.ImageProcessingMixin.push_to_hub.example",$$slots:{default:[ao]},$$scope:{ctx:Ie}}}),be=new $({props:{name:"register_for_auto_class",anchor:"transformers.ImageProcessingMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoImageProcessor'"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoImageProcessor &quot;</code>) &#x2014;
The auto class to register this new image processor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L500"}}),ve=new $({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L205"}}),ye=new $({props:{name:"to_dict",anchor:"transformers.ImageProcessingMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L434",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this image processor instance.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>dict[str, Any]</code></p>
`}}),xe=new $({props:{name:"to_json_file",anchor:"transformers.ImageProcessingMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this image_processor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L486"}}),$e=new $({props:{name:"to_json_string",anchor:"transformers.ImageProcessingMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_base.py#L465",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>str</code></p>
`}}),we=new oo({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/internal/image_processing_utils.md"}}),{c(){v=a("meta"),U=o(),I=a("p"),w=o(),l(M.$$.fragment),c=o(),T=a("p"),T.textContent=xr,nt=o(),O=a("p"),O.textContent=$r,at=o(),l(K.$$.fragment),st=o(),N=a("div"),l(ee.$$.fragment),kt=o(),Te=a("p"),Te.innerHTML=wr,it=o(),C=a("div"),l(te.$$.fragment),Zt=o(),Ce=a("p"),Ce.textContent=Mr,Dt=o(),Pe=a("p"),Pe.textContent=Ir,mt=o(),P=a("div"),l(re.$$.fragment),Wt=o(),je=a("p"),je.textContent=Tr,qt=o(),Ue=a("p"),Ue.textContent=Cr,ct=o(),k=a("div"),l(oe.$$.fragment),Bt=o(),ze=a("p"),ze.textContent=Pr,dt=o(),j=a("div"),l(ne.$$.fragment),Vt=o(),Le=a("p"),Le.innerHTML=jr,Et=o(),Je=a("p"),Je.textContent=Ur,lt=o(),Z=a("div"),l(ae.$$.fragment),Ht=o(),Ne=a("p"),Ne.innerHTML=zr,pt=o(),D=a("div"),l(se.$$.fragment),Rt=o(),ke=a("p"),ke.textContent=Lr,gt=o(),W=a("div"),l(ie.$$.fragment),Ft=o(),Ze=a("p"),Ze.innerHTML=Jr,ft=o(),q=a("div"),l(me.$$.fragment),Xt=o(),De=a("p"),De.innerHTML=Nr,ht=o(),B=a("div"),l(ce.$$.fragment),St=o(),We=a("p"),We.innerHTML=kr,ut=o(),l(de.$$.fragment),_t=o(),d=a("div"),l(le.$$.fragment),Yt=o(),qe=a("p"),qe.textContent=Zr,Gt=o(),z=a("div"),l(pe.$$.fragment),At=o(),Be=a("p"),Be.innerHTML=Dr,Qt=o(),Ve=a("p"),Ve.textContent=Wr,Ot=o(),E=a("div"),l(ge.$$.fragment),Kt=o(),Ee=a("p"),Ee.innerHTML=qr,er=o(),H=a("div"),l(fe.$$.fragment),tr=o(),He=a("p"),He.innerHTML=Br,rr=o(),L=a("div"),l(he.$$.fragment),or=o(),Re=a("p"),Re.innerHTML=Vr,nr=o(),l(R.$$.fragment),ar=o(),F=a("div"),l(ue.$$.fragment),sr=o(),Fe=a("p"),Fe.innerHTML=Er,ir=o(),J=a("div"),l(_e.$$.fragment),mr=o(),Xe=a("p"),Xe.textContent=Hr,cr=o(),l(X.$$.fragment),dr=o(),S=a("div"),l(be.$$.fragment),lr=o(),Se=a("p"),Se.innerHTML=Rr,pr=o(),Y=a("div"),l(ve.$$.fragment),gr=o(),Ye=a("p"),Ye.innerHTML=Fr,fr=o(),G=a("div"),l(ye.$$.fragment),hr=o(),Ge=a("p"),Ge.textContent=Xr,ur=o(),A=a("div"),l(xe.$$.fragment),_r=o(),Ae=a("p"),Ae.textContent=Sr,br=o(),Q=a("div"),l($e.$$.fragment),vr=o(),Qe=a("p"),Qe.textContent=Yr,bt=o(),l(we.$$.fragment),vt=o(),ot=a("p"),this.h()},l(e){const i=ro("svelte-u9bgzb",document.head);v=s(i,"META",{name:!0,content:!0}),i.forEach(r),U=n(e),I=s(e,"P",{}),y(I).forEach(r),w=n(e),p(M.$$.fragment,e),c=n(e),T=s(e,"P",{"data-svelte-h":!0}),g(T)!=="svelte-nip4ii"&&(T.textContent=xr),nt=n(e),O=s(e,"P",{"data-svelte-h":!0}),g(O)!=="svelte-60lkqv"&&(O.textContent=$r),at=n(e),p(K.$$.fragment,e),st=n(e),N=s(e,"DIV",{class:!0});var Me=y(N);p(ee.$$.fragment,Me),kt=n(Me),Te=s(Me,"P",{"data-svelte-h":!0}),g(Te)!=="svelte-2qw57k"&&(Te.innerHTML=wr),Me.forEach(r),it=n(e),C=s(e,"DIV",{class:!0});var V=y(C);p(te.$$.fragment,V),Zt=n(V),Ce=s(V,"P",{"data-svelte-h":!0}),g(Ce)!=="svelte-kjzox5"&&(Ce.textContent=Mr),Dt=n(V),Pe=s(V,"P",{"data-svelte-h":!0}),g(Pe)!=="svelte-1xywn04"&&(Pe.textContent=Ir),V.forEach(r),mt=n(e),P=s(e,"DIV",{class:!0});var Oe=y(P);p(re.$$.fragment,Oe),Wt=n(Oe),je=s(Oe,"P",{"data-svelte-h":!0}),g(je)!=="svelte-m5ejz9"&&(je.textContent=Tr),qt=n(Oe),Ue=s(Oe,"P",{"data-svelte-h":!0}),g(Ue)!=="svelte-1isflmb"&&(Ue.textContent=Cr),Oe.forEach(r),ct=n(e),k=s(e,"DIV",{class:!0});var xt=y(k);p(oe.$$.fragment,xt),Bt=n(xt),ze=s(xt,"P",{"data-svelte-h":!0}),g(ze)!=="svelte-19ts50v"&&(ze.textContent=Pr),xt.forEach(r),dt=n(e),j=s(e,"DIV",{class:!0});var Ke=y(j);p(ne.$$.fragment,Ke),Vt=n(Ke),Le=s(Ke,"P",{"data-svelte-h":!0}),g(Le)!=="svelte-cgeryj"&&(Le.innerHTML=jr),Et=n(Ke),Je=s(Ke,"P",{"data-svelte-h":!0}),g(Je)!=="svelte-1vabayc"&&(Je.textContent=Ur),Ke.forEach(r),lt=n(e),Z=s(e,"DIV",{class:!0});var $t=y(Z);p(ae.$$.fragment,$t),Ht=n($t),Ne=s($t,"P",{"data-svelte-h":!0}),g(Ne)!=="svelte-1v3jx5j"&&(Ne.innerHTML=zr),$t.forEach(r),pt=n(e),D=s(e,"DIV",{class:!0});var wt=y(D);p(se.$$.fragment,wt),Rt=n(wt),ke=s(wt,"P",{"data-svelte-h":!0}),g(ke)!=="svelte-78trhr"&&(ke.textContent=Lr),wt.forEach(r),gt=n(e),W=s(e,"DIV",{class:!0});var Mt=y(W);p(ie.$$.fragment,Mt),Ft=n(Mt),Ze=s(Mt,"P",{"data-svelte-h":!0}),g(Ze)!=="svelte-skn2h6"&&(Ze.innerHTML=Jr),Mt.forEach(r),ft=n(e),q=s(e,"DIV",{class:!0});var It=y(q);p(me.$$.fragment,It),Xt=n(It),De=s(It,"P",{"data-svelte-h":!0}),g(De)!=="svelte-avham3"&&(De.innerHTML=Nr),It.forEach(r),ht=n(e),B=s(e,"DIV",{class:!0});var Tt=y(B);p(ce.$$.fragment,Tt),St=n(Tt),We=s(Tt,"P",{"data-svelte-h":!0}),g(We)!=="svelte-e557ju"&&(We.innerHTML=kr),Tt.forEach(r),ut=n(e),p(de.$$.fragment,e),_t=n(e),d=s(e,"DIV",{class:!0});var b=y(d);p(le.$$.fragment,b),Yt=n(b),qe=s(b,"P",{"data-svelte-h":!0}),g(qe)!=="svelte-16ht4m3"&&(qe.textContent=Zr),Gt=n(b),z=s(b,"DIV",{class:!0});var et=y(z);p(pe.$$.fragment,et),At=n(et),Be=s(et,"P",{"data-svelte-h":!0}),g(Be)!=="svelte-2773v6"&&(Be.innerHTML=Dr),Qt=n(et),Ve=s(et,"P",{"data-svelte-h":!0}),g(Ve)!=="svelte-1q8uymn"&&(Ve.textContent=Wr),et.forEach(r),Ot=n(b),E=s(b,"DIV",{class:!0});var Ct=y(E);p(ge.$$.fragment,Ct),Kt=n(Ct),Ee=s(Ct,"P",{"data-svelte-h":!0}),g(Ee)!=="svelte-1empbfe"&&(Ee.innerHTML=qr),Ct.forEach(r),er=n(b),H=s(b,"DIV",{class:!0});var Pt=y(H);p(fe.$$.fragment,Pt),tr=n(Pt),He=s(Pt,"P",{"data-svelte-h":!0}),g(He)!=="svelte-1g0sqty"&&(He.innerHTML=Br),Pt.forEach(r),rr=n(b),L=s(b,"DIV",{class:!0});var tt=y(L);p(he.$$.fragment,tt),or=n(tt),Re=s(tt,"P",{"data-svelte-h":!0}),g(Re)!=="svelte-lkptaz"&&(Re.innerHTML=Vr),nr=n(tt),p(R.$$.fragment,tt),tt.forEach(r),ar=n(b),F=s(b,"DIV",{class:!0});var jt=y(F);p(ue.$$.fragment,jt),sr=n(jt),Fe=s(jt,"P",{"data-svelte-h":!0}),g(Fe)!=="svelte-d2g2ab"&&(Fe.innerHTML=Er),jt.forEach(r),ir=n(b),J=s(b,"DIV",{class:!0});var rt=y(J);p(_e.$$.fragment,rt),mr=n(rt),Xe=s(rt,"P",{"data-svelte-h":!0}),g(Xe)!=="svelte-1qlyz3u"&&(Xe.textContent=Hr),cr=n(rt),p(X.$$.fragment,rt),rt.forEach(r),dr=n(b),S=s(b,"DIV",{class:!0});var Ut=y(S);p(be.$$.fragment,Ut),lr=n(Ut),Se=s(Ut,"P",{"data-svelte-h":!0}),g(Se)!=="svelte-ofyqve"&&(Se.innerHTML=Rr),Ut.forEach(r),pr=n(b),Y=s(b,"DIV",{class:!0});var zt=y(Y);p(ve.$$.fragment,zt),gr=n(zt),Ye=s(zt,"P",{"data-svelte-h":!0}),g(Ye)!=="svelte-1gn5bqe"&&(Ye.innerHTML=Fr),zt.forEach(r),fr=n(b),G=s(b,"DIV",{class:!0});var Lt=y(G);p(ye.$$.fragment,Lt),hr=n(Lt),Ge=s(Lt,"P",{"data-svelte-h":!0}),g(Ge)!=="svelte-1ww3wqq"&&(Ge.textContent=Xr),Lt.forEach(r),ur=n(b),A=s(b,"DIV",{class:!0});var Jt=y(A);p(xe.$$.fragment,Jt),_r=n(Jt),Ae=s(Jt,"P",{"data-svelte-h":!0}),g(Ae)!=="svelte-1g70y32"&&(Ae.textContent=Sr),Jt.forEach(r),br=n(b),Q=s(b,"DIV",{class:!0});var Nt=y(Q);p($e.$$.fragment,Nt),vr=n(Nt),Qe=s(Nt,"P",{"data-svelte-h":!0}),g(Qe)!=="svelte-5ayq1f"&&(Qe.textContent=Yr),Nt.forEach(r),b.forEach(r),bt=n(e),p(we.$$.fragment,e),vt=n(e),ot=s(e,"P",{}),y(ot).forEach(r),this.h()},h(){x(v,"name","hf:doc:metadata"),x(v,"content",io),x(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,i){t(document.head,v),m(e,U,i),m(e,I,i),m(e,w,i),f(M,e,i),m(e,c,i),m(e,T,i),m(e,nt,i),m(e,O,i),m(e,at,i),f(K,e,i),m(e,st,i),m(e,N,i),f(ee,N,null),t(N,kt),t(N,Te),m(e,it,i),m(e,C,i),f(te,C,null),t(C,Zt),t(C,Ce),t(C,Dt),t(C,Pe),m(e,mt,i),m(e,P,i),f(re,P,null),t(P,Wt),t(P,je),t(P,qt),t(P,Ue),m(e,ct,i),m(e,k,i),f(oe,k,null),t(k,Bt),t(k,ze),m(e,dt,i),m(e,j,i),f(ne,j,null),t(j,Vt),t(j,Le),t(j,Et),t(j,Je),m(e,lt,i),m(e,Z,i),f(ae,Z,null),t(Z,Ht),t(Z,Ne),m(e,pt,i),m(e,D,i),f(se,D,null),t(D,Rt),t(D,ke),m(e,gt,i),m(e,W,i),f(ie,W,null),t(W,Ft),t(W,Ze),m(e,ft,i),m(e,q,i),f(me,q,null),t(q,Xt),t(q,De),m(e,ht,i),m(e,B,i),f(ce,B,null),t(B,St),t(B,We),m(e,ut,i),f(de,e,i),m(e,_t,i),m(e,d,i),f(le,d,null),t(d,Yt),t(d,qe),t(d,Gt),t(d,z),f(pe,z,null),t(z,At),t(z,Be),t(z,Qt),t(z,Ve),t(d,Ot),t(d,E),f(ge,E,null),t(E,Kt),t(E,Ee),t(d,er),t(d,H),f(fe,H,null),t(H,tr),t(H,He),t(d,rr),t(d,L),f(he,L,null),t(L,or),t(L,Re),t(L,nr),f(R,L,null),t(d,ar),t(d,F),f(ue,F,null),t(F,sr),t(F,Fe),t(d,ir),t(d,J),f(_e,J,null),t(J,mr),t(J,Xe),t(J,cr),f(X,J,null),t(d,dr),t(d,S),f(be,S,null),t(S,lr),t(S,Se),t(d,pr),t(d,Y),f(ve,Y,null),t(Y,gr),t(Y,Ye),t(d,fr),t(d,G),f(ye,G,null),t(G,hr),t(G,Ge),t(d,ur),t(d,A),f(xe,A,null),t(A,_r),t(A,Ae),t(d,br),t(d,Q),f($e,Q,null),t(Q,vr),t(Q,Qe),m(e,bt,i),f(we,e,i),m(e,vt,i),m(e,ot,i),yt=!0},p(e,[i]){const Me={};i&2&&(Me.$$scope={dirty:i,ctx:e}),R.$set(Me);const V={};i&2&&(V.$$scope={dirty:i,ctx:e}),X.$set(V)},i(e){yt||(h(M.$$.fragment,e),h(K.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(re.$$.fragment,e),h(oe.$$.fragment,e),h(ne.$$.fragment,e),h(ae.$$.fragment,e),h(se.$$.fragment,e),h(ie.$$.fragment,e),h(me.$$.fragment,e),h(ce.$$.fragment,e),h(de.$$.fragment,e),h(le.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(fe.$$.fragment,e),h(he.$$.fragment,e),h(R.$$.fragment,e),h(ue.$$.fragment,e),h(_e.$$.fragment,e),h(X.$$.fragment,e),h(be.$$.fragment,e),h(ve.$$.fragment,e),h(ye.$$.fragment,e),h(xe.$$.fragment,e),h($e.$$.fragment,e),h(we.$$.fragment,e),yt=!0)},o(e){u(M.$$.fragment,e),u(K.$$.fragment,e),u(ee.$$.fragment,e),u(te.$$.fragment,e),u(re.$$.fragment,e),u(oe.$$.fragment,e),u(ne.$$.fragment,e),u(ae.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(me.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),u(le.$$.fragment,e),u(pe.$$.fragment,e),u(ge.$$.fragment,e),u(fe.$$.fragment,e),u(he.$$.fragment,e),u(R.$$.fragment,e),u(ue.$$.fragment,e),u(_e.$$.fragment,e),u(X.$$.fragment,e),u(be.$$.fragment,e),u(ve.$$.fragment,e),u(ye.$$.fragment,e),u(xe.$$.fragment,e),u($e.$$.fragment,e),u(we.$$.fragment,e),yt=!1},d(e){e&&(r(U),r(I),r(w),r(c),r(T),r(nt),r(O),r(at),r(st),r(N),r(it),r(C),r(mt),r(P),r(ct),r(k),r(dt),r(j),r(lt),r(Z),r(pt),r(D),r(gt),r(W),r(ft),r(q),r(ht),r(B),r(ut),r(_t),r(d),r(bt),r(vt),r(ot)),r(v),_(M,e),_(K,e),_(ee),_(te),_(re),_(oe),_(ne),_(ae),_(se),_(ie),_(me),_(ce),_(de,e),_(le),_(pe),_(ge),_(fe),_(he),_(R),_(ue),_(_e),_(X),_(be),_(ve),_(ye),_(xe),_($e),_(we,e)}}}const io='{"title":"Utilities for Image Processors","local":"utilities-for-image-processors","sections":[{"title":"Image Transformations","local":"transformers.image_transforms.center_crop","sections":[],"depth":2},{"title":"ImageProcessingMixin","local":"transformers.ImageProcessingMixin","sections":[],"depth":2}],"depth":1}';function mo(Ie){return Kr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uo extends eo{constructor(v){super(),to(this,v,mo,so,Or,{})}}export{uo as component};
