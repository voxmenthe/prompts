import{s as Nt,o as Bt,n as Be}from"../chunks/scheduler.18a86fab.js";import{S as Xt,i as Ht,g as d,s as r,r as f,A as Ft,h as u,f as o,c as l,j as Z,x as J,u as y,k as F,l as Et,y as c,a as m,v as _,d as M,t as b,w as T}from"../chunks/index.98837b22.js";import{T as St}from"../chunks/Tip.77304350.js";import{D as oe}from"../chunks/Docstring.a1ef7999.js";import{C as ve}from"../chunks/CodeBlock.8d0c2e8a.js";import{F as Pt,M as Lt}from"../chunks/Markdown.ae01904b.js";import{E as Vt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ne,E as Yt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as qt,a as Rt}from"../chunks/HfOption.6641485e.js";function Qt(v){let t,h='This model was contributed by <a href="https://huggingface.co/stevenbucaille" rel="nofollow">stevenbucaille</a>.',s,i,p="Click on the SuperGlue models in the right sidebar for more examples of how to apply SuperGlue to different computer vision tasks.";return{c(){t=d("p"),t.innerHTML=h,s=r(),i=d("p"),i.textContent=p},l(n){t=u(n,"P",{"data-svelte-h":!0}),J(t)!=="svelte-1ir9jnx"&&(t.innerHTML=h),s=l(n),i=u(n,"P",{"data-svelte-h":!0}),J(i)!=="svelte-1v82opc"&&(i.textContent=p)},m(n,$){m(n,t,$),m(n,s,$),m(n,i,$)},p:Be,d(n){n&&(o(t),o(s),o(i))}}}function At(v){let t,h;return t=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBa2V5cG9pbnRfbWF0Y2hlciUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJrZXlwb2ludC1tYXRjaGluZyUyMiUyQyUyMG1vZGVsJTNEJTIybWFnaWMtbGVhcC1jb21tdW5pdHklMkZzdXBlcmdsdWVfb3V0ZG9vciUyMiklMEElMEF1cmxfMCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGcmVmcyUyRmhlYWRzJTJGbWFzdGVyJTJGYXNzZXRzJTJGcGhvdG90b3VyaXNtX3NhbXBsZV9pbWFnZXMlMkZ1bml0ZWRfc3RhdGVzX2NhcGl0b2xfOTgxNjk4ODhfMzM0NzcxMDg1Mi5qcGclMjIlMEF1cmxfMSUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGcmVmcyUyRmhlYWRzJTJGbWFzdGVyJTJGYXNzZXRzJTJGcGhvdG90b3VyaXNtX3NhbXBsZV9pbWFnZXMlMkZ1bml0ZWRfc3RhdGVzX2NhcGl0b2xfMjY3NTcwMjdfNjcxNzA4NDA2MS5qcGclMjIlMEElMEFyZXN1bHRzJTIwJTNEJTIwa2V5cG9pbnRfbWF0Y2hlciglNUJ1cmxfMCUyQyUyMHVybF8xJTVEJTJDJTIwdGhyZXNob2xkJTNEMC45KSUwQXByaW50KHJlc3VsdHMlNUIwJTVEKSUwQSUyMyUyMCU3QidrZXlwb2ludF9pbWFnZV8wJyUzQSUyMCU3Qid4JyUzQSUyMC4uLiUyQyUyMCd5JyUzQSUyMC4uLiU3RCUyQyUyMCdrZXlwb2ludF9pbWFnZV8xJyUzQSUyMCU3Qid4JyUzQSUyMC4uLiUyQyUyMCd5JyUzQSUyMC4uLiU3RCUyQyUyMCdzY29yZSclM0ElMjAuLi4lN0Q=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

keypoint_matcher = pipeline(task=<span class="hljs-string">&quot;keypoint-matching&quot;</span>, model=<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)

url_0 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
url_1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>

results = keypoint_matcher([url_0, url_1], threshold=<span class="hljs-number">0.9</span>)
<span class="hljs-built_in">print</span>(results[<span class="hljs-number">0</span>])
<span class="hljs-comment"># {&#x27;keypoint_image_0&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;keypoint_image_1&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;score&#x27;: ...}</span>`,wrap:!1}}),{c(){f(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){_(t,s,i),h=!0},p:Be,i(s){h||(M(t.$$.fragment,s),h=!0)},o(s){b(t.$$.fragment,s),h=!1},d(s){T(t,s)}}}function Dt(v){let t,h;return t=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmxfaW1hZ2UxJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF85ODE2OTg4OF8zMzQ3NzEwODUyLmpwZyUyMiUwQWltYWdlMSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybF9pbWFnZTElMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBdXJsX2ltYWdlMiUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGcmVmcyUyRmhlYWRzJTJGbWFzdGVyJTJGYXNzZXRzJTJGcGhvdG90b3VyaXNtX3NhbXBsZV9pbWFnZXMlMkZ1bml0ZWRfc3RhdGVzX2NhcGl0b2xfMjY3NTcwMjdfNjcxNzA4NDA2MS5qcGclMjIlMEFpbWFnZTIlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmxfaW1hZ2UyJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlcyUyMCUzRCUyMCU1QmltYWdlMSUyQyUyMGltYWdlMiU1RCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWFnaWMtbGVhcC1jb21tdW5pdHklMkZzdXBlcmdsdWVfb3V0ZG9vciUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWFnaWMtbGVhcC1jb21tdW5pdHklMkZzdXBlcmdsdWVfb3V0ZG9vciUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2guaW5mZXJlbmNlX21vZGUoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjBQb3N0LXByb2Nlc3MlMjB0byUyMGdldCUyMGtleXBvaW50cyUyMGFuZCUyMG1hdGNoZXMlMEFpbWFnZV9zaXplcyUyMCUzRCUyMCU1QiU1QihpbWFnZS5oZWlnaHQlMkMlMjBpbWFnZS53aWR0aCklMjBmb3IlMjBpbWFnZSUyMGluJTIwaW1hZ2VzJTVEJTVEJTBBcHJvY2Vzc2VkX291dHB1dHMlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX2tleXBvaW50X21hdGNoaW5nKG91dHB1dHMlMkMlMjBpbWFnZV9zaXplcyUyQyUyMHRocmVzaG9sZCUzRDAuMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url_image1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image1, stream=<span class="hljs-literal">True</span>).raw)
url_image2 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>
image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image2, stream=<span class="hljs-literal">True</span>).raw)

images = [image1, image2]

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)

inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Post-process to get keypoints and matches</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)`,wrap:!1}}),{c(){f(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){_(t,s,i),h=!0},p:Be,i(s){h||(M(t.$$.fragment,s),h=!0)},o(s){b(t.$$.fragment,s),h=!1},d(s){T(t,s)}}}function Kt(v){let t,h,s,i;return t=new Rt({props:{id:"usage",option:"Pipeline",$$slots:{default:[At]},$$scope:{ctx:v}}}),s=new Rt({props:{id:"usage",option:"AutoModel",$$slots:{default:[Dt]},$$scope:{ctx:v}}}),{c(){f(t.$$.fragment),h=r(),f(s.$$.fragment)},l(p){y(t.$$.fragment,p),h=l(p),y(s.$$.fragment,p)},m(p,n){_(t,p,n),m(p,h,n),_(s,p,n),i=!0},p(p,n){const $={};n&2&&($.$$scope={dirty:n,ctx:p}),t.$set($);const C={};n&2&&(C.$$scope={dirty:n,ctx:p}),s.$set(C)},i(p){i||(M(t.$$.fragment,p),M(s.$$.fragment,p),i=!0)},o(p){b(t.$$.fragment,p),b(s.$$.fragment,p),i=!1},d(p){p&&o(h),T(t,p),T(s,p)}}}function Ot(v){let t,h="Examples:",s,i,p;return i=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN1cGVyR2x1ZUNvbmZpZyUyQyUyMFN1cGVyR2x1ZU1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFN1cGVyR2x1ZSUyMHN1cGVyZ2x1ZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBTdXBlckdsdWVDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjBzdXBlcmdsdWUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFN1cGVyR2x1ZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SuperGlueConfig, SuperGlueModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SuperGlue superglue style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SuperGlueConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the superglue style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SuperGlueModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,s=r(),f(i.$$.fragment)},l(n){t=u(n,"P",{"data-svelte-h":!0}),J(t)!=="svelte-kvfsh7"&&(t.textContent=h),s=l(n),y(i.$$.fragment,n)},m(n,$){m(n,t,$),m(n,s,$),_(i,n,$),p=!0},p:Be,i(n){p||(M(i.$$.fragment,n),p=!0)},o(n){b(i.$$.fragment,n),p=!1},d(n){n&&(o(t),o(s)),T(i,n)}}}function es(v){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=h},l(s){t=u(s,"P",{"data-svelte-h":!0}),J(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(s,i){m(s,t,i)},p:Be,d(s){s&&o(t)}}}function ts(v){let t,h="Examples:",s,i,p;return i=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZtYWdpY2xlYXAlMkZTdXBlckdsdWVQcmV0cmFpbmVkTmV0d29yayUyRmJsb2IlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRmxvbmRvbl9icmlkZ2VfNzg5MTY2NzVfNDU2ODE0MTI4OC5qcGclM0ZyYXclM0R0cnVlJTIyJTBBaW1hZ2UxJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGYmxvYiUyRm1hc3RlciUyRmFzc2V0cyUyRnBob3RvdG91cmlzbV9zYW1wbGVfaW1hZ2VzJTJGbG9uZG9uX2JyaWRnZV8xOTQ4MTc5N18yMjk1ODkyNDIxLmpwZyUzRnJhdyUzRHRydWUlMjIlMEFpbWFnZTIlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2VzJTIwJTNEJTIwJTVCaW1hZ2UxJTJDJTIwaW1hZ2UyJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtYWdpYy1sZWFwLWNvbW11bml0eSUyRnN1cGVyZ2x1ZV9vdXRkb29yJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtYWdpYy1sZWFwLWNvbW11bml0eSUyRnN1cGVyZ2x1ZV9vdXRkb29yJTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>images = [image1, image2]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">&gt;&gt;&gt; </span>    inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>    outputs = model(**inputs)`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,s=r(),f(i.$$.fragment)},l(n){t=u(n,"P",{"data-svelte-h":!0}),J(t)!=="svelte-kvfsh7"&&(t.textContent=h),s=l(n),y(i.$$.fragment,n)},m(n,$){m(n,t,$),m(n,s,$),_(i,n,$),p=!0},p:Be,i(n){p||(M(i.$$.fragment,n),p=!0)},o(n){b(i.$$.fragment,n),p=!1},d(n){n&&(o(t),o(s)),T(i,n)}}}function ss(v){let t,h,s,i,p,n,$="SuperGlue model taking images as inputs and outputting the matching of them.",C,L,ne=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,V,N,E=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ge,U,z,Ue,B,X='The <a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueForKeypointMatching">SuperGlueForKeypointMatching</a> forward method, overrides the <code>__call__</code> special method.',q,j,Ie,x,W,R,P="<li>forward</li>",Y;return t=new Ne({props:{title:"SuperGlueForKeypointMatching",local:"transformers.SuperGlueForKeypointMatching",headingTag:"h2"}}),i=new oe({props:{name:"class transformers.SuperGlueForKeypointMatching",anchor:"transformers.SuperGlueForKeypointMatching",parameters:[{name:"config",val:": SuperGlueConfig"}],parametersDescription:[{anchor:"transformers.SuperGlueForKeypointMatching.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueConfig">SuperGlueConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/modeling_superglue.py#L545"}}),z=new oe({props:{name:"forward",anchor:"transformers.SuperGlueForKeypointMatching.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.SuperGlueForKeypointMatching.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueImageProcessor">SuperGlueImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">SuperGlueImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueImageProcessor">SuperGlueImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.SuperGlueForKeypointMatching.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.SuperGlueForKeypointMatching.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.SuperGlueForKeypointMatching.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SuperGlueForKeypointMatching.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/modeling_superglue.py#L724",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.superglue.modeling_superglue.KeypointMatchingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueConfig"
>SuperGlueConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>) — Loss computed during training.</li>
<li><strong>matches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Index of keypoint matched in the other image.</li>
<li><strong>matching_scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Scores of predicted matches.</li>
<li><strong>keypoints</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, 2)</code>) — Absolute (x, y) coordinates of predicted keypoints in a given image.</li>
<li><strong>mask</strong> (<code>torch.IntTensor</code> of shape <code>(batch_size, num_keypoints)</code>) — Mask indicating which values in matches and matching_scores are keypoint matching information.</li>
<li><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, 2, num_channels, num_keypoints)</code>, returned when <code>output_hidden_states=True</code> is passed or when
<code>config.output_hidden_states=True</code>)</li>
<li><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, 2, num_heads, num_keypoints, num_keypoints)</code>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.superglue.modeling_superglue.KeypointMatchingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),j=new St({props:{$$slots:{default:[es]},$$scope:{ctx:v}}}),x=new Vt({props:{anchor:"transformers.SuperGlueForKeypointMatching.forward.example",$$slots:{default:[ts]},$$scope:{ctx:v}}}),{c(){f(t.$$.fragment),h=r(),s=d("div"),f(i.$$.fragment),p=r(),n=d("p"),n.textContent=$,C=r(),L=d("p"),L.innerHTML=ne,V=r(),N=d("p"),N.innerHTML=E,Ge=r(),U=d("div"),f(z.$$.fragment),Ue=r(),B=d("p"),B.innerHTML=X,q=r(),f(j.$$.fragment),Ie=r(),f(x.$$.fragment),W=r(),R=d("ul"),R.innerHTML=P,this.h()},l(g){y(t.$$.fragment,g),h=l(g),s=u(g,"DIV",{class:!0});var w=Z(s);y(i.$$.fragment,w),p=l(w),n=u(w,"P",{"data-svelte-h":!0}),J(n)!=="svelte-td3ten"&&(n.textContent=$),C=l(w),L=u(w,"P",{"data-svelte-h":!0}),J(L)!=="svelte-q52n56"&&(L.innerHTML=ne),V=l(w),N=u(w,"P",{"data-svelte-h":!0}),J(N)!=="svelte-hswkmf"&&(N.innerHTML=E),Ge=l(w),U=u(w,"DIV",{class:!0});var G=Z(U);y(z.$$.fragment,G),Ue=l(G),B=u(G,"P",{"data-svelte-h":!0}),J(B)!=="svelte-1r2483r"&&(B.innerHTML=X),q=l(G),y(j.$$.fragment,G),Ie=l(G),y(x.$$.fragment,G),G.forEach(o),w.forEach(o),W=l(g),R=u(g,"UL",{"data-svelte-h":!0}),J(R)!=="svelte-n3ow4o"&&(R.innerHTML=P),this.h()},h(){F(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(s,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(g,w){_(t,g,w),m(g,h,w),m(g,s,w),_(i,s,null),c(s,p),c(s,n),c(s,C),c(s,L),c(s,V),c(s,N),c(s,Ge),c(s,U),_(z,U,null),c(U,Ue),c(U,B),c(U,q),_(j,U,null),c(U,Ie),_(x,U,null),m(g,W,w),m(g,R,w),Y=!0},p(g,w){const G={};w&2&&(G.$$scope={dirty:w,ctx:g}),j.$set(G);const Xe={};w&2&&(Xe.$$scope={dirty:w,ctx:g}),x.$set(Xe)},i(g){Y||(M(t.$$.fragment,g),M(i.$$.fragment,g),M(z.$$.fragment,g),M(j.$$.fragment,g),M(x.$$.fragment,g),Y=!0)},o(g){b(t.$$.fragment,g),b(i.$$.fragment,g),b(z.$$.fragment,g),b(j.$$.fragment,g),b(x.$$.fragment,g),Y=!1},d(g){g&&(o(h),o(s),o(W),o(R)),T(t,g),T(i),T(z),T(j),T(x)}}}function os(v){let t,h;return t=new Lt({props:{$$slots:{default:[ss]},$$scope:{ctx:v}}}),{c(){f(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){_(t,s,i),h=!0},p(s,i){const p={};i&2&&(p.$$scope={dirty:i,ctx:s}),t.$set(p)},i(s){h||(M(t.$$.fragment,s),h=!0)},o(s){b(t.$$.fragment,s),h=!1},d(s){T(t,s)}}}function ns(v){let t,h,s,i,p,n="<em>This model was released on 2019-11-26 and added to Hugging Face Transformers on 2025-01-20.</em>",$,C,L='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ne,V,N,E,Ge='<a href="https://huggingface.co/papers/1911.11763" rel="nofollow">SuperGlue</a> is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. SuperGlue introduces a flexible context aggregation mechanism based on attention, enabling it to reason about the underlying 3D scene and feature assignments jointly. Paired with the <a href="https://huggingface.co/magic-leap-community/superpoint" rel="nofollow">SuperPoint model</a>, it can be used to match two images and estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.',U,z,Ue='You can find all the original SuperGlue checkpoints under the <a href="https://huggingface.co/magic-leap-community" rel="nofollow">Magic Leap Community</a> organization.',B,X,q,j,Ie='The example below demonstrates how to match keypoints between two images with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',x,W,R,P,Y,g,w,G,Xe="SuperGlue performs feature matching between two images simultaneously, requiring pairs of images as input.",at,ae,rt,je,Jt="<p>The model outputs matching indices, keypoints, and confidence scores for each match.</p>",lt,re,Ce,$t='For better visualization and analysis, use the <a href="/docs/transformers/v4.56.2/en/model_doc/superglue#transformers.SuperGlueImageProcessor.post_process_keypoint_matching">SuperGlueImageProcessor.post_process_keypoint_matching()</a> method to get matches in a more readable format.',it,le,ct,ie,ze,vt="Visualize the matches between the images using the built-in plotting functionality.",pt,ce,Fe,Q,Gt='<img src="https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/01ZYaLB1NL5XdA8u7yCo4.png"/>',Ee,pe,Pe,me,Ut='<li>Refer to the <a href="https://github.com/magicleap/SuperGluePretrainedNetwork" rel="nofollow">original SuperGlue repository</a> for more examples and implementation details.</li>',Le,de,Ye,k,ue,mt,Ze,It=`This is the configuration class to store the configuration of a <code>SuperGlueModel</code>. It is used to instantiate a
SuperGlue model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SuperGlue
<a href="https://huggingface.co/magic-leap-community/superglue_indoor" rel="nofollow">magic-leap-community/superglue_indoor</a> architecture.`,dt,xe,jt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ut,A,qe,ge,Qe,I,he,gt,We,Ct="Constructs a SuperGlue image processor.",ht,D,fe,ft,ke,zt=`Converts the raw output of <code>KeypointMatchingOutput</code> into lists of keypoints, scores and descriptors
with coordinates absolute to the original image sizes.`,yt,K,ye,_t,Re,Zt="Preprocess an image or batch of images.",Mt,O,_e,bt,Se,xt="Resize an image.",Tt,ee,Me,wt,Ve,Wt="Plots the image pairs side by side with the detected keypoints as well as the matching between them.",Ae,be,kt="<li>preprocess</li> <li>post_process_keypoint_matching</li> <li>visualize_keypoint_matching</li>",De,te,Ke,Te,Oe,He,et;return V=new Ne({props:{title:"SuperGlue",local:"superglue",headingTag:"h1"}}),X=new St({props:{warning:!1,$$slots:{default:[Qt]},$$scope:{ctx:v}}}),W=new qt({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[Kt]},$$scope:{ctx:v}}}),P=new Ne({props:{title:"Notes",local:"notes",headingTag:"h2"}}),ae=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1hZ2ljLWxlYXAtY29tbXVuaXR5JTJGc3VwZXJnbHVlX291dGRvb3IlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm1hZ2ljLWxlYXAtY29tbXVuaXR5JTJGc3VwZXJnbHVlX291dGRvb3IlMjIpJTBBJTBBJTIzJTIwU3VwZXJHbHVlJTIwcmVxdWlyZXMlMjBwYWlycyUyMG9mJTIwaW1hZ2VzJTBBaW1hZ2VzJTIwJTNEJTIwJTVCaW1hZ2UxJTJDJTIwaW1hZ2UyJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBd2l0aCUyMHRvcmNoLmluZmVyZW5jZV9tb2RlKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwRXh0cmFjdCUyMG1hdGNoaW5nJTIwaW5mb3JtYXRpb24lMEFrZXlwb2ludHMwJTIwJTNEJTIwb3V0cHV0cy5rZXlwb2ludHMwJTIwJTIwJTIzJTIwS2V5cG9pbnRzJTIwaW4lMjBmaXJzdCUyMGltYWdlJTBBa2V5cG9pbnRzMSUyMCUzRCUyMG91dHB1dHMua2V5cG9pbnRzMSUyMCUyMCUyMyUyMEtleXBvaW50cyUyMGluJTIwc2Vjb25kJTIwaW1hZ2UlMEFtYXRjaGVzJTIwJTNEJTIwb3V0cHV0cy5tYXRjaGVzJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwTWF0Y2hpbmclMjBpbmRpY2VzJTBBbWF0Y2hpbmdfc2NvcmVzJTIwJTNEJTIwb3V0cHV0cy5tYXRjaGluZ19zY29yZXMlMjAlMjAlMjMlMjBDb25maWRlbmNlJTIwc2NvcmVz",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;magic-leap-community/superglue_outdoor&quot;</span>)

<span class="hljs-comment"># SuperGlue requires pairs of images</span>
images = [image1, image2]
inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Extract matching information</span>
keypoints0 = outputs.keypoints0  <span class="hljs-comment"># Keypoints in first image</span>
keypoints1 = outputs.keypoints1  <span class="hljs-comment"># Keypoints in second image</span>
matches = outputs.matches        <span class="hljs-comment"># Matching indices</span>
matching_scores = outputs.matching_scores  <span class="hljs-comment"># Confidence scores</span>`,wrap:!1}}),le=new ve({props:{code:"JTIzJTIwUHJvY2VzcyUyMG91dHB1dHMlMjBmb3IlMjB2aXN1YWxpemF0aW9uJTBBaW1hZ2Vfc2l6ZXMlMjAlM0QlMjAlNUIlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTIwZm9yJTIwaW1hZ2UlMjBpbiUyMGltYWdlcyU1RCU1RCUwQXByb2Nlc3NlZF9vdXRwdXRzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19rZXlwb2ludF9tYXRjaGluZyhvdXRwdXRzJTJDJTIwaW1hZ2Vfc2l6ZXMlMkMlMjB0aHJlc2hvbGQlM0QwLjIpJTBBJTBBZm9yJTIwaSUyQyUyMG91dHB1dCUyMGluJTIwZW51bWVyYXRlKHByb2Nlc3NlZF9vdXRwdXRzKSUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGYlMjJGb3IlMjB0aGUlMjBpbWFnZSUyMHBhaXIlMjAlN0JpJTdEJTIyKSUwQSUyMCUyMCUyMCUyMGZvciUyMGtleXBvaW50MCUyQyUyMGtleXBvaW50MSUyQyUyMG1hdGNoaW5nX3Njb3JlJTIwaW4lMjB6aXAoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3V0cHV0JTVCJTIya2V5cG9pbnRzMCUyMiU1RCUyQyUyMG91dHB1dCU1QiUyMmtleXBvaW50czElMjIlNUQlMkMlMjBvdXRwdXQlNUIlMjJtYXRjaGluZ19zY29yZXMlMjIlNUQlMEElMjAlMjAlMjAlMjApJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMktleXBvaW50JTIwYXQlMjAlN0JrZXlwb2ludDAubnVtcHkoKSU3RCUyMG1hdGNoZXMlMjB3aXRoJTIwa2V5cG9pbnQlMjBhdCUyMCU3QmtleXBvaW50MS5udW1weSgpJTdEJTIwd2l0aCUyMHNjb3JlJTIwJTdCbWF0Y2hpbmdfc2NvcmUlN0QlMjIp",highlighted:`<span class="hljs-comment"># Process outputs for visualization</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)

<span class="hljs-keyword">for</span> i, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(processed_outputs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;For the image pair <span class="hljs-subst">{i}</span>&quot;</span>)
    <span class="hljs-keyword">for</span> keypoint0, keypoint1, matching_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(
            output[<span class="hljs-string">&quot;keypoints0&quot;</span>], output[<span class="hljs-string">&quot;keypoints1&quot;</span>], output[<span class="hljs-string">&quot;matching_scores&quot;</span>]
    ):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keypoint at <span class="hljs-subst">{keypoint0.numpy()}</span> matches with keypoint at <span class="hljs-subst">{keypoint1.numpy()}</span> with score <span class="hljs-subst">{matching_score}</span>&quot;</span>)`,wrap:!1}}),ce=new ve({props:{code:"JTIzJTIwRWFzeSUyMHZpc3VhbGl6YXRpb24lMjB1c2luZyUyMHRoZSUyMGJ1aWx0LWluJTIwcGxvdHRpbmclMjBtZXRob2QlMEFwcm9jZXNzb3IudmlzdWFsaXplX2tleXBvaW50X21hdGNoaW5nKGltYWdlcyUyQyUyMHByb2Nlc3NlZF9vdXRwdXRzKQ==",highlighted:`<span class="hljs-comment"># Easy visualization using the built-in plotting method</span>
processor.visualize_keypoint_matching(images, processed_outputs)`,wrap:!1}}),pe=new Ne({props:{title:"Resources",local:"resources",headingTag:"h2"}}),de=new Ne({props:{title:"SuperGlueConfig",local:"transformers.SuperGlueConfig",headingTag:"h2"}}),ue=new oe({props:{name:"class transformers.SuperGlueConfig",anchor:"transformers.SuperGlueConfig",parameters:[{name:"keypoint_detector_config",val:": SuperPointConfig = None"},{name:"hidden_size",val:": int = 256"},{name:"keypoint_encoder_sizes",val:": typing.Optional[list[int]] = None"},{name:"gnn_layers_types",val:": typing.Optional[list[str]] = None"},{name:"num_attention_heads",val:": int = 4"},{name:"sinkhorn_iterations",val:": int = 100"},{name:"matching_threshold",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperGlueConfig.keypoint_detector_config",description:`<strong>keypoint_detector_config</strong> (<code>Union[AutoConfig, dict]</code>,  <em>optional</em>, defaults to <code>SuperPointConfig</code>) &#x2014;
The config object or dictionary of the keypoint detector.`,name:"keypoint_detector_config"},{anchor:"transformers.SuperGlueConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimension of the descriptors.`,name:"hidden_size"},{anchor:"transformers.SuperGlueConfig.keypoint_encoder_sizes",description:`<strong>keypoint_encoder_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[32, 64, 128, 256]</code>) &#x2014;
The sizes of the keypoint encoder layers.`,name:"keypoint_encoder_sizes"},{anchor:"transformers.SuperGlueConfig.gnn_layers_types",description:`<strong>gnn_layers_types</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[&apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;, &apos;self&apos;, &apos;cross&apos;]</code>) &#x2014;
The types of the GNN layers. Must be either &#x2018;self&#x2019; or &#x2018;cross&#x2019;.`,name:"gnn_layers_types"},{anchor:"transformers.SuperGlueConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of heads in the GNN layers.`,name:"num_attention_heads"},{anchor:"transformers.SuperGlueConfig.sinkhorn_iterations",description:`<strong>sinkhorn_iterations</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The number of Sinkhorn iterations.`,name:"sinkhorn_iterations"},{anchor:"transformers.SuperGlueConfig.matching_threshold",description:`<strong>matching_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The matching threshold.`,name:"matching_threshold"},{anchor:"transformers.SuperGlueConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/configuration_superglue.py#L27"}}),A=new Vt({props:{anchor:"transformers.SuperGlueConfig.example",$$slots:{default:[Ot]},$$scope:{ctx:v}}}),ge=new Ne({props:{title:"SuperGlueImageProcessor",local:"transformers.SuperGlueImageProcessor",headingTag:"h2"}}),he=new oe({props:{name:"class transformers.SuperGlueImageProcessor",anchor:"transformers.SuperGlueImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": float = 0.00392156862745098"},{name:"do_grayscale",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperGlueImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden
by <code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.SuperGlueImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 480, &quot;width&quot;: 640}</code>):
Resolution of the output image after <code>resize</code> is applied. Only has an effect if <code>do_resize</code> is set to
<code>True</code>. Can be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.SuperGlueImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.SuperGlueImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.SuperGlueImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.SuperGlueImageProcessor.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to grayscale. Can be overridden by <code>do_grayscale</code> in the <code>preprocess</code> method.`,name:"do_grayscale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/image_processing_superglue.py#L138"}}),fe=new oe({props:{name:"post_process_keypoint_matching",anchor:"transformers.SuperGlueImageProcessor.post_process_keypoint_matching",parameters:[{name:"outputs",val:": KeypointMatchingOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]]"},{name:"threshold",val:": float = 0.0"}],parametersDescription:[{anchor:"transformers.SuperGlueImageProcessor.post_process_keypoint_matching.outputs",description:`<strong>outputs</strong> (<code>KeypointMatchingOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.SuperGlueImageProcessor.post_process_keypoint_matching.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>list[tuple[tuple[int, int]]]</code>, <em>optional</em>) &#x2014;
Tensor of shape <code>(batch_size, 2, 2)</code> or list of tuples of tuples (<code>tuple[int, int]</code>) containing the
target size <code>(height, width)</code> of each image in the batch. This must be the original image size (before
any processing).`,name:"target_sizes"},{anchor:"transformers.SuperGlueImageProcessor.post_process_keypoint_matching.threshold",description:`<strong>threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold to filter out the matches with low scores.`,name:"threshold"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/image_processing_superglue.py#L342",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints in the first and second image
of the pair, the matching scores and the matching indices.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[Dict]</code></p>
`}}),ye=new oe({props:{name:"preprocess",anchor:"transformers.SuperGlueImageProcessor.preprocess",parameters:[{name:"images",val:""},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_grayscale",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperGlueImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with
pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set
<code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output image after <code>resize</code> has been applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image
is resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the
image will be matched to <code>int(size[&quot;shortest_edge&quot;]/ crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of <code>PILImageResampling</code>, filters. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_grayscale</code>) &#x2014;
Whether to convert the image to grayscale.`,name:"do_grayscale"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SuperGlueImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/image_processing_superglue.py#L224"}}),_e=new oe({props:{name:"resize",anchor:"transformers.SuperGlueImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SuperGlueImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.SuperGlueImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary of the form <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code>, specifying the size of the output image.`,name:"size"},{anchor:"transformers.SuperGlueImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If not provided, it will be inferred from the input
image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.SuperGlueImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/image_processing_superglue.py#L185"}}),Me=new oe({props:{name:"visualize_keypoint_matching",anchor:"transformers.SuperGlueImageProcessor.visualize_keypoint_matching",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"keypoint_matching_output",val:": list"}],parametersDescription:[{anchor:"transformers.SuperGlueImageProcessor.visualize_keypoint_matching.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to plot. Same as <code>SuperGlueImageProcessor.preprocess</code>. Expects either a list of 2
images or a list of list of 2 images list with pixel values ranging from 0 to 255.`,name:"images"},{anchor:"transformers.SuperGlueImageProcessor.visualize_keypoint_matching.keypoint_matching_output",description:`<strong>keypoint_matching_output</strong> (List[Dict[str, torch.Tensor]]]) &#x2014;
A post processed keypoint matching output`,name:"keypoint_matching_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/superglue/image_processing_superglue.py#L411",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of PIL images, each containing the image pairs side by side with the detected
keypoints as well as the matching between them.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[PIL.Image.Image]</code></p>
`}}),te=new Pt({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[os]},$$scope:{ctx:v}}}),Te=new Yt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/superglue.md"}}),{c(){t=d("meta"),h=r(),s=d("p"),i=r(),p=d("p"),p.innerHTML=n,$=r(),C=d("div"),C.innerHTML=L,ne=r(),f(V.$$.fragment),N=r(),E=d("p"),E.innerHTML=Ge,U=r(),z=d("p"),z.innerHTML=Ue,B=r(),f(X.$$.fragment),q=r(),j=d("p"),j.innerHTML=Ie,x=r(),f(W.$$.fragment),R=r(),f(P.$$.fragment),Y=r(),g=d("ul"),w=d("li"),G=d("p"),G.textContent=Xe,at=r(),f(ae.$$.fragment),rt=r(),je=d("li"),je.innerHTML=Jt,lt=r(),re=d("li"),Ce=d("p"),Ce.innerHTML=$t,it=r(),f(le.$$.fragment),ct=r(),ie=d("li"),ze=d("p"),ze.textContent=vt,pt=r(),f(ce.$$.fragment),Fe=r(),Q=d("div"),Q.innerHTML=Gt,Ee=r(),f(pe.$$.fragment),Pe=r(),me=d("ul"),me.innerHTML=Ut,Le=r(),f(de.$$.fragment),Ye=r(),k=d("div"),f(ue.$$.fragment),mt=r(),Ze=d("p"),Ze.innerHTML=It,dt=r(),xe=d("p"),xe.innerHTML=jt,ut=r(),f(A.$$.fragment),qe=r(),f(ge.$$.fragment),Qe=r(),I=d("div"),f(he.$$.fragment),gt=r(),We=d("p"),We.textContent=Ct,ht=r(),D=d("div"),f(fe.$$.fragment),ft=r(),ke=d("p"),ke.innerHTML=zt,yt=r(),K=d("div"),f(ye.$$.fragment),_t=r(),Re=d("p"),Re.textContent=Zt,Mt=r(),O=d("div"),f(_e.$$.fragment),bt=r(),Se=d("p"),Se.textContent=xt,Tt=r(),ee=d("div"),f(Me.$$.fragment),wt=r(),Ve=d("p"),Ve.textContent=Wt,Ae=r(),be=d("ul"),be.innerHTML=kt,De=r(),f(te.$$.fragment),Ke=r(),f(Te.$$.fragment),Oe=r(),He=d("p"),this.h()},l(e){const a=Ft("svelte-u9bgzb",document.head);t=u(a,"META",{name:!0,content:!0}),a.forEach(o),h=l(e),s=u(e,"P",{}),Z(s).forEach(o),i=l(e),p=u(e,"P",{"data-svelte-h":!0}),J(p)!=="svelte-1wedyog"&&(p.innerHTML=n),$=l(e),C=u(e,"DIV",{style:!0,"data-svelte-h":!0}),J(C)!=="svelte-wa5t4p"&&(C.innerHTML=L),ne=l(e),y(V.$$.fragment,e),N=l(e),E=u(e,"P",{"data-svelte-h":!0}),J(E)!=="svelte-ll34mv"&&(E.innerHTML=Ge),U=l(e),z=u(e,"P",{"data-svelte-h":!0}),J(z)!=="svelte-j9yqrp"&&(z.innerHTML=Ue),B=l(e),y(X.$$.fragment,e),q=l(e),j=u(e,"P",{"data-svelte-h":!0}),J(j)!=="svelte-jt8707"&&(j.innerHTML=Ie),x=l(e),y(W.$$.fragment,e),R=l(e),y(P.$$.fragment,e),Y=l(e),g=u(e,"UL",{});var H=Z(g);w=u(H,"LI",{});var we=Z(w);G=u(we,"P",{"data-svelte-h":!0}),J(G)!=="svelte-ixg03t"&&(G.textContent=Xe),at=l(we),y(ae.$$.fragment,we),we.forEach(o),rt=l(H),je=u(H,"LI",{"data-svelte-h":!0}),J(je)!=="svelte-d6hv7i"&&(je.innerHTML=Jt),lt=l(H),re=u(H,"LI",{});var Je=Z(re);Ce=u(Je,"P",{"data-svelte-h":!0}),J(Ce)!=="svelte-1lea315"&&(Ce.innerHTML=$t),it=l(Je),y(le.$$.fragment,Je),Je.forEach(o),ct=l(H),ie=u(H,"LI",{});var $e=Z(ie);ze=u($e,"P",{"data-svelte-h":!0}),J(ze)!=="svelte-155ueni"&&(ze.textContent=vt),pt=l($e),y(ce.$$.fragment,$e),$e.forEach(o),H.forEach(o),Fe=l(e),Q=u(e,"DIV",{class:!0,"data-svelte-h":!0}),J(Q)!=="svelte-cmxcu0"&&(Q.innerHTML=Gt),Ee=l(e),y(pe.$$.fragment,e),Pe=l(e),me=u(e,"UL",{"data-svelte-h":!0}),J(me)!=="svelte-1mr68yt"&&(me.innerHTML=Ut),Le=l(e),y(de.$$.fragment,e),Ye=l(e),k=u(e,"DIV",{class:!0});var se=Z(k);y(ue.$$.fragment,se),mt=l(se),Ze=u(se,"P",{"data-svelte-h":!0}),J(Ze)!=="svelte-mrmg6g"&&(Ze.innerHTML=It),dt=l(se),xe=u(se,"P",{"data-svelte-h":!0}),J(xe)!=="svelte-1ek1ss9"&&(xe.innerHTML=jt),ut=l(se),y(A.$$.fragment,se),se.forEach(o),qe=l(e),y(ge.$$.fragment,e),Qe=l(e),I=u(e,"DIV",{class:!0});var S=Z(I);y(he.$$.fragment,S),gt=l(S),We=u(S,"P",{"data-svelte-h":!0}),J(We)!=="svelte-2i9uog"&&(We.textContent=Ct),ht=l(S),D=u(S,"DIV",{class:!0});var tt=Z(D);y(fe.$$.fragment,tt),ft=l(tt),ke=u(tt,"P",{"data-svelte-h":!0}),J(ke)!=="svelte-y19agd"&&(ke.innerHTML=zt),tt.forEach(o),yt=l(S),K=u(S,"DIV",{class:!0});var st=Z(K);y(ye.$$.fragment,st),_t=l(st),Re=u(st,"P",{"data-svelte-h":!0}),J(Re)!=="svelte-1x3yxsa"&&(Re.textContent=Zt),st.forEach(o),Mt=l(S),O=u(S,"DIV",{class:!0});var ot=Z(O);y(_e.$$.fragment,ot),bt=l(ot),Se=u(ot,"P",{"data-svelte-h":!0}),J(Se)!=="svelte-1eb2h1k"&&(Se.textContent=xt),ot.forEach(o),Tt=l(S),ee=u(S,"DIV",{class:!0});var nt=Z(ee);y(Me.$$.fragment,nt),wt=l(nt),Ve=u(nt,"P",{"data-svelte-h":!0}),J(Ve)!=="svelte-5x4wxx"&&(Ve.textContent=Wt),nt.forEach(o),S.forEach(o),Ae=l(e),be=u(e,"UL",{"data-svelte-h":!0}),J(be)!=="svelte-74hxmd"&&(be.innerHTML=kt),De=l(e),y(te.$$.fragment,e),Ke=l(e),y(Te.$$.fragment,e),Oe=l(e),He=u(e,"P",{}),Z(He).forEach(o),this.h()},h(){F(t,"name","hf:doc:metadata"),F(t,"content",as),Et(C,"float","right"),F(Q,"class","flex justify-center"),F(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),F(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){c(document.head,t),m(e,h,a),m(e,s,a),m(e,i,a),m(e,p,a),m(e,$,a),m(e,C,a),m(e,ne,a),_(V,e,a),m(e,N,a),m(e,E,a),m(e,U,a),m(e,z,a),m(e,B,a),_(X,e,a),m(e,q,a),m(e,j,a),m(e,x,a),_(W,e,a),m(e,R,a),_(P,e,a),m(e,Y,a),m(e,g,a),c(g,w),c(w,G),c(w,at),_(ae,w,null),c(g,rt),c(g,je),c(g,lt),c(g,re),c(re,Ce),c(re,it),_(le,re,null),c(g,ct),c(g,ie),c(ie,ze),c(ie,pt),_(ce,ie,null),m(e,Fe,a),m(e,Q,a),m(e,Ee,a),_(pe,e,a),m(e,Pe,a),m(e,me,a),m(e,Le,a),_(de,e,a),m(e,Ye,a),m(e,k,a),_(ue,k,null),c(k,mt),c(k,Ze),c(k,dt),c(k,xe),c(k,ut),_(A,k,null),m(e,qe,a),_(ge,e,a),m(e,Qe,a),m(e,I,a),_(he,I,null),c(I,gt),c(I,We),c(I,ht),c(I,D),_(fe,D,null),c(D,ft),c(D,ke),c(I,yt),c(I,K),_(ye,K,null),c(K,_t),c(K,Re),c(I,Mt),c(I,O),_(_e,O,null),c(O,bt),c(O,Se),c(I,Tt),c(I,ee),_(Me,ee,null),c(ee,wt),c(ee,Ve),m(e,Ae,a),m(e,be,a),m(e,De,a),_(te,e,a),m(e,Ke,a),_(Te,e,a),m(e,Oe,a),m(e,He,a),et=!0},p(e,[a]){const H={};a&2&&(H.$$scope={dirty:a,ctx:e}),X.$set(H);const we={};a&2&&(we.$$scope={dirty:a,ctx:e}),W.$set(we);const Je={};a&2&&(Je.$$scope={dirty:a,ctx:e}),A.$set(Je);const $e={};a&2&&($e.$$scope={dirty:a,ctx:e}),te.$set($e)},i(e){et||(M(V.$$.fragment,e),M(X.$$.fragment,e),M(W.$$.fragment,e),M(P.$$.fragment,e),M(ae.$$.fragment,e),M(le.$$.fragment,e),M(ce.$$.fragment,e),M(pe.$$.fragment,e),M(de.$$.fragment,e),M(ue.$$.fragment,e),M(A.$$.fragment,e),M(ge.$$.fragment,e),M(he.$$.fragment,e),M(fe.$$.fragment,e),M(ye.$$.fragment,e),M(_e.$$.fragment,e),M(Me.$$.fragment,e),M(te.$$.fragment,e),M(Te.$$.fragment,e),et=!0)},o(e){b(V.$$.fragment,e),b(X.$$.fragment,e),b(W.$$.fragment,e),b(P.$$.fragment,e),b(ae.$$.fragment,e),b(le.$$.fragment,e),b(ce.$$.fragment,e),b(pe.$$.fragment,e),b(de.$$.fragment,e),b(ue.$$.fragment,e),b(A.$$.fragment,e),b(ge.$$.fragment,e),b(he.$$.fragment,e),b(fe.$$.fragment,e),b(ye.$$.fragment,e),b(_e.$$.fragment,e),b(Me.$$.fragment,e),b(te.$$.fragment,e),b(Te.$$.fragment,e),et=!1},d(e){e&&(o(h),o(s),o(i),o(p),o($),o(C),o(ne),o(N),o(E),o(U),o(z),o(B),o(q),o(j),o(x),o(R),o(Y),o(g),o(Fe),o(Q),o(Ee),o(Pe),o(me),o(Le),o(Ye),o(k),o(qe),o(Qe),o(I),o(Ae),o(be),o(De),o(Ke),o(Oe),o(He)),o(t),T(V,e),T(X,e),T(W,e),T(P,e),T(ae),T(le),T(ce),T(pe,e),T(de,e),T(ue),T(A),T(ge,e),T(he),T(fe),T(ye),T(_e),T(Me),T(te,e),T(Te,e)}}}const as='{"title":"SuperGlue","local":"superglue","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"SuperGlueConfig","local":"transformers.SuperGlueConfig","sections":[],"depth":2},{"title":"SuperGlueImageProcessor","local":"transformers.SuperGlueImageProcessor","sections":[],"depth":2},{"title":"SuperGlueForKeypointMatching","local":"transformers.SuperGlueForKeypointMatching","sections":[],"depth":2}],"depth":1}';function rs(v){return Bt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fs extends Xt{constructor(t){super(),Ht(this,t,rs,ns,Nt,{})}}export{fs as component};
