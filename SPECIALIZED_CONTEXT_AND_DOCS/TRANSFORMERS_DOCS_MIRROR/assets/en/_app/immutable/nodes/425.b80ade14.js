import{s as Ws,z as Rs,o as Hs,n as ge}from"../chunks/scheduler.18a86fab.js";import{S as Ds,i as Es,g as r,s as o,r as u,A as Os,h as i,f as s,c as n,j as w,x as c,u as h,k,y as t,a as l,v as f,d as g,t as _,w as b}from"../chunks/index.98837b22.js";import{T as Kt}from"../chunks/Tip.77304350.js";import{D as U}from"../chunks/Docstring.a1ef7999.js";import{C as eo}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Dn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as R,E as Vs}from"../chunks/getInferenceSnippets.06c2775f.js";function Xs($){let d,y="This method is deprecated, <code>__call__</code> should be used instead.";return{c(){d=r("p"),d.innerHTML=y},l(p){d=i(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-1phrc72"&&(d.innerHTML=y)},m(p,v){l(p,d,v)},p:ge,d(p){p&&s(d)}}}function As($){let d,y="This method is deprecated, <code>__call__</code> should be used instead.";return{c(){d=r("p"),d.innerHTML=y},l(p){d=i(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-1phrc72"&&(d.innerHTML=y)},m(p,v){l(p,d,v)},p:ge,d(p){p&&s(d)}}}function Ss($){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=r("p"),d.innerHTML=y},l(p){d=i(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(p,v){l(p,d,v)},p:ge,d(p){p&&s(d)}}}function Ys($){let d,y="Example:",p,v,x;return v=new eo({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWwlMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQSUyMyUyMGxvYWQlMjBtb2RlbCUyMGFuZCUyMHByb2Nlc3NvciUwQSUyMyUyMGluJTIwdGhpcyUyMGNhc2UlMkMlMjB3ZSUyMGFscmVhZHklMjBoYXZlJTIwcGVyZm9ybWVkJTIwT0NSJTIwb3Vyc2VsdmVzJTBBJTIzJTIwc28lMjB3ZSUyMGluaXRpYWxpemUlMjB0aGUlMjBwcm9jZXNzb3IlMjB3aXRoJTIwJTYwYXBwbHlfb2NyJTNERmFsc2UlNjAlMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZ1ZG9wLWxhcmdlJTIyJTJDJTIwYXBwbHlfb2NyJTNERmFsc2UpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnVkb3AtbGFyZ2UlMjIpJTBBJTBBJTIzJTIwbG9hZCUyMGFuJTIwZXhhbXBsZSUyMGltYWdlJTJDJTIwYWxvbmclMjB3aXRoJTIwdGhlJTIwd29yZHMlMjBhbmQlMjBjb29yZGluYXRlcyUwQSUyMyUyMHdoaWNoJTIwd2VyZSUyMGV4dHJhY3RlZCUyMHVzaW5nJTIwYW4lMjBPQ1IlMjBlbmdpbmUlMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMm5pZWxzciUyRmZ1bnNkLWxheW91dGxtdjMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQWV4YW1wbGUlMjAlM0QlMjBkYXRhc2V0JTVCMCU1RCUwQWltYWdlJTIwJTNEJTIwZXhhbXBsZSU1QiUyMmltYWdlJTIyJTVEJTBBd29yZHMlMjAlM0QlMjBleGFtcGxlJTVCJTIydG9rZW5zJTIyJTVEJTBBYm94ZXMlMjAlM0QlMjBleGFtcGxlJTVCJTIyYmJveGVzJTIyJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlJTJDJTIwd29yZHMlMkMlMjBib3hlcyUzRGJveGVzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFkZWNvZGVyX2lucHV0X2lkcyUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIlNUJtb2RlbC5jb25maWcuZGVjb2Rlcl9zdGFydF90b2tlbl9pZCU1RCU1RCklMEElMEElMjMlMjBmb3J3YXJkJTIwcGFzcyUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGRlY29kZXJfaW5wdXRfaWRzJTNEZGVjb2Rlcl9pbnB1dF9pZHMpJTBBbGFzdF9oaWRkZW5fc3RhdGVzJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQWxpc3QobGFzdF9oaWRkZW5fc3RhdGVzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load model and processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># in this case, we already have performed OCR ourselves</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># so we initialize the processor with \`apply_ocr=False\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>, apply_ocr=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load an example image, along with the words and coordinates</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># which were extracted using an OCR engine</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;nielsr/funsd-layoutlmv3&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example = dataset[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>image = example[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>words = example[<span class="hljs-string">&quot;tokens&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = example[<span class="hljs-string">&quot;bboxes&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(image, words, boxes=boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1024</span>]`,wrap:!1}}),{c(){d=r("p"),d.textContent=y,p=o(),u(v.$$.fragment)},l(m){d=i(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),p=n(m),h(v.$$.fragment,m)},m(m,J){l(m,d,J),l(m,p,J),f(v,m,J),x=!0},p:ge,i(m){x||(g(v.$$.fragment,m),x=!0)},o(m){_(v.$$.fragment,m),x=!1},d(m){m&&(s(d),s(p)),b(v,m)}}}function Qs($){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=r("p"),d.innerHTML=y},l(p){d=i(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(p,v){l(p,d,v)},p:ge,d(p){p&&s(d)}}}function Ks($){let d,y="Examples:",p,v,x;return v=new eo({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, UdopForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load model and processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># in this case, we already have performed OCR ourselves</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># so we initialize the processor with \`apply_ocr=False\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>, apply_ocr=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UdopForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load an example image, along with the words and coordinates</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># which were extracted using an OCR engine</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;nielsr/funsd-layoutlmv3&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example = dataset[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>image = example[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>words = example[<span class="hljs-string">&quot;tokens&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = example[<span class="hljs-string">&quot;bboxes&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># one can use the various task prefixes (prompts) used during pre-training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># e.g. the task prefix for DocVQA is &quot;Question answering. &quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Question answering. What is the date on the form?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = processor(image, question, text_pair=words, boxes=boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># autoregressive generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = model.generate(**encoding)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(processor.batch_decode(predicted_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
<span class="hljs-number">9</span>/<span class="hljs-number">30</span>/<span class="hljs-number">92</span>`,wrap:!1}}),{c(){d=r("p"),d.textContent=y,p=o(),u(v.$$.fragment)},l(m){d=i(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-kvfsh7"&&(d.textContent=y),p=n(m),h(v.$$.fragment,m)},m(m,J){l(m,d,J),l(m,p,J),f(v,m,J),x=!0},p:ge,i(m){x||(g(v.$$.fragment,m),x=!0)},o(m){_(v.$$.fragment,m),x=!1},d(m){m&&(s(d),s(p)),b(v,m)}}}function ea($){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=r("p"),d.innerHTML=y},l(p){d=i(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(p,v){l(p,d,v)},p:ge,d(p){p&&s(d)}}}function ta($){let d,y="Example:",p,v,x;return v=new eo({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBVZG9wRW5jb2Rlck1vZGVsJTBBZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMGhmX2h1Yl9kb3dubG9hZCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQSUyMyUyMGxvYWQlMjBtb2RlbCUyMGFuZCUyMHByb2Nlc3NvciUwQSUyMyUyMGluJTIwdGhpcyUyMGNhc2UlMkMlMjB3ZSUyMGFscmVhZHklMjBoYXZlJTIwcGVyZm9ybWVkJTIwT0NSJTIwb3Vyc2VsdmVzJTBBJTIzJTIwc28lMjB3ZSUyMGluaXRpYWxpemUlMjB0aGUlMjBwcm9jZXNzb3IlMjB3aXRoJTIwJTYwYXBwbHlfb2NyJTNERmFsc2UlNjAlMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZ1ZG9wLWxhcmdlJTIyJTJDJTIwYXBwbHlfb2NyJTNERmFsc2UpJTBBbW9kZWwlMjAlM0QlMjBVZG9wRW5jb2Rlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZ1ZG9wLWxhcmdlJTIyKSUwQSUwQSUyMyUyMGxvYWQlMjBhbiUyMGV4YW1wbGUlMjBpbWFnZSUyQyUyMGFsb25nJTIwd2l0aCUyMHRoZSUyMHdvcmRzJTIwYW5kJTIwY29vcmRpbmF0ZXMlMEElMjMlMjB3aGljaCUyMHdlcmUlMjBleHRyYWN0ZWQlMjB1c2luZyUyMGFuJTIwT0NSJTIwZW5naW5lJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJuaWVsc3IlMkZmdW5zZC1sYXlvdXRsbXYzJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiUyMiklMEFleGFtcGxlJTIwJTNEJTIwZGF0YXNldCU1QjAlNUQlMEFpbWFnZSUyMCUzRCUyMGV4YW1wbGUlNUIlMjJpbWFnZSUyMiU1RCUwQXdvcmRzJTIwJTNEJTIwZXhhbXBsZSU1QiUyMnRva2VucyUyMiU1RCUwQWJveGVzJTIwJTNEJTIwZXhhbXBsZSU1QiUyMmJib3hlcyUyMiU1RCUwQWVuY29kaW5nJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlJTJDJTIwd29yZHMlMkMlMjBib3hlcyUzRGJveGVzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGluZyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, UdopEncoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load model and processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># in this case, we already have performed OCR ourselves</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># so we initialize the processor with \`apply_ocr=False\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>, apply_ocr=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UdopEncoderModel.from_pretrained(<span class="hljs-string">&quot;microsoft/udop-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load an example image, along with the words and coordinates</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># which were extracted using an OCR engine</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;nielsr/funsd-layoutlmv3&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example = dataset[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>image = example[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>words = example[<span class="hljs-string">&quot;tokens&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes = example[<span class="hljs-string">&quot;bboxes&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = processor(image, words, boxes=boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoding)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){d=r("p"),d.textContent=y,p=o(),u(v.$$.fragment)},l(m){d=i(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),p=n(m),h(v.$$.fragment,m)},m(m,J){l(m,d,J),l(m,p,J),f(v,m,J),x=!0},p:ge,i(m){x||(g(v.$$.fragment,m),x=!0)},o(m){_(v.$$.fragment,m),x=!1},d(m){m&&(s(d),s(p)),b(v,m)}}}function oa($){let d,y,p,v,x,m="<em>This model was released on 2022-12-05 and added to Hugging Face Transformers on 2024-03-04.</em>",J,_e,to,Q,En='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',oo,be,no,ve,On=`The UDOP model was proposed in <a href="https://huggingface.co/papers/2212.02623" rel="nofollow">Unifying Vision, Text, and Layout for Universal Document Processing</a> by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal.
UDOP adopts an encoder-decoder Transformer architecture based on <a href="t5">T5</a> for document AI tasks like document image classification, document parsing and document visual question answering.`,so,ye,Vn="The abstract from the paper is the following:",ao,ke,Xn="We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 9 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark (DUE).*",ro,K,An,io,Te,Sn='UDOP architecture. Taken from the <a href="https://huggingface.co/papers/2212.02623">original paper.</a>',lo,we,co,xe,Yn=`<li>In addition to <em>input_ids</em>, <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a> also expects the input <code>bbox</code>, which are
the bounding boxes (i.e. 2D-positions) of the input tokens. These can be obtained using an external OCR engine such
as Google‚Äôs <a href="https://github.com/tesseract-ocr/tesseract" rel="nofollow">Tesseract</a> (there‚Äôs a <a href="https://pypi.org/project/pytesseract/" rel="nofollow">Python wrapper</a> available). Each bounding box should be in (x0, y0, x1, y1) format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the
position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000
scale. To normalize, you can use the following function:</li>`,po,Me,mo,Ue,Qn=`Here, <code>width</code> and <code>height</code> correspond to the width and height of the original document in which the token
occurs. Those can be obtained using the Python Image Library (PIL) library for example, as follows:`,uo,ze,ho,$e,Kn='One can use <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> to prepare images and text for the model, which takes care of all of this. By default, this class uses the Tesseract engine to extract a list of words and boxes (coordinates) from a given document. Its functionality is equivalent to that of <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor">LayoutLMv3Processor</a>, hence it supports passing either <code>apply_ocr=False</code> in case you prefer to use your own OCR engine or <code>apply_ocr=True</code> in case you want the default OCR engine to be used. Refer to the <a href="layoutlmv2#usage-layoutlmv2processor">usage guide of LayoutLMv2</a> regarding all possible use cases (the functionality of <code>UdopProcessor</code> is identical).',fo,Ce,es='<li>If using an own OCR engine of choice, one recommendation is Azure‚Äôs <a href="https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/call-read-api" rel="nofollow">Read API</a>, which supports so-called line segments. Use of segment position embeddings typically results in better performance.</li> <li>At inference time, it‚Äôs recommended to use the <code>generate</code> method to autoregressively generate text given a document image.</li> <li>The model has been pre-trained on both self-supervised and supervised objectives. One can use the various task prefixes (prompts) used during pre-training to test out the out-of-the-box capabilities. For instance, the model can be prompted with ‚ÄúQuestion answering. What is the date?‚Äù, as ‚ÄúQuestion answering.‚Äù is the task prefix used during pre-training for DocVQA. Refer to the <a href="https://huggingface.co/papers/2212.02623" rel="nofollow">paper</a> (table 1) for all task prefixes.</li> <li>One can also fine-tune <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopEncoderModel">UdopEncoderModel</a>, which is the encoder-only part of UDOP, which can be seen as a LayoutLMv3-like Transformer encoder. For discriminative tasks, one can just add a linear classifier on top of it and fine-tune it on a labeled dataset.</li>',go,je,ts=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/microsoft/UDOP" rel="nofollow">here</a>.`,_o,Ie,bo,Je,os=`A list of official Hugging Face and community (indicated by üåé) resources to help you get started with UDOP. If
you‚Äôre interested in submitting a resource to be included here, please feel free to open a Pull Request and we‚Äôll
review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.`,vo,qe,ns=`<li>Demo notebooks regarding UDOP can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UDOP" rel="nofollow">here</a> that show how
to fine-tune UDOP on a custom dataset as well as inference. üåé</li> <li><a href="../tasks/document_question_answering">Document question answering task guide</a></li>`,yo,Fe,ko,W,Pe,Go,pt,ss=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a>. It is used to
instantiate a UDOP model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UDOP
<a href="https://huggingface.co/microsoft/udop-large" rel="nofollow">microsoft/udop-large</a> architecture.`,Zo,mt,as=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,To,Le,wo,z,Ne,Wo,ut,rs=`Adapted from <a href="/docs/transformers/v4.56.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer">LayoutXLMTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a>. Based on
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.`,Ro,ht,is=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Ho,H,Be,Do,ft,ds=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`,Eo,gt,ls="<li>single sequence: <code>X &lt;/s&gt;</code></li> <li>pair of sequences: <code>A &lt;/s&gt; B &lt;/s&gt;</code></li>",Oo,ee,Ge,Vo,_t,cs=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Xo,te,Ze,Ao,bt,ps=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`,So,vt,We,xo,Re,Mo,T,He,Yo,yt,ms=`Construct a ‚Äúfast‚Äù UDOP tokenizer (backed by HuggingFace‚Äôs <em>tokenizers</em> library). Adapted from
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer">LayoutXLMTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5Tokenizer">T5Tokenizer</a>. Based on
<a href="https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models" rel="nofollow">BPE</a>.`,Qo,kt,us=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Ko,D,De,en,Tt,hs="Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.",tn,oe,on,E,Ee,nn,wt,fs=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLM-RoBERTa sequence has the following format:`,sn,xt,gs="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",an,ne,Oe,rn,Mt,_s=`Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
sequences with word-level normalized bounding boxes and optional labels.`,dn,se,Ve,ln,Ut,bs=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLM-RoBERTa does
not make use of token type ids, therefore a list of zeros is returned.`,cn,zt,Xe,pn,O,Ae,mn,$t,vs="Tokenize and prepare for the model a sequence or a pair of sequences.",un,ae,Uo,Se,zo,C,Ye,hn,Ct,ys="Constructs a UDOP processor which combines a LayoutLMv3 image processor and a UDOP tokenizer into a single processor.",fn,jt,ks='<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> offers all the functionalities you need to prepare data for the model.',gn,It,Ts=`It first uses <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> to resize, rescale and normalize document images, and optionally applies OCR
to get words and normalized bounding boxes. These are then provided to <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizer">UdopTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizerFast">UdopTokenizerFast</a>,
which turns the words and bounding boxes into token-level <code>input_ids</code>, <code>attention_mask</code>, <code>token_type_ids</code>, <code>bbox</code>.
Optionally, one can provide integer <code>word_labels</code>, which are turned into token-level <code>labels</code> for token
classification tasks (such as FUNSD, CORD).`,_n,Jt,ws=`Additionally, it also supports passing <code>text_target</code> and <code>text_pair_target</code> to the tokenizer, which can be used to
prepare labels for language modeling tasks.`,bn,P,Qe,vn,qt,xs=`This method first forwards the <code>images</code> argument to <code>~UdopImageProcessor.__call__</code>. In case
<code>UdopImageProcessor</code> was initialized with <code>apply_ocr</code> set to <code>True</code>, it passes the obtained words and
bounding boxes along with the additional arguments to <code>__call__()</code> and returns the output,
together with the prepared <code>pixel_values</code>. In case <code>UdopImageProcessor</code> was initialized with <code>apply_ocr</code> set
to <code>False</code>, it passes the words (<code>text</code>/\`<code>text_pair</code>) and <code>boxes</code> specified by the user along with the
additional arguments to <code>__call__()</code> and returns the output, together with the prepared
<code>pixel_values</code>.`,yn,Ft,Ms="Alternatively, one can pass <code>text_target</code> and <code>text_pair_target</code> to prepare the targets of UDOP.",kn,Pt,Us="Please refer to the docstring of the above two methods for more information.",$o,Ke,Co,q,et,Tn,Lt,zs="The bare Udop Model outputting raw hidden-states without any specific head on top.",wn,Nt,$s=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xn,Bt,Cs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Mn,L,tt,Un,Gt,js='The <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopModel">UdopModel</a> forward method, overrides the <code>__call__</code> special method.',zn,re,$n,ie,jo,ot,Io,j,nt,Cn,Zt,Is=`The UDOP encoder-decoder Transformer with a language modeling head on top, enabling to generate text given document
images and an optional prompt.`,jn,Wt,Js='This class is based on <a href="/docs/transformers/v4.56.2/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a>, extended to deal with images and layout (2D) data.',In,Rt,qs=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Jn,Ht,Fs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qn,N,st,Fn,Dt,Ps='The <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Pn,de,Ln,le,Jo,at,qo,F,rt,Nn,Et,Ls="The bare Udop Model outputting raw hidden-states without any specific head on top.",Bn,Ot,Ns=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Gn,Vt,Bs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Zn,B,it,Wn,Xt,Gs='The <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopEncoderModel">UdopEncoderModel</a> forward method, overrides the <code>__call__</code> special method.',Rn,ce,Hn,pe,Fo,dt,Po,Yt,Lo;return _e=new R({props:{title:"UDOP",local:"udop",headingTag:"h1"}}),be=new R({props:{title:"Overview",local:"overview",headingTag:"h2"}}),we=new R({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Me=new eo({props:{code:"ZGVmJTIwbm9ybWFsaXplX2Jib3goYmJveCUyQyUyMHdpZHRoJTJDJTIwaGVpZ2h0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMCU1QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGludCgxMDAwJTIwKiUyMChiYm94JTVCMCU1RCUyMCUyRiUyMHdpZHRoKSklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpbnQoMTAwMCUyMColMjAoYmJveCU1QjElNUQlMjAlMkYlMjBoZWlnaHQpKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGludCgxMDAwJTIwKiUyMChiYm94JTVCMiU1RCUyMCUyRiUyMHdpZHRoKSklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpbnQoMTAwMCUyMColMjAoYmJveCU1QjMlNUQlMjAlMkYlMjBoZWlnaHQpKSUyQyUwQSUyMCUyMCUyMCUyMCU1RA==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">normalize_bbox</span>(<span class="hljs-params">bbox, width, height</span>):
    <span class="hljs-keyword">return</span> [
        <span class="hljs-built_in">int</span>(<span class="hljs-number">1000</span> * (bbox[<span class="hljs-number">0</span>] / width)),
        <span class="hljs-built_in">int</span>(<span class="hljs-number">1000</span> * (bbox[<span class="hljs-number">1</span>] / height)),
        <span class="hljs-built_in">int</span>(<span class="hljs-number">1000</span> * (bbox[<span class="hljs-number">2</span>] / width)),
        <span class="hljs-built_in">int</span>(<span class="hljs-number">1000</span> * (bbox[<span class="hljs-number">3</span>] / height)),
    ]`,wrap:!1}}),ze=new eo({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBJTIzJTIwRG9jdW1lbnQlMjBjYW4lMjBiZSUyMGElMjBwbmclMkMlMjBqcGclMkMlMjBldGMuJTIwUERGcyUyMG11c3QlMjBiZSUyMGNvbnZlcnRlZCUyMHRvJTIwaW1hZ2VzLiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihuYW1lX29mX3lvdXJfZG9jdW1lbnQpLmNvbnZlcnQoJTIyUkdCJTIyKSUwQSUwQXdpZHRoJTJDJTIwaGVpZ2h0JTIwJTNEJTIwaW1hZ2Uuc2l6ZQ==",highlighted:`<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-comment"># Document can be a png, jpg, etc. PDFs must be converted to images.</span>
image = Image.<span class="hljs-built_in">open</span>(name_of_your_document).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

width, height = image.size`,wrap:!1}}),Ie=new R({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Fe=new R({props:{title:"UdopConfig",local:"transformers.UdopConfig",headingTag:"h2"}}),Pe=new U({props:{name:"class transformers.UdopConfig",anchor:"transformers.UdopConfig",parameters:[{name:"vocab_size",val:" = 33201"},{name:"d_model",val:" = 1024"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 4096"},{name:"num_layers",val:" = 24"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 16"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"relative_bias_args",val:" = [{'type': '1d'}, {'type': 'horizontal'}, {'type': 'vertical'}]"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'relu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"max_2d_position_embeddings",val:" = 1024"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 33201) &#x2014;
Vocabulary size of the UDOP model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a>.`,name:"vocab_size"},{anchor:"transformers.UdopConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.UdopConfig.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. The <code>inner_dim</code> of the projection layer will
be defined as <code>num_heads * d_kv</code>.`,name:"d_kv"},{anchor:"transformers.UdopConfig.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Size of the intermediate feed forward layer in each <code>UdopBlock</code>.`,name:"d_ff"},{anchor:"transformers.UdopConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder and decoder.`,name:"num_layers"},{anchor:"transformers.UdopConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.UdopConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder and decoder.`,name:"num_heads"},{anchor:"transformers.UdopConfig.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.UdopConfig.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.UdopConfig.relative_bias_args",description:`<strong>relative_bias_args</strong> (<code>list[dict]</code>, <em>optional</em>, defaults to <code>[{&apos;type&apos; -- &apos;1d&apos;}, {&apos;type&apos;: &apos;horizontal&apos;}, {&apos;type&apos;: &apos;vertical&apos;}]</code>):
A list of dictionaries containing the arguments for the relative bias layers.`,name:"relative_bias_args"},{anchor:"transformers.UdopConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.UdopConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.UdopConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.UdopConfig.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>. Udopv1.1 uses the
<code>&quot;gated-gelu&quot;</code> feed forward projection. Original Udop uses <code>&quot;relu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.UdopConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the model should behave as an encoder/decoder or not.`,name:"is_encoder_decoder"},{anchor:"transformers.UdopConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.UdopConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The id of the padding token in the vocabulary.`,name:"pad_token_id"},{anchor:"transformers.UdopConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The id of the end-of-sequence token in the vocabulary.`,name:"eos_token_id"},{anchor:"transformers.UdopConfig.max_2d_position_embeddings",description:`<strong>max_2d_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum absolute position embeddings for relative position encoding.`,name:"max_2d_position_embeddings"},{anchor:"transformers.UdopConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size of the input images.`,name:"image_size"},{anchor:"transformers.UdopConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The patch size used by the vision encoder.`,name:"patch_size"},{anchor:"transformers.UdopConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of channels in the input images.`,name:"num_channels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/configuration_udop.py#L24"}}),Le=new R({props:{title:"UdopTokenizer",local:"transformers.UdopTokenizer",headingTag:"h2"}}),Ne=new U({props:{name:"class transformers.UdopTokenizer",anchor:"transformers.UdopTokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"sep_token_box",val:" = [1000, 1000, 1000, 1000]"},{name:"pad_token_box",val:" = [0, 0, 0, 0]"},{name:"pad_token_label",val:" = -100"},{name:"only_label_first_subword",val:" = True"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"legacy",val:" = True"},{name:"add_prefix_space",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.UdopTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.UdopTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.UdopTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.UdopTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.UdopTokenizer.sep_token_box",description:`<strong>sep_token_box</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1000, 1000, 1000, 1000]</code>) &#x2014;
The bounding box to use for the special [SEP] token.`,name:"sep_token_box"},{anchor:"transformers.UdopTokenizer.pad_token_box",description:`<strong>pad_token_box</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[0, 0, 0, 0]</code>) &#x2014;
The bounding box to use for the special [PAD] token.`,name:"pad_token_box"},{anchor:"transformers.UdopTokenizer.pad_token_label",description:`<strong>pad_token_label</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The label to use for padding tokens. Defaults to -100, which is the <code>ignore_index</code> of PyTorch&#x2019;s
CrossEntropyLoss.`,name:"pad_token_label"},{anchor:"transformers.UdopTokenizer.only_label_first_subword",description:`<strong>only_label_first_subword</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to only label the first subword, in case word labels are provided.`,name:"only_label_first_subword"},{anchor:"transformers.UdopTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.UdopTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.UdopTokenizer.legacy",description:`<strong>legacy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the <code>legacy</code> behaviour of the tokenizer should be used. Legacy is before the merge of #24622
which includes fixes to properly handle tokens that appear after special tokens. A simple example:</p>
<ul>
<li><code>legacy=True</code>:</li>
</ul>`,name:"legacy"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop.py#L152"}}),Be=new U({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.UdopTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.UdopTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.UdopTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop.py#L384",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Ge=new U({props:{name:"get_special_tokens_mask",anchor:"transformers.UdopTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.UdopTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.UdopTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.UdopTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop.py#L310",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Ze=new U({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.UdopTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.UdopTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.UdopTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop.py#L361",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),We=new U({props:{name:"save_vocabulary",anchor:"transformers.UdopTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop.py#L492"}}),Re=new R({props:{title:"UdopTokenizerFast",local:"transformers.UdopTokenizerFast",headingTag:"h2"}}),He=new U({props:{name:"class transformers.UdopTokenizerFast",anchor:"transformers.UdopTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"sep_token_box",val:" = [1000, 1000, 1000, 1000]"},{name:"pad_token_box",val:" = [0, 0, 0, 0]"},{name:"pad_token_label",val:" = -100"},{name:"only_label_first_subword",val:" = True"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.UdopTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to the tokenizer file.`,name:"tokenizer_file"},{anchor:"transformers.UdopTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.UdopTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.UdopTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.UdopTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.UdopTokenizerFast.sep_token_box",description:`<strong>sep_token_box</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1000, 1000, 1000, 1000]</code>) &#x2014;
The bounding box to use for the special [SEP] token.`,name:"sep_token_box"},{anchor:"transformers.UdopTokenizerFast.pad_token_box",description:`<strong>pad_token_box</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[0, 0, 0, 0]</code>) &#x2014;
The bounding box to use for the special [PAD] token.`,name:"pad_token_box"},{anchor:"transformers.UdopTokenizerFast.pad_token_label",description:`<strong>pad_token_label</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The label to use for padding tokens. Defaults to -100, which is the <code>ignore_index</code> of PyTorch&#x2019;s
CrossEntropyLoss.`,name:"pad_token_label"},{anchor:"transformers.UdopTokenizerFast.only_label_first_subword",description:`<strong>only_label_first_subword</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to only label the first subword, in case word labels are provided.`,name:"only_label_first_subword"},{anchor:"transformers.UdopTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L148"}}),De=new U({props:{name:"batch_encode_plus_boxes",anchor:"transformers.UdopTokenizerFast.batch_encode_plus_boxes",parameters:[{name:"batch_text_or_text_pairs",val:": typing.Union[list[str], list[tuple[str, str]], list[list[str]]]"},{name:"is_pair",val:": typing.Optional[bool] = None"},{name:"boxes",val:": typing.Optional[list[list[list[int]]]] = None"},{name:"word_labels",val:": typing.Optional[list[list[int]]] = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"is_split_into_words",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_token_type_ids",val:": typing.Optional[bool] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.batch_encode_plus_boxes.batch_text_or_text_pairs",description:`<strong>batch_text_or_text_pairs</strong> (<code>list[str]</code>, <code>list[tuple[str, str]]</code>, <code>list[list[str]]</code>, <code>list[tuple[list[str], list[str]]]</code>, and for not-fast tokenizers, also <code>list[list[int]]</code>, <code>list[tuple[list[int], list[int]]]</code>) &#x2014;
Batch of sequences or pair of sequences to be encoded. This can be a list of
string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see
details in <code>encode_plus</code>).`,name:"batch_text_or_text_pairs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L438"}}),oe=new Kt({props:{warning:!0,$$slots:{default:[Xs]},$$scope:{ctx:$}}}),Ee=new U({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.UdopTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.UdopTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L957",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Oe=new U({props:{name:"call_boxes",anchor:"transformers.UdopTokenizerFast.call_boxes",parameters:[{name:"text",val:": typing.Union[str, list[str], list[list[str]]]"},{name:"text_pair",val:": typing.Union[list[str], list[list[str]], NoneType] = None"},{name:"boxes",val:": typing.Union[list[list[int]], list[list[list[int]]], NoneType] = None"},{name:"word_labels",val:": typing.Union[list[int], list[list[int]], NoneType] = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_token_type_ids",val:": typing.Optional[bool] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.call_boxes.text",description:`<strong>text</strong> (<code>str</code>, <code>list[str]</code>, <code>list[list[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string, a list of strings
(words of a single example or questions of a batch of examples) or a list of list of strings (batch of
words).`,name:"text"},{anchor:"transformers.UdopTokenizerFast.call_boxes.text_pair",description:`<strong>text_pair</strong> (<code>list[str]</code>, <code>list[list[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence should be a list of strings
(pretokenized string).`,name:"text_pair"},{anchor:"transformers.UdopTokenizerFast.call_boxes.boxes",description:`<strong>boxes</strong> (<code>list[list[int]]</code>, <code>list[list[list[int]]]</code>) &#x2014;
Word-level bounding boxes. Each bounding box should be normalized to be on a 0-1000 scale.`,name:"boxes"},{anchor:"transformers.UdopTokenizerFast.call_boxes.word_labels",description:`<strong>word_labels</strong> (<code>list[int]</code>, <code>list[list[int]]</code>, <em>optional</em>) &#x2014;
Word-level integer labels (for token classification tasks such as FUNSD, CORD).`,name:"word_labels"},{anchor:"transformers.UdopTokenizerFast.call_boxes.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to encode the sequences with the special tokens relative to their model.`,name:"add_special_tokens"},{anchor:"transformers.UdopTokenizerFast.call_boxes.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.UdopTokenizerFast.call_boxes.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the
maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.UdopTokenizerFast.call_boxes.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.UdopTokenizerFast.call_boxes.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to a number along with <code>max_length</code>, the overflowing tokens returned when
<code>return_overflowing_tokens=True</code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.`,name:"stride"},{anchor:"transformers.UdopTokenizerFast.call_boxes.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability <code>&gt;= 7.5</code> (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_token_type_ids",description:`<strong>return_token_type_ids</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"return_token_type_ids"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_overflowing_tokens",description:`<strong>return_overflowing_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch
of pairs) is provided with <code>truncation_strategy = longest_first</code> or <code>True</code>, an error is raised instead
of returning overflowing tokens.`,name:"return_overflowing_tokens"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_special_tokens_mask",description:`<strong>return_special_tokens_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return special tokens mask information.`,name:"return_special_tokens_mask"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_offsets_mapping",description:`<strong>return_offsets_mapping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return <code>(char_start, char_end)</code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>, if using
Python&#x2019;s tokenizer, this method will raise <code>NotImplementedError</code>.`,name:"return_offsets_mapping"},{anchor:"transformers.UdopTokenizerFast.call_boxes.return_length",description:`<strong>return_length</strong>  (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the lengths of the encoded inputs.`,name:"return_length"},{anchor:"transformers.UdopTokenizerFast.call_boxes.verbose",description:`<strong>verbose</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to print more information and warnings.`,name:"verbose"},{anchor:"transformers.UdopTokenizerFast.call_boxes.*kwargs",description:"*<strong>*kwargs</strong> &#x2014; passed to the <code>self.tokenize()</code> method",name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L272",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields:</p>
<ul>
<li>
<p><strong>input_ids</strong> ‚Äî List of token ids to be fed to a model.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
</li>
<li>
<p><strong>bbox</strong> ‚Äî List of bounding boxes to be fed to a model.</p>
</li>
<li>
<p><strong>token_type_ids</strong> ‚Äî List of token type ids to be fed to a model (when <code>return_token_type_ids=True</code> or
if <em>‚Äútoken_type_ids‚Äù</em> is in <code>self.model_input_names</code>).</p>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a></p>
</li>
<li>
<p><strong>attention_mask</strong> ‚Äî List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>‚Äúattention_mask‚Äù</em> is in <code>self.model_input_names</code>).</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
</li>
<li>
<p><strong>labels</strong> ‚Äî List of labels to be fed to a model. (when <code>word_labels</code> is specified).</p>
</li>
<li>
<p><strong>overflowing_tokens</strong> ‚Äî List of overflowing tokens sequences (when a <code>max_length</code> is specified and
<code>return_overflowing_tokens=True</code>).</p>
</li>
<li>
<p><strong>num_truncated_tokens</strong> ‚Äî Number of tokens truncated (when a <code>max_length</code> is specified and
<code>return_overflowing_tokens=True</code>).</p>
</li>
<li>
<p><strong>special_tokens_mask</strong> ‚Äî List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code>add_special_tokens=True</code> and <code>return_special_tokens_mask=True</code>).</p>
</li>
<li>
<p><strong>length</strong> ‚Äî The length of the inputs (when <code>return_length=True</code>).</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),Ve=new U({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.UdopTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.UdopTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L982",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Xe=new U({props:{name:"encode_boxes",anchor:"transformers.UdopTokenizerFast.encode_boxes",parameters:[{name:"text",val:": typing.Union[str, list[str], list[int]]"},{name:"text_pair",val:": typing.Union[str, list[str], list[int], NoneType] = None"},{name:"boxes",val:": typing.Optional[list[list[int]]] = None"},{name:"word_labels",val:": typing.Optional[list[list[int]]] = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.encode_boxes.Converts",description:"<strong>Converts</strong> a string to a sequence of ids (integer), using the tokenizer and vocabulary. Same as doing &#x2014;",name:"Converts"},{anchor:"transformers.UdopTokenizerFast.encode_boxes.self.convert_tokens_to_ids(self.tokenize(text)).",description:`<strong><code>self.convert_tokens_to_ids(self.tokenize(text))</code>.</strong> &#x2014;
text (<code>str</code>, <code>list[str]</code> or <code>list[int]</code>):
The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code>tokenize</code> method) or a list of integers (tokenized string ids using the <code>convert_tokens_to_ids</code>
method).
text_pair (<code>str</code>, <code>list[str]</code> or <code>list[int]</code>, <em>optional</em>):
Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code>tokenize</code> method) or a list of integers (tokenized string ids using the <code>convert_tokens_to_ids</code>
method).`,name:"self.convert_tokens_to_ids(self.tokenize(text))."}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L746"}}),Ae=new U({props:{name:"encode_plus_boxes",anchor:"transformers.UdopTokenizerFast.encode_plus_boxes",parameters:[{name:"text",val:": typing.Union[str, list[str]]"},{name:"text_pair",val:": typing.Optional[list[str]] = None"},{name:"boxes",val:": typing.Optional[list[list[int]]] = None"},{name:"word_labels",val:": typing.Optional[list[list[int]]] = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"is_split_into_words",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_token_type_ids",val:": typing.Optional[bool] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UdopTokenizerFast.encode_plus_boxes.text",description:`<strong>text</strong> (<code>str</code>, <code>list[str]</code> or (for non-fast tokenizers) <code>list[int]</code>) &#x2014;
The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code>tokenize</code> method) or a list of integers (tokenized string ids using the <code>convert_tokens_to_ids</code>
method).`,name:"text"},{anchor:"transformers.UdopTokenizerFast.encode_plus_boxes.text_pair",description:`<strong>text_pair</strong> (<code>str</code>, <code>list[str]</code> or <code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code>tokenize</code> method) or a list of integers (tokenized string ids using the <code>convert_tokens_to_ids</code>
method).`,name:"text_pair"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/tokenization_udop_fast.py#L789"}}),ae=new Kt({props:{warning:!0,$$slots:{default:[As]},$$scope:{ctx:$}}}),Se=new R({props:{title:"UdopProcessor",local:"transformers.UdopProcessor",headingTag:"h2"}}),Ye=new U({props:{name:"class transformers.UdopProcessor",anchor:"transformers.UdopProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.UdopProcessor.image_processor",description:`<strong>image_processor</strong> (<code>LayoutLMv3ImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.UdopProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>UdopTokenizer</code> or <code>UdopTokenizerFast</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizer">UdopTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopTokenizerFast">UdopTokenizerFast</a>. The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/processing_udop.py#L55"}}),Qe=new U({props:{name:"__call__",anchor:"transformers.UdopProcessor.__call__",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"text",val:": typing.Union[str, list[str], list[list[str]]] = None"},{name:"audio",val:" = None"},{name:"videos",val:" = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.udop.processing_udop.UdopProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/processing_udop.py#L84"}}),Ke=new R({props:{title:"UdopModel",local:"transformers.UdopModel",headingTag:"h2"}}),et=new U({props:{name:"class transformers.UdopModel",anchor:"transformers.UdopModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.UdopModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopModel">UdopModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1481"}}),tt=new U({props:{name:"forward",anchor:"transformers.UdopModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"visual_bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:" = True"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.UdopModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.UdopModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.UdopModel.forward.bbox",description:`<strong>bbox</strong> (<code>torch.LongTensor</code> of shape <code>({0}, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each input sequence tokens. Selected in the range <code>[0, config.max_2d_position_embeddings-1]</code>. Each bounding box should be a normalized version in (x0, y0, x1, y1)
format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,
y1) represents the position of the lower right corner.</p>
<p>Note that <code>sequence_length = token_sequence_length + patch_sequence_length + 1</code> where <code>1</code> is for [CLS]
token. See <code>pixel_values</code> for <code>patch_sequence_length</code>.`,name:"bbox"},{anchor:"transformers.UdopModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">LayoutLMv3ImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.UdopModel.forward.visual_bbox",description:`<strong>visual_bbox</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, patch_sequence_length, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each patch in the image. If not provided, bounding boxes are created in the model.`,name:"visual_bbox"},{anchor:"transformers.UdopModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary. Indices can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.
<a href="../glossary#decoder-input-ids">What are decoder input IDs?</a> T5 uses the <code>pad_token_id</code> as the starting
token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code> is used, optionally only the last
<code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>). To know more on how to prepare
<code>decoder_input_ids</code> for pretraining take a look at <a href="./t5#training">T5 Training</a>.`,name:"decoder_input_ids"},{anchor:"transformers.UdopModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.UdopModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.UdopModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.UdopModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.UdopModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.UdopModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be
input (see <code>past_key_values</code>). This is useful if you want more control over how to convert
<code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code> takes the value
of <code>inputs_embeds</code>.`,name:"decoder_inputs_embeds"},{anchor:"transformers.UdopModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.UdopModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in
<code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.UdopModel.forward.use_cache",description:"<strong>use_cache</strong> (`<code>, defaults to </code>True<code>) -- If set to </code>True<code>, </code>past_key_values<code>key value states are returned and can be used to speed up decoding (see</code>past_key_values`).",name:"use_cache"},{anchor:"transformers.UdopModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UdopModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UdopModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UdopModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1524",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[torch.Tensor, ...]</code></p>
`}}),re=new Kt({props:{$$slots:{default:[Ss]},$$scope:{ctx:$}}}),ie=new Dn({props:{anchor:"transformers.UdopModel.forward.example",$$slots:{default:[Ys]},$$scope:{ctx:$}}}),ot=new R({props:{title:"UdopForConditionalGeneration",local:"transformers.UdopForConditionalGeneration",headingTag:"h2"}}),nt=new U({props:{name:"class transformers.UdopForConditionalGeneration",anchor:"transformers.UdopForConditionalGeneration",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.UdopForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopForConditionalGeneration">UdopForConditionalGeneration</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1673"}}),st=new U({props:{name:"forward",anchor:"transformers.UdopForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"visual_bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:" = True"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.UdopForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.UdopForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.UdopForConditionalGeneration.forward.bbox",description:`<strong>bbox</strong> (<code>torch.LongTensor</code> of shape <code>({0}, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each input sequence tokens. Selected in the range <code>[0, config.max_2d_position_embeddings-1]</code>. Each bounding box should be a normalized version in (x0, y0, x1, y1)
format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,
y1) represents the position of the lower right corner.</p>
<p>Note that <code>sequence_length = token_sequence_length + patch_sequence_length + 1</code> where <code>1</code> is for [CLS]
token. See <code>pixel_values</code> for <code>patch_sequence_length</code>.`,name:"bbox"},{anchor:"transformers.UdopForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">LayoutLMv3ImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.UdopForConditionalGeneration.forward.visual_bbox",description:`<strong>visual_bbox</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, patch_sequence_length, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each patch in the image. If not provided, bounding boxes are created in the model.`,name:"visual_bbox"},{anchor:"transformers.UdopForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary. Indices can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.
<a href="../glossary#decoder-input-ids">What are decoder input IDs?</a> T5 uses the <code>pad_token_id</code> as the starting
token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code> is used, optionally only the last
<code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>). To know more on how to prepare
<code>decoder_input_ids</code> for pretraining take a look at <a href="./t5#training">T5 Training</a>.`,name:"decoder_input_ids"},{anchor:"transformers.UdopForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.UdopForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.UdopForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.UdopForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.UdopForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.UdopForConditionalGeneration.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be
input (see <code>past_key_values</code>). This is useful if you want more control over how to convert
<code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code> takes the value
of <code>inputs_embeds</code>.`,name:"decoder_inputs_embeds"},{anchor:"transformers.UdopForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.UdopForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in
<code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.UdopForConditionalGeneration.forward.use_cache",description:"<strong>use_cache</strong> (`<code>, defaults to </code>True<code>) -- If set to </code>True<code>, </code>past_key_values<code>key value states are returned and can be used to speed up decoding (see</code>past_key_values`).",name:"use_cache"},{anchor:"transformers.UdopForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UdopForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UdopForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UdopForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.UdopForConditionalGeneration.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1720",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[torch.Tensor, ...]</code></p>
`}}),de=new Kt({props:{$$slots:{default:[Qs]},$$scope:{ctx:$}}}),le=new Dn({props:{anchor:"transformers.UdopForConditionalGeneration.forward.example",$$slots:{default:[Ks]},$$scope:{ctx:$}}}),at=new R({props:{title:"UdopEncoderModel",local:"transformers.UdopEncoderModel",headingTag:"h2"}}),rt=new U({props:{name:"class transformers.UdopEncoderModel",anchor:"transformers.UdopEncoderModel",parameters:[{name:"config",val:": UdopConfig"}],parametersDescription:[{anchor:"transformers.UdopEncoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopConfig">UdopConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1884"}}),it=new U({props:{name:"forward",anchor:"transformers.UdopEncoderModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"visual_bbox",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.UdopEncoderModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you
should be able to pad the inputs on both the right and the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for detail.</p>
<p>To know more on how to prepare <code>input_ids</code> for pretraining take a look a <a href="./t5#training">T5 Training</a>.`,name:"input_ids"},{anchor:"transformers.UdopEncoderModel.forward.bbox",description:`<strong>bbox</strong> (<code>torch.LongTensor</code> of shape <code>({0}, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each input sequence tokens. Selected in the range <code>[0, config.max_2d_position_embeddings-1]</code>. Each bounding box should be a normalized version in (x0, y0, x1, y1)
format, where (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1,
y1) represents the position of the lower right corner.</p>
<p>Note that <code>sequence_length = token_sequence_length + patch_sequence_length + 1</code> where <code>1</code> is for [CLS]
token. See <code>pixel_values</code> for <code>patch_sequence_length</code>.`,name:"bbox"},{anchor:"transformers.UdopEncoderModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.UdopEncoderModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">LayoutLMv3ImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopProcessor">UdopProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.UdopEncoderModel.forward.visual_bbox",description:`<strong>visual_bbox</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, patch_sequence_length, 4)</code>, <em>optional</em>) &#x2014;
Bounding boxes of each patch in the image. If not provided, bounding boxes are created in the model.`,name:"visual_bbox"},{anchor:"transformers.UdopEncoderModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.UdopEncoderModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.UdopEncoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UdopEncoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UdopEncoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/udop/modeling_udop.py#L1926",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.udop.modeling_udop.BaseModelOutputWithAttentionMask</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/udop#transformers.UdopConfig"
>UdopConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model. If <code>past_key_values</code> is used only
the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</li>
<li><strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) ‚Äî Attention mask used in the model‚Äôs forward pass to avoid performing attention on padding token indices.
Mask values selected in <code>[0, 1]</code> ‚Äî - 1 for tokens that are <strong>not masked</strong>,<ul>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul></li>
<li><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>. Contains pre-computed hidden-states (key and values in the
self-attention blocks and optionally if <code>config.is_encoder_decoder=True</code> in the cross-attention blocks)
that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of
the model at the output of each layer plus the optional initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder‚Äôs cross-attention layer, after the attention softmax,
used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.udop.modeling_udop.BaseModelOutputWithAttentionMask</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ce=new Kt({props:{$$slots:{default:[ea]},$$scope:{ctx:$}}}),pe=new Dn({props:{anchor:"transformers.UdopEncoderModel.forward.example",$$slots:{default:[ta]},$$scope:{ctx:$}}}),dt=new Vs({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/udop.md"}}),{c(){d=r("meta"),y=o(),p=r("p"),v=o(),x=r("p"),x.innerHTML=m,J=o(),u(_e.$$.fragment),to=o(),Q=r("div"),Q.innerHTML=En,oo=o(),u(be.$$.fragment),no=o(),ve=r("p"),ve.innerHTML=On,so=o(),ye=r("p"),ye.textContent=Vn,ao=o(),ke=r("p"),ke.textContent=Xn,ro=o(),K=r("img"),io=o(),Te=r("small"),Te.innerHTML=Sn,lo=o(),u(we.$$.fragment),co=o(),xe=r("ul"),xe.innerHTML=Yn,po=o(),u(Me.$$.fragment),mo=o(),Ue=r("p"),Ue.innerHTML=Qn,uo=o(),u(ze.$$.fragment),ho=o(),$e=r("p"),$e.innerHTML=Kn,fo=o(),Ce=r("ul"),Ce.innerHTML=es,go=o(),je=r("p"),je.innerHTML=ts,_o=o(),u(Ie.$$.fragment),bo=o(),Je=r("p"),Je.textContent=os,vo=o(),qe=r("ul"),qe.innerHTML=ns,yo=o(),u(Fe.$$.fragment),ko=o(),W=r("div"),u(Pe.$$.fragment),Go=o(),pt=r("p"),pt.innerHTML=ss,Zo=o(),mt=r("p"),mt.innerHTML=as,To=o(),u(Le.$$.fragment),wo=o(),z=r("div"),u(Ne.$$.fragment),Wo=o(),ut=r("p"),ut.innerHTML=rs,Ro=o(),ht=r("p"),ht.innerHTML=is,Ho=o(),H=r("div"),u(Be.$$.fragment),Do=o(),ft=r("p"),ft.textContent=ds,Eo=o(),gt=r("ul"),gt.innerHTML=ls,Oo=o(),ee=r("div"),u(Ge.$$.fragment),Vo=o(),_t=r("p"),_t.innerHTML=cs,Xo=o(),te=r("div"),u(Ze.$$.fragment),Ao=o(),bt=r("p"),bt.textContent=ps,So=o(),vt=r("div"),u(We.$$.fragment),xo=o(),u(Re.$$.fragment),Mo=o(),T=r("div"),u(He.$$.fragment),Yo=o(),yt=r("p"),yt.innerHTML=ms,Qo=o(),kt=r("p"),kt.innerHTML=us,Ko=o(),D=r("div"),u(De.$$.fragment),en=o(),Tt=r("p"),Tt.textContent=hs,tn=o(),u(oe.$$.fragment),on=o(),E=r("div"),u(Ee.$$.fragment),nn=o(),wt=r("p"),wt.textContent=fs,sn=o(),xt=r("ul"),xt.innerHTML=gs,an=o(),ne=r("div"),u(Oe.$$.fragment),rn=o(),Mt=r("p"),Mt.textContent=_s,dn=o(),se=r("div"),u(Ve.$$.fragment),ln=o(),Ut=r("p"),Ut.textContent=bs,cn=o(),zt=r("div"),u(Xe.$$.fragment),pn=o(),O=r("div"),u(Ae.$$.fragment),mn=o(),$t=r("p"),$t.textContent=vs,un=o(),u(ae.$$.fragment),Uo=o(),u(Se.$$.fragment),zo=o(),C=r("div"),u(Ye.$$.fragment),hn=o(),Ct=r("p"),Ct.textContent=ys,fn=o(),jt=r("p"),jt.innerHTML=ks,gn=o(),It=r("p"),It.innerHTML=Ts,_n=o(),Jt=r("p"),Jt.innerHTML=ws,bn=o(),P=r("div"),u(Qe.$$.fragment),vn=o(),qt=r("p"),qt.innerHTML=xs,yn=o(),Ft=r("p"),Ft.innerHTML=Ms,kn=o(),Pt=r("p"),Pt.textContent=Us,$o=o(),u(Ke.$$.fragment),Co=o(),q=r("div"),u(et.$$.fragment),Tn=o(),Lt=r("p"),Lt.textContent=zs,wn=o(),Nt=r("p"),Nt.innerHTML=$s,xn=o(),Bt=r("p"),Bt.innerHTML=Cs,Mn=o(),L=r("div"),u(tt.$$.fragment),Un=o(),Gt=r("p"),Gt.innerHTML=js,zn=o(),u(re.$$.fragment),$n=o(),u(ie.$$.fragment),jo=o(),u(ot.$$.fragment),Io=o(),j=r("div"),u(nt.$$.fragment),Cn=o(),Zt=r("p"),Zt.textContent=Is,jn=o(),Wt=r("p"),Wt.innerHTML=Js,In=o(),Rt=r("p"),Rt.innerHTML=qs,Jn=o(),Ht=r("p"),Ht.innerHTML=Fs,qn=o(),N=r("div"),u(st.$$.fragment),Fn=o(),Dt=r("p"),Dt.innerHTML=Ps,Pn=o(),u(de.$$.fragment),Ln=o(),u(le.$$.fragment),Jo=o(),u(at.$$.fragment),qo=o(),F=r("div"),u(rt.$$.fragment),Nn=o(),Et=r("p"),Et.textContent=Ls,Bn=o(),Ot=r("p"),Ot.innerHTML=Ns,Gn=o(),Vt=r("p"),Vt.innerHTML=Bs,Zn=o(),B=r("div"),u(it.$$.fragment),Wn=o(),Xt=r("p"),Xt.innerHTML=Gs,Rn=o(),u(ce.$$.fragment),Hn=o(),u(pe.$$.fragment),Fo=o(),u(dt.$$.fragment),Po=o(),Yt=r("p"),this.h()},l(e){const a=Os("svelte-u9bgzb",document.head);d=i(a,"META",{name:!0,content:!0}),a.forEach(s),y=n(e),p=i(e,"P",{}),w(p).forEach(s),v=n(e),x=i(e,"P",{"data-svelte-h":!0}),c(x)!=="svelte-l22f9r"&&(x.innerHTML=m),J=n(e),h(_e.$$.fragment,e),to=n(e),Q=i(e,"DIV",{class:!0,"data-svelte-h":!0}),c(Q)!=="svelte-13t8s2t"&&(Q.innerHTML=En),oo=n(e),h(be.$$.fragment,e),no=n(e),ve=i(e,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-h4a6lv"&&(ve.innerHTML=On),so=n(e),ye=i(e,"P",{"data-svelte-h":!0}),c(ye)!=="svelte-vfdo9a"&&(ye.textContent=Vn),ao=n(e),ke=i(e,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-xdaud3"&&(ke.textContent=Xn),ro=n(e),K=i(e,"IMG",{src:!0,alt:!0,width:!0}),io=n(e),Te=i(e,"SMALL",{"data-svelte-h":!0}),c(Te)!=="svelte-1ar8lp1"&&(Te.innerHTML=Sn),lo=n(e),h(we.$$.fragment,e),co=n(e),xe=i(e,"UL",{"data-svelte-h":!0}),c(xe)!=="svelte-vpnqnh"&&(xe.innerHTML=Yn),po=n(e),h(Me.$$.fragment,e),mo=n(e),Ue=i(e,"P",{"data-svelte-h":!0}),c(Ue)!=="svelte-rq1xjz"&&(Ue.innerHTML=Qn),uo=n(e),h(ze.$$.fragment,e),ho=n(e),$e=i(e,"P",{"data-svelte-h":!0}),c($e)!=="svelte-31calq"&&($e.innerHTML=Kn),fo=n(e),Ce=i(e,"UL",{"data-svelte-h":!0}),c(Ce)!=="svelte-d89yqu"&&(Ce.innerHTML=es),go=n(e),je=i(e,"P",{"data-svelte-h":!0}),c(je)!=="svelte-1cfgh3j"&&(je.innerHTML=ts),_o=n(e),h(Ie.$$.fragment,e),bo=n(e),Je=i(e,"P",{"data-svelte-h":!0}),c(Je)!=="svelte-qaon2f"&&(Je.textContent=os),vo=n(e),qe=i(e,"UL",{"data-svelte-h":!0}),c(qe)!=="svelte-157jvhr"&&(qe.innerHTML=ns),yo=n(e),h(Fe.$$.fragment,e),ko=n(e),W=i(e,"DIV",{class:!0});var A=w(W);h(Pe.$$.fragment,A),Go=n(A),pt=i(A,"P",{"data-svelte-h":!0}),c(pt)!=="svelte-ldn0bv"&&(pt.innerHTML=ss),Zo=n(A),mt=i(A,"P",{"data-svelte-h":!0}),c(mt)!=="svelte-1ek1ss9"&&(mt.innerHTML=as),A.forEach(s),To=n(e),h(Le.$$.fragment,e),wo=n(e),z=i(e,"DIV",{class:!0});var I=w(z);h(Ne.$$.fragment,I),Wo=n(I),ut=i(I,"P",{"data-svelte-h":!0}),c(ut)!=="svelte-1te1ttf"&&(ut.innerHTML=rs),Ro=n(I),ht=i(I,"P",{"data-svelte-h":!0}),c(ht)!=="svelte-ntrhio"&&(ht.innerHTML=is),Ho=n(I),H=i(I,"DIV",{class:!0});var S=w(H);h(Be.$$.fragment,S),Do=n(S),ft=i(S,"P",{"data-svelte-h":!0}),c(ft)!=="svelte-1wjq39d"&&(ft.textContent=ds),Eo=n(S),gt=i(S,"UL",{"data-svelte-h":!0}),c(gt)!=="svelte-8gh3n2"&&(gt.innerHTML=ls),S.forEach(s),Oo=n(I),ee=i(I,"DIV",{class:!0});var lt=w(ee);h(Ge.$$.fragment,lt),Vo=n(lt),_t=i(lt,"P",{"data-svelte-h":!0}),c(_t)!=="svelte-1f4f5kp"&&(_t.innerHTML=cs),lt.forEach(s),Xo=n(I),te=i(I,"DIV",{class:!0});var ct=w(te);h(Ze.$$.fragment,ct),Ao=n(ct),bt=i(ct,"P",{"data-svelte-h":!0}),c(bt)!=="svelte-fl5ab0"&&(bt.textContent=ps),ct.forEach(s),So=n(I),vt=i(I,"DIV",{class:!0});var Qt=w(vt);h(We.$$.fragment,Qt),Qt.forEach(s),I.forEach(s),xo=n(e),h(Re.$$.fragment,e),Mo=n(e),T=i(e,"DIV",{class:!0});var M=w(T);h(He.$$.fragment,M),Yo=n(M),yt=i(M,"P",{"data-svelte-h":!0}),c(yt)!=="svelte-1o19wgr"&&(yt.innerHTML=ms),Qo=n(M),kt=i(M,"P",{"data-svelte-h":!0}),c(kt)!=="svelte-gxzj9w"&&(kt.innerHTML=us),Ko=n(M),D=i(M,"DIV",{class:!0});var Y=w(D);h(De.$$.fragment,Y),en=n(Y),Tt=i(Y,"P",{"data-svelte-h":!0}),c(Tt)!=="svelte-6p21pf"&&(Tt.textContent=hs),tn=n(Y),h(oe.$$.fragment,Y),Y.forEach(s),on=n(M),E=i(M,"DIV",{class:!0});var At=w(E);h(Ee.$$.fragment,At),nn=n(At),wt=i(At,"P",{"data-svelte-h":!0}),c(wt)!=="svelte-1ooxl9e"&&(wt.textContent=fs),sn=n(At),xt=i(At,"UL",{"data-svelte-h":!0}),c(xt)!=="svelte-rq8uot"&&(xt.innerHTML=gs),At.forEach(s),an=n(M),ne=i(M,"DIV",{class:!0});var No=w(ne);h(Oe.$$.fragment,No),rn=n(No),Mt=i(No,"P",{"data-svelte-h":!0}),c(Mt)!=="svelte-1w6bb17"&&(Mt.textContent=_s),No.forEach(s),dn=n(M),se=i(M,"DIV",{class:!0});var Bo=w(se);h(Ve.$$.fragment,Bo),ln=n(Bo),Ut=i(Bo,"P",{"data-svelte-h":!0}),c(Ut)!=="svelte-bub0ru"&&(Ut.textContent=bs),Bo.forEach(s),cn=n(M),zt=i(M,"DIV",{class:!0});var Zs=w(zt);h(Xe.$$.fragment,Zs),Zs.forEach(s),pn=n(M),O=i(M,"DIV",{class:!0});var St=w(O);h(Ae.$$.fragment,St),mn=n(St),$t=i(St,"P",{"data-svelte-h":!0}),c($t)!=="svelte-ma945j"&&($t.textContent=vs),un=n(St),h(ae.$$.fragment,St),St.forEach(s),M.forEach(s),Uo=n(e),h(Se.$$.fragment,e),zo=n(e),C=i(e,"DIV",{class:!0});var G=w(C);h(Ye.$$.fragment,G),hn=n(G),Ct=i(G,"P",{"data-svelte-h":!0}),c(Ct)!=="svelte-1ti6n5s"&&(Ct.textContent=ys),fn=n(G),jt=i(G,"P",{"data-svelte-h":!0}),c(jt)!=="svelte-1v0we3"&&(jt.innerHTML=ks),gn=n(G),It=i(G,"P",{"data-svelte-h":!0}),c(It)!=="svelte-jarsow"&&(It.innerHTML=Ts),_n=n(G),Jt=i(G,"P",{"data-svelte-h":!0}),c(Jt)!=="svelte-mfoszy"&&(Jt.innerHTML=ws),bn=n(G),P=i(G,"DIV",{class:!0});var me=w(P);h(Qe.$$.fragment,me),vn=n(me),qt=i(me,"P",{"data-svelte-h":!0}),c(qt)!=="svelte-23yewj"&&(qt.innerHTML=xs),yn=n(me),Ft=i(me,"P",{"data-svelte-h":!0}),c(Ft)!=="svelte-1c5ykyr"&&(Ft.innerHTML=Ms),kn=n(me),Pt=i(me,"P",{"data-svelte-h":!0}),c(Pt)!=="svelte-ws0hzs"&&(Pt.textContent=Us),me.forEach(s),G.forEach(s),$o=n(e),h(Ke.$$.fragment,e),Co=n(e),q=i(e,"DIV",{class:!0});var V=w(q);h(et.$$.fragment,V),Tn=n(V),Lt=i(V,"P",{"data-svelte-h":!0}),c(Lt)!=="svelte-dwop3u"&&(Lt.textContent=zs),wn=n(V),Nt=i(V,"P",{"data-svelte-h":!0}),c(Nt)!=="svelte-q52n56"&&(Nt.innerHTML=$s),xn=n(V),Bt=i(V,"P",{"data-svelte-h":!0}),c(Bt)!=="svelte-hswkmf"&&(Bt.innerHTML=Cs),Mn=n(V),L=i(V,"DIV",{class:!0});var ue=w(L);h(tt.$$.fragment,ue),Un=n(ue),Gt=i(ue,"P",{"data-svelte-h":!0}),c(Gt)!=="svelte-tze8tx"&&(Gt.innerHTML=js),zn=n(ue),h(re.$$.fragment,ue),$n=n(ue),h(ie.$$.fragment,ue),ue.forEach(s),V.forEach(s),jo=n(e),h(ot.$$.fragment,e),Io=n(e),j=i(e,"DIV",{class:!0});var Z=w(j);h(nt.$$.fragment,Z),Cn=n(Z),Zt=i(Z,"P",{"data-svelte-h":!0}),c(Zt)!=="svelte-170n5ee"&&(Zt.textContent=Is),jn=n(Z),Wt=i(Z,"P",{"data-svelte-h":!0}),c(Wt)!=="svelte-15jr0o"&&(Wt.innerHTML=Js),In=n(Z),Rt=i(Z,"P",{"data-svelte-h":!0}),c(Rt)!=="svelte-q52n56"&&(Rt.innerHTML=qs),Jn=n(Z),Ht=i(Z,"P",{"data-svelte-h":!0}),c(Ht)!=="svelte-hswkmf"&&(Ht.innerHTML=Fs),qn=n(Z),N=i(Z,"DIV",{class:!0});var he=w(N);h(st.$$.fragment,he),Fn=n(he),Dt=i(he,"P",{"data-svelte-h":!0}),c(Dt)!=="svelte-knv3c3"&&(Dt.innerHTML=Ps),Pn=n(he),h(de.$$.fragment,he),Ln=n(he),h(le.$$.fragment,he),he.forEach(s),Z.forEach(s),Jo=n(e),h(at.$$.fragment,e),qo=n(e),F=i(e,"DIV",{class:!0});var X=w(F);h(rt.$$.fragment,X),Nn=n(X),Et=i(X,"P",{"data-svelte-h":!0}),c(Et)!=="svelte-dwop3u"&&(Et.textContent=Ls),Bn=n(X),Ot=i(X,"P",{"data-svelte-h":!0}),c(Ot)!=="svelte-q52n56"&&(Ot.innerHTML=Ns),Gn=n(X),Vt=i(X,"P",{"data-svelte-h":!0}),c(Vt)!=="svelte-hswkmf"&&(Vt.innerHTML=Bs),Zn=n(X),B=i(X,"DIV",{class:!0});var fe=w(B);h(it.$$.fragment,fe),Wn=n(fe),Xt=i(fe,"P",{"data-svelte-h":!0}),c(Xt)!=="svelte-sg6pon"&&(Xt.innerHTML=Gs),Rn=n(fe),h(ce.$$.fragment,fe),Hn=n(fe),h(pe.$$.fragment,fe),fe.forEach(s),X.forEach(s),Fo=n(e),h(dt.$$.fragment,e),Po=n(e),Yt=i(e,"P",{}),w(Yt).forEach(s),this.h()},h(){k(d,"name","hf:doc:metadata"),k(d,"content",na),k(Q,"class","flex flex-wrap space-x-1"),Rs(K.src,An="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/udop_architecture.jpg")||k(K,"src",An),k(K,"alt","drawing"),k(K,"width","600"),k(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){t(document.head,d),l(e,y,a),l(e,p,a),l(e,v,a),l(e,x,a),l(e,J,a),f(_e,e,a),l(e,to,a),l(e,Q,a),l(e,oo,a),f(be,e,a),l(e,no,a),l(e,ve,a),l(e,so,a),l(e,ye,a),l(e,ao,a),l(e,ke,a),l(e,ro,a),l(e,K,a),l(e,io,a),l(e,Te,a),l(e,lo,a),f(we,e,a),l(e,co,a),l(e,xe,a),l(e,po,a),f(Me,e,a),l(e,mo,a),l(e,Ue,a),l(e,uo,a),f(ze,e,a),l(e,ho,a),l(e,$e,a),l(e,fo,a),l(e,Ce,a),l(e,go,a),l(e,je,a),l(e,_o,a),f(Ie,e,a),l(e,bo,a),l(e,Je,a),l(e,vo,a),l(e,qe,a),l(e,yo,a),f(Fe,e,a),l(e,ko,a),l(e,W,a),f(Pe,W,null),t(W,Go),t(W,pt),t(W,Zo),t(W,mt),l(e,To,a),f(Le,e,a),l(e,wo,a),l(e,z,a),f(Ne,z,null),t(z,Wo),t(z,ut),t(z,Ro),t(z,ht),t(z,Ho),t(z,H),f(Be,H,null),t(H,Do),t(H,ft),t(H,Eo),t(H,gt),t(z,Oo),t(z,ee),f(Ge,ee,null),t(ee,Vo),t(ee,_t),t(z,Xo),t(z,te),f(Ze,te,null),t(te,Ao),t(te,bt),t(z,So),t(z,vt),f(We,vt,null),l(e,xo,a),f(Re,e,a),l(e,Mo,a),l(e,T,a),f(He,T,null),t(T,Yo),t(T,yt),t(T,Qo),t(T,kt),t(T,Ko),t(T,D),f(De,D,null),t(D,en),t(D,Tt),t(D,tn),f(oe,D,null),t(T,on),t(T,E),f(Ee,E,null),t(E,nn),t(E,wt),t(E,sn),t(E,xt),t(T,an),t(T,ne),f(Oe,ne,null),t(ne,rn),t(ne,Mt),t(T,dn),t(T,se),f(Ve,se,null),t(se,ln),t(se,Ut),t(T,cn),t(T,zt),f(Xe,zt,null),t(T,pn),t(T,O),f(Ae,O,null),t(O,mn),t(O,$t),t(O,un),f(ae,O,null),l(e,Uo,a),f(Se,e,a),l(e,zo,a),l(e,C,a),f(Ye,C,null),t(C,hn),t(C,Ct),t(C,fn),t(C,jt),t(C,gn),t(C,It),t(C,_n),t(C,Jt),t(C,bn),t(C,P),f(Qe,P,null),t(P,vn),t(P,qt),t(P,yn),t(P,Ft),t(P,kn),t(P,Pt),l(e,$o,a),f(Ke,e,a),l(e,Co,a),l(e,q,a),f(et,q,null),t(q,Tn),t(q,Lt),t(q,wn),t(q,Nt),t(q,xn),t(q,Bt),t(q,Mn),t(q,L),f(tt,L,null),t(L,Un),t(L,Gt),t(L,zn),f(re,L,null),t(L,$n),f(ie,L,null),l(e,jo,a),f(ot,e,a),l(e,Io,a),l(e,j,a),f(nt,j,null),t(j,Cn),t(j,Zt),t(j,jn),t(j,Wt),t(j,In),t(j,Rt),t(j,Jn),t(j,Ht),t(j,qn),t(j,N),f(st,N,null),t(N,Fn),t(N,Dt),t(N,Pn),f(de,N,null),t(N,Ln),f(le,N,null),l(e,Jo,a),f(at,e,a),l(e,qo,a),l(e,F,a),f(rt,F,null),t(F,Nn),t(F,Et),t(F,Bn),t(F,Ot),t(F,Gn),t(F,Vt),t(F,Zn),t(F,B),f(it,B,null),t(B,Wn),t(B,Xt),t(B,Rn),f(ce,B,null),t(B,Hn),f(pe,B,null),l(e,Fo,a),f(dt,e,a),l(e,Po,a),l(e,Yt,a),Lo=!0},p(e,[a]){const A={};a&2&&(A.$$scope={dirty:a,ctx:e}),oe.$set(A);const I={};a&2&&(I.$$scope={dirty:a,ctx:e}),ae.$set(I);const S={};a&2&&(S.$$scope={dirty:a,ctx:e}),re.$set(S);const lt={};a&2&&(lt.$$scope={dirty:a,ctx:e}),ie.$set(lt);const ct={};a&2&&(ct.$$scope={dirty:a,ctx:e}),de.$set(ct);const Qt={};a&2&&(Qt.$$scope={dirty:a,ctx:e}),le.$set(Qt);const M={};a&2&&(M.$$scope={dirty:a,ctx:e}),ce.$set(M);const Y={};a&2&&(Y.$$scope={dirty:a,ctx:e}),pe.$set(Y)},i(e){Lo||(g(_e.$$.fragment,e),g(be.$$.fragment,e),g(we.$$.fragment,e),g(Me.$$.fragment,e),g(ze.$$.fragment,e),g(Ie.$$.fragment,e),g(Fe.$$.fragment,e),g(Pe.$$.fragment,e),g(Le.$$.fragment,e),g(Ne.$$.fragment,e),g(Be.$$.fragment,e),g(Ge.$$.fragment,e),g(Ze.$$.fragment,e),g(We.$$.fragment,e),g(Re.$$.fragment,e),g(He.$$.fragment,e),g(De.$$.fragment,e),g(oe.$$.fragment,e),g(Ee.$$.fragment,e),g(Oe.$$.fragment,e),g(Ve.$$.fragment,e),g(Xe.$$.fragment,e),g(Ae.$$.fragment,e),g(ae.$$.fragment,e),g(Se.$$.fragment,e),g(Ye.$$.fragment,e),g(Qe.$$.fragment,e),g(Ke.$$.fragment,e),g(et.$$.fragment,e),g(tt.$$.fragment,e),g(re.$$.fragment,e),g(ie.$$.fragment,e),g(ot.$$.fragment,e),g(nt.$$.fragment,e),g(st.$$.fragment,e),g(de.$$.fragment,e),g(le.$$.fragment,e),g(at.$$.fragment,e),g(rt.$$.fragment,e),g(it.$$.fragment,e),g(ce.$$.fragment,e),g(pe.$$.fragment,e),g(dt.$$.fragment,e),Lo=!0)},o(e){_(_e.$$.fragment,e),_(be.$$.fragment,e),_(we.$$.fragment,e),_(Me.$$.fragment,e),_(ze.$$.fragment,e),_(Ie.$$.fragment,e),_(Fe.$$.fragment,e),_(Pe.$$.fragment,e),_(Le.$$.fragment,e),_(Ne.$$.fragment,e),_(Be.$$.fragment,e),_(Ge.$$.fragment,e),_(Ze.$$.fragment,e),_(We.$$.fragment,e),_(Re.$$.fragment,e),_(He.$$.fragment,e),_(De.$$.fragment,e),_(oe.$$.fragment,e),_(Ee.$$.fragment,e),_(Oe.$$.fragment,e),_(Ve.$$.fragment,e),_(Xe.$$.fragment,e),_(Ae.$$.fragment,e),_(ae.$$.fragment,e),_(Se.$$.fragment,e),_(Ye.$$.fragment,e),_(Qe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(ot.$$.fragment,e),_(nt.$$.fragment,e),_(st.$$.fragment,e),_(de.$$.fragment,e),_(le.$$.fragment,e),_(at.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(ce.$$.fragment,e),_(pe.$$.fragment,e),_(dt.$$.fragment,e),Lo=!1},d(e){e&&(s(y),s(p),s(v),s(x),s(J),s(to),s(Q),s(oo),s(no),s(ve),s(so),s(ye),s(ao),s(ke),s(ro),s(K),s(io),s(Te),s(lo),s(co),s(xe),s(po),s(mo),s(Ue),s(uo),s(ho),s($e),s(fo),s(Ce),s(go),s(je),s(_o),s(bo),s(Je),s(vo),s(qe),s(yo),s(ko),s(W),s(To),s(wo),s(z),s(xo),s(Mo),s(T),s(Uo),s(zo),s(C),s($o),s(Co),s(q),s(jo),s(Io),s(j),s(Jo),s(qo),s(F),s(Fo),s(Po),s(Yt)),s(d),b(_e,e),b(be,e),b(we,e),b(Me,e),b(ze,e),b(Ie,e),b(Fe,e),b(Pe),b(Le,e),b(Ne),b(Be),b(Ge),b(Ze),b(We),b(Re,e),b(He),b(De),b(oe),b(Ee),b(Oe),b(Ve),b(Xe),b(Ae),b(ae),b(Se,e),b(Ye),b(Qe),b(Ke,e),b(et),b(tt),b(re),b(ie),b(ot,e),b(nt),b(st),b(de),b(le),b(at,e),b(rt),b(it),b(ce),b(pe),b(dt,e)}}}const na='{"title":"UDOP","local":"udop","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"UdopConfig","local":"transformers.UdopConfig","sections":[],"depth":2},{"title":"UdopTokenizer","local":"transformers.UdopTokenizer","sections":[],"depth":2},{"title":"UdopTokenizerFast","local":"transformers.UdopTokenizerFast","sections":[],"depth":2},{"title":"UdopProcessor","local":"transformers.UdopProcessor","sections":[],"depth":2},{"title":"UdopModel","local":"transformers.UdopModel","sections":[],"depth":2},{"title":"UdopForConditionalGeneration","local":"transformers.UdopForConditionalGeneration","sections":[],"depth":2},{"title":"UdopEncoderModel","local":"transformers.UdopEncoderModel","sections":[],"depth":2}],"depth":1}';function sa($){return Hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ma extends Ds{constructor(d){super(),Es(this,d,sa,oa,Ws,{})}}export{ma as component};
