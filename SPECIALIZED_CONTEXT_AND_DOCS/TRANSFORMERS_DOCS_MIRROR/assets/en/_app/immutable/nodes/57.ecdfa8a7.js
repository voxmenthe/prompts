import{s as Mr,o as Fr,n as Tr}from"../chunks/scheduler.18a86fab.js";import{S as wr,i as Ir,g as o,s as r,r as d,A as Er,h as n,f as s,c as a,j as v,u as l,x,k as y,y as t,a as _,v as m,d as p,t as u,w as f}from"../chunks/index.98837b22.js";import{T as zr}from"../chunks/Tip.77304350.js";import{D as $}from"../chunks/Docstring.a1ef7999.js";import{C as Jr}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as jr}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as De,E as Lr}from"../chunks/getInferenceSnippets.06c2775f.js";function kr($e){let g,L="Examples:",T,F,I;return F=new Jr({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqRmVhdHVyZUV4dHJhY3Rpb25NaXhpbiolMjBub3IlMjAqU2VxdWVuY2VGZWF0dXJlRXh0cmFjdG9yKiUyMHNvJTIwbGV0J3MlMjBzaG93JTIwdGhlJTIwZXhhbXBsZXMlMjBvbiUyMGElMEElMjMlMjBkZXJpdmVkJTIwY2xhc3MlM0ElMjAqV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yKiUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UtOTYwaCUyMiUwQSklMjAlMjAlMjMlMjBEb3dubG9hZCUyMGZlYXR1cmVfZXh0cmFjdGlvbl9jb25maWclMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMFdhdjJWZWMyRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwZmVhdHVyZV9leHRyYWN0b3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGcHJlcHJvY2Vzc29yX2NvbmZpZy5qc29uJTIyKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UtOTYwaCUyMiUyQyUyMHJldHVybl9hdHRlbnRpb25fbWFzayUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwZmVhdHVyZV9leHRyYWN0b3IucmV0dXJuX2F0dGVudGlvbl9tYXNrJTIwaXMlMjBGYWxzZSUwQWZlYXR1cmVfZXh0cmFjdG9yJTJDJTIwdW51c2VkX2t3YXJncyUyMCUzRCUyMFdhdjJWZWMyRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIlMkMlMjByZXR1cm5fYXR0ZW50aW9uX21hc2slM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGZlYXR1cmVfZXh0cmFjdG9yLnJldHVybl9hdHRlbnRpb25fbWFzayUyMGlzJTIwRmFsc2UlMEFhc3NlcnQlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTNEJTIwJTdCJTIyZm9vJTIyJTNBJTIwRmFsc2UlN0Q=",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){g=o("p"),g.textContent=L,T=r(),d(F.$$.fragment)},l(b){g=n(b,"P",{"data-svelte-h":!0}),x(g)!=="svelte-kvfsh7"&&(g.textContent=L),T=a(b),l(F.$$.fragment,b)},m(b,z){_(b,g,z),_(b,T,z),m(F,b,z),I=!0},p:Tr,i(b){I||(p(F.$$.fragment,b),I=!0)},o(b){u(F.$$.fragment,b),I=!1},d(b){b&&(s(g),s(T)),f(F,b)}}}function Pr($e){let g,L=`If the <code>processed_features</code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code>return_tensors</code>. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`;return{c(){g=o("p"),g.innerHTML=L},l(T){g=n(T,"P",{"data-svelte-h":!0}),x(g)!=="svelte-yvqf1m"&&(g.innerHTML=L)},m(T,F){_(T,g,F)},p:Tr,d(T){T&&s(g)}}}function Zr($e){let g,L,T,F,I,b,z,tr="A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy and PyTorch tensors.",Ye,O,Ge,w,K,xt,Te,rr=`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,_t,k,ee,vt,Me,ar=`Instantiate a type of <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin">FeatureExtractionMixin</a> from a feature extractor, <em>e.g.</em> a
derived class of <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a>.`,yt,V,bt,C,te,$t,Fe,or=`Save a feature_extractor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> class method.`,Se,re,Ae,J,ae,Tt,we,nr="This is a general feature extraction class for speech recognition.",Mt,E,oe,Ft,Ie,sr=`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`,wt,Ee,ir=`Padding side (left/right) padding values are defined at the feature extractor level (with <code>self.padding_side</code>,
<code>self.padding_value</code>)`,It,H,Qe,ne,Oe,M,se,Et,ze,cr='Holds the output of the <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> and feature extractor specific <code>__call__</code> methods.',zt,Je,dr="This class is derived from a python dictionary and can be used as a dictionary.",Jt,q,ie,jt,je,lr="Convert the inner content to tensors.",Lt,N,ce,kt,Le,mr=`Send all values to device by calling <code>v.to(*args, **kwargs)</code> (PyTorch only). This should support casting in
different <code>dtypes</code> and sending the <code>BatchFeature</code> to a different <code>device</code>.`,Ke,de,et,c,le,Pt,ke,pr="Mixin that contain utilities for preparing image features.",Zt,R,me,Vt,Pe,ur=`Crops <code>image</code> to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`,Ct,W,pe,Ht,Ze,fr="Converts <code>PIL.Image.Image</code> to RGB format.",qt,B,ue,Nt,Ve,gr="Expands 2-dimensional <code>image</code> to 3 dimensions.",Rt,U,fe,Wt,Ce,hr=`Flips the channel order of <code>image</code> from RGB to BGR, or vice versa. Note that this will trigger a conversion of
<code>image</code> to a NumPy array if it’s a PIL Image.`,Bt,X,ge,Ut,He,xr=`Normalizes <code>image</code> with <code>mean</code> and <code>std</code>. Note that this will trigger a conversion of <code>image</code> to a NumPy array
if it’s a PIL Image.`,Xt,D,he,Dt,qe,_r="Rescale a numpy image by scale amount",Yt,Y,xe,Gt,Ne,vr="Resizes <code>image</code>. Enforces conversion of input to PIL.Image.",St,G,_e,At,Re,yr=`Returns a rotated copy of <code>image</code>. This method returns a copy of <code>image</code>, rotated the given number of degrees
counter clockwise around its centre.`,Qt,S,ve,Ot,We,br=`Converts <code>image</code> to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`,Kt,A,ye,er,Be,$r=`Converts <code>image</code> to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`,tt,be,rt,Xe,at;return I=new De({props:{title:"Feature Extractor",local:"feature-extractor",headingTag:"h1"}}),O=new De({props:{title:"FeatureExtractionMixin",local:"transformers.FeatureExtractionMixin",headingTag:"h2"}}),K=new $({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L251"}}),ee=new $({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> &#x2014;
Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L275",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A feature extractor of type <a
  href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),V=new jr({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[kr]},$$scope:{ctx:$e}}}),te=new $({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L387"}}),re=new De({props:{title:"SequenceFeatureExtractor",local:"transformers.SequenceFeatureExtractor",headingTag:"h2"}}),ae=new $({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_sequence_utils.py#L29"}}),oe=new $({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, list[transformers.feature_extraction_utils.BatchFeature], dict[str, transformers.feature_extraction_utils.BatchFeature], dict[str, list[transformers.feature_extraction_utils.BatchFeature]], list[dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, <code>dict[str, list[float]]</code>, <code>dict[str, list[list[float]]</code> or <code>list[dict[str, list[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a> or <code>dict[str, list[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, <em>dict[str, list[list[float]]]</em> or <em>list[dict[str,
list[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>list[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_sequence_utils.py#L52"}}),H=new zr({props:{$$slots:{default:[Pr]},$$scope:{ctx:$e}}}),ne=new De({props:{title:"BatchFeature",local:"transformers.BatchFeature",headingTag:"h2"}}),se=new $({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L63"}}),ie=new $({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L172"}}),ce=new $({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BatchFeature.to.args",description:`<strong>args</strong> (<code>Tuple</code>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"args"},{anchor:"transformers.BatchFeature.to.kwargs",description:`<strong>kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.
To enable asynchronous data transfer, set the <code>non_blocking</code> flag in <code>kwargs</code> (defaults to <code>False</code>).`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/feature_extraction_utils.py#L203",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The same instance after modification.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),de=new De({props:{title:"ImageFeatureExtractionMixin",local:"transformers.ImageFeatureExtractionMixin",headingTag:"h2"}}),le=new $({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L573"}}),me=new $({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape (n_channels, height, width) or (height, width, n_channels)) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L807",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A center cropped <code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape: (n_channels,
height, width).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>new_image</p>
`}}),pe=new $({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L615"}}),ue=new $({props:{name:"expand_dims",anchor:"transformers.ImageFeatureExtractionMixin.expand_dims",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.expand_dims.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to expand.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L668"}}),fe=new $({props:{name:"flip_channel_order",anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image whose color channels to flip. If <code>np.ndarray</code> or <code>torch.Tensor</code>, the channel dimension should
be first.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L882"}}),ge=new $({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""},{name:"rescale",val:" = False"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>list[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>list[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale the image to be between 0 and 1. If a PIL image is provided, scaling will
happen automatically.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L688"}}),he=new $({props:{name:"rescale",anchor:"transformers.ImageFeatureExtractionMixin.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[float, int]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L629"}}),xe=new $({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = None"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L740",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A resized <code>PIL.Image.Image</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image</p>
`}}),_e=new $({props:{name:"rotate",anchor:"transformers.ImageFeatureExtractionMixin.rotate",parameters:[{name:"image",val:""},{name:"angle",val:""},{name:"resample",val:" = None"},{name:"expand",val:" = 0"},{name:"center",val:" = None"},{name:"translate",val:" = None"},{name:"fillcolor",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.rotate.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to rotate. If <code>np.ndarray</code> or <code>torch.Tensor</code>, will be converted to <code>PIL.Image.Image</code> before
rotating.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L899",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A rotated <code>PIL.Image.Image</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image</p>
`}}),ve=new $({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L636"}}),ye=new $({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_utils.py#L585"}}),be=new Lr({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/feature_extractor.md"}}),{c(){g=o("meta"),L=r(),T=o("p"),F=r(),d(I.$$.fragment),b=r(),z=o("p"),z.textContent=tr,Ye=r(),d(O.$$.fragment),Ge=r(),w=o("div"),d(K.$$.fragment),xt=r(),Te=o("p"),Te.textContent=rr,_t=r(),k=o("div"),d(ee.$$.fragment),vt=r(),Me=o("p"),Me.innerHTML=ar,yt=r(),d(V.$$.fragment),bt=r(),C=o("div"),d(te.$$.fragment),$t=r(),Fe=o("p"),Fe.innerHTML=or,Se=r(),d(re.$$.fragment),Ae=r(),J=o("div"),d(ae.$$.fragment),Tt=r(),we=o("p"),we.textContent=nr,Mt=r(),E=o("div"),d(oe.$$.fragment),Ft=r(),Ie=o("p"),Ie.textContent=sr,wt=r(),Ee=o("p"),Ee.innerHTML=ir,It=r(),d(H.$$.fragment),Qe=r(),d(ne.$$.fragment),Oe=r(),M=o("div"),d(se.$$.fragment),Et=r(),ze=o("p"),ze.innerHTML=cr,zt=r(),Je=o("p"),Je.textContent=dr,Jt=r(),q=o("div"),d(ie.$$.fragment),jt=r(),je=o("p"),je.textContent=lr,Lt=r(),N=o("div"),d(ce.$$.fragment),kt=r(),Le=o("p"),Le.innerHTML=mr,Ke=r(),d(de.$$.fragment),et=r(),c=o("div"),d(le.$$.fragment),Pt=r(),ke=o("p"),ke.textContent=pr,Zt=r(),R=o("div"),d(me.$$.fragment),Vt=r(),Pe=o("p"),Pe.innerHTML=ur,Ct=r(),W=o("div"),d(pe.$$.fragment),Ht=r(),Ze=o("p"),Ze.innerHTML=fr,qt=r(),B=o("div"),d(ue.$$.fragment),Nt=r(),Ve=o("p"),Ve.innerHTML=gr,Rt=r(),U=o("div"),d(fe.$$.fragment),Wt=r(),Ce=o("p"),Ce.innerHTML=hr,Bt=r(),X=o("div"),d(ge.$$.fragment),Ut=r(),He=o("p"),He.innerHTML=xr,Xt=r(),D=o("div"),d(he.$$.fragment),Dt=r(),qe=o("p"),qe.textContent=_r,Yt=r(),Y=o("div"),d(xe.$$.fragment),Gt=r(),Ne=o("p"),Ne.innerHTML=vr,St=r(),G=o("div"),d(_e.$$.fragment),At=r(),Re=o("p"),Re.innerHTML=yr,Qt=r(),S=o("div"),d(ve.$$.fragment),Ot=r(),We=o("p"),We.innerHTML=br,Kt=r(),A=o("div"),d(ye.$$.fragment),er=r(),Be=o("p"),Be.innerHTML=$r,tt=r(),d(be.$$.fragment),rt=r(),Xe=o("p"),this.h()},l(e){const i=Er("svelte-u9bgzb",document.head);g=n(i,"META",{name:!0,content:!0}),i.forEach(s),L=a(e),T=n(e,"P",{}),v(T).forEach(s),F=a(e),l(I.$$.fragment,e),b=a(e),z=n(e,"P",{"data-svelte-h":!0}),x(z)!=="svelte-7rp9ar"&&(z.textContent=tr),Ye=a(e),l(O.$$.fragment,e),Ge=a(e),w=n(e,"DIV",{class:!0});var j=v(w);l(K.$$.fragment,j),xt=a(j),Te=n(j,"P",{"data-svelte-h":!0}),x(Te)!=="svelte-1u6z21f"&&(Te.textContent=rr),_t=a(j),k=n(j,"DIV",{class:!0});var Z=v(k);l(ee.$$.fragment,Z),vt=a(Z),Me=n(Z,"P",{"data-svelte-h":!0}),x(Me)!=="svelte-v4hwhh"&&(Me.innerHTML=ar),yt=a(Z),l(V.$$.fragment,Z),Z.forEach(s),bt=a(j),C=n(j,"DIV",{class:!0});var ot=v(C);l(te.$$.fragment,ot),$t=a(ot),Fe=n(ot,"P",{"data-svelte-h":!0}),x(Fe)!=="svelte-1sh16co"&&(Fe.innerHTML=or),ot.forEach(s),j.forEach(s),Se=a(e),l(re.$$.fragment,e),Ae=a(e),J=n(e,"DIV",{class:!0});var Ue=v(J);l(ae.$$.fragment,Ue),Tt=a(Ue),we=n(Ue,"P",{"data-svelte-h":!0}),x(we)!=="svelte-fkistw"&&(we.textContent=nr),Mt=a(Ue),E=n(Ue,"DIV",{class:!0});var Q=v(E);l(oe.$$.fragment,Q),Ft=a(Q),Ie=n(Q,"P",{"data-svelte-h":!0}),x(Ie)!=="svelte-1ocd2wm"&&(Ie.textContent=sr),wt=a(Q),Ee=n(Q,"P",{"data-svelte-h":!0}),x(Ee)!=="svelte-oeftzc"&&(Ee.innerHTML=ir),It=a(Q),l(H.$$.fragment,Q),Q.forEach(s),Ue.forEach(s),Qe=a(e),l(ne.$$.fragment,e),Oe=a(e),M=n(e,"DIV",{class:!0});var P=v(M);l(se.$$.fragment,P),Et=a(P),ze=n(P,"P",{"data-svelte-h":!0}),x(ze)!=="svelte-lzs6g2"&&(ze.innerHTML=cr),zt=a(P),Je=n(P,"P",{"data-svelte-h":!0}),x(Je)!=="svelte-saqdtk"&&(Je.textContent=dr),Jt=a(P),q=n(P,"DIV",{class:!0});var nt=v(q);l(ie.$$.fragment,nt),jt=a(nt),je=n(nt,"P",{"data-svelte-h":!0}),x(je)!=="svelte-pxfh9u"&&(je.textContent=lr),nt.forEach(s),Lt=a(P),N=n(P,"DIV",{class:!0});var st=v(N);l(ce.$$.fragment,st),kt=a(st),Le=n(st,"P",{"data-svelte-h":!0}),x(Le)!=="svelte-d0cfhs"&&(Le.innerHTML=mr),st.forEach(s),P.forEach(s),Ke=a(e),l(de.$$.fragment,e),et=a(e),c=n(e,"DIV",{class:!0});var h=v(c);l(le.$$.fragment,h),Pt=a(h),ke=n(h,"P",{"data-svelte-h":!0}),x(ke)!=="svelte-393l39"&&(ke.textContent=pr),Zt=a(h),R=n(h,"DIV",{class:!0});var it=v(R);l(me.$$.fragment,it),Vt=a(it),Pe=n(it,"P",{"data-svelte-h":!0}),x(Pe)!=="svelte-klynef"&&(Pe.innerHTML=ur),it.forEach(s),Ct=a(h),W=n(h,"DIV",{class:!0});var ct=v(W);l(pe.$$.fragment,ct),Ht=a(ct),Ze=n(ct,"P",{"data-svelte-h":!0}),x(Ze)!=="svelte-133v6j"&&(Ze.innerHTML=fr),ct.forEach(s),qt=a(h),B=n(h,"DIV",{class:!0});var dt=v(B);l(ue.$$.fragment,dt),Nt=a(dt),Ve=n(dt,"P",{"data-svelte-h":!0}),x(Ve)!=="svelte-1041ew6"&&(Ve.innerHTML=gr),dt.forEach(s),Rt=a(h),U=n(h,"DIV",{class:!0});var lt=v(U);l(fe.$$.fragment,lt),Wt=a(lt),Ce=n(lt,"P",{"data-svelte-h":!0}),x(Ce)!=="svelte-w2v4kz"&&(Ce.innerHTML=hr),lt.forEach(s),Bt=a(h),X=n(h,"DIV",{class:!0});var mt=v(X);l(ge.$$.fragment,mt),Ut=a(mt),He=n(mt,"P",{"data-svelte-h":!0}),x(He)!=="svelte-1omk197"&&(He.innerHTML=xr),mt.forEach(s),Xt=a(h),D=n(h,"DIV",{class:!0});var pt=v(D);l(he.$$.fragment,pt),Dt=a(pt),qe=n(pt,"P",{"data-svelte-h":!0}),x(qe)!=="svelte-92qzed"&&(qe.textContent=_r),pt.forEach(s),Yt=a(h),Y=n(h,"DIV",{class:!0});var ut=v(Y);l(xe.$$.fragment,ut),Gt=a(ut),Ne=n(ut,"P",{"data-svelte-h":!0}),x(Ne)!=="svelte-dheqn4"&&(Ne.innerHTML=vr),ut.forEach(s),St=a(h),G=n(h,"DIV",{class:!0});var ft=v(G);l(_e.$$.fragment,ft),At=a(ft),Re=n(ft,"P",{"data-svelte-h":!0}),x(Re)!=="svelte-sbsh56"&&(Re.innerHTML=yr),ft.forEach(s),Qt=a(h),S=n(h,"DIV",{class:!0});var gt=v(S);l(ve.$$.fragment,gt),Ot=a(gt),We=n(gt,"P",{"data-svelte-h":!0}),x(We)!=="svelte-pgs2u0"&&(We.innerHTML=br),gt.forEach(s),Kt=a(h),A=n(h,"DIV",{class:!0});var ht=v(A);l(ye.$$.fragment,ht),er=a(ht),Be=n(ht,"P",{"data-svelte-h":!0}),x(Be)!=="svelte-e557ju"&&(Be.innerHTML=$r),ht.forEach(s),h.forEach(s),tt=a(e),l(be.$$.fragment,e),rt=a(e),Xe=n(e,"P",{}),v(Xe).forEach(s),this.h()},h(){y(g,"name","hf:doc:metadata"),y(g,"content",Vr),y(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,i){t(document.head,g),_(e,L,i),_(e,T,i),_(e,F,i),m(I,e,i),_(e,b,i),_(e,z,i),_(e,Ye,i),m(O,e,i),_(e,Ge,i),_(e,w,i),m(K,w,null),t(w,xt),t(w,Te),t(w,_t),t(w,k),m(ee,k,null),t(k,vt),t(k,Me),t(k,yt),m(V,k,null),t(w,bt),t(w,C),m(te,C,null),t(C,$t),t(C,Fe),_(e,Se,i),m(re,e,i),_(e,Ae,i),_(e,J,i),m(ae,J,null),t(J,Tt),t(J,we),t(J,Mt),t(J,E),m(oe,E,null),t(E,Ft),t(E,Ie),t(E,wt),t(E,Ee),t(E,It),m(H,E,null),_(e,Qe,i),m(ne,e,i),_(e,Oe,i),_(e,M,i),m(se,M,null),t(M,Et),t(M,ze),t(M,zt),t(M,Je),t(M,Jt),t(M,q),m(ie,q,null),t(q,jt),t(q,je),t(M,Lt),t(M,N),m(ce,N,null),t(N,kt),t(N,Le),_(e,Ke,i),m(de,e,i),_(e,et,i),_(e,c,i),m(le,c,null),t(c,Pt),t(c,ke),t(c,Zt),t(c,R),m(me,R,null),t(R,Vt),t(R,Pe),t(c,Ct),t(c,W),m(pe,W,null),t(W,Ht),t(W,Ze),t(c,qt),t(c,B),m(ue,B,null),t(B,Nt),t(B,Ve),t(c,Rt),t(c,U),m(fe,U,null),t(U,Wt),t(U,Ce),t(c,Bt),t(c,X),m(ge,X,null),t(X,Ut),t(X,He),t(c,Xt),t(c,D),m(he,D,null),t(D,Dt),t(D,qe),t(c,Yt),t(c,Y),m(xe,Y,null),t(Y,Gt),t(Y,Ne),t(c,St),t(c,G),m(_e,G,null),t(G,At),t(G,Re),t(c,Qt),t(c,S),m(ve,S,null),t(S,Ot),t(S,We),t(c,Kt),t(c,A),m(ye,A,null),t(A,er),t(A,Be),_(e,tt,i),m(be,e,i),_(e,rt,i),_(e,Xe,i),at=!0},p(e,[i]){const j={};i&2&&(j.$$scope={dirty:i,ctx:e}),V.$set(j);const Z={};i&2&&(Z.$$scope={dirty:i,ctx:e}),H.$set(Z)},i(e){at||(p(I.$$.fragment,e),p(O.$$.fragment,e),p(K.$$.fragment,e),p(ee.$$.fragment,e),p(V.$$.fragment,e),p(te.$$.fragment,e),p(re.$$.fragment,e),p(ae.$$.fragment,e),p(oe.$$.fragment,e),p(H.$$.fragment,e),p(ne.$$.fragment,e),p(se.$$.fragment,e),p(ie.$$.fragment,e),p(ce.$$.fragment,e),p(de.$$.fragment,e),p(le.$$.fragment,e),p(me.$$.fragment,e),p(pe.$$.fragment,e),p(ue.$$.fragment,e),p(fe.$$.fragment,e),p(ge.$$.fragment,e),p(he.$$.fragment,e),p(xe.$$.fragment,e),p(_e.$$.fragment,e),p(ve.$$.fragment,e),p(ye.$$.fragment,e),p(be.$$.fragment,e),at=!0)},o(e){u(I.$$.fragment,e),u(O.$$.fragment,e),u(K.$$.fragment,e),u(ee.$$.fragment,e),u(V.$$.fragment,e),u(te.$$.fragment,e),u(re.$$.fragment,e),u(ae.$$.fragment,e),u(oe.$$.fragment,e),u(H.$$.fragment,e),u(ne.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),u(le.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(ue.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),u(he.$$.fragment,e),u(xe.$$.fragment,e),u(_e.$$.fragment,e),u(ve.$$.fragment,e),u(ye.$$.fragment,e),u(be.$$.fragment,e),at=!1},d(e){e&&(s(L),s(T),s(F),s(b),s(z),s(Ye),s(Ge),s(w),s(Se),s(Ae),s(J),s(Qe),s(Oe),s(M),s(Ke),s(et),s(c),s(tt),s(rt),s(Xe)),s(g),f(I,e),f(O,e),f(K),f(ee),f(V),f(te),f(re,e),f(ae),f(oe),f(H),f(ne,e),f(se),f(ie),f(ce),f(de,e),f(le),f(me),f(pe),f(ue),f(fe),f(ge),f(he),f(xe),f(_e),f(ve),f(ye),f(be,e)}}}const Vr='{"title":"Feature Extractor","local":"feature-extractor","sections":[{"title":"FeatureExtractionMixin","local":"transformers.FeatureExtractionMixin","sections":[],"depth":2},{"title":"SequenceFeatureExtractor","local":"transformers.SequenceFeatureExtractor","sections":[],"depth":2},{"title":"BatchFeature","local":"transformers.BatchFeature","sections":[],"depth":2},{"title":"ImageFeatureExtractionMixin","local":"transformers.ImageFeatureExtractionMixin","sections":[],"depth":2}],"depth":1}';function Cr($e){return Fr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xr extends wr{constructor(g){super(),Ir(this,g,Cr,Zr,Mr,{})}}export{Xr as component};
