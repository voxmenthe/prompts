import{s as nr,o as ar,n as ue}from"../chunks/scheduler.18a86fab.js";import{S as sr,i as rr,g as i,s as n,r as m,A as ir,h as l,f as o,c as a,j as M,x as c,u as p,k as T,l as lr,y as r,a as s,v as h,d as f,t as u,w as g}from"../chunks/index.98837b22.js";import{T as On}from"../chunks/Tip.77304350.js";import{D as C}from"../chunks/Docstring.a1ef7999.js";import{C as F}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as An}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as w,E as dr}from"../chunks/getInferenceSnippets.06c2775f.js";function cr(z){let d,v="Example:",b,k,y;return k=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyay1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIpJTBBJTBBJTIzJTIwVG8lMjBhZGQlMjBhJTIwdm9pY2UlMjBwcmVzZXQlMkMlMjB5b3UlMjBjYW4lMjBwYXNzJTIwJTYwdm9pY2VfcHJlc2V0JTYwJTIwdG8lMjAlNjBCYXJrUHJvY2Vzc29yLl9fY2FsbF9fKC4uLiklNjAlMEF2b2ljZV9wcmVzZXQlMjAlM0QlMjAlMjJ2MiUyRmVuX3NwZWFrZXJfNiUyMiUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTJDJTIwSSUyMG5lZWQlMjBoaW0lMjBpbiUyMG15JTIwbGlmZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc2VtYW50aWNfbWF4X25ld190b2tlbnMlM0QxMDApJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To add a voice preset, you can pass \`voice_preset\` to \`BarkProcessor.__call__(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute, I need him in my life&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs, semantic_max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,b=n(),m(k.$$.fragment)},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=v),b=a(_),p(k.$$.fragment,_)},m(_,B){s(_,d,B),s(_,b,B),h(k,_,B),y=!0},p:ue,i(_){y||(f(k.$$.fragment,_),y=!0)},o(_){u(k.$$.fragment,_),y=!1},d(_){_&&(o(d),o(b)),g(k,_)}}}function mr(z){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(b,k){s(b,d,k)},p:ue,d(b){b&&o(d)}}}function pr(z){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(b,k){s(b,d,k)},p:ue,d(b){b&&o(d)}}}function hr(z){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(b,k){s(b,d,k)},p:ue,d(b){b&&o(d)}}}function fr(z){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(b,k){s(b,d,k)},p:ue,d(b){b&&o(d)}}}function ur(z){let d,v="Example:",b,k,y;return k=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtDb2Fyc2VDb25maWclMkMlMjBCYXJrQ29hcnNlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQmFyayUyMHN1Yi1tb2R1bGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmFya0NvYXJzZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0NvYXJzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkCoarseConfig, BarkCoarseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkCoarseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkCoarseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,b=n(),m(k.$$.fragment)},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=v),b=a(_),p(k.$$.fragment,_)},m(_,B){s(_,d,B),s(_,b,B),h(k,_,B),y=!0},p:ue,i(_){y||(f(k.$$.fragment,_),y=!0)},o(_){u(k.$$.fragment,_),y=!1},d(_){_&&(o(d),o(b)),g(k,_)}}}function gr(z){let d,v="Example:",b,k,y;return k=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtGaW5lQ29uZmlnJTJDJTIwQmFya0ZpbmVNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCYXJrJTIwc3ViLW1vZHVsZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCYXJrRmluZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0ZpbmVNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkFineConfig, BarkFineModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkFineConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkFineModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,b=n(),m(k.$$.fragment)},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=v),b=a(_),p(k.$$.fragment,_)},m(_,B){s(_,d,B),s(_,b,B),h(k,_,B),y=!0},p:ue,i(_){y||(f(k.$$.fragment,_),y=!0)},o(_){u(k.$$.fragment,_),y=!1},d(_){_&&(o(d),o(b)),g(k,_)}}}function _r(z){let d,v="Example:",b,k,y;return k=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtTZW1hbnRpY0NvbmZpZyUyQyUyMEJhcmtTZW1hbnRpY01vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJhcmslMjBzdWItbW9kdWxlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEJhcmtTZW1hbnRpY0NvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya1NlbWFudGljTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkSemanticConfig, BarkSemanticModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkSemanticConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkSemanticModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,b=n(),m(k.$$.fragment)},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=v),b=a(_),p(k.$$.fragment,_)},m(_,B){s(_,d,B),s(_,b,B),h(k,_,B),y=!0},p:ue,i(_){y||(f(k.$$.fragment,_),y=!0)},o(_){u(k.$$.fragment,_),y=!1},d(_){_&&(o(d),o(b)),g(k,_)}}}function br(z){let d,v,b,k,y,_="<em>This model was released on 2023-04-09 and added to Hugging Face Transformers on 2023-07-17.</em>",B,ge,Jo,A,Oa='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/>',jo,_e,Io,be,Aa='<a href="https://huggingface.co/suno/bark" rel="nofollow">Bark</a> is a transformer-based text-to-speech model proposed by Suno AI in <a href="https://github.com/suno-ai/bark" rel="nofollow">suno-ai/bark</a>.',Uo,ke,Qa="Bark is made of 4 main models:",Wo,ve,Ka='<li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the ‚Äòtext‚Äô model): a causal auto-regressive transformer model that takes as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also referred to as the ‚Äòcoarse acoustics‚Äô model): a causal autoregressive transformer, that takes as input the results of the <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> model. It aims at predicting the first two audio codebooks necessary for EnCodec.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the ‚Äòfine acoustics‚Äô model), this time a non-causal autoencoder transformer, which iteratively predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a>, Bark uses it to decode the output audio array.</li>',Fo,ye,es="It should be noted that each of the first three modules can support conditional speaker embeddings to condition the output sound according to specific predefined voice.",Po,Te,ts=`This model was contributed by <a href="https://huggingface.co/ylacombe" rel="nofollow">Yoach Lacombe (ylacombe)</a> and <a href="https://github.com/sanchit-gandhi" rel="nofollow">Sanchit Gandhi (sanchit-gandhi)</a>.
The original code can be found <a href="https://github.com/suno-ai/bark" rel="nofollow">here</a>.`,Zo,Me,No,we,os="Bark can be optimized with just a few extra lines of code, which <strong>significantly reduces its memory footprint</strong> and <strong>accelerates inference</strong>.",Ho,$e,Lo,Be,ns="You can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.",qo,Ce,Go,xe,So,ze,as="As mentioned above, Bark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.",Vo,Je,ss="If you‚Äôre using a CUDA GPU or Intel XPU, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from device to CPU when they‚Äôre idle. This operation is called <em>CPU offloading</em>. You can use it with one line of code as follows:",Xo,je,Eo,Ie,rs='Note that ü§ó Accelerate must be installed before using this feature. <a href="https://huggingface.co/docs/accelerate/basic_tutorials/install" rel="nofollow">Here‚Äôs how to install it.</a>',Yo,Ue,Ro,We,is="Better Transformer is an ü§ó Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to ü§ó Better Transformer:",Do,Fe,Oo,Pe,ls='Note that ü§ó Optimum must be installed before using this feature. <a href="https://huggingface.co/docs/optimum/installation" rel="nofollow">Here‚Äôs how to install it.</a>',Ao,Ze,Qo,Ne,ds="Flash Attention 2 is an even faster, optimized version of the previous optimization.",Ko,He,en,Le,cs='First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the <a href="https://github.com/Dao-AILab/flash-attention#installation-and-features" rel="nofollow">official documentation</a>. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered <a href="https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer" rel="nofollow">above</a>.',tn,qe,ms='Next, <a href="https://github.com/Dao-AILab/flash-attention#installation-and-features" rel="nofollow">install</a> the latest version of Flash Attention 2:',on,Ge,nn,Se,an,Ve,ps='To load a model using Flash Attention 2, we can pass the <code>attn_implementation=&quot;flash_attention_2&quot;</code> flag to <a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained" rel="nofollow"><code>.from_pretrained</code></a>. We‚Äôll also load the model in half-precision (e.g. <code>torch.float16</code>), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:',sn,Xe,rn,Ee,ln,Ye,hs="The following diagram shows the latency for the native attention implementation (no optimisation) against Better Transformer and Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1. Flash Attention 2 is also consistently faster than Better Transformer, and its performance improves even more as batch sizes increase:",dn,Q,fs='<img src="https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/Bark%20Optimization%20Benchmark.png"/>',cn,Re,us='To put this into perspective, on an NVIDIA A100 and when generating 400 semantic tokens with a batch size of 16, you can get 17 times the <a href="https://huggingface.co/blog/optimizing-bark#throughput" rel="nofollow">throughput</a> and still be 2 seconds faster than generating sentences one by one with the native model implementation. In other words, all the samples will be generated 17 times faster.',mn,De,gs="At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Better Transformer, and at batch size 16, 25%.",pn,Oe,hn,Ae,_s="You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or ü§ó Better Transformer) all at once.",fn,Qe,un,Ke,bs='Find out more on inference optimization techniques <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" rel="nofollow">here</a>.',gn,et,_n,tt,ks=`Suno offers a library of voice presets in a number of languages <a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a>.
These presets are also uploaded in the hub <a href="https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings" rel="nofollow">here</a> or <a href="https://huggingface.co/suno/bark/tree/main/speaker_embeddings" rel="nofollow">here</a>.`,bn,ot,kn,nt,vs="Bark can generate highly realistic, <strong>multilingual</strong> speech as well as other audio - including music, background noise and simple sound effects.",vn,at,yn,st,ys="The model can also produce <strong>nonverbal communications</strong> like laughing, sighing and crying.",Tn,rt,Mn,it,Ts="To save the audio, simply take the sample rate from the model config and some scipy utility:",wn,lt,$n,dt,Bn,J,ct,Qn,Vt,Ms=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkModel">BarkModel</a>. It is used to instantiate a Bark
model according to the specified sub-models configurations, defining the model architecture.`,Kn,Xt,ws=`Instantiating a configuration with the defaults will yield a similar configuration to that of the Bark
<a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a> architecture.`,ea,Et,$s=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ta,K,mt,oa,Yt,Bs='Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (or a derived class) from bark sub-models configuration.',Cn,pt,xn,j,ht,na,Rt,Cs="Constructs a Bark processor which wraps a text tokenizer and optional Bark voice presets into a single processor.",aa,ee,ft,sa,Dt,xs=`Main method to prepare for the model one or several sequences(s). This method forwards the <code>text</code> and <code>kwargs</code>
arguments to the AutoTokenizer‚Äôs <code>__call__()</code> to encode the text. The method also proposes a
voice preset which is a dictionary of arrays that conditions <code>Bark</code>‚Äôs output. <code>kwargs</code> arguments are forwarded
to the tokenizer and to <code>cached_file</code> method if <code>voice_preset</code> is a valid filename.`,ra,te,ut,ia,Ot,zs="Instantiate a Bark processor associated with a pretrained model.",la,oe,gt,da,At,Js=`Saves the attributes of this processor (tokenizer‚Ä¶) in the specified directory so that it can be reloaded
using the <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkProcessor.from_pretrained">from_pretrained()</a> method.`,zn,_t,Jn,$,bt,ca,Qt,js="The full Bark model, a text-to-speech model composed of 4 sub-models:",ma,Kt,Is=`<li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the ‚Äòtext‚Äô model): a causal auto-regressive transformer model that
takes
as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also referred to as the ‚Äòcoarse acoustics‚Äô model), also a causal autoregressive transformer,
that takes into input the results of the last model. It aims at regressing the first two audio codebooks necessary
to <code>encodec</code>.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the ‚Äòfine acoustics‚Äô model), this time a non-causal autoencoder transformer, which iteratively
predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <a href="/docs/transformers/v4.56.2/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a>, Bark uses it to decode the output audio
array.</li>`,pa,eo,Us=`It should be noted that each of the first three modules can support conditional speaker embeddings to condition the
output sound according to specific predefined voice.`,ha,to,Ws=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fa,oo,Fs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ua,q,kt,ga,no,Ps="Generates audio from an input prompt and an additional optional <code>Bark</code> speaker prompt.",_a,ne,ba,ae,vt,ka,ao,Zs=`Offloads all sub-models to CPU using accelerate, reducing memory usage with a low impact on performance. This
method moves one whole sub-model at a time to the accelerator when it is used, and the sub-model remains in accelerator until the next sub-model runs.`,jn,yt,In,I,Tt,va,so,Ns=`Bark semantic (or text) model. It shares the same architecture as the coarse model.
It is a GPT-2 like autoregressive model with a language modeling head on top.`,ya,ro,Hs=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ta,io,Ls=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ma,G,Mt,wa,lo,qs='The <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',$a,se,Un,wt,Wn,U,$t,Ba,co,Gs=`Bark coarse acoustics model.
It shares the same architecture as the semantic (or text) model. It is a GPT-2 like autoregressive model with a
language modeling head on top.`,Ca,mo,Ss=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xa,po,Vs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,za,S,Bt,Ja,ho,Xs='The <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',ja,re,Fn,Ct,Pn,W,xt,Ia,fo,Es=`Bark fine acoustics model. It is a non-causal GPT-like model with <code>config.n_codes_total</code> embedding layers and
language modeling heads, one for each codebook.`,Ua,uo,Ys=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Wa,go,Rs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Fa,V,zt,Pa,_o,Ds='The <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> forward method, overrides the <code>__call__</code> special method.',Za,ie,Zn,Jt,Nn,D,jt,Na,X,It,Ha,bo,Os='The <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',La,le,Hn,Ut,Ln,P,Wt,qa,ko,As=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Ga,vo,Qs=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Sa,de,qn,Ft,Gn,Z,Pt,Va,yo,Ks=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Xa,To,er=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ea,ce,Sn,Zt,Vn,N,Nt,Ya,Mo,tr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Ra,wo,or=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Da,me,Xn,Ht,En,zo,Yn;return ge=new w({props:{title:"Bark",local:"bark",headingTag:"h1"}}),_e=new w({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Me=new w({props:{title:"Optimizing Bark",local:"optimizing-bark",headingTag:"h3"}}),$e=new w({props:{title:"Using half-precision",local:"using-half-precision",headingTag:"h4"}}),Ce=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUyQyUyMGluZmVyX2RldmljZSUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKGRldmljZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel, infer_device
<span class="hljs-keyword">import</span> torch

device = infer_device()
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, dtype=torch.float16).to(device)`,wrap:!1}}),xe=new w({props:{title:"Using CPU offload",local:"using-cpu-offload",headingTag:"h4"}}),je=new F({props:{code:"bW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:"model.enable_cpu_offload()",wrap:!1}}),Ue=new w({props:{title:"Using Better Transformer",local:"using-better-transformer",headingTag:"h4"}}),Fe=new F({props:{code:"bW9kZWwlMjAlM0QlMjAlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model =  model.to_bettertransformer()",wrap:!1}}),Ze=new w({props:{title:"Using Flash Attention 2",local:"using-flash-attention-2",headingTag:"h4"}}),He=new w({props:{title:"Installation",local:"installation",headingTag:"h5"}}),Ge=new F({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),Se=new w({props:{title:"Usage",local:"usage",headingTag:"h5"}}),Xe=new F({props:{code:"bW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKS50byhkZXZpY2Up",highlighted:'model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>).to(device)',wrap:!1}}),Ee=new w({props:{title:"Performance comparison",local:"performance-comparison",headingTag:"h5"}}),Oe=new w({props:{title:"Combining optimization techniques",local:"combining-optimization-techniques",headingTag:"h4"}}),Qe=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUyQyUyMGluZmVyX2RldmljZSUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEElMjMlMjBsb2FkJTIwaW4lMjBmcDE2JTIwYW5kJTIwdXNlJTIwRmxhc2glMjBBdHRlbnRpb24lMjAyJTBBbW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKS50byhkZXZpY2UpJTBBJTBBJTIzJTIwZW5hYmxlJTIwQ1BVJTIwb2ZmbG9hZCUwQW1vZGVsLmVuYWJsZV9jcHVfb2ZmbG9hZCgp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel, infer_device
<span class="hljs-keyword">import</span> torch

device = infer_device()

<span class="hljs-comment"># load in fp16 and use Flash Attention 2</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>).to(device)

<span class="hljs-comment"># enable CPU offload</span>
model.enable_cpu_offload()`,wrap:!1}}),et=new w({props:{title:"Usage tips",local:"usage-tips",headingTag:"h3"}}),ot=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmslMjIpJTBBJTBBdm9pY2VfcHJlc2V0JTIwJTNEJTIwJTIydjIlMkZlbl9zcGVha2VyXzYlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),at=new F({props:{code:"JTIzJTIwTXVsdGlsaW5ndWFsJTIwc3BlZWNoJTIwLSUyMHNpbXBsaWZpZWQlMjBDaGluZXNlJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUyMiVFNiU4MyU4QSVFNCVCQSVCQSVFNyU5QSU4NCVFRiVCQyU4MSVFNiU4OCU5MSVFNCVCQyU5QSVFOCVBRiVCNCVFNCVCOCVBRCVFNiU5NiU4NyUyMiklMEElMEElMjMlMjBNdWx0aWxpbmd1YWwlMjBzcGVlY2glMjAtJTIwRnJlbmNoJTIwLSUyMGxldCdzJTIwdXNlJTIwYSUyMHZvaWNlX3ByZXNldCUyMGFzJTIwd2VsbCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJJbmNyb3lhYmxlISUyMEplJTIwcGV1eCUyMGclQzMlQTluJUMzJUE5cmVyJTIwZHUlMjBzb24uJTIyJTJDJTIwdm9pY2VfcHJlc2V0JTNEJTIyZnJfc3BlYWtlcl81JTIyKSUwQSUwQSUyMyUyMEJhcmslMjBjYW4lMjBhbHNvJTIwZ2VuZXJhdGUlMjBtdXNpYy4lMjBZb3UlMjBjYW4lMjBoZWxwJTIwaXQlMjBvdXQlMjBieSUyMGFkZGluZyUyMG11c2ljJTIwbm90ZXMlMjBhcm91bmQlMjB5b3VyJTIwbHlyaWNzLiUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjIlRTIlOTklQUElMjBIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIwJUUyJTk5JUFBJTIyKSUwQSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - simplified Chinese</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;ÊÉä‰∫∫ÁöÑÔºÅÊàë‰ºöËØ¥‰∏≠Êñá&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - French - let&#x27;s use a voice_preset as well</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Incroyable! Je peux g√©n√©rer du son.&quot;</span>, voice_preset=<span class="hljs-string">&quot;fr_speaker_5&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Bark can also generate music. You can help it out by adding music notes around your lyrics.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;‚ô™ Hello, my dog is cute ‚ô™&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),rt=new F({props:{code:"JTIzJTIwQWRkaW5nJTIwbm9uLXNwZWVjaCUyMGN1ZXMlMjB0byUyMHRoZSUyMGlucHV0JTIwdGV4dCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyMHVoJTIwLi4uJTIwJTVCY2xlYXJzJTIwdGhyb2F0JTVEJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjAlNUJsYXVnaHRlciU1RCUyMiklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Adding non-speech cues to the input text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello uh ... [clears throat], my dog is cute [laughter]&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),lt=new F({props:{code:"ZnJvbSUyMHNjaXB5LmlvLndhdmZpbGUlMjBpbXBvcnQlMjB3cml0ZSUyMGFzJTIwd3JpdGVfd2F2JTBBJTBBJTIzJTIwc2F2ZSUyMGF1ZGlvJTIwdG8lMjBkaXNrJTJDJTIwYnV0JTIwZmlyc3QlMjB0YWtlJTIwdGhlJTIwc2FtcGxlJTIwcmF0ZSUyMGZyb20lMjB0aGUlMjBtb2RlbCUyMGNvbmZpZyUwQXNhbXBsZV9yYXRlJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGlvbl9jb25maWcuc2FtcGxlX3JhdGUlMEF3cml0ZV93YXYoJTIyYmFya19nZW5lcmF0aW9uLndhdiUyMiUyQyUyMHNhbXBsZV9yYXRlJTJDJTIwYXVkaW9fYXJyYXkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> scipy.io.wavfile <span class="hljs-keyword">import</span> write <span class="hljs-keyword">as</span> write_wav

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save audio to disk, but first take the sample rate from the model config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sample_rate = model.generation_config.sample_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>write_wav(<span class="hljs-string">&quot;bark_generation.wav&quot;</span>, sample_rate, audio_array)`,wrap:!1}}),dt=new w({props:{title:"BarkConfig",local:"transformers.BarkConfig",headingTag:"h2"}}),ct=new C({props:{name:"class transformers.BarkConfig",anchor:"transformers.BarkConfig",parameters:[{name:"semantic_config",val:": typing.Optional[dict] = None"},{name:"coarse_acoustics_config",val:": typing.Optional[dict] = None"},{name:"fine_acoustics_config",val:": typing.Optional[dict] = None"},{name:"codec_config",val:": typing.Optional[dict] = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkConfig.semantic_config",description:`<strong>semantic_config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying semantic sub-model.`,name:"semantic_config"},{anchor:"transformers.BarkConfig.coarse_acoustics_config",description:`<strong>coarse_acoustics_config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying coarse acoustics sub-model.`,name:"coarse_acoustics_config"},{anchor:"transformers.BarkConfig.fine_acoustics_config",description:`<strong>fine_acoustics_config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying fine acoustics sub-model.`,name:"fine_acoustics_config"},{anchor:"transformers.BarkConfig.codec_config",description:`<strong>codec_config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying codec sub-model.`,name:"codec_config"},{anchor:"transformers.BarkConfig.Example",description:"<strong>Example</strong> &#x2014;",name:"Example"},{anchor:"transformers.BarkConfig.```python",description:`<strong>\`\`\`python</strong> &#x2014;</p>
<blockquote>
<blockquote>
<blockquote>
<p>from transformers import (</p>
</blockquote>
</blockquote>
</blockquote>`,name:"```python"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     BarkSemanticConfig, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     BarkCoarseConfig, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     BarkFineConfig, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     BarkModel, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     BarkConfig, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong>     AutoConfig, &#x2014;",name:"‚Ä¶"},{anchor:"transformers.BarkConfig.‚Ä¶",description:"<strong>&#x2026;</strong> ) &#x2014;",name:"‚Ä¶"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/configuration_bark.py#L183"}}),mt=new C({props:{name:"from_sub_model_configs",anchor:"transformers.BarkConfig.from_sub_model_configs",parameters:[{name:"semantic_config",val:": BarkSemanticConfig"},{name:"coarse_acoustics_config",val:": BarkCoarseConfig"},{name:"fine_acoustics_config",val:": BarkFineConfig"},{name:"codec_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/configuration_bark.py#L279",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a></p>
`}}),pt=new w({props:{title:"BarkProcessor",local:"transformers.BarkProcessor",headingTag:"h2"}}),ht=new C({props:{name:"class transformers.BarkProcessor",anchor:"transformers.BarkProcessor",parameters:[{name:"tokenizer",val:""},{name:"speaker_embeddings",val:" = None"}],parametersDescription:[{anchor:"transformers.BarkProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>.`,name:"tokenizer"},{anchor:"transformers.BarkProcessor.speaker_embeddings",description:`<strong>speaker_embeddings</strong> (<code>dict[dict[str]]</code>, <em>optional</em>) &#x2014;
Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g
<code>&quot;en_speaker_4&quot;</code>). The second level contains <code>&quot;semantic_prompt&quot;</code>, <code>&quot;coarse_prompt&quot;</code> and <code>&quot;fine_prompt&quot;</code>
embeddings. The values correspond to the path of the corresponding <code>np.ndarray</code>. See
<a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a> for
a list of <code>voice_preset_names</code>.`,name:"speaker_embeddings"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/processing_bark.py#L36"}}),ft=new C({props:{name:"__call__",anchor:"transformers.BarkProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"voice_preset",val:" = None"},{name:"return_tensors",val:" = 'pt'"},{name:"max_length",val:" = 256"},{name:"add_special_tokens",val:" = False"},{name:"return_attention_mask",val:" = True"},{name:"return_token_type_ids",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>list[str]</code>, <code>list[list[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.BarkProcessor.__call__.voice_preset",description:`<strong>voice_preset</strong> (<code>str</code>, <code>dict[np.ndarray]</code>) &#x2014;
The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g
<code>&quot;en_speaker_1&quot;</code>, or directly a dictionary of <code>np.ndarray</code> embeddings for each submodel of <code>Bark</code>. Or
it can be a valid file name of a local <code>.npz</code> single voice preset.`,name:"voice_preset"},{anchor:"transformers.BarkProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/processing_bark.py#L226",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> object containing the output of the <code>tokenizer</code>.
If a voice preset is provided, the returned object will include a <code>"history_prompt"</code> key
containing a <a
  href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>, i.e the voice preset with the right tensors type.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),ut=new C({props:{name:"from_pretrained",anchor:"transformers.BarkProcessor.from_pretrained",parameters:[{name:"pretrained_processor_name_or_path",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor saved using the <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkProcessor.save_pretrained">save_pretrained()</a>
method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BarkProcessor.from_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file containing the speaker_embeddings dictionary located in
<code>pretrained_model_name_or_path</code>. If <code>None</code>, no speaker_embeddings is loaded.`,name:"speaker_embeddings_dict_path"},{anchor:"transformers.BarkProcessor.from_pretrained.*kwargs",description:`*<strong>*kwargs</strong> &#x2014;
Additional keyword arguments passed along to both
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/processing_bark.py#L66"}}),gt=new C({props:{name:"save_pretrained",anchor:"transformers.BarkProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"speaker_embeddings_directory",val:" = 'speaker_embeddings'"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the tokenizer files and the speaker embeddings will be saved (directory will be created
if it does not exist).`,name:"save_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file that will contains the speaker_embeddings nested path dictionary, if it
exists, and that will be located in <code>pretrained_model_name_or_path/speaker_embeddings_directory</code>.`,name:"speaker_embeddings_dict_path"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_directory",description:`<strong>speaker_embeddings_directory</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings/&quot;</code>) &#x2014;
The name of the folder in which the speaker_embeddings arrays will be saved.`,name:"speaker_embeddings_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.BarkProcessor.save_pretrained.kwargs",description:`<strong>kwargs</strong> &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/processing_bark.py#L122"}}),_t=new w({props:{title:"BarkModel",local:"transformers.BarkModel",headingTag:"h2"}}),bt=new C({props:{name:"class transformers.BarkModel",anchor:"transformers.BarkModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkModel">BarkModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L1349"}}),kt=new C({props:{name:"generate",anchor:"transformers.BarkModel.generate",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"history_prompt",val:": typing.Optional[dict[str, torch.Tensor]] = None"},{name:"return_output_lengths",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.generate.input_ids",description:`<strong>input_ids</strong> (<code>Optional[torch.Tensor]</code> of shape (batch_size, seq_len), <em>optional</em>) &#x2014;
Input ids. Will be truncated up to 256 tokens. Note that the output audios will be as long as the
longest generation among the batch.`,name:"input_ids"},{anchor:"transformers.BarkModel.generate.history_prompt",description:`<strong>history_prompt</strong> (<code>Optional[dict[str,torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Optional <code>Bark</code> speaker prompt. Note that for now, this model takes only one speaker prompt per batch.`,name:"history_prompt"},{anchor:"transformers.BarkModel.generate.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014; Remaining dictionary of keyword arguments. Keyword arguments are of two types:</p>
<ul>
<li>Without a prefix, they will be entered as <code>**kwargs</code> for the <code>generate</code> method of each sub-model.</li>
<li>With a <em>semantic_</em>, <em>coarse_</em>, <em>fine_</em> prefix, they will be input for the <code>generate</code> method of the
semantic, coarse and fine respectively. It has the priority over the keywords without a prefix.</li>
</ul>
<p>This means you can, for example, specify a generation strategy for all sub-models except one.`,name:"kwargs"},{anchor:"transformers.BarkModel.generate.return_output_lengths",description:`<strong>return_output_lengths</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the waveform lengths. Useful when batching.`,name:"return_output_lengths"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L1465",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.
When <code>return_output_lengths=True</code>:
Returns a tuple made of:</li>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.</li>
<li><strong>output_lengths</strong> (<code>torch.Tensor</code> of shape (batch_size)): The length of each waveform in the batch</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>By default</p>
`}}),ne=new An({props:{anchor:"transformers.BarkModel.generate.example",$$slots:{default:[cr]},$$scope:{ctx:z}}}),vt=new C({props:{name:"enable_cpu_offload",anchor:"transformers.BarkModel.enable_cpu_offload",parameters:[{name:"accelerator_id",val:": typing.Optional[int] = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.enable_cpu_offload.accelerator_id",description:`<strong>accelerator_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
accelerator id on which the sub-models will be loaded and offloaded. This argument is deprecated.`,name:"accelerator_id"},{anchor:"transformers.BarkModel.enable_cpu_offload.kwargs",description:`<strong>kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
additional keyword arguments:
<code>gpu_id</code>: accelerator id on which the sub-models will be loaded and offloaded.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L1389"}}),yt=new w({props:{title:"BarkSemanticModel",local:"transformers.BarkSemanticModel",headingTag:"h2"}}),Tt=new C({props:{name:"class transformers.BarkSemanticModel",anchor:"transformers.BarkSemanticModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L592"}}),Mt=new C({props:{name:"forward",anchor:"transformers.BarkSemanticModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkSemanticModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkSemanticModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkSemanticModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkSemanticModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkSemanticModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.BarkSemanticModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkSemanticModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkSemanticModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkSemanticModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkSemanticModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BarkSemanticModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L437",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new On({props:{$$slots:{default:[mr]},$$scope:{ctx:z}}}),wt=new w({props:{title:"BarkCoarseModel",local:"transformers.BarkCoarseModel",headingTag:"h2"}}),$t=new C({props:{name:"class transformers.BarkCoarseModel",anchor:"transformers.BarkCoarseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.config",description:`<strong>config</strong> ([<code>[BarkCausalModel](/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCausalModel)</code>]) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L703"}}),Bt=new C({props:{name:"forward",anchor:"transformers.BarkCoarseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCoarseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCoarseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCoarseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCoarseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCoarseModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.BarkCoarseModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCoarseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCoarseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCoarseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCoarseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BarkCoarseModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L437",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new On({props:{$$slots:{default:[pr]},$$scope:{ctx:z}}}),Ct=new w({props:{title:"BarkFineModel",local:"transformers.BarkFineModel",headingTag:"h2"}}),xt=new C({props:{name:"class transformers.BarkFineModel",anchor:"transformers.BarkFineModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkFineModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L924"}}),zt=new C({props:{name:"forward",anchor:"transformers.BarkFineModel.forward",parameters:[{name:"codebook_idx",val:": int"},{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.BarkFineModel.forward.codebook_idx",description:`<strong>codebook_idx</strong> (<code>int</code>) &#x2014;
Index of the codebook that will be predicted.`,name:"codebook_idx"},{anchor:"transformers.BarkFineModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkFineModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkFineModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkFineModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkFineModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
NOT IMPLEMENTED YET.`,name:"labels"},{anchor:"transformers.BarkFineModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. If
<code>past_key_values</code> is used, optionally only the last <code>input_embeds</code> have to be input (see
<code>past_key_values</code>). This is useful if you want more control over how to convert <code>input_ids</code> indices into
associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"input_embeds"},{anchor:"transformers.BarkFineModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkFineModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkFineModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L1071",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new On({props:{$$slots:{default:[hr]},$$scope:{ctx:z}}}),Jt=new w({props:{title:"BarkCausalModel",local:"transformers.BarkCausalModel",headingTag:"h2"}}),jt=new C({props:{name:"class transformers.BarkCausalModel",anchor:"transformers.BarkCausalModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L376"}}),It=new C({props:{name:"forward",anchor:"transformers.BarkCausalModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.BarkCausalModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCausalModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCausalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCausalModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCausalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCausalModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.BarkCausalModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCausalModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCausalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCausalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCausalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BarkCausalModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/modeling_bark.py#L437",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new On({props:{$$slots:{default:[fr]},$$scope:{ctx:z}}}),Ut=new w({props:{title:"BarkCoarseConfig",local:"transformers.BarkCoarseConfig",headingTag:"h2"}}),Wt=new C({props:{name:"class transformers.BarkCoarseConfig",anchor:"transformers.BarkCoarseConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkCoarseConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkCoarseConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkCoarseConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkCoarseConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkCoarseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkCoarseConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkCoarseConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkCoarseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkCoarseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/configuration_bark.py#L144"}}),de=new An({props:{anchor:"transformers.BarkCoarseConfig.example",$$slots:{default:[ur]},$$scope:{ctx:z}}}),Ft=new w({props:{title:"BarkFineConfig",local:"transformers.BarkFineConfig",headingTag:"h2"}}),Pt=new C({props:{name:"class transformers.BarkFineConfig",anchor:"transformers.BarkFineConfig",parameters:[{name:"tie_word_embeddings",val:" = True"},{name:"n_codes_total",val:" = 8"},{name:"n_codes_given",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkFineConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkFineConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkFineConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkFineConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkFineConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkFineConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkFineConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkFineConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkFineConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkFineConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.BarkFineConfig.n_codes_total",description:`<strong>n_codes_total</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The total number of audio codebooks predicted. Used in the fine acoustics sub-model.`,name:"n_codes_total"},{anchor:"transformers.BarkFineConfig.n_codes_given",description:`<strong>n_codes_given</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of audio codebooks predicted in the coarse acoustics sub-model. Used in the acoustics
sub-models.`,name:"n_codes_given"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/configuration_bark.py#L172"}}),ce=new An({props:{anchor:"transformers.BarkFineConfig.example",$$slots:{default:[gr]},$$scope:{ctx:z}}}),Zt=new w({props:{title:"BarkSemanticConfig",local:"transformers.BarkSemanticConfig",headingTag:"h2"}}),Nt=new C({props:{name:"class transformers.BarkSemanticConfig",anchor:"transformers.BarkSemanticConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkSemanticConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkSemanticConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/v4.56.2/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkSemanticConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkSemanticConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkSemanticConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkSemanticConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkSemanticConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkSemanticConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkSemanticConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/bark/configuration_bark.py#L121"}}),me=new An({props:{anchor:"transformers.BarkSemanticConfig.example",$$slots:{default:[_r]},$$scope:{ctx:z}}}),Ht=new dr({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md"}}),{c(){d=i("meta"),v=n(),b=i("p"),k=n(),y=i("p"),y.innerHTML=_,B=n(),m(ge.$$.fragment),Jo=n(),A=i("div"),A.innerHTML=Oa,jo=n(),m(_e.$$.fragment),Io=n(),be=i("p"),be.innerHTML=Aa,Uo=n(),ke=i("p"),ke.textContent=Qa,Wo=n(),ve=i("ul"),ve.innerHTML=Ka,Fo=n(),ye=i("p"),ye.textContent=es,Po=n(),Te=i("p"),Te.innerHTML=ts,Zo=n(),m(Me.$$.fragment),No=n(),we=i("p"),we.innerHTML=os,Ho=n(),m($e.$$.fragment),Lo=n(),Be=i("p"),Be.textContent=ns,qo=n(),m(Ce.$$.fragment),Go=n(),m(xe.$$.fragment),So=n(),ze=i("p"),ze.textContent=as,Vo=n(),Je=i("p"),Je.innerHTML=ss,Xo=n(),m(je.$$.fragment),Eo=n(),Ie=i("p"),Ie.innerHTML=rs,Yo=n(),m(Ue.$$.fragment),Ro=n(),We=i("p"),We.textContent=is,Do=n(),m(Fe.$$.fragment),Oo=n(),Pe=i("p"),Pe.innerHTML=ls,Ao=n(),m(Ze.$$.fragment),Qo=n(),Ne=i("p"),Ne.textContent=ds,Ko=n(),m(He.$$.fragment),en=n(),Le=i("p"),Le.innerHTML=cs,tn=n(),qe=i("p"),qe.innerHTML=ms,on=n(),m(Ge.$$.fragment),nn=n(),m(Se.$$.fragment),an=n(),Ve=i("p"),Ve.innerHTML=ps,sn=n(),m(Xe.$$.fragment),rn=n(),m(Ee.$$.fragment),ln=n(),Ye=i("p"),Ye.textContent=hs,dn=n(),Q=i("div"),Q.innerHTML=fs,cn=n(),Re=i("p"),Re.innerHTML=us,mn=n(),De=i("p"),De.textContent=gs,pn=n(),m(Oe.$$.fragment),hn=n(),Ae=i("p"),Ae.textContent=_s,fn=n(),m(Qe.$$.fragment),un=n(),Ke=i("p"),Ke.innerHTML=bs,gn=n(),m(et.$$.fragment),_n=n(),tt=i("p"),tt.innerHTML=ks,bn=n(),m(ot.$$.fragment),kn=n(),nt=i("p"),nt.innerHTML=vs,vn=n(),m(at.$$.fragment),yn=n(),st=i("p"),st.innerHTML=ys,Tn=n(),m(rt.$$.fragment),Mn=n(),it=i("p"),it.textContent=Ts,wn=n(),m(lt.$$.fragment),$n=n(),m(dt.$$.fragment),Bn=n(),J=i("div"),m(ct.$$.fragment),Qn=n(),Vt=i("p"),Vt.innerHTML=Ms,Kn=n(),Xt=i("p"),Xt.innerHTML=ws,ea=n(),Et=i("p"),Et.innerHTML=$s,ta=n(),K=i("div"),m(mt.$$.fragment),oa=n(),Yt=i("p"),Yt.innerHTML=Bs,Cn=n(),m(pt.$$.fragment),xn=n(),j=i("div"),m(ht.$$.fragment),na=n(),Rt=i("p"),Rt.textContent=Cs,aa=n(),ee=i("div"),m(ft.$$.fragment),sa=n(),Dt=i("p"),Dt.innerHTML=xs,ra=n(),te=i("div"),m(ut.$$.fragment),ia=n(),Ot=i("p"),Ot.textContent=zs,la=n(),oe=i("div"),m(gt.$$.fragment),da=n(),At=i("p"),At.innerHTML=Js,zn=n(),m(_t.$$.fragment),Jn=n(),$=i("div"),m(bt.$$.fragment),ca=n(),Qt=i("p"),Qt.textContent=js,ma=n(),Kt=i("ul"),Kt.innerHTML=Is,pa=n(),eo=i("p"),eo.textContent=Us,ha=n(),to=i("p"),to.innerHTML=Ws,fa=n(),oo=i("p"),oo.innerHTML=Fs,ua=n(),q=i("div"),m(kt.$$.fragment),ga=n(),no=i("p"),no.innerHTML=Ps,_a=n(),m(ne.$$.fragment),ba=n(),ae=i("div"),m(vt.$$.fragment),ka=n(),ao=i("p"),ao.textContent=Zs,jn=n(),m(yt.$$.fragment),In=n(),I=i("div"),m(Tt.$$.fragment),va=n(),so=i("p"),so.textContent=Ns,ya=n(),ro=i("p"),ro.innerHTML=Hs,Ta=n(),io=i("p"),io.innerHTML=Ls,Ma=n(),G=i("div"),m(Mt.$$.fragment),wa=n(),lo=i("p"),lo.innerHTML=qs,$a=n(),m(se.$$.fragment),Un=n(),m(wt.$$.fragment),Wn=n(),U=i("div"),m($t.$$.fragment),Ba=n(),co=i("p"),co.textContent=Gs,Ca=n(),mo=i("p"),mo.innerHTML=Ss,xa=n(),po=i("p"),po.innerHTML=Vs,za=n(),S=i("div"),m(Bt.$$.fragment),Ja=n(),ho=i("p"),ho.innerHTML=Xs,ja=n(),m(re.$$.fragment),Fn=n(),m(Ct.$$.fragment),Pn=n(),W=i("div"),m(xt.$$.fragment),Ia=n(),fo=i("p"),fo.innerHTML=Es,Ua=n(),uo=i("p"),uo.innerHTML=Ys,Wa=n(),go=i("p"),go.innerHTML=Rs,Fa=n(),V=i("div"),m(zt.$$.fragment),Pa=n(),_o=i("p"),_o.innerHTML=Ds,Za=n(),m(ie.$$.fragment),Zn=n(),m(Jt.$$.fragment),Nn=n(),D=i("div"),m(jt.$$.fragment),Na=n(),X=i("div"),m(It.$$.fragment),Ha=n(),bo=i("p"),bo.innerHTML=Os,La=n(),m(le.$$.fragment),Hn=n(),m(Ut.$$.fragment),Ln=n(),P=i("div"),m(Wt.$$.fragment),qa=n(),ko=i("p"),ko.innerHTML=As,Ga=n(),vo=i("p"),vo.innerHTML=Qs,Sa=n(),m(de.$$.fragment),qn=n(),m(Ft.$$.fragment),Gn=n(),Z=i("div"),m(Pt.$$.fragment),Va=n(),yo=i("p"),yo.innerHTML=Ks,Xa=n(),To=i("p"),To.innerHTML=er,Ea=n(),m(ce.$$.fragment),Sn=n(),m(Zt.$$.fragment),Vn=n(),N=i("div"),m(Nt.$$.fragment),Ya=n(),Mo=i("p"),Mo.innerHTML=tr,Ra=n(),wo=i("p"),wo.innerHTML=or,Da=n(),m(me.$$.fragment),Xn=n(),m(Ht.$$.fragment),En=n(),zo=i("p"),this.h()},l(e){const t=ir("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),v=a(e),b=l(e,"P",{}),M(b).forEach(o),k=a(e),y=l(e,"P",{"data-svelte-h":!0}),c(y)!=="svelte-auapkq"&&(y.innerHTML=_),B=a(e),p(ge.$$.fragment,e),Jo=a(e),A=l(e,"DIV",{class:!0,"data-svelte-h":!0}),c(A)!=="svelte-1bxez4j"&&(A.innerHTML=Oa),jo=a(e),p(_e.$$.fragment,e),Io=a(e),be=l(e,"P",{"data-svelte-h":!0}),c(be)!=="svelte-1nwmoe4"&&(be.innerHTML=Aa),Uo=a(e),ke=l(e,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-x1eizn"&&(ke.textContent=Qa),Wo=a(e),ve=l(e,"UL",{"data-svelte-h":!0}),c(ve)!=="svelte-1vq79nz"&&(ve.innerHTML=Ka),Fo=a(e),ye=l(e,"P",{"data-svelte-h":!0}),c(ye)!=="svelte-birews"&&(ye.textContent=es),Po=a(e),Te=l(e,"P",{"data-svelte-h":!0}),c(Te)!=="svelte-1m4odbp"&&(Te.innerHTML=ts),Zo=a(e),p(Me.$$.fragment,e),No=a(e),we=l(e,"P",{"data-svelte-h":!0}),c(we)!=="svelte-d9f63q"&&(we.innerHTML=os),Ho=a(e),p($e.$$.fragment,e),Lo=a(e),Be=l(e,"P",{"data-svelte-h":!0}),c(Be)!=="svelte-1c57ze6"&&(Be.textContent=ns),qo=a(e),p(Ce.$$.fragment,e),Go=a(e),p(xe.$$.fragment,e),So=a(e),ze=l(e,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-1gm6phf"&&(ze.textContent=as),Vo=a(e),Je=l(e,"P",{"data-svelte-h":!0}),c(Je)!=="svelte-1bslrcf"&&(Je.innerHTML=ss),Xo=a(e),p(je.$$.fragment,e),Eo=a(e),Ie=l(e,"P",{"data-svelte-h":!0}),c(Ie)!=="svelte-jn1vz4"&&(Ie.innerHTML=rs),Yo=a(e),p(Ue.$$.fragment,e),Ro=a(e),We=l(e,"P",{"data-svelte-h":!0}),c(We)!=="svelte-1r0gpm4"&&(We.textContent=is),Do=a(e),p(Fe.$$.fragment,e),Oo=a(e),Pe=l(e,"P",{"data-svelte-h":!0}),c(Pe)!=="svelte-bu0gze"&&(Pe.innerHTML=ls),Ao=a(e),p(Ze.$$.fragment,e),Qo=a(e),Ne=l(e,"P",{"data-svelte-h":!0}),c(Ne)!=="svelte-ud6z0l"&&(Ne.textContent=ds),Ko=a(e),p(He.$$.fragment,e),en=a(e),Le=l(e,"P",{"data-svelte-h":!0}),c(Le)!=="svelte-ib5l43"&&(Le.innerHTML=cs),tn=a(e),qe=l(e,"P",{"data-svelte-h":!0}),c(qe)!=="svelte-1pp3dkd"&&(qe.innerHTML=ms),on=a(e),p(Ge.$$.fragment,e),nn=a(e),p(Se.$$.fragment,e),an=a(e),Ve=l(e,"P",{"data-svelte-h":!0}),c(Ve)!=="svelte-1qm2lzz"&&(Ve.innerHTML=ps),sn=a(e),p(Xe.$$.fragment,e),rn=a(e),p(Ee.$$.fragment,e),ln=a(e),Ye=l(e,"P",{"data-svelte-h":!0}),c(Ye)!=="svelte-19fzd9"&&(Ye.textContent=hs),dn=a(e),Q=l(e,"DIV",{style:!0,"data-svelte-h":!0}),c(Q)!=="svelte-odbvi5"&&(Q.innerHTML=fs),cn=a(e),Re=l(e,"P",{"data-svelte-h":!0}),c(Re)!=="svelte-1uahz5n"&&(Re.innerHTML=us),mn=a(e),De=l(e,"P",{"data-svelte-h":!0}),c(De)!=="svelte-106f51x"&&(De.textContent=gs),pn=a(e),p(Oe.$$.fragment,e),hn=a(e),Ae=l(e,"P",{"data-svelte-h":!0}),c(Ae)!=="svelte-rwb7y1"&&(Ae.textContent=_s),fn=a(e),p(Qe.$$.fragment,e),un=a(e),Ke=l(e,"P",{"data-svelte-h":!0}),c(Ke)!=="svelte-1xrlue6"&&(Ke.innerHTML=bs),gn=a(e),p(et.$$.fragment,e),_n=a(e),tt=l(e,"P",{"data-svelte-h":!0}),c(tt)!=="svelte-llmun5"&&(tt.innerHTML=ks),bn=a(e),p(ot.$$.fragment,e),kn=a(e),nt=l(e,"P",{"data-svelte-h":!0}),c(nt)!=="svelte-ff55s5"&&(nt.innerHTML=vs),vn=a(e),p(at.$$.fragment,e),yn=a(e),st=l(e,"P",{"data-svelte-h":!0}),c(st)!=="svelte-sc737i"&&(st.innerHTML=ys),Tn=a(e),p(rt.$$.fragment,e),Mn=a(e),it=l(e,"P",{"data-svelte-h":!0}),c(it)!=="svelte-1bkdry6"&&(it.textContent=Ts),wn=a(e),p(lt.$$.fragment,e),$n=a(e),p(dt.$$.fragment,e),Bn=a(e),J=l(e,"DIV",{class:!0});var H=M(J);p(ct.$$.fragment,H),Qn=a(H),Vt=l(H,"P",{"data-svelte-h":!0}),c(Vt)!=="svelte-qs9ivf"&&(Vt.innerHTML=Ms),Kn=a(H),Xt=l(H,"P",{"data-svelte-h":!0}),c(Xt)!=="svelte-c6ui3q"&&(Xt.innerHTML=ws),ea=a(H),Et=l(H,"P",{"data-svelte-h":!0}),c(Et)!=="svelte-1ek1ss9"&&(Et.innerHTML=$s),ta=a(H),K=l(H,"DIV",{class:!0});var Lt=M(K);p(mt.$$.fragment,Lt),oa=a(Lt),Yt=l(Lt,"P",{"data-svelte-h":!0}),c(Yt)!=="svelte-orkuph"&&(Yt.innerHTML=Bs),Lt.forEach(o),H.forEach(o),Cn=a(e),p(pt.$$.fragment,e),xn=a(e),j=l(e,"DIV",{class:!0});var L=M(j);p(ht.$$.fragment,L),na=a(L),Rt=l(L,"P",{"data-svelte-h":!0}),c(Rt)!=="svelte-1xfrjvw"&&(Rt.textContent=Cs),aa=a(L),ee=l(L,"DIV",{class:!0});var qt=M(ee);p(ft.$$.fragment,qt),sa=a(qt),Dt=l(qt,"P",{"data-svelte-h":!0}),c(Dt)!=="svelte-1hlbbl8"&&(Dt.innerHTML=xs),qt.forEach(o),ra=a(L),te=l(L,"DIV",{class:!0});var Gt=M(te);p(ut.$$.fragment,Gt),ia=a(Gt),Ot=l(Gt,"P",{"data-svelte-h":!0}),c(Ot)!=="svelte-1xt1aup"&&(Ot.textContent=zs),Gt.forEach(o),la=a(L),oe=l(L,"DIV",{class:!0});var St=M(oe);p(gt.$$.fragment,St),da=a(St),At=l(St,"P",{"data-svelte-h":!0}),c(At)!=="svelte-zrnqah"&&(At.innerHTML=Js),St.forEach(o),L.forEach(o),zn=a(e),p(_t.$$.fragment,e),Jn=a(e),$=l(e,"DIV",{class:!0});var x=M($);p(bt.$$.fragment,x),ca=a(x),Qt=l(x,"P",{"data-svelte-h":!0}),c(Qt)!=="svelte-xp33tl"&&(Qt.textContent=js),ma=a(x),Kt=l(x,"UL",{"data-svelte-h":!0}),c(Kt)!=="svelte-35a3ng"&&(Kt.innerHTML=Is),pa=a(x),eo=l(x,"P",{"data-svelte-h":!0}),c(eo)!=="svelte-beeiv6"&&(eo.textContent=Us),ha=a(x),to=l(x,"P",{"data-svelte-h":!0}),c(to)!=="svelte-q52n56"&&(to.innerHTML=Ws),fa=a(x),oo=l(x,"P",{"data-svelte-h":!0}),c(oo)!=="svelte-hswkmf"&&(oo.innerHTML=Fs),ua=a(x),q=l(x,"DIV",{class:!0});var O=M(q);p(kt.$$.fragment,O),ga=a(O),no=l(O,"P",{"data-svelte-h":!0}),c(no)!=="svelte-4azpa"&&(no.innerHTML=Ps),_a=a(O),p(ne.$$.fragment,O),O.forEach(o),ba=a(x),ae=l(x,"DIV",{class:!0});var Rn=M(ae);p(vt.$$.fragment,Rn),ka=a(Rn),ao=l(Rn,"P",{"data-svelte-h":!0}),c(ao)!=="svelte-1k9ciky"&&(ao.textContent=Zs),Rn.forEach(o),x.forEach(o),jn=a(e),p(yt.$$.fragment,e),In=a(e),I=l(e,"DIV",{class:!0});var E=M(I);p(Tt.$$.fragment,E),va=a(E),so=l(E,"P",{"data-svelte-h":!0}),c(so)!=="svelte-1f7uoqa"&&(so.textContent=Ns),ya=a(E),ro=l(E,"P",{"data-svelte-h":!0}),c(ro)!=="svelte-q52n56"&&(ro.innerHTML=Hs),Ta=a(E),io=l(E,"P",{"data-svelte-h":!0}),c(io)!=="svelte-hswkmf"&&(io.innerHTML=Ls),Ma=a(E),G=l(E,"DIV",{class:!0});var $o=M(G);p(Mt.$$.fragment,$o),wa=a($o),lo=l($o,"P",{"data-svelte-h":!0}),c(lo)!=="svelte-1scew2t"&&(lo.innerHTML=qs),$a=a($o),p(se.$$.fragment,$o),$o.forEach(o),E.forEach(o),Un=a(e),p(wt.$$.fragment,e),Wn=a(e),U=l(e,"DIV",{class:!0});var Y=M(U);p($t.$$.fragment,Y),Ba=a(Y),co=l(Y,"P",{"data-svelte-h":!0}),c(co)!=="svelte-1mte7ik"&&(co.textContent=Gs),Ca=a(Y),mo=l(Y,"P",{"data-svelte-h":!0}),c(mo)!=="svelte-q52n56"&&(mo.innerHTML=Ss),xa=a(Y),po=l(Y,"P",{"data-svelte-h":!0}),c(po)!=="svelte-hswkmf"&&(po.innerHTML=Vs),za=a(Y),S=l(Y,"DIV",{class:!0});var Bo=M(S);p(Bt.$$.fragment,Bo),Ja=a(Bo),ho=l(Bo,"P",{"data-svelte-h":!0}),c(ho)!=="svelte-1scew2t"&&(ho.innerHTML=Xs),ja=a(Bo),p(re.$$.fragment,Bo),Bo.forEach(o),Y.forEach(o),Fn=a(e),p(Ct.$$.fragment,e),Pn=a(e),W=l(e,"DIV",{class:!0});var R=M(W);p(xt.$$.fragment,R),Ia=a(R),fo=l(R,"P",{"data-svelte-h":!0}),c(fo)!=="svelte-18nedn9"&&(fo.innerHTML=Es),Ua=a(R),uo=l(R,"P",{"data-svelte-h":!0}),c(uo)!=="svelte-q52n56"&&(uo.innerHTML=Ys),Wa=a(R),go=l(R,"P",{"data-svelte-h":!0}),c(go)!=="svelte-hswkmf"&&(go.innerHTML=Rs),Fa=a(R),V=l(R,"DIV",{class:!0});var Co=M(V);p(zt.$$.fragment,Co),Pa=a(Co),_o=l(Co,"P",{"data-svelte-h":!0}),c(_o)!=="svelte-lm41kl"&&(_o.innerHTML=Ds),Za=a(Co),p(ie.$$.fragment,Co),Co.forEach(o),R.forEach(o),Zn=a(e),p(Jt.$$.fragment,e),Nn=a(e),D=l(e,"DIV",{class:!0});var Dn=M(D);p(jt.$$.fragment,Dn),Na=a(Dn),X=l(Dn,"DIV",{class:!0});var xo=M(X);p(It.$$.fragment,xo),Ha=a(xo),bo=l(xo,"P",{"data-svelte-h":!0}),c(bo)!=="svelte-1scew2t"&&(bo.innerHTML=Os),La=a(xo),p(le.$$.fragment,xo),xo.forEach(o),Dn.forEach(o),Hn=a(e),p(Ut.$$.fragment,e),Ln=a(e),P=l(e,"DIV",{class:!0});var pe=M(P);p(Wt.$$.fragment,pe),qa=a(pe),ko=l(pe,"P",{"data-svelte-h":!0}),c(ko)!=="svelte-1rhjscb"&&(ko.innerHTML=As),Ga=a(pe),vo=l(pe,"P",{"data-svelte-h":!0}),c(vo)!=="svelte-1ek1ss9"&&(vo.innerHTML=Qs),Sa=a(pe),p(de.$$.fragment,pe),pe.forEach(o),qn=a(e),p(Ft.$$.fragment,e),Gn=a(e),Z=l(e,"DIV",{class:!0});var he=M(Z);p(Pt.$$.fragment,he),Va=a(he),yo=l(he,"P",{"data-svelte-h":!0}),c(yo)!=="svelte-qs5id7"&&(yo.innerHTML=Ks),Xa=a(he),To=l(he,"P",{"data-svelte-h":!0}),c(To)!=="svelte-1ek1ss9"&&(To.innerHTML=er),Ea=a(he),p(ce.$$.fragment,he),he.forEach(o),Sn=a(e),p(Zt.$$.fragment,e),Vn=a(e),N=l(e,"DIV",{class:!0});var fe=M(N);p(Nt.$$.fragment,fe),Ya=a(fe),Mo=l(fe,"P",{"data-svelte-h":!0}),c(Mo)!=="svelte-1q9szan"&&(Mo.innerHTML=tr),Ra=a(fe),wo=l(fe,"P",{"data-svelte-h":!0}),c(wo)!=="svelte-1ek1ss9"&&(wo.innerHTML=or),Da=a(fe),p(me.$$.fragment,fe),fe.forEach(o),Xn=a(e),p(Ht.$$.fragment,e),En=a(e),zo=l(e,"P",{}),M(zo).forEach(o),this.h()},h(){T(d,"name","hf:doc:metadata"),T(d,"content",kr),T(A,"class","flex flex-wrap space-x-1"),lr(Q,"text-align","center"),T(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){r(document.head,d),s(e,v,t),s(e,b,t),s(e,k,t),s(e,y,t),s(e,B,t),h(ge,e,t),s(e,Jo,t),s(e,A,t),s(e,jo,t),h(_e,e,t),s(e,Io,t),s(e,be,t),s(e,Uo,t),s(e,ke,t),s(e,Wo,t),s(e,ve,t),s(e,Fo,t),s(e,ye,t),s(e,Po,t),s(e,Te,t),s(e,Zo,t),h(Me,e,t),s(e,No,t),s(e,we,t),s(e,Ho,t),h($e,e,t),s(e,Lo,t),s(e,Be,t),s(e,qo,t),h(Ce,e,t),s(e,Go,t),h(xe,e,t),s(e,So,t),s(e,ze,t),s(e,Vo,t),s(e,Je,t),s(e,Xo,t),h(je,e,t),s(e,Eo,t),s(e,Ie,t),s(e,Yo,t),h(Ue,e,t),s(e,Ro,t),s(e,We,t),s(e,Do,t),h(Fe,e,t),s(e,Oo,t),s(e,Pe,t),s(e,Ao,t),h(Ze,e,t),s(e,Qo,t),s(e,Ne,t),s(e,Ko,t),h(He,e,t),s(e,en,t),s(e,Le,t),s(e,tn,t),s(e,qe,t),s(e,on,t),h(Ge,e,t),s(e,nn,t),h(Se,e,t),s(e,an,t),s(e,Ve,t),s(e,sn,t),h(Xe,e,t),s(e,rn,t),h(Ee,e,t),s(e,ln,t),s(e,Ye,t),s(e,dn,t),s(e,Q,t),s(e,cn,t),s(e,Re,t),s(e,mn,t),s(e,De,t),s(e,pn,t),h(Oe,e,t),s(e,hn,t),s(e,Ae,t),s(e,fn,t),h(Qe,e,t),s(e,un,t),s(e,Ke,t),s(e,gn,t),h(et,e,t),s(e,_n,t),s(e,tt,t),s(e,bn,t),h(ot,e,t),s(e,kn,t),s(e,nt,t),s(e,vn,t),h(at,e,t),s(e,yn,t),s(e,st,t),s(e,Tn,t),h(rt,e,t),s(e,Mn,t),s(e,it,t),s(e,wn,t),h(lt,e,t),s(e,$n,t),h(dt,e,t),s(e,Bn,t),s(e,J,t),h(ct,J,null),r(J,Qn),r(J,Vt),r(J,Kn),r(J,Xt),r(J,ea),r(J,Et),r(J,ta),r(J,K),h(mt,K,null),r(K,oa),r(K,Yt),s(e,Cn,t),h(pt,e,t),s(e,xn,t),s(e,j,t),h(ht,j,null),r(j,na),r(j,Rt),r(j,aa),r(j,ee),h(ft,ee,null),r(ee,sa),r(ee,Dt),r(j,ra),r(j,te),h(ut,te,null),r(te,ia),r(te,Ot),r(j,la),r(j,oe),h(gt,oe,null),r(oe,da),r(oe,At),s(e,zn,t),h(_t,e,t),s(e,Jn,t),s(e,$,t),h(bt,$,null),r($,ca),r($,Qt),r($,ma),r($,Kt),r($,pa),r($,eo),r($,ha),r($,to),r($,fa),r($,oo),r($,ua),r($,q),h(kt,q,null),r(q,ga),r(q,no),r(q,_a),h(ne,q,null),r($,ba),r($,ae),h(vt,ae,null),r(ae,ka),r(ae,ao),s(e,jn,t),h(yt,e,t),s(e,In,t),s(e,I,t),h(Tt,I,null),r(I,va),r(I,so),r(I,ya),r(I,ro),r(I,Ta),r(I,io),r(I,Ma),r(I,G),h(Mt,G,null),r(G,wa),r(G,lo),r(G,$a),h(se,G,null),s(e,Un,t),h(wt,e,t),s(e,Wn,t),s(e,U,t),h($t,U,null),r(U,Ba),r(U,co),r(U,Ca),r(U,mo),r(U,xa),r(U,po),r(U,za),r(U,S),h(Bt,S,null),r(S,Ja),r(S,ho),r(S,ja),h(re,S,null),s(e,Fn,t),h(Ct,e,t),s(e,Pn,t),s(e,W,t),h(xt,W,null),r(W,Ia),r(W,fo),r(W,Ua),r(W,uo),r(W,Wa),r(W,go),r(W,Fa),r(W,V),h(zt,V,null),r(V,Pa),r(V,_o),r(V,Za),h(ie,V,null),s(e,Zn,t),h(Jt,e,t),s(e,Nn,t),s(e,D,t),h(jt,D,null),r(D,Na),r(D,X),h(It,X,null),r(X,Ha),r(X,bo),r(X,La),h(le,X,null),s(e,Hn,t),h(Ut,e,t),s(e,Ln,t),s(e,P,t),h(Wt,P,null),r(P,qa),r(P,ko),r(P,Ga),r(P,vo),r(P,Sa),h(de,P,null),s(e,qn,t),h(Ft,e,t),s(e,Gn,t),s(e,Z,t),h(Pt,Z,null),r(Z,Va),r(Z,yo),r(Z,Xa),r(Z,To),r(Z,Ea),h(ce,Z,null),s(e,Sn,t),h(Zt,e,t),s(e,Vn,t),s(e,N,t),h(Nt,N,null),r(N,Ya),r(N,Mo),r(N,Ra),r(N,wo),r(N,Da),h(me,N,null),s(e,Xn,t),h(Ht,e,t),s(e,En,t),s(e,zo,t),Yn=!0},p(e,[t]){const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),ne.$set(H);const Lt={};t&2&&(Lt.$$scope={dirty:t,ctx:e}),se.$set(Lt);const L={};t&2&&(L.$$scope={dirty:t,ctx:e}),re.$set(L);const qt={};t&2&&(qt.$$scope={dirty:t,ctx:e}),ie.$set(qt);const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),le.$set(Gt);const St={};t&2&&(St.$$scope={dirty:t,ctx:e}),de.$set(St);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),ce.$set(x);const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),me.$set(O)},i(e){Yn||(f(ge.$$.fragment,e),f(_e.$$.fragment,e),f(Me.$$.fragment,e),f($e.$$.fragment,e),f(Ce.$$.fragment,e),f(xe.$$.fragment,e),f(je.$$.fragment,e),f(Ue.$$.fragment,e),f(Fe.$$.fragment,e),f(Ze.$$.fragment,e),f(He.$$.fragment,e),f(Ge.$$.fragment,e),f(Se.$$.fragment,e),f(Xe.$$.fragment,e),f(Ee.$$.fragment,e),f(Oe.$$.fragment,e),f(Qe.$$.fragment,e),f(et.$$.fragment,e),f(ot.$$.fragment,e),f(at.$$.fragment,e),f(rt.$$.fragment,e),f(lt.$$.fragment,e),f(dt.$$.fragment,e),f(ct.$$.fragment,e),f(mt.$$.fragment,e),f(pt.$$.fragment,e),f(ht.$$.fragment,e),f(ft.$$.fragment,e),f(ut.$$.fragment,e),f(gt.$$.fragment,e),f(_t.$$.fragment,e),f(bt.$$.fragment,e),f(kt.$$.fragment,e),f(ne.$$.fragment,e),f(vt.$$.fragment,e),f(yt.$$.fragment,e),f(Tt.$$.fragment,e),f(Mt.$$.fragment,e),f(se.$$.fragment,e),f(wt.$$.fragment,e),f($t.$$.fragment,e),f(Bt.$$.fragment,e),f(re.$$.fragment,e),f(Ct.$$.fragment,e),f(xt.$$.fragment,e),f(zt.$$.fragment,e),f(ie.$$.fragment,e),f(Jt.$$.fragment,e),f(jt.$$.fragment,e),f(It.$$.fragment,e),f(le.$$.fragment,e),f(Ut.$$.fragment,e),f(Wt.$$.fragment,e),f(de.$$.fragment,e),f(Ft.$$.fragment,e),f(Pt.$$.fragment,e),f(ce.$$.fragment,e),f(Zt.$$.fragment,e),f(Nt.$$.fragment,e),f(me.$$.fragment,e),f(Ht.$$.fragment,e),Yn=!0)},o(e){u(ge.$$.fragment,e),u(_e.$$.fragment,e),u(Me.$$.fragment,e),u($e.$$.fragment,e),u(Ce.$$.fragment,e),u(xe.$$.fragment,e),u(je.$$.fragment,e),u(Ue.$$.fragment,e),u(Fe.$$.fragment,e),u(Ze.$$.fragment,e),u(He.$$.fragment,e),u(Ge.$$.fragment,e),u(Se.$$.fragment,e),u(Xe.$$.fragment,e),u(Ee.$$.fragment,e),u(Oe.$$.fragment,e),u(Qe.$$.fragment,e),u(et.$$.fragment,e),u(ot.$$.fragment,e),u(at.$$.fragment,e),u(rt.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(ct.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(ht.$$.fragment,e),u(ft.$$.fragment,e),u(ut.$$.fragment,e),u(gt.$$.fragment,e),u(_t.$$.fragment,e),u(bt.$$.fragment,e),u(kt.$$.fragment,e),u(ne.$$.fragment,e),u(vt.$$.fragment,e),u(yt.$$.fragment,e),u(Tt.$$.fragment,e),u(Mt.$$.fragment,e),u(se.$$.fragment,e),u(wt.$$.fragment,e),u($t.$$.fragment,e),u(Bt.$$.fragment,e),u(re.$$.fragment,e),u(Ct.$$.fragment,e),u(xt.$$.fragment,e),u(zt.$$.fragment,e),u(ie.$$.fragment,e),u(Jt.$$.fragment,e),u(jt.$$.fragment,e),u(It.$$.fragment,e),u(le.$$.fragment,e),u(Ut.$$.fragment,e),u(Wt.$$.fragment,e),u(de.$$.fragment,e),u(Ft.$$.fragment,e),u(Pt.$$.fragment,e),u(ce.$$.fragment,e),u(Zt.$$.fragment,e),u(Nt.$$.fragment,e),u(me.$$.fragment,e),u(Ht.$$.fragment,e),Yn=!1},d(e){e&&(o(v),o(b),o(k),o(y),o(B),o(Jo),o(A),o(jo),o(Io),o(be),o(Uo),o(ke),o(Wo),o(ve),o(Fo),o(ye),o(Po),o(Te),o(Zo),o(No),o(we),o(Ho),o(Lo),o(Be),o(qo),o(Go),o(So),o(ze),o(Vo),o(Je),o(Xo),o(Eo),o(Ie),o(Yo),o(Ro),o(We),o(Do),o(Oo),o(Pe),o(Ao),o(Qo),o(Ne),o(Ko),o(en),o(Le),o(tn),o(qe),o(on),o(nn),o(an),o(Ve),o(sn),o(rn),o(ln),o(Ye),o(dn),o(Q),o(cn),o(Re),o(mn),o(De),o(pn),o(hn),o(Ae),o(fn),o(un),o(Ke),o(gn),o(_n),o(tt),o(bn),o(kn),o(nt),o(vn),o(yn),o(st),o(Tn),o(Mn),o(it),o(wn),o($n),o(Bn),o(J),o(Cn),o(xn),o(j),o(zn),o(Jn),o($),o(jn),o(In),o(I),o(Un),o(Wn),o(U),o(Fn),o(Pn),o(W),o(Zn),o(Nn),o(D),o(Hn),o(Ln),o(P),o(qn),o(Gn),o(Z),o(Sn),o(Vn),o(N),o(Xn),o(En),o(zo)),o(d),g(ge,e),g(_e,e),g(Me,e),g($e,e),g(Ce,e),g(xe,e),g(je,e),g(Ue,e),g(Fe,e),g(Ze,e),g(He,e),g(Ge,e),g(Se,e),g(Xe,e),g(Ee,e),g(Oe,e),g(Qe,e),g(et,e),g(ot,e),g(at,e),g(rt,e),g(lt,e),g(dt,e),g(ct),g(mt),g(pt,e),g(ht),g(ft),g(ut),g(gt),g(_t,e),g(bt),g(kt),g(ne),g(vt),g(yt,e),g(Tt),g(Mt),g(se),g(wt,e),g($t),g(Bt),g(re),g(Ct,e),g(xt),g(zt),g(ie),g(Jt,e),g(jt),g(It),g(le),g(Ut,e),g(Wt),g(de),g(Ft,e),g(Pt),g(ce),g(Zt,e),g(Nt),g(me),g(Ht,e)}}}const kr='{"title":"Bark","local":"bark","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Optimizing Bark","local":"optimizing-bark","sections":[{"title":"Using half-precision","local":"using-half-precision","sections":[],"depth":4},{"title":"Using CPU offload","local":"using-cpu-offload","sections":[],"depth":4},{"title":"Using Better Transformer","local":"using-better-transformer","sections":[],"depth":4},{"title":"Using Flash Attention 2","local":"using-flash-attention-2","sections":[{"title":"Installation","local":"installation","sections":[],"depth":5},{"title":"Usage","local":"usage","sections":[],"depth":5},{"title":"Performance comparison","local":"performance-comparison","sections":[],"depth":5}],"depth":4},{"title":"Combining optimization techniques","local":"combining-optimization-techniques","sections":[],"depth":4}],"depth":3},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":3}],"depth":2},{"title":"BarkConfig","local":"transformers.BarkConfig","sections":[],"depth":2},{"title":"BarkProcessor","local":"transformers.BarkProcessor","sections":[],"depth":2},{"title":"BarkModel","local":"transformers.BarkModel","sections":[],"depth":2},{"title":"BarkSemanticModel","local":"transformers.BarkSemanticModel","sections":[],"depth":2},{"title":"BarkCoarseModel","local":"transformers.BarkCoarseModel","sections":[],"depth":2},{"title":"BarkFineModel","local":"transformers.BarkFineModel","sections":[],"depth":2},{"title":"BarkCausalModel","local":"transformers.BarkCausalModel","sections":[],"depth":2},{"title":"BarkCoarseConfig","local":"transformers.BarkCoarseConfig","sections":[],"depth":2},{"title":"BarkFineConfig","local":"transformers.BarkFineConfig","sections":[],"depth":2},{"title":"BarkSemanticConfig","local":"transformers.BarkSemanticConfig","sections":[],"depth":2}],"depth":1}';function vr(z){return ar(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xr extends sr{constructor(d){super(),rr(this,d,vr,br,nr,{})}}export{xr as component};
