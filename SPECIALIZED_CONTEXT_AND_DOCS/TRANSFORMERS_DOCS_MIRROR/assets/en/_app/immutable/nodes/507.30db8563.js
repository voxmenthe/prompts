import{s as bt,o as At,n as ft}from"../chunks/scheduler.18a86fab.js";import{S as gt,i as _t,g as d,s as o,r as m,A as Zt,h as y,f as l,c as r,j as jt,u as w,x as T,k as It,y as Bt,a as s,v as c,d as U,t as u,w as j}from"../chunks/index.98837b22.js";import{C as V}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ht,E as vt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Et,a as Ct}from"../chunks/HfOption.6641485e.js";function Wt(h){let a,i;return a=new V({props:{code:"cGlwJTIwaW5zdGFsbCUyMGNvbXByZXNzZWQtdGVuc29ycw==",highlighted:"pip install compressed-tensors",wrap:!1}}),{c(){m(a.$$.fragment)},l(M){w(a.$$.fragment,M)},m(M,p){c(a,M,p),i=!0},p:ft,i(M){i||(U(a.$$.fragment,M),i=!0)},o(M){u(a.$$.fragment,M),i=!1},d(M){j(a,M)}}}function Xt(h){let a,i;return a=new V({props:{code:"Z2l0JTIwY2xvbmUlMjBodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZuZXVyYWxtYWdpYyUyRmNvbXByZXNzZWQtdGVuc29ycyUwQWNkJTIwY29tcHJlc3NlZC10ZW5zb3JzJTBBcGlwJTIwaW5zdGFsbCUyMC1lJTIwLg==",highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/neuralmagic/compressed-tensors
<span class="hljs-built_in">cd</span> compressed-tensors
pip install -e .`,wrap:!1}}),{c(){m(a.$$.fragment)},l(M){w(a.$$.fragment,M)},m(M,p){c(a,M,p),i=!0},p:ft,i(M){i||(U(a.$$.fragment,M),i=!0)},o(M){u(a.$$.fragment,M),i=!1},d(M){j(a,M)}}}function Nt(h){let a,i,M,p;return a=new Ct({props:{id:"install",option:"PyPI",$$slots:{default:[Wt]},$$scope:{ctx:h}}}),M=new Ct({props:{id:"install",option:"source code",$$slots:{default:[Xt]},$$scope:{ctx:h}}}),{c(){m(a.$$.fragment),i=o(),m(M.$$.fragment)},l(n){w(a.$$.fragment,n),i=r(n),w(M.$$.fragment,n)},m(n,J){c(a,n,J),s(n,i,J),c(M,n,J),p=!0},p(n,J){const I={};J&2&&(I.$$scope={dirty:J,ctx:n}),a.$set(I);const k={};J&2&&(k.$$scope={dirty:J,ctx:n}),M.$set(k)},i(n){p||(U(a.$$.fragment,n),U(M.$$.fragment,n),p=!0)},o(n){u(a.$$.fragment,n),u(M.$$.fragment,n),p=!1},d(n){n&&l(i),j(a,n),j(M,n)}}}function Ft(h){let a,i,M,p,n,J,I,k='<a href="https://github.com/neuralmagic/compressed-tensors" rel="nofollow">compressed-tensors</a> extends <a href="https://github.com/huggingface/safetensors" rel="nofollow">safetensors</a> files to compressed tensor data types to provide a unified checkpoint format for storing and loading various quantization and sparsity formats such dense, int-quantized (int8), float-quantized (fp8), and pack-quantized (int4 or int8 weight-quantized packed into int32).',q,f,rt='compressed-tensors supports fine-tuning with <a href="https://huggingface.co/docs/peft" rel="nofollow">PEFT</a> and includes the following features as well.',Y,b,it='<li>fp8, int4, int8 weight and activation precisions.</li> <li>Quantization scales and zero-points strategies for <a href="https://github.com/neuralmagic/compressed-tensors/blob/83b2e7a969d70606421a76b9a3d112646077c8de/src/compressed_tensors/quantization/quant_args.py#L43-L52" rel="nofollow">tensor, channel, group, block, token</a>.</li> <li>Dynamic per-token activation quantization (or any static strategy).</li> <li>Weight sparsity (unstructured or semi-structured like 2:4) can be composed with quantization for extreme compression.</li> <li>Quantization of arbitrary modules, not just <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="nofollow">nn.Linear</a> modules.</li> <li>Targeted support for specific modules by name or class.</li>',G,A,dt='Install compressed-tensors from <a href="https://pypi.org/project/compressed-tensors" rel="nofollow">PyPI</a> to get the latest stable release (recommended) or install it from source to get the latest features.',R,C,L,g,yt='Search using the compressed-tensors <a href="https://huggingface.co/models?other=compressed-tensors" rel="nofollow">tag</a> to find a compatible model on the Hugging Face Hub.',x,_,pt='Only models that have already been quantized can be loaded at the moment, and once a model is loaded, it cannot be saved. To quantize a model into the compressed-tensors format, see <a href="https://github.com/vllm-project/llm-compressor" rel="nofollow">llm-compressor</a>. Alternatively, models can be created independently and serizlied with a compressed-tensors config.',H,Z,K,B,D,v,Jt='compressed-tensor models are defined through its configuration entry. The following example is taken from the <a href="https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json" rel="nofollow">nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf</a> <code>config.json</code> file.',P,E,Tt="There are a lot of entries to allow for flexible expression both during and after compression, but the entries for loading and inference can be simplified to focus on just a few key entries.",O,W,tt,X,mt="The config file specifies the quantization of a config group (<code>group_0</code>), which includes weight and activation quantization to fp8 with a static per-tensor strategy. The <code>lm_head</code> module is unquantized as shown in the <code>ignore</code> key.",et,N,wt='For a more detailed look at the model weights, use the <a href="https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf?show_file_info=model.safetensors.index.json" rel="nofollow">safetensors viewer</a> on the model card to see the quantized weights, input scale, and weight scale for all <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="nofollow">nn.Linear</a> modules.',lt,F,ct="<thead><tr><th>Tensors</th> <th>Shape</th> <th>Precision</th></tr></thead> <tbody><tr><td>model.layers.0.input_layernorm.weight</td> <td>[4 096]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.down_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.down_proj.weight</td> <td>[4 096, 14 336]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.mlp.down_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.gate_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.gate_proj.weight</td> <td>[14 336, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.mlp.gate_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.up_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.mlp.up_proj.weight</td> <td>[14 336, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.mlp.up_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.post_attention_layernorm.weight</td> <td>[4 096]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.k_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.k_proj.weight</td> <td>[1 024, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.self_attn.k_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.o_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.o_proj.weight</td> <td>[4 096, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.self_attn.o_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.q_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.q_proj.weight</td> <td>[4 096, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.self_attn.q_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.v_proj.input_scale</td> <td>[1]</td> <td>BF16</td></tr> <tr><td>model.layers.0.self_attn.v_proj.weight</td> <td>[1 024, 4 096]</td> <td>F8_E4M3</td></tr> <tr><td>model.layers.0.self_attn.v_proj.weight_scale</td> <td>[1]</td> <td>BF16</td></tr></tbody>",st,$,Ut='When loading a compressed-tensors model with the <code>~quantizers.HFQuantizer</code> integration, all the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="nofollow">nn.Linear</a> modules specified in the quantization config are replaced by <a href="https://github.com/neuralmagic/compressed-tensors/blob/975cb223b19fcac2b98a4271d17668462d4d6e1d/src/compressed_tensors/linear/compressed_linear.py#L30" rel="nofollow">CompressedLinear</a> modules that manage the compressed weights and forward pass for inference. The <code>lm_head</code> module is still kept as an unquantized nn.Linear module.',at,z,Mt,Q,nt,S,ot;return n=new ht({props:{title:"compressed-tensors",local:"compressed-tensors",headingTag:"h1"}}),C=new Et({props:{id:"install",options:["PyPI","source code"],$$slots:{default:[Nt]},$$scope:{ctx:h}}}),Z=new V({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBY3RfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybm0tdGVzdGluZyUyRk1ldGEtTGxhbWEtMy4xLThCLUluc3RydWN0LUZQOC1oZiUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyKSUwQSUwQSUyMyUyMG1lYXN1cmUlMjBtZW1vcnklMjB1c2FnZSUwQW1lbV9wYXJhbXMlMjAlM0QlMjBzdW0oJTVCcGFyYW0ubmVsZW1lbnQoKSpwYXJhbS5lbGVtZW50X3NpemUoKSUyMGZvciUyMHBhcmFtJTIwaW4lMjBjdF9tb2RlbC5wYXJhbWV0ZXJzKCklNUQpJTBBcHJpbnQoZiUyMiU3Qm1lbV9wYXJhbXMlMkYyKiozMCUzQS40ZiU3RCUyMEdCJTIyKSUwQSUyMyUyMDguNDU3NSUyMEdC",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

ct_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

<span class="hljs-comment"># measure memory usage</span>
mem_params = <span class="hljs-built_in">sum</span>([param.nelement()*param.element_size() <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> ct_model.parameters()])
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{mem_params/<span class="hljs-number">2</span>**<span class="hljs-number">30</span>:<span class="hljs-number">.4</span>f}</span> GB&quot;</span>)
<span class="hljs-comment"># 8.4575 GB</span>`,wrap:!1}}),B=new ht({props:{title:"Model checkpoint",local:"model-checkpoint",headingTag:"h2"}}),W=new V({props:{code:"JTIycXVhbnRpemF0aW9uX2NvbmZpZyUyMiUzQSUyMCU3QiUwQSUyMCUyMCUyMmNvbmZpZ19ncm91cHMlMjIlM0ElMjAlN0IlMEElMjAlMjAlMjAlMjAlMjJncm91cF8wJTIyJTNBJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIyaW5wdXRfYWN0aXZhdGlvbnMlMjIlM0ElMjAlN0IlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJudW1fYml0cyUyMiUzQSUyMDglMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJzdHJhdGVneSUyMiUzQSUyMCUyMnRlbnNvciUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMnR5cGUlMjIlM0ElMjAlMjJmbG9hdCUyMiUwQSUyMCUyMCUyMCUyMCUyMCUyMCU3RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMnRhcmdldHMlMjIlM0ElMjAlNUIlMjJMaW5lYXIlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjJ3ZWlnaHRzJTIyJTNBJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIybnVtX2JpdHMlMjIlM0ElMjA4JTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyc3RyYXRlZ3klMjIlM0ElMjAlMjJ0ZW5zb3IlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJ0eXBlJTIyJTNBJTIwJTIyZmxvYXQlMjIlMEElMjAlMjAlMjAlMjAlMjAlMjAlN0QlMEElMjAlMjAlMjAlMjAlN0QlMEElMjAlMjAlN0QlMkMlMEElMjAlMjAlMjJmb3JtYXQlMjIlM0ElMjAlMjJuYWl2ZS1xdWFudGl6ZWQlMjIlMkMlMEElMjAlMjAlMjJpZ25vcmUlMjIlM0ElMjAlNUIlMjJsbV9oZWFkJTIyJTVEJTJDJTBBJTIwJTIwJTIycXVhbnRfbWV0aG9kJTIyJTNBJTIwJTIyY29tcHJlc3NlZC10ZW5zb3JzJTIyJTJDJTBBJTIwJTIwJTIycXVhbnRpemF0aW9uX3N0YXR1cyUyMiUzQSUyMCUyMmZyb3plbiUyMiUwQSU3RCUyQw==",highlighted:`<span class="hljs-attr">&quot;quantization_config&quot;:</span> {
  <span class="hljs-attr">&quot;config_groups&quot;:</span> {
    <span class="hljs-attr">&quot;group_0&quot;:</span> {
      <span class="hljs-attr">&quot;input_activations&quot;:</span> {
        <span class="hljs-attr">&quot;num_bits&quot;:</span> <span class="hljs-number">8</span>,
        <span class="hljs-attr">&quot;strategy&quot;:</span> <span class="hljs-string">&quot;tensor&quot;</span>,
        <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;float&quot;</span>
      },
      <span class="hljs-attr">&quot;targets&quot;:</span> [<span class="hljs-string">&quot;Linear&quot;</span>],
      <span class="hljs-attr">&quot;weights&quot;:</span> {
        <span class="hljs-attr">&quot;num_bits&quot;:</span> <span class="hljs-number">8</span>,
        <span class="hljs-attr">&quot;strategy&quot;:</span> <span class="hljs-string">&quot;tensor&quot;</span>,
        <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;float&quot;</span>
      }
    }
  },
  <span class="hljs-attr">&quot;format&quot;:</span> <span class="hljs-string">&quot;naive-quantized&quot;</span>,
  <span class="hljs-attr">&quot;ignore&quot;:</span> [<span class="hljs-string">&quot;lm_head&quot;</span>],
  <span class="hljs-attr">&quot;quant_method&quot;:</span> <span class="hljs-string">&quot;compressed-tensors&quot;</span>,
  <span class="hljs-attr">&quot;quantization_status&quot;:</span> <span class="hljs-string">&quot;frozen&quot;</span>
}<span class="hljs-string">,</span>`,wrap:!1}}),z=new V({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

ct_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf&quot;</span>)
<span class="hljs-built_in">print</span>(ct_model)
<span class="hljs-string">&quot;&quot;&quot;
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): CompressedLinear(
            in_features=4096, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (k_proj): CompressedLinear(
            in_features=4096, out_features=1024, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (v_proj): CompressedLinear(
            in_features=4096, out_features=1024, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (o_proj): CompressedLinear(
            in_features=4096, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): CompressedLinear(
            in_features=4096, out_features=14336, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (up_proj): CompressedLinear(
            in_features=4096, out_features=14336, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (down_proj): CompressedLinear(
            in_features=14336, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
&quot;&quot;&quot;</span>`,wrap:!1}}),Q=new vt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/compressed_tensors.md"}}),{c(){a=d("meta"),i=o(),M=d("p"),p=o(),m(n.$$.fragment),J=o(),I=d("p"),I.innerHTML=k,q=o(),f=d("p"),f.innerHTML=rt,Y=o(),b=d("ul"),b.innerHTML=it,G=o(),A=d("p"),A.innerHTML=dt,R=o(),m(C.$$.fragment),L=o(),g=d("p"),g.innerHTML=yt,x=o(),_=d("p"),_.innerHTML=pt,H=o(),m(Z.$$.fragment),K=o(),m(B.$$.fragment),D=o(),v=d("p"),v.innerHTML=Jt,P=o(),E=d("p"),E.textContent=Tt,O=o(),m(W.$$.fragment),tt=o(),X=d("p"),X.innerHTML=mt,et=o(),N=d("p"),N.innerHTML=wt,lt=o(),F=d("table"),F.innerHTML=ct,st=o(),$=d("p"),$.innerHTML=Ut,at=o(),m(z.$$.fragment),Mt=o(),m(Q.$$.fragment),nt=o(),S=d("p"),this.h()},l(t){const e=Zt("svelte-u9bgzb",document.head);a=y(e,"META",{name:!0,content:!0}),e.forEach(l),i=r(t),M=y(t,"P",{}),jt(M).forEach(l),p=r(t),w(n.$$.fragment,t),J=r(t),I=y(t,"P",{"data-svelte-h":!0}),T(I)!=="svelte-1625r46"&&(I.innerHTML=k),q=r(t),f=y(t,"P",{"data-svelte-h":!0}),T(f)!=="svelte-haujfi"&&(f.innerHTML=rt),Y=r(t),b=y(t,"UL",{"data-svelte-h":!0}),T(b)!=="svelte-1usmb9k"&&(b.innerHTML=it),G=r(t),A=y(t,"P",{"data-svelte-h":!0}),T(A)!=="svelte-zg8sal"&&(A.innerHTML=dt),R=r(t),w(C.$$.fragment,t),L=r(t),g=y(t,"P",{"data-svelte-h":!0}),T(g)!=="svelte-o4tfur"&&(g.innerHTML=yt),x=r(t),_=y(t,"P",{"data-svelte-h":!0}),T(_)!=="svelte-97790q"&&(_.innerHTML=pt),H=r(t),w(Z.$$.fragment,t),K=r(t),w(B.$$.fragment,t),D=r(t),v=y(t,"P",{"data-svelte-h":!0}),T(v)!=="svelte-361m6"&&(v.innerHTML=Jt),P=r(t),E=y(t,"P",{"data-svelte-h":!0}),T(E)!=="svelte-fipnho"&&(E.textContent=Tt),O=r(t),w(W.$$.fragment,t),tt=r(t),X=y(t,"P",{"data-svelte-h":!0}),T(X)!=="svelte-67wddu"&&(X.innerHTML=mt),et=r(t),N=y(t,"P",{"data-svelte-h":!0}),T(N)!=="svelte-xjewi7"&&(N.innerHTML=wt),lt=r(t),F=y(t,"TABLE",{"data-svelte-h":!0}),T(F)!=="svelte-1ol90rr"&&(F.innerHTML=ct),st=r(t),$=y(t,"P",{"data-svelte-h":!0}),T($)!=="svelte-9js76q"&&($.innerHTML=Ut),at=r(t),w(z.$$.fragment,t),Mt=r(t),w(Q.$$.fragment,t),nt=r(t),S=y(t,"P",{}),jt(S).forEach(l),this.h()},h(){It(a,"name","hf:doc:metadata"),It(a,"content",$t)},m(t,e){Bt(document.head,a),s(t,i,e),s(t,M,e),s(t,p,e),c(n,t,e),s(t,J,e),s(t,I,e),s(t,q,e),s(t,f,e),s(t,Y,e),s(t,b,e),s(t,G,e),s(t,A,e),s(t,R,e),c(C,t,e),s(t,L,e),s(t,g,e),s(t,x,e),s(t,_,e),s(t,H,e),c(Z,t,e),s(t,K,e),c(B,t,e),s(t,D,e),s(t,v,e),s(t,P,e),s(t,E,e),s(t,O,e),c(W,t,e),s(t,tt,e),s(t,X,e),s(t,et,e),s(t,N,e),s(t,lt,e),s(t,F,e),s(t,st,e),s(t,$,e),s(t,at,e),c(z,t,e),s(t,Mt,e),c(Q,t,e),s(t,nt,e),s(t,S,e),ot=!0},p(t,[e]){const ut={};e&2&&(ut.$$scope={dirty:e,ctx:t}),C.$set(ut)},i(t){ot||(U(n.$$.fragment,t),U(C.$$.fragment,t),U(Z.$$.fragment,t),U(B.$$.fragment,t),U(W.$$.fragment,t),U(z.$$.fragment,t),U(Q.$$.fragment,t),ot=!0)},o(t){u(n.$$.fragment,t),u(C.$$.fragment,t),u(Z.$$.fragment,t),u(B.$$.fragment,t),u(W.$$.fragment,t),u(z.$$.fragment,t),u(Q.$$.fragment,t),ot=!1},d(t){t&&(l(i),l(M),l(p),l(J),l(I),l(q),l(f),l(Y),l(b),l(G),l(A),l(R),l(L),l(g),l(x),l(_),l(H),l(K),l(D),l(v),l(P),l(E),l(O),l(tt),l(X),l(et),l(N),l(lt),l(F),l(st),l($),l(at),l(Mt),l(nt),l(S)),l(a),j(n,t),j(C,t),j(Z,t),j(B,t),j(W,t),j(z,t),j(Q,t)}}}const $t='{"title":"compressed-tensors","local":"compressed-tensors","sections":[{"title":"Model checkpoint","local":"model-checkpoint","sections":[],"depth":2}],"depth":1}';function zt(h){return At(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yt extends gt{constructor(a){super(),_t(this,a,zt,Ft,bt,{})}}export{Yt as component};
