import{s as Wt,o as vt,n as $e}from"../chunks/scheduler.18a86fab.js";import{S as Ct,i as Jt,g as c,s as n,r as g,A as It,h as d,f as a,c as r,j as B,u as f,x as y,k as x,y as i,a as l,v as u,d as h,t as b,w as M}from"../chunks/index.98837b22.js";import{T as pt}from"../chunks/Tip.77304350.js";import{D as ce}from"../chunks/Docstring.a1ef7999.js";import{C as He}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as it}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as kt}from"../chunks/PipelineTag.7749150e.js";import{H as de,E as Ut}from"../chunks/getInferenceSnippets.06c2775f.js";function xt(W){let s,_='For a more detailed overview please read the <a href="https://huggingface.co/blog/timm-transformers" rel="nofollow">official blog post</a> on the timm integration.';return{c(){s=c("p"),s.innerHTML=_},l(m){s=d(m,"P",{"data-svelte-h":!0}),y(s)!=="svelte-f92w4d"&&(s.innerHTML=_)},m(m,p){l(m,s,p)},p:$e,d(m){m&&a(s)}}}function Zt(W){let s,_="Example:",m,p,w;return p=new He({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRpbW1XcmFwcGVyTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwdGltbSUyMG1vZGVsJTBBbW9kZWwlMjAlM0QlMjBUaW1tV3JhcHBlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ0aW1tJTJGcmVzbmV0MTguYTFfaW4xayUyMiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TimmWrapperModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a timm model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TimmWrapperModel.from_pretrained(<span class="hljs-string">&quot;timm/resnet18.a1_in1k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=c("p"),s.textContent=_,m=n(),g(p.$$.fragment)},l(o){s=d(o,"P",{"data-svelte-h":!0}),y(s)!=="svelte-11lpom8"&&(s.textContent=_),m=r(o),f(p.$$.fragment,o)},m(o,T){l(o,s,T),l(o,m,T),u(p,o,T),w=!0},p:$e,i(o){w||(h(p.$$.fragment,o),w=!0)},o(o){b(p.$$.fragment,o),w=!1},d(o){o&&(a(s),a(m)),M(p,o)}}}function Ft(W){let s,_=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=c("p"),s.innerHTML=_},l(m){s=d(m,"P",{"data-svelte-h":!0}),y(s)!=="svelte-fincs2"&&(s.innerHTML=_)},m(m,p){l(m,s,p)},p:$e,d(m){m&&a(s)}}}function Et(W){let s,_="Examples:",m,p,w;return p=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFmcm9tJTIwdXJsbGliLnJlcXVlc3QlMjBpbXBvcnQlMjB1cmxvcGVuJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUyQyUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQSUyMyUyMExvYWQlMjBpbWFnZSUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3Blbih1cmxvcGVuKCUwQSUyMCUyMCUyMCUyMCdodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZiZWlnbmV0cy10YXNrLWd1aWRlLnBuZyclMEEpKSUwQSUwQSUyMyUyMExvYWQlMjBtb2RlbCUyMGFuZCUyMGltYWdlJTIwcHJvY2Vzc29yJTBBY2hlY2twb2ludCUyMCUzRCUyMCUyMnRpbW0lMkZyZXNuZXQ1MC5hMV9pbjFrJTIyJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KS5ldmFsKCklMEElMEElMjMlMjBQcmVwcm9jZXNzJTIwaW1hZ2UlMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UpJTBBJTBBJTIzJTIwRm9yd2FyZCUyMHBhc3MlMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQSUyMyUyMEdldCUyMHBvb2xlZCUyMG91dHB1dCUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMEElMEElMjMlMjBHZXQlMjBsYXN0JTIwaGlkZGVuJTIwc3RhdGUlMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(urlopen(
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#x27;</span>
<span class="hljs-meta">... </span>))

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load model and image processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;timm/resnet50.a1_in1k&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(checkpoint).<span class="hljs-built_in">eval</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Preprocess image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get pooled output</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get last hidden state</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state`,wrap:!1}}),{c(){s=c("p"),s.textContent=_,m=n(),g(p.$$.fragment)},l(o){s=d(o,"P",{"data-svelte-h":!0}),y(s)!=="svelte-kvfsh7"&&(s.textContent=_),m=r(o),f(p.$$.fragment,o)},m(o,T){l(o,s,T),l(o,m,T),u(p,o,T),w=!0},p:$e,i(o){w||(h(p.$$.fragment,o),w=!0)},o(o){b(p.$$.fragment,o),w=!1},d(o){o&&(a(s),a(m)),M(p,o)}}}function Bt(W){let s,_=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=c("p"),s.innerHTML=_},l(m){s=d(m,"P",{"data-svelte-h":!0}),y(s)!=="svelte-fincs2"&&(s.innerHTML=_)},m(m,p){l(m,s,p)},p:$e,d(m){m&&a(s)}}}function Gt(W){let s,_="Examples:",m,p,w;return p=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFmcm9tJTIwdXJsbGliLnJlcXVlc3QlMjBpbXBvcnQlMjB1cmxvcGVuJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMkMlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMEElMEElMjMlMjBMb2FkJTIwaW1hZ2UlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4odXJsb3BlbiglMEElMjAlMjAlMjAlMjAnaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGYmVpZ25ldHMtdGFzay1ndWlkZS5wbmcnJTBBKSklMEElMEElMjMlMjBMb2FkJTIwbW9kZWwlMjBhbmQlMjBpbWFnZSUyMHByb2Nlc3NvciUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJ0aW1tJTJGcmVzbmV0NTAuYTFfaW4xayUyMiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpLmV2YWwoKSUwQSUwQSUyMyUyMFByZXByb2Nlc3MlMjBpbWFnZSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSklMEElMEElMjMlMjBGb3J3YXJkJTIwcGFzcyUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwR2V0JTIwdG9wJTIwNSUyMHByZWRpY3Rpb25zJTBBdG9wNV9wcm9iYWJpbGl0aWVzJTJDJTIwdG9wNV9jbGFzc19pbmRpY2VzJTIwJTNEJTIwdG9yY2gudG9wayhsb2dpdHMuc29mdG1heChkaW0lM0QxKSUyMColMjAxMDAlMkMlMjBrJTNENSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(urlopen(
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#x27;</span>
<span class="hljs-meta">... </span>))

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load model and image processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;timm/resnet50.a1_in1k&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(checkpoint).<span class="hljs-built_in">eval</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Preprocess image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get top 5 predictions</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=<span class="hljs-number">1</span>) * <span class="hljs-number">100</span>, k=<span class="hljs-number">5</span>)`,wrap:!1}}),{c(){s=c("p"),s.textContent=_,m=n(),g(p.$$.fragment)},l(o){s=d(o,"P",{"data-svelte-h":!0}),y(s)!=="svelte-kvfsh7"&&(s.textContent=_),m=r(o),f(p.$$.fragment,o)},m(o,T){l(o,s,T),l(o,m,T),u(p,o,T),w=!0},p:$e,i(o){w||(h(p.$$.fragment,o),w=!0)},o(o){b(p.$$.fragment,o),w=!1},d(o){o&&(a(s),a(m)),M(p,o)}}}function Nt(W){let s,_,m,p,w,o,T,ct='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',We,Q,ve,X,dt="Helper class to enable loading timm models to be used with the transformers library and its autoclasses.",Ce,S,Je,L,Ie,A,gt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with TimmWrapper.",ke,q,Ue,D,ft='<li><a href="https://github.com/ariG23498/timm-wrapper-examples" rel="nofollow">Collection of Example Notebook</a> ðŸŒŽ</li>',xe,G,Ze,O,Fe,$,K,Qe,ge,ut="This is the configuration class to store the configuration for a timm backbone <code>TimmWrapper</code>.",Xe,fe,ht="It is used to instantiate a timm model according to the specified arguments, defining the model.",Se,ue,bt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Le,he,Mt=`Config loads imagenet label descriptions and stores them in <code>id2label</code> attribute, <code>label2id</code> attribute for default
imagenet models is set to <code>None</code> due to occlusions in the label descriptions.`,Ae,N,Ee,ee,Be,J,te,qe,be,_t="Wrapper class for timm models to be used within transformers.",De,V,se,Oe,Me,wt="Preprocess an image or batch of images.",Ge,ae,Ne,I,oe,Ke,_e,Tt="Wrapper class for timm models to be used in transformers.",et,v,ne,tt,we,yt='The <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperModel">TimmWrapperModel</a> forward method, overrides the <code>__call__</code> special method.',st,R,at,z,Ve,re,Re,k,le,ot,Te,$t="Wrapper class for timm models to be used in transformers for image classification.",nt,C,me,rt,ye,jt='The <a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperForImageClassification">TimmWrapperForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',lt,Y,mt,P,ze,pe,Ye,je,Pe;return w=new de({props:{title:"TimmWrapper",local:"timmwrapper",headingTag:"h1"}}),Q=new de({props:{title:"Overview",local:"overview",headingTag:"h2"}}),S=new He({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFmcm9tJTIwdXJsbGliLnJlcXVlc3QlMjBpbXBvcnQlMjB1cmxvcGVuJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMkMlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMEElMEElMjMlMjBMb2FkJTIwaW1hZ2UlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4odXJsb3BlbiglMEElMjAlMjAlMjAlMjAnaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGYmVpZ25ldHMtdGFzay1ndWlkZS5wbmcnJTBBKSklMEElMEElMjMlMjBMb2FkJTIwbW9kZWwlMjBhbmQlMjBpbWFnZSUyMHByb2Nlc3NvciUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJ0aW1tJTJGcmVzbmV0NTAuYTFfaW4xayUyMiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpLmV2YWwoKSUwQSUwQSUyMyUyMFByZXByb2Nlc3MlMjBpbWFnZSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSklMEElMEElMjMlMjBGb3J3YXJkJTIwcGFzcyUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwR2V0JTIwdG9wJTIwNSUyMHByZWRpY3Rpb25zJTBBdG9wNV9wcm9iYWJpbGl0aWVzJTJDJTIwdG9wNV9jbGFzc19pbmRpY2VzJTIwJTNEJTIwdG9yY2gudG9wayhsb2dpdHMuc29mdG1heChkaW0lM0QxKSUyMColMjAxMDAlMkMlMjBrJTNENSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(urlopen(
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png&#x27;</span>
<span class="hljs-meta">... </span>))

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load model and image processor</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;timm/resnet50.a1_in1k&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(checkpoint).<span class="hljs-built_in">eval</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Preprocess image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get top 5 predictions</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=<span class="hljs-number">1</span>) * <span class="hljs-number">100</span>, k=<span class="hljs-number">5</span>)`,wrap:!1}}),L=new de({props:{title:"Resources:",local:"resources",headingTag:"h2"}}),q=new kt({props:{pipeline:"image-classification"}}),G=new pt({props:{warning:!1,$$slots:{default:[xt]},$$scope:{ctx:W}}}),O=new de({props:{title:"TimmWrapperConfig",local:"transformers.TimmWrapperConfig",headingTag:"h2"}}),K=new ce({props:{name:"class transformers.TimmWrapperConfig",anchor:"transformers.TimmWrapperConfig",parameters:[{name:"initializer_range",val:": float = 0.02"},{name:"do_pooling",val:": bool = True"},{name:"model_args",val:": typing.Optional[dict[str, typing.Any]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TimmWrapperConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.TimmWrapperConfig.do_pooling",description:`<strong>do_pooling</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do pooling for the last_hidden_state in <code>TimmWrapperModel</code> or not.`,name:"do_pooling"},{anchor:"transformers.TimmWrapperConfig.model_args",description:`<strong>model_args</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional keyword arguments to pass to the <code>timm.create_model</code> function. e.g. <code>model_args={&quot;depth&quot;: 3}</code>
for <code>timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k</code> to create a model with 3 blocks. Defaults to <code>None</code>.`,name:"model_args"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/configuration_timm_wrapper.py#L31"}}),N=new it({props:{anchor:"transformers.TimmWrapperConfig.example",$$slots:{default:[Zt]},$$scope:{ctx:W}}}),ee=new de({props:{title:"TimmWrapperImageProcessor",local:"transformers.TimmWrapperImageProcessor",headingTag:"h2"}}),te=new ce({props:{name:"class transformers.TimmWrapperImageProcessor",anchor:"transformers.TimmWrapperImageProcessor",parameters:[{name:"pretrained_cfg",val:": dict"},{name:"architecture",val:": typing.Optional[str] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TimmWrapperImageProcessor.pretrained_cfg",description:`<strong>pretrained_cfg</strong> (<code>dict[str, Any]</code>) &#x2014;
The configuration of the pretrained model used to resolve evaluation and
training transforms.`,name:"pretrained_cfg"},{anchor:"transformers.TimmWrapperImageProcessor.architecture",description:`<strong>architecture</strong> (<code>Optional[str]</code>, <em>optional</em>) &#x2014;
Name of the architecture of the model.`,name:"architecture"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py#L39"}}),se=new ce({props:{name:"preprocess",anchor:"transformers.TimmWrapperImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = 'pt'"}],parametersDescription:[{anchor:"transformers.TimmWrapperImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images`,name:"images"},{anchor:"transformers.TimmWrapperImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/image_processing_timm_wrapper.py#L97"}}),ae=new de({props:{title:"TimmWrapperModel",local:"transformers.TimmWrapperModel",headingTag:"h2"}}),oe=new ce({props:{name:"class transformers.TimmWrapperModel",anchor:"transformers.TimmWrapperModel",parameters:[{name:"config",val:": TimmWrapperConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/modeling_timm_wrapper.py#L133"}}),ne=new ce({props:{name:"forward",anchor:"transformers.TimmWrapperModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Union[bool, list[int], NoneType] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"do_pooling",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TimmWrapperModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor">TimmWrapperImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">TimmWrapperImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor">TimmWrapperImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.TimmWrapperModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. Not compatible with timm wrapped models.`,name:"output_attentions"},{anchor:"transformers.TimmWrapperModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. Not compatible with timm wrapped models.`,name:"output_hidden_states"},{anchor:"transformers.TimmWrapperModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TimmWrapperModel.forward.do_pooling",description:`<strong>do_pooling</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to do pooling for the last_hidden_state in <code>TimmWrapperModel</code> or not. If <code>None</code> is passed, the
<code>do_pooling</code> value from the config is used.`,name:"do_pooling"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/modeling_timm_wrapper.py#L145",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.timm_wrapper.modeling_timm_wrapper.TimmWrapperModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig"
>TimmWrapperConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>&lt;class 'torch.FloatTensor'&gt;.last_hidden_state</code>) â€” The last hidden state of the model, output before applying the classification head.</li>
<li><strong>pooler_output</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) â€” The pooled output derived from the last hidden state, if applicable.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned if <code>output_hidden_states=True</code> is set or if <code>config.output_hidden_states=True</code>) â€” A tuple containing the intermediate hidden states of the model at the output of each layer or specified layers.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned if <code>output_attentions=True</code> is set or if <code>config.output_attentions=True</code>.) â€” A tuple containing the intermediate attention weights of the model at the output of each layer.
Note: Currently, Timm models do not support attentions output.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.timm_wrapper.modeling_timm_wrapper.TimmWrapperModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new pt({props:{$$slots:{default:[Ft]},$$scope:{ctx:W}}}),z=new it({props:{anchor:"transformers.TimmWrapperModel.forward.example",$$slots:{default:[Et]},$$scope:{ctx:W}}}),re=new de({props:{title:"TimmWrapperForImageClassification",local:"transformers.TimmWrapperForImageClassification",headingTag:"h2"}}),le=new ce({props:{name:"class transformers.TimmWrapperForImageClassification",anchor:"transformers.TimmWrapperForImageClassification",parameters:[{name:"config",val:": TimmWrapperConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/modeling_timm_wrapper.py#L242"}}),me=new ce({props:{name:"forward",anchor:"transformers.TimmWrapperForImageClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Union[bool, list[int], NoneType] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TimmWrapperForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor">TimmWrapperImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">TimmWrapperImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperImageProcessor">TimmWrapperImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.TimmWrapperForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.TimmWrapperForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. Not compatible with timm wrapped models.`,name:"output_attentions"},{anchor:"transformers.TimmWrapperForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. Not compatible with timm wrapped models.`,name:"output_hidden_states"},{anchor:"transformers.TimmWrapperForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.
**kwargs:
Additional keyword arguments passed along to the <code>timm</code> model forward.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/timm_wrapper/modeling_timm_wrapper.py#L264",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/timm_wrapper#transformers.TimmWrapperConfig"
>TimmWrapperConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Y=new pt({props:{$$slots:{default:[Bt]},$$scope:{ctx:W}}}),P=new it({props:{anchor:"transformers.TimmWrapperForImageClassification.forward.example",$$slots:{default:[Gt]},$$scope:{ctx:W}}}),pe=new Ut({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/timm_wrapper.md"}}),{c(){s=c("meta"),_=n(),m=c("p"),p=n(),g(w.$$.fragment),o=n(),T=c("div"),T.innerHTML=ct,We=n(),g(Q.$$.fragment),ve=n(),X=c("p"),X.textContent=dt,Ce=n(),g(S.$$.fragment),Je=n(),g(L.$$.fragment),Ie=n(),A=c("p"),A.textContent=gt,ke=n(),g(q.$$.fragment),Ue=n(),D=c("ul"),D.innerHTML=ft,xe=n(),g(G.$$.fragment),Ze=n(),g(O.$$.fragment),Fe=n(),$=c("div"),g(K.$$.fragment),Qe=n(),ge=c("p"),ge.innerHTML=ut,Xe=n(),fe=c("p"),fe.textContent=ht,Se=n(),ue=c("p"),ue.innerHTML=bt,Le=n(),he=c("p"),he.innerHTML=Mt,Ae=n(),g(N.$$.fragment),Ee=n(),g(ee.$$.fragment),Be=n(),J=c("div"),g(te.$$.fragment),qe=n(),be=c("p"),be.textContent=_t,De=n(),V=c("div"),g(se.$$.fragment),Oe=n(),Me=c("p"),Me.textContent=wt,Ge=n(),g(ae.$$.fragment),Ne=n(),I=c("div"),g(oe.$$.fragment),Ke=n(),_e=c("p"),_e.textContent=Tt,et=n(),v=c("div"),g(ne.$$.fragment),tt=n(),we=c("p"),we.innerHTML=yt,st=n(),g(R.$$.fragment),at=n(),g(z.$$.fragment),Ve=n(),g(re.$$.fragment),Re=n(),k=c("div"),g(le.$$.fragment),ot=n(),Te=c("p"),Te.textContent=$t,nt=n(),C=c("div"),g(me.$$.fragment),rt=n(),ye=c("p"),ye.innerHTML=jt,lt=n(),g(Y.$$.fragment),mt=n(),g(P.$$.fragment),ze=n(),g(pe.$$.fragment),Ye=n(),je=c("p"),this.h()},l(e){const t=It("svelte-u9bgzb",document.head);s=d(t,"META",{name:!0,content:!0}),t.forEach(a),_=r(e),m=d(e,"P",{}),B(m).forEach(a),p=r(e),f(w.$$.fragment,e),o=r(e),T=d(e,"DIV",{class:!0,"data-svelte-h":!0}),y(T)!=="svelte-13t8s2t"&&(T.innerHTML=ct),We=r(e),f(Q.$$.fragment,e),ve=r(e),X=d(e,"P",{"data-svelte-h":!0}),y(X)!=="svelte-1d2nr32"&&(X.textContent=dt),Ce=r(e),f(S.$$.fragment,e),Je=r(e),f(L.$$.fragment,e),Ie=r(e),A=d(e,"P",{"data-svelte-h":!0}),y(A)!=="svelte-1dvhj7j"&&(A.textContent=gt),ke=r(e),f(q.$$.fragment,e),Ue=r(e),D=d(e,"UL",{"data-svelte-h":!0}),y(D)!=="svelte-7pesi5"&&(D.innerHTML=ft),xe=r(e),f(G.$$.fragment,e),Ze=r(e),f(O.$$.fragment,e),Fe=r(e),$=d(e,"DIV",{class:!0});var j=B($);f(K.$$.fragment,j),Qe=r(j),ge=d(j,"P",{"data-svelte-h":!0}),y(ge)!=="svelte-gylkd1"&&(ge.innerHTML=ut),Xe=r(j),fe=d(j,"P",{"data-svelte-h":!0}),y(fe)!=="svelte-1yhg4yu"&&(fe.textContent=ht),Se=r(j),ue=d(j,"P",{"data-svelte-h":!0}),y(ue)!=="svelte-1ek1ss9"&&(ue.innerHTML=bt),Le=r(j),he=d(j,"P",{"data-svelte-h":!0}),y(he)!=="svelte-1bai37y"&&(he.innerHTML=Mt),Ae=r(j),f(N.$$.fragment,j),j.forEach(a),Ee=r(e),f(ee.$$.fragment,e),Be=r(e),J=d(e,"DIV",{class:!0});var Z=B(J);f(te.$$.fragment,Z),qe=r(Z),be=d(Z,"P",{"data-svelte-h":!0}),y(be)!=="svelte-89wik1"&&(be.textContent=_t),De=r(Z),V=d(Z,"DIV",{class:!0});var ie=B(V);f(se.$$.fragment,ie),Oe=r(ie),Me=d(ie,"P",{"data-svelte-h":!0}),y(Me)!=="svelte-1x3yxsa"&&(Me.textContent=wt),ie.forEach(a),Z.forEach(a),Ge=r(e),f(ae.$$.fragment,e),Ne=r(e),I=d(e,"DIV",{class:!0});var F=B(I);f(oe.$$.fragment,F),Ke=r(F),_e=d(F,"P",{"data-svelte-h":!0}),y(_e)!=="svelte-1w306ql"&&(_e.textContent=Tt),et=r(F),v=d(F,"DIV",{class:!0});var U=B(v);f(ne.$$.fragment,U),tt=r(U),we=d(U,"P",{"data-svelte-h":!0}),y(we)!=="svelte-1z02t88"&&(we.innerHTML=yt),st=r(U),f(R.$$.fragment,U),at=r(U),f(z.$$.fragment,U),U.forEach(a),F.forEach(a),Ve=r(e),f(re.$$.fragment,e),Re=r(e),k=d(e,"DIV",{class:!0});var E=B(k);f(le.$$.fragment,E),ot=r(E),Te=d(E,"P",{"data-svelte-h":!0}),y(Te)!=="svelte-drqqzb"&&(Te.textContent=$t),nt=r(E),C=d(E,"DIV",{class:!0});var H=B(C);f(me.$$.fragment,H),rt=r(H),ye=d(H,"P",{"data-svelte-h":!0}),y(ye)!=="svelte-1h2cfoq"&&(ye.innerHTML=jt),lt=r(H),f(Y.$$.fragment,H),mt=r(H),f(P.$$.fragment,H),H.forEach(a),E.forEach(a),ze=r(e),f(pe.$$.fragment,e),Ye=r(e),je=d(e,"P",{}),B(je).forEach(a),this.h()},h(){x(s,"name","hf:doc:metadata"),x(s,"content",Vt),x(T,"class","flex flex-wrap space-x-1"),x($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){i(document.head,s),l(e,_,t),l(e,m,t),l(e,p,t),u(w,e,t),l(e,o,t),l(e,T,t),l(e,We,t),u(Q,e,t),l(e,ve,t),l(e,X,t),l(e,Ce,t),u(S,e,t),l(e,Je,t),u(L,e,t),l(e,Ie,t),l(e,A,t),l(e,ke,t),u(q,e,t),l(e,Ue,t),l(e,D,t),l(e,xe,t),u(G,e,t),l(e,Ze,t),u(O,e,t),l(e,Fe,t),l(e,$,t),u(K,$,null),i($,Qe),i($,ge),i($,Xe),i($,fe),i($,Se),i($,ue),i($,Le),i($,he),i($,Ae),u(N,$,null),l(e,Ee,t),u(ee,e,t),l(e,Be,t),l(e,J,t),u(te,J,null),i(J,qe),i(J,be),i(J,De),i(J,V),u(se,V,null),i(V,Oe),i(V,Me),l(e,Ge,t),u(ae,e,t),l(e,Ne,t),l(e,I,t),u(oe,I,null),i(I,Ke),i(I,_e),i(I,et),i(I,v),u(ne,v,null),i(v,tt),i(v,we),i(v,st),u(R,v,null),i(v,at),u(z,v,null),l(e,Ve,t),u(re,e,t),l(e,Re,t),l(e,k,t),u(le,k,null),i(k,ot),i(k,Te),i(k,nt),i(k,C),u(me,C,null),i(C,rt),i(C,ye),i(C,lt),u(Y,C,null),i(C,mt),u(P,C,null),l(e,ze,t),u(pe,e,t),l(e,Ye,t),l(e,je,t),Pe=!0},p(e,[t]){const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),G.$set(j);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),N.$set(Z);const ie={};t&2&&(ie.$$scope={dirty:t,ctx:e}),R.$set(ie);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),z.$set(F);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),Y.$set(U);const E={};t&2&&(E.$$scope={dirty:t,ctx:e}),P.$set(E)},i(e){Pe||(h(w.$$.fragment,e),h(Q.$$.fragment,e),h(S.$$.fragment,e),h(L.$$.fragment,e),h(q.$$.fragment,e),h(G.$$.fragment,e),h(O.$$.fragment,e),h(K.$$.fragment,e),h(N.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(se.$$.fragment,e),h(ae.$$.fragment,e),h(oe.$$.fragment,e),h(ne.$$.fragment,e),h(R.$$.fragment,e),h(z.$$.fragment,e),h(re.$$.fragment,e),h(le.$$.fragment,e),h(me.$$.fragment,e),h(Y.$$.fragment,e),h(P.$$.fragment,e),h(pe.$$.fragment,e),Pe=!0)},o(e){b(w.$$.fragment,e),b(Q.$$.fragment,e),b(S.$$.fragment,e),b(L.$$.fragment,e),b(q.$$.fragment,e),b(G.$$.fragment,e),b(O.$$.fragment,e),b(K.$$.fragment,e),b(N.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(se.$$.fragment,e),b(ae.$$.fragment,e),b(oe.$$.fragment,e),b(ne.$$.fragment,e),b(R.$$.fragment,e),b(z.$$.fragment,e),b(re.$$.fragment,e),b(le.$$.fragment,e),b(me.$$.fragment,e),b(Y.$$.fragment,e),b(P.$$.fragment,e),b(pe.$$.fragment,e),Pe=!1},d(e){e&&(a(_),a(m),a(p),a(o),a(T),a(We),a(ve),a(X),a(Ce),a(Je),a(Ie),a(A),a(ke),a(Ue),a(D),a(xe),a(Ze),a(Fe),a($),a(Ee),a(Be),a(J),a(Ge),a(Ne),a(I),a(Ve),a(Re),a(k),a(ze),a(Ye),a(je)),a(s),M(w,e),M(Q,e),M(S,e),M(L,e),M(q,e),M(G,e),M(O,e),M(K),M(N),M(ee,e),M(te),M(se),M(ae,e),M(oe),M(ne),M(R),M(z),M(re,e),M(le),M(me),M(Y),M(P),M(pe,e)}}}const Vt='{"title":"TimmWrapper","local":"timmwrapper","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources:","local":"resources","sections":[],"depth":2},{"title":"TimmWrapperConfig","local":"transformers.TimmWrapperConfig","sections":[],"depth":2},{"title":"TimmWrapperImageProcessor","local":"transformers.TimmWrapperImageProcessor","sections":[],"depth":2},{"title":"TimmWrapperModel","local":"transformers.TimmWrapperModel","sections":[],"depth":2},{"title":"TimmWrapperForImageClassification","local":"transformers.TimmWrapperForImageClassification","sections":[],"depth":2}],"depth":1}';function Rt(W){return vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class At extends Ct{constructor(s){super(),Jt(this,s,Rt,Nt,Wt,{})}}export{At as component};
