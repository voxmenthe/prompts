import{s as js,o as Us,n as Rn}from"../chunks/scheduler.18a86fab.js";import{S as Is,i as Cs,g as i,s as o,r as d,A as $s,h as l,f as t,c as s,j as J,x as f,u as m,k as T,y as a,a as r,v as p,d as u,t as g,w as h}from"../chunks/index.98837b22.js";import{T as Ft}from"../chunks/Tip.77304350.js";import{D as w}from"../chunks/Docstring.a1ef7999.js";import{C as Hn}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as vs}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as I,E as xs}from"../chunks/getInferenceSnippets.06c2775f.js";function ks(z){let c,b="Example:",M,y,j;return y=new Hn({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEphbnVzRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTJDJTIwSmFudXNDb25maWclMkMlMjBKYW51c1Zpc2lvbkNvbmZpZyUyQyUyMEphbnVzVlFWQUVDb25maWclMkMlMjBMbGFtYUNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBKYW51cyUyMHZpc2lvbiUyMGNvbmZpZyUwQXZpc2lvbl9jb25maWclMjAlM0QlMjBKYW51c1Zpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMExsYW1hJTIwY29uZmlnJTBBdGV4dF9jb25maWclMjAlM0QlMjBMbGFtYUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZRJTIwY29uZmlnJTBBdnFfY29uZmlnJTIwJTNEJTIwSmFudXNWUVZBRUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEphbnVzJTIwUHJvJTIwMUIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwSmFudXNDb25maWcodmlzaW9uX2NvbmZpZyUzRHZpc2lvbl9jb25maWclMkMlMjB0ZXh0X2NvbmZpZyUzRHRleHRfY29uZmlnJTJDJTIwdnFfY29uZmlnJTNEdnFfY29uZmlnKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjBKYW51cyUyMFBybyUyMDFCJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBKYW51c0ZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbihjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JanusForConditionalGeneration, JanusConfig, JanusVisionConfig, JanusVQVAEConfig, LlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Janus vision config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = JanusVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Llama config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = LlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VQ config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vq_config = JanusVQVAEConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Janus Pro 1B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = JanusConfig(vision_config=vision_config, text_config=text_config, vq_config=vq_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the Janus Pro 1B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = JanusForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){c=i("p"),c.textContent=b,M=o(),d(y.$$.fragment)},l(_){c=l(_,"P",{"data-svelte-h":!0}),f(c)!=="svelte-11lpom8"&&(c.textContent=b),M=s(_),m(y.$$.fragment,_)},m(_,V){r(_,c,V),r(_,M,V),p(y,_,V),j=!0},p:Rn,i(_){j||(u(y.$$.fragment,_),j=!0)},o(_){g(y.$$.fragment,_),j=!1},d(_){_&&(t(c),t(M)),h(y,_)}}}function zs(z){let c,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){c=i("p"),c.innerHTML=b},l(M){c=l(M,"P",{"data-svelte-h":!0}),f(c)!=="svelte-fincs2"&&(c.innerHTML=b)},m(M,y){r(M,c,y)},p:Rn,d(M){M&&t(c)}}}function As(z){let c,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){c=i("p"),c.innerHTML=b},l(M){c=l(M,"P",{"data-svelte-h":!0}),f(c)!=="svelte-fincs2"&&(c.innerHTML=b)},m(M,y){r(M,c,y)},p:Rn,d(M){M&&t(c)}}}function Zs(z){let c,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){c=i("p"),c.innerHTML=b},l(M){c=l(M,"P",{"data-svelte-h":!0}),f(c)!=="svelte-fincs2"&&(c.innerHTML=b)},m(M,y){r(M,c,y)},p:Rn,d(M){M&&t(c)}}}function Vs(z){let c,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){c=i("p"),c.innerHTML=b},l(M){c=l(M,"P",{"data-svelte-h":!0}),f(c)!=="svelte-fincs2"&&(c.innerHTML=b)},m(M,y){r(M,c,y)},p:Rn,d(M){M&&t(c)}}}function qs(z){let c,b="Example:",M,y,j;return y=new Hn({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEphbnVzRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBKYW51c0ZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZGVlcHNlZWstY29tbXVuaXR5JTJGSmFudXMtUHJvLTFCJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmRlZXBzZWVrLWNvbW11bml0eSUyRkphbnVzLVByby0xQiUyMiklMEElMEFtZXNzYWdlcyUyMCUzRCUyMCU1QiUwQSUyMCUyMCUyMCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMnJvbGUlMjIlM0ElMjAlMjJ1c2VyJTIyJTJDJTIwJTIyY29udGVudCUyMiUzQSUyMCU1QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU3QiUyMnR5cGUlMjIlM0ElMjAlMjJpbWFnZSUyMiUyQyUyMCUyMnVybCUyMiUzQSUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnBpcGVsaW5lLWNhdC1jaG9uay5qcGVnJTIyJTdEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTdCJTIydHlwZSUyMiUzQSUyMCUyMnRleHQlMjIlMkMlMjAlMjJ0ZXh0JTIyJTNBJTIwJTIyV2hlcmUlMjBpcyUyMHRoZSUyMGNhdCUyMHN0YW5kaW5nJTNGJTIyJTdEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVEJTBBJTIwJTIwJTIwJTIwJTdEJTJDJTBBJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yLmFwcGx5X2NoYXRfdGVtcGxhdGUoJTBBJTIwJTIwJTIwJTIwbWVzc2FnZXMlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjByZXR1cm5fZGljdCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTBBJTIwJTIwJTIwJTIwYWRkX2dlbmVyYXRpb25fcHJvbXB0JTNEVHJ1ZSUwQSklMEElMjMlMjBHZW5lcmF0ZSUwQWdlbmVyYXRlX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQXByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, JanusForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>model = JanusForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;deepseek-community/Janus-Pro-1B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;deepseek-community/Janus-Pro-1B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>messages = [
<span class="hljs-meta">... </span>    {
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [
<span class="hljs-meta">... </span>            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>},
<span class="hljs-meta">... </span>            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Where is the cat standing?&quot;</span>},
<span class="hljs-meta">... </span>        ]
<span class="hljs-meta">... </span>    },
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor.apply_chat_template(
<span class="hljs-meta">... </span>    messages,
<span class="hljs-meta">... </span>    tokenize=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    return_dict=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>    add_generation_prompt=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){c=i("p"),c.textContent=b,M=o(),d(y.$$.fragment)},l(_){c=l(_,"P",{"data-svelte-h":!0}),f(c)!=="svelte-11lpom8"&&(c.textContent=b),M=s(_),m(y.$$.fragment,_)},m(_,V){r(_,c,V),r(_,M,V),p(y,_,V),j=!0},p:Rn,i(_){j||(u(y.$$.fragment,_),j=!0)},o(_){g(y.$$.fragment,_),j=!1},d(_){_&&(t(c),t(M)),h(y,_)}}}function Bs(z){let c,b,M,y,j,_="<em>This model was released on 2024-10-17 and added to Hugging Face Transformers on 2025-04-17.</em>",V,ce,Sn,de,Xn,me,Bo='The Janus Model was originally proposed in <a href="https://huggingface.co/papers/2410.13848" rel="nofollow">Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</a> by DeepSeek AI team and later refined in <a href="https://huggingface.co/papers/2501.17811" rel="nofollow">Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling</a>. Janus is a vision-language model that can generate both image and text output, it can also take both images and text as input.',Ln,pe,Wo=`<p>[!NOTE]
The model doesn’t generate both images and text in an interleaved format. The user has to pass a parameter indicating whether to generate text or image.</p>`,Dn,ue,Eo="The abstract from the original paper is the following:",Yn,ge,Po="<em>In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder’s roles in understanding and generation, but also enhances the framework’s flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.</em>",On,he,No="The abstract from the aforementioned <code>Janus-Pro</code> paper, released afterwards, is the following:",Kn,fe,Fo="<em>In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strate (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.</em>",et,Me,Qo=`This model was contributed by <a href="https://huggingface.co/yaswanthgali" rel="nofollow">Yaswanth Gali</a> and <a href="https://huggingface.co/hugosilva664" rel="nofollow">Hugo Silva</a>.
The original code can be found <a href="https://github.com/deepseek-ai/Janus" rel="nofollow">here</a>.`,nt,_e,tt,ye,ot,Je,Ro="Here is the example of visual understanding with a single image.",st,Te,Go=`<p>[!NOTE]
Note that the model has been trained with a specific prompt format for chatting. Use <code>processor.apply_chat_template(my_conversation_dict)</code> to correctly format your prompts.</p>`,at,be,rt,we,it,ve,Ho="Janus can perform inference with multiple images as input, where images can belong to the same prompt or different prompts in batched inference, where the model processes many conversations in parallel. Here is how you can do it:",lt,je,ct,Ue,dt,Ie,So="Janus can also generate images given a prompt.",mt,Ce,pt,$e,ut,C,xe,Qt,dn,Xo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusModel">JanusModel</a>. It is used to instantiate an
Janus model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Janus-1B or Janus-7B models.`,Rt,mn,Lo=`e.g. <a href="https://huggingface.co/deepseek-community/Janus-Pro-1B" rel="nofollow">deepseek-community/Janus-Pro-1B</a> or
<a href="https://huggingface.co/deepseek-community/Janus-Pro-7B" rel="nofollow">deepseek-community/Janus-Pro-7B</a>`,Gt,pn,Do=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ht,L,gt,ke,ht,B,ze,St,un,Yo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusVisionModel">JanusVisionModel</a>. It is used to instantiate a
<code>JanusVisionModel</code> according to the specified arguments, defining the model architecture.`,Xt,gn,Oo=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ft,Ae,Mt,H,Ze,Lt,hn,Ko=`This is the configuration class to store the configuration of a <code>JanusVQVAEModel</code>. It is used to instantiate a
<code>JanusVQVAEModel</code> according to the specified arguments, defining the model architecture.
Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information. Instantiating a
configuration with the defaults will yield a similar configuration to the VQModel of the
<a href="https://huggingface.co/deepseek-community/Janus-Pro-1B" rel="nofollow">deepseek-community/Janus-Pro-1B</a>.`,_t,Ve,yt,A,qe,Dt,fn,es="Constructs a Janus processor which wraps a Janus Image Processor and a Llama tokenizer into a single processor.",Yt,Mn,ns=`<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Ot,D,Be,Kt,_n,ts=`Forwards all arguments to the image processor’s <code>postprocess</code> method.
Refer to the original method’s docstring for more details.`,Jt,We,Tt,v,Ee,eo,yn,os="Constructs a JANUS image processor.",no,Y,Pe,to,Jn,ss="Pads an image to a square based on the longest edge.",oo,O,Ne,so,Tn,as="Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.",ao,K,Fe,ro,bn,rs="Preprocess an image or batch of images.",io,ee,Qe,lo,wn,is="Resize an image to dynamically calculated size.",co,ne,Re,mo,vn,ls=`Unnormalizes <code>image</code> using the mean and standard deviation specified by <code>mean</code> and <code>std</code>.
image = (image * image_std) + image_mean`,bt,Ge,wt,W,He,po,jn,cs="Constructs a fast Janus image processor.",uo,te,Se,go,Un,ds="Pads an image to a square based on the longest edge.",vt,Xe,jt,$,Le,ho,In,ms="The bare Janus Model outputting raw hidden-states without any specific head on top.",fo,Cn,ps=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Mo,$n,us=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,_o,P,De,yo,xn,gs='The <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusVisionModel">JanusVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Jo,oe,Ut,Ye,It,x,Oe,To,kn,hs=`The VQ-VAE model used in Janus for encoding/decoding images into discrete tokens.
This model follows the “Make-a-scene: Scene-based text-to-image generation with human priors” paper from
<a href="https://huggingface.co/papers/2203.13131" rel="nofollow">Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
Taigman</a>.`,bo,zn,fs=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,wo,An,Ms=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,vo,N,Ke,jo,Zn,_s='The <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusVQVAE">JanusVQVAE</a> forward method, overrides the <code>__call__</code> special method.',Uo,se,Ct,en,$t,k,nn,Io,Vn,ys="The Janus model which consists of a siglip vision backbone, a Llama language model and a VQ model.",Co,qn,Js=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,$o,Bn,Ts=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,xo,F,tn,ko,Wn,bs='The <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusModel">JanusModel</a> forward method, overrides the <code>__call__</code> special method.',zo,ae,xt,on,kt,S,sn,Ao,q,an,Zo,En,ws='The <a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusForConditionalGeneration">JanusForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Vo,re,qo,ie,zt,rn,At,Gn,Zt;return ce=new I({props:{title:"Janus",local:"janus",headingTag:"h1"}}),de=new I({props:{title:"Overview",local:"overview",headingTag:"h2"}}),_e=new I({props:{title:"Usage Example",local:"usage-example",headingTag:"h2"}}),ye=new I({props:{title:"Single image inference",local:"single-image-inference",headingTag:"h3"}}),be=new Hn({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBKYW51c0ZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUyQyUyMEphbnVzUHJvY2Vzc29yJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJkZWVwc2Vlay1jb21tdW5pdHklMkZKYW51cy1Qcm8tMUIlMjIlMEElMjMlMjBQcmVwYXJlJTIwSW5wdXQlMjBmb3IlMjBnZW5lcmF0aW9uLiUwQW1lc3NhZ2VzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIycm9sZSUyMiUzQSUyMCUyMnVzZXIlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJjb250ZW50JTIyJTNBJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTdCJ3R5cGUnJTNBJ2ltYWdlJyUyQyUyMCd1cmwnJTNBJTIwJ2h0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGcnJTdEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTdCJ3R5cGUnJTNBJTIydGV4dCUyMiUyQyUyMCUyMnRleHQlMjIlM0ElMjJXaGF0JTIwZG8lMjB5b3UlMjBzZWUlMjBpbiUyMHRoaXMlMjBpbWFnZSUzRi4lMjIlN0QlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUQlMEElMjAlMjAlMjAlMjAlN0QlMkMlMEElNUQlMEElMEElMjMlMjBTZXQlMjBnZW5lcmF0aW9uJTIwbW9kZSUyMHRvJTIwJTYwdGV4dCU2MCUyMHRvJTIwcGVyZm9ybSUyMHRleHQlMjBnZW5lcmF0aW9uLiUwQXByb2Nlc3NvciUyMCUzRCUyMEphbnVzUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEFtb2RlbCUyMCUzRCUyMEphbnVzRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMCUyMCUyMCUyMCUyMCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IuYXBwbHlfY2hhdF90ZW1wbGF0ZSglMEElMjAlMjAlMjAlMjBtZXNzYWdlcyUyQyUwQSUyMCUyMCUyMCUyMGFkZF9nZW5lcmF0aW9uX3Byb21wdCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBnZW5lcmF0aW9uX21vZGUlM0QlMjJ0ZXh0JTIyJTJDJTBBJTIwJTIwJTIwJTIwdG9rZW5pemUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX2RpY3QlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSkudG8obW9kZWwuZGV2aWNlJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiklMEElMEFvdXRwdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENDAlMkNnZW5lcmF0aW9uX21vZGUlM0QndGV4dCclMkNkb19zYW1wbGUlM0RUcnVlKSUwQXRleHQlMjAlM0QlMjBwcm9jZXNzb3IuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludCh0ZXh0KQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JanusForConditionalGeneration, JanusProcessor

model_id = <span class="hljs-string">&quot;deepseek-community/Janus-Pro-1B&quot;</span>
<span class="hljs-comment"># Prepare Input for generation.</span>
messages = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&#x27;type&#x27;</span>:<span class="hljs-string">&#x27;image&#x27;</span>, <span class="hljs-string">&#x27;url&#x27;</span>: <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>},
            {<span class="hljs-string">&#x27;type&#x27;</span>:<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>:<span class="hljs-string">&quot;What do you see in this image?.&quot;</span>}
        ]
    },
]

<span class="hljs-comment"># Set generation mode to \`text\` to perform text generation.</span>
processor = JanusProcessor.from_pretrained(model_id)
model = JanusForConditionalGeneration.from_pretrained(model_id,     
        dtype=torch.bfloat16,
        device_map=<span class="hljs-string">&quot;auto&quot;</span>)

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=<span class="hljs-literal">True</span>,
    generation_mode=<span class="hljs-string">&quot;text&quot;</span>,
    tokenize=<span class="hljs-literal">True</span>,
    return_dict=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
).to(model.device, dtype=torch.bfloat16)

output = model.generate(**inputs, max_new_tokens=<span class="hljs-number">40</span>,generation_mode=<span class="hljs-string">&#x27;text&#x27;</span>,do_sample=<span class="hljs-literal">True</span>)
text = processor.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(text)`,wrap:!1}}),we=new I({props:{title:"Multi image inference",local:"multi-image-inference",headingTag:"h3"}}),je=new Hn({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JanusForConditionalGeneration, JanusProcessor

model_id = <span class="hljs-string">&quot;deepseek-community/Janus-Pro-1B&quot;</span>

image_urls = [
    <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>,
    <span class="hljs-string">&quot;https://www.ilankelman.org/stopsigns/australia.jpg&quot;</span>,
    <span class="hljs-string">&quot;https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg&quot;</span>
]

messages = [
    [
        {
            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
            <span class="hljs-string">&quot;content&quot;</span>: [
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What’s the difference between&quot;</span>},
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>: image_urls[<span class="hljs-number">0</span>]},
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot; and &quot;</span>},
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>: image_urls[<span class="hljs-number">1</span>]}
            ]
        }
    ],
    [
        {
            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
            <span class="hljs-string">&quot;content&quot;</span>: [
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>: image_urls[<span class="hljs-number">2</span>]},
                {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What do you see in this image?&quot;</span>}
            ]
        }
    ]
]

<span class="hljs-comment"># Load model and processor</span>
processor = JanusProcessor.from_pretrained(model_id)
model = JanusForConditionalGeneration.from_pretrained(
    model_id, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>
)

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=<span class="hljs-literal">True</span>,
    generation_mode=<span class="hljs-string">&quot;text&quot;</span>,
    tokenize=<span class="hljs-literal">True</span>,
    padding=<span class="hljs-literal">True</span>,
    return_dict=<span class="hljs-literal">True</span>,
    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
).to(model.device, dtype=torch.bfloat16)

<span class="hljs-comment"># Generate response</span>
output = model.generate(**inputs, max_new_tokens=<span class="hljs-number">40</span>, generation_mode=<span class="hljs-string">&#x27;text&#x27;</span>, do_sample=<span class="hljs-literal">False</span>)
text = processor.batch_decode(output, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(text)`,wrap:!1}}),Ue=new I({props:{title:"Text to Image generation",local:"text-to-image-generation",headingTag:"h2"}}),Ce=new Hn({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JanusForConditionalGeneration, JanusProcessor

<span class="hljs-comment"># Set generation mode to \`image\` to prepare inputs for image generation..</span>

model_id = <span class="hljs-string">&quot;deepseek-community/Janus-Pro-1B&quot;</span>
processor = JanusProcessor.from_pretrained(model_id)
model = JanusForConditionalGeneration.from_pretrained(model_id,
        dtype=torch.bfloat16,
        device_map=<span class="hljs-string">&quot;auto&quot;</span>)

messages = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: [
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;A dog running under the rain.&quot;</span>},
        ],
     }
]

prompt = processor.apply_chat_template(messages, add_generation_prompt=<span class="hljs-literal">True</span>)
inputs = processor(text=prompt,generation_mode=<span class="hljs-string">&quot;image&quot;</span>,return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device, dtype=torch.bfloat16)

<span class="hljs-comment"># Set num_return_sequence parameter to generate multiple images per prompt.</span>
model.generation_config.num_return_sequences = <span class="hljs-number">2</span>
outputs = model.generate(**inputs,
                         generation_mode=<span class="hljs-string">&quot;image&quot;</span>,
                         do_sample=<span class="hljs-literal">True</span>,
                         use_cache=<span class="hljs-literal">True</span>,
                         )
<span class="hljs-comment"># Perform post-processing on the generated token ids.</span>
decoded_image = model.decode_image_tokens(outputs)
images = processor.postprocess(<span class="hljs-built_in">list</span>(decoded_image.<span class="hljs-built_in">float</span>()),return_tensors=<span class="hljs-string">&quot;PIL.Image.Image&quot;</span>)
<span class="hljs-comment"># Save the image</span>
<span class="hljs-keyword">for</span> i, image <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(images[<span class="hljs-string">&#x27;pixel_values&#x27;</span>]):
    image.save(<span class="hljs-string">f&quot;result<span class="hljs-subst">{i}</span>.png&quot;</span>)`,wrap:!1}}),$e=new I({props:{title:"JanusConfig",local:"transformers.JanusConfig",headingTag:"h2"}}),xe=new w({props:{name:"class transformers.JanusConfig",anchor:"transformers.JanusConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"vq_config",val:" = None"},{name:"image_token_id",val:" = 100581"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusConfig.text_config",description:`<strong>text_config</strong> (<code>Union[AutoConfig, dict]</code>, <em>optional</em>, defaults to <code>LlamaConfig</code>) &#x2014;
The config object or dictionary of the text backbone.`,name:"text_config"},{anchor:"transformers.JanusConfig.vision_config",description:`<strong>vision_config</strong> (<code>Union[AutoConfig, dict]</code>,  <em>optional</em>, defaults to <code>JanusVisionConfig</code>) &#x2014;
The config object or dictionary of the vision backbone.`,name:"vision_config"},{anchor:"transformers.JanusConfig.vq_config",description:`<strong>vq_config</strong> (<code>Union[AutoConfig, dict]</code>,  <em>optional</em>, defaults to <code>JanusVQVAEConfig</code>) &#x2014;
The config object or dictionary of the VQVAE backbone.`,name:"vq_config"},{anchor:"transformers.JanusConfig.image_token_id",description:`<strong>image_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 100581) &#x2014;
Token index of a placeholder image token.`,name:"image_token_id"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/configuration_janus.py#L212"}}),L=new vs({props:{anchor:"transformers.JanusConfig.example",$$slots:{default:[ks]},$$scope:{ctx:z}}}),ke=new I({props:{title:"JanusVisionConfig",local:"transformers.JanusVisionConfig",headingTag:"h2"}}),ze=new w({props:{name:"class transformers.JanusVisionConfig",anchor:"transformers.JanusVisionConfig",parameters:[{name:"hidden_size",val:" = 1024"},{name:"num_hidden_layers",val:" = 24"},{name:"num_attention_heads",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"patch_size",val:" = 16"},{name:"image_size",val:" = 384"},{name:"attention_dropout",val:" = 0.0"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"hidden_act",val:" = 'gelu'"},{name:"mlp_ratio",val:" = 4.0"},{name:"attention_bias",val:" = True"},{name:"hidden_dropout_rate",val:" = 0.0"},{name:"projection_dim",val:" = 2048"},{name:"projection_dropout",val:" = 0.0"},{name:"use_qk_norm",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"depth",val:" = 2"},{name:"num_image_tokens",val:" = 576"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.JanusVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.JanusVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.JanusVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.JanusVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.JanusVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.JanusVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Dropout probability for attention weights.`,name:"attention_dropout"},{anchor:"transformers.JanusVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.JanusVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code>, and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.JanusVisionConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.JanusVisionConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys, and values in the attention layers.`,name:"attention_bias"},{anchor:"transformers.JanusVisionConfig.hidden_dropout_rate",description:`<strong>hidden_dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for fully connected layers in the encoder.`,name:"hidden_dropout_rate"},{anchor:"transformers.JanusVisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the MLP projection head.`,name:"projection_dim"},{anchor:"transformers.JanusVisionConfig.projection_dropout",description:`<strong>projection_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Dropout probability for the projection layer.`,name:"projection_dropout"},{anchor:"transformers.JanusVisionConfig.use_qk_norm",description:`<strong>use_qk_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to normalize the query and key matrices.`,name:"use_qk_norm"},{anchor:"transformers.JanusVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated normal initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.JanusVisionConfig.depth",description:`<strong>depth</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of hidden layers in the aligner module.`,name:"depth"},{anchor:"transformers.JanusVisionConfig.num_image_tokens",description:`<strong>num_image_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 576) &#x2014;
Number of image tokens.`,name:"num_image_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/configuration_janus.py#L30"}}),Ae=new I({props:{title:"JanusVQVAEConfig",local:"transformers.JanusVQVAEConfig",headingTag:"h2"}}),Ze=new w({props:{name:"class transformers.JanusVQVAEConfig",anchor:"transformers.JanusVQVAEConfig",parameters:[{name:"embed_dim",val:": int = 8"},{name:"num_embeddings",val:": int = 16384"},{name:"double_latent",val:": bool = False"},{name:"latent_channels",val:": int = 256"},{name:"num_patches",val:": int = 32"},{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"base_channels",val:": int = 128"},{name:"channel_multiplier",val:": list = [1, 1, 2, 2, 4]"},{name:"num_res_blocks",val:": int = 2"},{name:"dropout",val:": float = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"projection_dim",val:" = 2048"},{name:"num_hidden_layers",val:" = 2"},{name:"hidden_act",val:" = 'gelu'"},{name:"image_token_embed_dim",val:" = 2048"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusVQVAEConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Dimensionality of each embedding vector.`,name:"embed_dim"},{anchor:"transformers.JanusVQVAEConfig.num_embeddings",description:`<strong>num_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16384) &#x2014;
Number of codebook embeddings.`,name:"num_embeddings"},{anchor:"transformers.JanusVQVAEConfig.double_latent",description:`<strong>double_latent</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use double z channels.`,name:"double_latent"},{anchor:"transformers.JanusVQVAEConfig.latent_channels",description:`<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of channels for the latent space.`,name:"latent_channels"},{anchor:"transformers.JanusVQVAEConfig.num_patches",description:`<strong>num_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Num of patches the input images can be divided into.`,name:"num_patches"},{anchor:"transformers.JanusVQVAEConfig.in_channels",description:`<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of input channels.`,name:"in_channels"},{anchor:"transformers.JanusVQVAEConfig.out_channels",description:`<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of out channels.`,name:"out_channels"},{anchor:"transformers.JanusVQVAEConfig.base_channels",description:`<strong>base_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Base channel count.`,name:"base_channels"},{anchor:"transformers.JanusVQVAEConfig.channel_multiplier",description:`<strong>channel_multiplier</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 1, 2, 2, 4]</code>) &#x2014;
Channel multipliers for each resolution.`,name:"channel_multiplier"},{anchor:"transformers.JanusVQVAEConfig.num_res_blocks",description:`<strong>num_res_blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of residual blocks.`,name:"num_res_blocks"},{anchor:"transformers.JanusVQVAEConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Dropout rate.`,name:"dropout"},{anchor:"transformers.JanusVQVAEConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.JanusVQVAEConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the MLP projection head.`,name:"projection_dim"},{anchor:"transformers.JanusVQVAEConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of hidden layers in VAVAE MLP Connecter module.`,name:"num_hidden_layers"},{anchor:"transformers.JanusVQVAEConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.JanusVQVAEConfig.image_token_embed_dim",description:`<strong>image_token_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of image embeddings. It should be same as the dimensionality of text embeddings.`,name:"image_token_embed_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/configuration_janus.py#L125"}}),Ve=new I({props:{title:"JanusProcessor",local:"transformers.JanusProcessor",headingTag:"h2"}}),qe=new w({props:{name:"class transformers.JanusProcessor",anchor:"transformers.JanusProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""},{name:"chat_template",val:" = None"},{name:"use_default_system_prompt",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.JanusProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"},{anchor:"transformers.JanusProcessor.chat_template",description:`<strong>chat_template</strong> (<code>str</code>, <em>optional</em>) &#x2014; A Jinja template which will be used to convert lists of messages
in a chat into a tokenizable string.`,name:"chat_template"},{anchor:"transformers.JanusProcessor.use_default_system_prompt",description:`<strong>use_default_system_prompt</strong> (<code>str</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Use default system prompt for Text Generation.`,name:"use_default_system_prompt"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/processing_janus.py#L49"}}),Be=new w({props:{name:"postprocess",anchor:"transformers.JanusProcessor.postprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/processing_janus.py#L156"}}),We=new I({props:{title:"JanusImageProcessor",local:"transformers.JanusImageProcessor",headingTag:"h2"}}),Ee=new w({props:{name:"class transformers.JanusImageProcessor",anchor:"transformers.JanusImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"min_size",val:": int = 14"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.JanusImageProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 384, &quot;width&quot;: 384}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.JanusImageProcessor.min_size",description:`<strong>min_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The minimum allowed size for the resized image. Ensures that neither the height nor width
falls below this value after resizing.`,name:"min_size"},{anchor:"transformers.JanusImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.JanusImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.JanusImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.JanusImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.JanusImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.JanusImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.JanusImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L59"}}),Pe=new w({props:{name:"pad_to_square",anchor:"transformers.JanusImageProcessor.pad_to_square",parameters:[{name:"image",val:": ndarray"},{name:"background_color",val:": typing.Union[int, tuple[int, int, int]] = 0"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.JanusImageProcessor.pad_to_square.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to pad.`,name:"image"},{anchor:"transformers.JanusImageProcessor.pad_to_square.background_color",description:`<strong>background_color</strong> (<code>int</code> or <code>tuple[int, int, int]</code>, <em>optional</em>, defaults to 0) &#x2014;
The color to use for the padding. Can be an integer for single channel or a
tuple of integers representing for multi-channel images. If passed as integer
in mutli-channel mode, it will default to <code>0</code> in subsequent channels.`,name:"background_color"},{anchor:"transformers.JanusImageProcessor.pad_to_square.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use same as the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.JanusImageProcessor.pad_to_square.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L345",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The padded image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),Ne=new w({props:{name:"postprocess",anchor:"transformers.JanusImageProcessor.postprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Optional[list[float]] = None"},{name:"image_std",val:": typing.Optional[list[float]] = None"},{name:"input_data_format",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L419"}}),Fe=new w({props:{name:"preprocess",anchor:"transformers.JanusImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.JanusImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.JanusImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.JanusImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Controls the size of the image after <code>resize</code>. The shortest edge of the image is resized to
<code>size[&quot;shortest_edge&quot;]</code> whilst preserving the aspect ratio. If the longest edge of this resized image
is &gt; <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>, then the image is resized again to make the longest
edge equal to <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>.`,name:"size"},{anchor:"transformers.JanusImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.JanusImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.JanusImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.JanusImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.JanusImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.JanusImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.JanusImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.JanusImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.JanusImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.JanusImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L208"}}),Qe=new w({props:{name:"resize",anchor:"transformers.JanusImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Union[dict[str, int], int]"},{name:"background_color",val:": typing.Optional[tuple[int, int, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.JanusImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code> or <code>int</code>) &#x2014;
The size to resize the image to. If a dictionary, it should have the keys <code>&quot;height&quot;</code> and <code>&quot;width&quot;</code>.`,name:"size"},{anchor:"transformers.JanusImageProcessor.resize.background_color",description:`<strong>background_color</strong> (<code>tuple[int, int, int]</code>) &#x2014;
The background color to use for the padding.`,name:"background_color"},{anchor:"transformers.JanusImageProcessor.resize.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
<code>PILImageResampling</code> filter to use when resizing the image e.g. <code>PILImageResampling.BICUBIC</code>.`,name:"resample"},{anchor:"transformers.JanusImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>None</code>: will be inferred from input</li>
</ul>`,name:"data_format"},{anchor:"transformers.JanusImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L133",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),Re=new w({props:{name:"unnormalize",anchor:"transformers.JanusImageProcessor.unnormalize",parameters:[{name:"image",val:": <built-in function array>"},{name:"image_mean",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"image_std",val:": typing.Union[float, collections.abc.Iterable[float]]"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.JanusImageProcessor.unnormalize.image",description:`<strong>image</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code> or <code>(num_channels, image_size, image_size)</code>) &#x2014;
Batch of pixel values to postprocess.`,name:"image"},{anchor:"transformers.JanusImageProcessor.unnormalize.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The mean to use for unnormalization.`,name:"image_mean"},{anchor:"transformers.JanusImageProcessor.unnormalize.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The standard deviation to use for unnormalization.`,name:"image_std"},{anchor:"transformers.JanusImageProcessor.unnormalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus.py#L470"}}),Ge=new I({props:{title:"JanusImageProcessorFast",local:"transformers.JanusImageProcessorFast",headingTag:"h2"}}),He=new w({props:{name:"class transformers.JanusImageProcessorFast",anchor:"transformers.JanusImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.janus.image_processing_janus_fast.JanusFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus_fast.py#L62"}}),Se=new w({props:{name:"pad_to_square",anchor:"transformers.JanusImageProcessorFast.pad_to_square",parameters:[{name:"images",val:": torch.Tensor"},{name:"background_color",val:": typing.Union[int, tuple[int, int, int]] = 0"}],parametersDescription:[{anchor:"transformers.JanusImageProcessorFast.pad_to_square.images",description:`<strong>images</strong> (<code>torch.Tensor</code>) &#x2014;
The images to pad.`,name:"images"},{anchor:"transformers.JanusImageProcessorFast.pad_to_square.background_color",description:`<strong>background_color</strong> (<code>int</code> or <code>tuple[int, int, int]</code>, <em>optional</em>, defaults to 0) &#x2014;
The color to use for the padding. Can be an integer for single channel or a
tuple of integers representing for multi-channel images. If passed as integer
in mutli-channel mode, it will default to <code>0</code> in subsequent channels.`,name:"background_color"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/image_processing_janus_fast.py#L108",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The padded images.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),Xe=new I({props:{title:"JanusVisionModel",local:"transformers.JanusVisionModel",headingTag:"h2"}}),Le=new w({props:{name:"class transformers.JanusVisionModel",anchor:"transformers.JanusVisionModel",parameters:[{name:"config",val:": JanusVisionConfig"}],parametersDescription:[{anchor:"transformers.JanusVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusVisionConfig">JanusVisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L496"}}),De=new w({props:{name:"forward",anchor:"transformers.JanusVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.JanusVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">JanusImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.JanusVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.JanusVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.JanusVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.JanusVisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L511",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig"
>JanusConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new Ft({props:{$$slots:{default:[zs]},$$scope:{ctx:z}}}),Ye=new I({props:{title:"JanusVQVAE",local:"transformers.JanusVQVAE",headingTag:"h2"}}),Oe=new w({props:{name:"class transformers.JanusVQVAE",anchor:"transformers.JanusVQVAE",parameters:[{name:"config",val:": JanusVQVAEConfig"}],parametersDescription:[{anchor:"transformers.JanusVQVAE.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusVQVAEConfig">JanusVQVAEConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L916"}}),Ke=new w({props:{name:"forward",anchor:"transformers.JanusVQVAE.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],parametersDescription:[{anchor:"transformers.JanusVQVAE.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">JanusImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> for processing images).`,name:"pixel_values"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L964",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[torch.FloatTensor, torch.FloatTensor]</code></p>
`}}),se=new Ft({props:{$$slots:{default:[As]},$$scope:{ctx:z}}}),en=new I({props:{title:"JanusModel",local:"transformers.JanusModel",headingTag:"h2"}}),nn=new w({props:{name:"class transformers.JanusModel",anchor:"transformers.JanusModel",parameters:[{name:"config",val:": JanusConfig"}],parametersDescription:[{anchor:"transformers.JanusModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusConfig">JanusConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L1016"}}),tn=new w({props:{name:"forward",anchor:"transformers.JanusModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"pixel_values",val:": FloatTensor = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"logits_to_keep",val:": typing.Union[int, torch.Tensor] = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JanusModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.JanusModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">JanusImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.JanusModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.JanusModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.JanusModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.JanusModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.JanusModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.JanusModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.JanusModel.forward.logits_to_keep",description:`<strong>logits_to_keep</strong> (<code>Union[int, torch.Tensor]</code>, defaults to <code>0</code>) &#x2014;
If an <code>int</code>, compute logits for the last <code>logits_to_keep</code> tokens. If <code>0</code>, calculate logits for all
<code>input_ids</code> (special case). Only last token logits are needed for generation, and calculating them only for that
token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
If a <code>torch.Tensor</code>, must be 1D corresponding to the indices to keep in the sequence length dimension.
This is useful when using packed tensor format (single dimension for batch and sequence length).`,name:"logits_to_keep"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L1073"}}),ae=new Ft({props:{$$slots:{default:[Zs]},$$scope:{ctx:z}}}),on=new I({props:{title:"JanusForConditionalGeneration",local:"transformers.JanusForConditionalGeneration",headingTag:"h2"}}),sn=new w({props:{name:"class transformers.JanusForConditionalGeneration",anchor:"transformers.JanusForConditionalGeneration",parameters:[{name:"config",val:": JanusConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L1124"}}),an=new w({props:{name:"forward",anchor:"transformers.JanusForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"pixel_values",val:": FloatTensor = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"logits_to_keep",val:": typing.Union[int, torch.Tensor] = 0"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.JanusForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.JanusForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">JanusImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusProcessor">JanusProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/janus#transformers.JanusImageProcessor">JanusImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.JanusForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.JanusForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.JanusForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.JanusForConditionalGeneration.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.JanusForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.JanusForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.JanusForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.JanusForConditionalGeneration.forward.logits_to_keep",description:`<strong>logits_to_keep</strong> (<code>Union[int, torch.Tensor]</code>, defaults to <code>0</code>) &#x2014;
If an <code>int</code>, compute logits for the last <code>logits_to_keep</code> tokens. If <code>0</code>, calculate logits for all
<code>input_ids</code> (special case). Only last token logits are needed for generation, and calculating them only for that
token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
If a <code>torch.Tensor</code>, must be 1D corresponding to the indices to keep in the sequence length dimension.
This is useful when using packed tensor format (single dimension for batch and sequence length).`,name:"logits_to_keep"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/janus/modeling_janus.py#L1148"}}),re=new Ft({props:{$$slots:{default:[Vs]},$$scope:{ctx:z}}}),ie=new vs({props:{anchor:"transformers.JanusForConditionalGeneration.forward.example",$$slots:{default:[qs]},$$scope:{ctx:z}}}),rn=new xs({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/janus.md"}}),{c(){c=i("meta"),b=o(),M=i("p"),y=o(),j=i("p"),j.innerHTML=_,V=o(),d(ce.$$.fragment),Sn=o(),d(de.$$.fragment),Xn=o(),me=i("p"),me.innerHTML=Bo,Ln=o(),pe=i("blockquote"),pe.innerHTML=Wo,Dn=o(),ue=i("p"),ue.textContent=Eo,Yn=o(),ge=i("p"),ge.innerHTML=Po,On=o(),he=i("p"),he.innerHTML=No,Kn=o(),fe=i("p"),fe.innerHTML=Fo,et=o(),Me=i("p"),Me.innerHTML=Qo,nt=o(),d(_e.$$.fragment),tt=o(),d(ye.$$.fragment),ot=o(),Je=i("p"),Je.textContent=Ro,st=o(),Te=i("blockquote"),Te.innerHTML=Go,at=o(),d(be.$$.fragment),rt=o(),d(we.$$.fragment),it=o(),ve=i("p"),ve.textContent=Ho,lt=o(),d(je.$$.fragment),ct=o(),d(Ue.$$.fragment),dt=o(),Ie=i("p"),Ie.textContent=So,mt=o(),d(Ce.$$.fragment),pt=o(),d($e.$$.fragment),ut=o(),C=i("div"),d(xe.$$.fragment),Qt=o(),dn=i("p"),dn.innerHTML=Xo,Rt=o(),mn=i("p"),mn.innerHTML=Lo,Gt=o(),pn=i("p"),pn.innerHTML=Do,Ht=o(),d(L.$$.fragment),gt=o(),d(ke.$$.fragment),ht=o(),B=i("div"),d(ze.$$.fragment),St=o(),un=i("p"),un.innerHTML=Yo,Xt=o(),gn=i("p"),gn.innerHTML=Oo,ft=o(),d(Ae.$$.fragment),Mt=o(),H=i("div"),d(Ze.$$.fragment),Lt=o(),hn=i("p"),hn.innerHTML=Ko,_t=o(),d(Ve.$$.fragment),yt=o(),A=i("div"),d(qe.$$.fragment),Dt=o(),fn=i("p"),fn.textContent=es,Yt=o(),Mn=i("p"),Mn.innerHTML=ns,Ot=o(),D=i("div"),d(Be.$$.fragment),Kt=o(),_n=i("p"),_n.innerHTML=ts,Jt=o(),d(We.$$.fragment),Tt=o(),v=i("div"),d(Ee.$$.fragment),eo=o(),yn=i("p"),yn.textContent=os,no=o(),Y=i("div"),d(Pe.$$.fragment),to=o(),Jn=i("p"),Jn.textContent=ss,oo=o(),O=i("div"),d(Ne.$$.fragment),so=o(),Tn=i("p"),Tn.textContent=as,ao=o(),K=i("div"),d(Fe.$$.fragment),ro=o(),bn=i("p"),bn.textContent=rs,io=o(),ee=i("div"),d(Qe.$$.fragment),lo=o(),wn=i("p"),wn.textContent=is,co=o(),ne=i("div"),d(Re.$$.fragment),mo=o(),vn=i("p"),vn.innerHTML=ls,bt=o(),d(Ge.$$.fragment),wt=o(),W=i("div"),d(He.$$.fragment),po=o(),jn=i("p"),jn.textContent=cs,uo=o(),te=i("div"),d(Se.$$.fragment),go=o(),Un=i("p"),Un.textContent=ds,vt=o(),d(Xe.$$.fragment),jt=o(),$=i("div"),d(Le.$$.fragment),ho=o(),In=i("p"),In.textContent=ms,fo=o(),Cn=i("p"),Cn.innerHTML=ps,Mo=o(),$n=i("p"),$n.innerHTML=us,_o=o(),P=i("div"),d(De.$$.fragment),yo=o(),xn=i("p"),xn.innerHTML=gs,Jo=o(),d(oe.$$.fragment),Ut=o(),d(Ye.$$.fragment),It=o(),x=i("div"),d(Oe.$$.fragment),To=o(),kn=i("p"),kn.innerHTML=hs,bo=o(),zn=i("p"),zn.innerHTML=fs,wo=o(),An=i("p"),An.innerHTML=Ms,vo=o(),N=i("div"),d(Ke.$$.fragment),jo=o(),Zn=i("p"),Zn.innerHTML=_s,Uo=o(),d(se.$$.fragment),Ct=o(),d(en.$$.fragment),$t=o(),k=i("div"),d(nn.$$.fragment),Io=o(),Vn=i("p"),Vn.textContent=ys,Co=o(),qn=i("p"),qn.innerHTML=Js,$o=o(),Bn=i("p"),Bn.innerHTML=Ts,xo=o(),F=i("div"),d(tn.$$.fragment),ko=o(),Wn=i("p"),Wn.innerHTML=bs,zo=o(),d(ae.$$.fragment),xt=o(),d(on.$$.fragment),kt=o(),S=i("div"),d(sn.$$.fragment),Ao=o(),q=i("div"),d(an.$$.fragment),Zo=o(),En=i("p"),En.innerHTML=ws,Vo=o(),d(re.$$.fragment),qo=o(),d(ie.$$.fragment),zt=o(),d(rn.$$.fragment),At=o(),Gn=i("p"),this.h()},l(e){const n=$s("svelte-u9bgzb",document.head);c=l(n,"META",{name:!0,content:!0}),n.forEach(t),b=s(e),M=l(e,"P",{}),J(M).forEach(t),y=s(e),j=l(e,"P",{"data-svelte-h":!0}),f(j)!=="svelte-h5dsbc"&&(j.innerHTML=_),V=s(e),m(ce.$$.fragment,e),Sn=s(e),m(de.$$.fragment,e),Xn=s(e),me=l(e,"P",{"data-svelte-h":!0}),f(me)!=="svelte-mw3s3p"&&(me.innerHTML=Bo),Ln=s(e),pe=l(e,"BLOCKQUOTE",{"data-svelte-h":!0}),f(pe)!=="svelte-1x4u6cq"&&(pe.innerHTML=Wo),Dn=s(e),ue=l(e,"P",{"data-svelte-h":!0}),f(ue)!=="svelte-177aco3"&&(ue.textContent=Eo),Yn=s(e),ge=l(e,"P",{"data-svelte-h":!0}),f(ge)!=="svelte-jhuj19"&&(ge.innerHTML=Po),On=s(e),he=l(e,"P",{"data-svelte-h":!0}),f(he)!=="svelte-1vgoitc"&&(he.innerHTML=No),Kn=s(e),fe=l(e,"P",{"data-svelte-h":!0}),f(fe)!=="svelte-49qhap"&&(fe.innerHTML=Fo),et=s(e),Me=l(e,"P",{"data-svelte-h":!0}),f(Me)!=="svelte-5svxst"&&(Me.innerHTML=Qo),nt=s(e),m(_e.$$.fragment,e),tt=s(e),m(ye.$$.fragment,e),ot=s(e),Je=l(e,"P",{"data-svelte-h":!0}),f(Je)!=="svelte-8gjb1g"&&(Je.textContent=Ro),st=s(e),Te=l(e,"BLOCKQUOTE",{"data-svelte-h":!0}),f(Te)!=="svelte-jn2bty"&&(Te.innerHTML=Go),at=s(e),m(be.$$.fragment,e),rt=s(e),m(we.$$.fragment,e),it=s(e),ve=l(e,"P",{"data-svelte-h":!0}),f(ve)!=="svelte-1d12bbh"&&(ve.textContent=Ho),lt=s(e),m(je.$$.fragment,e),ct=s(e),m(Ue.$$.fragment,e),dt=s(e),Ie=l(e,"P",{"data-svelte-h":!0}),f(Ie)!=="svelte-1qde7m1"&&(Ie.textContent=So),mt=s(e),m(Ce.$$.fragment,e),pt=s(e),m($e.$$.fragment,e),ut=s(e),C=l(e,"DIV",{class:!0});var Z=J(C);m(xe.$$.fragment,Z),Qt=s(Z),dn=l(Z,"P",{"data-svelte-h":!0}),f(dn)!=="svelte-1h81kfu"&&(dn.innerHTML=Xo),Rt=s(Z),mn=l(Z,"P",{"data-svelte-h":!0}),f(mn)!=="svelte-wbmgyb"&&(mn.innerHTML=Lo),Gt=s(Z),pn=l(Z,"P",{"data-svelte-h":!0}),f(pn)!=="svelte-1ek1ss9"&&(pn.innerHTML=Do),Ht=s(Z),m(L.$$.fragment,Z),Z.forEach(t),gt=s(e),m(ke.$$.fragment,e),ht=s(e),B=l(e,"DIV",{class:!0});var X=J(B);m(ze.$$.fragment,X),St=s(X),un=l(X,"P",{"data-svelte-h":!0}),f(un)!=="svelte-44e0bi"&&(un.innerHTML=Yo),Xt=s(X),gn=l(X,"P",{"data-svelte-h":!0}),f(gn)!=="svelte-1ek1ss9"&&(gn.innerHTML=Oo),X.forEach(t),ft=s(e),m(Ae.$$.fragment,e),Mt=s(e),H=l(e,"DIV",{class:!0});var ln=J(H);m(Ze.$$.fragment,ln),Lt=s(ln),hn=l(ln,"P",{"data-svelte-h":!0}),f(hn)!=="svelte-19q9ehj"&&(hn.innerHTML=Ko),ln.forEach(t),_t=s(e),m(Ve.$$.fragment,e),yt=s(e),A=l(e,"DIV",{class:!0});var E=J(A);m(qe.$$.fragment,E),Dt=s(E),fn=l(E,"P",{"data-svelte-h":!0}),f(fn)!=="svelte-c68n8g"&&(fn.textContent=es),Yt=s(E),Mn=l(E,"P",{"data-svelte-h":!0}),f(Mn)!=="svelte-qatj89"&&(Mn.innerHTML=ns),Ot=s(E),D=l(E,"DIV",{class:!0});var cn=J(D);m(Be.$$.fragment,cn),Kt=s(cn),_n=l(cn,"P",{"data-svelte-h":!0}),f(_n)!=="svelte-1l9zllg"&&(_n.innerHTML=ts),cn.forEach(t),E.forEach(t),Jt=s(e),m(We.$$.fragment,e),Tt=s(e),v=l(e,"DIV",{class:!0});var U=J(v);m(Ee.$$.fragment,U),eo=s(U),yn=l(U,"P",{"data-svelte-h":!0}),f(yn)!=="svelte-1aahezl"&&(yn.textContent=os),no=s(U),Y=l(U,"DIV",{class:!0});var Vt=J(Y);m(Pe.$$.fragment,Vt),to=s(Vt),Jn=l(Vt,"P",{"data-svelte-h":!0}),f(Jn)!=="svelte-9psyrp"&&(Jn.textContent=ss),Vt.forEach(t),oo=s(U),O=l(U,"DIV",{class:!0});var qt=J(O);m(Ne.$$.fragment,qt),so=s(qt),Tn=l(qt,"P",{"data-svelte-h":!0}),f(Tn)!=="svelte-19uby4j"&&(Tn.textContent=as),qt.forEach(t),ao=s(U),K=l(U,"DIV",{class:!0});var Bt=J(K);m(Fe.$$.fragment,Bt),ro=s(Bt),bn=l(Bt,"P",{"data-svelte-h":!0}),f(bn)!=="svelte-1x3yxsa"&&(bn.textContent=rs),Bt.forEach(t),io=s(U),ee=l(U,"DIV",{class:!0});var Wt=J(ee);m(Qe.$$.fragment,Wt),lo=s(Wt),wn=l(Wt,"P",{"data-svelte-h":!0}),f(wn)!=="svelte-uoznpt"&&(wn.textContent=is),Wt.forEach(t),co=s(U),ne=l(U,"DIV",{class:!0});var Et=J(ne);m(Re.$$.fragment,Et),mo=s(Et),vn=l(Et,"P",{"data-svelte-h":!0}),f(vn)!=="svelte-1x9mhw3"&&(vn.innerHTML=ls),Et.forEach(t),U.forEach(t),bt=s(e),m(Ge.$$.fragment,e),wt=s(e),W=l(e,"DIV",{class:!0});var Pn=J(W);m(He.$$.fragment,Pn),po=s(Pn),jn=l(Pn,"P",{"data-svelte-h":!0}),f(jn)!=="svelte-1m7s6v3"&&(jn.textContent=cs),uo=s(Pn),te=l(Pn,"DIV",{class:!0});var Pt=J(te);m(Se.$$.fragment,Pt),go=s(Pt),Un=l(Pt,"P",{"data-svelte-h":!0}),f(Un)!=="svelte-9psyrp"&&(Un.textContent=ds),Pt.forEach(t),Pn.forEach(t),vt=s(e),m(Xe.$$.fragment,e),jt=s(e),$=l(e,"DIV",{class:!0});var Q=J($);m(Le.$$.fragment,Q),ho=s(Q),In=l(Q,"P",{"data-svelte-h":!0}),f(In)!=="svelte-1r45011"&&(In.textContent=ms),fo=s(Q),Cn=l(Q,"P",{"data-svelte-h":!0}),f(Cn)!=="svelte-q52n56"&&(Cn.innerHTML=ps),Mo=s(Q),$n=l(Q,"P",{"data-svelte-h":!0}),f($n)!=="svelte-hswkmf"&&($n.innerHTML=us),_o=s(Q),P=l(Q,"DIV",{class:!0});var Nn=J(P);m(De.$$.fragment,Nn),yo=s(Nn),xn=l(Nn,"P",{"data-svelte-h":!0}),f(xn)!=="svelte-snncso"&&(xn.innerHTML=gs),Jo=s(Nn),m(oe.$$.fragment,Nn),Nn.forEach(t),Q.forEach(t),Ut=s(e),m(Ye.$$.fragment,e),It=s(e),x=l(e,"DIV",{class:!0});var R=J(x);m(Oe.$$.fragment,R),To=s(R),kn=l(R,"P",{"data-svelte-h":!0}),f(kn)!=="svelte-xtpa69"&&(kn.innerHTML=hs),bo=s(R),zn=l(R,"P",{"data-svelte-h":!0}),f(zn)!=="svelte-q52n56"&&(zn.innerHTML=fs),wo=s(R),An=l(R,"P",{"data-svelte-h":!0}),f(An)!=="svelte-hswkmf"&&(An.innerHTML=Ms),vo=s(R),N=l(R,"DIV",{class:!0});var Fn=J(N);m(Ke.$$.fragment,Fn),jo=s(Fn),Zn=l(Fn,"P",{"data-svelte-h":!0}),f(Zn)!=="svelte-oqtrdg"&&(Zn.innerHTML=_s),Uo=s(Fn),m(se.$$.fragment,Fn),Fn.forEach(t),R.forEach(t),Ct=s(e),m(en.$$.fragment,e),$t=s(e),k=l(e,"DIV",{class:!0});var G=J(k);m(nn.$$.fragment,G),Io=s(G),Vn=l(G,"P",{"data-svelte-h":!0}),f(Vn)!=="svelte-8k58lq"&&(Vn.textContent=ys),Co=s(G),qn=l(G,"P",{"data-svelte-h":!0}),f(qn)!=="svelte-q52n56"&&(qn.innerHTML=Js),$o=s(G),Bn=l(G,"P",{"data-svelte-h":!0}),f(Bn)!=="svelte-hswkmf"&&(Bn.innerHTML=Ts),xo=s(G),F=l(G,"DIV",{class:!0});var Qn=J(F);m(tn.$$.fragment,Qn),ko=s(Qn),Wn=l(Qn,"P",{"data-svelte-h":!0}),f(Wn)!=="svelte-1evi6o"&&(Wn.innerHTML=bs),zo=s(Qn),m(ae.$$.fragment,Qn),Qn.forEach(t),G.forEach(t),xt=s(e),m(on.$$.fragment,e),kt=s(e),S=l(e,"DIV",{class:!0});var Nt=J(S);m(sn.$$.fragment,Nt),Ao=s(Nt),q=l(Nt,"DIV",{class:!0});var le=J(q);m(an.$$.fragment,le),Zo=s(le),En=l(le,"P",{"data-svelte-h":!0}),f(En)!=="svelte-1e87a1g"&&(En.innerHTML=ws),Vo=s(le),m(re.$$.fragment,le),qo=s(le),m(ie.$$.fragment,le),le.forEach(t),Nt.forEach(t),zt=s(e),m(rn.$$.fragment,e),At=s(e),Gn=l(e,"P",{}),J(Gn).forEach(t),this.h()},h(){T(c,"name","hf:doc:metadata"),T(c,"content",Ws),T(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){a(document.head,c),r(e,b,n),r(e,M,n),r(e,y,n),r(e,j,n),r(e,V,n),p(ce,e,n),r(e,Sn,n),p(de,e,n),r(e,Xn,n),r(e,me,n),r(e,Ln,n),r(e,pe,n),r(e,Dn,n),r(e,ue,n),r(e,Yn,n),r(e,ge,n),r(e,On,n),r(e,he,n),r(e,Kn,n),r(e,fe,n),r(e,et,n),r(e,Me,n),r(e,nt,n),p(_e,e,n),r(e,tt,n),p(ye,e,n),r(e,ot,n),r(e,Je,n),r(e,st,n),r(e,Te,n),r(e,at,n),p(be,e,n),r(e,rt,n),p(we,e,n),r(e,it,n),r(e,ve,n),r(e,lt,n),p(je,e,n),r(e,ct,n),p(Ue,e,n),r(e,dt,n),r(e,Ie,n),r(e,mt,n),p(Ce,e,n),r(e,pt,n),p($e,e,n),r(e,ut,n),r(e,C,n),p(xe,C,null),a(C,Qt),a(C,dn),a(C,Rt),a(C,mn),a(C,Gt),a(C,pn),a(C,Ht),p(L,C,null),r(e,gt,n),p(ke,e,n),r(e,ht,n),r(e,B,n),p(ze,B,null),a(B,St),a(B,un),a(B,Xt),a(B,gn),r(e,ft,n),p(Ae,e,n),r(e,Mt,n),r(e,H,n),p(Ze,H,null),a(H,Lt),a(H,hn),r(e,_t,n),p(Ve,e,n),r(e,yt,n),r(e,A,n),p(qe,A,null),a(A,Dt),a(A,fn),a(A,Yt),a(A,Mn),a(A,Ot),a(A,D),p(Be,D,null),a(D,Kt),a(D,_n),r(e,Jt,n),p(We,e,n),r(e,Tt,n),r(e,v,n),p(Ee,v,null),a(v,eo),a(v,yn),a(v,no),a(v,Y),p(Pe,Y,null),a(Y,to),a(Y,Jn),a(v,oo),a(v,O),p(Ne,O,null),a(O,so),a(O,Tn),a(v,ao),a(v,K),p(Fe,K,null),a(K,ro),a(K,bn),a(v,io),a(v,ee),p(Qe,ee,null),a(ee,lo),a(ee,wn),a(v,co),a(v,ne),p(Re,ne,null),a(ne,mo),a(ne,vn),r(e,bt,n),p(Ge,e,n),r(e,wt,n),r(e,W,n),p(He,W,null),a(W,po),a(W,jn),a(W,uo),a(W,te),p(Se,te,null),a(te,go),a(te,Un),r(e,vt,n),p(Xe,e,n),r(e,jt,n),r(e,$,n),p(Le,$,null),a($,ho),a($,In),a($,fo),a($,Cn),a($,Mo),a($,$n),a($,_o),a($,P),p(De,P,null),a(P,yo),a(P,xn),a(P,Jo),p(oe,P,null),r(e,Ut,n),p(Ye,e,n),r(e,It,n),r(e,x,n),p(Oe,x,null),a(x,To),a(x,kn),a(x,bo),a(x,zn),a(x,wo),a(x,An),a(x,vo),a(x,N),p(Ke,N,null),a(N,jo),a(N,Zn),a(N,Uo),p(se,N,null),r(e,Ct,n),p(en,e,n),r(e,$t,n),r(e,k,n),p(nn,k,null),a(k,Io),a(k,Vn),a(k,Co),a(k,qn),a(k,$o),a(k,Bn),a(k,xo),a(k,F),p(tn,F,null),a(F,ko),a(F,Wn),a(F,zo),p(ae,F,null),r(e,xt,n),p(on,e,n),r(e,kt,n),r(e,S,n),p(sn,S,null),a(S,Ao),a(S,q),p(an,q,null),a(q,Zo),a(q,En),a(q,Vo),p(re,q,null),a(q,qo),p(ie,q,null),r(e,zt,n),p(rn,e,n),r(e,At,n),r(e,Gn,n),Zt=!0},p(e,[n]){const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),L.$set(Z);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),oe.$set(X);const ln={};n&2&&(ln.$$scope={dirty:n,ctx:e}),se.$set(ln);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),ae.$set(E);const cn={};n&2&&(cn.$$scope={dirty:n,ctx:e}),re.$set(cn);const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),ie.$set(U)},i(e){Zt||(u(ce.$$.fragment,e),u(de.$$.fragment,e),u(_e.$$.fragment,e),u(ye.$$.fragment,e),u(be.$$.fragment,e),u(we.$$.fragment,e),u(je.$$.fragment,e),u(Ue.$$.fragment,e),u(Ce.$$.fragment,e),u($e.$$.fragment,e),u(xe.$$.fragment,e),u(L.$$.fragment,e),u(ke.$$.fragment,e),u(ze.$$.fragment,e),u(Ae.$$.fragment,e),u(Ze.$$.fragment,e),u(Ve.$$.fragment,e),u(qe.$$.fragment,e),u(Be.$$.fragment,e),u(We.$$.fragment,e),u(Ee.$$.fragment,e),u(Pe.$$.fragment,e),u(Ne.$$.fragment,e),u(Fe.$$.fragment,e),u(Qe.$$.fragment,e),u(Re.$$.fragment,e),u(Ge.$$.fragment,e),u(He.$$.fragment,e),u(Se.$$.fragment,e),u(Xe.$$.fragment,e),u(Le.$$.fragment,e),u(De.$$.fragment,e),u(oe.$$.fragment,e),u(Ye.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(se.$$.fragment,e),u(en.$$.fragment,e),u(nn.$$.fragment,e),u(tn.$$.fragment,e),u(ae.$$.fragment,e),u(on.$$.fragment,e),u(sn.$$.fragment,e),u(an.$$.fragment,e),u(re.$$.fragment,e),u(ie.$$.fragment,e),u(rn.$$.fragment,e),Zt=!0)},o(e){g(ce.$$.fragment,e),g(de.$$.fragment,e),g(_e.$$.fragment,e),g(ye.$$.fragment,e),g(be.$$.fragment,e),g(we.$$.fragment,e),g(je.$$.fragment,e),g(Ue.$$.fragment,e),g(Ce.$$.fragment,e),g($e.$$.fragment,e),g(xe.$$.fragment,e),g(L.$$.fragment,e),g(ke.$$.fragment,e),g(ze.$$.fragment,e),g(Ae.$$.fragment,e),g(Ze.$$.fragment,e),g(Ve.$$.fragment,e),g(qe.$$.fragment,e),g(Be.$$.fragment,e),g(We.$$.fragment,e),g(Ee.$$.fragment,e),g(Pe.$$.fragment,e),g(Ne.$$.fragment,e),g(Fe.$$.fragment,e),g(Qe.$$.fragment,e),g(Re.$$.fragment,e),g(Ge.$$.fragment,e),g(He.$$.fragment,e),g(Se.$$.fragment,e),g(Xe.$$.fragment,e),g(Le.$$.fragment,e),g(De.$$.fragment,e),g(oe.$$.fragment,e),g(Ye.$$.fragment,e),g(Oe.$$.fragment,e),g(Ke.$$.fragment,e),g(se.$$.fragment,e),g(en.$$.fragment,e),g(nn.$$.fragment,e),g(tn.$$.fragment,e),g(ae.$$.fragment,e),g(on.$$.fragment,e),g(sn.$$.fragment,e),g(an.$$.fragment,e),g(re.$$.fragment,e),g(ie.$$.fragment,e),g(rn.$$.fragment,e),Zt=!1},d(e){e&&(t(b),t(M),t(y),t(j),t(V),t(Sn),t(Xn),t(me),t(Ln),t(pe),t(Dn),t(ue),t(Yn),t(ge),t(On),t(he),t(Kn),t(fe),t(et),t(Me),t(nt),t(tt),t(ot),t(Je),t(st),t(Te),t(at),t(rt),t(it),t(ve),t(lt),t(ct),t(dt),t(Ie),t(mt),t(pt),t(ut),t(C),t(gt),t(ht),t(B),t(ft),t(Mt),t(H),t(_t),t(yt),t(A),t(Jt),t(Tt),t(v),t(bt),t(wt),t(W),t(vt),t(jt),t($),t(Ut),t(It),t(x),t(Ct),t($t),t(k),t(xt),t(kt),t(S),t(zt),t(At),t(Gn)),t(c),h(ce,e),h(de,e),h(_e,e),h(ye,e),h(be,e),h(we,e),h(je,e),h(Ue,e),h(Ce,e),h($e,e),h(xe),h(L),h(ke,e),h(ze),h(Ae,e),h(Ze),h(Ve,e),h(qe),h(Be),h(We,e),h(Ee),h(Pe),h(Ne),h(Fe),h(Qe),h(Re),h(Ge,e),h(He),h(Se),h(Xe,e),h(Le),h(De),h(oe),h(Ye,e),h(Oe),h(Ke),h(se),h(en,e),h(nn),h(tn),h(ae),h(on,e),h(sn),h(an),h(re),h(ie),h(rn,e)}}}const Ws='{"title":"Janus","local":"janus","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage Example","local":"usage-example","sections":[{"title":"Single image inference","local":"single-image-inference","sections":[],"depth":3},{"title":"Multi image inference","local":"multi-image-inference","sections":[],"depth":3}],"depth":2},{"title":"Text to Image generation","local":"text-to-image-generation","sections":[],"depth":2},{"title":"JanusConfig","local":"transformers.JanusConfig","sections":[],"depth":2},{"title":"JanusVisionConfig","local":"transformers.JanusVisionConfig","sections":[],"depth":2},{"title":"JanusVQVAEConfig","local":"transformers.JanusVQVAEConfig","sections":[],"depth":2},{"title":"JanusProcessor","local":"transformers.JanusProcessor","sections":[],"depth":2},{"title":"JanusImageProcessor","local":"transformers.JanusImageProcessor","sections":[],"depth":2},{"title":"JanusImageProcessorFast","local":"transformers.JanusImageProcessorFast","sections":[],"depth":2},{"title":"JanusVisionModel","local":"transformers.JanusVisionModel","sections":[],"depth":2},{"title":"JanusVQVAE","local":"transformers.JanusVQVAE","sections":[],"depth":2},{"title":"JanusModel","local":"transformers.JanusModel","sections":[],"depth":2},{"title":"JanusForConditionalGeneration","local":"transformers.JanusForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function Es(z){return Us(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ss extends Is{constructor(c){super(),Cs(this,c,Es,Bs,js,{})}}export{Ss as component};
