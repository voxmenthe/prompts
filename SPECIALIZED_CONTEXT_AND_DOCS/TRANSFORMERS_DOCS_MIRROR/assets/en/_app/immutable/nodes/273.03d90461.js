import{s as sa,o as ra,n as q}from"../chunks/scheduler.18a86fab.js";import{S as ia,i as la,g as h,s,r as u,A as da,h as f,f as n,c as r,j as U,x as T,u as g,k as I,l as ma,y as c,a as l,v as b,d as M,t as _,w as y}from"../chunks/index.98837b22.js";import{T as Gt}from"../chunks/Tip.77304350.js";import{D as H}from"../chunks/Docstring.a1ef7999.js";import{C as A}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Xt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as He,E as ca}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as pa,a as Rt}from"../chunks/HfOption.6641485e.js";function ha($){let t,p=`This model was contributed by <a href="https://huggingface.co/Molbap" rel="nofollow">Molbap</a> and <a href="https://huggingface.co/AntonV" rel="nofollow">AntonV</a>.
Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language tasks.`;return{c(){t=h("p"),t.innerHTML=p},l(a){t=f(a,"P",{"data-svelte-h":!0}),T(t)!=="svelte-115em2i"&&(t.innerHTML=p)},m(a,d){l(a,t,d)},p:q,d(a){a&&n(t)}}}function fa($){let t,p;return t=new A({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMEElMjAlMjAlMjAlMjBtb2RlbCUzRCUyMnN0YXRlLXNwYWNlcyUyRm1hbWJhLTEzMG0taGYlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2UlM0QwJTBBKSUwQXBpcGVsaW5lKCUyMlBsYW50cyUyMGNyZWF0ZSUyMGVuZXJneSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;text-generation&quot;</span>,
    model=<span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(a){g(t.$$.fragment,a)},m(a,d){b(t,a,d),p=!0},p:q,i(a){p||(M(t.$$.fragment,a),p=!0)},o(a){_(t.$$.fragment,a),p=!1},d(a){y(t,a)}}}function ua($){let t,p;return t=new A({props:{code:"aW1wb3J0JTIwdG9yY2glMjAlMjAlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTIwJTIwJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyc3RhdGUtc3BhY2VzJTJGbWFtYmEtMTMwbS1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJzdGF0ZS1zcGFjZXMlMkZtYW1iYS0xMzBtLWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMpJTIwJTIwJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMlBsYW50cyUyMGNyZWF0ZSUyMGVuZXJneSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTIwJTIwJTBBJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMpJTIwJTIwJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUp",highlighted:`<span class="hljs-keyword">import</span> torch  
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer  

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>,)  
input_ids = tokenizer(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)  

output = model.generate(**input_ids)  
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(a){g(t.$$.fragment,a)},m(a,d){b(t,a,d),p=!0},p:q,i(a){p||(M(t.$$.fragment,a),p=!0)},o(a){_(t.$$.fragment,a),p=!1},d(a){y(t,a)}}}function ga($){let t,p;return t=new A({props:{code:"ZWNobyUyMC1lJTIwJTIyUGxhbnRzJTIwY3JlYXRlJTIwZW5lcmd5JTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMiUyMCU3QyUyMHRyYW5zZm9ybWVycyUyMHJ1biUyMC0tdGFzayUyMHRleHQtZ2VuZXJhdGlvbiUyMC0tbW9kZWwlMjBzdGF0ZS1zcGFjZXMlMkZtYW1iYS0xMzBtLWhmJTIwLS1kZXZpY2UlMjAw",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span> | transformers run --task text-generation --model state-spaces/mamba-130m-hf --device 0',wrap:!1}}),{c(){u(t.$$.fragment)},l(a){g(t.$$.fragment,a)},m(a,d){b(t,a,d),p=!0},p:q,i(a){p||(M(t.$$.fragment,a),p=!0)},o(a){_(t.$$.fragment,a),p=!1},d(a){y(t,a)}}}function ba($){let t,p,a,d,w,i;return t=new Rt({props:{id:"usage",option:"Pipeline",$$slots:{default:[fa]},$$scope:{ctx:$}}}),a=new Rt({props:{id:"usage",option:"AutoModel",$$slots:{default:[ua]},$$scope:{ctx:$}}}),w=new Rt({props:{id:"usage",option:"transformers CLI",$$slots:{default:[ga]},$$scope:{ctx:$}}}),{c(){u(t.$$.fragment),p=s(),u(a.$$.fragment),d=s(),u(w.$$.fragment)},l(m){g(t.$$.fragment,m),p=r(m),g(a.$$.fragment,m),d=r(m),g(w.$$.fragment,m)},m(m,v){b(t,m,v),l(m,p,v),b(a,m,v),l(m,d,v),b(w,m,v),i=!0},p(m,v){const qe={};v&2&&(qe.$$scope={dirty:v,ctx:m}),t.$set(qe);const D={};v&2&&(D.$$scope={dirty:v,ctx:m}),a.$set(D);const F={};v&2&&(F.$$scope={dirty:v,ctx:m}),w.$set(F)},i(m){i||(M(t.$$.fragment,m),M(a.$$.fragment,m),M(w.$$.fragment,m),i=!0)},o(m){_(t.$$.fragment,m),_(a.$$.fragment,m),_(w.$$.fragment,m),i=!1},d(m){m&&(n(p),n(d)),y(t,m),y(a,m),y(w,m)}}}function Ma($){let t,p="Example:",a,d,w;return d=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNYW1iYUZvckNhdXNhbExNJTJDJTIwTWFtYmFDYWNoZSUwQSUwQW1vZGVsJTIwJTNEJTIwTWFtYmFGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhdGUtc3BhY2VzJTJGbWFtYmEtMTMwbS1oZiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGF0ZS1zcGFjZXMlMkZtYW1iYS0xMzBtLWhmJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcih0ZXh0JTNEJTIyTXklMjBuYW1lJTIwaXMlMjBNYW1iYSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwUHJlcGFyZSUyMGElMjBjYWNoZSUyMGNsYXNzJTIwYW5kJTIwcGFzcyUyMGl0JTIwdG8lMjBtb2RlbCdzJTIwZm9yd2FyZCUwQXBhc3Rfa2V5X3ZhbHVlcyUyMCUzRCUyME1hbWJhQ2FjaGUoY29uZmlnJTNEbW9kZWwuY29uZmlnJTJDJTIwbWF4X2JhdGNoX3NpemUlM0QxJTJDJTIwZGV2aWNlJTNEbW9kZWwuZGV2aWNlJTJDJTIwZHR5cGUlM0Rtb2RlbC5kdHlwZSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjBwYXN0X2tleV92YWx1ZXMlM0RwYXN0X2tleV92YWx1ZXMlMkMlMjB1c2VfY2FjaGUlM0RUcnVlKSUwQW91dHB1dHMucGFzdF9rZXlfdmFsdWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MambaForCausalLM, MambaCache

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MambaForCausalLM.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text=<span class="hljs-string">&quot;My name is Mamba&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Prepare a cache class and pass it to model&#x27;s forward</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>past_key_values = MambaCache(config=model.config, max_batch_size=<span class="hljs-number">1</span>, device=model.device, dtype=model.dtype)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, past_key_values=past_key_values, use_cache=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs.past_key_values
MambaCache()`,wrap:!1}}),{c(){t=h("p"),t.textContent=p,a=s(),u(d.$$.fragment)},l(i){t=f(i,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=p),a=r(i),g(d.$$.fragment,i)},m(i,m){l(i,t,m),l(i,a,m),b(d,i,m),w=!0},p:q,i(i){w||(M(d.$$.fragment,i),w=!0)},o(i){_(d.$$.fragment,i),w=!1},d(i){i&&(n(t),n(a)),y(d,i)}}}function _a($){let t,p="Example:",a,d,w;return d=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1hbWJhQ29uZmlnJTJDJTIwTWFtYmFNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBNYW1iYSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTWFtYmFDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwTWFtYmFNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MambaConfig, MambaModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Mamba configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MambaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MambaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=h("p"),t.textContent=p,a=s(),u(d.$$.fragment)},l(i){t=f(i,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=p),a=r(i),g(d.$$.fragment,i)},m(i,m){l(i,t,m),l(i,a,m),b(d,i,m),w=!0},p:q,i(i){w||(M(d.$$.fragment,i),w=!0)},o(i){_(d.$$.fragment,i),w=!1},d(i){i&&(n(t),n(a)),y(d,i)}}}function ya($){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=p},l(a){t=f(a,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(a,d){l(a,t,d)},p:q,d(a){a&&n(t)}}}function wa($){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=p},l(a){t=f(a,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(a,d){l(a,t,d)},p:q,d(a){a&&n(t)}}}function Ta($){let t,p="Example:",a,d,w;return d=new A({props:{code:"",highlighted:"",wrap:!1}}),{c(){t=h("p"),t.textContent=p,a=s(),u(d.$$.fragment)},l(i){t=f(i,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=p),a=r(i),g(d.$$.fragment,i)},m(i,m){l(i,t,m),l(i,a,m),b(d,i,m),w=!0},p:q,i(i){w||(M(d.$$.fragment,i),w=!0)},o(i){_(d.$$.fragment,i),w=!1},d(i){i&&(n(t),n(a)),y(d,i)}}}function $a($){let t,p,a,d,w,i="<em>This model was released on 2023-12-01 and added to Hugging Face Transformers on 2024-03-05.</em>",m,v,qe='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',D,F,Ye,O,Ht='<a href="https://huggingface.co/papers/2312.00752" rel="nofollow">Mamba</a> is a selective structured state space model (SSMs) designed to work around Transformers computational inefficiency when dealing with long sequences.  It is a completely attention-free architecture, and comprised of a combination of H3 and gated MLP blocks (Mamba block). Mamba’s “content-based reasoning” allows it to focus on specific parts of an input depending on the current token. Mamba also uses a new hardware-aware parallel algorithm to compensate for the lack of convolutional operations. As a result, Mamba has fast inference and can scale to very long sequences.',Se,K,qt='You can find all the original Mamba checkpoints under the <a href="https://huggingface.co/state-spaces" rel="nofollow">State Space Models</a> organization.',Pe,N,Qe,ee,Nt='The example below demonstrates how to generate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',Ae,V,De,te,Vt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',Oe,ae,Et='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to 4-bit integers.',Ke,oe,et,ne,tt,W,Te,Bt='<p>The current implementation uses the original CUDA kernels. The FlashAttention equivalent implementation is hosted in the <a href="https://github.com/state-spaces/mamba" rel="nofollow">mamba-ssm</a> and <a href="https://github.com/Dao-AILab/causal-conv1d" rel="nofollow">causal_conv1d</a> repositories. Make sure to install them if your hardware supports it!</p>',ht,$e,Yt="<p>Mamba stacks <code>mixer</code> layers which are equivalent to <code>Attention</code> layers. You can find the main logic of Mamba in the <code>MambaMixer</code> class.</p>",ft,se,ve,St='The example below demonstrates how to fine-tune Mamba with <a href="https://huggingface.co/docs/peft" rel="nofollow">PEFT</a>.',ut,re,at,ie,ot,C,le,gt,Ce,Pt="Cache for mamba model which does not have attention mechanism and key value states.",bt,E,Mt,ke,de,_t,Je,me,yt,xe,ce,nt,pe,st,j,he,wt,je,Qt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a>. It is used to instantiate a MAMBA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the MAMBA
<a href="https://huggingface.co/state-spaces/mamba-2.8b" rel="nofollow">state-spaces/mamba-2.8b</a> architecture.`,Tt,Ue,At=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,$t,B,rt,fe,it,k,ue,vt,Ze,Dt="The bare Mamba Model outputting raw hidden-states without any specific head on top.",Ct,ze,Ot=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,kt,Ie,Kt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Jt,L,ge,xt,Fe,ea='The <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a> forward method, overrides the <code>__call__</code> special method.',jt,Y,lt,be,dt,J,Me,Ut,We,ta=`The MAMBA Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`,Zt,Le,aa=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,zt,Ge,oa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,It,Z,_e,Ft,Xe,na='The <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',Wt,S,Lt,P,mt,ye,ct,Ne,pt;return F=new He({props:{title:"Mamba",local:"mamba",headingTag:"h1"}}),N=new Gt({props:{warning:!1,$$slots:{default:[ha]},$$scope:{ctx:$}}}),V=new pa({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[ba]},$$scope:{ctx:$}}}),oe=new A({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwVG9yY2hBb0NvbmZpZyUwQWZyb20lMjB0b3JjaGFvLnF1YW50aXphdGlvbiUyMGltcG9ydCUyMEludDRXZWlnaHRPbmx5Q29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEludDRXZWlnaHRPbmx5Q29uZmlnKGdyb3VwX3NpemUlM0QxMjgpJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMFRvcmNoQW9Db25maWcocXVhbnRfdHlwZSUzRHF1YW50X2NvbmZpZyklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGF0ZS1zcGFjZXMlMkZtYW1iYS0yLjhiLWhmJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMnN0YXRlLXNwYWNlcyUyRm1hbWJhLTIuOGItaGYlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyklMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyUGxhbnRzJTIwY3JlYXRlJTIwZW5lcmd5JTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFvdXRwdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0X2lkcyklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dCU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, TorchAoConfig
<span class="hljs-keyword">from</span> torchao.quantization <span class="hljs-keyword">import</span> Int4WeightOnlyConfig

quantization_config = Int4WeightOnlyConfig(group_size=<span class="hljs-number">128</span>)
quantization_config = TorchAoConfig(quant_type=quant_config)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-2.8b-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;state-spaces/mamba-2.8b-hf&quot;</span>, dtype=torch.bfloat16, quantization_config=quantization_config, device_map=<span class="hljs-string">&quot;auto&quot;</span>,)
input_ids = tokenizer(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

output = model.generate(**input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),ne=new He({props:{title:"Notes",local:"notes",headingTag:"h2"}}),re=new A({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRybCUyMGltcG9ydCUyMFNGVENvbmZpZyUyQyUyMFNGVFRyYWluZXIlMEFmcm9tJTIwcGVmdCUyMGltcG9ydCUyMExvcmFDb25maWclMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMnN0YXRlLXNwYWNlcyUyRm1hbWJhLTEzMG0taGYlMjIlMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMkFiaXJhdGUlMkZlbmdsaXNoX3F1b3RlcyUyMiUyQyUyMHNwbGl0JTNEJTIydHJhaW4lMjIpJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFNGVENvbmZpZyhkYXRhc2V0X3RleHRfZmllbGQlM0QlMjJxdW90ZSUyMiklMEFsb3JhX2NvbmZpZyUyMCUzRCUyMCUyMExvcmFDb25maWcodGFyZ2V0X21vZHVsZXMlM0QlNUIlMjJ4X3Byb2olMjIlMkMlMjAlMjJlbWJlZGRpbmdzJTIyJTJDJTIwJTIyaW5fcHJvaiUyMiUyQyUyMCUyMm91dF9wcm9qJTIyJTVEKSUwQXRyYWluZXIlMjAlM0QlMjBTRlRUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWxfaWQlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0RkYXRhc2V0JTJDJTBBJTIwJTIwJTIwJTIwcGVmdF9jb25maWclM0Rsb3JhX2NvbmZpZyUyQyUwQSklMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTConfig, SFTTrainer
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

model_id = <span class="hljs-string">&quot;state-spaces/mamba-130m-hf&quot;</span>
dataset = load_dataset(<span class="hljs-string">&quot;Abirate/english_quotes&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
training_args = SFTConfig(dataset_text_field=<span class="hljs-string">&quot;quote&quot;</span>)
lora_config =  LoraConfig(target_modules=[<span class="hljs-string">&quot;x_proj&quot;</span>, <span class="hljs-string">&quot;embeddings&quot;</span>, <span class="hljs-string">&quot;in_proj&quot;</span>, <span class="hljs-string">&quot;out_proj&quot;</span>])
trainer = SFTTrainer(
    model=model_id,
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,
)
trainer.train()`,wrap:!1}}),ie=new He({props:{title:"MambaCache",local:"transformers.MambaCache",headingTag:"h2"}}),le=new H({props:{name:"class transformers.MambaCache",anchor:"transformers.MambaCache",parameters:[{name:"config",val:": PretrainedConfig"},{name:"max_batch_size",val:": int"},{name:"dtype",val:": dtype = torch.float16"},{name:"device",val:": typing.Union[torch.device, str, NoneType] = None"}],parametersDescription:[{anchor:"transformers.MambaCache.config",description:"<strong>config</strong> (`PretrainedConfig) &#x2014;\nThe configuration file defining the shape-related attributes required to initialize the static cache.",name:"config"},{anchor:"transformers.MambaCache.max_batch_size",description:`<strong>max_batch_size</strong> (<code>int</code>) &#x2014;
The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.`,name:"max_batch_size"},{anchor:"transformers.MambaCache.dtype",description:`<strong>dtype</strong> (<code>torch.dtype</code>, <em>optional</em>, defaults to <code>torch.float16</code>) &#x2014;
The default <code>dtype</code> to use when initializing the layer.`,name:"dtype"},{anchor:"transformers.MambaCache.device",description:`<strong>device</strong> (<code>torch.device</code> or <code>str</code>, <em>optional</em>) &#x2014;
The device on which the cache should be initialized. Should be the same as the layer.`,name:"device"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L59"}}),E=new Xt({props:{anchor:"transformers.MambaCache.example",$$slots:{default:[Ma]},$$scope:{ctx:$}}}),de=new H({props:{name:"update_conv_state",anchor:"transformers.MambaCache.update_conv_state",parameters:[{name:"layer_idx",val:": int"},{name:"new_conv_state",val:": Tensor"},{name:"cache_position",val:": LongTensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L131"}}),me=new H({props:{name:"update_ssm_state",anchor:"transformers.MambaCache.update_ssm_state",parameters:[{name:"layer_idx",val:": int"},{name:"new_ssm_state",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L148"}}),ce=new H({props:{name:"reset",anchor:"transformers.MambaCache.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L153"}}),pe=new He({props:{title:"MambaConfig",local:"transformers.MambaConfig",headingTag:"h2"}}),he=new H({props:{name:"class transformers.MambaConfig",anchor:"transformers.MambaConfig",parameters:[{name:"vocab_size",val:" = 50280"},{name:"hidden_size",val:" = 768"},{name:"state_size",val:" = 16"},{name:"num_hidden_layers",val:" = 32"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 0"},{name:"expand",val:" = 2"},{name:"conv_kernel",val:" = 4"},{name:"use_bias",val:" = False"},{name:"use_conv_bias",val:" = True"},{name:"hidden_act",val:" = 'silu'"},{name:"initializer_range",val:" = 0.1"},{name:"residual_in_fp32",val:" = True"},{name:"time_step_rank",val:" = 'auto'"},{name:"time_step_scale",val:" = 1.0"},{name:"time_step_min",val:" = 0.001"},{name:"time_step_max",val:" = 0.1"},{name:"time_step_init_scheme",val:" = 'random'"},{name:"time_step_floor",val:" = 0.0001"},{name:"rescale_prenorm_residual",val:" = False"},{name:"use_cache",val:" = True"},{name:"use_mambapy",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MambaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50280) &#x2014;
Vocabulary size of the MAMBA model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a>.`,name:"vocab_size"},{anchor:"transformers.MambaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the embeddings and hidden states.`,name:"hidden_size"},{anchor:"transformers.MambaConfig.state_size",description:"<strong>state_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014; shape of the state space latents.",name:"state_size"},{anchor:"transformers.MambaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the model.`,name:"num_hidden_layers"},{anchor:"transformers.MambaConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon to use in the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.MambaConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.MambaConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The id of the beginning of sentence token in the vocabulary.`,name:"bos_token_id"},{anchor:"transformers.MambaConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The id of the end of sentence token in the vocabulary.`,name:"eos_token_id"},{anchor:"transformers.MambaConfig.expand",description:"<strong>expand</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; Expanding factor used to determine the intermediate size.",name:"expand"},{anchor:"transformers.MambaConfig.conv_kernel",description:"<strong>conv_kernel</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; Size of the convolution kernel.",name:"conv_kernel"},{anchor:"transformers.MambaConfig.use_bias",description:`<strong>use_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use bias in [&#x201C;in_proj&#x201D;, &#x201C;out_proj&#x201D;] of the mixer block`,name:"use_bias"},{anchor:"transformers.MambaConfig.use_conv_bias",description:`<strong>use_conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the convolution layer of the mixer block.`,name:"use_conv_bias"},{anchor:"transformers.MambaConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.MambaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MambaConfig.residual_in_fp32",description:`<strong>residual_in_fp32</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not residuals should be in <code>float32</code>. If set to <code>False</code> residuals will keep the same <code>dtype</code> as the rest of the model`,name:"residual_in_fp32"},{anchor:"transformers.MambaConfig.time_step_rank",description:`<strong>time_step_rank</strong> (<code>Union[int,str]</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
Rank of the discretization projection matrix. <code>&quot;auto&quot;</code> means that it will default to <code>math.ceil(self.hidden_size / 16)</code>`,name:"time_step_rank"},{anchor:"transformers.MambaConfig.time_step_scale",description:`<strong>time_step_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Scale used used to scale <code>dt_proj.bias</code>.`,name:"time_step_scale"},{anchor:"transformers.MambaConfig.time_step_min",description:`<strong>time_step_min</strong> (<code>float</code>, <em>optional</em>, defaults to 0.001) &#x2014;
Minimum <code>time_step</code> used to bound <code>dt_proj.bias</code>.`,name:"time_step_min"},{anchor:"transformers.MambaConfig.time_step_max",description:`<strong>time_step_max</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Maximum <code>time_step</code> used to bound <code>dt_proj.bias</code>.`,name:"time_step_max"},{anchor:"transformers.MambaConfig.time_step_init_scheme",description:`<strong>time_step_init_scheme</strong> (<code>float</code>, <em>optional</em>, defaults to <code>&quot;random&quot;</code>) &#x2014;
Init scheme used for <code>dt_proj.weight</code>. Should be one of <code>[&quot;random&quot;,&quot;uniform&quot;]</code>`,name:"time_step_init_scheme"},{anchor:"transformers.MambaConfig.time_step_floor",description:`<strong>time_step_floor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0001) &#x2014;
Minimum clamping value of the <code>dt_proj.bias</code> layer initialization.`,name:"time_step_floor"},{anchor:"transformers.MambaConfig.rescale_prenorm_residual",description:`<strong>rescale_prenorm_residual</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale <code>out_proj</code> weights when initializing.`,name:"rescale_prenorm_residual"},{anchor:"transformers.MambaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the cache should be used.`,name:"use_cache"},{anchor:"transformers.MambaConfig.use_mambapy",description:`<strong>use_mambapy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not available. If <code>True</code>, the mamba.py implementation is used. If <code>False</code>, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.`,name:"use_mambapy"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/configuration_mamba.py#L26"}}),B=new Xt({props:{anchor:"transformers.MambaConfig.example",$$slots:{default:[_a]},$$scope:{ctx:$}}}),fe=new He({props:{title:"MambaModel",local:"transformers.MambaModel",headingTag:"h2"}}),ue=new H({props:{name:"class transformers.MambaModel",anchor:"transformers.MambaModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MambaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaModel">MambaModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L596"}}),ge=new H({props:{name:"forward",anchor:"transformers.MambaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.LongTensor] = None"},{name:"cache_params",val:": typing.Optional[transformers.models.mamba.modeling_mamba.MambaCache] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.MambaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MambaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MambaModel.forward.cache_params",description:`<strong>cache_params</strong> (<code>MambaCache</code>, <em>optional</em>) &#x2014;
If passed along, the model uses the previous state in all the blocks (which will give the output for the
<code>input_ids</code> provided as if the model add <code>state_input_ids + input_ids</code> as context).`,name:"cache_params"},{anchor:"transformers.MambaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the <code>cache_params</code> is returned and can be used to quickly generate the next logits.`,name:"use_cache"},{anchor:"transformers.MambaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MambaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MambaModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.MambaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L621",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.mamba.modeling_mamba.MambaOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig"
>MambaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>cache_params</strong> (<code>~models.mamba.modeling_mamba.MambaCache</code>, <em>optional</em>, defaults to <code>None</code>) — The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.mamba.modeling_mamba.MambaOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Y=new Gt({props:{$$slots:{default:[ya]},$$scope:{ctx:$}}}),be=new He({props:{title:"MambaLMHeadModel",local:"transformers.MambaForCausalLM",headingTag:"h2"}}),Me=new H({props:{name:"class transformers.MambaForCausalLM",anchor:"transformers.MambaForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MambaForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaForCausalLM">MambaForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L707"}}),_e=new H({props:{name:"forward",anchor:"transformers.MambaForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"cache_params",val:": typing.Optional[transformers.models.mamba.modeling_mamba.MambaCache] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MambaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MambaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MambaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MambaForCausalLM.forward.cache_params",description:`<strong>cache_params</strong> (<code>MambaCache</code>, <em>optional</em>) &#x2014;
If passed along, the model uses the previous state in all the blocks (which will give the output for the
<code>input_ids</code> provided as if the model add <code>state_input_ids + input_ids</code> as context).`,name:"cache_params"},{anchor:"transformers.MambaForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.MambaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MambaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MambaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the <code>cache_params</code> is returned and can be used to quickly generate the next logits.`,name:"use_cache"},{anchor:"transformers.MambaForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba/modeling_mamba.py#L784",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.mamba.modeling_mamba.MambaCausalLMOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mamba#transformers.MambaConfig"
>MambaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>cache_params</strong> (<code>~models.mamba.modeling_mamba.MambaCache</code>, <em>optional</em>, defaults to <code>None</code>) — The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.mamba.modeling_mamba.MambaCausalLMOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new Gt({props:{$$slots:{default:[wa]},$$scope:{ctx:$}}}),P=new Xt({props:{anchor:"transformers.MambaForCausalLM.forward.example",$$slots:{default:[Ta]},$$scope:{ctx:$}}}),ye=new ca({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba.md"}}),{c(){t=h("meta"),p=s(),a=h("p"),d=s(),w=h("p"),w.innerHTML=i,m=s(),v=h("div"),v.innerHTML=qe,D=s(),u(F.$$.fragment),Ye=s(),O=h("p"),O.innerHTML=Ht,Se=s(),K=h("p"),K.innerHTML=qt,Pe=s(),u(N.$$.fragment),Qe=s(),ee=h("p"),ee.innerHTML=Nt,Ae=s(),u(V.$$.fragment),De=s(),te=h("p"),te.innerHTML=Vt,Oe=s(),ae=h("p"),ae.innerHTML=Et,Ke=s(),u(oe.$$.fragment),et=s(),u(ne.$$.fragment),tt=s(),W=h("ul"),Te=h("li"),Te.innerHTML=Bt,ht=s(),$e=h("li"),$e.innerHTML=Yt,ft=s(),se=h("li"),ve=h("p"),ve.innerHTML=St,ut=s(),u(re.$$.fragment),at=s(),u(ie.$$.fragment),ot=s(),C=h("div"),u(le.$$.fragment),gt=s(),Ce=h("p"),Ce.textContent=Pt,bt=s(),u(E.$$.fragment),Mt=s(),ke=h("div"),u(de.$$.fragment),_t=s(),Je=h("div"),u(me.$$.fragment),yt=s(),xe=h("div"),u(ce.$$.fragment),nt=s(),u(pe.$$.fragment),st=s(),j=h("div"),u(he.$$.fragment),wt=s(),je=h("p"),je.innerHTML=Qt,Tt=s(),Ue=h("p"),Ue.innerHTML=At,$t=s(),u(B.$$.fragment),rt=s(),u(fe.$$.fragment),it=s(),k=h("div"),u(ue.$$.fragment),vt=s(),Ze=h("p"),Ze.textContent=Dt,Ct=s(),ze=h("p"),ze.innerHTML=Ot,kt=s(),Ie=h("p"),Ie.innerHTML=Kt,Jt=s(),L=h("div"),u(ge.$$.fragment),xt=s(),Fe=h("p"),Fe.innerHTML=ea,jt=s(),u(Y.$$.fragment),lt=s(),u(be.$$.fragment),dt=s(),J=h("div"),u(Me.$$.fragment),Ut=s(),We=h("p"),We.textContent=ta,Zt=s(),Le=h("p"),Le.innerHTML=aa,zt=s(),Ge=h("p"),Ge.innerHTML=oa,It=s(),Z=h("div"),u(_e.$$.fragment),Ft=s(),Xe=h("p"),Xe.innerHTML=na,Wt=s(),u(S.$$.fragment),Lt=s(),u(P.$$.fragment),mt=s(),u(ye.$$.fragment),ct=s(),Ne=h("p"),this.h()},l(e){const o=da("svelte-u9bgzb",document.head);t=f(o,"META",{name:!0,content:!0}),o.forEach(n),p=r(e),a=f(e,"P",{}),U(a).forEach(n),d=r(e),w=f(e,"P",{"data-svelte-h":!0}),T(w)!=="svelte-y6ui3p"&&(w.innerHTML=i),m=r(e),v=f(e,"DIV",{style:!0,"data-svelte-h":!0}),T(v)!=="svelte-100e61"&&(v.innerHTML=qe),D=r(e),g(F.$$.fragment,e),Ye=r(e),O=f(e,"P",{"data-svelte-h":!0}),T(O)!=="svelte-10hop3f"&&(O.innerHTML=Ht),Se=r(e),K=f(e,"P",{"data-svelte-h":!0}),T(K)!=="svelte-1ezxumx"&&(K.innerHTML=qt),Pe=r(e),g(N.$$.fragment,e),Qe=r(e),ee=f(e,"P",{"data-svelte-h":!0}),T(ee)!=="svelte-17pa8jt"&&(ee.innerHTML=Nt),Ae=r(e),g(V.$$.fragment,e),De=r(e),te=f(e,"P",{"data-svelte-h":!0}),T(te)!=="svelte-nf5ooi"&&(te.innerHTML=Vt),Oe=r(e),ae=f(e,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-1bwmz6y"&&(ae.innerHTML=Et),Ke=r(e),g(oe.$$.fragment,e),et=r(e),g(ne.$$.fragment,e),tt=r(e),W=f(e,"UL",{});var R=U(W);Te=f(R,"LI",{"data-svelte-h":!0}),T(Te)!=="svelte-nawk27"&&(Te.innerHTML=Bt),ht=r(R),$e=f(R,"LI",{"data-svelte-h":!0}),T($e)!=="svelte-1y0e6y6"&&($e.innerHTML=Yt),ft=r(R),se=f(R,"LI",{});var we=U(se);ve=f(we,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-1v5uyx2"&&(ve.innerHTML=St),ut=r(we),g(re.$$.fragment,we),we.forEach(n),R.forEach(n),at=r(e),g(ie.$$.fragment,e),ot=r(e),C=f(e,"DIV",{class:!0});var x=U(C);g(le.$$.fragment,x),gt=r(x),Ce=f(x,"P",{"data-svelte-h":!0}),T(Ce)!=="svelte-plgsbf"&&(Ce.textContent=Pt),bt=r(x),g(E.$$.fragment,x),Mt=r(x),ke=f(x,"DIV",{class:!0});var Ve=U(ke);g(de.$$.fragment,Ve),Ve.forEach(n),_t=r(x),Je=f(x,"DIV",{class:!0});var Ee=U(Je);g(me.$$.fragment,Ee),Ee.forEach(n),yt=r(x),xe=f(x,"DIV",{class:!0});var Be=U(xe);g(ce.$$.fragment,Be),Be.forEach(n),x.forEach(n),nt=r(e),g(pe.$$.fragment,e),st=r(e),j=f(e,"DIV",{class:!0});var z=U(j);g(he.$$.fragment,z),wt=r(z),je=f(z,"P",{"data-svelte-h":!0}),T(je)!=="svelte-rf7qlh"&&(je.innerHTML=Qt),Tt=r(z),Ue=f(z,"P",{"data-svelte-h":!0}),T(Ue)!=="svelte-1ek1ss9"&&(Ue.innerHTML=At),$t=r(z),g(B.$$.fragment,z),z.forEach(n),rt=r(e),g(fe.$$.fragment,e),it=r(e),k=f(e,"DIV",{class:!0});var G=U(k);g(ue.$$.fragment,G),vt=r(G),Ze=f(G,"P",{"data-svelte-h":!0}),T(Ze)!=="svelte-gm7jra"&&(Ze.textContent=Dt),Ct=r(G),ze=f(G,"P",{"data-svelte-h":!0}),T(ze)!=="svelte-q52n56"&&(ze.innerHTML=Ot),kt=r(G),Ie=f(G,"P",{"data-svelte-h":!0}),T(Ie)!=="svelte-hswkmf"&&(Ie.innerHTML=Kt),Jt=r(G),L=f(G,"DIV",{class:!0});var Re=U(L);g(ge.$$.fragment,Re),xt=r(Re),Fe=f(Re,"P",{"data-svelte-h":!0}),T(Fe)!=="svelte-19xzafp"&&(Fe.innerHTML=ea),jt=r(Re),g(Y.$$.fragment,Re),Re.forEach(n),G.forEach(n),lt=r(e),g(be.$$.fragment,e),dt=r(e),J=f(e,"DIV",{class:!0});var X=U(J);g(Me.$$.fragment,X),Ut=r(X),We=f(X,"P",{"data-svelte-h":!0}),T(We)!=="svelte-xhqj3f"&&(We.textContent=ta),Zt=r(X),Le=f(X,"P",{"data-svelte-h":!0}),T(Le)!=="svelte-q52n56"&&(Le.innerHTML=aa),zt=r(X),Ge=f(X,"P",{"data-svelte-h":!0}),T(Ge)!=="svelte-hswkmf"&&(Ge.innerHTML=oa),It=r(X),Z=f(X,"DIV",{class:!0});var Q=U(Z);g(_e.$$.fragment,Q),Ft=r(Q),Xe=f(Q,"P",{"data-svelte-h":!0}),T(Xe)!=="svelte-1ir0qg1"&&(Xe.innerHTML=na),Wt=r(Q),g(S.$$.fragment,Q),Lt=r(Q),g(P.$$.fragment,Q),Q.forEach(n),X.forEach(n),mt=r(e),g(ye.$$.fragment,e),ct=r(e),Ne=f(e,"P",{}),U(Ne).forEach(n),this.h()},h(){I(t,"name","hf:doc:metadata"),I(t,"content",va),ma(v,"float","right"),I(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){c(document.head,t),l(e,p,o),l(e,a,o),l(e,d,o),l(e,w,o),l(e,m,o),l(e,v,o),l(e,D,o),b(F,e,o),l(e,Ye,o),l(e,O,o),l(e,Se,o),l(e,K,o),l(e,Pe,o),b(N,e,o),l(e,Qe,o),l(e,ee,o),l(e,Ae,o),b(V,e,o),l(e,De,o),l(e,te,o),l(e,Oe,o),l(e,ae,o),l(e,Ke,o),b(oe,e,o),l(e,et,o),b(ne,e,o),l(e,tt,o),l(e,W,o),c(W,Te),c(W,ht),c(W,$e),c(W,ft),c(W,se),c(se,ve),c(se,ut),b(re,se,null),l(e,at,o),b(ie,e,o),l(e,ot,o),l(e,C,o),b(le,C,null),c(C,gt),c(C,Ce),c(C,bt),b(E,C,null),c(C,Mt),c(C,ke),b(de,ke,null),c(C,_t),c(C,Je),b(me,Je,null),c(C,yt),c(C,xe),b(ce,xe,null),l(e,nt,o),b(pe,e,o),l(e,st,o),l(e,j,o),b(he,j,null),c(j,wt),c(j,je),c(j,Tt),c(j,Ue),c(j,$t),b(B,j,null),l(e,rt,o),b(fe,e,o),l(e,it,o),l(e,k,o),b(ue,k,null),c(k,vt),c(k,Ze),c(k,Ct),c(k,ze),c(k,kt),c(k,Ie),c(k,Jt),c(k,L),b(ge,L,null),c(L,xt),c(L,Fe),c(L,jt),b(Y,L,null),l(e,lt,o),b(be,e,o),l(e,dt,o),l(e,J,o),b(Me,J,null),c(J,Ut),c(J,We),c(J,Zt),c(J,Le),c(J,zt),c(J,Ge),c(J,It),c(J,Z),b(_e,Z,null),c(Z,Ft),c(Z,Xe),c(Z,Wt),b(S,Z,null),c(Z,Lt),b(P,Z,null),l(e,mt,o),b(ye,e,o),l(e,ct,o),l(e,Ne,o),pt=!0},p(e,[o]){const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),N.$set(R);const we={};o&2&&(we.$$scope={dirty:o,ctx:e}),V.$set(we);const x={};o&2&&(x.$$scope={dirty:o,ctx:e}),E.$set(x);const Ve={};o&2&&(Ve.$$scope={dirty:o,ctx:e}),B.$set(Ve);const Ee={};o&2&&(Ee.$$scope={dirty:o,ctx:e}),Y.$set(Ee);const Be={};o&2&&(Be.$$scope={dirty:o,ctx:e}),S.$set(Be);const z={};o&2&&(z.$$scope={dirty:o,ctx:e}),P.$set(z)},i(e){pt||(M(F.$$.fragment,e),M(N.$$.fragment,e),M(V.$$.fragment,e),M(oe.$$.fragment,e),M(ne.$$.fragment,e),M(re.$$.fragment,e),M(ie.$$.fragment,e),M(le.$$.fragment,e),M(E.$$.fragment,e),M(de.$$.fragment,e),M(me.$$.fragment,e),M(ce.$$.fragment,e),M(pe.$$.fragment,e),M(he.$$.fragment,e),M(B.$$.fragment,e),M(fe.$$.fragment,e),M(ue.$$.fragment,e),M(ge.$$.fragment,e),M(Y.$$.fragment,e),M(be.$$.fragment,e),M(Me.$$.fragment,e),M(_e.$$.fragment,e),M(S.$$.fragment,e),M(P.$$.fragment,e),M(ye.$$.fragment,e),pt=!0)},o(e){_(F.$$.fragment,e),_(N.$$.fragment,e),_(V.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(E.$$.fragment,e),_(de.$$.fragment,e),_(me.$$.fragment,e),_(ce.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(B.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(ge.$$.fragment,e),_(Y.$$.fragment,e),_(be.$$.fragment,e),_(Me.$$.fragment,e),_(_e.$$.fragment,e),_(S.$$.fragment,e),_(P.$$.fragment,e),_(ye.$$.fragment,e),pt=!1},d(e){e&&(n(p),n(a),n(d),n(w),n(m),n(v),n(D),n(Ye),n(O),n(Se),n(K),n(Pe),n(Qe),n(ee),n(Ae),n(De),n(te),n(Oe),n(ae),n(Ke),n(et),n(tt),n(W),n(at),n(ot),n(C),n(nt),n(st),n(j),n(rt),n(it),n(k),n(lt),n(dt),n(J),n(mt),n(ct),n(Ne)),n(t),y(F,e),y(N,e),y(V,e),y(oe,e),y(ne,e),y(re),y(ie,e),y(le),y(E),y(de),y(me),y(ce),y(pe,e),y(he),y(B),y(fe,e),y(ue),y(ge),y(Y),y(be,e),y(Me),y(_e),y(S),y(P),y(ye,e)}}}const va='{"title":"Mamba","local":"mamba","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"MambaCache","local":"transformers.MambaCache","sections":[],"depth":2},{"title":"MambaConfig","local":"transformers.MambaConfig","sections":[],"depth":2},{"title":"MambaModel","local":"transformers.MambaModel","sections":[],"depth":2},{"title":"MambaLMHeadModel","local":"transformers.MambaForCausalLM","sections":[],"depth":2}],"depth":1}';function Ca($){return ra(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fa extends ia{constructor(t){super(),la(this,t,Ca,$a,sa,{})}}export{Fa as component};
