import{s as he,z as Je,o as $e,n as oe}from"../chunks/scheduler.18a86fab.js";import{S as be,i as ye,g as c,s as r,r as M,A as Te,h as d,f as a,c as m,j as se,x as Z,u as g,k as ne,l as we,y as le,a as i,v as h,d as J,t as $,w as b}from"../chunks/index.98837b22.js";import{T as Ze}from"../chunks/Tip.77304350.js";import{C as re}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ie,E as je}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as _e,a as ge}from"../chunks/HfOption.6641485e.js";function Ue(y){let t,o='Refer to the <a href="./beit">BEiT</a> docs for more examples of how to apply DiT to different vision tasks.';return{c(){t=c("p"),t.innerHTML=o},l(s){t=d(s,"P",{"data-svelte-h":!0}),Z(t)!=="svelte-1hsjalr"&&(t.innerHTML=o)},m(s,p){i(s,t,p)},p:oe,d(s){s&&a(t)}}}function We(y){let t,o;return t=new re({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJpbWFnZS1jbGFzc2lmaWNhdGlvbiUyMiUyQyUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEJTIybWljcm9zb2Z0JTJGZGl0LWJhc2UtZmluZXR1bmVkLXJ2bGNkaXAlMjIlMkMlMEElMjAlMjAlMjAlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2UlM0QwJTBBKSUwQXBpcGVsaW5lKCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnRyYW5zZm9ybWVycyUyRm1vZGVsX2RvYyUyRmRpdC1leGFtcGxlLmpwZyUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;image-classification&quot;</span>,
    model=<span class="hljs-string">&quot;microsoft/dit-base-finetuned-rvlcdip&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dit-example.jpg&quot;</span>)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){g(t.$$.fragment,s)},m(s,p){h(t,s,p),o=!0},p:oe,i(s){o||(J(t.$$.fragment,s),o=!0)},o(s){$(t.$$.fragment,s),o=!1},d(s){b(t,s)}}}function ke(y){let t,o;return t=new re({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTJDJTIwQXV0b0ltYWdlUHJvY2Vzc29yJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtaWNyb3NvZnQlMkZkaXQtYmFzZS1maW5ldHVuZWQtcnZsY2RpcCUyMiUyQyUwQSUyMCUyMCUyMCUyMHVzZV9mYXN0JTNEVHJ1ZSUyQyUwQSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1pY3Jvc29mdCUyRmRpdC1iYXNlLWZpbmV0dW5lZC1ydmxjZGlwJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEEpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGdHJhbnNmb3JtZXJzJTJGbW9kZWxfZG9jJTJGZGl0LWV4YW1wbGUuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBcHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heChkaW0lM0QtMSkuaXRlbSgpJTBBJTBBY2xhc3NfbGFiZWxzJTIwJTNEJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTBBcHJlZGljdGVkX2NsYXNzX2xhYmVsJTIwJTNEJTIwY2xhc3NfbGFiZWxzJTVCcHJlZGljdGVkX2NsYXNzX2lkJTVEJTBBcHJpbnQoZiUyMlRoZSUyMHByZWRpY3RlZCUyMGNsYXNzJTIwbGFiZWwlMjBpcyUzQSUyMCU3QnByZWRpY3RlZF9jbGFzc19sYWJlbCU3RCUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;microsoft/dit-base-finetuned-rvlcdip&quot;</span>,
    use_fast=<span class="hljs-literal">True</span>,
)
model = AutoModelForImageClassification.from_pretrained(
    <span class="hljs-string">&quot;microsoft/dit-base-finetuned-rvlcdip&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
)
url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dit-example.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
  logits = model(**inputs).logits
predicted_class_id = logits.argmax(dim=-<span class="hljs-number">1</span>).item()

class_labels = model.config.id2label
predicted_class_label = class_labels[predicted_class_id]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted class label is: <span class="hljs-subst">{predicted_class_label}</span>&quot;</span>)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){g(t.$$.fragment,s)},m(s,p){h(t,s,p),o=!0},p:oe,i(s){o||(J(t.$$.fragment,s),o=!0)},o(s){$(t.$$.fragment,s),o=!1},d(s){b(t,s)}}}function ve(y){let t,o,s,p;return t=new ge({props:{id:"usage",option:"Pipeline",$$slots:{default:[We]},$$scope:{ctx:y}}}),s=new ge({props:{id:"usage",option:"AutoModel",$$slots:{default:[ke]},$$scope:{ctx:y}}}),{c(){M(t.$$.fragment),o=r(),M(s.$$.fragment)},l(n){g(t.$$.fragment,n),o=m(n),g(s.$$.fragment,n)},m(n,f){h(t,n,f),i(n,o,f),h(s,n,f),p=!0},p(n,f){const j={};f&2&&(j.$$scope={dirty:f,ctx:n}),t.$set(j);const u={};f&2&&(u.$$scope={dirty:f,ctx:n}),s.$set(u)},i(n){p||(J(t.$$.fragment,n),J(s.$$.fragment,n),p=!0)},o(n){$(t.$$.fragment,n),$(s.$$.fragment,n),p=!1},d(n){n&&a(o),b(t,n),b(s,n)}}}function Be(y){let t,o,s,p,n,f="<em>This model was released on 2022-03-04 and added to Hugging Face Transformers on 2022-03-10.</em>",j,u,me='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',z,_,F,U,pe='<a href="https://huggingface.co/papers/2203.02378" rel="nofollow">DiT</a> is an image transformer pretrained on large-scale unlabeled document images. It learns to predict the missing visual tokens from a corrupted input image. The pretrained DiT model can be used as a backbone in other models for visual document tasks like document image classification and table detection.',x,Y,ce,Q,W,de='You can find all the original DiT checkpoints under the <a href="https://huggingface.co/microsoft?search_models=dit" rel="nofollow">Microsoft</a> organization.',L,T,S,k,fe='The example below demonstrates how to classify an image with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',q,w,A,v,P,I,B,N,ue="The pretrained DiT weights can be loaded in a [BEiT] model with a modeling head to predict visual tokens.",ae,C,D,G,K,R,Me='<li>Refer to this <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DiT/Inference_with_DiT_(Document_Image_Transformer)_for_document_image_classification.ipynb" rel="nofollow">notebook</a> for a document image classification inference example.</li>',O,X,ee,V,te;return _=new ie({props:{title:"DiT",local:"dit",headingTag:"h1"}}),T=new Ze({props:{warning:!1,$$slots:{default:[Ue]},$$scope:{ctx:y}}}),w=new _e({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[ve]},$$scope:{ctx:y}}}),v=new ie({props:{title:"Notes",local:"notes",headingTag:"h2"}}),C=new re({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBbW9kZWwlMjAlM0QlMjBCZWl0Rm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluaW5nKCUyMm1pY3Jvc29mdCUyRmRpdC1iYXNlJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BeitForMaskedImageModeling

model = BeitForMaskedImageModeling.from_pretraining(<span class="hljs-string">&quot;microsoft/dit-base&quot;</span>)`,wrap:!1}}),G=new ie({props:{title:"Resources",local:"resources",headingTag:"h2"}}),X=new je({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dit.md"}}),{c(){t=c("meta"),o=r(),s=c("p"),p=r(),n=c("p"),n.innerHTML=f,j=r(),u=c("div"),u.innerHTML=me,z=r(),M(_.$$.fragment),F=r(),U=c("p"),U.innerHTML=pe,x=r(),Y=c("img"),Q=r(),W=c("p"),W.innerHTML=de,L=r(),M(T.$$.fragment),S=r(),k=c("p"),k.innerHTML=fe,q=r(),M(w.$$.fragment),A=r(),M(v.$$.fragment),P=r(),I=c("ul"),B=c("li"),N=c("p"),N.textContent=ue,ae=r(),M(C.$$.fragment),D=r(),M(G.$$.fragment),K=r(),R=c("ul"),R.innerHTML=Me,O=r(),M(X.$$.fragment),ee=r(),V=c("p"),this.h()},l(e){const l=Te("svelte-u9bgzb",document.head);t=d(l,"META",{name:!0,content:!0}),l.forEach(a),o=m(e),s=d(e,"P",{}),se(s).forEach(a),p=m(e),n=d(e,"P",{"data-svelte-h":!0}),Z(n)!=="svelte-1bj8a3n"&&(n.innerHTML=f),j=m(e),u=d(e,"DIV",{style:!0,"data-svelte-h":!0}),Z(u)!=="svelte-wa5t4p"&&(u.innerHTML=me),z=m(e),g(_.$$.fragment,e),F=m(e),U=d(e,"P",{"data-svelte-h":!0}),Z(U)!=="svelte-1q8ga9m"&&(U.innerHTML=pe),x=m(e),Y=d(e,"IMG",{src:!0}),Q=m(e),W=d(e,"P",{"data-svelte-h":!0}),Z(W)!=="svelte-f29tzc"&&(W.innerHTML=de),L=m(e),g(T.$$.fragment,e),S=m(e),k=d(e,"P",{"data-svelte-h":!0}),Z(k)!=="svelte-7bwa3a"&&(k.innerHTML=fe),q=m(e),g(w.$$.fragment,e),A=m(e),g(v.$$.fragment,e),P=m(e),I=d(e,"UL",{});var E=se(I);B=d(E,"LI",{});var H=se(B);N=d(H,"P",{"data-svelte-h":!0}),Z(N)!=="svelte-195jiw"&&(N.textContent=ue),ae=m(H),g(C.$$.fragment,H),H.forEach(a),E.forEach(a),D=m(e),g(G.$$.fragment,e),K=m(e),R=d(e,"UL",{"data-svelte-h":!0}),Z(R)!=="svelte-1wonkxd"&&(R.innerHTML=Me),O=m(e),g(X.$$.fragment,e),ee=m(e),V=d(e,"P",{}),se(V).forEach(a),this.h()},h(){ne(t,"name","hf:doc:metadata"),ne(t,"content",Ce),we(u,"float","right"),Je(Y.src,ce="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dit_architecture.jpg")||ne(Y,"src",ce)},m(e,l){le(document.head,t),i(e,o,l),i(e,s,l),i(e,p,l),i(e,n,l),i(e,j,l),i(e,u,l),i(e,z,l),h(_,e,l),i(e,F,l),i(e,U,l),i(e,x,l),i(e,Y,l),i(e,Q,l),i(e,W,l),i(e,L,l),h(T,e,l),i(e,S,l),i(e,k,l),i(e,q,l),h(w,e,l),i(e,A,l),h(v,e,l),i(e,P,l),i(e,I,l),le(I,B),le(B,N),le(B,ae),h(C,B,null),i(e,D,l),h(G,e,l),i(e,K,l),i(e,R,l),i(e,O,l),h(X,e,l),i(e,ee,l),i(e,V,l),te=!0},p(e,[l]){const E={};l&2&&(E.$$scope={dirty:l,ctx:e}),T.$set(E);const H={};l&2&&(H.$$scope={dirty:l,ctx:e}),w.$set(H)},i(e){te||(J(_.$$.fragment,e),J(T.$$.fragment,e),J(w.$$.fragment,e),J(v.$$.fragment,e),J(C.$$.fragment,e),J(G.$$.fragment,e),J(X.$$.fragment,e),te=!0)},o(e){$(_.$$.fragment,e),$(T.$$.fragment,e),$(w.$$.fragment,e),$(v.$$.fragment,e),$(C.$$.fragment,e),$(G.$$.fragment,e),$(X.$$.fragment,e),te=!1},d(e){e&&(a(o),a(s),a(p),a(n),a(j),a(u),a(z),a(F),a(U),a(x),a(Y),a(Q),a(W),a(L),a(S),a(k),a(q),a(A),a(P),a(I),a(D),a(K),a(R),a(O),a(ee),a(V)),a(t),b(_,e),b(T,e),b(w,e),b(v,e),b(C),b(G,e),b(X,e)}}}const Ce='{"title":"DiT","local":"dit","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2}],"depth":1}';function Ge(y){return $e(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ve extends be{constructor(t){super(),ye(this,t,Ge,Be,he,{})}}export{Ve as component};
