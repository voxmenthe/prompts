import{s as Tn,o as Jn,n as k}from"../chunks/scheduler.18a86fab.js";import{S as wn,i as _n,g as b,s as r,r as u,A as Un,h as y,f as s,c as p,j as bn,u as f,x as T,k as yn,y as kn,a,v as c,d as M,t as g,w as h}from"../chunks/index.98837b22.js";import{T as $n}from"../chunks/Tip.77304350.js";import{C as w}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as Zn}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as U,E as jn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Rt,a as X}from"../chunks/HfOption.6641485e.js";function Gn($){let l,i='You can also chat with a model directly from the command line. (<a href="./conversations.md#transformers-cli">reference</a>)',n,d,o;return d=new w({props:{code:"dHJhbnNmb3JtZXJzJTIwY2hhdCUyMFF3ZW4lMkZRd2VuMi41LTAuNUItSW5zdHJ1Y3Q=",highlighted:"transformers chat Qwen/Qwen2.5-0.5B-Instruct",wrap:!1}}),{c(){l=b("p"),l.innerHTML=i,n=r(),u(d.$$.fragment)},l(m){l=y(m,"P",{"data-svelte-h":!0}),T(l)!=="svelte-18t31kj"&&(l.innerHTML=i),n=p(m),f(d.$$.fragment,m)},m(m,J){a(m,l,J),a(m,n,J),c(d,m,J),o=!0},p:k,i(m){o||(M(d.$$.fragment,m),o=!0)},o(m){g(d.$$.fragment,m),o=!1},d(m){m&&(s(l),s(n)),h(d,m)}}}function vn($){let l,i="Process more than one prompt at a time by passing a list of strings to the tokenizer. Batch the inputs to improve throughput at a small cost to latency and memory.";return{c(){l=b("p"),l.textContent=i},l(n){l=y(n,"P",{"data-svelte-h":!0}),T(l)!=="svelte-1jozp27"&&(l.textContent=i)},m(n,d){a(n,l,d)},p:k,d(n){n&&s(l)}}}function In($){let l,i;return l=new w({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEEnQSUyMHNlcXVlbmNlJTIwb2YlMjBudW1iZXJzJTNBJTIwMSUyQyUyMDIlMkMlMjAzJTJDJTIwNCUyQyUyMDUn",highlighted:`generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;A sequence of numbers: 1, 2, 3, 4, 5&#x27;</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Wn($){let l,i;return l=new w({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwbWF4X25ld190b2tlbnMlM0Q1MCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEJTBBJ0ElMjBzZXF1ZW5jZSUyMG9mJTIwbnVtYmVycyUzQSUyMDElMkMlMjAyJTJDJTIwMyUyQyUyMDQlMkMlMjA1JTJDJTIwNiUyQyUyMDclMkMlMjA4JTJDJTIwOSUyQyUyMDEwJTJDJTIwMTElMkMlMjAxMiUyQyUyMDEzJTJDJTIwMTQlMkMlMjAxNSUyQyUyMDE2JTJDJw==",highlighted:`generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">50</span>)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,&#x27;</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Xn($){let l,i,n,d;return l=new X({props:{id:"output-length",option:"default length",$$slots:{default:[In]},$$scope:{ctx:$}}}),n=new X({props:{id:"output-length",option:"max_new_tokens",$$slots:{default:[Wn]},$$scope:{ctx:$}}}),{c(){u(l.$$.fragment),i=r(),u(n.$$.fragment)},l(o){f(l.$$.fragment,o),i=p(o),f(n.$$.fragment,o)},m(o,m){c(l,o,m),a(o,i,m),c(n,o,m),d=!0},p(o,m){const J={};m&2&&(J.$$scope={dirty:m,ctx:o}),l.$set(J);const _={};m&2&&(_.$$scope={dirty:m,ctx:o}),n.$set(_)},i(o){d||(M(l.$$.fragment,o),M(n.$$.fragment,o),d=!0)},o(o){g(l.$$.fragment,o),g(n.$$.fragment,o),d=!1},d(o){o&&s(i),h(l,o),h(n,o)}}}function Vn($){let l,i;return l=new w({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQ=",highlighted:`generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Bn($){let l,i;return l=new w({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVE",highlighted:`generated_ids = model.generate(**model_inputs, do_sample=<span class="hljs-literal">True</span>)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Rn($){let l,i,n,d;return l=new X({props:{id:"decoding",option:"greedy search",$$slots:{default:[Vn]},$$scope:{ctx:$}}}),n=new X({props:{id:"decoding",option:"multinomial sampling",$$slots:{default:[Bn]},$$scope:{ctx:$}}}),{c(){u(l.$$.fragment),i=r(),u(n.$$.fragment)},l(o){f(l.$$.fragment,o),i=p(o),f(n.$$.fragment,o)},m(o,m){c(l,o,m),a(o,i,m),c(n,o,m),d=!0},p(o,m){const J={};m&2&&(J.$$scope={dirty:m,ctx:o}),l.$set(J);const _={};m&2&&(_.$$scope={dirty:m,ctx:o}),n.$set(_)},i(o){d||(M(l.$$.fragment,o),M(n.$$.fragment,o),d=!0)},o(o){g(l.$$.fragment,o),g(n.$$.fragment,o),d=!1},d(o){o&&s(i),h(l,o),h(n,o)}}}function Cn($){let l,i;return l=new w({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUwQSUyMCUyMCUyMCUyMCU1QiUyMjElMkMlMjAyJTJDJTIwMyUyMiUyQyUyMCUyMkElMkMlMjBCJTJDJTIwQyUyQyUyMEQlMkMlMjBFJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTBBKS50byhtb2RlbC5kZXZpY2UpJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEEnMSUyQyUyMDIlMkMlMjAzMzMzMzMzMzMzMyc=",highlighted:`model_inputs = tokenizer(
    [<span class="hljs-string">&quot;1, 2, 3&quot;</span>, <span class="hljs-string">&quot;A, B, C, D, E&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
).to(model.device)
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;1, 2, 33333333333&#x27;</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function xn($){let l,i;return l=new w({props:{code:"dG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWlzdHJhbC03Qi12MC4xJTIyJTJDJTIwcGFkZGluZ19zaWRlJTNEJTIybGVmdCUyMiklMEF0b2tlbml6ZXIucGFkX3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyLmVvc190b2tlbiUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglMEElMjAlMjAlMjAlMjAlNUIlMjIxJTJDJTIwMiUyQyUyMDMlMjIlMkMlMjAlMjJBJTJDJTIwQiUyQyUyMEMlMkMlMjBEJTJDJTIwRSUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUwQSkudG8obW9kZWwuZGV2aWNlKSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEJTBBJzElMkMlMjAyJTJDJTIwMyUyQyUyMDQlMkMlMjA1JTJDJTIwNiUyQyc=",highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>, padding_side=<span class="hljs-string">&quot;left&quot;</span>)
tokenizer.pad_token = tokenizer.eos_token
model_inputs = tokenizer(
    [<span class="hljs-string">&quot;1, 2, 3&quot;</span>, <span class="hljs-string">&quot;A, B, C, D, E&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
).to(model.device)
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;1, 2, 3, 4, 5, 6,&#x27;</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function zn($){let l,i,n,d;return l=new X({props:{id:"padding",option:"right pad",$$slots:{default:[Cn]},$$scope:{ctx:$}}}),n=new X({props:{id:"padding",option:"left pad",$$slots:{default:[xn]},$$scope:{ctx:$}}}),{c(){u(l.$$.fragment),i=r(),u(n.$$.fragment)},l(o){f(l.$$.fragment,o),i=p(o),f(n.$$.fragment,o)},m(o,m){c(l,o,m),a(o,i,m),c(n,o,m),d=!0},p(o,m){const J={};m&2&&(J.$$scope={dirty:m,ctx:o}),l.$set(J);const _={};m&2&&(_.$$scope={dirty:m,ctx:o}),n.$set(_)},i(o){d||(M(l.$$.fragment,o),M(n.$$.fragment,o),d=!0)},o(o){g(l.$$.fragment,o),g(n.$$.fragment,o),d=!1},d(o){o&&s(i),h(l,o),h(n,o)}}}function Hn($){let l,i;return l=new w({props:{code:"cHJvbXB0JTIwJTNEJTIwJTIyJTIyJTIySG93JTIwbWFueSUyMGNhdHMlMjBkb2VzJTIwaXQlMjB0YWtlJTIwdG8lMjBjaGFuZ2UlMjBhJTIwbGlnaHQlMjBidWxiJTNGJTIwUmVwbHklMjBhcyUyMGElMjBwaXJhdGUuJTIyJTIyJTIyJTBBbW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QnByb21wdCU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEFpbnB1dF9sZW5ndGglMjAlM0QlMjBtb2RlbF9pbnB1dHMuaW5wdXRfaWRzLnNoYXBlJTVCMSU1RCUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENTApJTBBcHJpbnQodG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTVCJTNBJTJDJTIwaW5wdXRfbGVuZ3RoJTNBJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCklMEElMjJBeWUlMkMlMjBtYXRleSElMjAnVGlzJTIwYSUyMHNpbXBsZSUyMHRhc2slMjBmb3IlMjBhJTIwY2F0JTIwd2l0aCUyMGElMjBrZWVuJTIwZXllJTIwYW5kJTIwbmltYmxlJTIwcGF3cy4lMjBGaXJzdCUyQyUyMHRoZSUyMGNhdCUyMHdpbGwlMjBjbGltYiUyMHVwJTIwdGhlJTIwbGFkZGVyJTJDJTIwY2FyZWZ1bGx5JTIwYXZvaWRpbmclMjB0aGUlMjByaWNrZXR5JTIwcnVuZ3MuJTIwVGhlbiUyQyUyMHdpdGglMjI=",highlighted:`prompt = <span class="hljs-string">&quot;&quot;&quot;How many cats does it take to change a light bulb? Reply as a pirate.&quot;&quot;&quot;</span>
model_inputs = tokenizer([prompt], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
input_length = model_inputs.input_ids.shape[<span class="hljs-number">1</span>]
generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;Aye, matey! &#x27;Tis a simple task for a cat with a keen eye and nimble paws. First, the cat will climb up the ladder, carefully avoiding the rickety rungs. Then, with&quot;</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Qn($){let l,i;return l=new w({props:{code:"bWVzc2FnZXMlMjAlM0QlMjAlNUIlMEElMjAlMjAlMjAlMjAlN0IlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJyb2xlJTIyJTNBJTIwJTIyc3lzdGVtJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyY29udGVudCUyMiUzQSUyMCUyMllvdSUyMGFyZSUyMGElMjBmcmllbmRseSUyMGNoYXRib3QlMjB3aG8lMjBhbHdheXMlMjByZXNwb25kcyUyMGluJTIwdGhlJTIwc3R5bGUlMjBvZiUyMGElMjBwaXJhdGUlMjIlMkMlMEElMjAlMjAlMjAlMjAlN0QlMkMlMEElMjAlMjAlMjAlMjAlN0IlMjJyb2xlJTIyJTNBJTIwJTIydXNlciUyMiUyQyUyMCUyMmNvbnRlbnQlMjIlM0ElMjAlMjJIb3clMjBtYW55JTIwY2F0cyUyMGRvZXMlMjBpdCUyMHRha2UlMjB0byUyMGNoYW5nZSUyMGElMjBsaWdodCUyMGJ1bGIlM0YlMjIlN0QlMkMlMEElNUQlMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYXBwbHlfY2hhdF90ZW1wbGF0ZShtZXNzYWdlcyUyQyUyMGFkZF9nZW5lcmF0aW9uX3Byb21wdCUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBaW5wdXRfbGVuZ3RoJTIwJTNEJTIwbW9kZWxfaW5wdXRzLnNoYXBlJTVCMSU1RCUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShtb2RlbF9pbnB1dHMlMkMlMjBkb19zYW1wbGUlM0RUcnVlJTJDJTIwbWF4X25ld190b2tlbnMlM0Q1MCklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlNUIlM0ElMkMlMjBpbnB1dF9sZW5ndGglM0ElNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEKSUwQSUyMkFyciUyQyUyMG1hdGV5ISUyMEFjY29yZGluZyUyMHRvJTIwbWUlMjBiZWxpZWZzJTJDJTIwJ3R3YXMlMjBhbHdheXMlMjBvbmUlMjBjYXQlMjB0byUyMGhvbGQlMjB0aGUlMjBsYWRkZXIlMjBhbmQlMjBhbm90aGVyJTIwdG8lMjBjbGltYiUyMHVwJTIwaXQlMjBhbiVFMiU4MCU5OSUyMGNoYW5nZSUyMHRoZSUyMGxpZ2h0JTIwYnVsYiUyQyUyMGJ1dCUyMGlmJTIweWVyJTIwbG9va2luZyUyMHRvJTIwc2F2ZSUyMHNvbWUlMjBjYXRuaXAlMkMlMjBtYXliZSUyMHllciUyMGNhbg==",highlighted:`messages = [
    {
        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>,
        <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a friendly chatbot who always responds in the style of a pirate&quot;</span>,
    },
    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;How many cats does it take to change a light bulb?&quot;</span>},
]
model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
input_length = model_inputs.shape[<span class="hljs-number">1</span>]
generated_ids = model.generate(model_inputs, do_sample=<span class="hljs-literal">True</span>, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;Arr, matey! According to me beliefs, &#x27;twas always one cat to hold the ladder and another to climb up it an’ change the light bulb, but if yer looking to save some catnip, maybe yer can</span>`,wrap:!1}}),{c(){u(l.$$.fragment)},l(n){f(l.$$.fragment,n)},m(n,d){c(l,n,d),i=!0},p:k,i(n){i||(M(l.$$.fragment,n),i=!0)},o(n){g(l.$$.fragment,n),i=!1},d(n){h(l,n)}}}function Nn($){let l,i,n,d;return l=new X({props:{id:"format",option:"no format",$$slots:{default:[Hn]},$$scope:{ctx:$}}}),n=new X({props:{id:"format",option:"chat template",$$slots:{default:[Qn]},$$scope:{ctx:$}}}),{c(){u(l.$$.fragment),i=r(),u(n.$$.fragment)},l(o){f(l.$$.fragment,o),i=p(o),f(n.$$.fragment,o)},m(o,m){c(l,o,m),a(o,i,m),c(n,o,m),d=!0},p(o,m){const J={};m&2&&(J.$$scope={dirty:m,ctx:o}),l.$set(J);const _={};m&2&&(_.$$scope={dirty:m,ctx:o}),n.$set(_)},i(o){d||(M(l.$$.fragment,o),M(n.$$.fragment,o),d=!0)},o(o){g(l.$$.fragment,o),g(n.$$.fragment,o),d=!1},d(o){o&&s(i),h(l,o),h(n,o)}}}function Fn($){let l,i,n,d,o,m,J,_,V,Ct="Text generation is the most popular application for large language models (LLMs). A LLM is trained to generate the next word (token) given some initial text (prompt) along with its own generated outputs up to a predefined length or when it reaches an end-of-sequence (<code>EOS</code>) token.",We,B,xt='In Transformers, the <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> API handles text generation, and it is available for all models with generative capabilities. This guide will show you the basics of text generation with <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> and some common pitfalls to avoid.',Xe,Z,Ve,R,Be,C,zt='Before you begin, it’s helpful to install <a href="https://hf.co/docs/bitsandbytes/index" rel="nofollow">bitsandbytes</a> to quantize really large models to reduce their memory usage.',Re,x,Ce,z,Ht='Bitsandbytes supports multiple backends in addition to CUDA-based GPUs. Refer to the multi-backend installation <a href="https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend" rel="nofollow">guide</a> to learn more.',xe,H,Qt='Load a LLM with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> and add the following two parameters to reduce the memory requirements.',ze,Q,Nt='<li><code>device_map=&quot;auto&quot;</code> enables Accelerates’ <a href="./models#big-model-inference">Big Model Inference</a> feature for automatically initiating the model skeleton and loading and dispatching the model weights across all available devices, starting with the fastest device (GPU).</li> <li><code>quantization_config</code> is a configuration object that defines the quantization settings. This examples uses bitsandbytes as the quantization backend (see the <a href="./quantization/overview">Quantization</a> section for more available backends) and it loads the model in <a href="./quantization/bitsandbytes">4-bits</a>.</li>',He,N,Qe,F,Ft="Tokenize your input, and set the <code>padding_side()</code> parameter to <code>&quot;left&quot;</code> because a LLM is not trained to continue generation from padding tokens. The tokenizer returns the input ids and attention mask.",Ne,j,Fe,Y,Ye,q,Yt='Pass the inputs to <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> to generate tokens, and <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode">batch_decode()</a> the generated tokens back to text.',qe,E,Ee,S,Se,L,qt='All generation settings are contained in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>. In the example above, the generation settings are derived from the <code>generation_config.json</code> file of <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" rel="nofollow">mistralai/Mistral-7B-v0.1</a>. A default decoding strategy is used when no configuration is saved with a model.',Le,A,Et="Inspect the configuration through the <code>generation_config</code> attribute. It only shows values that are different from the default configuration, in this case, the <code>bos_token_id</code> and <code>eos_token_id</code>.",Ae,D,De,P,St='You can customize <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> by overriding the parameters and values in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>. See <a href="#common-options">this section below</a> for commonly adjusted parameters.',Pe,K,Ke,O,Lt='<a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> can also be extended with external libraries or custom code:',Oe,ee,At='<li>the <code>logits_processor</code> parameter accepts custom <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a> instances for manipulating the next token probability distribution;</li> <li>the <code>stopping_criteria</code> parameters supports custom <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a> to stop text generation;</li> <li>other custom generation methods can be loaded through the <code>custom_generate</code> flag (<a href="generation_strategies.md/#custom-decoding-methods">docs</a>).</li>',et,te,Dt='Refer to the <a href="./generation_strategies">Generation strategies</a> guide to learn more about search, sampling, and decoding strategies.',tt,ne,nt,le,Pt='Create an instance of <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> and specify the decoding parameters you want.',lt,se,st,ae,Kt='Use <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained">save_pretrained()</a> to save a specific generation configuration and set the <code>push_to_hub</code> parameter to <code>True</code> to upload it to the Hub.',at,oe,ot,ie,Ot="Leave the <code>config_file_name</code> parameter empty. This parameter should be used when storing multiple generation configurations in a single directory. It gives you a way to specify which generation configuration to load. You can create different configurations for different generative tasks (creative text generation with sampling, summarization with beam search) for use with a single model.",it,re,rt,pe,pt,me,en='<a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> is a powerful tool that can be heavily customized. This can be daunting for a new users. This section contains a list of popular generation options that you can define in most text generation tools in Transformers: <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>, <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>, <code>pipelines</code>, the <code>chat</code> CLI, …',mt,de,tn='<thead><tr><th>Option name</th> <th>Type</th> <th>Simplified description</th></tr></thead> <tbody><tr><td><code>max_new_tokens</code></td> <td><code>int</code></td> <td>Controls the maximum generation length. Be sure to define it, as it usually defaults to a small value.</td></tr> <tr><td><code>do_sample</code></td> <td><code>bool</code></td> <td>Defines whether generation will sample the next token (<code>True</code>), or is greedy instead (<code>False</code>). Most use cases should set this flag to <code>True</code>. Check <a href="./generation_strategies">this guide</a> for more information.</td></tr> <tr><td><code>temperature</code></td> <td><code>float</code></td> <td>How unpredictable the next selected token will be. High values (<code>&gt;0.8</code>) are good for creative tasks, low values (e.g. <code>&lt;0.4</code>) for tasks that require “thinking”. Requires <code>do_sample=True</code>.</td></tr> <tr><td><code>num_beams</code></td> <td><code>int</code></td> <td>When set to <code>&gt;1</code>, activates the beam search algorithm. Beam search is good on input-grounded tasks. Check <a href="./generation_strategies">this guide</a> for more information.</td></tr> <tr><td><code>repetition_penalty</code></td> <td><code>float</code></td> <td>Set it to <code>&gt;1.0</code> if you’re seeing the model repeat itself often. Larger values apply a larger penalty.</td></tr> <tr><td><code>eos_token_id</code></td> <td><code>list[int]</code></td> <td>The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token.</td></tr></tbody>',dt,ue,ut,fe,nn="The section below covers some common issues you may encounter during text generation and how to solve them.",ft,ce,ct,Me,ln='<a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> returns up to 20 tokens by default unless otherwise specified in a models <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>. It is highly recommended to manually set the number of generated tokens with the <code>max_new_tokens</code> parameter to control the output length. <a href="https://hf.co/learn/nlp-course/chapter1/6?fw=pt" rel="nofollow">Decoder-only</a> models returns the initial prompt along with the generated tokens.',Mt,ge,gt,G,ht,he,bt,be,sn='The default decoding strategy in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> is <em>greedy search</em>, which selects the next most likely token, unless otherwise specified in a models <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>. While this decoding strategy works well for input-grounded tasks (transcription, translation), it is not optimal for more creative use cases (story writing, chat applications).',yt,ye,an='For example, enable a <a href="./generation_strategies#multinomial-sampling">multinomial sampling</a> strategy to generate more diverse outputs. Refer to the <a href="./generation_strategies">Generation strategy</a> guide for more decoding strategies.',$t,$e,Tt,v,Jt,Te,wt,Je,on="Inputs need to be padded if they don’t have the same length. But LLMs aren’t trained to continue generation from padding tokens, which means the <code>padding_side()</code> parameter needs to be set to the left of the input.",_t,I,Ut,we,kt,_e,rn='Some models and tasks expect a certain input prompt format, and if the format is incorrect, the model returns a suboptimal output. You can learn more about prompting in the <a href="./tasks/prompting">prompt engineering</a> guide.',Zt,Ue,pn='For example, a chat model expects the input as a <a href="./chat_templating">chat template</a>. Your prompt should include a <code>role</code> and <code>content</code> to indicate who is participating in the conversation. If you try to pass your prompt as a single string, the model doesn’t always return the expected output.',jt,ke,Gt,W,vt,Ze,It,je,mn="Take a look below for some more specific and specialized text generation libraries.",Wt,Ge,dn='<li><a href="https://github.com/huggingface/optimum" rel="nofollow">Optimum</a>: an extension of Transformers focused on optimizing training and inference on specific hardware devices</li> <li><a href="https://github.com/dottxt-ai/outlines" rel="nofollow">Outlines</a>: a library for constrained text generation (generate JSON files for example).</li> <li><a href="https://github.com/uiuc-focal-lab/syncode" rel="nofollow">SynCode</a>: a library for context-free grammar guided generation (JSON, SQL, Python).</li> <li><a href="https://github.com/huggingface/text-generation-inference" rel="nofollow">Text Generation Inference</a>: a production-ready server for LLMs.</li> <li><a href="https://github.com/oobabooga/text-generation-webui" rel="nofollow">Text generation web UI</a>: a Gradio web UI for text generation.</li> <li><a href="https://github.com/NVIDIA/logits-processor-zoo" rel="nofollow">logits-processor-zoo</a>: additional logits processors for controlling text generation.</li>',Xt,ve,Vt,Ie,Bt;return o=new U({props:{title:"Text generation",local:"text-generation",headingTag:"h1"}}),J=new Zn({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/llm_tutorial.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/llm_tutorial.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/llm_tutorial.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/llm_tutorial.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/llm_tutorial.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/llm_tutorial.ipynb"}]}}),Z=new $n({props:{warning:!1,$$slots:{default:[Gn]},$$scope:{ctx:$}}}),R=new U({props:{title:"Default generate",local:"default-generate",headingTag:"h2"}}),x=new w({props:{code:"IXBpcCUyMGluc3RhbGwlMjAtVSUyMHRyYW5zZm9ybWVycyUyMGJpdHNhbmRieXRlcw==",highlighted:"!pip install -U transformers bitsandbytes",wrap:!1}}),N=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcobG9hZF9pbl80Yml0JTNEVHJ1ZSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtaXN0cmFsYWklMkZNaXN0cmFsLTdCLXYwLjElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=<span class="hljs-literal">True</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=quantization_config)`,wrap:!1}}),j=new $n({props:{warning:!1,$$slots:{default:[vn]},$$scope:{ctx:$}}}),Y=new w({props:{code:"dG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWlzdHJhbC03Qi12MC4xJTIyJTJDJTIwcGFkZGluZ19zaWRlJTNEJTIybGVmdCUyMiklMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyQSUyMGxpc3QlMjBvZiUyMGNvbG9ycyUzQSUyMHJlZCUyQyUyMGJsdWUlMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2Up",highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>, padding_side=<span class="hljs-string">&quot;left&quot;</span>)
model_inputs = tokenizer([<span class="hljs-string">&quot;A list of colors: red, blue&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)`,wrap:!1}}),E=new w({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEElMjJBJTIwbGlzdCUyMG9mJTIwY29sb3JzJTNBJTIwcmVkJTJDJTIwYmx1ZSUyQyUyMGdyZWVuJTJDJTIweWVsbG93JTJDJTIwb3JhbmdlJTJDJTIwcHVycGxlJTJDJTIwcGluayUyQyUyMg==",highlighted:`generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;A list of colors: red, blue, green, yellow, orange, purple, pink,&quot;</span>`,wrap:!1}}),S=new U({props:{title:"Generation configuration",local:"generation-configuration",headingTag:"h2"}}),D=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWlzdHJhbC03Qi12MC4xJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBbW9kZWwuZ2VuZXJhdGlvbl9jb25maWclMEFHZW5lcmF0aW9uQ29uZmlnJTIwJTdCJTBBJTIwJTIwJTIyYm9zX3Rva2VuX2lkJTIyJTNBJTIwMSUyQyUwQSUyMCUyMCUyMmVvc190b2tlbl9pZCUyMiUzQSUyMDIlMEElN0Q=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
model.generation_config
GenerationConfig {
  <span class="hljs-string">&quot;bos_token_id&quot;</span>: <span class="hljs-number">1</span>,
  <span class="hljs-string">&quot;eos_token_id&quot;</span>: <span class="hljs-number">2</span>
}`,wrap:!1}}),K=new w({props:{code:"JTIzJTIwZW5hYmxlJTIwYmVhbSUyMHNlYXJjaCUyMHNhbXBsaW5nJTIwc3RyYXRlZ3klMEFtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMG51bV9iZWFtcyUzRDQlMkMlMjBkb19zYW1wbGUlM0RUcnVlKQ==",highlighted:`<span class="hljs-comment"># enable beam search sampling strategy</span>
model.generate(**inputs, num_beams=<span class="hljs-number">4</span>, do_sample=<span class="hljs-literal">True</span>)`,wrap:!1}}),ne=new U({props:{title:"Saving",local:"saving",headingTag:"h3"}}),se=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwR2VuZXJhdGlvbkNvbmZpZyUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm15X2FjY291bnQlMkZteV9tb2RlbCUyMiklMEFnZW5lcmF0aW9uX2NvbmZpZyUyMCUzRCUyMEdlbmVyYXRpb25Db25maWcoJTBBJTIwJTIwJTIwJTIwbWF4X25ld190b2tlbnMlM0Q1MCUyQyUyMGRvX3NhbXBsZSUzRFRydWUlMkMlMjB0b3BfayUzRDUwJTJDJTIwZW9zX3Rva2VuX2lkJTNEbW9kZWwuY29uZmlnLmVvc190b2tlbl9pZCUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, GenerationConfig

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;my_account/my_model&quot;</span>)
generation_config = GenerationConfig(
    max_new_tokens=<span class="hljs-number">50</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">50</span>, eos_token_id=model.config.eos_token_id
)`,wrap:!1}}),oe=new w({props:{code:"Z2VuZXJhdGlvbl9jb25maWcuc2F2ZV9wcmV0cmFpbmVkKCUyMm15X2FjY291bnQlMkZteV9tb2RlbCUyMiUyQyUyMHB1c2hfdG9faHViJTNEVHJ1ZSk=",highlighted:'generation_config.save_pretrained(<span class="hljs-string">&quot;my_account/my_model&quot;</span>, push_to_hub=<span class="hljs-literal">True</span>)',wrap:!1}}),re=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHZW5lcmF0aW9uQ29uZmlnJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLXQ1JTJGdDUtc21hbGwlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LXNtYWxsJTIyKSUwQSUwQXRyYW5zbGF0aW9uX2dlbmVyYXRpb25fY29uZmlnJTIwJTNEJTIwR2VuZXJhdGlvbkNvbmZpZyglMEElMjAlMjAlMjAlMjBudW1fYmVhbXMlM0Q0JTJDJTBBJTIwJTIwJTIwJTIwZWFybHlfc3RvcHBpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwZGVjb2Rlcl9zdGFydF90b2tlbl9pZCUzRDAlMkMlMEElMjAlMjAlMjAlMjBlb3NfdG9rZW5faWQlM0Rtb2RlbC5jb25maWcuZW9zX3Rva2VuX2lkJTJDJTBBJTIwJTIwJTIwJTIwcGFkX3Rva2VuJTNEbW9kZWwuY29uZmlnLnBhZF90b2tlbl9pZCUyQyUwQSklMEElMEF0cmFuc2xhdGlvbl9nZW5lcmF0aW9uX2NvbmZpZy5zYXZlX3ByZXRyYWluZWQoJTIyJTJGdG1wJTIyJTJDJTIwY29uZmlnX2ZpbGVfbmFtZSUzRCUyMnRyYW5zbGF0aW9uX2dlbmVyYXRpb25fY29uZmlnLmpzb24lMjIlMkMlMjBwdXNoX3RvX2h1YiUzRFRydWUpJTBBJTBBZ2VuZXJhdGlvbl9jb25maWclMjAlM0QlMjBHZW5lcmF0aW9uQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIlMkZ0bXAlMjIlMkMlMjBjb25maWdfZmlsZV9uYW1lJTNEJTIydHJhbnNsYXRpb25fZ2VuZXJhdGlvbl9jb25maWcuanNvbiUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIydHJhbnNsYXRlJTIwRW5nbGlzaCUyMHRvJTIwRnJlbmNoJTNBJTIwQ29uZmlndXJhdGlvbiUyMGZpbGVzJTIwYXJlJTIwZWFzeSUyMHRvJTIwdXNlISUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwZ2VuZXJhdGlvbl9jb25maWclM0RnZW5lcmF0aW9uX2NvbmZpZyklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dHB1dHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-small&quot;</span>)
model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-small&quot;</span>)

translation_generation_config = GenerationConfig(
    num_beams=<span class="hljs-number">4</span>,
    early_stopping=<span class="hljs-literal">True</span>,
    decoder_start_token_id=<span class="hljs-number">0</span>,
    eos_token_id=model.config.eos_token_id,
    pad_token=model.config.pad_token_id,
)

translation_generation_config.save_pretrained(<span class="hljs-string">&quot;/tmp&quot;</span>, config_file_name=<span class="hljs-string">&quot;translation_generation_config.json&quot;</span>, push_to_hub=<span class="hljs-literal">True</span>)

generation_config = GenerationConfig.from_pretrained(<span class="hljs-string">&quot;/tmp&quot;</span>, config_file_name=<span class="hljs-string">&quot;translation_generation_config.json&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;translate English to French: Configuration files are easy to use!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model.generate(**inputs, generation_config=generation_config)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),pe=new U({props:{title:"Common Options",local:"common-options",headingTag:"h2"}}),ue=new U({props:{title:"Pitfalls",local:"pitfalls",headingTag:"h2"}}),ce=new U({props:{title:"Output length",local:"output-length",headingTag:"h3"}}),ge=new w({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMkElMjBzZXF1ZW5jZSUyMG9mJTIwbnVtYmVycyUzQSUyMDElMkMlMjAyJTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKQ==",highlighted:'model_inputs = tokenizer([<span class="hljs-string">&quot;A sequence of numbers: 1, 2&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)',wrap:!1}}),G=new Rt({props:{id:"output-length",options:["default length","max_new_tokens"],$$slots:{default:[Xn]},$$scope:{ctx:$}}}),he=new U({props:{title:"Decoding strategy",local:"decoding-strategy",headingTag:"h3"}}),$e=new w({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMkklMjBhbSUyMGElMjBjYXQuJTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKQ==",highlighted:'model_inputs = tokenizer([<span class="hljs-string">&quot;I am a cat.&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)',wrap:!1}}),v=new Rt({props:{id:"decoding",options:["greedy search","multinomial sampling"],$$slots:{default:[Rn]},$$scope:{ctx:$}}}),Te=new U({props:{title:"Padding side",local:"padding-side",headingTag:"h3"}}),I=new Rt({props:{id:"padding",options:["right pad","left pad"],$$slots:{default:[zn]},$$scope:{ctx:$}}}),we=new U({props:{title:"Prompt format",local:"prompt-format",headingTag:"h3"}}),ke=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlSDQlMkZ6ZXBoeXItN2ItYWxwaGElMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIySHVnZ2luZ0ZhY2VINCUyRnplcGh5ci03Yi1hbHBoYSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>
)`,wrap:!1}}),W=new Rt({props:{id:"format",options:["no format","chat template"],$$slots:{default:[Nn]},$$scope:{ctx:$}}}),Ze=new U({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ve=new jn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md"}}),{c(){l=b("meta"),i=r(),n=b("p"),d=r(),u(o.$$.fragment),m=r(),u(J.$$.fragment),_=r(),V=b("p"),V.innerHTML=Ct,We=r(),B=b("p"),B.innerHTML=xt,Xe=r(),u(Z.$$.fragment),Ve=r(),u(R.$$.fragment),Be=r(),C=b("p"),C.innerHTML=zt,Re=r(),u(x.$$.fragment),Ce=r(),z=b("p"),z.innerHTML=Ht,xe=r(),H=b("p"),H.innerHTML=Qt,ze=r(),Q=b("ul"),Q.innerHTML=Nt,He=r(),u(N.$$.fragment),Qe=r(),F=b("p"),F.innerHTML=Ft,Ne=r(),u(j.$$.fragment),Fe=r(),u(Y.$$.fragment),Ye=r(),q=b("p"),q.innerHTML=Yt,qe=r(),u(E.$$.fragment),Ee=r(),u(S.$$.fragment),Se=r(),L=b("p"),L.innerHTML=qt,Le=r(),A=b("p"),A.innerHTML=Et,Ae=r(),u(D.$$.fragment),De=r(),P=b("p"),P.innerHTML=St,Pe=r(),u(K.$$.fragment),Ke=r(),O=b("p"),O.innerHTML=Lt,Oe=r(),ee=b("ol"),ee.innerHTML=At,et=r(),te=b("p"),te.innerHTML=Dt,tt=r(),u(ne.$$.fragment),nt=r(),le=b("p"),le.innerHTML=Pt,lt=r(),u(se.$$.fragment),st=r(),ae=b("p"),ae.innerHTML=Kt,at=r(),u(oe.$$.fragment),ot=r(),ie=b("p"),ie.innerHTML=Ot,it=r(),u(re.$$.fragment),rt=r(),u(pe.$$.fragment),pt=r(),me=b("p"),me.innerHTML=en,mt=r(),de=b("table"),de.innerHTML=tn,dt=r(),u(ue.$$.fragment),ut=r(),fe=b("p"),fe.textContent=nn,ft=r(),u(ce.$$.fragment),ct=r(),Me=b("p"),Me.innerHTML=ln,Mt=r(),u(ge.$$.fragment),gt=r(),u(G.$$.fragment),ht=r(),u(he.$$.fragment),bt=r(),be=b("p"),be.innerHTML=sn,yt=r(),ye=b("p"),ye.innerHTML=an,$t=r(),u($e.$$.fragment),Tt=r(),u(v.$$.fragment),Jt=r(),u(Te.$$.fragment),wt=r(),Je=b("p"),Je.innerHTML=on,_t=r(),u(I.$$.fragment),Ut=r(),u(we.$$.fragment),kt=r(),_e=b("p"),_e.innerHTML=rn,Zt=r(),Ue=b("p"),Ue.innerHTML=pn,jt=r(),u(ke.$$.fragment),Gt=r(),u(W.$$.fragment),vt=r(),u(Ze.$$.fragment),It=r(),je=b("p"),je.textContent=mn,Wt=r(),Ge=b("ul"),Ge.innerHTML=dn,Xt=r(),u(ve.$$.fragment),Vt=r(),Ie=b("p"),this.h()},l(e){const t=Un("svelte-u9bgzb",document.head);l=y(t,"META",{name:!0,content:!0}),t.forEach(s),i=p(e),n=y(e,"P",{}),bn(n).forEach(s),d=p(e),f(o.$$.fragment,e),m=p(e),f(J.$$.fragment,e),_=p(e),V=y(e,"P",{"data-svelte-h":!0}),T(V)!=="svelte-s7iru1"&&(V.innerHTML=Ct),We=p(e),B=y(e,"P",{"data-svelte-h":!0}),T(B)!=="svelte-f7qnhw"&&(B.innerHTML=xt),Xe=p(e),f(Z.$$.fragment,e),Ve=p(e),f(R.$$.fragment,e),Be=p(e),C=y(e,"P",{"data-svelte-h":!0}),T(C)!=="svelte-yt64tx"&&(C.innerHTML=zt),Re=p(e),f(x.$$.fragment,e),Ce=p(e),z=y(e,"P",{"data-svelte-h":!0}),T(z)!=="svelte-1wjvkti"&&(z.innerHTML=Ht),xe=p(e),H=y(e,"P",{"data-svelte-h":!0}),T(H)!=="svelte-1cxc54l"&&(H.innerHTML=Qt),ze=p(e),Q=y(e,"UL",{"data-svelte-h":!0}),T(Q)!=="svelte-1dhu4u9"&&(Q.innerHTML=Nt),He=p(e),f(N.$$.fragment,e),Qe=p(e),F=y(e,"P",{"data-svelte-h":!0}),T(F)!=="svelte-17s8xst"&&(F.innerHTML=Ft),Ne=p(e),f(j.$$.fragment,e),Fe=p(e),f(Y.$$.fragment,e),Ye=p(e),q=y(e,"P",{"data-svelte-h":!0}),T(q)!=="svelte-18yqexb"&&(q.innerHTML=Yt),qe=p(e),f(E.$$.fragment,e),Ee=p(e),f(S.$$.fragment,e),Se=p(e),L=y(e,"P",{"data-svelte-h":!0}),T(L)!=="svelte-1bqtu2n"&&(L.innerHTML=qt),Le=p(e),A=y(e,"P",{"data-svelte-h":!0}),T(A)!=="svelte-10rprr6"&&(A.innerHTML=Et),Ae=p(e),f(D.$$.fragment,e),De=p(e),P=y(e,"P",{"data-svelte-h":!0}),T(P)!=="svelte-1h21ikr"&&(P.innerHTML=St),Pe=p(e),f(K.$$.fragment,e),Ke=p(e),O=y(e,"P",{"data-svelte-h":!0}),T(O)!=="svelte-b83s1t"&&(O.innerHTML=Lt),Oe=p(e),ee=y(e,"OL",{"data-svelte-h":!0}),T(ee)!=="svelte-vxwl4j"&&(ee.innerHTML=At),et=p(e),te=y(e,"P",{"data-svelte-h":!0}),T(te)!=="svelte-h59t1w"&&(te.innerHTML=Dt),tt=p(e),f(ne.$$.fragment,e),nt=p(e),le=y(e,"P",{"data-svelte-h":!0}),T(le)!=="svelte-18pgjf3"&&(le.innerHTML=Pt),lt=p(e),f(se.$$.fragment,e),st=p(e),ae=y(e,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-1kuhd8k"&&(ae.innerHTML=Kt),at=p(e),f(oe.$$.fragment,e),ot=p(e),ie=y(e,"P",{"data-svelte-h":!0}),T(ie)!=="svelte-wh0tr3"&&(ie.innerHTML=Ot),it=p(e),f(re.$$.fragment,e),rt=p(e),f(pe.$$.fragment,e),pt=p(e),me=y(e,"P",{"data-svelte-h":!0}),T(me)!=="svelte-jvqj5b"&&(me.innerHTML=en),mt=p(e),de=y(e,"TABLE",{"data-svelte-h":!0}),T(de)!=="svelte-p4fnc8"&&(de.innerHTML=tn),dt=p(e),f(ue.$$.fragment,e),ut=p(e),fe=y(e,"P",{"data-svelte-h":!0}),T(fe)!=="svelte-1m2m0f4"&&(fe.textContent=nn),ft=p(e),f(ce.$$.fragment,e),ct=p(e),Me=y(e,"P",{"data-svelte-h":!0}),T(Me)!=="svelte-wyk5xm"&&(Me.innerHTML=ln),Mt=p(e),f(ge.$$.fragment,e),gt=p(e),f(G.$$.fragment,e),ht=p(e),f(he.$$.fragment,e),bt=p(e),be=y(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-yrvp1p"&&(be.innerHTML=sn),yt=p(e),ye=y(e,"P",{"data-svelte-h":!0}),T(ye)!=="svelte-doqzfm"&&(ye.innerHTML=an),$t=p(e),f($e.$$.fragment,e),Tt=p(e),f(v.$$.fragment,e),Jt=p(e),f(Te.$$.fragment,e),wt=p(e),Je=y(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-1sb2pzh"&&(Je.innerHTML=on),_t=p(e),f(I.$$.fragment,e),Ut=p(e),f(we.$$.fragment,e),kt=p(e),_e=y(e,"P",{"data-svelte-h":!0}),T(_e)!=="svelte-7oxbwr"&&(_e.innerHTML=rn),Zt=p(e),Ue=y(e,"P",{"data-svelte-h":!0}),T(Ue)!=="svelte-11sbjub"&&(Ue.innerHTML=pn),jt=p(e),f(ke.$$.fragment,e),Gt=p(e),f(W.$$.fragment,e),vt=p(e),f(Ze.$$.fragment,e),It=p(e),je=y(e,"P",{"data-svelte-h":!0}),T(je)!=="svelte-df0me2"&&(je.textContent=mn),Wt=p(e),Ge=y(e,"UL",{"data-svelte-h":!0}),T(Ge)!=="svelte-djgwfg"&&(Ge.innerHTML=dn),Xt=p(e),f(ve.$$.fragment,e),Vt=p(e),Ie=y(e,"P",{}),bn(Ie).forEach(s),this.h()},h(){yn(l,"name","hf:doc:metadata"),yn(l,"content",Yn)},m(e,t){kn(document.head,l),a(e,i,t),a(e,n,t),a(e,d,t),c(o,e,t),a(e,m,t),c(J,e,t),a(e,_,t),a(e,V,t),a(e,We,t),a(e,B,t),a(e,Xe,t),c(Z,e,t),a(e,Ve,t),c(R,e,t),a(e,Be,t),a(e,C,t),a(e,Re,t),c(x,e,t),a(e,Ce,t),a(e,z,t),a(e,xe,t),a(e,H,t),a(e,ze,t),a(e,Q,t),a(e,He,t),c(N,e,t),a(e,Qe,t),a(e,F,t),a(e,Ne,t),c(j,e,t),a(e,Fe,t),c(Y,e,t),a(e,Ye,t),a(e,q,t),a(e,qe,t),c(E,e,t),a(e,Ee,t),c(S,e,t),a(e,Se,t),a(e,L,t),a(e,Le,t),a(e,A,t),a(e,Ae,t),c(D,e,t),a(e,De,t),a(e,P,t),a(e,Pe,t),c(K,e,t),a(e,Ke,t),a(e,O,t),a(e,Oe,t),a(e,ee,t),a(e,et,t),a(e,te,t),a(e,tt,t),c(ne,e,t),a(e,nt,t),a(e,le,t),a(e,lt,t),c(se,e,t),a(e,st,t),a(e,ae,t),a(e,at,t),c(oe,e,t),a(e,ot,t),a(e,ie,t),a(e,it,t),c(re,e,t),a(e,rt,t),c(pe,e,t),a(e,pt,t),a(e,me,t),a(e,mt,t),a(e,de,t),a(e,dt,t),c(ue,e,t),a(e,ut,t),a(e,fe,t),a(e,ft,t),c(ce,e,t),a(e,ct,t),a(e,Me,t),a(e,Mt,t),c(ge,e,t),a(e,gt,t),c(G,e,t),a(e,ht,t),c(he,e,t),a(e,bt,t),a(e,be,t),a(e,yt,t),a(e,ye,t),a(e,$t,t),c($e,e,t),a(e,Tt,t),c(v,e,t),a(e,Jt,t),c(Te,e,t),a(e,wt,t),a(e,Je,t),a(e,_t,t),c(I,e,t),a(e,Ut,t),c(we,e,t),a(e,kt,t),a(e,_e,t),a(e,Zt,t),a(e,Ue,t),a(e,jt,t),c(ke,e,t),a(e,Gt,t),c(W,e,t),a(e,vt,t),c(Ze,e,t),a(e,It,t),a(e,je,t),a(e,Wt,t),a(e,Ge,t),a(e,Xt,t),c(ve,e,t),a(e,Vt,t),a(e,Ie,t),Bt=!0},p(e,[t]){const un={};t&2&&(un.$$scope={dirty:t,ctx:e}),Z.$set(un);const fn={};t&2&&(fn.$$scope={dirty:t,ctx:e}),j.$set(fn);const cn={};t&2&&(cn.$$scope={dirty:t,ctx:e}),G.$set(cn);const Mn={};t&2&&(Mn.$$scope={dirty:t,ctx:e}),v.$set(Mn);const gn={};t&2&&(gn.$$scope={dirty:t,ctx:e}),I.$set(gn);const hn={};t&2&&(hn.$$scope={dirty:t,ctx:e}),W.$set(hn)},i(e){Bt||(M(o.$$.fragment,e),M(J.$$.fragment,e),M(Z.$$.fragment,e),M(R.$$.fragment,e),M(x.$$.fragment,e),M(N.$$.fragment,e),M(j.$$.fragment,e),M(Y.$$.fragment,e),M(E.$$.fragment,e),M(S.$$.fragment,e),M(D.$$.fragment,e),M(K.$$.fragment,e),M(ne.$$.fragment,e),M(se.$$.fragment,e),M(oe.$$.fragment,e),M(re.$$.fragment,e),M(pe.$$.fragment,e),M(ue.$$.fragment,e),M(ce.$$.fragment,e),M(ge.$$.fragment,e),M(G.$$.fragment,e),M(he.$$.fragment,e),M($e.$$.fragment,e),M(v.$$.fragment,e),M(Te.$$.fragment,e),M(I.$$.fragment,e),M(we.$$.fragment,e),M(ke.$$.fragment,e),M(W.$$.fragment,e),M(Ze.$$.fragment,e),M(ve.$$.fragment,e),Bt=!0)},o(e){g(o.$$.fragment,e),g(J.$$.fragment,e),g(Z.$$.fragment,e),g(R.$$.fragment,e),g(x.$$.fragment,e),g(N.$$.fragment,e),g(j.$$.fragment,e),g(Y.$$.fragment,e),g(E.$$.fragment,e),g(S.$$.fragment,e),g(D.$$.fragment,e),g(K.$$.fragment,e),g(ne.$$.fragment,e),g(se.$$.fragment,e),g(oe.$$.fragment,e),g(re.$$.fragment,e),g(pe.$$.fragment,e),g(ue.$$.fragment,e),g(ce.$$.fragment,e),g(ge.$$.fragment,e),g(G.$$.fragment,e),g(he.$$.fragment,e),g($e.$$.fragment,e),g(v.$$.fragment,e),g(Te.$$.fragment,e),g(I.$$.fragment,e),g(we.$$.fragment,e),g(ke.$$.fragment,e),g(W.$$.fragment,e),g(Ze.$$.fragment,e),g(ve.$$.fragment,e),Bt=!1},d(e){e&&(s(i),s(n),s(d),s(m),s(_),s(V),s(We),s(B),s(Xe),s(Ve),s(Be),s(C),s(Re),s(Ce),s(z),s(xe),s(H),s(ze),s(Q),s(He),s(Qe),s(F),s(Ne),s(Fe),s(Ye),s(q),s(qe),s(Ee),s(Se),s(L),s(Le),s(A),s(Ae),s(De),s(P),s(Pe),s(Ke),s(O),s(Oe),s(ee),s(et),s(te),s(tt),s(nt),s(le),s(lt),s(st),s(ae),s(at),s(ot),s(ie),s(it),s(rt),s(pt),s(me),s(mt),s(de),s(dt),s(ut),s(fe),s(ft),s(ct),s(Me),s(Mt),s(gt),s(ht),s(bt),s(be),s(yt),s(ye),s($t),s(Tt),s(Jt),s(wt),s(Je),s(_t),s(Ut),s(kt),s(_e),s(Zt),s(Ue),s(jt),s(Gt),s(vt),s(It),s(je),s(Wt),s(Ge),s(Xt),s(Vt),s(Ie)),s(l),h(o,e),h(J,e),h(Z,e),h(R,e),h(x,e),h(N,e),h(j,e),h(Y,e),h(E,e),h(S,e),h(D,e),h(K,e),h(ne,e),h(se,e),h(oe,e),h(re,e),h(pe,e),h(ue,e),h(ce,e),h(ge,e),h(G,e),h(he,e),h($e,e),h(v,e),h(Te,e),h(I,e),h(we,e),h(ke,e),h(W,e),h(Ze,e),h(ve,e)}}}const Yn='{"title":"Text generation","local":"text-generation","sections":[{"title":"Default generate","local":"default-generate","sections":[],"depth":2},{"title":"Generation configuration","local":"generation-configuration","sections":[{"title":"Saving","local":"saving","sections":[],"depth":3}],"depth":2},{"title":"Common Options","local":"common-options","sections":[],"depth":2},{"title":"Pitfalls","local":"pitfalls","sections":[{"title":"Output length","local":"output-length","sections":[],"depth":3},{"title":"Decoding strategy","local":"decoding-strategy","sections":[],"depth":3},{"title":"Padding side","local":"padding-side","sections":[],"depth":3},{"title":"Prompt format","local":"prompt-format","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2}],"depth":1}';function qn($){return Jn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class On extends wn{constructor(l){super(),_n(this,l,qn,Fn,Tn,{})}}export{On as component};
