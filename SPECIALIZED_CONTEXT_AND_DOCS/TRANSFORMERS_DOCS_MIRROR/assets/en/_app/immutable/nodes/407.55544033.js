import{s as Et,o as At,n as U}from"../chunks/scheduler.18a86fab.js";import{S as Dt,i as Ot,g as h,s as i,r as u,A as Kt,h as f,f as r,c as l,j as V,x as M,u as g,k as R,l as eo,y as p,a as m,v as _,d as w,t as v,w as b}from"../chunks/index.98837b22.js";import{T as Fe}from"../chunks/Tip.77304350.js";import{D as ge}from"../chunks/Docstring.a1ef7999.js";import{C as We}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ke}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Ze,E as to}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as oo,a as qt}from"../chunks/HfOption.6641485e.js";function no(y){let t,d="Click on the Swin Transformer V2 models in the right sidebar for more examples of how to apply Swin Transformer V2 to vision tasks.";return{c(){t=h("p"),t.textContent=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-jkaekw"&&(t.textContent=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function so(y){let t,d;return t=new We({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJpbWFnZS1jbGFzc2lmaWNhdGlvbiUyMiUyQyUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlJTNEMCUwQSklMEFwaXBlbGluZSglMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZwaXBlbGluZS1jYXQtY2hvbmsuanBlZyUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;image-classification&quot;</span>,
    model=<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>,
    dtype=torch.float16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){g(t.$$.fragment,o)},m(o,a){_(t,o,a),d=!0},p:U,i(o){d||(w(t.$$.fragment,o),d=!0)},o(o){v(t.$$.fragment,o),d=!1},d(o){b(t,o)}}}function ao(y){let t,d;return t=new We({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTJDJTIwQXV0b0ltYWdlUHJvY2Vzc29yJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtaWNyb3NvZnQlMkZzd2ludjItdGlueS1wYXRjaDQtd2luZG93OC0yNTYlMjIlMkMlMEEpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJtaWNyb3NvZnQlMkZzd2ludjItdGlueS1wYXRjaDQtd2luZG93OC0yNTYlMjIlMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUwQSklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZwaXBlbGluZS1jYXQtY2hvbmsuanBlZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQXByZWRpY3RlZF9jbGFzc19pZCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoZGltJTNELTEpLml0ZW0oKSUwQXByZWRpY3RlZF9jbGFzc19sYWJlbCUyMCUzRCUyMG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RCUwQXByaW50KGYlMjJUaGUlMjBwcmVkaWN0ZWQlMjBjbGFzcyUyMGxhYmVsJTIwaXMlM0ElMjAlN0JwcmVkaWN0ZWRfY2xhc3NfbGFiZWwlN0QlMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>,
)
model = AutoModelForImageClassification.from_pretrained(
    <span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-keyword">with</span> torch.no_grad():
  logits = model(**inputs).logits

predicted_class_id = logits.argmax(dim=-<span class="hljs-number">1</span>).item()
predicted_class_label = model.config.id2label[predicted_class_id]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The predicted class label is: <span class="hljs-subst">{predicted_class_label}</span>&quot;</span>)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){g(t.$$.fragment,o)},m(o,a){_(t,o,a),d=!0},p:U,i(o){d||(w(t.$$.fragment,o),d=!0)},o(o){v(t.$$.fragment,o),d=!1},d(o){b(t,o)}}}function ro(y){let t,d,o,a;return t=new qt({props:{id:"usage",option:"Pipeline",$$slots:{default:[so]},$$scope:{ctx:y}}}),o=new qt({props:{id:"usage",option:"AutoModel",$$slots:{default:[ao]},$$scope:{ctx:y}}}),{c(){u(t.$$.fragment),d=i(),u(o.$$.fragment)},l(c){g(t.$$.fragment,c),d=l(c),g(o.$$.fragment,c)},m(c,n){_(t,c,n),m(c,d,n),_(o,c,n),a=!0},p(c,n){const T={};n&2&&(T.$$scope={dirty:n,ctx:c}),t.$set(T);const x={};n&2&&(x.$$scope={dirty:n,ctx:c}),o.$set(x)},i(c){a||(w(t.$$.fragment,c),w(o.$$.fragment,c),a=!0)},o(c){v(t.$$.fragment,c),v(o.$$.fragment,c),a=!1},d(c){c&&r(d),b(t,c),b(o,c)}}}function io(y){let t,d="Example:",o,a,c;return a=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aW52MkNvbmZpZyUyQyUyMFN3aW52Mk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFN3aW52MiUyMG1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBTd2ludjJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwbWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBTd2ludjJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Swinv2Config, Swinv2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Swinv2 microsoft/swinv2-tiny-patch4-window8-256 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Swinv2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the microsoft/swinv2-tiny-patch4-window8-256 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=h("p"),t.textContent=d,o=i(),u(a.$$.fragment)},l(n){t=f(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=d),o=l(n),g(a.$$.fragment,n)},m(n,T){m(n,t,T),m(n,o,T),_(a,n,T),c=!0},p:U,i(n){c||(w(a.$$.fragment,n),c=!0)},o(n){v(a.$$.fragment,n),c=!1},d(n){n&&(r(t),r(o)),b(a,n)}}}function lo(y){let t,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function co(y){let t,d="Example:",o,a,c;return a=new We({props:{code:"",highlighted:"",wrap:!1}}),{c(){t=h("p"),t.textContent=d,o=i(),u(a.$$.fragment)},l(n){t=f(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=d),o=l(n),g(a.$$.fragment,n)},m(n,T){m(n,t,T),m(n,o,T),_(a,n,T),c=!0},p:U,i(n){c||(w(a.$$.fragment,n),c=!0)},o(n){v(a.$$.fragment,n),c=!1},d(n){n&&(r(t),r(o)),b(a,n)}}}function mo(y){let t,d=`Note that we provide a script to pre-train this model on custom data in our <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">examples
directory</a>.`;return{c(){t=h("p"),t.innerHTML=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-7i3y9o"&&(t.innerHTML=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function po(y){let t,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function ho(y){let t,d="Examples:",o,a,c;return a=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aW52MkZvck1hc2tlZEltYWdlTW9kZWxpbmclMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyKSUwQW1vZGVsJTIwJTNEJTIwU3dpbnYyRm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyKSUwQSUwQW51bV9wYXRjaGVzJTIwJTNEJTIwKG1vZGVsLmNvbmZpZy5pbWFnZV9zaXplJTIwJTJGJTJGJTIwbW9kZWwuY29uZmlnLnBhdGNoX3NpemUpJTIwKiolMjAyJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVzJTBBJTIzJTIwY3JlYXRlJTIwcmFuZG9tJTIwYm9vbGVhbiUyMG1hc2slMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fcGF0Y2hlcyklMEFib29sX21hc2tlZF9wb3MlMjAlM0QlMjB0b3JjaC5yYW5kaW50KGxvdyUzRDAlMkMlMjBoaWdoJTNEMiUyQyUyMHNpemUlM0QoMSUyQyUyMG51bV9wYXRjaGVzKSkuYm9vbCgpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyUyQyUyMGJvb2xfbWFza2VkX3BvcyUzRGJvb2xfbWFza2VkX3BvcyklMEFsb3NzJTJDJTIwcmVjb25zdHJ1Y3RlZF9waXhlbF92YWx1ZXMlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMkMlMjBvdXRwdXRzLnJlY29uc3RydWN0aW9uJTBBbGlzdChyZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Swinv2ForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2ForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(reconstructed_pixel_values.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>]`,wrap:!1}}),{c(){t=h("p"),t.textContent=d,o=i(),u(a.$$.fragment)},l(n){t=f(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=d),o=l(n),g(a.$$.fragment,n)},m(n,T){m(n,t,T),m(n,o,T),_(a,n,T),c=!0},p:U,i(n){c||(w(a.$$.fragment,n),c=!0)},o(n){v(a.$$.fragment,n),c=!1},d(n){n&&(r(t),r(o)),b(a,n)}}}function fo(y){let t,d=`Note that it’s possible to fine-tune SwinV2 on higher resolution images than the ones it has been trained on, by
setting <code>interpolate_pos_encoding</code> to <code>True</code> in the forward of the model. This will interpolate the pre-trained
position embeddings to the higher resolution.`;return{c(){t=h("p"),t.innerHTML=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-1dn4x3r"&&(t.innerHTML=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function uo(y){let t,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=d},l(o){t=f(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=d)},m(o,a){m(o,t,a)},p:U,d(o){o&&r(t)}}}function go(y){let t,d="Example:",o,a,c;return a=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aW52MkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMiklMEFtb2RlbCUyMCUzRCUyMFN3aW52MkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Swinv2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){t=h("p"),t.textContent=d,o=i(),u(a.$$.fragment)},l(n){t=f(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=d),o=l(n),g(a.$$.fragment,n)},m(n,T){m(n,t,T),m(n,o,T),_(a,n,T),c=!0},p:U,i(n){c||(w(a.$$.fragment,n),c=!0)},o(n){v(a.$$.fragment,n),c=!1},d(n){n&&(r(t),r(o)),b(a,n)}}}function _o(y){let t,d,o,a,c,n="<em>This model was released on 2021-11-18 and added to Hugging Face Transformers on 2022-07-27.</em>",T,x,kt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',xe,K,ze,ee,Jt='<a href="https://huggingface.co/papers/2111.09883" rel="nofollow">Swin Transformer V2</a> is a 3B parameter model that focuses on how to scale a vision model to billions of parameters. It introduces techniques like residual-post-norm combined with cosine attention for improved training stability, log-spaced continuous position bias to better handle varying image resolutions between pre-training and fine-tuning, and a new pre-training method (SimMIM) to reduce the need for large amounts of labeled data. These improvements enable efficiently training very large models (up to 3 billion parameters) capable of processing high-resolution images.',Ne,te,St='You can find official Swin Transformer V2 checkpoints under the <a href="https://huggingface.co/microsoft?search_models=swinv2" rel="nofollow">Microsoft</a> organization.',Be,X,Ge,Y,Ve,oe,Re,ne,It='<li>Swin Transformer V2 can pad the inputs for any input height and width divisible by <code>32</code>.</li> <li>Swin Transformer V2 can be used as a <a href="../backbones">backbone</a>. When <code>output_hidden_states = True</code>, it outputs both <code>hidden_states</code> and <code>reshaped_hidden_states</code>. The <code>reshaped_hidden_states</code> have a shape of <code>(batch, num_channels, height, width)</code> rather than <code>(batch_size, sequence_length, num_channels)</code>.</li>',Xe,se,Ye,I,ae,et,_e,Ct=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a>. It is used to instantiate a Swin
Transformer v2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Swin Transformer v2
<a href="https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256" rel="nofollow">microsoft/swinv2-tiny-patch4-window8-256</a>
architecture.`,tt,we,Ft=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ot,H,He,re,Le,k,ie,nt,ve,Zt="The bare Swinv2 Model outputting raw hidden-states without any specific head on top.",st,be,Wt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,at,Me,Ut=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,rt,F,le,it,ye,xt='The <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> forward method, overrides the <code>__call__</code> special method.',lt,L,dt,Q,Qe,de,Pe,$,ce,ct,Te,zt=`Swinv2 Model with a decoder on top for masked image modeling, as proposed in
<a href="https://huggingface.co/papers/2111.09886" rel="nofollow">SimMIM</a>.`,mt,P,pt,$e,Nt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ht,je,Bt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ft,Z,me,ut,ke,Gt='The <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',gt,q,_t,E,qe,pe,Ee,j,he,wt,Je,Vt=`Swinv2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,vt,A,bt,Se,Rt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Mt,Ie,Xt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,yt,W,fe,Tt,Ce,Yt='The <a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',$t,D,jt,O,Ae,ue,De,Ue,Oe;return K=new Ze({props:{title:"Swin Transformer V2",local:"swin-transformer-v2",headingTag:"h1"}}),X=new Fe({props:{warning:!1,$$slots:{default:[no]},$$scope:{ctx:y}}}),Y=new oo({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[ro]},$$scope:{ctx:y}}}),oe=new Ze({props:{title:"Notes",local:"notes",headingTag:"h2"}}),se=new Ze({props:{title:"Swinv2Config",local:"transformers.Swinv2Config",headingTag:"h2"}}),ae=new ge({props:{name:"class transformers.Swinv2Config",anchor:"transformers.Swinv2Config",parameters:[{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 4"},{name:"num_channels",val:" = 3"},{name:"embed_dim",val:" = 96"},{name:"depths",val:" = [2, 2, 6, 2]"},{name:"num_heads",val:" = [3, 6, 12, 24]"},{name:"window_size",val:" = 7"},{name:"pretrained_window_sizes",val:" = [0, 0, 0, 0]"},{name:"mlp_ratio",val:" = 4.0"},{name:"qkv_bias",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.1"},{name:"hidden_act",val:" = 'gelu'"},{name:"use_absolute_embeddings",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"encoder_stride",val:" = 32"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Swinv2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Swinv2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Swinv2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.Swinv2Config.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Dimensionality of patch embedding.`,name:"embed_dim"},{anchor:"transformers.Swinv2Config.depths",description:`<strong>depths</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[2, 2, 6, 2]</code>) &#x2014;
Depth of each layer in the Transformer encoder.`,name:"depths"},{anchor:"transformers.Swinv2Config.num_heads",description:`<strong>num_heads</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[3, 6, 12, 24]</code>) &#x2014;
Number of attention heads in each layer of the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.Swinv2Config.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Size of windows.`,name:"window_size"},{anchor:"transformers.Swinv2Config.pretrained_window_sizes",description:`<strong>pretrained_window_sizes</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[0, 0, 0, 0]</code>) &#x2014;
Size of windows during pretraining.`,name:"pretrained_window_sizes"},{anchor:"transformers.Swinv2Config.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.Swinv2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not a learnable bias should be added to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.Swinv2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings and encoder.`,name:"hidden_dropout_prob"},{anchor:"transformers.Swinv2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Swinv2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.Swinv2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Swinv2Config.use_absolute_embeddings",description:`<strong>use_absolute_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add absolute position embeddings to the patch embeddings.`,name:"use_absolute_embeddings"},{anchor:"transformers.Swinv2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Swinv2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Swinv2Config.encoder_stride",description:`<strong>encoder_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Factor to increase the spatial resolution by in the decoder head for masked image modeling.`,name:"encoder_stride"},{anchor:"transformers.Swinv2Config.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage.`,name:"out_features"},{anchor:"transformers.Swinv2Config.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage.`,name:"out_indices"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/configuration_swinv2.py#L25"}}),H=new Ke({props:{anchor:"transformers.Swinv2Config.example",$$slots:{default:[io]},$$scope:{ctx:y}}}),re=new Ze({props:{title:"Swinv2Model",local:"transformers.Swinv2Model",headingTag:"h2"}}),ie=new ge({props:{name:"class transformers.Swinv2Model",anchor:"transformers.Swinv2Model",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"},{name:"use_mask_token",val:" = False"}],parametersDescription:[{anchor:"transformers.Swinv2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.Swinv2Model.add_pooling_layer",description:`<strong>add_pooling_layer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to apply pooling layer.`,name:"add_pooling_layer"},{anchor:"transformers.Swinv2Model.use_mask_token",description:`<strong>use_mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create and apply mask tokens in the embedding layer.`,name:"use_mask_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L950"}}),le=new ge({props:{name:"forward",anchor:"transformers.Swinv2Model.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Swinv2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Swinv2Model.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.Swinv2Model.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2Model.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.Swinv2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L983",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2ModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>, <em>optional</em>, returned when <code>add_pooling_layer=True</code> is passed) — Average pooling of the last layer hidden-state.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2ModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),L=new Fe({props:{$$slots:{default:[lo]},$$scope:{ctx:y}}}),Q=new Ke({props:{anchor:"transformers.Swinv2Model.forward.example",$$slots:{default:[co]},$$scope:{ctx:y}}}),de=new Ze({props:{title:"Swinv2ForMaskedImageModeling",local:"transformers.Swinv2ForMaskedImageModeling",headingTag:"h2"}}),ce=new ge({props:{name:"class transformers.Swinv2ForMaskedImageModeling",anchor:"transformers.Swinv2ForMaskedImageModeling",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Swinv2ForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L1063"}}),P=new Fe({props:{$$slots:{default:[mo]},$$scope:{ctx:y}}}),me=new ge({props:{name:"forward",anchor:"transformers.Swinv2ForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L1080",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2MaskedImageModelingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>bool_masked_pos</code> is provided) — Masked image modeling (MLM) loss.</p>
</li>
<li>
<p><strong>reconstruction</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Reconstructed pixel values.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2MaskedImageModelingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),q=new Fe({props:{$$slots:{default:[po]},$$scope:{ctx:y}}}),E=new Ke({props:{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.example",$$slots:{default:[ho]},$$scope:{ctx:y}}}),pe=new Ze({props:{title:"Swinv2ForImageClassification",local:"transformers.Swinv2ForImageClassification",headingTag:"h2"}}),he=new ge({props:{name:"class transformers.Swinv2ForImageClassification",anchor:"transformers.Swinv2ForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Swinv2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L1181"}}),A=new Fe({props:{$$slots:{default:[fo]},$$scope:{ctx:y}}}),fe=new ge({props:{name:"forward",anchor:"transformers.Swinv2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Swinv2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Swinv2ForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.Swinv2ForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2ForImageClassification.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.Swinv2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/swinv2/modeling_swinv2.py#L1196",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2ImageClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2ImageClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),D=new Fe({props:{$$slots:{default:[uo]},$$scope:{ctx:y}}}),O=new Ke({props:{anchor:"transformers.Swinv2ForImageClassification.forward.example",$$slots:{default:[go]},$$scope:{ctx:y}}}),ue=new to({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md"}}),{c(){t=h("meta"),d=i(),o=h("p"),a=i(),c=h("p"),c.innerHTML=n,T=i(),x=h("div"),x.innerHTML=kt,xe=i(),u(K.$$.fragment),ze=i(),ee=h("p"),ee.innerHTML=Jt,Ne=i(),te=h("p"),te.innerHTML=St,Be=i(),u(X.$$.fragment),Ge=i(),u(Y.$$.fragment),Ve=i(),u(oe.$$.fragment),Re=i(),ne=h("ul"),ne.innerHTML=It,Xe=i(),u(se.$$.fragment),Ye=i(),I=h("div"),u(ae.$$.fragment),et=i(),_e=h("p"),_e.innerHTML=Ct,tt=i(),we=h("p"),we.innerHTML=Ft,ot=i(),u(H.$$.fragment),He=i(),u(re.$$.fragment),Le=i(),k=h("div"),u(ie.$$.fragment),nt=i(),ve=h("p"),ve.textContent=Zt,st=i(),be=h("p"),be.innerHTML=Wt,at=i(),Me=h("p"),Me.innerHTML=Ut,rt=i(),F=h("div"),u(le.$$.fragment),it=i(),ye=h("p"),ye.innerHTML=xt,lt=i(),u(L.$$.fragment),dt=i(),u(Q.$$.fragment),Qe=i(),u(de.$$.fragment),Pe=i(),$=h("div"),u(ce.$$.fragment),ct=i(),Te=h("p"),Te.innerHTML=zt,mt=i(),u(P.$$.fragment),pt=i(),$e=h("p"),$e.innerHTML=Nt,ht=i(),je=h("p"),je.innerHTML=Bt,ft=i(),Z=h("div"),u(me.$$.fragment),ut=i(),ke=h("p"),ke.innerHTML=Gt,gt=i(),u(q.$$.fragment),_t=i(),u(E.$$.fragment),qe=i(),u(pe.$$.fragment),Ee=i(),j=h("div"),u(he.$$.fragment),wt=i(),Je=h("p"),Je.textContent=Vt,vt=i(),u(A.$$.fragment),bt=i(),Se=h("p"),Se.innerHTML=Rt,Mt=i(),Ie=h("p"),Ie.innerHTML=Xt,yt=i(),W=h("div"),u(fe.$$.fragment),Tt=i(),Ce=h("p"),Ce.innerHTML=Yt,$t=i(),u(D.$$.fragment),jt=i(),u(O.$$.fragment),Ae=i(),u(ue.$$.fragment),De=i(),Ue=h("p"),this.h()},l(e){const s=Kt("svelte-u9bgzb",document.head);t=f(s,"META",{name:!0,content:!0}),s.forEach(r),d=l(e),o=f(e,"P",{}),V(o).forEach(r),a=l(e),c=f(e,"P",{"data-svelte-h":!0}),M(c)!=="svelte-vwvwjk"&&(c.innerHTML=n),T=l(e),x=f(e,"DIV",{style:!0,"data-svelte-h":!0}),M(x)!=="svelte-wa5t4p"&&(x.innerHTML=kt),xe=l(e),g(K.$$.fragment,e),ze=l(e),ee=f(e,"P",{"data-svelte-h":!0}),M(ee)!=="svelte-1k5jmqs"&&(ee.innerHTML=Jt),Ne=l(e),te=f(e,"P",{"data-svelte-h":!0}),M(te)!=="svelte-1b5t8xh"&&(te.innerHTML=St),Be=l(e),g(X.$$.fragment,e),Ge=l(e),g(Y.$$.fragment,e),Ve=l(e),g(oe.$$.fragment,e),Re=l(e),ne=f(e,"UL",{"data-svelte-h":!0}),M(ne)!=="svelte-133le3i"&&(ne.innerHTML=It),Xe=l(e),g(se.$$.fragment,e),Ye=l(e),I=f(e,"DIV",{class:!0});var z=V(I);g(ae.$$.fragment,z),et=l(z),_e=f(z,"P",{"data-svelte-h":!0}),M(_e)!=="svelte-1ahet2w"&&(_e.innerHTML=Ct),tt=l(z),we=f(z,"P",{"data-svelte-h":!0}),M(we)!=="svelte-1ek1ss9"&&(we.innerHTML=Ft),ot=l(z),g(H.$$.fragment,z),z.forEach(r),He=l(e),g(re.$$.fragment,e),Le=l(e),k=f(e,"DIV",{class:!0});var C=V(k);g(ie.$$.fragment,C),nt=l(C),ve=f(C,"P",{"data-svelte-h":!0}),M(ve)!=="svelte-pfx5dl"&&(ve.textContent=Zt),st=l(C),be=f(C,"P",{"data-svelte-h":!0}),M(be)!=="svelte-q52n56"&&(be.innerHTML=Wt),at=l(C),Me=f(C,"P",{"data-svelte-h":!0}),M(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Ut),rt=l(C),F=f(C,"DIV",{class:!0});var N=V(F);g(le.$$.fragment,N),it=l(N),ye=f(N,"P",{"data-svelte-h":!0}),M(ye)!=="svelte-1z94wm"&&(ye.innerHTML=xt),lt=l(N),g(L.$$.fragment,N),dt=l(N),g(Q.$$.fragment,N),N.forEach(r),C.forEach(r),Qe=l(e),g(de.$$.fragment,e),Pe=l(e),$=f(e,"DIV",{class:!0});var J=V($);g(ce.$$.fragment,J),ct=l(J),Te=f(J,"P",{"data-svelte-h":!0}),M(Te)!=="svelte-1ia68bj"&&(Te.innerHTML=zt),mt=l(J),g(P.$$.fragment,J),pt=l(J),$e=f(J,"P",{"data-svelte-h":!0}),M($e)!=="svelte-q52n56"&&($e.innerHTML=Nt),ht=l(J),je=f(J,"P",{"data-svelte-h":!0}),M(je)!=="svelte-hswkmf"&&(je.innerHTML=Bt),ft=l(J),Z=f(J,"DIV",{class:!0});var B=V(Z);g(me.$$.fragment,B),ut=l(B),ke=f(B,"P",{"data-svelte-h":!0}),M(ke)!=="svelte-3q9s3c"&&(ke.innerHTML=Gt),gt=l(B),g(q.$$.fragment,B),_t=l(B),g(E.$$.fragment,B),B.forEach(r),J.forEach(r),qe=l(e),g(pe.$$.fragment,e),Ee=l(e),j=f(e,"DIV",{class:!0});var S=V(j);g(he.$$.fragment,S),wt=l(S),Je=f(S,"P",{"data-svelte-h":!0}),M(Je)!=="svelte-13r27gg"&&(Je.textContent=Vt),vt=l(S),g(A.$$.fragment,S),bt=l(S),Se=f(S,"P",{"data-svelte-h":!0}),M(Se)!=="svelte-q52n56"&&(Se.innerHTML=Rt),Mt=l(S),Ie=f(S,"P",{"data-svelte-h":!0}),M(Ie)!=="svelte-hswkmf"&&(Ie.innerHTML=Xt),yt=l(S),W=f(S,"DIV",{class:!0});var G=V(W);g(fe.$$.fragment,G),Tt=l(G),Ce=f(G,"P",{"data-svelte-h":!0}),M(Ce)!=="svelte-1vqbztg"&&(Ce.innerHTML=Yt),$t=l(G),g(D.$$.fragment,G),jt=l(G),g(O.$$.fragment,G),G.forEach(r),S.forEach(r),Ae=l(e),g(ue.$$.fragment,e),De=l(e),Ue=f(e,"P",{}),V(Ue).forEach(r),this.h()},h(){R(t,"name","hf:doc:metadata"),R(t,"content",wo),eo(x,"float","right"),R(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){p(document.head,t),m(e,d,s),m(e,o,s),m(e,a,s),m(e,c,s),m(e,T,s),m(e,x,s),m(e,xe,s),_(K,e,s),m(e,ze,s),m(e,ee,s),m(e,Ne,s),m(e,te,s),m(e,Be,s),_(X,e,s),m(e,Ge,s),_(Y,e,s),m(e,Ve,s),_(oe,e,s),m(e,Re,s),m(e,ne,s),m(e,Xe,s),_(se,e,s),m(e,Ye,s),m(e,I,s),_(ae,I,null),p(I,et),p(I,_e),p(I,tt),p(I,we),p(I,ot),_(H,I,null),m(e,He,s),_(re,e,s),m(e,Le,s),m(e,k,s),_(ie,k,null),p(k,nt),p(k,ve),p(k,st),p(k,be),p(k,at),p(k,Me),p(k,rt),p(k,F),_(le,F,null),p(F,it),p(F,ye),p(F,lt),_(L,F,null),p(F,dt),_(Q,F,null),m(e,Qe,s),_(de,e,s),m(e,Pe,s),m(e,$,s),_(ce,$,null),p($,ct),p($,Te),p($,mt),_(P,$,null),p($,pt),p($,$e),p($,ht),p($,je),p($,ft),p($,Z),_(me,Z,null),p(Z,ut),p(Z,ke),p(Z,gt),_(q,Z,null),p(Z,_t),_(E,Z,null),m(e,qe,s),_(pe,e,s),m(e,Ee,s),m(e,j,s),_(he,j,null),p(j,wt),p(j,Je),p(j,vt),_(A,j,null),p(j,bt),p(j,Se),p(j,Mt),p(j,Ie),p(j,yt),p(j,W),_(fe,W,null),p(W,Tt),p(W,Ce),p(W,$t),_(D,W,null),p(W,jt),_(O,W,null),m(e,Ae,s),_(ue,e,s),m(e,De,s),m(e,Ue,s),Oe=!0},p(e,[s]){const z={};s&2&&(z.$$scope={dirty:s,ctx:e}),X.$set(z);const C={};s&2&&(C.$$scope={dirty:s,ctx:e}),Y.$set(C);const N={};s&2&&(N.$$scope={dirty:s,ctx:e}),H.$set(N);const J={};s&2&&(J.$$scope={dirty:s,ctx:e}),L.$set(J);const B={};s&2&&(B.$$scope={dirty:s,ctx:e}),Q.$set(B);const S={};s&2&&(S.$$scope={dirty:s,ctx:e}),P.$set(S);const G={};s&2&&(G.$$scope={dirty:s,ctx:e}),q.$set(G);const Ht={};s&2&&(Ht.$$scope={dirty:s,ctx:e}),E.$set(Ht);const Lt={};s&2&&(Lt.$$scope={dirty:s,ctx:e}),A.$set(Lt);const Qt={};s&2&&(Qt.$$scope={dirty:s,ctx:e}),D.$set(Qt);const Pt={};s&2&&(Pt.$$scope={dirty:s,ctx:e}),O.$set(Pt)},i(e){Oe||(w(K.$$.fragment,e),w(X.$$.fragment,e),w(Y.$$.fragment,e),w(oe.$$.fragment,e),w(se.$$.fragment,e),w(ae.$$.fragment,e),w(H.$$.fragment,e),w(re.$$.fragment,e),w(ie.$$.fragment,e),w(le.$$.fragment,e),w(L.$$.fragment,e),w(Q.$$.fragment,e),w(de.$$.fragment,e),w(ce.$$.fragment,e),w(P.$$.fragment,e),w(me.$$.fragment,e),w(q.$$.fragment,e),w(E.$$.fragment,e),w(pe.$$.fragment,e),w(he.$$.fragment,e),w(A.$$.fragment,e),w(fe.$$.fragment,e),w(D.$$.fragment,e),w(O.$$.fragment,e),w(ue.$$.fragment,e),Oe=!0)},o(e){v(K.$$.fragment,e),v(X.$$.fragment,e),v(Y.$$.fragment,e),v(oe.$$.fragment,e),v(se.$$.fragment,e),v(ae.$$.fragment,e),v(H.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(le.$$.fragment,e),v(L.$$.fragment,e),v(Q.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(P.$$.fragment,e),v(me.$$.fragment,e),v(q.$$.fragment,e),v(E.$$.fragment,e),v(pe.$$.fragment,e),v(he.$$.fragment,e),v(A.$$.fragment,e),v(fe.$$.fragment,e),v(D.$$.fragment,e),v(O.$$.fragment,e),v(ue.$$.fragment,e),Oe=!1},d(e){e&&(r(d),r(o),r(a),r(c),r(T),r(x),r(xe),r(ze),r(ee),r(Ne),r(te),r(Be),r(Ge),r(Ve),r(Re),r(ne),r(Xe),r(Ye),r(I),r(He),r(Le),r(k),r(Qe),r(Pe),r($),r(qe),r(Ee),r(j),r(Ae),r(De),r(Ue)),r(t),b(K,e),b(X,e),b(Y,e),b(oe,e),b(se,e),b(ae),b(H),b(re,e),b(ie),b(le),b(L),b(Q),b(de,e),b(ce),b(P),b(me),b(q),b(E),b(pe,e),b(he),b(A),b(fe),b(D),b(O),b(ue,e)}}}const wo='{"title":"Swin Transformer V2","local":"swin-transformer-v2","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Swinv2Config","local":"transformers.Swinv2Config","sections":[],"depth":2},{"title":"Swinv2Model","local":"transformers.Swinv2Model","sections":[],"depth":2},{"title":"Swinv2ForMaskedImageModeling","local":"transformers.Swinv2ForMaskedImageModeling","sections":[],"depth":2},{"title":"Swinv2ForImageClassification","local":"transformers.Swinv2ForImageClassification","sections":[],"depth":2}],"depth":1}';function vo(y){return At(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class So extends Dt{constructor(t){super(),Ot(this,t,vo,_o,Et,{})}}export{So as component};
