import{s as ge,o as Me,n as we}from"../chunks/scheduler.18a86fab.js";import{S as be,i as ye,g as o,s,r as d,A as Te,h as r,f as n,c as a,j as he,x as m,u as h,k as le,y as $e,a as l,v as f,d as g,t as M,w}from"../chunks/index.98837b22.js";import{T as Je}from"../chunks/Tip.77304350.js";import{C as fe}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ne,E as ve}from"../chunks/getInferenceSnippets.06c2775f.js";function xe(X){let i,b='DePlot is a model trained using <code>Pix2Struct</code> architecture. For API reference, see <a href="pix2struct"><code>Pix2Struct</code> documentation</a>.';return{c(){i=o("p"),i.innerHTML=b},l(p){i=r(p,"P",{"data-svelte-h":!0}),m(i)!=="svelte-zgoioq"&&(i.innerHTML=b)},m(p,B){l(p,i,B)},p:we,d(p){p&&n(i)}}}function je(X){let i,b,p,B,y,se="<em>This model was released on 2022-12-20 and added to Hugging Face Transformers on 2023-06-20.</em>",H,T,V,c,ae='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Y,$,I,J,ie='DePlot was proposed in the paper <a href="https://huggingface.co/papers/2212.10505" rel="nofollow">DePlot: One-shot visual language reasoning by plot-to-table translation</a> from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.',z,v,oe="The abstract of the paper states the following:",L,x,re="<em>Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than &gt;28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.</em>",F,j,pe=`DePlot is a model that is trained using <code>Pix2Struct</code> architecture. You can find more information about <code>Pix2Struct</code> in the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct" rel="nofollow">Pix2Struct documentation</a>.
DePlot is a Visual Question Answering subset of <code>Pix2Struct</code> architecture. It renders the input question on the image and predicts the answer.`,D,_,E,k,me="Currently one checkpoint is available for DePlot:",S,R,ce="<li><code>google/deplot</code>: DePlot fine-tuned on ChartQA dataset</li>",N,U,A,P,Q,C,ue='To fine-tune DePlot, refer to the pix2struct <a href="https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb" rel="nofollow">fine-tuning notebook</a>. For <code>Pix2Struct</code> models, we have found out that fine-tuning the model with Adafactor and cosine learning rate scheduler leads to faster convergence:',q,Z,K,u,O,W,ee,G,te;return T=new ne({props:{title:"DePlot",local:"deplot",headingTag:"h1"}}),$=new ne({props:{title:"Overview",local:"overview",headingTag:"h2"}}),_=new ne({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),U=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBQaXgyU3RydWN0Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEFtb2RlbCUyMCUzRCUyMFBpeDJTdHJ1Y3RGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmRlcGxvdCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZkZXBsb3QlMjIpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGdmlzLW5scCUyRkNoYXJ0UUElMkZtYWluJTJGQ2hhcnRRQSUyNTIwRGF0YXNldCUyRnZhbCUyRnBuZyUyRjUwOTAucG5nJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwdGV4dCUzRCUyMkdlbmVyYXRlJTIwdW5kZXJseWluZyUyMGRhdGElMjB0YWJsZSUyMG9mJTIwdGhlJTIwZmlndXJlJTIwYmVsb3clM0ElMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXByZWRpY3Rpb25zJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUxMiklMEFwcmludChwcm9jZXNzb3IuZGVjb2RlKHByZWRpY3Rpb25zJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Pix2StructForConditionalGeneration
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

model = Pix2StructForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/deplot&quot;</span>)
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;google/deplot&quot;</span>)
url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

inputs = processor(images=image, text=<span class="hljs-string">&quot;Generate underlying data table of the figure below:&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
predictions = model.generate(**inputs, max_new_tokens=<span class="hljs-number">512</span>)
<span class="hljs-built_in">print</span>(processor.decode(predictions[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),P=new ne({props:{title:"Fine-tuning",local:"fine-tuning",headingTag:"h2"}}),Z=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vcHRpbWl6YXRpb24lMjBpbXBvcnQlMjBBZGFmYWN0b3IlMkMlMjBnZXRfY29zaW5lX3NjaGVkdWxlX3dpdGhfd2FybXVwJTBBJTBBb3B0aW1pemVyJTIwJTNEJTIwQWRhZmFjdG9yKHNlbGYucGFyYW1ldGVycygpJTJDJTIwc2NhbGVfcGFyYW1ldGVyJTNERmFsc2UlMkMlMjByZWxhdGl2ZV9zdGVwJTNERmFsc2UlMkMlMjBsciUzRDAuMDElMkMlMjB3ZWlnaHRfZGVjYXklM0QxZS0wNSklMEFzY2hlZHVsZXIlMjAlM0QlMjBnZXRfY29zaW5lX3NjaGVkdWxlX3dpdGhfd2FybXVwKG9wdGltaXplciUyQyUyMG51bV93YXJtdXBfc3RlcHMlM0QxMDAwJTJDJTIwbnVtX3RyYWluaW5nX3N0ZXBzJTNENDAwMDAp",highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, get_cosine_schedule_with_warmup

optimizer = Adafactor(self.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">0.01</span>, weight_decay=<span class="hljs-number">1e-05</span>)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=<span class="hljs-number">1000</span>, num_training_steps=<span class="hljs-number">40000</span>)`,wrap:!1}}),u=new Je({props:{$$slots:{default:[xe]},$$scope:{ctx:X}}}),W=new ve({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deplot.md"}}),{c(){i=o("meta"),b=s(),p=o("p"),B=s(),y=o("p"),y.innerHTML=se,H=s(),d(T.$$.fragment),V=s(),c=o("div"),c.innerHTML=ae,Y=s(),d($.$$.fragment),I=s(),J=o("p"),J.innerHTML=ie,z=s(),v=o("p"),v.textContent=oe,L=s(),x=o("p"),x.innerHTML=re,F=s(),j=o("p"),j.innerHTML=pe,D=s(),d(_.$$.fragment),E=s(),k=o("p"),k.textContent=me,S=s(),R=o("ul"),R.innerHTML=ce,N=s(),d(U.$$.fragment),A=s(),d(P.$$.fragment),Q=s(),C=o("p"),C.innerHTML=ue,q=s(),d(Z.$$.fragment),K=s(),d(u.$$.fragment),O=s(),d(W.$$.fragment),ee=s(),G=o("p"),this.h()},l(e){const t=Te("svelte-u9bgzb",document.head);i=r(t,"META",{name:!0,content:!0}),t.forEach(n),b=a(e),p=r(e,"P",{}),he(p).forEach(n),B=a(e),y=r(e,"P",{"data-svelte-h":!0}),m(y)!=="svelte-1hxx824"&&(y.innerHTML=se),H=a(e),h(T.$$.fragment,e),V=a(e),c=r(e,"DIV",{class:!0,"data-svelte-h":!0}),m(c)!=="svelte-13t8s2t"&&(c.innerHTML=ae),Y=a(e),h($.$$.fragment,e),I=a(e),J=r(e,"P",{"data-svelte-h":!0}),m(J)!=="svelte-1k37og7"&&(J.innerHTML=ie),z=a(e),v=r(e,"P",{"data-svelte-h":!0}),m(v)!=="svelte-1pvthah"&&(v.textContent=oe),L=a(e),x=r(e,"P",{"data-svelte-h":!0}),m(x)!=="svelte-tgrbdp"&&(x.innerHTML=re),F=a(e),j=r(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-1ihm3oy"&&(j.innerHTML=pe),D=a(e),h(_.$$.fragment,e),E=a(e),k=r(e,"P",{"data-svelte-h":!0}),m(k)!=="svelte-1010knq"&&(k.textContent=me),S=a(e),R=r(e,"UL",{"data-svelte-h":!0}),m(R)!=="svelte-kycb4s"&&(R.innerHTML=ce),N=a(e),h(U.$$.fragment,e),A=a(e),h(P.$$.fragment,e),Q=a(e),C=r(e,"P",{"data-svelte-h":!0}),m(C)!=="svelte-11vjzlg"&&(C.innerHTML=ue),q=a(e),h(Z.$$.fragment,e),K=a(e),h(u.$$.fragment,e),O=a(e),h(W.$$.fragment,e),ee=a(e),G=r(e,"P",{}),he(G).forEach(n),this.h()},h(){le(i,"name","hf:doc:metadata"),le(i,"content",_e),le(c,"class","flex flex-wrap space-x-1")},m(e,t){$e(document.head,i),l(e,b,t),l(e,p,t),l(e,B,t),l(e,y,t),l(e,H,t),f(T,e,t),l(e,V,t),l(e,c,t),l(e,Y,t),f($,e,t),l(e,I,t),l(e,J,t),l(e,z,t),l(e,v,t),l(e,L,t),l(e,x,t),l(e,F,t),l(e,j,t),l(e,D,t),f(_,e,t),l(e,E,t),l(e,k,t),l(e,S,t),l(e,R,t),l(e,N,t),f(U,e,t),l(e,A,t),f(P,e,t),l(e,Q,t),l(e,C,t),l(e,q,t),f(Z,e,t),l(e,K,t),f(u,e,t),l(e,O,t),f(W,e,t),l(e,ee,t),l(e,G,t),te=!0},p(e,[t]){const de={};t&2&&(de.$$scope={dirty:t,ctx:e}),u.$set(de)},i(e){te||(g(T.$$.fragment,e),g($.$$.fragment,e),g(_.$$.fragment,e),g(U.$$.fragment,e),g(P.$$.fragment,e),g(Z.$$.fragment,e),g(u.$$.fragment,e),g(W.$$.fragment,e),te=!0)},o(e){M(T.$$.fragment,e),M($.$$.fragment,e),M(_.$$.fragment,e),M(U.$$.fragment,e),M(P.$$.fragment,e),M(Z.$$.fragment,e),M(u.$$.fragment,e),M(W.$$.fragment,e),te=!1},d(e){e&&(n(b),n(p),n(B),n(y),n(H),n(V),n(c),n(Y),n(I),n(J),n(z),n(v),n(L),n(x),n(F),n(j),n(D),n(E),n(k),n(S),n(R),n(N),n(A),n(Q),n(C),n(q),n(K),n(O),n(ee),n(G)),n(i),w(T,e),w($,e),w(_,e),w(U,e),w(P,e),w(Z,e),w(u,e),w(W,e)}}}const _e='{"title":"DePlot","local":"deplot","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"Fine-tuning","local":"fine-tuning","sections":[],"depth":2}],"depth":1}';function ke(X){return Me(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class We extends be{constructor(i){super(),ye(this,i,ke,je,ge,{})}}export{We as component};
