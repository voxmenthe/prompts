import{s as St,o as Yt,n as Qe}from"../chunks/scheduler.18a86fab.js";import{S as Dt,i as At,g as l,s as a,r as y,A as Kt,h as c,f as n,c as r,j as x,x as f,u as _,k as R,l as Ot,y as i,a as d,v as M,d as w,t as b,w as T}from"../chunks/index.98837b22.js";import{T as qt}from"../chunks/Tip.77304350.js";import{D as q}from"../chunks/Docstring.a1ef7999.js";import{C as Pe}from"../chunks/CodeBlock.8d0c2e8a.js";import{F as eo,M as to}from"../chunks/Markdown.ae01904b.js";import{E as oo}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Fe,E as no}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as so,a as Qt}from"../chunks/HfOption.6641485e.js";function ao(G){let o,g='This model was contributed by <a href="https://huggingface.co/stevenbucaille" rel="nofollow">stevenbucaille</a>.',t,m,p="Click on the LightGlue models in the right sidebar for more examples of how to apply LightGlue to different computer vision tasks.";return{c(){o=l("p"),o.innerHTML=g,t=a(),m=l("p"),m.textContent=p},l(h){o=c(h,"P",{"data-svelte-h":!0}),f(o)!=="svelte-1ir9jnx"&&(o.innerHTML=g),t=r(h),m=c(h,"P",{"data-svelte-h":!0}),f(m)!=="svelte-221e44"&&(m.textContent=p)},m(h,I){d(h,o,I),d(h,t,I),d(h,m,I)},p:Qe,d(h){h&&(n(o),n(t),n(m))}}}function ro(G){let o,g;return o=new Pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBa2V5cG9pbnRfbWF0Y2hlciUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJrZXlwb2ludC1tYXRjaGluZyUyMiUyQyUyMG1vZGVsJTNEJTIyRVRILUNWRyUyRmxpZ2h0Z2x1ZV9zdXBlcnBvaW50JTIyKSUwQSUwQXVybF8wJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF85ODE2OTg4OF8zMzQ3NzEwODUyLmpwZyUyMiUwQXVybF8xJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF8yNjc1NzAyN182NzE3MDg0MDYxLmpwZyUyMiUwQSUwQXJlc3VsdHMlMjAlM0QlMjBrZXlwb2ludF9tYXRjaGVyKCU1QnVybF8wJTJDJTIwdXJsXzElNUQlMkMlMjB0aHJlc2hvbGQlM0QwLjkpJTBBcHJpbnQocmVzdWx0cyU1QjAlNUQpJTBBJTIzJTIwJTdCJ2tleXBvaW50X2ltYWdlXzAnJTNBJTIwJTdCJ3gnJTNBJTIwLi4uJTJDJTIwJ3knJTNBJTIwLi4uJTdEJTJDJTIwJ2tleXBvaW50X2ltYWdlXzEnJTNBJTIwJTdCJ3gnJTNBJTIwLi4uJTJDJTIwJ3knJTNBJTIwLi4uJTdEJTJDJTIwJ3Njb3JlJyUzQSUyMC4uLiU3RA==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

keypoint_matcher = pipeline(task=<span class="hljs-string">&quot;keypoint-matching&quot;</span>, model=<span class="hljs-string">&quot;ETH-CVG/lightglue_superpoint&quot;</span>)

url_0 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
url_1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>

results = keypoint_matcher([url_0, url_1], threshold=<span class="hljs-number">0.9</span>)
<span class="hljs-built_in">print</span>(results[<span class="hljs-number">0</span>])
<span class="hljs-comment"># {&#x27;keypoint_image_0&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;keypoint_image_1&#x27;: {&#x27;x&#x27;: ..., &#x27;y&#x27;: ...}, &#x27;score&#x27;: ...}</span>`,wrap:!1}}),{c(){y(o.$$.fragment)},l(t){_(o.$$.fragment,t)},m(t,m){M(o,t,m),g=!0},p:Qe,i(t){g||(w(o.$$.fragment,t),g=!0)},o(t){b(o.$$.fragment,t),g=!1},d(t){T(o,t)}}}function io(G){let o,g;return o=new Pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmxfaW1hZ2UxJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGbWFnaWNsZWFwJTJGU3VwZXJHbHVlUHJldHJhaW5lZE5ldHdvcmslMkZyZWZzJTJGaGVhZHMlMkZtYXN0ZXIlMkZhc3NldHMlMkZwaG90b3RvdXJpc21fc2FtcGxlX2ltYWdlcyUyRnVuaXRlZF9zdGF0ZXNfY2FwaXRvbF85ODE2OTg4OF8zMzQ3NzEwODUyLmpwZyUyMiUwQWltYWdlMSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybF9pbWFnZTElMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBdXJsX2ltYWdlMiUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRm1hZ2ljbGVhcCUyRlN1cGVyR2x1ZVByZXRyYWluZWROZXR3b3JrJTJGcmVmcyUyRmhlYWRzJTJGbWFzdGVyJTJGYXNzZXRzJTJGcGhvdG90b3VyaXNtX3NhbXBsZV9pbWFnZXMlMkZ1bml0ZWRfc3RhdGVzX2NhcGl0b2xfMjY3NTcwMjdfNjcxNzA4NDA2MS5qcGclMjIlMEFpbWFnZTIlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmxfaW1hZ2UyJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlcyUyMCUzRCUyMCU1QmltYWdlMSUyQyUyMGltYWdlMiU1RCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyRVRILUNWRyUyRmxpZ2h0Z2x1ZV9zdXBlcnBvaW50JTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJFVEgtQ1ZHJTJGbGlnaHRnbHVlX3N1cGVycG9pbnQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBd2l0aCUyMHRvcmNoLmluZmVyZW5jZV9tb2RlKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwUG9zdC1wcm9jZXNzJTIwdG8lMjBnZXQlMjBrZXlwb2ludHMlMjBhbmQlMjBtYXRjaGVzJTBBaW1hZ2Vfc2l6ZXMlMjAlM0QlMjAlNUIlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTIwZm9yJTIwaW1hZ2UlMjBpbiUyMGltYWdlcyU1RCU1RCUwQXByb2Nlc3NlZF9vdXRwdXRzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19rZXlwb2ludF9tYXRjaGluZyhvdXRwdXRzJTJDJTIwaW1hZ2Vfc2l6ZXMlMkMlMjB0aHJlc2hvbGQlM0QwLjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url_image1 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg&quot;</span>
image1 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image1, stream=<span class="hljs-literal">True</span>).raw)
url_image2 = <span class="hljs-string">&quot;https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg&quot;</span>
image2 = Image.<span class="hljs-built_in">open</span>(requests.get(url_image2, stream=<span class="hljs-literal">True</span>).raw)

images = [image1, image2]

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;ETH-CVG/lightglue_superpoint&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;ETH-CVG/lightglue_superpoint&quot;</span>)

inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Post-process to get keypoints and matches</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)`,wrap:!1}}),{c(){y(o.$$.fragment)},l(t){_(o.$$.fragment,t)},m(t,m){M(o,t,m),g=!0},p:Qe,i(t){g||(w(o.$$.fragment,t),g=!0)},o(t){b(o.$$.fragment,t),g=!1},d(t){T(o,t)}}}function lo(G){let o,g,t,m;return o=new Qt({props:{id:"usage",option:"Pipeline",$$slots:{default:[ro]},$$scope:{ctx:G}}}),t=new Qt({props:{id:"usage",option:"AutoModel",$$slots:{default:[io]},$$scope:{ctx:G}}}),{c(){y(o.$$.fragment),g=a(),y(t.$$.fragment)},l(p){_(o.$$.fragment,p),g=r(p),_(t.$$.fragment,p)},m(p,h){M(o,p,h),d(p,g,h),M(t,p,h),m=!0},p(p,h){const I={};h&2&&(I.$$scope={dirty:h,ctx:p}),o.$set(I);const C={};h&2&&(C.$$scope={dirty:h,ctx:p}),t.$set(C)},i(p){m||(w(o.$$.fragment,p),w(t.$$.fragment,p),m=!0)},o(p){b(o.$$.fragment,p),b(t.$$.fragment,p),m=!1},d(p){p&&n(g),T(o,p),T(t,p)}}}function co(G){let o,g="Examples:",t,m,p;return m=new Pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExpZ2h0R2x1ZUNvbmZpZyUyQyUyMExpZ2h0R2x1ZUZvcktleXBvaW50TWF0Y2hpbmclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTGlnaHRHbHVlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMExpZ2h0R2x1ZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMExpZ2h0R2x1ZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwTGlnaHRHbHVlRm9yS2V5cG9pbnRNYXRjaGluZyhjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LightGlueConfig, LightGlueForKeypointMatching

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a LightGlue style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = LightGlueConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the LightGlue style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = LightGlueForKeypointMatching(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=l("p"),o.textContent=g,t=a(),y(m.$$.fragment)},l(h){o=c(h,"P",{"data-svelte-h":!0}),f(o)!=="svelte-kvfsh7"&&(o.textContent=g),t=r(h),_(m.$$.fragment,h)},m(h,I){d(h,o,I),d(h,t,I),M(m,h,I),p=!0},p:Qe,i(h){p||(w(m.$$.fragment,h),p=!0)},o(h){b(m.$$.fragment,h),p=!1},d(h){h&&(n(o),n(t)),T(m,h)}}}function mo(G){let o,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=l("p"),o.innerHTML=g},l(t){o=c(t,"P",{"data-svelte-h":!0}),f(o)!=="svelte-fincs2"&&(o.innerHTML=g)},m(t,m){d(t,o,m)},p:Qe,d(t){t&&n(o)}}}function po(G){let o,g,t,m,p,h,I="LightGlue model taking images as inputs and outputting the matching of them.",C,E,ne=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,W,H,F=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ie,L,k,Ue,N,B='The <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching">LightGlueForKeypointMatching</a> forward method, overrides the <code>__call__</code> special method.',S,U,se,j,X="<li>forward</li>",Q;return o=new Fe({props:{title:"LightGlueForKeypointMatching",local:"transformers.LightGlueForKeypointMatching",headingTag:"h2"}}),m=new q({props:{name:"class transformers.LightGlueForKeypointMatching",anchor:"transformers.LightGlueForKeypointMatching",parameters:[{name:"config",val:": LightGlueConfig"}],parametersDescription:[{anchor:"transformers.LightGlueForKeypointMatching.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueConfig">LightGlueConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/modeling_lightglue.py#L491"}}),k=new q({props:{name:"forward",anchor:"transformers.LightGlueForKeypointMatching.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.LightGlueForKeypointMatching.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueImageProcessor">LightGlueImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">LightGlueImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueImageProcessor">LightGlueImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.LightGlueForKeypointMatching.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.LightGlueForKeypointMatching.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LightGlueForKeypointMatching.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/modeling_lightglue.py#L860",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.lightglue.modeling_lightglue.LightGlueKeypointMatchingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueConfig"
>LightGlueConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>) — Loss computed during training.</li>
<li><strong>matches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Index of keypoint matched in the other image.</li>
<li><strong>matching_scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2, num_matches)</code>) — Scores of predicted matches.</li>
<li><strong>keypoints</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_keypoints, 2)</code>) — Absolute (x, y) coordinates of predicted keypoints in a given image.</li>
<li><strong>prune</strong> (<code>torch.IntTensor</code> of shape <code>(batch_size, num_keypoints)</code>) — Pruning mask indicating which keypoints are removed and at which layer.</li>
<li><strong>mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_keypoints)</code>) — Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching
information.</li>
<li><strong>hidden_states</strong> (<code>Tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, 2, num_channels, num_keypoints)</code> returned when <code>output_hidden_states=True</code> is passed or when
<code>config.output_hidden_states=True</code></li>
<li><strong>attentions</strong> (<code>Tuple[torch.FloatTensor, ...]</code>, <em>optional</em>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, 2, num_heads, num_keypoints, num_keypoints)</code> returned when <code>output_attentions=True</code> is passed or when
<code>config.output_attentions=True</code></li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.lightglue.modeling_lightglue.LightGlueKeypointMatchingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),U=new qt({props:{$$slots:{default:[mo]},$$scope:{ctx:G}}}),{c(){y(o.$$.fragment),g=a(),t=l("div"),y(m.$$.fragment),p=a(),h=l("p"),h.textContent=I,C=a(),E=l("p"),E.innerHTML=ne,W=a(),H=l("p"),H.innerHTML=F,Ie=a(),L=l("div"),y(k.$$.fragment),Ue=a(),N=l("p"),N.innerHTML=B,S=a(),y(U.$$.fragment),se=a(),j=l("ul"),j.innerHTML=X,this.h()},l(u){_(o.$$.fragment,u),g=r(u),t=c(u,"DIV",{class:!0});var v=x(t);_(m.$$.fragment,v),p=r(v),h=c(v,"P",{"data-svelte-h":!0}),f(h)!=="svelte-bilf7k"&&(h.textContent=I),C=r(v),E=c(v,"P",{"data-svelte-h":!0}),f(E)!=="svelte-q52n56"&&(E.innerHTML=ne),W=r(v),H=c(v,"P",{"data-svelte-h":!0}),f(H)!=="svelte-hswkmf"&&(H.innerHTML=F),Ie=r(v),L=c(v,"DIV",{class:!0});var $=x(L);_(k.$$.fragment,$),Ue=r($),N=c($,"P",{"data-svelte-h":!0}),f(N)!=="svelte-7fh3kk"&&(N.innerHTML=B),S=r($),_(U.$$.fragment,$),$.forEach(n),v.forEach(n),se=r(u),j=c(u,"UL",{"data-svelte-h":!0}),f(j)!=="svelte-n3ow4o"&&(j.innerHTML=X),this.h()},h(){R(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(u,v){M(o,u,v),d(u,g,v),d(u,t,v),M(m,t,null),i(t,p),i(t,h),i(t,C),i(t,E),i(t,W),i(t,H),i(t,Ie),i(t,L),M(k,L,null),i(L,Ue),i(L,N),i(L,S),M(U,L,null),d(u,se,v),d(u,j,v),Q=!0},p(u,v){const $={};v&2&&($.$$scope={dirty:v,ctx:u}),U.$set($)},i(u){Q||(w(o.$$.fragment,u),w(m.$$.fragment,u),w(k.$$.fragment,u),w(U.$$.fragment,u),Q=!0)},o(u){b(o.$$.fragment,u),b(m.$$.fragment,u),b(k.$$.fragment,u),b(U.$$.fragment,u),Q=!1},d(u){u&&(n(g),n(t),n(se),n(j)),T(o,u),T(m),T(k),T(U)}}}function ho(G){let o,g;return o=new to({props:{$$slots:{default:[po]},$$scope:{ctx:G}}}),{c(){y(o.$$.fragment)},l(t){_(o.$$.fragment,t)},m(t,m){M(o,t,m),g=!0},p(t,m){const p={};m&2&&(p.$$scope={dirty:m,ctx:t}),o.$set(p)},i(t){g||(w(o.$$.fragment,t),g=!0)},o(t){b(o.$$.fragment,t),g=!1},d(t){T(o,t)}}}function go(G){let o,g,t,m,p,h="<em>This model was released on 2023-06-23 and added to Hugging Face Transformers on 2025-06-17.</em>",I,C,E='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ne,W,H,F,Ie='<a href="https://huggingface.co/papers/2306.13643" rel="nofollow">LightGlue</a> is a deep neural network that learns to match local features across images. It revisits multiple design decisions of SuperGlue and derives simple but effective improvements. Cumulatively, these improvements make LightGlue more efficient - in terms of both memory and computation, more accurate, and much easier to train. Similar to <a href="https://huggingface.co/magic-leap-community/superglue_outdoor" rel="nofollow">SuperGlue</a>, this model consists of matching two sets of local features extracted from two images, with the goal of being faster than SuperGlue. Paired with the <a href="https://huggingface.co/magic-leap-community/superpoint" rel="nofollow">SuperPoint model</a>, it can be used to match two images and estimate the pose between them.',L,k,Ue='You can find all the original LightGlue checkpoints under the <a href="https://huggingface.co/ETH-CVG" rel="nofollow">ETH-CVG</a> organization.',N,B,S,U,se='The example below demonstrates how to match keypoints between two images with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',j,X,Q,u,v,$,ae,Ce,Lt="LightGlue is adaptive to the task difficulty. Inference is much faster on image pairs that are intuitively easy to match, for example, because of a larger visual overlap or limited appearance change.",ct,re,dt,Le,kt="<p>The model outputs matching indices, keypoints, and confidence scores for each match, similar to SuperGlue but with improved efficiency.</p>",mt,ie,ke,xt='For better visualization and analysis, use the <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueImageProcessor.post_process_keypoint_matching">LightGlueImageProcessor.post_process_keypoint_matching()</a> method to get matches in a more readable format.',pt,le,ht,ce,xe,Zt="Visualize the matches between the images using the built-in plotting functionality.",gt,de,qe,Y,zt='<img src="https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/duPp09ty8NRZlMZS18ccP.png"/>',Se,me,Ye,pe,jt='<li>Refer to the <a href="https://github.com/cvg/LightGlue" rel="nofollow">original LightGlue repository</a> for more examples and implementation details.</li>',De,he,Ae,z,ge,ut,Ze,Rt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/lightglue#transformers.LightGlueForKeypointMatching">LightGlueForKeypointMatching</a>. It is used to
instantiate a LightGlue model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the LightGlue
<a href="https://huggingface.co/ETH-CVG/lightglue_superpoint" rel="nofollow">ETH-CVG/lightglue_superpoint</a> architecture.`,ft,ze,Wt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,yt,D,Ke,ue,Oe,J,fe,_t,je,Ht="Constructs a LightGlue image processor.",Mt,P,ye,wt,Re,Nt=`Plots the image pairs side by side with the detected keypoints as well as the matching between them. Requires
matplotlib to be installed.`,bt,We,Bt=`.. deprecated::
<code>plot_keypoint_matching</code> is deprecated and will be removed in a future version. Use <code>visualize_keypoint_matching</code> instead.`,Tt,A,_e,vt,He,Xt=`Converts the raw output of <code>KeypointMatchingOutput</code> into lists of keypoints, scores and descriptors
with coordinates absolute to the original image sizes.`,$t,K,Me,Jt,Ne,Vt="Preprocess an image or batch of images.",Gt,O,we,It,Be,Ft="Resize an image.",Ut,ee,be,Ct,Xe,Pt="Plots the image pairs side by side with the detected keypoints as well as the matching between them.",et,Te,Et="<li>preprocess</li> <li>post_process_keypoint_matching</li> <li>visualize_keypoint_matching</li>",tt,te,ot,ve,nt,Ee,st;return W=new Fe({props:{title:"LightGlue",local:"lightglue",headingTag:"h1"}}),B=new qt({props:{warning:!1,$$slots:{default:[ao]},$$scope:{ctx:G}}}),X=new so({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[lo]},$$scope:{ctx:G}}}),u=new Fe({props:{title:"Notes",local:"notes",headingTag:"h2"}}),re=new Pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkVUSC1DVkclMkZsaWdodGdsdWVfc3VwZXJwb2ludCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyRVRILUNWRyUyRmxpZ2h0Z2x1ZV9zdXBlcnBvaW50JTIyKSUwQSUwQSUyMyUyMExpZ2h0R2x1ZSUyMHJlcXVpcmVzJTIwcGFpcnMlMjBvZiUyMGltYWdlcyUwQWltYWdlcyUyMCUzRCUyMCU1QmltYWdlMSUyQyUyMGltYWdlMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXdpdGglMjB0b3JjaC5pbmZlcmVuY2VfbW9kZSgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQSUyMyUyMEV4dHJhY3QlMjBtYXRjaGluZyUyMGluZm9ybWF0aW9uJTBBa2V5cG9pbnRzMCUyMCUzRCUyMG91dHB1dHMua2V5cG9pbnRzMCUyMCUyMCUyMyUyMEtleXBvaW50cyUyMGluJTIwZmlyc3QlMjBpbWFnZSUwQWtleXBvaW50czElMjAlM0QlMjBvdXRwdXRzLmtleXBvaW50czElMjAlMjAlMjMlMjBLZXlwb2ludHMlMjBpbiUyMHNlY29uZCUyMGltYWdlJTBBbWF0Y2hlcyUyMCUzRCUyMG91dHB1dHMubWF0Y2hlcyUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMyUyME1hdGNoaW5nJTIwaW5kaWNlcyUwQW1hdGNoaW5nX3Njb3JlcyUyMCUzRCUyMG91dHB1dHMubWF0Y2hpbmdfc2NvcmVzJTIwJTIwJTIzJTIwQ29uZmlkZW5jZSUyMHNjb3Jlcw==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;ETH-CVG/lightglue_superpoint&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;ETH-CVG/lightglue_superpoint&quot;</span>)

<span class="hljs-comment"># LightGlue requires pairs of images</span>
images = [image1, image2]
inputs = processor(images, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-keyword">with</span> torch.inference_mode():
    outputs = model(**inputs)

<span class="hljs-comment"># Extract matching information</span>
keypoints0 = outputs.keypoints0  <span class="hljs-comment"># Keypoints in first image</span>
keypoints1 = outputs.keypoints1  <span class="hljs-comment"># Keypoints in second image</span>
matches = outputs.matches        <span class="hljs-comment"># Matching indices</span>
matching_scores = outputs.matching_scores  <span class="hljs-comment"># Confidence scores</span>`,wrap:!1}}),le=new Pe({props:{code:"JTIzJTIwUHJvY2VzcyUyMG91dHB1dHMlMjBmb3IlMjB2aXN1YWxpemF0aW9uJTBBaW1hZ2Vfc2l6ZXMlMjAlM0QlMjAlNUIlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTIwZm9yJTIwaW1hZ2UlMjBpbiUyMGltYWdlcyU1RCU1RCUwQXByb2Nlc3NlZF9vdXRwdXRzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19rZXlwb2ludF9tYXRjaGluZyhvdXRwdXRzJTJDJTIwaW1hZ2Vfc2l6ZXMlMkMlMjB0aHJlc2hvbGQlM0QwLjIpJTBBJTBBZm9yJTIwaSUyQyUyMG91dHB1dCUyMGluJTIwZW51bWVyYXRlKHByb2Nlc3NlZF9vdXRwdXRzKSUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGYlMjJGb3IlMjB0aGUlMjBpbWFnZSUyMHBhaXIlMjAlN0JpJTdEJTIyKSUwQSUyMCUyMCUyMCUyMGZvciUyMGtleXBvaW50MCUyQyUyMGtleXBvaW50MSUyQyUyMG1hdGNoaW5nX3Njb3JlJTIwaW4lMjB6aXAoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3V0cHV0JTVCJTIya2V5cG9pbnRzMCUyMiU1RCUyQyUyMG91dHB1dCU1QiUyMmtleXBvaW50czElMjIlNUQlMkMlMjBvdXRwdXQlNUIlMjJtYXRjaGluZ19zY29yZXMlMjIlNUQlMEElMjAlMjAlMjAlMjApJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMktleXBvaW50JTIwYXQlMjAlN0JrZXlwb2ludDAubnVtcHkoKSU3RCUyMG1hdGNoZXMlMjB3aXRoJTIwa2V5cG9pbnQlMjBhdCUyMCU3QmtleXBvaW50MS5udW1weSgpJTdEJTIwd2l0aCUyMHNjb3JlJTIwJTdCbWF0Y2hpbmdfc2NvcmUlN0QlMjIp",highlighted:`<span class="hljs-comment"># Process outputs for visualization</span>
image_sizes = [[(image.height, image.width) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]]
processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=<span class="hljs-number">0.2</span>)

<span class="hljs-keyword">for</span> i, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(processed_outputs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;For the image pair <span class="hljs-subst">{i}</span>&quot;</span>)
    <span class="hljs-keyword">for</span> keypoint0, keypoint1, matching_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(
            output[<span class="hljs-string">&quot;keypoints0&quot;</span>], output[<span class="hljs-string">&quot;keypoints1&quot;</span>], output[<span class="hljs-string">&quot;matching_scores&quot;</span>]
    ):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keypoint at <span class="hljs-subst">{keypoint0.numpy()}</span> matches with keypoint at <span class="hljs-subst">{keypoint1.numpy()}</span> with score <span class="hljs-subst">{matching_score}</span>&quot;</span>)`,wrap:!1}}),de=new Pe({props:{code:"JTIzJTIwRWFzeSUyMHZpc3VhbGl6YXRpb24lMjB1c2luZyUyMHRoZSUyMGJ1aWx0LWluJTIwcGxvdHRpbmclMjBtZXRob2QlMEFwcm9jZXNzb3IudmlzdWFsaXplX2tleXBvaW50X21hdGNoaW5nKGltYWdlcyUyQyUyMHByb2Nlc3NlZF9vdXRwdXRzKQ==",highlighted:`<span class="hljs-comment"># Easy visualization using the built-in plotting method</span>
processor.visualize_keypoint_matching(images, processed_outputs)`,wrap:!1}}),me=new Fe({props:{title:"Resources",local:"resources",headingTag:"h2"}}),he=new Fe({props:{title:"LightGlueConfig",local:"transformers.LightGlueConfig",headingTag:"h2"}}),ge=new q({props:{name:"class transformers.LightGlueConfig",anchor:"transformers.LightGlueConfig",parameters:[{name:"keypoint_detector_config",val:": SuperPointConfig = None"},{name:"descriptor_dim",val:": int = 256"},{name:"num_hidden_layers",val:": int = 9"},{name:"num_attention_heads",val:": int = 4"},{name:"num_key_value_heads",val:" = None"},{name:"depth_confidence",val:": float = 0.95"},{name:"width_confidence",val:": float = 0.99"},{name:"filter_threshold",val:": float = 0.1"},{name:"initializer_range",val:": float = 0.02"},{name:"hidden_act",val:": str = 'gelu'"},{name:"attention_dropout",val:" = 0.0"},{name:"attention_bias",val:" = True"},{name:"trust_remote_code",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LightGlueConfig.keypoint_detector_config",description:`<strong>keypoint_detector_config</strong> (<code>Union[AutoConfig, dict]</code>,  <em>optional</em>, defaults to <code>SuperPointConfig</code>) &#x2014;
The config object or dictionary of the keypoint detector.`,name:"keypoint_detector_config"},{anchor:"transformers.LightGlueConfig.descriptor_dim",description:`<strong>descriptor_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimension of the descriptors.`,name:"descriptor_dim"},{anchor:"transformers.LightGlueConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 9) &#x2014;
The number of self and cross attention layers.`,name:"num_hidden_layers"},{anchor:"transformers.LightGlueConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of heads in the multi-head attention.`,name:"num_attention_heads"},{anchor:"transformers.LightGlueConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA) otherwise GQA is used. When
converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
by meanpooling all the original heads within that group. For more details checkout <a href="https://huggingface.co/papers/2305.13245" rel="nofollow">this
paper</a>. If it is not specified, will default to
<code>num_attention_heads</code>.`,name:"num_key_value_heads"},{anchor:"transformers.LightGlueConfig.depth_confidence",description:`<strong>depth_confidence</strong> (<code>float</code>, <em>optional</em>, defaults to 0.95) &#x2014;
The confidence threshold used to perform early stopping`,name:"depth_confidence"},{anchor:"transformers.LightGlueConfig.width_confidence",description:`<strong>width_confidence</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;
The confidence threshold used to prune points`,name:"width_confidence"},{anchor:"transformers.LightGlueConfig.filter_threshold",description:`<strong>filter_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The confidence threshold used to filter matches`,name:"filter_threshold"},{anchor:"transformers.LightGlueConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.LightGlueConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The activation function to be used in the hidden layers.`,name:"hidden_act"},{anchor:"transformers.LightGlueConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.LightGlueConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a bias in the query, key, value and output projection layers during self-attention.`,name:"attention_bias"},{anchor:"transformers.LightGlueConfig.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to trust remote code when using other models than SuperPoint as keypoint detector.`,name:"trust_remote_code"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/configuration_lightglue.py#L27"}}),D=new oo({props:{anchor:"transformers.LightGlueConfig.example",$$slots:{default:[co]},$$scope:{ctx:G}}}),ue=new Fe({props:{title:"LightGlueImageProcessor",local:"transformers.LightGlueImageProcessor",headingTag:"h2"}}),fe=new q({props:{name:"class transformers.LightGlueImageProcessor",anchor:"transformers.LightGlueImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": float = 0.00392156862745098"},{name:"do_grayscale",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden
by <code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.LightGlueImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 480, &quot;width&quot;: 640}</code>):
Resolution of the output image after <code>resize</code> is applied. Only has an effect if <code>do_resize</code> is set to
<code>True</code>. Can be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.LightGlueImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.LightGlueImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.LightGlueImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.LightGlueImageProcessor.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to grayscale. Can be overridden by <code>do_grayscale</code> in the <code>preprocess</code> method.`,name:"do_grayscale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L138"}}),ye=new q({props:{name:"plot_keypoint_matching",anchor:"transformers.LightGlueImageProcessor.plot_keypoint_matching",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"keypoint_matching_output",val:": LightGlueKeypointMatchingOutput"}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.plot_keypoint_matching.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to plot. Same as <code>LightGlueImageProcessor.preprocess</code>. Expects either a list of 2 images or
a list of list of 2 images list with pixel values ranging from 0 to 255.`,name:"images"},{anchor:"transformers.LightGlueImageProcessor.plot_keypoint_matching.keypoint_matching_output",description:`<strong>keypoint_matching_output</strong> (<code>LightGlueKeypointMatchingOutput</code>) &#x2014;
Raw outputs of the model.`,name:"keypoint_matching_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L470"}}),_e=new q({props:{name:"post_process_keypoint_matching",anchor:"transformers.LightGlueImageProcessor.post_process_keypoint_matching",parameters:[{name:"outputs",val:": LightGlueKeypointMatchingOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple]]"},{name:"threshold",val:": float = 0.0"}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.post_process_keypoint_matching.outputs",description:`<strong>outputs</strong> (<code>KeypointMatchingOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.LightGlueImageProcessor.post_process_keypoint_matching.target_sizes",description:`<strong>target_sizes</strong> (<code>torch.Tensor</code> or <code>list[tuple[tuple[int, int]]]</code>, <em>optional</em>) &#x2014;
Tensor of shape <code>(batch_size, 2, 2)</code> or list of tuples of tuples (<code>tuple[int, int]</code>) containing the
target size <code>(height, width)</code> of each image in the batch. This must be the original image size (before
any processing).`,name:"target_sizes"},{anchor:"transformers.LightGlueImageProcessor.post_process_keypoint_matching.threshold",description:`<strong>threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold to filter out the matches with low scores.`,name:"threshold"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L341",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries, each dictionary containing the keypoints in the first and second image
of the pair, the matching scores and the matching indices.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[Dict]</code></p>
`}}),Me=new q({props:{name:"preprocess",anchor:"transformers.LightGlueImageProcessor.preprocess",parameters:[{name:"images",val:""},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_grayscale",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with
pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set
<code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.LightGlueImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.LightGlueImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output image after <code>resize</code> has been applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image
is resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the
image will be matched to <code>int(size[&quot;shortest_edge&quot;]/ crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.LightGlueImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of <code>PILImageResampling</code>, filters. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.LightGlueImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.LightGlueImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.LightGlueImageProcessor.preprocess.do_grayscale",description:`<strong>do_grayscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_grayscale</code>) &#x2014;
Whether to convert the image to grayscale.`,name:"do_grayscale"},{anchor:"transformers.LightGlueImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.LightGlueImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.LightGlueImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L223"}}),we=new q({props:{name:"resize",anchor:"transformers.LightGlueImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": dict"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.LightGlueImageProcessor.resize.size",description:`<strong>size</strong> (<code>dict[str, int]</code>) &#x2014;
Dictionary of the form <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code>, specifying the size of the output image.`,name:"size"},{anchor:"transformers.LightGlueImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If not provided, it will be inferred from the input
image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.LightGlueImageProcessor.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L184"}}),be=new q({props:{name:"visualize_keypoint_matching",anchor:"transformers.LightGlueImageProcessor.visualize_keypoint_matching",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"keypoint_matching_output",val:": list"}],parametersDescription:[{anchor:"transformers.LightGlueImageProcessor.visualize_keypoint_matching.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image pairs to plot. Same as <code>LightGlueImageProcessor.preprocess</code>. Expects either a list of 2
images or a list of list of 2 images list with pixel values ranging from 0 to 255.`,name:"images"},{anchor:"transformers.LightGlueImageProcessor.visualize_keypoint_matching.keypoint_matching_output",description:`<strong>keypoint_matching_output</strong> (List[Dict[str, torch.Tensor]]]) &#x2014;
A post processed keypoint matching output`,name:"keypoint_matching_output"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/lightglue/image_processing_lightglue.py#L409",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of PIL images, each containing the image pairs side by side with the detected
keypoints as well as the matching between them.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[PIL.Image.Image]</code></p>
`}}),te=new eo({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[ho]},$$scope:{ctx:G}}}),ve=new no({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/lightglue.md"}}),{c(){o=l("meta"),g=a(),t=l("p"),m=a(),p=l("p"),p.innerHTML=h,I=a(),C=l("div"),C.innerHTML=E,ne=a(),y(W.$$.fragment),H=a(),F=l("p"),F.innerHTML=Ie,L=a(),k=l("p"),k.innerHTML=Ue,N=a(),y(B.$$.fragment),S=a(),U=l("p"),U.innerHTML=se,j=a(),y(X.$$.fragment),Q=a(),y(u.$$.fragment),v=a(),$=l("ul"),ae=l("li"),Ce=l("p"),Ce.textContent=Lt,ct=a(),y(re.$$.fragment),dt=a(),Le=l("li"),Le.innerHTML=kt,mt=a(),ie=l("li"),ke=l("p"),ke.innerHTML=xt,pt=a(),y(le.$$.fragment),ht=a(),ce=l("li"),xe=l("p"),xe.textContent=Zt,gt=a(),y(de.$$.fragment),qe=a(),Y=l("div"),Y.innerHTML=zt,Se=a(),y(me.$$.fragment),Ye=a(),pe=l("ul"),pe.innerHTML=jt,De=a(),y(he.$$.fragment),Ae=a(),z=l("div"),y(ge.$$.fragment),ut=a(),Ze=l("p"),Ze.innerHTML=Rt,ft=a(),ze=l("p"),ze.innerHTML=Wt,yt=a(),y(D.$$.fragment),Ke=a(),y(ue.$$.fragment),Oe=a(),J=l("div"),y(fe.$$.fragment),_t=a(),je=l("p"),je.textContent=Ht,Mt=a(),P=l("div"),y(ye.$$.fragment),wt=a(),Re=l("p"),Re.textContent=Nt,bt=a(),We=l("p"),We.innerHTML=Bt,Tt=a(),A=l("div"),y(_e.$$.fragment),vt=a(),He=l("p"),He.innerHTML=Xt,$t=a(),K=l("div"),y(Me.$$.fragment),Jt=a(),Ne=l("p"),Ne.textContent=Vt,Gt=a(),O=l("div"),y(we.$$.fragment),It=a(),Be=l("p"),Be.textContent=Ft,Ut=a(),ee=l("div"),y(be.$$.fragment),Ct=a(),Xe=l("p"),Xe.textContent=Pt,et=a(),Te=l("ul"),Te.innerHTML=Et,tt=a(),y(te.$$.fragment),ot=a(),y(ve.$$.fragment),nt=a(),Ee=l("p"),this.h()},l(e){const s=Kt("svelte-u9bgzb",document.head);o=c(s,"META",{name:!0,content:!0}),s.forEach(n),g=r(e),t=c(e,"P",{}),x(t).forEach(n),m=r(e),p=c(e,"P",{"data-svelte-h":!0}),f(p)!=="svelte-16o6feh"&&(p.innerHTML=h),I=r(e),C=c(e,"DIV",{style:!0,"data-svelte-h":!0}),f(C)!=="svelte-wa5t4p"&&(C.innerHTML=E),ne=r(e),_(W.$$.fragment,e),H=r(e),F=c(e,"P",{"data-svelte-h":!0}),f(F)!=="svelte-wuebmi"&&(F.innerHTML=Ie),L=r(e),k=c(e,"P",{"data-svelte-h":!0}),f(k)!=="svelte-1s2wt9o"&&(k.innerHTML=Ue),N=r(e),_(B.$$.fragment,e),S=r(e),U=c(e,"P",{"data-svelte-h":!0}),f(U)!=="svelte-jt8707"&&(U.innerHTML=se),j=r(e),_(X.$$.fragment,e),Q=r(e),_(u.$$.fragment,e),v=r(e),$=c(e,"UL",{});var V=x($);ae=c(V,"LI",{});var $e=x(ae);Ce=c($e,"P",{"data-svelte-h":!0}),f(Ce)!=="svelte-1bv7bo8"&&(Ce.textContent=Lt),ct=r($e),_(re.$$.fragment,$e),$e.forEach(n),dt=r(V),Le=c(V,"LI",{"data-svelte-h":!0}),f(Le)!=="svelte-1kwlblq"&&(Le.innerHTML=kt),mt=r(V),ie=c(V,"LI",{});var Je=x(ie);ke=c(Je,"P",{"data-svelte-h":!0}),f(ke)!=="svelte-16n5qao"&&(ke.innerHTML=xt),pt=r(Je),_(le.$$.fragment,Je),Je.forEach(n),ht=r(V),ce=c(V,"LI",{});var Ge=x(ce);xe=c(Ge,"P",{"data-svelte-h":!0}),f(xe)!=="svelte-155ueni"&&(xe.textContent=Zt),gt=r(Ge),_(de.$$.fragment,Ge),Ge.forEach(n),V.forEach(n),qe=r(e),Y=c(e,"DIV",{class:!0,"data-svelte-h":!0}),f(Y)!=="svelte-1cyqm8f"&&(Y.innerHTML=zt),Se=r(e),_(me.$$.fragment,e),Ye=r(e),pe=c(e,"UL",{"data-svelte-h":!0}),f(pe)!=="svelte-10kdcpq"&&(pe.innerHTML=jt),De=r(e),_(he.$$.fragment,e),Ae=r(e),z=c(e,"DIV",{class:!0});var oe=x(z);_(ge.$$.fragment,oe),ut=r(oe),Ze=c(oe,"P",{"data-svelte-h":!0}),f(Ze)!=="svelte-vfv2rs"&&(Ze.innerHTML=Rt),ft=r(oe),ze=c(oe,"P",{"data-svelte-h":!0}),f(ze)!=="svelte-1ek1ss9"&&(ze.innerHTML=Wt),yt=r(oe),_(D.$$.fragment,oe),oe.forEach(n),Ke=r(e),_(ue.$$.fragment,e),Oe=r(e),J=c(e,"DIV",{class:!0});var Z=x(J);_(fe.$$.fragment,Z),_t=r(Z),je=c(Z,"P",{"data-svelte-h":!0}),f(je)!=="svelte-1828nv5"&&(je.textContent=Ht),Mt=r(Z),P=c(Z,"DIV",{class:!0});var Ve=x(P);_(ye.$$.fragment,Ve),wt=r(Ve),Re=c(Ve,"P",{"data-svelte-h":!0}),f(Re)!=="svelte-wdmo4l"&&(Re.textContent=Nt),bt=r(Ve),We=c(Ve,"P",{"data-svelte-h":!0}),f(We)!=="svelte-rdzmlz"&&(We.innerHTML=Bt),Ve.forEach(n),Tt=r(Z),A=c(Z,"DIV",{class:!0});var at=x(A);_(_e.$$.fragment,at),vt=r(at),He=c(at,"P",{"data-svelte-h":!0}),f(He)!=="svelte-y19agd"&&(He.innerHTML=Xt),at.forEach(n),$t=r(Z),K=c(Z,"DIV",{class:!0});var rt=x(K);_(Me.$$.fragment,rt),Jt=r(rt),Ne=c(rt,"P",{"data-svelte-h":!0}),f(Ne)!=="svelte-1x3yxsa"&&(Ne.textContent=Vt),rt.forEach(n),Gt=r(Z),O=c(Z,"DIV",{class:!0});var it=x(O);_(we.$$.fragment,it),It=r(it),Be=c(it,"P",{"data-svelte-h":!0}),f(Be)!=="svelte-1eb2h1k"&&(Be.textContent=Ft),it.forEach(n),Ut=r(Z),ee=c(Z,"DIV",{class:!0});var lt=x(ee);_(be.$$.fragment,lt),Ct=r(lt),Xe=c(lt,"P",{"data-svelte-h":!0}),f(Xe)!=="svelte-5x4wxx"&&(Xe.textContent=Pt),lt.forEach(n),Z.forEach(n),et=r(e),Te=c(e,"UL",{"data-svelte-h":!0}),f(Te)!=="svelte-74hxmd"&&(Te.innerHTML=Et),tt=r(e),_(te.$$.fragment,e),ot=r(e),_(ve.$$.fragment,e),nt=r(e),Ee=c(e,"P",{}),x(Ee).forEach(n),this.h()},h(){R(o,"name","hf:doc:metadata"),R(o,"content",uo),Ot(C,"float","right"),R(Y,"class","flex justify-center"),R(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){i(document.head,o),d(e,g,s),d(e,t,s),d(e,m,s),d(e,p,s),d(e,I,s),d(e,C,s),d(e,ne,s),M(W,e,s),d(e,H,s),d(e,F,s),d(e,L,s),d(e,k,s),d(e,N,s),M(B,e,s),d(e,S,s),d(e,U,s),d(e,j,s),M(X,e,s),d(e,Q,s),M(u,e,s),d(e,v,s),d(e,$,s),i($,ae),i(ae,Ce),i(ae,ct),M(re,ae,null),i($,dt),i($,Le),i($,mt),i($,ie),i(ie,ke),i(ie,pt),M(le,ie,null),i($,ht),i($,ce),i(ce,xe),i(ce,gt),M(de,ce,null),d(e,qe,s),d(e,Y,s),d(e,Se,s),M(me,e,s),d(e,Ye,s),d(e,pe,s),d(e,De,s),M(he,e,s),d(e,Ae,s),d(e,z,s),M(ge,z,null),i(z,ut),i(z,Ze),i(z,ft),i(z,ze),i(z,yt),M(D,z,null),d(e,Ke,s),M(ue,e,s),d(e,Oe,s),d(e,J,s),M(fe,J,null),i(J,_t),i(J,je),i(J,Mt),i(J,P),M(ye,P,null),i(P,wt),i(P,Re),i(P,bt),i(P,We),i(J,Tt),i(J,A),M(_e,A,null),i(A,vt),i(A,He),i(J,$t),i(J,K),M(Me,K,null),i(K,Jt),i(K,Ne),i(J,Gt),i(J,O),M(we,O,null),i(O,It),i(O,Be),i(J,Ut),i(J,ee),M(be,ee,null),i(ee,Ct),i(ee,Xe),d(e,et,s),d(e,Te,s),d(e,tt,s),M(te,e,s),d(e,ot,s),M(ve,e,s),d(e,nt,s),d(e,Ee,s),st=!0},p(e,[s]){const V={};s&2&&(V.$$scope={dirty:s,ctx:e}),B.$set(V);const $e={};s&2&&($e.$$scope={dirty:s,ctx:e}),X.$set($e);const Je={};s&2&&(Je.$$scope={dirty:s,ctx:e}),D.$set(Je);const Ge={};s&2&&(Ge.$$scope={dirty:s,ctx:e}),te.$set(Ge)},i(e){st||(w(W.$$.fragment,e),w(B.$$.fragment,e),w(X.$$.fragment,e),w(u.$$.fragment,e),w(re.$$.fragment,e),w(le.$$.fragment,e),w(de.$$.fragment,e),w(me.$$.fragment,e),w(he.$$.fragment,e),w(ge.$$.fragment,e),w(D.$$.fragment,e),w(ue.$$.fragment,e),w(fe.$$.fragment,e),w(ye.$$.fragment,e),w(_e.$$.fragment,e),w(Me.$$.fragment,e),w(we.$$.fragment,e),w(be.$$.fragment,e),w(te.$$.fragment,e),w(ve.$$.fragment,e),st=!0)},o(e){b(W.$$.fragment,e),b(B.$$.fragment,e),b(X.$$.fragment,e),b(u.$$.fragment,e),b(re.$$.fragment,e),b(le.$$.fragment,e),b(de.$$.fragment,e),b(me.$$.fragment,e),b(he.$$.fragment,e),b(ge.$$.fragment,e),b(D.$$.fragment,e),b(ue.$$.fragment,e),b(fe.$$.fragment,e),b(ye.$$.fragment,e),b(_e.$$.fragment,e),b(Me.$$.fragment,e),b(we.$$.fragment,e),b(be.$$.fragment,e),b(te.$$.fragment,e),b(ve.$$.fragment,e),st=!1},d(e){e&&(n(g),n(t),n(m),n(p),n(I),n(C),n(ne),n(H),n(F),n(L),n(k),n(N),n(S),n(U),n(j),n(Q),n(v),n($),n(qe),n(Y),n(Se),n(Ye),n(pe),n(De),n(Ae),n(z),n(Ke),n(Oe),n(J),n(et),n(Te),n(tt),n(ot),n(nt),n(Ee)),n(o),T(W,e),T(B,e),T(X,e),T(u,e),T(re),T(le),T(de),T(me,e),T(he,e),T(ge),T(D),T(ue,e),T(fe),T(ye),T(_e),T(Me),T(we),T(be),T(te,e),T(ve,e)}}}const uo='{"title":"LightGlue","local":"lightglue","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"LightGlueConfig","local":"transformers.LightGlueConfig","sections":[],"depth":2},{"title":"LightGlueImageProcessor","local":"transformers.LightGlueImageProcessor","sections":[],"depth":2},{"title":"LightGlueForKeypointMatching","local":"transformers.LightGlueForKeypointMatching","sections":[],"depth":2}],"depth":1}';function fo(G){return Yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Go extends Dt{constructor(o){super(),At(this,o,fo,go,St,{})}}export{Go as component};
