import{s as we,o as Je,n as Ys}from"../chunks/scheduler.18a86fab.js";import{S as Ue,i as Te,g as c,s as n,r as h,A as $e,h as m,f as e,c as p,j as fe,u as d,x as g,k as ge,y as _e,a,v as u,d as M,t as y,w as f}from"../chunks/index.98837b22.js";import{T as Nt}from"../chunks/Tip.77304350.js";import{Y as Ce}from"../chunks/Youtube.14fb207c.js";import{C as J}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as ve}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{F as je,M as be}from"../chunks/Markdown.ae01904b.js";import{H as Bs,E as Ie}from"../chunks/getInferenceSnippets.06c2775f.js";function ke(v){let l,j='To see all architectures and checkpoints compatible with this task, we recommend checking the <a href="https://huggingface.co/tasks/automatic-speech-recognition" rel="nofollow">task-page</a>';return{c(){l=c("p"),l.innerHTML=j},l(r){l=m(r,"P",{"data-svelte-h":!0}),g(l)!=="svelte-1gz4c50"&&(l.innerHTML=j)},m(r,b){a(r,l,b)},p:Ys,d(r){r&&e(l)}}}function Re(v){let l,j='If you arenâ€™t familiar with finetuning a model with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#train-with-pytorch-trainer">here</a>!';return{c(){l=c("p"),l.innerHTML=j},l(r){l=m(r,"P",{"data-svelte-h":!0}),g(l)!=="svelte-p303g8"&&(l.innerHTML=j)},m(r,b){a(r,l,b)},p:Ys,d(r){r&&e(l)}}}function Ze(v){let l,j,r,b='You are now ready to start training your model! Load Wav2Vec2 with <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModelForCTC">AutoModelForCTC</a>. Specify the reduction to apply with the <code>ctc_loss_reduction</code> parameter. It is often better to use the average instead of the default summation:',U,C,R,k,I="At this point, only three steps remain:",Z,T,B='<li>Define your training hyperparameters in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. The only required parameter is <code>output_dir</code> which specifies where to save your model. Youâ€™ll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> will evaluate the WER and save the training checkpoint.</li> <li>Pass the training arguments to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, data collator, and <code>compute_metrics</code> function.</li> <li>Call <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.train">train()</a> to fine-tune your model.</li>',W,$,G,o,_='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so it can be accessible to everyone:',Q,X,x;return l=new Nt({props:{$$slots:{default:[Re]},$$scope:{ctx:v}}}),C=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNUQyUyQyUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UlMjIlMkMlMEElMjAlMjAlMjAlMjBjdGNfbG9zc19yZWR1Y3Rpb24lM0QlMjJtZWFuJTIyJTJDJTBBJTIwJTIwJTIwJTIwcGFkX3Rva2VuX2lkJTNEcHJvY2Vzc29yLnRva2VuaXplci5wYWRfdG9rZW5faWQlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>,
<span class="hljs-meta">... </span>    ctc_loss_reduction=<span class="hljs-string">&quot;mean&quot;</span>,
<span class="hljs-meta">... </span>    pad_token_id=processor.tokenizer.pad_token_id,
<span class="hljs-meta">... </span>)`,wrap:!1}}),$=new J({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX2Fzcl9taW5kX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMGdyYWRpZW50X2FjY3VtdWxhdGlvbl9zdGVwcyUzRDIlMkMlMEElMjAlMjAlMjAlMjBsZWFybmluZ19yYXRlJTNEMWUtNSUyQyUwQSUyMCUyMCUyMCUyMHdhcm11cF9zdGVwcyUzRDUwMCUyQyUwQSUyMCUyMCUyMCUyMG1heF9zdGVwcyUzRDIwMDAlMkMlMEElMjAlMjAlMjAlMjBncmFkaWVudF9jaGVja3BvaW50aW5nJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGZwMTYlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwZ3JvdXBfYnlfbGVuZ3RoJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfc3RyYXRlZ3klM0QlMjJzdGVwcyUyMiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfc3RlcHMlM0QxMDAwJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9zdGVwcyUzRDEwMDAlMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMjUlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMG1ldHJpY19mb3JfYmVzdF9tb2RlbCUzRCUyMndlciUyMiUyQyUwQSUyMCUyMCUyMCUyMGdyZWF0ZXJfaXNfYmV0dGVyJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBwdXNoX3RvX2h1YiUzRFRydWUlMkMlMEEpJTBBJTBBdHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRGVuY29kZWRfbWluZHMlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRGVuY29kZWRfbWluZHMlNUIlMjJ0ZXN0JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcHJvY2Vzc2luZ19jbGFzcyUzRHByb2Nlc3NvciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_asr_mind_model&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    gradient_accumulation_steps=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">1e-5</span>,
<span class="hljs-meta">... </span>    warmup_steps=<span class="hljs-number">500</span>,
<span class="hljs-meta">... </span>    max_steps=<span class="hljs-number">2000</span>,
<span class="hljs-meta">... </span>    gradient_checkpointing=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    group_by_length=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    eval_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">1000</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">1000</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">25</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;wer&quot;</span>,
<span class="hljs-meta">... </span>    greater_is_better=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=encoded_minds[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=encoded_minds[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    processing_class=processor,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),X=new J({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){h(l.$$.fragment),j=n(),r=c("p"),r.innerHTML=b,U=n(),h(C.$$.fragment),R=n(),k=c("p"),k.textContent=I,Z=n(),T=c("ol"),T.innerHTML=B,W=n(),h($.$$.fragment),G=n(),o=c("p"),o.innerHTML=_,Q=n(),h(X.$$.fragment)},l(i){d(l.$$.fragment,i),j=p(i),r=m(i,"P",{"data-svelte-h":!0}),g(r)!=="svelte-5uwbye"&&(r.innerHTML=b),U=p(i),d(C.$$.fragment,i),R=p(i),k=m(i,"P",{"data-svelte-h":!0}),g(k)!=="svelte-l42k0i"&&(k.textContent=I),Z=p(i),T=m(i,"OL",{"data-svelte-h":!0}),g(T)!=="svelte-1rcjfot"&&(T.innerHTML=B),W=p(i),d($.$$.fragment,i),G=p(i),o=m(i,"P",{"data-svelte-h":!0}),g(o)!=="svelte-axoj7w"&&(o.innerHTML=_),Q=p(i),d(X.$$.fragment,i)},m(i,w){u(l,i,w),a(i,j,w),a(i,r,w),a(i,U,w),u(C,i,w),a(i,R,w),a(i,k,w),a(i,Z,w),a(i,T,w),a(i,W,w),u($,i,w),a(i,G,w),a(i,o,w),a(i,Q,w),u(X,i,w),x=!0},p(i,w){const Vs={};w&2&&(Vs.$$scope={dirty:w,ctx:i}),l.$set(Vs)},i(i){x||(M(l.$$.fragment,i),M(C.$$.fragment,i),M($.$$.fragment,i),M(X.$$.fragment,i),x=!0)},o(i){y(l.$$.fragment,i),y(C.$$.fragment,i),y($.$$.fragment,i),y(X.$$.fragment,i),x=!1},d(i){i&&(e(j),e(r),e(U),e(R),e(k),e(Z),e(T),e(W),e(G),e(o),e(Q)),f(l,i),f(C,i),f($,i),f(X,i)}}}function We(v){let l,j;return l=new be({props:{$$slots:{default:[Ze]},$$scope:{ctx:v}}}),{c(){h(l.$$.fragment)},l(r){d(l.$$.fragment,r)},m(r,b){u(l,r,b),j=!0},p(r,b){const U={};b&2&&(U.$$scope={dirty:b,ctx:r}),l.$set(U)},i(r){j||(M(l.$$.fragment,r),j=!0)},o(r){y(l.$$.fragment,r),j=!1},d(r){f(l,r)}}}function xe(v){let l,j='For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog <a href="https://huggingface.co/blog/fine-tune-wav2vec2-english" rel="nofollow">post</a> for English ASR and this <a href="https://huggingface.co/blog/fine-tune-xlsr-wav2vec2" rel="nofollow">post</a> for multilingual ASR.';return{c(){l=c("p"),l.innerHTML=j},l(r){l=m(r,"P",{"data-svelte-h":!0}),g(l)!=="svelte-n9os4d"&&(l.innerHTML=j)},m(r,b){a(r,l,b)},p:Ys,d(r){r&&e(l)}}}function Ge(v){let l,j="The transcription is decent, but it could be better! Try finetuning your model on more examples to get even better results!";return{c(){l=c("p"),l.textContent=j},l(r){l=m(r,"P",{"data-svelte-h":!0}),g(l)!=="svelte-1a2d12i"&&(l.textContent=j)},m(r,b){a(r,l,b)},p:Ys,d(r){r&&e(l)}}}function Xe(v){let l,j="Load a processor to preprocess the audio file and transcription and return the <code>input</code> as PyTorch tensors:",r,b,U,C,R="Pass your inputs to the model and return the logits:",k,I,Z,T,B="Get the predicted <code>input_ids</code> with the highest probability, and use the processor to decode the predicted <code>input_ids</code> back into text:",W,$,G;return b=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_asr_mind_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),I=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNUQyUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRfbW9kZWwlMjIpJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_asr_mind_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),$=new J({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFwcmVkaWN0ZWRfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQXRyYW5zY3JpcHRpb24lMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKHByZWRpY3RlZF9pZHMpJTBBdHJhbnNjcmlwdGlvbg==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription
[<span class="hljs-string">&#x27;I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER&#x27;</span>]`,wrap:!1}}),{c(){l=c("p"),l.innerHTML=j,r=n(),h(b.$$.fragment),U=n(),C=c("p"),C.textContent=R,k=n(),h(I.$$.fragment),Z=n(),T=c("p"),T.innerHTML=B,W=n(),h($.$$.fragment)},l(o){l=m(o,"P",{"data-svelte-h":!0}),g(l)!=="svelte-apwwja"&&(l.innerHTML=j),r=p(o),d(b.$$.fragment,o),U=p(o),C=m(o,"P",{"data-svelte-h":!0}),g(C)!=="svelte-1at92g"&&(C.textContent=R),k=p(o),d(I.$$.fragment,o),Z=p(o),T=m(o,"P",{"data-svelte-h":!0}),g(T)!=="svelte-1hxzj8m"&&(T.innerHTML=B),W=p(o),d($.$$.fragment,o)},m(o,_){a(o,l,_),a(o,r,_),u(b,o,_),a(o,U,_),a(o,C,_),a(o,k,_),u(I,o,_),a(o,Z,_),a(o,T,_),a(o,W,_),u($,o,_),G=!0},p:Ys,i(o){G||(M(b.$$.fragment,o),M(I.$$.fragment,o),M($.$$.fragment,o),G=!0)},o(o){y(b.$$.fragment,o),y(I.$$.fragment,o),y($.$$.fragment,o),G=!1},d(o){o&&(e(l),e(r),e(U),e(C),e(k),e(Z),e(T),e(W)),f(b,o),f(I,o),f($,o)}}}function Be(v){let l,j;return l=new be({props:{$$slots:{default:[Xe]},$$scope:{ctx:v}}}),{c(){h(l.$$.fragment)},l(r){d(l.$$.fragment,r)},m(r,b){u(l,r,b),j=!0},p(r,b){const U={};b&2&&(U.$$scope={dirty:b,ctx:r}),l.$set(U)},i(r){j||(M(l.$$.fragment,r),j=!0)},o(r){y(l.$$.fragment,r),j=!1},d(r){f(l,r)}}}function Ve(v){let l,j,r,b,U,C,R,k,I,Z,T,B="Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users every day, and there are many other useful user-facing applications like live captioning and note-taking during meetings.",W,$,G="This guide will show you how to:",o,_,Q='<li>Fine-tune <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">Wav2Vec2</a> on the <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> dataset to transcribe audio to text.</li> <li>Use your fine-tuned model for inference.</li>',X,x,i,w,Vs="Before you begin, make sure you have all the necessary libraries installed:",Ns,z,Qs,E,Qt="We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:",zs,A,Es,F,As,q,zt='Start by loading a smaller subset of the <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> dataset from the ðŸ¤— Datasets library. This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset.',Fs,S,qs,L,Et="Split the datasetâ€™s <code>train</code> split into a train and test set with the <code>~Dataset.train_test_split</code> method:",Ss,P,Ls,D,At="Then take a look at the dataset:",Ps,K,Ds,O,Ft='While the dataset contains a lot of useful information, like <code>lang_id</code> and <code>english_transcription</code>, this guide focuses on the <code>audio</code> and <code>transcription</code>. Remove the other columns with the <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.remove_columns" rel="nofollow">remove_columns</a> method:',Ks,ss,Os,ts,qt="Review the example again:",st,es,tt,as,St="There are two fields:",et,ls,Lt="<li><code>audio</code>: a 1-dimensional <code>array</code> of the speech signal that must be called to load and resample the audio file.</li> <li><code>transcription</code>: the target text.</li>",at,ns,lt,ps,Pt="The next step is to load a Wav2Vec2 processor to process the audio signal:",nt,rs,pt,is,Dt='The MInDS-14 dataset has a sampling rate of 8000Hz (you can find this information in its <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">dataset card</a>), which means youâ€™ll need to resample the dataset to 16000Hz to use the pretrained Wav2Vec2 model:',rt,os,it,cs,Kt="As you can see in the <code>transcription</code> above, the text contains a mix of uppercase and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase characters so youâ€™ll need to make sure the text matches the tokenizerâ€™s vocabulary:",ot,ms,ct,hs,Ot="Now create a preprocessing function that:",mt,ds,se="<li>Calls the <code>audio</code> column to load and resample the audio file.</li> <li>Extracts the <code>input_values</code> from the audio file and tokenize the <code>transcription</code> column with the processor.</li>",ht,us,dt,Ms,te='To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> function. You can speed up <code>map</code> by increasing the number of processes with the <code>num_proc</code> parameter. Remove the columns you donâ€™t need with the <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.remove_columns" rel="nofollow">remove_columns</a> method:',ut,ys,Mt,fs,ee='ðŸ¤— Transformers doesnâ€™t have a data collator for ASR, so youâ€™ll need to adapt the <a href="/docs/transformers/v4.56.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a> to create a batch of examples. Itâ€™ll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length. While it is possible to pad your text in the <code>tokenizer</code> function by setting <code>padding=True</code>, dynamic padding is more efficient.',yt,gs,ae="Unlike other data collators, this specific data collator needs to apply a different padding method to <code>input_values</code> and <code>labels</code>:",ft,js,gt,bs,le="Now instantiate your <code>DataCollatorForCTCWithPadding</code>:",jt,ws,bt,Js,wt,Us,ne='Including a metric during training is often helpful for evaluating your modelâ€™s performance. You can quickly load an evaluation method with the ðŸ¤— <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> library. For this task, load the <a href="https://huggingface.co/spaces/evaluate-metric/wer" rel="nofollow">word error rate</a> (WER) metric (refer to the ðŸ¤— Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a> to learn more about loading and computing metrics):',Jt,Ts,Ut,$s,pe='Then create a function that passes your predictions and labels to <a href="https://huggingface.co/docs/evaluate/v0.4.5/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a> to calculate the WER:',Tt,_s,$t,Cs,re="Your <code>compute_metrics</code> function is ready to go now, and youâ€™ll return to it when you setup your training.",_t,vs,Ct,V,vt,H,It,Is,kt,ks,ie="Great, now that youâ€™ve fine-tuned a model, you can use it for inference!",Rt,Rs,oe="Load an audio file youâ€™d like to run inference on. Remember to resample the sampling rate of the audio file to match the sampling rate of the model if you need to!",Zt,Zs,Wt,Ws,ce='The simplest way to try out your fine-tuned model for inference is to use it in a <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>. Instantiate a <code>pipeline</code> for automatic speech recognition with your model, and pass your audio file to it:',xt,xs,Gt,Y,Xt,Gs,me="You can also manually replicate the results of the <code>pipeline</code> if youâ€™d like:",Bt,N,Vt,Xs,Ht,Hs,Yt;return U=new Bs({props:{title:"Automatic speech recognition",local:"automatic-speech-recognition",headingTag:"h1"}}),R=new ve({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/asr.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/asr.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/asr.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/asr.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/asr.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/asr.ipynb"}]}}),I=new Ce({props:{id:"TksaY_FDgnk"}}),x=new Nt({props:{$$slots:{default:[ke]},$$scope:{ctx:v}}}),z=new J({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBqaXdlcg==",highlighted:"pip install transformers datasets evaluate jiwer",wrap:!1}}),A=new J({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),F=new Bs({props:{title:"Load MInDS-14 dataset",local:"load-minds-14-dataset",headingTag:"h2"}}),S=new J({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFtaW5kcyUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJQb2x5QUklMkZtaW5kczE0JTIyJTJDJTIwbmFtZSUzRCUyMmVuLVVTJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiU1QiUzQTEwMCU1RCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>minds = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`,wrap:!1}}),P=new J({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),K=new J({props:{code:"bWluZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">16</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">4</span>
    })
})`,wrap:!1}}),ss=new J({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5yZW1vdmVfY29sdW1ucyglNUIlMjJlbmdsaXNoX3RyYW5zY3JpcHRpb24lMjIlMkMlMjAlMjJpbnRlbnRfY2xhc3MlMjIlMkMlMjAlMjJsYW5nX2lkJTIyJTVEKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.remove_columns([<span class="hljs-string">&quot;english_transcription&quot;</span>, <span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;lang_id&quot;</span>])',wrap:!1}}),es=new J({props:{code:"bWluZHMlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , ...,  <span class="hljs-number">0.00024414</span>,
          <span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.00024414</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`,wrap:!1}}),ns=new Bs({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),rs=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`,wrap:!1}}),os=new J({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0QxNl8wMDApKSUwQW1pbmRzJTVCJTIydHJhaW4lMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">2.38064706e-04</span>, -<span class="hljs-number">1.58618059e-04</span>, -<span class="hljs-number">5.43987835e-06</span>, ...,
          <span class="hljs-number">2.78103951e-04</span>,  <span class="hljs-number">2.38446111e-04</span>,  <span class="hljs-number">1.18740834e-04</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`,wrap:!1}}),ms=new J({props:{code:"ZGVmJTIwdXBwZXJjYXNlKGV4YW1wbGUpJTNBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTdCJTIydHJhbnNjcmlwdGlvbiUyMiUzQSUyMGV4YW1wbGUlNUIlMjJ0cmFuc2NyaXB0aW9uJTIyJTVELnVwcGVyKCklN0QlMEElMEElMEFtaW5kcyUyMCUzRCUyMG1pbmRzLm1hcCh1cHBlcmNhc2Up",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">uppercase</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;transcription&quot;</span>: example[<span class="hljs-string">&quot;transcription&quot;</span>].upper()}


<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.<span class="hljs-built_in">map</span>(uppercase)`,wrap:!1}}),us=new J({props:{code:"ZGVmJTIwcHJlcGFyZV9kYXRhc2V0KGJhdGNoKSUzQSUwQSUyMCUyMCUyMCUyMGF1ZGlvJTIwJTNEJTIwYmF0Y2glNUIlMjJhdWRpbyUyMiU1RCUwQSUyMCUyMCUyMCUyMGJhdGNoJTIwJTNEJTIwcHJvY2Vzc29yKGF1ZGlvJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEYXVkaW8lNUIlMjJzYW1wbGluZ19yYXRlJTIyJTVEJTJDJTIwdGV4dCUzRGJhdGNoJTVCJTIydHJhbnNjcmlwdGlvbiUyMiU1RCklMEElMjAlMjAlMjAlMjBiYXRjaCU1QiUyMmlucHV0X2xlbmd0aCUyMiU1RCUyMCUzRCUyMGxlbihiYXRjaCU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCU1QjAlNUQpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYmF0Y2g=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
<span class="hljs-meta">... </span>    batch = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>], text=batch[<span class="hljs-string">&quot;transcription&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch`,wrap:!1}}),ys=new J({props:{code:"ZW5jb2RlZF9taW5kcyUyMCUzRCUyMG1pbmRzLm1hcChwcmVwYXJlX2RhdGFzZXQlMkMlMjByZW1vdmVfY29sdW1ucyUzRG1pbmRzLmNvbHVtbl9uYW1lcyU1QiUyMnRyYWluJTIyJTVEJTJDJTIwbnVtX3Byb2MlM0Q0KQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = minds.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=minds.column_names[<span class="hljs-string">&quot;train&quot;</span>], num_proc=<span class="hljs-number">4</span>)',wrap:!1}}),js=new J({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>


<span class="hljs-meta">&gt;&gt;&gt; </span>@dataclass
<span class="hljs-meta">... </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorCTCWithPadding</span>:
<span class="hljs-meta">... </span>    processor: AutoProcessor
<span class="hljs-meta">... </span>    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>] = <span class="hljs-string">&quot;longest&quot;</span>

<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], torch.Tensor]]]</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
<span class="hljs-meta">... </span>        <span class="hljs-comment"># different padding methods</span>
<span class="hljs-meta">... </span>        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
<span class="hljs-meta">... </span>        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

<span class="hljs-meta">... </span>        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
<span class="hljs-meta">... </span>        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> batch`,wrap:!1}}),ws=new J({props:{code:"ZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvckNUQ1dpdGhQYWRkaW5nKHByb2Nlc3NvciUzRHByb2Nlc3NvciUyQyUyMHBhZGRpbmclM0QlMjJsb25nZXN0JTIyKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorCTCWithPadding(processor=processor, padding=<span class="hljs-string">&quot;longest&quot;</span>)',wrap:!1}}),Js=new Bs({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),Ts=new J({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEF3ZXIlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMndlciUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>wer = evaluate.load(<span class="hljs-string">&quot;wer&quot;</span>)`,wrap:!1}}),_s=new J({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKHByZWQpJTNBJTBBJTIwJTIwJTIwJTIwcHJlZF9sb2dpdHMlMjAlM0QlMjBwcmVkLnByZWRpY3Rpb25zJTBBJTIwJTIwJTIwJTIwcHJlZF9pZHMlMjAlM0QlMjBucC5hcmdtYXgocHJlZF9sb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBJTBBJTIwJTIwJTIwJTIwcHJlZC5sYWJlbF9pZHMlNUJwcmVkLmxhYmVsX2lkcyUyMCUzRCUzRCUyMC0xMDAlNUQlMjAlM0QlMjBwcm9jZXNzb3IudG9rZW5pemVyLnBhZF90b2tlbl9pZCUwQSUwQSUyMCUyMCUyMCUyMHByZWRfc3RyJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShwcmVkX2lkcyklMEElMjAlMjAlMjAlMjBsYWJlbF9zdHIlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKHByZWQubGFiZWxfaWRzJTJDJTIwZ3JvdXBfdG9rZW5zJTNERmFsc2UpJTBBJTBBJTIwJTIwJTIwJTIwd2VyX3Njb3JlJTIwJTNEJTIwd2VyLmNvbXB1dGUocHJlZGljdGlvbnMlM0RwcmVkX3N0ciUyQyUyMHJlZmVyZW5jZXMlM0RsYWJlbF9zdHIpJTBBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTdCJTIyd2VyJTIyJTNBJTIwd2VyX3Njb3JlJTdE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">pred</span>):
<span class="hljs-meta">... </span>    pred_logits = pred.predictions
<span class="hljs-meta">... </span>    pred_ids = np.argmax(pred_logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>    pred.label_ids[pred.label_ids == -<span class="hljs-number">100</span>] = processor.tokenizer.pad_token_id

<span class="hljs-meta">... </span>    pred_str = processor.batch_decode(pred_ids)
<span class="hljs-meta">... </span>    label_str = processor.batch_decode(pred.label_ids, group_tokens=<span class="hljs-literal">False</span>)

<span class="hljs-meta">... </span>    wer_score = wer.compute(predictions=pred_str, references=label_str)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;wer&quot;</span>: wer_score}`,wrap:!1}}),vs=new Bs({props:{title:"Train",local:"train",headingTag:"h2"}}),V=new je({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[We]},$$scope:{ctx:v}}}),H=new Nt({props:{$$slots:{default:[xe]},$$scope:{ctx:v}}}),Is=new Bs({props:{title:"Inference",local:"inference",headingTag:"h2"}}),Zs=new J({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjAlMjJlbi1VUyUyMiUyQyUyMHNwbGl0JTNEJTIydHJhaW4lMjIpJTBBZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEMTYwMDApKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQWF1ZGlvX2ZpbGUlMjAlM0QlMjBkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIycGF0aCUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, <span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_file = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;path&quot;</span>]`,wrap:!1}}),xs=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBdHJhbnNjcmliZXIlMjAlM0QlMjBwaXBlbGluZSglMjJhdXRvbWF0aWMtc3BlZWNoLXJlY29nbml0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRzX21vZGVsJTIyKSUwQXRyYW5zY3JpYmVyKGF1ZGlvX2ZpbGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>transcriber = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_asr_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcriber(audio_file)
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER&#x27;</span>}`,wrap:!1}}),Y=new Nt({props:{$$slots:{default:[Ge]},$$scope:{ctx:v}}}),N=new je({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Be]},$$scope:{ctx:v}}}),Xs=new Ie({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md"}}),{c(){l=c("meta"),j=n(),r=c("p"),b=n(),h(U.$$.fragment),C=n(),h(R.$$.fragment),k=n(),h(I.$$.fragment),Z=n(),T=c("p"),T.textContent=B,W=n(),$=c("p"),$.textContent=G,o=n(),_=c("ol"),_.innerHTML=Q,X=n(),h(x.$$.fragment),i=n(),w=c("p"),w.textContent=Vs,Ns=n(),h(z.$$.fragment),Qs=n(),E=c("p"),E.textContent=Qt,zs=n(),h(A.$$.fragment),Es=n(),h(F.$$.fragment),As=n(),q=c("p"),q.innerHTML=zt,Fs=n(),h(S.$$.fragment),qs=n(),L=c("p"),L.innerHTML=Et,Ss=n(),h(P.$$.fragment),Ls=n(),D=c("p"),D.textContent=At,Ps=n(),h(K.$$.fragment),Ds=n(),O=c("p"),O.innerHTML=Ft,Ks=n(),h(ss.$$.fragment),Os=n(),ts=c("p"),ts.textContent=qt,st=n(),h(es.$$.fragment),tt=n(),as=c("p"),as.textContent=St,et=n(),ls=c("ul"),ls.innerHTML=Lt,at=n(),h(ns.$$.fragment),lt=n(),ps=c("p"),ps.textContent=Pt,nt=n(),h(rs.$$.fragment),pt=n(),is=c("p"),is.innerHTML=Dt,rt=n(),h(os.$$.fragment),it=n(),cs=c("p"),cs.innerHTML=Kt,ot=n(),h(ms.$$.fragment),ct=n(),hs=c("p"),hs.textContent=Ot,mt=n(),ds=c("ol"),ds.innerHTML=se,ht=n(),h(us.$$.fragment),dt=n(),Ms=c("p"),Ms.innerHTML=te,ut=n(),h(ys.$$.fragment),Mt=n(),fs=c("p"),fs.innerHTML=ee,yt=n(),gs=c("p"),gs.innerHTML=ae,ft=n(),h(js.$$.fragment),gt=n(),bs=c("p"),bs.innerHTML=le,jt=n(),h(ws.$$.fragment),bt=n(),h(Js.$$.fragment),wt=n(),Us=c("p"),Us.innerHTML=ne,Jt=n(),h(Ts.$$.fragment),Ut=n(),$s=c("p"),$s.innerHTML=pe,Tt=n(),h(_s.$$.fragment),$t=n(),Cs=c("p"),Cs.innerHTML=re,_t=n(),h(vs.$$.fragment),Ct=n(),h(V.$$.fragment),vt=n(),h(H.$$.fragment),It=n(),h(Is.$$.fragment),kt=n(),ks=c("p"),ks.textContent=ie,Rt=n(),Rs=c("p"),Rs.textContent=oe,Zt=n(),h(Zs.$$.fragment),Wt=n(),Ws=c("p"),Ws.innerHTML=ce,xt=n(),h(xs.$$.fragment),Gt=n(),h(Y.$$.fragment),Xt=n(),Gs=c("p"),Gs.innerHTML=me,Bt=n(),h(N.$$.fragment),Vt=n(),h(Xs.$$.fragment),Ht=n(),Hs=c("p"),this.h()},l(s){const t=$e("svelte-u9bgzb",document.head);l=m(t,"META",{name:!0,content:!0}),t.forEach(e),j=p(s),r=m(s,"P",{}),fe(r).forEach(e),b=p(s),d(U.$$.fragment,s),C=p(s),d(R.$$.fragment,s),k=p(s),d(I.$$.fragment,s),Z=p(s),T=m(s,"P",{"data-svelte-h":!0}),g(T)!=="svelte-xxpt4p"&&(T.textContent=B),W=p(s),$=m(s,"P",{"data-svelte-h":!0}),g($)!=="svelte-1aff4p7"&&($.textContent=G),o=p(s),_=m(s,"OL",{"data-svelte-h":!0}),g(_)!=="svelte-4q000i"&&(_.innerHTML=Q),X=p(s),d(x.$$.fragment,s),i=p(s),w=m(s,"P",{"data-svelte-h":!0}),g(w)!=="svelte-1c9nexd"&&(w.textContent=Vs),Ns=p(s),d(z.$$.fragment,s),Qs=p(s),E=m(s,"P",{"data-svelte-h":!0}),g(E)!=="svelte-k76o1m"&&(E.textContent=Qt),zs=p(s),d(A.$$.fragment,s),Es=p(s),d(F.$$.fragment,s),As=p(s),q=m(s,"P",{"data-svelte-h":!0}),g(q)!=="svelte-10uh6ot"&&(q.innerHTML=zt),Fs=p(s),d(S.$$.fragment,s),qs=p(s),L=m(s,"P",{"data-svelte-h":!0}),g(L)!=="svelte-1mnfdd3"&&(L.innerHTML=Et),Ss=p(s),d(P.$$.fragment,s),Ls=p(s),D=m(s,"P",{"data-svelte-h":!0}),g(D)!=="svelte-2twqg0"&&(D.textContent=At),Ps=p(s),d(K.$$.fragment,s),Ds=p(s),O=m(s,"P",{"data-svelte-h":!0}),g(O)!=="svelte-qygcd6"&&(O.innerHTML=Ft),Ks=p(s),d(ss.$$.fragment,s),Os=p(s),ts=m(s,"P",{"data-svelte-h":!0}),g(ts)!=="svelte-1pea4m9"&&(ts.textContent=qt),st=p(s),d(es.$$.fragment,s),tt=p(s),as=m(s,"P",{"data-svelte-h":!0}),g(as)!=="svelte-bf7elb"&&(as.textContent=St),et=p(s),ls=m(s,"UL",{"data-svelte-h":!0}),g(ls)!=="svelte-k1dj8f"&&(ls.innerHTML=Lt),at=p(s),d(ns.$$.fragment,s),lt=p(s),ps=m(s,"P",{"data-svelte-h":!0}),g(ps)!=="svelte-1u4mmu7"&&(ps.textContent=Pt),nt=p(s),d(rs.$$.fragment,s),pt=p(s),is=m(s,"P",{"data-svelte-h":!0}),g(is)!=="svelte-1xny8tf"&&(is.innerHTML=Dt),rt=p(s),d(os.$$.fragment,s),it=p(s),cs=m(s,"P",{"data-svelte-h":!0}),g(cs)!=="svelte-1nybp3u"&&(cs.innerHTML=Kt),ot=p(s),d(ms.$$.fragment,s),ct=p(s),hs=m(s,"P",{"data-svelte-h":!0}),g(hs)!=="svelte-8cflje"&&(hs.textContent=Ot),mt=p(s),ds=m(s,"OL",{"data-svelte-h":!0}),g(ds)!=="svelte-1ydcdgg"&&(ds.innerHTML=se),ht=p(s),d(us.$$.fragment,s),dt=p(s),Ms=m(s,"P",{"data-svelte-h":!0}),g(Ms)!=="svelte-sz5cc8"&&(Ms.innerHTML=te),ut=p(s),d(ys.$$.fragment,s),Mt=p(s),fs=m(s,"P",{"data-svelte-h":!0}),g(fs)!=="svelte-c1bhtt"&&(fs.innerHTML=ee),yt=p(s),gs=m(s,"P",{"data-svelte-h":!0}),g(gs)!=="svelte-ik80vf"&&(gs.innerHTML=ae),ft=p(s),d(js.$$.fragment,s),gt=p(s),bs=m(s,"P",{"data-svelte-h":!0}),g(bs)!=="svelte-hven70"&&(bs.innerHTML=le),jt=p(s),d(ws.$$.fragment,s),bt=p(s),d(Js.$$.fragment,s),wt=p(s),Us=m(s,"P",{"data-svelte-h":!0}),g(Us)!=="svelte-1wez95o"&&(Us.innerHTML=ne),Jt=p(s),d(Ts.$$.fragment,s),Ut=p(s),$s=m(s,"P",{"data-svelte-h":!0}),g($s)!=="svelte-2abulw"&&($s.innerHTML=pe),Tt=p(s),d(_s.$$.fragment,s),$t=p(s),Cs=m(s,"P",{"data-svelte-h":!0}),g(Cs)!=="svelte-183aynn"&&(Cs.innerHTML=re),_t=p(s),d(vs.$$.fragment,s),Ct=p(s),d(V.$$.fragment,s),vt=p(s),d(H.$$.fragment,s),It=p(s),d(Is.$$.fragment,s),kt=p(s),ks=m(s,"P",{"data-svelte-h":!0}),g(ks)!=="svelte-l3g61e"&&(ks.textContent=ie),Rt=p(s),Rs=m(s,"P",{"data-svelte-h":!0}),g(Rs)!=="svelte-1j24vrm"&&(Rs.textContent=oe),Zt=p(s),d(Zs.$$.fragment,s),Wt=p(s),Ws=m(s,"P",{"data-svelte-h":!0}),g(Ws)!=="svelte-odijn6"&&(Ws.innerHTML=ce),xt=p(s),d(xs.$$.fragment,s),Gt=p(s),d(Y.$$.fragment,s),Xt=p(s),Gs=m(s,"P",{"data-svelte-h":!0}),g(Gs)!=="svelte-1njl8vm"&&(Gs.innerHTML=me),Bt=p(s),d(N.$$.fragment,s),Vt=p(s),d(Xs.$$.fragment,s),Ht=p(s),Hs=m(s,"P",{}),fe(Hs).forEach(e),this.h()},h(){ge(l,"name","hf:doc:metadata"),ge(l,"content",He)},m(s,t){_e(document.head,l),a(s,j,t),a(s,r,t),a(s,b,t),u(U,s,t),a(s,C,t),u(R,s,t),a(s,k,t),u(I,s,t),a(s,Z,t),a(s,T,t),a(s,W,t),a(s,$,t),a(s,o,t),a(s,_,t),a(s,X,t),u(x,s,t),a(s,i,t),a(s,w,t),a(s,Ns,t),u(z,s,t),a(s,Qs,t),a(s,E,t),a(s,zs,t),u(A,s,t),a(s,Es,t),u(F,s,t),a(s,As,t),a(s,q,t),a(s,Fs,t),u(S,s,t),a(s,qs,t),a(s,L,t),a(s,Ss,t),u(P,s,t),a(s,Ls,t),a(s,D,t),a(s,Ps,t),u(K,s,t),a(s,Ds,t),a(s,O,t),a(s,Ks,t),u(ss,s,t),a(s,Os,t),a(s,ts,t),a(s,st,t),u(es,s,t),a(s,tt,t),a(s,as,t),a(s,et,t),a(s,ls,t),a(s,at,t),u(ns,s,t),a(s,lt,t),a(s,ps,t),a(s,nt,t),u(rs,s,t),a(s,pt,t),a(s,is,t),a(s,rt,t),u(os,s,t),a(s,it,t),a(s,cs,t),a(s,ot,t),u(ms,s,t),a(s,ct,t),a(s,hs,t),a(s,mt,t),a(s,ds,t),a(s,ht,t),u(us,s,t),a(s,dt,t),a(s,Ms,t),a(s,ut,t),u(ys,s,t),a(s,Mt,t),a(s,fs,t),a(s,yt,t),a(s,gs,t),a(s,ft,t),u(js,s,t),a(s,gt,t),a(s,bs,t),a(s,jt,t),u(ws,s,t),a(s,bt,t),u(Js,s,t),a(s,wt,t),a(s,Us,t),a(s,Jt,t),u(Ts,s,t),a(s,Ut,t),a(s,$s,t),a(s,Tt,t),u(_s,s,t),a(s,$t,t),a(s,Cs,t),a(s,_t,t),u(vs,s,t),a(s,Ct,t),u(V,s,t),a(s,vt,t),u(H,s,t),a(s,It,t),u(Is,s,t),a(s,kt,t),a(s,ks,t),a(s,Rt,t),a(s,Rs,t),a(s,Zt,t),u(Zs,s,t),a(s,Wt,t),a(s,Ws,t),a(s,xt,t),u(xs,s,t),a(s,Gt,t),u(Y,s,t),a(s,Xt,t),a(s,Gs,t),a(s,Bt,t),u(N,s,t),a(s,Vt,t),u(Xs,s,t),a(s,Ht,t),a(s,Hs,t),Yt=!0},p(s,[t]){const he={};t&2&&(he.$$scope={dirty:t,ctx:s}),x.$set(he);const de={};t&2&&(de.$$scope={dirty:t,ctx:s}),V.$set(de);const ue={};t&2&&(ue.$$scope={dirty:t,ctx:s}),H.$set(ue);const Me={};t&2&&(Me.$$scope={dirty:t,ctx:s}),Y.$set(Me);const ye={};t&2&&(ye.$$scope={dirty:t,ctx:s}),N.$set(ye)},i(s){Yt||(M(U.$$.fragment,s),M(R.$$.fragment,s),M(I.$$.fragment,s),M(x.$$.fragment,s),M(z.$$.fragment,s),M(A.$$.fragment,s),M(F.$$.fragment,s),M(S.$$.fragment,s),M(P.$$.fragment,s),M(K.$$.fragment,s),M(ss.$$.fragment,s),M(es.$$.fragment,s),M(ns.$$.fragment,s),M(rs.$$.fragment,s),M(os.$$.fragment,s),M(ms.$$.fragment,s),M(us.$$.fragment,s),M(ys.$$.fragment,s),M(js.$$.fragment,s),M(ws.$$.fragment,s),M(Js.$$.fragment,s),M(Ts.$$.fragment,s),M(_s.$$.fragment,s),M(vs.$$.fragment,s),M(V.$$.fragment,s),M(H.$$.fragment,s),M(Is.$$.fragment,s),M(Zs.$$.fragment,s),M(xs.$$.fragment,s),M(Y.$$.fragment,s),M(N.$$.fragment,s),M(Xs.$$.fragment,s),Yt=!0)},o(s){y(U.$$.fragment,s),y(R.$$.fragment,s),y(I.$$.fragment,s),y(x.$$.fragment,s),y(z.$$.fragment,s),y(A.$$.fragment,s),y(F.$$.fragment,s),y(S.$$.fragment,s),y(P.$$.fragment,s),y(K.$$.fragment,s),y(ss.$$.fragment,s),y(es.$$.fragment,s),y(ns.$$.fragment,s),y(rs.$$.fragment,s),y(os.$$.fragment,s),y(ms.$$.fragment,s),y(us.$$.fragment,s),y(ys.$$.fragment,s),y(js.$$.fragment,s),y(ws.$$.fragment,s),y(Js.$$.fragment,s),y(Ts.$$.fragment,s),y(_s.$$.fragment,s),y(vs.$$.fragment,s),y(V.$$.fragment,s),y(H.$$.fragment,s),y(Is.$$.fragment,s),y(Zs.$$.fragment,s),y(xs.$$.fragment,s),y(Y.$$.fragment,s),y(N.$$.fragment,s),y(Xs.$$.fragment,s),Yt=!1},d(s){s&&(e(j),e(r),e(b),e(C),e(k),e(Z),e(T),e(W),e($),e(o),e(_),e(X),e(i),e(w),e(Ns),e(Qs),e(E),e(zs),e(Es),e(As),e(q),e(Fs),e(qs),e(L),e(Ss),e(Ls),e(D),e(Ps),e(Ds),e(O),e(Ks),e(Os),e(ts),e(st),e(tt),e(as),e(et),e(ls),e(at),e(lt),e(ps),e(nt),e(pt),e(is),e(rt),e(it),e(cs),e(ot),e(ct),e(hs),e(mt),e(ds),e(ht),e(dt),e(Ms),e(ut),e(Mt),e(fs),e(yt),e(gs),e(ft),e(gt),e(bs),e(jt),e(bt),e(wt),e(Us),e(Jt),e(Ut),e($s),e(Tt),e($t),e(Cs),e(_t),e(Ct),e(vt),e(It),e(kt),e(ks),e(Rt),e(Rs),e(Zt),e(Wt),e(Ws),e(xt),e(Gt),e(Xt),e(Gs),e(Bt),e(Vt),e(Ht),e(Hs)),e(l),f(U,s),f(R,s),f(I,s),f(x,s),f(z,s),f(A,s),f(F,s),f(S,s),f(P,s),f(K,s),f(ss,s),f(es,s),f(ns,s),f(rs,s),f(os,s),f(ms,s),f(us,s),f(ys,s),f(js,s),f(ws,s),f(Js,s),f(Ts,s),f(_s,s),f(vs,s),f(V,s),f(H,s),f(Is,s),f(Zs,s),f(xs,s),f(Y,s),f(N,s),f(Xs,s)}}}const He='{"title":"Automatic speech recognition","local":"automatic-speech-recognition","sections":[{"title":"Load MInDS-14 dataset","local":"load-minds-14-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Ye(v){return Je(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Le extends Ue{constructor(l){super(),Te(this,l,Ye,Ve,we,{})}}export{Le as component};
