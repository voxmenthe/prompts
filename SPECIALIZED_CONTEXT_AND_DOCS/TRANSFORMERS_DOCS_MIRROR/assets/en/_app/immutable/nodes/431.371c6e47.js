import{s as _t,z as yt,o as Mt,n as De}from"../chunks/scheduler.18a86fab.js";import{S as vt,i as wt,g as l,s as a,r as u,A as Ut,h as m,f as n,c as s,j as fe,x as p,u as h,k as j,y as f,a as o,v as b,d as _,t as y,w as M}from"../chunks/index.98837b22.js";import{T as $t}from"../chunks/Tip.77304350.js";import{D as Ae}from"../chunks/Docstring.a1ef7999.js";import{C as Ge}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as bt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as pe,E as Tt}from"../chunks/getInferenceSnippets.06c2775f.js";function xt(k){let r,w="Examples:",d,c,g;return c=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFVwZXJOZXRDb25maWclMkMlMjBVcGVyTmV0Rm9yU2VtYW50aWNTZWdtZW50YXRpb24lMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBVcGVyTmV0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbihjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UperNetConfig, UperNetForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UperNetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UperNetForSemanticSegmentation(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=l("p"),r.textContent=w,d=a(),u(c.$$.fragment)},l(i){r=m(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-kvfsh7"&&(r.textContent=w),d=s(i),h(c.$$.fragment,i)},m(i,T){o(i,r,T),o(i,d,T),b(c,i,T),g=!0},p:De,i(i){g||(_(c.$$.fragment,i),g=!0)},o(i){y(c.$$.fragment,i),g=!1},d(i){i&&(n(r),n(d)),M(c,i)}}}function Nt(k){let r,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=l("p"),r.innerHTML=w},l(d){r=m(d,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=w)},m(d,c){o(d,r,c)},p:De,d(d){d&&n(r)}}}function Ct(k){let r,w="Examples:",d,c,g;return c=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjBodWdnaW5nZmFjZV9odWIlMjBpbXBvcnQlMjBoZl9odWJfZG93bmxvYWQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5tbWxhYiUyRnVwZXJuZXQtY29udm5leHQtdGlueSUyMiklMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyb3Blbm1tbGFiJTJGdXBlcm5ldC1jb252bmV4dC10aW55JTIyKSUwQSUwQWZpbGVwYXRoJTIwJTNEJTIwaGZfaHViX2Rvd25sb2FkKCUwQSUyMCUyMCUyMCUyMHJlcG9faWQlM0QlMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGZml4dHVyZXNfYWRlMjBrJTIyJTJDJTIwZmlsZW5hbWUlM0QlMjJBREVfdmFsXzAwMDAwMDAxLmpwZyUyMiUyQyUyMHJlcG9fdHlwZSUzRCUyMmRhdGFzZXQlMjIlMEEpJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKGZpbGVwYXRoKS5jb252ZXJ0KCUyMlJHQiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUyMCUyMCUyMyUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fbGFiZWxzJTJDJTIwaGVpZ2h0JTJDJTIwd2lkdGgpJTBBbGlzdChsb2dpdHMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, UperNetForSemanticSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;openmmlab/upernet-convnext-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UperNetForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;openmmlab/upernet-convnext-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>filepath = hf_hub_download(
<span class="hljs-meta">... </span>    repo_id=<span class="hljs-string">&quot;hf-internal-testing/fixtures_ade20k&quot;</span>, filename=<span class="hljs-string">&quot;ADE_val_00000001.jpg&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(filepath).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits  <span class="hljs-comment"># shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">150</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>]`,wrap:!1}}),{c(){r=l("p"),r.textContent=w,d=a(),u(c.$$.fragment)},l(i){r=m(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-kvfsh7"&&(r.textContent=w),d=s(i),h(c.$$.fragment,i)},m(i,T){o(i,r,T),o(i,d,T),b(c,i,T),g=!0},p:De,i(i){g||(_(c.$$.fragment,i),g=!0)},o(i){y(c.$$.fragment,i),g=!1},d(i){i&&(n(r),n(d)),M(c,i)}}}function jt(k){let r,w,d,c,g,i="<em>This model was released on 2018-07-26 and added to Hugging Face Transformers on 2023-01-16.</em>",T,P,ge,Z,Oe='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ue,R,he,V,Ke=`The UPerNet model was proposed in <a href="https://huggingface.co/papers/1807.10221" rel="nofollow">Unified Perceptual Parsing for Scene Understanding</a>
by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment
a wide range of concepts from images, leveraging any vision backbone like <a href="convnext">ConvNeXt</a> or <a href="swin">Swin</a>.`,be,G,et="The abstract from the paper is the following:",_e,z,tt="<em>Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes.</em>",ye,S,nt,Me,I,ot='UPerNet framework. Taken from the <a href="https://huggingface.co/papers/1807.10221">original paper</a>.',ve,X,at='This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>. The original code is based on OpenMMLabâ€™s mmsegmentation <a href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py" rel="nofollow">here</a>.',we,B,Ue,H,st="UPerNet is a general framework for semantic segmentation. It can be used with any vision backbone, like so:",$e,E,Te,Q,rt='To use another vision backbone, like <a href="convnext">ConvNeXt</a>, simply instantiate the model with the appropriate backbone:',xe,Y,Ne,L,it="Note that this will randomly initialize all the weights of the model.",Ce,q,je,A,lt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with UPerNet.",ke,D,mt='<li>Demo notebooks for UPerNet can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet" rel="nofollow">here</a>.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/semantic_segmentation">Semantic segmentation task guide</a></li>',Ze,O,ct="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Se,K,Fe,U,ee,ze,se,pt=`This is the configuration class to store the configuration of an <a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a>. It is used to
instantiate an UperNet model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UperNet
<a href="https://huggingface.co/openmmlab/upernet-convnext-tiny" rel="nofollow">openmmlab/upernet-convnext-tiny</a> architecture.`,Ie,re,dt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Xe,F,Je,te,We,v,ne,Be,ie,ft="UperNet framework leveraging any vision backbone e.g. for ADE20k, CityScapes.",He,le,gt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ee,me,ut=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Qe,x,oe,Ye,ce,ht='The <a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',Le,J,qe,W,Pe,ae,Re,de,Ve;return P=new pe({props:{title:"UPerNet",local:"upernet",headingTag:"h1"}}),R=new pe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),B=new pe({props:{title:"Usage examples",local:"usage-examples",headingTag:"h2"}}),E=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aW5Db25maWclMkMlMjBVcGVyTmV0Q29uZmlnJTJDJTIwVXBlck5ldEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBYmFja2JvbmVfY29uZmlnJTIwJTNEJTIwU3dpbkNvbmZpZyhvdXRfZmVhdHVyZXMlM0QlNUIlMjJzdGFnZTElMjIlMkMlMjAlMjJzdGFnZTIlMjIlMkMlMjAlMjJzdGFnZTMlMjIlMkMlMjAlMjJzdGFnZTQlMjIlNUQpJTBBJTBBY29uZmlnJTIwJTNEJTIwVXBlck5ldENvbmZpZyhiYWNrYm9uZV9jb25maWclM0RiYWNrYm9uZV9jb25maWcpJTBBbW9kZWwlMjAlM0QlMjBVcGVyTmV0Rm9yU2VtYW50aWNTZWdtZW50YXRpb24oY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SwinConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = SwinConfig(out_features=[<span class="hljs-string">&quot;stage1&quot;</span>, <span class="hljs-string">&quot;stage2&quot;</span>, <span class="hljs-string">&quot;stage3&quot;</span>, <span class="hljs-string">&quot;stage4&quot;</span>])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)`,wrap:!1}}),Y=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENvbnZOZXh0Q29uZmlnJTJDJTIwVXBlck5ldENvbmZpZyUyQyUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQWJhY2tib25lX2NvbmZpZyUyMCUzRCUyMENvbnZOZXh0Q29uZmlnKG91dF9mZWF0dXJlcyUzRCU1QiUyMnN0YWdlMSUyMiUyQyUyMCUyMnN0YWdlMiUyMiUyQyUyMCUyMnN0YWdlMyUyMiUyQyUyMCUyMnN0YWdlNCUyMiU1RCklMEElMEFjb25maWclMjAlM0QlMjBVcGVyTmV0Q29uZmlnKGJhY2tib25lX2NvbmZpZyUzRGJhY2tib25lX2NvbmZpZyklMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbihjb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = ConvNextConfig(out_features=[<span class="hljs-string">&quot;stage1&quot;</span>, <span class="hljs-string">&quot;stage2&quot;</span>, <span class="hljs-string">&quot;stage3&quot;</span>, <span class="hljs-string">&quot;stage4&quot;</span>])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)`,wrap:!1}}),q=new pe({props:{title:"Resources",local:"resources",headingTag:"h2"}}),K=new pe({props:{title:"UperNetConfig",local:"transformers.UperNetConfig",headingTag:"h2"}}),ee=new Ae({props:{name:"class transformers.UperNetConfig",anchor:"transformers.UperNetConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"hidden_size",val:" = 512"},{name:"initializer_range",val:" = 0.02"},{name:"pool_scales",val:" = [1, 2, 3, 6]"},{name:"use_auxiliary_head",val:" = True"},{name:"auxiliary_loss_weight",val:" = 0.4"},{name:"auxiliary_in_channels",val:" = None"},{name:"auxiliary_channels",val:" = 256"},{name:"auxiliary_num_convs",val:" = 1"},{name:"auxiliary_concat_input",val:" = False"},{name:"loss_ignore_index",val:" = 255"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UperNetConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>, defaults to <code>ResNetConfig()</code>) &#x2014;
The configuration of the backbone model.`,name:"backbone_config"},{anchor:"transformers.UperNetConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.UperNetConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.UperNetConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, <code>False</code>) &#x2014;
Whether to load <code>backbone</code> from the timm library. If <code>False</code>, the backbone is loaded from the transformers
library.`,name:"use_timm_backbone"},{anchor:"transformers.UperNetConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.UperNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The number of hidden units in the convolutional layers.`,name:"hidden_size"},{anchor:"transformers.UperNetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UperNetConfig.pool_scales",description:`<strong>pool_scales</strong> (<code>tuple[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 3, 6]</code>) &#x2014;
Pooling scales used in Pooling Pyramid Module applied on the last feature map.`,name:"pool_scales"},{anchor:"transformers.UperNetConfig.use_auxiliary_head",description:`<strong>use_auxiliary_head</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an auxiliary head during training.`,name:"use_auxiliary_head"},{anchor:"transformers.UperNetConfig.auxiliary_loss_weight",description:`<strong>auxiliary_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Weight of the cross-entropy loss of the auxiliary head.`,name:"auxiliary_loss_weight"},{anchor:"transformers.UperNetConfig.auxiliary_channels",description:`<strong>auxiliary_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of channels to use in the auxiliary head.`,name:"auxiliary_channels"},{anchor:"transformers.UperNetConfig.auxiliary_num_convs",description:`<strong>auxiliary_num_convs</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of convolutional layers to use in the auxiliary head.`,name:"auxiliary_num_convs"},{anchor:"transformers.UperNetConfig.auxiliary_concat_input",description:`<strong>auxiliary_concat_input</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to concatenate the output of the auxiliary head with the input before the classification layer.`,name:"auxiliary_concat_input"},{anchor:"transformers.UperNetConfig.loss_ignore_index",description:`<strong>loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function.`,name:"loss_ignore_index"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/upernet/configuration_upernet.py#L26"}}),F=new bt({props:{anchor:"transformers.UperNetConfig.example",$$slots:{default:[xt]},$$scope:{ctx:k}}}),te=new pe({props:{title:"UperNetForSemanticSegmentation",local:"transformers.UperNetForSemanticSegmentation",headingTag:"h2"}}),ne=new Ae({props:{name:"class transformers.UperNetForSemanticSegmentation",anchor:"transformers.UperNetForSemanticSegmentation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.UperNetForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/upernet/modeling_upernet.py#L289"}}),oe=new Ae({props:{name:"forward",anchor:"transformers.UperNetForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.UperNetForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__">SegformerImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/upernet/modeling_upernet.py#L304",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/upernet#transformers.UperNetConfig"
>UperNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) â€” Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),J=new $t({props:{$$slots:{default:[Nt]},$$scope:{ctx:k}}}),W=new bt({props:{anchor:"transformers.UperNetForSemanticSegmentation.forward.example",$$slots:{default:[Ct]},$$scope:{ctx:k}}}),ae=new Tt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/upernet.md"}}),{c(){r=l("meta"),w=a(),d=l("p"),c=a(),g=l("p"),g.innerHTML=i,T=a(),u(P.$$.fragment),ge=a(),Z=l("div"),Z.innerHTML=Oe,ue=a(),u(R.$$.fragment),he=a(),V=l("p"),V.innerHTML=Ke,be=a(),G=l("p"),G.textContent=et,_e=a(),z=l("p"),z.innerHTML=tt,ye=a(),S=l("img"),Me=a(),I=l("small"),I.innerHTML=ot,ve=a(),X=l("p"),X.innerHTML=at,we=a(),u(B.$$.fragment),Ue=a(),H=l("p"),H.textContent=st,$e=a(),u(E.$$.fragment),Te=a(),Q=l("p"),Q.innerHTML=rt,xe=a(),u(Y.$$.fragment),Ne=a(),L=l("p"),L.textContent=it,Ce=a(),u(q.$$.fragment),je=a(),A=l("p"),A.textContent=lt,ke=a(),D=l("ul"),D.innerHTML=mt,Ze=a(),O=l("p"),O.textContent=ct,Se=a(),u(K.$$.fragment),Fe=a(),U=l("div"),u(ee.$$.fragment),ze=a(),se=l("p"),se.innerHTML=pt,Ie=a(),re=l("p"),re.innerHTML=dt,Xe=a(),u(F.$$.fragment),Je=a(),u(te.$$.fragment),We=a(),v=l("div"),u(ne.$$.fragment),Be=a(),ie=l("p"),ie.textContent=ft,He=a(),le=l("p"),le.innerHTML=gt,Ee=a(),me=l("p"),me.innerHTML=ut,Qe=a(),x=l("div"),u(oe.$$.fragment),Ye=a(),ce=l("p"),ce.innerHTML=ht,Le=a(),u(J.$$.fragment),qe=a(),u(W.$$.fragment),Pe=a(),u(ae.$$.fragment),Re=a(),de=l("p"),this.h()},l(e){const t=Ut("svelte-u9bgzb",document.head);r=m(t,"META",{name:!0,content:!0}),t.forEach(n),w=s(e),d=m(e,"P",{}),fe(d).forEach(n),c=s(e),g=m(e,"P",{"data-svelte-h":!0}),p(g)!=="svelte-1rvgt3v"&&(g.innerHTML=i),T=s(e),h(P.$$.fragment,e),ge=s(e),Z=m(e,"DIV",{class:!0,"data-svelte-h":!0}),p(Z)!=="svelte-13t8s2t"&&(Z.innerHTML=Oe),ue=s(e),h(R.$$.fragment,e),he=s(e),V=m(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-m6gi5c"&&(V.innerHTML=Ke),be=s(e),G=m(e,"P",{"data-svelte-h":!0}),p(G)!=="svelte-vfdo9a"&&(G.textContent=et),_e=s(e),z=m(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-1k7ta1y"&&(z.innerHTML=tt),ye=s(e),S=m(e,"IMG",{src:!0,alt:!0,width:!0}),Me=s(e),I=m(e,"SMALL",{"data-svelte-h":!0}),p(I)!=="svelte-5fyp21"&&(I.innerHTML=ot),ve=s(e),X=m(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-1xqy3xr"&&(X.innerHTML=at),we=s(e),h(B.$$.fragment,e),Ue=s(e),H=m(e,"P",{"data-svelte-h":!0}),p(H)!=="svelte-gv2geu"&&(H.textContent=st),$e=s(e),h(E.$$.fragment,e),Te=s(e),Q=m(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-1ffmd3f"&&(Q.innerHTML=rt),xe=s(e),h(Y.$$.fragment,e),Ne=s(e),L=m(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-a05sd1"&&(L.textContent=it),Ce=s(e),h(q.$$.fragment,e),je=s(e),A=m(e,"P",{"data-svelte-h":!0}),p(A)!=="svelte-108fh4w"&&(A.textContent=lt),ke=s(e),D=m(e,"UL",{"data-svelte-h":!0}),p(D)!=="svelte-1eb1mj4"&&(D.innerHTML=mt),Ze=s(e),O=m(e,"P",{"data-svelte-h":!0}),p(O)!=="svelte-1xesile"&&(O.textContent=ct),Se=s(e),h(K.$$.fragment,e),Fe=s(e),U=m(e,"DIV",{class:!0});var N=fe(U);h(ee.$$.fragment,N),ze=s(N),se=m(N,"P",{"data-svelte-h":!0}),p(se)!=="svelte-cvpeyo"&&(se.innerHTML=pt),Ie=s(N),re=m(N,"P",{"data-svelte-h":!0}),p(re)!=="svelte-1ek1ss9"&&(re.innerHTML=dt),Xe=s(N),h(F.$$.fragment,N),N.forEach(n),Je=s(e),h(te.$$.fragment,e),We=s(e),v=m(e,"DIV",{class:!0});var $=fe(v);h(ne.$$.fragment,$),Be=s($),ie=m($,"P",{"data-svelte-h":!0}),p(ie)!=="svelte-ohcumq"&&(ie.textContent=ft),He=s($),le=m($,"P",{"data-svelte-h":!0}),p(le)!=="svelte-q52n56"&&(le.innerHTML=gt),Ee=s($),me=m($,"P",{"data-svelte-h":!0}),p(me)!=="svelte-hswkmf"&&(me.innerHTML=ut),Qe=s($),x=m($,"DIV",{class:!0});var C=fe(x);h(oe.$$.fragment,C),Ye=s(C),ce=m(C,"P",{"data-svelte-h":!0}),p(ce)!=="svelte-1v0foys"&&(ce.innerHTML=ht),Le=s(C),h(J.$$.fragment,C),qe=s(C),h(W.$$.fragment,C),C.forEach(n),$.forEach(n),Pe=s(e),h(ae.$$.fragment,e),Re=s(e),de=m(e,"P",{}),fe(de).forEach(n),this.h()},h(){j(r,"name","hf:doc:metadata"),j(r,"content",kt),j(Z,"class","flex flex-wrap space-x-1"),yt(S.src,nt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/upernet_architecture.jpg")||j(S,"src",nt),j(S,"alt","drawing"),j(S,"width","600"),j(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){f(document.head,r),o(e,w,t),o(e,d,t),o(e,c,t),o(e,g,t),o(e,T,t),b(P,e,t),o(e,ge,t),o(e,Z,t),o(e,ue,t),b(R,e,t),o(e,he,t),o(e,V,t),o(e,be,t),o(e,G,t),o(e,_e,t),o(e,z,t),o(e,ye,t),o(e,S,t),o(e,Me,t),o(e,I,t),o(e,ve,t),o(e,X,t),o(e,we,t),b(B,e,t),o(e,Ue,t),o(e,H,t),o(e,$e,t),b(E,e,t),o(e,Te,t),o(e,Q,t),o(e,xe,t),b(Y,e,t),o(e,Ne,t),o(e,L,t),o(e,Ce,t),b(q,e,t),o(e,je,t),o(e,A,t),o(e,ke,t),o(e,D,t),o(e,Ze,t),o(e,O,t),o(e,Se,t),b(K,e,t),o(e,Fe,t),o(e,U,t),b(ee,U,null),f(U,ze),f(U,se),f(U,Ie),f(U,re),f(U,Xe),b(F,U,null),o(e,Je,t),b(te,e,t),o(e,We,t),o(e,v,t),b(ne,v,null),f(v,Be),f(v,ie),f(v,He),f(v,le),f(v,Ee),f(v,me),f(v,Qe),f(v,x),b(oe,x,null),f(x,Ye),f(x,ce),f(x,Le),b(J,x,null),f(x,qe),b(W,x,null),o(e,Pe,t),b(ae,e,t),o(e,Re,t),o(e,de,t),Ve=!0},p(e,[t]){const N={};t&2&&(N.$$scope={dirty:t,ctx:e}),F.$set(N);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),J.$set($);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),W.$set(C)},i(e){Ve||(_(P.$$.fragment,e),_(R.$$.fragment,e),_(B.$$.fragment,e),_(E.$$.fragment,e),_(Y.$$.fragment,e),_(q.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(F.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(J.$$.fragment,e),_(W.$$.fragment,e),_(ae.$$.fragment,e),Ve=!0)},o(e){y(P.$$.fragment,e),y(R.$$.fragment,e),y(B.$$.fragment,e),y(E.$$.fragment,e),y(Y.$$.fragment,e),y(q.$$.fragment,e),y(K.$$.fragment,e),y(ee.$$.fragment,e),y(F.$$.fragment,e),y(te.$$.fragment,e),y(ne.$$.fragment,e),y(oe.$$.fragment,e),y(J.$$.fragment,e),y(W.$$.fragment,e),y(ae.$$.fragment,e),Ve=!1},d(e){e&&(n(w),n(d),n(c),n(g),n(T),n(ge),n(Z),n(ue),n(he),n(V),n(be),n(G),n(_e),n(z),n(ye),n(S),n(Me),n(I),n(ve),n(X),n(we),n(Ue),n(H),n($e),n(Te),n(Q),n(xe),n(Ne),n(L),n(Ce),n(je),n(A),n(ke),n(D),n(Ze),n(O),n(Se),n(Fe),n(U),n(Je),n(We),n(v),n(Pe),n(Re),n(de)),n(r),M(P,e),M(R,e),M(B,e),M(E,e),M(Y,e),M(q,e),M(K,e),M(ee),M(F),M(te,e),M(ne),M(oe),M(J),M(W),M(ae,e)}}}const kt='{"title":"UPerNet","local":"upernet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage examples","local":"usage-examples","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"UperNetConfig","local":"transformers.UperNetConfig","sections":[],"depth":2},{"title":"UperNetForSemanticSegmentation","local":"transformers.UperNetForSemanticSegmentation","sections":[],"depth":2}],"depth":1}';function Zt(k){return Mt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gt extends vt{constructor(r){super(),wt(this,r,Zt,jt,_t,{})}}export{Gt as component};
