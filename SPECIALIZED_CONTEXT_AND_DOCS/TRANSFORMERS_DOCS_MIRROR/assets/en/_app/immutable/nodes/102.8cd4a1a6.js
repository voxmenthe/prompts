import{s as Ga,z as Qa,o as Xa,n as k}from"../chunks/scheduler.18a86fab.js";import{S as La,i as Ea,g as c,s as r,r as g,A as Ha,h as p,f as l,c as i,j as $,x as M,u as h,k as B,y as a,a as m,v as u,d as f,t as _,w as b}from"../chunks/index.98837b22.js";import{T as Ge}from"../chunks/Tip.77304350.js";import{D as x}from"../chunks/Docstring.a1ef7999.js";import{C as G}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as R}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as N,E as Sa}from"../chunks/getInferenceSnippets.06c2775f.js";function Ya(w){let t,y="Example:",n,d,T;return d=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBCbGlwMlZpc2lvbkNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEJsaXAyUUZvcm1lckNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyME9QVENvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEJsaXAyQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwQmxpcDJGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJsaXAyQ29uZmlnJTIwd2l0aCUyMFNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCbGlwMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJsaXAyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEJsaXAyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQmxpcDJDb25maWclMjBmcm9tJTIwYSUyMEJsaXAyVmlzaW9uQ29uZmlnJTJDJTIwQmxpcDJRRm9ybWVyQ29uZmlnJTIwYW5kJTIwYW55JTIwUHJldHJhaW5lZENvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMEJMSVAtMiUyMHZpc2lvbiUyQyUyMEJMSVAtMiUyMFEtRm9ybWVyJTIwYW5kJTIwbGFuZ3VhZ2UlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb25zJTBBdmlzaW9uX2NvbmZpZyUyMCUzRCUyMEJsaXAyVmlzaW9uQ29uZmlnKCklMEFxZm9ybWVyX2NvbmZpZyUyMCUzRCUyMEJsaXAyUUZvcm1lckNvbmZpZygpJTBBdGV4dF9jb25maWclMjAlM0QlMjBPUFRDb25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMEJsaXAyQ29uZmlnLmZyb21fdGV4dF92aXNpb25fY29uZmlncyh2aXNpb25fY29uZmlnJTJDJTIwcWZvcm1lcl9jb25maWclMkMlMjB0ZXh0X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Blip2VisionConfig,
<span class="hljs-meta">... </span>    Blip2QFormerConfig,
<span class="hljs-meta">... </span>    OPTConfig,
<span class="hljs-meta">... </span>    Blip2Config,
<span class="hljs-meta">... </span>    Blip2ForConditionalGeneration,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = Blip2VisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>qformer_config = Blip2QFormerConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = OPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function Aa(w){let t,y="Example:",n,d,T;return d=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJsaXAyVmlzaW9uQ29uZmlnJTJDJTIwQmxpcDJWaXNpb25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCbGlwMlZpc2lvbkNvbmZpZyUyMHdpdGglMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmxpcDJWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCbGlwMlZpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEJsaXAyVmlzaW9uTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2VisionConfig, Blip2VisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2VisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2VisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function Da(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJsaXAyUUZvcm1lckNvbmZpZyUyQyUyMEJsaXAyUUZvcm1lck1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJMSVAtMiUyMFNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCbGlwMlFGb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBCbGlwMlFGb3JtZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2QFormerConfig, Blip2QFormerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2QFormerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2QFormerModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function Oa(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function Ka(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function er(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function tr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQmxpcDJQcm9jZXNzb3IlMkMlMjBCbGlwMk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEElMEFkZXZpY2UlMjAlM0QlMjAlMjJjdWRhJTIyJTIwaWYlMjB0b3JjaC5jdWRhLmlzX2F2YWlsYWJsZSgpJTIwZWxzZSUyMCUyMmNwdSUyMiUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEJsaXAyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIpJTBBbW9kZWwlMjAlM0QlMjBCbGlwMk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBbW9kZWwudG8oZGV2aWNlKSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJRdWVzdGlvbiUzQSUyMGhvdyUyMG1hbnklMjBjYXRzJTIwYXJlJTIwdGhlcmUlM0YlMjBBbnN3ZXIlM0ElMjIlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjB0ZXh0JTNEcHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlJTJDJTIwdG9yY2guZmxvYXQxNiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, dtype=torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Question: how many cats are there? Answer:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function or(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEJsaXAyTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEJsaXAyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF0ZXh0X2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X3RleHRfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function nr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwQmxpcDJNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFpbWFnZV9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2V0X2ltYWdlX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_outputs = model.get_image_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function sr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBCbGlwMlByb2Nlc3NvciUyQyUyMEJsaXAyTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCbGlwMlByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXFmb3JtZXJfb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdldF9xZm9ybWVyX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>qformer_outputs = model.get_qformer_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function ar(w){let t,y="Note that Flan-T5 checkpoints cannot be cast to float16. They are pre-trained using bfloat16.";return{c(){t=c("p"),t.textContent=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-1cxmmi2"&&(t.textContent=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function rr(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function ir(w){let t,y;return t=new G({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQmxpcDJQcm9jZXNzb3IlMkMlMjBCbGlwMkZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCbGlwMlByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUlMkMlMjBkZXZpY2VfbWFwJTNEJTdCJTIyJTIyJTNBJTIwMCU3RCUyQyUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCSUdOT1JFX1JFU1VMVCUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2ForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, device_map={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-number">0</span>}, dtype=torch.float16
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,d){u(t,n,d),y=!0},p:k,i(n){y||(f(t.$$.fragment,n),y=!0)},o(n){_(t.$$.fragment,n),y=!1},d(n){b(t,n)}}}function lr(w){let t,y="Image captioning (without providing a text prompt):",n,d,T;return d=new G({props:{code:"aW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlJTJDJTIwdG9yY2guZmxvYXQxNiklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVELnN0cmlwKCklMEFwcmludChnZW5lcmF0ZWRfdGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two cats laying on a couch`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-1xz2grm"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function dr(w){let t,y="Visual question answering (prompt = question):",n,d,T;return d=new G({props:{code:"cHJvbXB0JTIwJTNEJTIwJTIyUXVlc3Rpb24lM0ElMjBob3clMjBtYW55JTIwY2F0cyUyMGFyZSUyMHRoZXJlJTNGJTIwQW5zd2VyJTNBJTIyJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwdGV4dCUzRHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSUzRCUyMmN1ZGElMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RC5zdHJpcCgpJTBBcHJpbnQoZ2VuZXJhdGVkX3RleHQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Question: how many cats are there? Answer:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device=<span class="hljs-string">&quot;cuda&quot;</span>, dtype=torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-dkukz2"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function cr(w){let t,y;return t=new G({props:{code:"bW9kZWwlMjAlM0QlMjBCbGlwMkZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyJTJDJTIwbG9hZF9pbl84Yml0JTNEVHJ1ZSUyQyUyMGRldmljZV9tYXAlM0QlN0IlMjIlMjIlM0ElMjAwJTdEJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCSUdOT1JFX1JFU1VMVCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHRleHQlM0Rwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UlM0QlMjJjdWRhJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVELnN0cmlwKCklMEFwcmludChnZW5lcmF0ZWRfdGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, device_map={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-number">0</span>}, dtype=torch.bfloat16
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device=<span class="hljs-string">&quot;cuda&quot;</span>, dtype=torch.bfloat16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){h(t.$$.fragment,n)},m(n,d){u(t,n,d),y=!0},p:k,i(n){y||(f(t.$$.fragment,n),y=!0)},o(n){_(t.$$.fragment,n),y=!1},d(n){b(t,n)}}}function pr(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function mr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Blip2ForImageTextRetrieval

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForImageTextRetrieval.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>, dtype=torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;two cats laying on a pink blanket&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>itm_out = model(**inputs, use_image_text_matching_head=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = torch.nn.functional.softmax(itm_out.logits_per_image, dim=<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{probs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.1</span>%}</span> that image 0 is not &#x27;<span class="hljs-subst">{text}</span>&#x27;&quot;</span>)
<span class="hljs-number">26.9</span>% that image <span class="hljs-number">0</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&#x27;two cats laying on a pink blanket&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{probs[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]:<span class="hljs-number">.1</span>%}</span> that image 0 is &#x27;<span class="hljs-subst">{text}</span>&#x27;&quot;</span>)
<span class="hljs-number">73.0</span>% that image <span class="hljs-number">0</span> <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;two cats laying on a pink blanket&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>itc_out = model(**inputs, use_image_text_matching_head=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = itc_out.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{probs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]:<span class="hljs-number">.1</span>%}</span> that image 0 is &#x27;<span class="hljs-subst">{texts[<span class="hljs-number">0</span>]}</span>&#x27;&quot;</span>)
<span class="hljs-number">55.3</span>% that image <span class="hljs-number">0</span> <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;a photo of a cat&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{probs[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]:<span class="hljs-number">.1</span>%}</span> that image 0 is &#x27;<span class="hljs-subst">{texts[<span class="hljs-number">1</span>]}</span>&#x27;&quot;</span>)
<span class="hljs-number">44.7</span>% that image <span class="hljs-number">0</span> <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;a photo of a dog&#x27;</span>`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function gr(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function hr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEJsaXAyVGV4dE1vZGVsV2l0aFByb2plY3Rpb24lMEElMEFkZXZpY2UlMjAlM0QlMjAlMjJjdWRhJTIyJTIwaWYlMjB0b3JjaC5jdWRhLmlzX2F2YWlsYWJsZSgpJTIwZWxzZSUyMCUyMmNwdSUyMiUwQSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJUZXh0TW9kZWxXaXRoUHJvamVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyU2FsZXNmb3JjZSUyRmJsaXAyLWl0bS12aXQtZyUyMiUyQyUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSklMEElMEFtb2RlbC50byhkZXZpY2UpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLWl0bS12aXQtZyUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBdGV4dF9lbWJlZHMlMjAlM0QlMjBvdXRwdXRzLnRleHRfZW1iZWRzJTBBcHJpbnQodGV4dF9lbWJlZHMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Blip2TextModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2TextModelWithProjection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>, dtype=torch.float16
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeds = outputs.text_embeds
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(text_embeds.shape)
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">256</span>])`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function ur(w){let t,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=y},l(n){t=p(n,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=y)},m(n,d){m(n,t,d)},p:k,d(n){n&&l(t)}}}function fr(w){let t,y="Examples:",n,d,T;return d=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwQmxpcDJWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItaXRtLXZpdC1nJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJTYWxlc2ZvcmNlJTJGYmxpcDItaXRtLXZpdC1nJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTBBKSUwQW1vZGVsLnRvKGRldmljZSklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlJTJDJTIwdG9yY2guZmxvYXQxNiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBaW1hZ2VfZW1iZWRzJTIwJTNEJTIwb3V0cHV0cy5pbWFnZV9lbWJlZHMlMEFwcmludChpbWFnZV9lbWJlZHMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Blip2VisionModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2VisionModelWithProjection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-itm-vit-g&quot;</span>, dtype=torch.float16
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_embeds = outputs.image_embeds
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(image_embeds.shape)
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">256</span>])`,wrap:!1}}),{c(){t=c("p"),t.textContent=y,n=r(),g(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=y),n=i(o),h(d.$$.fragment,o)},m(o,v){m(o,t,v),m(o,n,v),u(d,o,v),T=!0},p:k,i(o){T||(f(d.$$.fragment,o),T=!0)},o(o){_(d.$$.fragment,o),T=!1},d(o){o&&(l(t),l(n)),b(d,o)}}}function _r(w){let t,y,n,d,T,o="<em>This model was released on 2023-01-30 and added to Hugging Face Transformers on 2023-02-09.</em>",v,Qe,Wo,me,Gs='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Zo,Xe,Fo,Le,Qs=`The BLIP-2 model was proposed in <a href="https://huggingface.co/papers/2301.12597" rel="nofollow">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> by
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer
encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon <a href="https://huggingface.co/papers/2204.14198" rel="nofollow">Flamingo</a>, an 80 billion parameter model, by 8.7%
on zero-shot VQAv2 with 54x fewer trainable parameters.`,zo,Ee,Xs="The abstract from the paper is the following:",Po,He,Ls="<em>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the modelâ€™s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</em>",qo,ge,Es,Vo,Se,Hs='BLIP-2 architecture. Taken from the <a href="https://huggingface.co/papers/2301.12597">original paper.</a>',No,Ye,Ss=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207" rel="nofollow">here</a>.`,Ro,Ae,Go,De,Ys='<li>BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, itâ€™s recommended to use the <code>generate</code> method.</li> <li>One can use <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> to prepare images for the model, and decode the predicted tokens IDâ€™s back to text.</li>',Qo,Oe,As=`<p>[!NOTE]
BLIP models after release v4.46 will raise warnings about adding <code>processor.num_query_tokens = {{num_query_tokens}}</code> and expand model embeddings layer to add special <code>&lt;image&gt;</code> token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many <code>&lt;image&gt;</code> placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.
The attributes can be obtained from model config, as <code>model.config.num_query_tokens</code> and model embeddings expansion can be done by following <a href="https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042" rel="nofollow">this link</a>.</p>`,Xo,Ke,Lo,et,Ds="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BLIP-2.",Eo,tt,Os='<li>Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA) and chat-like conversations can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2" rel="nofollow">here</a>.</li>',Ho,ot,Ks="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",So,nt,Yo,Z,st,Mn,Qt,ea=`<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a>. It is
used to instantiate a BLIP-2 model according to the specified arguments, defining the vision model, Q-Former model
and language model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the BLIP-2 <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture.`,Tn,Xt,ta=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,vn,he,wn,ue,at,Bn,Lt,oa=`Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> (or a derived class) from a BLIP-2 vision model, Q-Former and language model
configurations.`,Ao,rt,Do,Q,it,$n,Et,na=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionModel">Blip2VisionModel</a>. It is used to instantiate a
BLIP-2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration defaults will yield a similar configuration to that of the BLIP-2
<a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture.`,Jn,Ht,sa=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,jn,fe,Oo,lt,Ko,X,dt,xn,St,aa=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a>. It is used to instantiate a
BLIP-2 Querying Transformer (Q-Former) model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the BLIP-2
<a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture. Configuration objects
inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the documentation from
<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,kn,Yt,ra='Note that <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a> is very similar to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> with interleaved cross-attention.',Cn,_e,en,ct,tn,te,pt,In,At,ia="Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.",Un,Dt,la=`<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipProcessor">BlipProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See the docstring
of <code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,on,mt,nn,F,gt,Wn,Ot,da="The bare Blip 2 Model outputting raw hidden-states without any specific head on top.",Zn,Kt,ca=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Fn,eo,pa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,zn,ie,ht,Pn,to,ma='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionModel">Blip2VisionModel</a> forward method, overrides the <code>__call__</code> special method.',qn,be,sn,ut,an,z,ft,Vn,oo,ga="BLIP-2 Querying Transformer (Q-Former).",Nn,no,ha=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Rn,so,ua=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Gn,le,_t,Qn,ao,fa='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a> forward method, overrides the <code>__call__</code> special method.',Xn,ye,rn,bt,ln,C,yt,Ln,ro,_a=`BLIP-2 Model for generating text and image features. The model consists of a vision encoder, Querying Transformer
(Q-Former) and a language model.`,En,io,ba=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Hn,lo,ya=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Sn,D,Mt,Yn,co,Ma='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> forward method, overrides the <code>__call__</code> special method.',An,Me,Dn,Te,On,ve,Tt,Kn,we,es,Be,vt,ts,$e,os,Je,wt,ns,je,dn,Bt,cn,I,$t,ss,po,Ta=`BLIP-2 Model for generating text given an image and an optional text prompt. The model consists of a vision
encoder, Querying Transformer (Q-Former) and a language model.`,as,mo,va=`One can optionally pass <code>input_ids</code> to the model, which serve as a text prompt, to make the language model continue
the prompt. Otherwise, the language model starts generating text from the [BOS] (beginning-of-sequence) token.`,rs,xe,is,go,wa=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ls,ho,Ba=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ds,J,Jt,cs,uo,$a='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',ps,ke,ms,fo,Ja="Examples:",gs,_o,ja="Prepare processor, model and image input",hs,Ce,us,Ie,fs,Ue,_s,bo,xa=`Note that int8 inference is also supported through <a href="https://github.com/TimDettmers/bitsandbytes" rel="nofollow">bitsandbytes</a>.
This greatly reduces the amount of memory used by the model while maintaining the same performance.`,bs,We,ys,Ze,jt,Ms,yo,ka="Overrides <code>generate</code> function to be able to use the model as a conditional generator.",pn,xt,mn,P,kt,Ts,Mo,Ca=`BLIP-2 Model with a vision and text projector, and a classification head on top. The model is used in the context
of image-text retrieval. Given an image and a text, the model returns the probability of the text being relevant to
the image.`,vs,To,Ia=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ws,vo,Ua=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Bs,O,Ct,$s,wo,Wa='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval">Blip2ForImageTextRetrieval</a> forward method, overrides the <code>__call__</code> special method.',Js,Fe,js,ze,gn,It,hn,q,Ut,xs,Bo,Za="The Blip 2 Model with a projection layer on top (a linear layer on top of the pooled output).",ks,$o,Fa=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Cs,Jo,za=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Is,K,Wt,Us,jo,Pa='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2TextModelWithProjection">Blip2TextModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',Ws,Pe,Zs,qe,un,Zt,fn,V,Ft,Fs,xo,qa="The Blip 2 Model with a projection layer on top (a linear layer on top of the pooled output).",zs,ko,Va=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ps,Co,Na=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qs,ee,zt,Vs,Io,Ra='The <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionModelWithProjection">Blip2VisionModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',Ns,Ve,Rs,Ne,_n,Pt,bn,Uo,yn;return Qe=new N({props:{title:"BLIP-2",local:"blip-2",headingTag:"h1"}}),Xe=new N({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ae=new N({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Ke=new N({props:{title:"Resources",local:"resources",headingTag:"h2"}}),nt=new N({props:{title:"Blip2Config",local:"transformers.Blip2Config",headingTag:"h2"}}),st=new x({props:{name:"class transformers.Blip2Config",anchor:"transformers.Blip2Config",parameters:[{name:"vision_config",val:" = None"},{name:"qformer_config",val:" = None"},{name:"text_config",val:" = None"},{name:"num_query_tokens",val:" = 32"},{name:"image_text_hidden_size",val:" = 256"},{name:"image_token_index",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2Config.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionConfig">Blip2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.Blip2Config.qformer_config",description:`<strong>qformer_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a>.`,name:"qformer_config"},{anchor:"transformers.Blip2Config.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize any <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>.`,name:"text_config"},{anchor:"transformers.Blip2Config.num_query_tokens",description:`<strong>num_query_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of query tokens passed through the Transformer.`,name:"num_query_tokens"},{anchor:"transformers.Blip2Config.image_text_hidden_size",description:`<strong>image_text_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden state of the image-text fusion layer.`,name:"image_text_hidden_size"},{anchor:"transformers.Blip2Config.image_token_index",description:`<strong>image_token_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Token index of special image token.`,name:"image_token_index"},{anchor:"transformers.Blip2Config.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/configuration_blip_2.py#L218"}}),he=new R({props:{anchor:"transformers.Blip2Config.example",$$slots:{default:[Ya]},$$scope:{ctx:w}}}),at=new x({props:{name:"from_vision_qformer_text_configs",anchor:"transformers.Blip2Config.from_vision_qformer_text_configs",parameters:[{name:"vision_config",val:": Blip2VisionConfig"},{name:"qformer_config",val:": Blip2QFormerConfig"},{name:"text_config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2Config.from_vision_qformer_text_configs.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionConfig">Blip2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.Blip2Config.from_vision_qformer_text_configs.qformer_config",description:`<strong>qformer_config</strong> (<code>dict</code>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a>.`,name:"qformer_config"},{anchor:"transformers.Blip2Config.from_vision_qformer_text_configs.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize any <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>.`,name:"text_config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/configuration_blip_2.py#L318",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a></p>
`}}),rt=new N({props:{title:"Blip2VisionConfig",local:"transformers.Blip2VisionConfig",headingTag:"h2"}}),it=new x({props:{name:"class transformers.Blip2VisionConfig",anchor:"transformers.Blip2VisionConfig",parameters:[{name:"hidden_size",val:" = 1408"},{name:"intermediate_size",val:" = 6144"},{name:"num_hidden_layers",val:" = 39"},{name:"num_attention_heads",val:" = 16"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 14"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 1e-10"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2VisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Blip2VisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 6144) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.Blip2VisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 39) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Blip2VisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Blip2VisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Blip2VisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Blip2VisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>, defaults
to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.Blip2VisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Blip2VisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Blip2VisionConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries and values in the self-attention layers.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/configuration_blip_2.py#L28"}}),fe=new R({props:{anchor:"transformers.Blip2VisionConfig.example",$$slots:{default:[Aa]},$$scope:{ctx:w}}}),lt=new N({props:{title:"Blip2QFormerConfig",local:"transformers.Blip2QFormerConfig",headingTag:"h2"}}),dt=new x({props:{name:"class transformers.Blip2QFormerConfig",anchor:"transformers.Blip2QFormerConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"cross_attention_frequency",val:" = 2"},{name:"encoder_hidden_size",val:" = 1408"},{name:"use_qformer_text_input",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2QFormerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Q-Former model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling the model.`,name:"vocab_size"},{anchor:"transformers.Blip2QFormerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Blip2QFormerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Blip2QFormerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Blip2QFormerConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.Blip2QFormerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Blip2QFormerConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.Blip2QFormerConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Blip2QFormerConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.Blip2QFormerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Blip2QFormerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Blip2QFormerConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Index to be used for padding token.`,name:"pad_token_id"},{anchor:"transformers.Blip2QFormerConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.Blip2QFormerConfig.cross_attention_frequency",description:`<strong>cross_attention_frequency</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The frequency of adding cross-attention to the Transformer layers.`,name:"cross_attention_frequency"},{anchor:"transformers.Blip2QFormerConfig.encoder_hidden_size",description:`<strong>encoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
The hidden size of the hidden states for cross-attention.`,name:"encoder_hidden_size"},{anchor:"transformers.Blip2QFormerConfig.use_qformer_text_input",description:`<strong>use_qformer_text_input</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use BERT-style embeddings.`,name:"use_qformer_text_input"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/configuration_blip_2.py#L110"}}),_e=new R({props:{anchor:"transformers.Blip2QFormerConfig.example",$$slots:{default:[Da]},$$scope:{ctx:w}}}),ct=new N({props:{title:"Blip2Processor",local:"transformers.Blip2Processor",headingTag:"h2"}}),pt=new x({props:{name:"class transformers.Blip2Processor",anchor:"transformers.Blip2Processor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""},{name:"num_query_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2Processor.image_processor",description:`<strong>image_processor</strong> (<code>BlipImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.Blip2Processor.tokenizer",description:"<strong>tokenizer</strong> (<code>AutoTokenizer</code>) &#x2014;\nAn instance of [&#x2018;PreTrainedTokenizer`]. The tokenizer is a required input.",name:"tokenizer"},{anchor:"transformers.Blip2Processor.num_query_tokens",description:`<strong>num_query_tokens</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of tokens used by the Qformer as queries, should be same as in model&#x2019;s config.`,name:"num_query_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/processing_blip_2.py#L48"}}),mt=new N({props:{title:"Blip2VisionModel",local:"transformers.Blip2VisionModel",headingTag:"h2"}}),gt=new x({props:{name:"class transformers.Blip2VisionModel",anchor:"transformers.Blip2VisionModel",parameters:[{name:"config",val:": Blip2VisionConfig"}],parametersDescription:[{anchor:"transformers.Blip2VisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionConfig">Blip2VisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L535"}}),ht=new x({props:{name:"forward",anchor:"transformers.Blip2VisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.Blip2VisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2VisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2VisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2VisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Blip2VisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L550",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),be=new Ge({props:{$$slots:{default:[Oa]},$$scope:{ctx:w}}}),ut=new N({props:{title:"Blip2QFormerModel",local:"transformers.Blip2QFormerModel",headingTag:"h2"}}),ft=new x({props:{name:"class transformers.Blip2QFormerModel",anchor:"transformers.Blip2QFormerModel",parameters:[{name:"config",val:": Blip2QFormerConfig"}],parametersDescription:[{anchor:"transformers.Blip2QFormerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1036"}}),_t=new x({props:{name:"forward",anchor:"transformers.Blip2QFormerModel.forward",parameters:[{name:"query_embeds",val:": FloatTensor"},{name:"query_length",val:": typing.Optional[int] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Blip2QFormerModel.forward.query_embeds",description:`<strong>query_embeds</strong> (<code>torch.FloatTensor</code>  of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Hidden states to be used in the attention computation. If cross-attention,
will be used for the query (i.e., key and value will use the encoder_hidden_states).`,name:"query_embeds"},{anchor:"transformers.Blip2QFormerModel.forward.query_length",description:`<strong>query_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Length of the query, usually based on the number of query tokens.
If no value is provided, query_length will be inferred by the query_embeds.`,name:"query_length"},{anchor:"transformers.Blip2QFormerModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2QFormerModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Blip2QFormerModel.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.Blip2QFormerModel.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.Blip2QFormerModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2QFormerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2QFormerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1110",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoderâ€™s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ye=new Ge({props:{$$slots:{default:[Ka]},$$scope:{ctx:w}}}),bt=new N({props:{title:"Blip2Model",local:"transformers.Blip2Model",headingTag:"h2"}}),yt=new x({props:{name:"class transformers.Blip2Model",anchor:"transformers.Blip2Model",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1220"}}),Mt=new x({props:{name:"forward",anchor:"transformers.Blip2Model.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": FloatTensor"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Blip2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2Model.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2Model.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.Blip2Model.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Blip2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Blip2Model.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1473",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, defaults to <code>None</code>) â€” Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, defaults to <code>None</code>) â€” Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) â€” Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new Ge({props:{$$slots:{default:[er]},$$scope:{ctx:w}}}),Te=new R({props:{anchor:"transformers.Blip2Model.forward.example",$$slots:{default:[tr]},$$scope:{ctx:w}}}),Tt=new x({props:{name:"get_text_features",anchor:"transformers.Blip2Model.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2Model.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2Model.get_text_features.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>T5 uses the <code>pad_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).</p>
<p>To know more on how to prepare <code>decoder_input_ids</code> for pretraining take a look at <a href="./t5#training">T5
Training</a>.`,name:"decoder_input_ids"},{anchor:"transformers.Blip2Model.get_text_features.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2Model.get_text_features.labels",description:`<strong>labels</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Blip2Model.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1272",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The language model outputs. If <code>return_dict=True</code>, the output is a <code>CausalLMOutputWithPast</code> that
contains the language model logits, the past key values and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_outputs (<code>CausalLMOutputWithPast</code>, or <code>tuple(torch.FloatTensor)</code> if <code>return_dict=False</code>)</p>
`}}),we=new R({props:{anchor:"transformers.Blip2Model.get_text_features.example",$$slots:{default:[or]},$$scope:{ctx:w}}}),vt=new x({props:{name:"get_image_features",anchor:"transformers.Blip2Model.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2Model.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Blip2Model.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1351",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The vision model outputs. If <code>return_dict=True</code>, the output is a <code>BaseModelOutputWithPooling</code> that
contains the image features, the pooled image features and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>vision_outputs (<code>BaseModelOutputWithPooling</code> or tuple of <code>torch.FloatTensor</code>)</p>
`}}),$e=new R({props:{anchor:"transformers.Blip2Model.get_image_features.example",$$slots:{default:[nr]},$$scope:{ctx:w}}}),wt=new x({props:{name:"get_qformer_features",anchor:"transformers.Blip2Model.get_qformer_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_qformer_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2Model.get_qformer_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_qformer_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_qformer_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Blip2Model.get_qformer_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The vision model outputs. If <code>return_dict=True</code>, the output is a <code>BaseModelOutputWithPooling</code> that
contains the image features, the pooled image features and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>vision_outputs (<code>BaseModelOutputWithPooling</code> or tuple of <code>torch.FloatTensor</code>)</p>
`}}),je=new R({props:{anchor:"transformers.Blip2Model.get_qformer_features.example",$$slots:{default:[sr]},$$scope:{ctx:w}}}),Bt=new N({props:{title:"Blip2ForConditionalGeneration",local:"transformers.Blip2ForConditionalGeneration",headingTag:"h2"}}),$t=new x({props:{name:"class transformers.Blip2ForConditionalGeneration",anchor:"transformers.Blip2ForConditionalGeneration",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1830"}}),xe=new Ge({props:{$$slots:{default:[ar]},$$scope:{ctx:w}}}),Jt=new x({props:{name:"forward",anchor:"transformers.Blip2ForConditionalGeneration.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1963",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) â€” Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, defaults to <code>None</code>) â€” Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, defaults to <code>None</code>) â€” Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) â€” Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ke=new Ge({props:{$$slots:{default:[rr]},$$scope:{ctx:w}}}),Ce=new R({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example",$$slots:{default:[ir]},$$scope:{ctx:w}}}),Ie=new R({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-2",$$slots:{default:[lr]},$$scope:{ctx:w}}}),Ue=new R({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-3",$$slots:{default:[dr]},$$scope:{ctx:w}}}),We=new R({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-4",$$slots:{default:[cr]},$$scope:{ctx:w}}}),jt=new x({props:{name:"generate",anchor:"transformers.Blip2ForConditionalGeneration.generate",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"**generate_kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.generate.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape (batch_size, num_channels, height, width)) &#x2014;
Input images to be processed.`,name:"pixel_values"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices`,name:"attention_mask"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Embedded representation of the inputs. Should be float, not int tokens.`,name:"inputs_embeds"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the positional encoding of the image embeddings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L2126",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of strings of length batch_size * num_captions.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>captions (list)</p>
`}}),xt=new N({props:{title:"Blip2ForImageTextRetrieval",local:"transformers.Blip2ForImageTextRetrieval",headingTag:"h2"}}),kt=new x({props:{name:"class transformers.Blip2ForImageTextRetrieval",anchor:"transformers.Blip2ForImageTextRetrieval",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2ForImageTextRetrieval.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L2220"}}),Ct=new x({props:{name:"forward",anchor:"transformers.Blip2ForImageTextRetrieval.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_image_text_matching_head",val:": typing.Optional[bool] = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Blip2ForImageTextRetrieval.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.use_image_text_matching_head",description:`<strong>use_image_text_matching_head</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the Image-Text Matching or Contrastive scores.`,name:"use_image_text_matching_head"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2ForImageTextRetrieval.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L2253",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2ImageTextMatchingModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) â€” Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) â€” The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) â€” The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The text embeddings obtained by applying the projection layer to the pooled output.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The image embeddings obtained by applying the projection layer to the pooled output.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2QFormerModel"
>Blip2QFormerModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2VisionModel"
>Blip2VisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2ImageTextMatchingModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Fe=new Ge({props:{$$slots:{default:[pr]},$$scope:{ctx:w}}}),ze=new R({props:{anchor:"transformers.Blip2ForImageTextRetrieval.forward.example",$$slots:{default:[mr]},$$scope:{ctx:w}}}),It=new N({props:{title:"Blip2TextModelWithProjection",local:"transformers.Blip2TextModelWithProjection",headingTag:"h2"}}),Ut=new x({props:{name:"class transformers.Blip2TextModelWithProjection",anchor:"transformers.Blip2TextModelWithProjection",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2TextModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1622"}}),Wt=new x({props:{name:"forward",anchor:"transformers.Blip2TextModelWithProjection.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Blip2TextModelWithProjection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2TextModelWithProjection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2TextModelWithProjection.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.Blip2TextModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2TextModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2TextModelWithProjection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1646",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2TextModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) â€” The text embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2TextModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Pe=new Ge({props:{$$slots:{default:[gr]},$$scope:{ctx:w}}}),qe=new R({props:{anchor:"transformers.Blip2TextModelWithProjection.forward.example",$$slots:{default:[hr]},$$scope:{ctx:w}}}),Zt=new N({props:{title:"Blip2VisionModelWithProjection",local:"transformers.Blip2VisionModelWithProjection",headingTag:"h2"}}),Ft=new x({props:{name:"class transformers.Blip2VisionModelWithProjection",anchor:"transformers.Blip2VisionModelWithProjection",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2VisionModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1715"}}),zt=new x({props:{name:"forward",anchor:"transformers.Blip2VisionModelWithProjection.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.Blip2VisionModelWithProjection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">BlipImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.Blip2VisionModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2VisionModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2VisionModelWithProjection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/blip_2/modeling_blip_2.py#L1737",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2VisionModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) â€” The image embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2VisionModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ve=new Ge({props:{$$slots:{default:[ur]},$$scope:{ctx:w}}}),Ne=new R({props:{anchor:"transformers.Blip2VisionModelWithProjection.forward.example",$$slots:{default:[fr]},$$scope:{ctx:w}}}),Pt=new Sa({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/blip-2.md"}}),{c(){t=c("meta"),y=r(),n=c("p"),d=r(),T=c("p"),T.innerHTML=o,v=r(),g(Qe.$$.fragment),Wo=r(),me=c("div"),me.innerHTML=Gs,Zo=r(),g(Xe.$$.fragment),Fo=r(),Le=c("p"),Le.innerHTML=Qs,zo=r(),Ee=c("p"),Ee.textContent=Xs,Po=r(),He=c("p"),He.innerHTML=Ls,qo=r(),ge=c("img"),Vo=r(),Se=c("small"),Se.innerHTML=Hs,No=r(),Ye=c("p"),Ye.innerHTML=Ss,Ro=r(),g(Ae.$$.fragment),Go=r(),De=c("ul"),De.innerHTML=Ys,Qo=r(),Oe=c("blockquote"),Oe.innerHTML=As,Xo=r(),g(Ke.$$.fragment),Lo=r(),et=c("p"),et.textContent=Ds,Eo=r(),tt=c("ul"),tt.innerHTML=Os,Ho=r(),ot=c("p"),ot.textContent=Ks,So=r(),g(nt.$$.fragment),Yo=r(),Z=c("div"),g(st.$$.fragment),Mn=r(),Qt=c("p"),Qt.innerHTML=ea,Tn=r(),Xt=c("p"),Xt.innerHTML=ta,vn=r(),g(he.$$.fragment),wn=r(),ue=c("div"),g(at.$$.fragment),Bn=r(),Lt=c("p"),Lt.innerHTML=oa,Ao=r(),g(rt.$$.fragment),Do=r(),Q=c("div"),g(it.$$.fragment),$n=r(),Et=c("p"),Et.innerHTML=na,Jn=r(),Ht=c("p"),Ht.innerHTML=sa,jn=r(),g(fe.$$.fragment),Oo=r(),g(lt.$$.fragment),Ko=r(),X=c("div"),g(dt.$$.fragment),xn=r(),St=c("p"),St.innerHTML=aa,kn=r(),Yt=c("p"),Yt.innerHTML=ra,Cn=r(),g(_e.$$.fragment),en=r(),g(ct.$$.fragment),tn=r(),te=c("div"),g(pt.$$.fragment),In=r(),At=c("p"),At.textContent=ia,Un=r(),Dt=c("p"),Dt.innerHTML=la,on=r(),g(mt.$$.fragment),nn=r(),F=c("div"),g(gt.$$.fragment),Wn=r(),Ot=c("p"),Ot.textContent=da,Zn=r(),Kt=c("p"),Kt.innerHTML=ca,Fn=r(),eo=c("p"),eo.innerHTML=pa,zn=r(),ie=c("div"),g(ht.$$.fragment),Pn=r(),to=c("p"),to.innerHTML=ma,qn=r(),g(be.$$.fragment),sn=r(),g(ut.$$.fragment),an=r(),z=c("div"),g(ft.$$.fragment),Vn=r(),oo=c("p"),oo.textContent=ga,Nn=r(),no=c("p"),no.innerHTML=ha,Rn=r(),so=c("p"),so.innerHTML=ua,Gn=r(),le=c("div"),g(_t.$$.fragment),Qn=r(),ao=c("p"),ao.innerHTML=fa,Xn=r(),g(ye.$$.fragment),rn=r(),g(bt.$$.fragment),ln=r(),C=c("div"),g(yt.$$.fragment),Ln=r(),ro=c("p"),ro.textContent=_a,En=r(),io=c("p"),io.innerHTML=ba,Hn=r(),lo=c("p"),lo.innerHTML=ya,Sn=r(),D=c("div"),g(Mt.$$.fragment),Yn=r(),co=c("p"),co.innerHTML=Ma,An=r(),g(Me.$$.fragment),Dn=r(),g(Te.$$.fragment),On=r(),ve=c("div"),g(Tt.$$.fragment),Kn=r(),g(we.$$.fragment),es=r(),Be=c("div"),g(vt.$$.fragment),ts=r(),g($e.$$.fragment),os=r(),Je=c("div"),g(wt.$$.fragment),ns=r(),g(je.$$.fragment),dn=r(),g(Bt.$$.fragment),cn=r(),I=c("div"),g($t.$$.fragment),ss=r(),po=c("p"),po.textContent=Ta,as=r(),mo=c("p"),mo.innerHTML=va,rs=r(),g(xe.$$.fragment),is=r(),go=c("p"),go.innerHTML=wa,ls=r(),ho=c("p"),ho.innerHTML=Ba,ds=r(),J=c("div"),g(Jt.$$.fragment),cs=r(),uo=c("p"),uo.innerHTML=$a,ps=r(),g(ke.$$.fragment),ms=r(),fo=c("p"),fo.textContent=Ja,gs=r(),_o=c("p"),_o.textContent=ja,hs=r(),g(Ce.$$.fragment),us=r(),g(Ie.$$.fragment),fs=r(),g(Ue.$$.fragment),_s=r(),bo=c("p"),bo.innerHTML=xa,bs=r(),g(We.$$.fragment),ys=r(),Ze=c("div"),g(jt.$$.fragment),Ms=r(),yo=c("p"),yo.innerHTML=ka,pn=r(),g(xt.$$.fragment),mn=r(),P=c("div"),g(kt.$$.fragment),Ts=r(),Mo=c("p"),Mo.textContent=Ca,vs=r(),To=c("p"),To.innerHTML=Ia,ws=r(),vo=c("p"),vo.innerHTML=Ua,Bs=r(),O=c("div"),g(Ct.$$.fragment),$s=r(),wo=c("p"),wo.innerHTML=Wa,Js=r(),g(Fe.$$.fragment),js=r(),g(ze.$$.fragment),gn=r(),g(It.$$.fragment),hn=r(),q=c("div"),g(Ut.$$.fragment),xs=r(),Bo=c("p"),Bo.textContent=Za,ks=r(),$o=c("p"),$o.innerHTML=Fa,Cs=r(),Jo=c("p"),Jo.innerHTML=za,Is=r(),K=c("div"),g(Wt.$$.fragment),Us=r(),jo=c("p"),jo.innerHTML=Pa,Ws=r(),g(Pe.$$.fragment),Zs=r(),g(qe.$$.fragment),un=r(),g(Zt.$$.fragment),fn=r(),V=c("div"),g(Ft.$$.fragment),Fs=r(),xo=c("p"),xo.textContent=qa,zs=r(),ko=c("p"),ko.innerHTML=Va,Ps=r(),Co=c("p"),Co.innerHTML=Na,qs=r(),ee=c("div"),g(zt.$$.fragment),Vs=r(),Io=c("p"),Io.innerHTML=Ra,Ns=r(),g(Ve.$$.fragment),Rs=r(),g(Ne.$$.fragment),_n=r(),g(Pt.$$.fragment),bn=r(),Uo=c("p"),this.h()},l(e){const s=Ha("svelte-u9bgzb",document.head);t=p(s,"META",{name:!0,content:!0}),s.forEach(l),y=i(e),n=p(e,"P",{}),$(n).forEach(l),d=i(e),T=p(e,"P",{"data-svelte-h":!0}),M(T)!=="svelte-153uk5x"&&(T.innerHTML=o),v=i(e),h(Qe.$$.fragment,e),Wo=i(e),me=p(e,"DIV",{class:!0,"data-svelte-h":!0}),M(me)!=="svelte-13t8s2t"&&(me.innerHTML=Gs),Zo=i(e),h(Xe.$$.fragment,e),Fo=i(e),Le=p(e,"P",{"data-svelte-h":!0}),M(Le)!=="svelte-1ow0gfc"&&(Le.innerHTML=Qs),zo=i(e),Ee=p(e,"P",{"data-svelte-h":!0}),M(Ee)!=="svelte-vfdo9a"&&(Ee.textContent=Xs),Po=i(e),He=p(e,"P",{"data-svelte-h":!0}),M(He)!=="svelte-k65nu1"&&(He.innerHTML=Ls),qo=i(e),ge=p(e,"IMG",{src:!0,alt:!0,width:!0}),Vo=i(e),Se=p(e,"SMALL",{"data-svelte-h":!0}),M(Se)!=="svelte-1f4fpbh"&&(Se.innerHTML=Hs),No=i(e),Ye=p(e,"P",{"data-svelte-h":!0}),M(Ye)!=="svelte-1xpociz"&&(Ye.innerHTML=Ss),Ro=i(e),h(Ae.$$.fragment,e),Go=i(e),De=p(e,"UL",{"data-svelte-h":!0}),M(De)!=="svelte-mzjdpm"&&(De.innerHTML=Ys),Qo=i(e),Oe=p(e,"BLOCKQUOTE",{"data-svelte-h":!0}),M(Oe)!=="svelte-8y5dv0"&&(Oe.innerHTML=As),Xo=i(e),h(Ke.$$.fragment,e),Lo=i(e),et=p(e,"P",{"data-svelte-h":!0}),M(et)!=="svelte-11jcle7"&&(et.textContent=Ds),Eo=i(e),tt=p(e,"UL",{"data-svelte-h":!0}),M(tt)!=="svelte-1tw7src"&&(tt.innerHTML=Os),Ho=i(e),ot=p(e,"P",{"data-svelte-h":!0}),M(ot)!=="svelte-1xesile"&&(ot.textContent=Ks),So=i(e),h(nt.$$.fragment,e),Yo=i(e),Z=p(e,"DIV",{class:!0});var L=$(Z);h(st.$$.fragment,L),Mn=i(L),Qt=p(L,"P",{"data-svelte-h":!0}),M(Qt)!=="svelte-vzpuq2"&&(Qt.innerHTML=ea),Tn=i(L),Xt=p(L,"P",{"data-svelte-h":!0}),M(Xt)!=="svelte-1ek1ss9"&&(Xt.innerHTML=ta),vn=i(L),h(he.$$.fragment,L),wn=i(L),ue=p(L,"DIV",{class:!0});var qt=$(ue);h(at.$$.fragment,qt),Bn=i(qt),Lt=p(qt,"P",{"data-svelte-h":!0}),M(Lt)!=="svelte-1qfka0m"&&(Lt.innerHTML=oa),qt.forEach(l),L.forEach(l),Ao=i(e),h(rt.$$.fragment,e),Do=i(e),Q=p(e,"DIV",{class:!0});var oe=$(Q);h(it.$$.fragment,oe),$n=i(oe),Et=p(oe,"P",{"data-svelte-h":!0}),M(Et)!=="svelte-vmhlyt"&&(Et.innerHTML=na),Jn=i(oe),Ht=p(oe,"P",{"data-svelte-h":!0}),M(Ht)!=="svelte-1ek1ss9"&&(Ht.innerHTML=sa),jn=i(oe),h(fe.$$.fragment,oe),oe.forEach(l),Oo=i(e),h(lt.$$.fragment,e),Ko=i(e),X=p(e,"DIV",{class:!0});var ne=$(X);h(dt.$$.fragment,ne),xn=i(ne),St=p(ne,"P",{"data-svelte-h":!0}),M(St)!=="svelte-1ywdzzf"&&(St.innerHTML=aa),kn=i(ne),Yt=p(ne,"P",{"data-svelte-h":!0}),M(Yt)!=="svelte-zth5di"&&(Yt.innerHTML=ra),Cn=i(ne),h(_e.$$.fragment,ne),ne.forEach(l),en=i(e),h(ct.$$.fragment,e),tn=i(e),te=p(e,"DIV",{class:!0});var de=$(te);h(pt.$$.fragment,de),In=i(de),At=p(de,"P",{"data-svelte-h":!0}),M(At)!=="svelte-qq98xh"&&(At.textContent=ia),Un=i(de),Dt=p(de,"P",{"data-svelte-h":!0}),M(Dt)!=="svelte-raehnz"&&(Dt.innerHTML=la),de.forEach(l),on=i(e),h(mt.$$.fragment,e),nn=i(e),F=p(e,"DIV",{class:!0});var E=$(F);h(gt.$$.fragment,E),Wn=i(E),Ot=p(E,"P",{"data-svelte-h":!0}),M(Ot)!=="svelte-1urjpp5"&&(Ot.textContent=da),Zn=i(E),Kt=p(E,"P",{"data-svelte-h":!0}),M(Kt)!=="svelte-q52n56"&&(Kt.innerHTML=ca),Fn=i(E),eo=p(E,"P",{"data-svelte-h":!0}),M(eo)!=="svelte-hswkmf"&&(eo.innerHTML=pa),zn=i(E),ie=p(E,"DIV",{class:!0});var ce=$(ie);h(ht.$$.fragment,ce),Pn=i(ce),to=p(ce,"P",{"data-svelte-h":!0}),M(to)!=="svelte-1emivcz"&&(to.innerHTML=ma),qn=i(ce),h(be.$$.fragment,ce),ce.forEach(l),E.forEach(l),sn=i(e),h(ut.$$.fragment,e),an=i(e),z=p(e,"DIV",{class:!0});var H=$(z);h(ft.$$.fragment,H),Vn=i(H),oo=p(H,"P",{"data-svelte-h":!0}),M(oo)!=="svelte-mwuqur"&&(oo.textContent=ga),Nn=i(H),no=p(H,"P",{"data-svelte-h":!0}),M(no)!=="svelte-q52n56"&&(no.innerHTML=ha),Rn=i(H),so=p(H,"P",{"data-svelte-h":!0}),M(so)!=="svelte-hswkmf"&&(so.innerHTML=ua),Gn=i(H),le=p(H,"DIV",{class:!0});var pe=$(le);h(_t.$$.fragment,pe),Qn=i(pe),ao=p(pe,"P",{"data-svelte-h":!0}),M(ao)!=="svelte-1x6hgnb"&&(ao.innerHTML=fa),Xn=i(pe),h(ye.$$.fragment,pe),pe.forEach(l),H.forEach(l),rn=i(e),h(bt.$$.fragment,e),ln=i(e),C=p(e,"DIV",{class:!0});var U=$(C);h(yt.$$.fragment,U),Ln=i(U),ro=p(U,"P",{"data-svelte-h":!0}),M(ro)!=="svelte-1ib14ae"&&(ro.textContent=_a),En=i(U),io=p(U,"P",{"data-svelte-h":!0}),M(io)!=="svelte-q52n56"&&(io.innerHTML=ba),Hn=i(U),lo=p(U,"P",{"data-svelte-h":!0}),M(lo)!=="svelte-hswkmf"&&(lo.innerHTML=ya),Sn=i(U),D=p(U,"DIV",{class:!0});var se=$(D);h(Mt.$$.fragment,se),Yn=i(se),co=p(se,"P",{"data-svelte-h":!0}),M(co)!=="svelte-1l99fkj"&&(co.innerHTML=Ma),An=i(se),h(Me.$$.fragment,se),Dn=i(se),h(Te.$$.fragment,se),se.forEach(l),On=i(U),ve=p(U,"DIV",{class:!0});var Vt=$(ve);h(Tt.$$.fragment,Vt),Kn=i(Vt),h(we.$$.fragment,Vt),Vt.forEach(l),es=i(U),Be=p(U,"DIV",{class:!0});var Nt=$(Be);h(vt.$$.fragment,Nt),ts=i(Nt),h($e.$$.fragment,Nt),Nt.forEach(l),os=i(U),Je=p(U,"DIV",{class:!0});var Rt=$(Je);h(wt.$$.fragment,Rt),ns=i(Rt),h(je.$$.fragment,Rt),Rt.forEach(l),U.forEach(l),dn=i(e),h(Bt.$$.fragment,e),cn=i(e),I=p(e,"DIV",{class:!0});var W=$(I);h($t.$$.fragment,W),ss=i(W),po=p(W,"P",{"data-svelte-h":!0}),M(po)!=="svelte-1fsmwyj"&&(po.textContent=Ta),as=i(W),mo=p(W,"P",{"data-svelte-h":!0}),M(mo)!=="svelte-1ks26sg"&&(mo.innerHTML=va),rs=i(W),h(xe.$$.fragment,W),is=i(W),go=p(W,"P",{"data-svelte-h":!0}),M(go)!=="svelte-q52n56"&&(go.innerHTML=wa),ls=i(W),ho=p(W,"P",{"data-svelte-h":!0}),M(ho)!=="svelte-hswkmf"&&(ho.innerHTML=Ba),ds=i(W),J=p(W,"DIV",{class:!0});var j=$(J);h(Jt.$$.fragment,j),cs=i(j),uo=p(j,"P",{"data-svelte-h":!0}),M(uo)!=="svelte-mmbhwv"&&(uo.innerHTML=$a),ps=i(j),h(ke.$$.fragment,j),ms=i(j),fo=p(j,"P",{"data-svelte-h":!0}),M(fo)!=="svelte-kvfsh7"&&(fo.textContent=Ja),gs=i(j),_o=p(j,"P",{"data-svelte-h":!0}),M(_o)!=="svelte-1c4jfk4"&&(_o.textContent=ja),hs=i(j),h(Ce.$$.fragment,j),us=i(j),h(Ie.$$.fragment,j),fs=i(j),h(Ue.$$.fragment,j),_s=i(j),bo=p(j,"P",{"data-svelte-h":!0}),M(bo)!=="svelte-gy1q6u"&&(bo.innerHTML=xa),bs=i(j),h(We.$$.fragment,j),j.forEach(l),ys=i(W),Ze=p(W,"DIV",{class:!0});var Gt=$(Ze);h(jt.$$.fragment,Gt),Ms=i(Gt),yo=p(Gt,"P",{"data-svelte-h":!0}),M(yo)!=="svelte-eq620n"&&(yo.innerHTML=ka),Gt.forEach(l),W.forEach(l),pn=i(e),h(xt.$$.fragment,e),mn=i(e),P=p(e,"DIV",{class:!0});var S=$(P);h(kt.$$.fragment,S),Ts=i(S),Mo=p(S,"P",{"data-svelte-h":!0}),M(Mo)!=="svelte-15b9m1j"&&(Mo.textContent=Ca),vs=i(S),To=p(S,"P",{"data-svelte-h":!0}),M(To)!=="svelte-q52n56"&&(To.innerHTML=Ia),ws=i(S),vo=p(S,"P",{"data-svelte-h":!0}),M(vo)!=="svelte-hswkmf"&&(vo.innerHTML=Ua),Bs=i(S),O=p(S,"DIV",{class:!0});var ae=$(O);h(Ct.$$.fragment,ae),$s=i(ae),wo=p(ae,"P",{"data-svelte-h":!0}),M(wo)!=="svelte-170bjyz"&&(wo.innerHTML=Wa),Js=i(ae),h(Fe.$$.fragment,ae),js=i(ae),h(ze.$$.fragment,ae),ae.forEach(l),S.forEach(l),gn=i(e),h(It.$$.fragment,e),hn=i(e),q=p(e,"DIV",{class:!0});var Y=$(q);h(Ut.$$.fragment,Y),xs=i(Y),Bo=p(Y,"P",{"data-svelte-h":!0}),M(Bo)!=="svelte-1jjx1cw"&&(Bo.textContent=Za),ks=i(Y),$o=p(Y,"P",{"data-svelte-h":!0}),M($o)!=="svelte-q52n56"&&($o.innerHTML=Fa),Cs=i(Y),Jo=p(Y,"P",{"data-svelte-h":!0}),M(Jo)!=="svelte-hswkmf"&&(Jo.innerHTML=za),Is=i(Y),K=p(Y,"DIV",{class:!0});var re=$(K);h(Wt.$$.fragment,re),Us=i(re),jo=p(re,"P",{"data-svelte-h":!0}),M(jo)!=="svelte-1yxtp57"&&(jo.innerHTML=Pa),Ws=i(re),h(Pe.$$.fragment,re),Zs=i(re),h(qe.$$.fragment,re),re.forEach(l),Y.forEach(l),un=i(e),h(Zt.$$.fragment,e),fn=i(e),V=p(e,"DIV",{class:!0});var A=$(V);h(Ft.$$.fragment,A),Fs=i(A),xo=p(A,"P",{"data-svelte-h":!0}),M(xo)!=="svelte-1jjx1cw"&&(xo.textContent=qa),zs=i(A),ko=p(A,"P",{"data-svelte-h":!0}),M(ko)!=="svelte-q52n56"&&(ko.innerHTML=Va),Ps=i(A),Co=p(A,"P",{"data-svelte-h":!0}),M(Co)!=="svelte-hswkmf"&&(Co.innerHTML=Na),qs=i(A),ee=p(A,"DIV",{class:!0});var Re=$(ee);h(zt.$$.fragment,Re),Vs=i(Re),Io=p(Re,"P",{"data-svelte-h":!0}),M(Io)!=="svelte-1u96gd5"&&(Io.innerHTML=Ra),Ns=i(Re),h(Ve.$$.fragment,Re),Rs=i(Re),h(Ne.$$.fragment,Re),Re.forEach(l),A.forEach(l),_n=i(e),h(Pt.$$.fragment,e),bn=i(e),Uo=p(e,"P",{}),$(Uo).forEach(l),this.h()},h(){B(t,"name","hf:doc:metadata"),B(t,"content",br),B(me,"class","flex flex-wrap space-x-1"),Qa(ge.src,Es="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg")||B(ge,"src",Es),B(ge,"alt","drawing"),B(ge,"width","600"),B(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){a(document.head,t),m(e,y,s),m(e,n,s),m(e,d,s),m(e,T,s),m(e,v,s),u(Qe,e,s),m(e,Wo,s),m(e,me,s),m(e,Zo,s),u(Xe,e,s),m(e,Fo,s),m(e,Le,s),m(e,zo,s),m(e,Ee,s),m(e,Po,s),m(e,He,s),m(e,qo,s),m(e,ge,s),m(e,Vo,s),m(e,Se,s),m(e,No,s),m(e,Ye,s),m(e,Ro,s),u(Ae,e,s),m(e,Go,s),m(e,De,s),m(e,Qo,s),m(e,Oe,s),m(e,Xo,s),u(Ke,e,s),m(e,Lo,s),m(e,et,s),m(e,Eo,s),m(e,tt,s),m(e,Ho,s),m(e,ot,s),m(e,So,s),u(nt,e,s),m(e,Yo,s),m(e,Z,s),u(st,Z,null),a(Z,Mn),a(Z,Qt),a(Z,Tn),a(Z,Xt),a(Z,vn),u(he,Z,null),a(Z,wn),a(Z,ue),u(at,ue,null),a(ue,Bn),a(ue,Lt),m(e,Ao,s),u(rt,e,s),m(e,Do,s),m(e,Q,s),u(it,Q,null),a(Q,$n),a(Q,Et),a(Q,Jn),a(Q,Ht),a(Q,jn),u(fe,Q,null),m(e,Oo,s),u(lt,e,s),m(e,Ko,s),m(e,X,s),u(dt,X,null),a(X,xn),a(X,St),a(X,kn),a(X,Yt),a(X,Cn),u(_e,X,null),m(e,en,s),u(ct,e,s),m(e,tn,s),m(e,te,s),u(pt,te,null),a(te,In),a(te,At),a(te,Un),a(te,Dt),m(e,on,s),u(mt,e,s),m(e,nn,s),m(e,F,s),u(gt,F,null),a(F,Wn),a(F,Ot),a(F,Zn),a(F,Kt),a(F,Fn),a(F,eo),a(F,zn),a(F,ie),u(ht,ie,null),a(ie,Pn),a(ie,to),a(ie,qn),u(be,ie,null),m(e,sn,s),u(ut,e,s),m(e,an,s),m(e,z,s),u(ft,z,null),a(z,Vn),a(z,oo),a(z,Nn),a(z,no),a(z,Rn),a(z,so),a(z,Gn),a(z,le),u(_t,le,null),a(le,Qn),a(le,ao),a(le,Xn),u(ye,le,null),m(e,rn,s),u(bt,e,s),m(e,ln,s),m(e,C,s),u(yt,C,null),a(C,Ln),a(C,ro),a(C,En),a(C,io),a(C,Hn),a(C,lo),a(C,Sn),a(C,D),u(Mt,D,null),a(D,Yn),a(D,co),a(D,An),u(Me,D,null),a(D,Dn),u(Te,D,null),a(C,On),a(C,ve),u(Tt,ve,null),a(ve,Kn),u(we,ve,null),a(C,es),a(C,Be),u(vt,Be,null),a(Be,ts),u($e,Be,null),a(C,os),a(C,Je),u(wt,Je,null),a(Je,ns),u(je,Je,null),m(e,dn,s),u(Bt,e,s),m(e,cn,s),m(e,I,s),u($t,I,null),a(I,ss),a(I,po),a(I,as),a(I,mo),a(I,rs),u(xe,I,null),a(I,is),a(I,go),a(I,ls),a(I,ho),a(I,ds),a(I,J),u(Jt,J,null),a(J,cs),a(J,uo),a(J,ps),u(ke,J,null),a(J,ms),a(J,fo),a(J,gs),a(J,_o),a(J,hs),u(Ce,J,null),a(J,us),u(Ie,J,null),a(J,fs),u(Ue,J,null),a(J,_s),a(J,bo),a(J,bs),u(We,J,null),a(I,ys),a(I,Ze),u(jt,Ze,null),a(Ze,Ms),a(Ze,yo),m(e,pn,s),u(xt,e,s),m(e,mn,s),m(e,P,s),u(kt,P,null),a(P,Ts),a(P,Mo),a(P,vs),a(P,To),a(P,ws),a(P,vo),a(P,Bs),a(P,O),u(Ct,O,null),a(O,$s),a(O,wo),a(O,Js),u(Fe,O,null),a(O,js),u(ze,O,null),m(e,gn,s),u(It,e,s),m(e,hn,s),m(e,q,s),u(Ut,q,null),a(q,xs),a(q,Bo),a(q,ks),a(q,$o),a(q,Cs),a(q,Jo),a(q,Is),a(q,K),u(Wt,K,null),a(K,Us),a(K,jo),a(K,Ws),u(Pe,K,null),a(K,Zs),u(qe,K,null),m(e,un,s),u(Zt,e,s),m(e,fn,s),m(e,V,s),u(Ft,V,null),a(V,Fs),a(V,xo),a(V,zs),a(V,ko),a(V,Ps),a(V,Co),a(V,qs),a(V,ee),u(zt,ee,null),a(ee,Vs),a(ee,Io),a(ee,Ns),u(Ve,ee,null),a(ee,Rs),u(Ne,ee,null),m(e,_n,s),u(Pt,e,s),m(e,bn,s),m(e,Uo,s),yn=!0},p(e,[s]){const L={};s&2&&(L.$$scope={dirty:s,ctx:e}),he.$set(L);const qt={};s&2&&(qt.$$scope={dirty:s,ctx:e}),fe.$set(qt);const oe={};s&2&&(oe.$$scope={dirty:s,ctx:e}),_e.$set(oe);const ne={};s&2&&(ne.$$scope={dirty:s,ctx:e}),be.$set(ne);const de={};s&2&&(de.$$scope={dirty:s,ctx:e}),ye.$set(de);const E={};s&2&&(E.$$scope={dirty:s,ctx:e}),Me.$set(E);const ce={};s&2&&(ce.$$scope={dirty:s,ctx:e}),Te.$set(ce);const H={};s&2&&(H.$$scope={dirty:s,ctx:e}),we.$set(H);const pe={};s&2&&(pe.$$scope={dirty:s,ctx:e}),$e.$set(pe);const U={};s&2&&(U.$$scope={dirty:s,ctx:e}),je.$set(U);const se={};s&2&&(se.$$scope={dirty:s,ctx:e}),xe.$set(se);const Vt={};s&2&&(Vt.$$scope={dirty:s,ctx:e}),ke.$set(Vt);const Nt={};s&2&&(Nt.$$scope={dirty:s,ctx:e}),Ce.$set(Nt);const Rt={};s&2&&(Rt.$$scope={dirty:s,ctx:e}),Ie.$set(Rt);const W={};s&2&&(W.$$scope={dirty:s,ctx:e}),Ue.$set(W);const j={};s&2&&(j.$$scope={dirty:s,ctx:e}),We.$set(j);const Gt={};s&2&&(Gt.$$scope={dirty:s,ctx:e}),Fe.$set(Gt);const S={};s&2&&(S.$$scope={dirty:s,ctx:e}),ze.$set(S);const ae={};s&2&&(ae.$$scope={dirty:s,ctx:e}),Pe.$set(ae);const Y={};s&2&&(Y.$$scope={dirty:s,ctx:e}),qe.$set(Y);const re={};s&2&&(re.$$scope={dirty:s,ctx:e}),Ve.$set(re);const A={};s&2&&(A.$$scope={dirty:s,ctx:e}),Ne.$set(A)},i(e){yn||(f(Qe.$$.fragment,e),f(Xe.$$.fragment,e),f(Ae.$$.fragment,e),f(Ke.$$.fragment,e),f(nt.$$.fragment,e),f(st.$$.fragment,e),f(he.$$.fragment,e),f(at.$$.fragment,e),f(rt.$$.fragment,e),f(it.$$.fragment,e),f(fe.$$.fragment,e),f(lt.$$.fragment,e),f(dt.$$.fragment,e),f(_e.$$.fragment,e),f(ct.$$.fragment,e),f(pt.$$.fragment,e),f(mt.$$.fragment,e),f(gt.$$.fragment,e),f(ht.$$.fragment,e),f(be.$$.fragment,e),f(ut.$$.fragment,e),f(ft.$$.fragment,e),f(_t.$$.fragment,e),f(ye.$$.fragment,e),f(bt.$$.fragment,e),f(yt.$$.fragment,e),f(Mt.$$.fragment,e),f(Me.$$.fragment,e),f(Te.$$.fragment,e),f(Tt.$$.fragment,e),f(we.$$.fragment,e),f(vt.$$.fragment,e),f($e.$$.fragment,e),f(wt.$$.fragment,e),f(je.$$.fragment,e),f(Bt.$$.fragment,e),f($t.$$.fragment,e),f(xe.$$.fragment,e),f(Jt.$$.fragment,e),f(ke.$$.fragment,e),f(Ce.$$.fragment,e),f(Ie.$$.fragment,e),f(Ue.$$.fragment,e),f(We.$$.fragment,e),f(jt.$$.fragment,e),f(xt.$$.fragment,e),f(kt.$$.fragment,e),f(Ct.$$.fragment,e),f(Fe.$$.fragment,e),f(ze.$$.fragment,e),f(It.$$.fragment,e),f(Ut.$$.fragment,e),f(Wt.$$.fragment,e),f(Pe.$$.fragment,e),f(qe.$$.fragment,e),f(Zt.$$.fragment,e),f(Ft.$$.fragment,e),f(zt.$$.fragment,e),f(Ve.$$.fragment,e),f(Ne.$$.fragment,e),f(Pt.$$.fragment,e),yn=!0)},o(e){_(Qe.$$.fragment,e),_(Xe.$$.fragment,e),_(Ae.$$.fragment,e),_(Ke.$$.fragment,e),_(nt.$$.fragment,e),_(st.$$.fragment,e),_(he.$$.fragment,e),_(at.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(fe.$$.fragment,e),_(lt.$$.fragment,e),_(dt.$$.fragment,e),_(_e.$$.fragment,e),_(ct.$$.fragment,e),_(pt.$$.fragment,e),_(mt.$$.fragment,e),_(gt.$$.fragment,e),_(ht.$$.fragment,e),_(be.$$.fragment,e),_(ut.$$.fragment,e),_(ft.$$.fragment,e),_(_t.$$.fragment,e),_(ye.$$.fragment,e),_(bt.$$.fragment,e),_(yt.$$.fragment,e),_(Mt.$$.fragment,e),_(Me.$$.fragment,e),_(Te.$$.fragment,e),_(Tt.$$.fragment,e),_(we.$$.fragment,e),_(vt.$$.fragment,e),_($e.$$.fragment,e),_(wt.$$.fragment,e),_(je.$$.fragment,e),_(Bt.$$.fragment,e),_($t.$$.fragment,e),_(xe.$$.fragment,e),_(Jt.$$.fragment,e),_(ke.$$.fragment,e),_(Ce.$$.fragment,e),_(Ie.$$.fragment,e),_(Ue.$$.fragment,e),_(We.$$.fragment,e),_(jt.$$.fragment,e),_(xt.$$.fragment,e),_(kt.$$.fragment,e),_(Ct.$$.fragment,e),_(Fe.$$.fragment,e),_(ze.$$.fragment,e),_(It.$$.fragment,e),_(Ut.$$.fragment,e),_(Wt.$$.fragment,e),_(Pe.$$.fragment,e),_(qe.$$.fragment,e),_(Zt.$$.fragment,e),_(Ft.$$.fragment,e),_(zt.$$.fragment,e),_(Ve.$$.fragment,e),_(Ne.$$.fragment,e),_(Pt.$$.fragment,e),yn=!1},d(e){e&&(l(y),l(n),l(d),l(T),l(v),l(Wo),l(me),l(Zo),l(Fo),l(Le),l(zo),l(Ee),l(Po),l(He),l(qo),l(ge),l(Vo),l(Se),l(No),l(Ye),l(Ro),l(Go),l(De),l(Qo),l(Oe),l(Xo),l(Lo),l(et),l(Eo),l(tt),l(Ho),l(ot),l(So),l(Yo),l(Z),l(Ao),l(Do),l(Q),l(Oo),l(Ko),l(X),l(en),l(tn),l(te),l(on),l(nn),l(F),l(sn),l(an),l(z),l(rn),l(ln),l(C),l(dn),l(cn),l(I),l(pn),l(mn),l(P),l(gn),l(hn),l(q),l(un),l(fn),l(V),l(_n),l(bn),l(Uo)),l(t),b(Qe,e),b(Xe,e),b(Ae,e),b(Ke,e),b(nt,e),b(st),b(he),b(at),b(rt,e),b(it),b(fe),b(lt,e),b(dt),b(_e),b(ct,e),b(pt),b(mt,e),b(gt),b(ht),b(be),b(ut,e),b(ft),b(_t),b(ye),b(bt,e),b(yt),b(Mt),b(Me),b(Te),b(Tt),b(we),b(vt),b($e),b(wt),b(je),b(Bt,e),b($t),b(xe),b(Jt),b(ke),b(Ce),b(Ie),b(Ue),b(We),b(jt),b(xt,e),b(kt),b(Ct),b(Fe),b(ze),b(It,e),b(Ut),b(Wt),b(Pe),b(qe),b(Zt,e),b(Ft),b(zt),b(Ve),b(Ne),b(Pt,e)}}}const br='{"title":"BLIP-2","local":"blip-2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Blip2Config","local":"transformers.Blip2Config","sections":[],"depth":2},{"title":"Blip2VisionConfig","local":"transformers.Blip2VisionConfig","sections":[],"depth":2},{"title":"Blip2QFormerConfig","local":"transformers.Blip2QFormerConfig","sections":[],"depth":2},{"title":"Blip2Processor","local":"transformers.Blip2Processor","sections":[],"depth":2},{"title":"Blip2VisionModel","local":"transformers.Blip2VisionModel","sections":[],"depth":2},{"title":"Blip2QFormerModel","local":"transformers.Blip2QFormerModel","sections":[],"depth":2},{"title":"Blip2Model","local":"transformers.Blip2Model","sections":[],"depth":2},{"title":"Blip2ForConditionalGeneration","local":"transformers.Blip2ForConditionalGeneration","sections":[],"depth":2},{"title":"Blip2ForImageTextRetrieval","local":"transformers.Blip2ForImageTextRetrieval","sections":[],"depth":2},{"title":"Blip2TextModelWithProjection","local":"transformers.Blip2TextModelWithProjection","sections":[],"depth":2},{"title":"Blip2VisionModelWithProjection","local":"transformers.Blip2VisionModelWithProjection","sections":[],"depth":2}],"depth":1}';function yr(w){return Xa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jr extends La{constructor(t){super(),Ea(this,t,yr,_r,Ga,{})}}export{jr as component};
