import{s as Lt,z as Pt,o as qt,n as Me}from"../chunks/scheduler.18a86fab.js";import{S as At,i as St,g as d,s as r,r as f,A as Vt,h as c,f as o,c as i,j as ue,x as p,u,k as W,y as h,a as n,v as g,d as _,t as b,w}from"../chunks/index.98837b22.js";import{T as ut}from"../chunks/Tip.77304350.js";import{D as Ce}from"../chunks/Docstring.a1ef7999.js";import{C as _t}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as gt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as Bt}from"../chunks/PipelineTag.7749150e.js";import{H as ge,E as Rt}from"../chunks/getInferenceSnippets.06c2775f.js";function Yt($){let a,y=`This model is in maintenance mode only, we donâ€™t accept any new PRs changing its code.
If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.
You can do so by running the following command: <code>pip install -U transformers==4.40.2</code>.`;return{c(){a=d("p"),a.innerHTML=y},l(l){a=c(l,"P",{"data-svelte-h":!0}),p(a)!=="svelte-1sq0hrb"&&(a.innerHTML=y)},m(l,m){n(l,a,m)},p:Me,d(l){l&&o(a)}}}function Xt($){let a,y="Example:",l,m,T;return m=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5hdENvbmZpZyUyQyUyME5hdE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME5hdCUyMHNoaS1sYWJzJTJGbmF0LW1pbmktaW4xay0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTmF0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMHNoaS1sYWJzJTJGbmF0LW1pbmktaW4xay0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME5hdE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NatConfig, NatModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Nat shi-labs/nat-mini-in1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = NatConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the shi-labs/nat-mini-in1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = NatModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){a=d("p"),a.textContent=y,l=r(),f(m.$$.fragment)},l(s){a=c(s,"P",{"data-svelte-h":!0}),p(a)!=="svelte-11lpom8"&&(a.textContent=y),l=i(s),u(m.$$.fragment,s)},m(s,v){n(s,a,v),n(s,l,v),g(m,s,v),T=!0},p:Me,i(s){T||(_(m.$$.fragment,s),T=!0)},o(s){b(m.$$.fragment,s),T=!1},d(s){s&&(o(a),o(l)),w(m,s)}}}function Qt($){let a,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=d("p"),a.innerHTML=y},l(l){a=c(l,"P",{"data-svelte-h":!0}),p(a)!=="svelte-fincs2"&&(a.innerHTML=y)},m(l,m){n(l,a,m)},p:Me,d(l){l&&o(a)}}}function Ot($){let a,y="Example:",l,m,T;return m=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME5hdE1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzaGktbGFicyUyRm5hdC1taW5pLWluMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwTmF0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnNoaS1sYWJzJTJGbmF0LW1pbmktaW4xay0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, NatModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;shi-labs/nat-mini-in1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = NatModel.from_pretrained(<span class="hljs-string">&quot;shi-labs/nat-mini-in1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>]`,wrap:!1}}),{c(){a=d("p"),a.textContent=y,l=r(),f(m.$$.fragment)},l(s){a=c(s,"P",{"data-svelte-h":!0}),p(a)!=="svelte-11lpom8"&&(a.textContent=y),l=i(s),u(m.$$.fragment,s)},m(s,v){n(s,a,v),n(s,l,v),g(m,s,v),T=!0},p:Me,i(s){T||(_(m.$$.fragment,s),T=!0)},o(s){b(m.$$.fragment,s),T=!1},d(s){s&&(o(a),o(l)),w(m,s)}}}function Dt($){let a,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=d("p"),a.innerHTML=y},l(l){a=c(l,"P",{"data-svelte-h":!0}),p(a)!=="svelte-fincs2"&&(a.innerHTML=y)},m(l,m){n(l,a,m)},p:Me,d(l){l&&o(a)}}}function Kt($){let a,y="Example:",l,m,T;return m=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME5hdEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMnNoaS1sYWJzJTJGbmF0LW1pbmktaW4xay0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBOYXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJzaGktbGFicyUyRm5hdC1taW5pLWluMWstMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, NatForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;shi-labs/nat-mini-in1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = NatForImageClassification.from_pretrained(<span class="hljs-string">&quot;shi-labs/nat-mini-in1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tiger cat`,wrap:!1}}),{c(){a=d("p"),a.textContent=y,l=r(),f(m.$$.fragment)},l(s){a=c(s,"P",{"data-svelte-h":!0}),p(a)!=="svelte-11lpom8"&&(a.textContent=y),l=i(s),u(m.$$.fragment,s)},m(s,v){n(s,a,v),n(s,l,v),g(m,s,v),T=!0},p:Me,i(s){T||(_(m.$$.fragment,s),T=!0)},o(s){b(m.$$.fragment,s),T=!1},d(s){s&&(o(a),o(l)),w(m,s)}}}function eo($){let a,y,l,m,T,s="<em>This model was released on 2022-04-14 and added to Hugging Face Transformers on 2023-06-20.</em>",v,A,xe,z,bt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',je,Z,Ie,S,Je,V,wt=`NAT was proposed in <a href="https://huggingface.co/papers/2204.07143" rel="nofollow">Neighborhood Attention Transformer</a>
by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.`,Fe,B,yt="It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.",Ue,R,Tt="The abstract from the paper is the following:",We,Y,vt=`<em>We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision.
NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a
linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NAâ€™s
receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike
Swin Transformerâ€™s Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package
with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swinâ€™s WSA while using up to 25% less
memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA
that boosts image classification and downstream vision performance. Experimental results on NAT are competitive;
NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9%
ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size.</em>`,ke,H,$t,ze,X,Mt=`Neighborhood Attention compared to other attention patterns.
Taken from the <a href="https://huggingface.co/papers/2204.07143">original paper</a>.`,Ze,Q,Nt=`This model was contributed by <a href="https://huggingface.co/alihassanijr" rel="nofollow">Ali Hassani</a>.
The original code can be found <a href="https://github.com/SHI-Labs/Neighborhood-Attention-Transformer" rel="nofollow">here</a>.`,He,O,Ee,D,Ct=`<li>One can use the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a> API to prepare images for the model.</li> <li>NAT can be used as a <em>backbone</em>. When <code>output_hidden_states = True</code>,
it will output both <code>hidden_states</code> and <code>reshaped_hidden_states</code>.
The <code>reshaped_hidden_states</code> have a shape of <code>(batch, num_channels, height, width)</code> rather than
<code>(batch_size, height, width, num_channels)</code>.</li>`,Ge,K,xt="Notes:",Le,ee,jt=`<li>NAT depends on <a href="https://github.com/SHI-Labs/NATTEN/" rel="nofollow">NATTEN</a>â€™s implementation of Neighborhood Attention.
You can install it with pre-built wheels for Linux by referring to <a href="https://shi-labs.com/natten" rel="nofollow">shi-labs.com/natten</a>,
or build on your system by running <code>pip install natten</code>.
Note that the latter will likely take time to compile. NATTEN does not support Windows devices yet.</li> <li>Patch size of 4 is only supported at the moment.</li>`,Pe,te,qe,oe,It="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with NAT.",Ae,ne,Se,ae,Jt='<li><a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',Ve,se,Ft="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Be,re,Re,M,ie,tt,_e,Ut=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatModel">NatModel</a>. It is used to instantiate a Nat model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Nat
<a href="https://huggingface.co/shi-labs/nat-mini-in1k-224" rel="nofollow">shi-labs/nat-mini-in1k-224</a> architecture.`,ot,be,Wt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,nt,E,Ye,le,Xe,j,de,at,we,kt=`The bare Nat Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,st,C,ce,rt,ye,zt='The <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatModel">NatModel</a> forward method, overrides the <code>__call__</code> special method.',it,G,lt,L,Qe,me,Oe,N,pe,dt,Te,Zt=`Nat Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
the [CLS] token) e.g. for ImageNet.`,ct,ve,Ht=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,mt,x,he,pt,$e,Et='The <a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',ht,P,ft,q,De,fe,Ke,Ne,et;return A=new ge({props:{title:"Neighborhood Attention Transformer",local:"neighborhood-attention-transformer",headingTag:"h1"}}),Z=new ut({props:{warning:!0,$$slots:{default:[Yt]},$$scope:{ctx:$}}}),S=new ge({props:{title:"Overview",local:"overview",headingTag:"h2"}}),O=new ge({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),te=new ge({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ne=new Bt({props:{pipeline:"image-classification"}}),re=new ge({props:{title:"NatConfig",local:"transformers.NatConfig",headingTag:"h2"}}),ie=new Ce({props:{name:"class transformers.NatConfig",anchor:"transformers.NatConfig",parameters:[{name:"patch_size",val:" = 4"},{name:"num_channels",val:" = 3"},{name:"embed_dim",val:" = 64"},{name:"depths",val:" = [3, 4, 6, 5]"},{name:"num_heads",val:" = [2, 4, 8, 16]"},{name:"kernel_size",val:" = 7"},{name:"mlp_ratio",val:" = 3.0"},{name:"qkv_bias",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.1"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"layer_scale_init_value",val:" = 0.0"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NatConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The size (resolution) of each patch. NOTE: Only patch size of 4 is supported at the moment.`,name:"patch_size"},{anchor:"transformers.NatConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.NatConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of patch embedding.`,name:"embed_dim"},{anchor:"transformers.NatConfig.depths",description:`<strong>depths</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 4, 6, 5]</code>) &#x2014;
Number of layers in each level of the encoder.`,name:"depths"},{anchor:"transformers.NatConfig.num_heads",description:`<strong>num_heads</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 4, 8, 16]</code>) &#x2014;
Number of attention heads in each layer of the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.NatConfig.kernel_size",description:`<strong>kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Neighborhood Attention kernel size.`,name:"kernel_size"},{anchor:"transformers.NatConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 3.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.NatConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not a learnable bias should be added to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.NatConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings and encoder.`,name:"hidden_dropout_prob"},{anchor:"transformers.NatConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.NatConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.NatConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.NatConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.NatConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.NatConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The initial value for the layer scale. Disabled if &lt;=0.`,name:"layer_scale_init_value"},{anchor:"transformers.NatConfig.out_features",description:`<strong>out_features</strong> (<code>list[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.NatConfig.out_indices",description:`<strong>out_indices</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/nat/configuration_nat.py#L25"}}),E=new gt({props:{anchor:"transformers.NatConfig.example",$$slots:{default:[Xt]},$$scope:{ctx:$}}}),le=new ge({props:{title:"NatModel",local:"transformers.NatModel",headingTag:"h2"}}),de=new Ce({props:{name:"class transformers.NatModel",anchor:"transformers.NatModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.NatModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig">NatConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/nat/modeling_nat.py#L668"}}),ce=new Ce({props:{name:"forward",anchor:"transformers.NatModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.NatModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.NatModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.NatModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.NatModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/nat/modeling_nat.py#L698",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.deprecated.nat.modeling_nat.NatModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig"
>NatConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>, <em>optional</em>, returned when <code>add_pooling_layer=True</code> is passed) â€” Average pooling of the last layer hidden-state.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.deprecated.nat.modeling_nat.NatModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new ut({props:{$$slots:{default:[Qt]},$$scope:{ctx:$}}}),L=new gt({props:{anchor:"transformers.NatModel.forward.example",$$slots:{default:[Ot]},$$scope:{ctx:$}}}),me=new ge({props:{title:"NatForImageClassification",local:"transformers.NatForImageClassification",headingTag:"h2"}}),pe=new Ce({props:{name:"class transformers.NatForImageClassification",anchor:"transformers.NatForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.NatForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig">NatConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/nat/modeling_nat.py#L760"}}),he=new Ce({props:{name:"forward",anchor:"transformers.NatForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.NatForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.NatForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.NatForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.NatForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.NatForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/nat/modeling_nat.py#L777",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.deprecated.nat.modeling_nat.NatImageClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/nat#transformers.NatConfig"
>NatConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.deprecated.nat.modeling_nat.NatImageClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),P=new ut({props:{$$slots:{default:[Dt]},$$scope:{ctx:$}}}),q=new gt({props:{anchor:"transformers.NatForImageClassification.forward.example",$$slots:{default:[Kt]},$$scope:{ctx:$}}}),fe=new Rt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nat.md"}}),{c(){a=d("meta"),y=r(),l=d("p"),m=r(),T=d("p"),T.innerHTML=s,v=r(),f(A.$$.fragment),xe=r(),z=d("div"),z.innerHTML=bt,je=r(),f(Z.$$.fragment),Ie=r(),f(S.$$.fragment),Je=r(),V=d("p"),V.innerHTML=wt,Fe=r(),B=d("p"),B.textContent=yt,Ue=r(),R=d("p"),R.textContent=Tt,We=r(),Y=d("p"),Y.innerHTML=vt,ke=r(),H=d("img"),ze=r(),X=d("small"),X.innerHTML=Mt,Ze=r(),Q=d("p"),Q.innerHTML=Nt,He=r(),f(O.$$.fragment),Ee=r(),D=d("ul"),D.innerHTML=Ct,Ge=r(),K=d("p"),K.textContent=xt,Le=r(),ee=d("ul"),ee.innerHTML=jt,Pe=r(),f(te.$$.fragment),qe=r(),oe=d("p"),oe.textContent=It,Ae=r(),f(ne.$$.fragment),Se=r(),ae=d("ul"),ae.innerHTML=Jt,Ve=r(),se=d("p"),se.textContent=Ft,Be=r(),f(re.$$.fragment),Re=r(),M=d("div"),f(ie.$$.fragment),tt=r(),_e=d("p"),_e.innerHTML=Ut,ot=r(),be=d("p"),be.innerHTML=Wt,nt=r(),f(E.$$.fragment),Ye=r(),f(le.$$.fragment),Xe=r(),j=d("div"),f(de.$$.fragment),at=r(),we=d("p"),we.innerHTML=kt,st=r(),C=d("div"),f(ce.$$.fragment),rt=r(),ye=d("p"),ye.innerHTML=zt,it=r(),f(G.$$.fragment),lt=r(),f(L.$$.fragment),Qe=r(),f(me.$$.fragment),Oe=r(),N=d("div"),f(pe.$$.fragment),dt=r(),Te=d("p"),Te.textContent=Zt,ct=r(),ve=d("p"),ve.innerHTML=Ht,mt=r(),x=d("div"),f(he.$$.fragment),pt=r(),$e=d("p"),$e.innerHTML=Et,ht=r(),f(P.$$.fragment),ft=r(),f(q.$$.fragment),De=r(),f(fe.$$.fragment),Ke=r(),Ne=d("p"),this.h()},l(e){const t=Vt("svelte-u9bgzb",document.head);a=c(t,"META",{name:!0,content:!0}),t.forEach(o),y=i(e),l=c(e,"P",{}),ue(l).forEach(o),m=i(e),T=c(e,"P",{"data-svelte-h":!0}),p(T)!=="svelte-1e9xprm"&&(T.innerHTML=s),v=i(e),u(A.$$.fragment,e),xe=i(e),z=c(e,"DIV",{class:!0,"data-svelte-h":!0}),p(z)!=="svelte-13t8s2t"&&(z.innerHTML=bt),je=i(e),u(Z.$$.fragment,e),Ie=i(e),u(S.$$.fragment,e),Je=i(e),V=c(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-miiww3"&&(V.innerHTML=wt),Fe=i(e),B=c(e,"P",{"data-svelte-h":!0}),p(B)!=="svelte-1e8auh6"&&(B.textContent=yt),Ue=i(e),R=c(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-vfdo9a"&&(R.textContent=Tt),We=i(e),Y=c(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-7imk69"&&(Y.innerHTML=vt),ke=i(e),H=c(e,"IMG",{src:!0,alt:!0,width:!0}),ze=i(e),X=c(e,"SMALL",{"data-svelte-h":!0}),p(X)!=="svelte-179twss"&&(X.innerHTML=Mt),Ze=i(e),Q=c(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-1emlw2j"&&(Q.innerHTML=Nt),He=i(e),u(O.$$.fragment,e),Ee=i(e),D=c(e,"UL",{"data-svelte-h":!0}),p(D)!=="svelte-tirson"&&(D.innerHTML=Ct),Ge=i(e),K=c(e,"P",{"data-svelte-h":!0}),p(K)!=="svelte-1biq3pv"&&(K.textContent=xt),Le=i(e),ee=c(e,"UL",{"data-svelte-h":!0}),p(ee)!=="svelte-5cm23e"&&(ee.innerHTML=jt),Pe=i(e),u(te.$$.fragment,e),qe=i(e),oe=c(e,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-nwqm6q"&&(oe.textContent=It),Ae=i(e),u(ne.$$.fragment,e),Se=i(e),ae=c(e,"UL",{"data-svelte-h":!0}),p(ae)!=="svelte-gnemog"&&(ae.innerHTML=Jt),Ve=i(e),se=c(e,"P",{"data-svelte-h":!0}),p(se)!=="svelte-1xesile"&&(se.textContent=Ft),Be=i(e),u(re.$$.fragment,e),Re=i(e),M=c(e,"DIV",{class:!0});var I=ue(M);u(ie.$$.fragment,I),tt=i(I),_e=c(I,"P",{"data-svelte-h":!0}),p(_e)!=="svelte-1qet02q"&&(_e.innerHTML=Ut),ot=i(I),be=c(I,"P",{"data-svelte-h":!0}),p(be)!=="svelte-1ek1ss9"&&(be.innerHTML=Wt),nt=i(I),u(E.$$.fragment,I),I.forEach(o),Ye=i(e),u(le.$$.fragment,e),Xe=i(e),j=c(e,"DIV",{class:!0});var k=ue(j);u(de.$$.fragment,k),at=i(k),we=c(k,"P",{"data-svelte-h":!0}),p(we)!=="svelte-dglxjv"&&(we.innerHTML=kt),st=i(k),C=c(k,"DIV",{class:!0});var J=ue(C);u(ce.$$.fragment,J),rt=i(J),ye=c(J,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-q9amq2"&&(ye.innerHTML=zt),it=i(J),u(G.$$.fragment,J),lt=i(J),u(L.$$.fragment,J),J.forEach(o),k.forEach(o),Qe=i(e),u(me.$$.fragment,e),Oe=i(e),N=c(e,"DIV",{class:!0});var F=ue(N);u(pe.$$.fragment,F),dt=i(F),Te=c(F,"P",{"data-svelte-h":!0}),p(Te)!=="svelte-100sr6y"&&(Te.textContent=Zt),ct=i(F),ve=c(F,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-68lg8f"&&(ve.innerHTML=Ht),mt=i(F),x=c(F,"DIV",{class:!0});var U=ue(x);u(he.$$.fragment,U),pt=i(U),$e=c(U,"P",{"data-svelte-h":!0}),p($e)!=="svelte-1abnms2"&&($e.innerHTML=Et),ht=i(U),u(P.$$.fragment,U),ft=i(U),u(q.$$.fragment,U),U.forEach(o),F.forEach(o),De=i(e),u(fe.$$.fragment,e),Ke=i(e),Ne=c(e,"P",{}),ue(Ne).forEach(o),this.h()},h(){W(a,"name","hf:doc:metadata"),W(a,"content",to),W(z,"class","flex flex-wrap space-x-1"),Pt(H.src,$t="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/neighborhood-attention-pattern.jpg")||W(H,"src",$t),W(H,"alt","drawing"),W(H,"width","600"),W(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){h(document.head,a),n(e,y,t),n(e,l,t),n(e,m,t),n(e,T,t),n(e,v,t),g(A,e,t),n(e,xe,t),n(e,z,t),n(e,je,t),g(Z,e,t),n(e,Ie,t),g(S,e,t),n(e,Je,t),n(e,V,t),n(e,Fe,t),n(e,B,t),n(e,Ue,t),n(e,R,t),n(e,We,t),n(e,Y,t),n(e,ke,t),n(e,H,t),n(e,ze,t),n(e,X,t),n(e,Ze,t),n(e,Q,t),n(e,He,t),g(O,e,t),n(e,Ee,t),n(e,D,t),n(e,Ge,t),n(e,K,t),n(e,Le,t),n(e,ee,t),n(e,Pe,t),g(te,e,t),n(e,qe,t),n(e,oe,t),n(e,Ae,t),g(ne,e,t),n(e,Se,t),n(e,ae,t),n(e,Ve,t),n(e,se,t),n(e,Be,t),g(re,e,t),n(e,Re,t),n(e,M,t),g(ie,M,null),h(M,tt),h(M,_e),h(M,ot),h(M,be),h(M,nt),g(E,M,null),n(e,Ye,t),g(le,e,t),n(e,Xe,t),n(e,j,t),g(de,j,null),h(j,at),h(j,we),h(j,st),h(j,C),g(ce,C,null),h(C,rt),h(C,ye),h(C,it),g(G,C,null),h(C,lt),g(L,C,null),n(e,Qe,t),g(me,e,t),n(e,Oe,t),n(e,N,t),g(pe,N,null),h(N,dt),h(N,Te),h(N,ct),h(N,ve),h(N,mt),h(N,x),g(he,x,null),h(x,pt),h(x,$e),h(x,ht),g(P,x,null),h(x,ft),g(q,x,null),n(e,De,t),g(fe,e,t),n(e,Ke,t),n(e,Ne,t),et=!0},p(e,[t]){const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),Z.$set(I);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),E.$set(k);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),G.$set(J);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),L.$set(F);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),P.$set(U);const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),q.$set(Gt)},i(e){et||(_(A.$$.fragment,e),_(Z.$$.fragment,e),_(S.$$.fragment,e),_(O.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(E.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(G.$$.fragment,e),_(L.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(P.$$.fragment,e),_(q.$$.fragment,e),_(fe.$$.fragment,e),et=!0)},o(e){b(A.$$.fragment,e),b(Z.$$.fragment,e),b(S.$$.fragment,e),b(O.$$.fragment,e),b(te.$$.fragment,e),b(ne.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(E.$$.fragment,e),b(le.$$.fragment,e),b(de.$$.fragment,e),b(ce.$$.fragment,e),b(G.$$.fragment,e),b(L.$$.fragment,e),b(me.$$.fragment,e),b(pe.$$.fragment,e),b(he.$$.fragment,e),b(P.$$.fragment,e),b(q.$$.fragment,e),b(fe.$$.fragment,e),et=!1},d(e){e&&(o(y),o(l),o(m),o(T),o(v),o(xe),o(z),o(je),o(Ie),o(Je),o(V),o(Fe),o(B),o(Ue),o(R),o(We),o(Y),o(ke),o(H),o(ze),o(X),o(Ze),o(Q),o(He),o(Ee),o(D),o(Ge),o(K),o(Le),o(ee),o(Pe),o(qe),o(oe),o(Ae),o(Se),o(ae),o(Ve),o(se),o(Be),o(Re),o(M),o(Ye),o(Xe),o(j),o(Qe),o(Oe),o(N),o(De),o(Ke),o(Ne)),o(a),w(A,e),w(Z,e),w(S,e),w(O,e),w(te,e),w(ne,e),w(re,e),w(ie),w(E),w(le,e),w(de),w(ce),w(G),w(L),w(me,e),w(pe),w(he),w(P),w(q),w(fe,e)}}}const to='{"title":"Neighborhood Attention Transformer","local":"neighborhood-attention-transformer","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"NatConfig","local":"transformers.NatConfig","sections":[],"depth":2},{"title":"NatModel","local":"transformers.NatModel","sections":[],"depth":2},{"title":"NatForImageClassification","local":"transformers.NatForImageClassification","sections":[],"depth":2}],"depth":1}';function oo($){return qt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class po extends At{constructor(a){super(),St(this,a,oo,eo,Lt,{})}}export{po as component};
