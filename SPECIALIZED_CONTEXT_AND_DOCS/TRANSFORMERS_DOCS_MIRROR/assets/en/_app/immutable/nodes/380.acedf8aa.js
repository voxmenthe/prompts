import{s as ra,o as ia,n as la}from"../chunks/scheduler.18a86fab.js";import{S as ma,i as da,g as r,s as o,r as m,A as ca,h as i,f as t,c as a,j as y,x as g,u as d,k as M,l as pa,y as s,a as l,v as c,d as p,t as f,w as _}from"../chunks/index.98837b22.js";import{D as v}from"../chunks/Docstring.a1ef7999.js";import{C as Ls}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as fa}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as U,E as _a}from"../chunks/getInferenceSnippets.06c2775f.js";function ga(Ks){let T,oe="Example:",$,x,V;return x=new Ls({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBTYW0yVmlzaW9uQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwU2FtMlByb21wdEVuY29kZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBTYW0yTWFza0RlY29kZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBTYW0yTW9kZWwlMkMlMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFNhbTJDb25maWclMjB3aXRoJTIwJTYwJTIyZmFjZWJvb2slMkZzYW0yLjFfaGllcmFfdGlueSUyMiU2MCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBTYW0yY29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwU2FtMk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjAlNjAlMjJmYWNlYm9vayUyRnNhbTIuMV9oaWVyYV90aW55JTIyJTYwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBTYW0yTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBTYW0yQ29uZmlnJTIwZnJvbSUyMGElMjBTYW0yVmlzaW9uQ29uZmlnJTJDJTIwU2FtMlByb21wdEVuY29kZXJDb25maWclMkMlMjBhbmQlMjBTYW0yTWFza0RlY29kZXJDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBTQU0yJTIwdmlzaW9uJTIwZW5jb2RlciUyQyUyMG1lbW9yeSUyMGF0dGVudGlvbiUyQyUyMGFuZCUyMG1lbW9yeSUyMGVuY29kZXIlMjBjb25maWd1cmF0aW9ucyUwQXZpc2lvbl9jb25maWclMjAlM0QlMjBTYW0yVmlzaW9uQ29uZmlnKCklMEFwcm9tcHRfZW5jb2Rlcl9jb25maWclMjAlM0QlMjBTYW0yUHJvbXB0RW5jb2RlckNvbmZpZygpJTBBbWFza19kZWNvZGVyX2NvbmZpZyUyMCUzRCUyMFNhbTJNYXNrRGVjb2RlckNvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwU2FtMkNvbmZpZyh2aXNpb25fY29uZmlnJTJDJTIwcHJvbXB0X2VuY29kZXJfY29uZmlnJTJDJTIwbWFza19kZWNvZGVyX2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Sam2VisionConfig,
<span class="hljs-meta">... </span>    Sam2PromptEncoderConfig,
<span class="hljs-meta">... </span>    Sam2MaskDecoderConfig,
<span class="hljs-meta">... </span>    Sam2Model,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Sam2Config with \`&quot;facebook/sam2.1_hiera_tiny&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Sam2config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Sam2Model (with random weights) from the \`&quot;facebook/sam2.1_hiera_tiny&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Sam2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a Sam2Config from a Sam2VisionConfig, Sam2PromptEncoderConfig, and Sam2MaskDecoderConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing SAM2 vision encoder, memory attention, and memory encoder configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = Sam2VisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_encoder_config = Sam2PromptEncoderConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_decoder_config = Sam2MaskDecoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = Sam2Config(vision_config, prompt_encoder_config, mask_decoder_config)`,wrap:!1}}),{c(){T=r("p"),T.textContent=oe,$=o(),m(x.$$.fragment)},l(j){T=i(j,"P",{"data-svelte-h":!0}),g(T)!=="svelte-11lpom8"&&(T.textContent=oe),$=a(j),d(x.$$.fragment,j)},m(j,W){l(j,T,W),l(j,$,W),c(x,j,W),V=!0},p:la,i(j){V||(p(x.$$.fragment,j),V=!0)},o(j){f(x.$$.fragment,j),V=!1},d(j){j&&(t(T),t($)),_(x,j)}}}function ha(Ks){let T,oe,$,x,V,j='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/></div>',W,ae,Os,re,et,ie,fo='SAM2 (Segment Anything Model 2) was proposed in <a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/" rel="nofollow">Segment Anything in Images and Videos</a> by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr DollÃ¡r, Christoph Feichtenhofer.',st,le,_o="The model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.",tt,me,go='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif" alt="example image"/>',nt,de,ho="The abstract from the paper is the following:",ot,ce,uo="<em>We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.</em>",at,pe,yo="Tips:",rt,fe,Mo="<li>Batch &amp; Video Support: SAM2 natively supports batch processing and seamless video segmentation, while original SAM is designed for static images and simpler one-image-at-a-time workflows.</li> <li>Accuracy &amp; Generalization: SAM2 shows improved segmentation quality, robustness, and zero-shot generalization to new domains compared to the original SAM, especially with mixed prompts.</li>",it,_e,vo=`This model was contributed by <a href="https://github.com/SangbumChoi" rel="nofollow">sangbumchoi</a> and <a href="https://huggingface.co/yonigozlan" rel="nofollow">yonigozlan</a>.
The original code can be found <a href="https://github.com/facebookresearch/sam2/tree/main" rel="nofollow">here</a>.`,lt,ge,mt,he,dt,ue,bo="SAM2â€™s key strength is its ability to track objects across video frames. Hereâ€™s how to use it for video segmentation:",ct,ye,pt,Me,ft,ve,_t,be,To="Track multiple objects simultaneously across video frames:",gt,Te,ht,je,ut,we,jo="You can add additional clicks on any frame to refine the tracking:",yt,Ue,Mt,Je,vt,Ve,wo="For real-time applications, SAM2 supports processing video frames as they arrive:",bt,Ce,Tt,xe,jt,ke,Uo="Track multiple objects simultaneously in video by adding them all at once:",wt,Ze,Ut,Ie,Jt,Se,Jo="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM.",Vt,$e,Vo='<li><a href="https://colab.research.google.com/drive/1Z0NGLE7p8qnc9UpuI8KBETHd2xBbOEhv?usp=sharing" rel="nofollow">Demo notebook ðŸŒŽ</a> for using the model, contributed by <a href="https://github.com/SangbumChoi" rel="nofollow">Sangbum Choi</a>.</li>',Ct,ze,xt,C,We,fn,hs,Co=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config">Sam2Config</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Model">Sam2Model</a>. It is used to instantiate a
SAM2 model according to the specified arguments, defining the memory attention, memory encoder, and image encoder
configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny
<a href="https://huggingface.co/facebook/sam2.1-hiera-tiny" rel="nofollow">facebook/sam2.1-hiera-tiny</a> architecture.`,_n,us,xo=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,gn,N,kt,Ne,Zt,Z,Be,hn,ys,ko=`This is the configuration class to store the configuration of a <code>Sam2VideoMaskDecoder</code>. It is used to instantiate a SAM2_VIDEO
memory encoder according to the specified arguments, defining the model architecture.`,un,Ms,Zo=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,It,Re,St,I,Xe,yn,vs,Io=`This is the configuration class to store the configuration of a <code>Sam2VideoPromptEncoder</code>. The <code>Sam2VideoPromptEncoder</code>
module is used to encode the input 2D points and bounding boxes.`,Mn,bs,So=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,$t,Ee,zt,b,Ge,vn,Ts,$o=`Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a
single processor.`,bn,js,zo=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoProcessor">Sam2VideoProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2ImageProcessorFast">Sam2ImageProcessorFast</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoProcessor">Sam2VideoProcessor</a>. See the docstring of
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoProcessor.__call__"><strong>call</strong>()</a> for more information.`,Tn,B,Ae,jn,ws,Wo=`This method uses <code>Sam2VideoImageProcessorFast.__call__</code> method to prepare image(s) for the model. It also prepares 2D
points and bounding boxes for the model if they are provided.`,wn,R,Qe,Un,Us,No="Remove padding and upscale masks to the original image size.",Jn,X,Ye,Vn,Js,Bo=`Initializes a video session for inference.
If a video is provided (async inference), the video will be processed and stored on the <code>video_storage_device</code>.`,Cn,E,He,xn,Vs,Ro="Process new points, boxes, or masks for a video frame and add them to the inference session.",Wt,Pe,Nt,z,Fe,kn,G,De,Zn,Cs,Xo="Remove padding and upscale masks to the original image size.",Bt,Le,Rt,h,qe,In,xs,Eo="Manages video inference session parameters, state and cache.",Sn,A,Ke,$n,ks,Go="Add mask inputs with automatic device placement.",zn,Q,Oe,Wn,Zs,Ao="Add new frame with automatic device placement.",Nn,Y,es,Bn,Is,Qo="Add point inputs with automatic device placement.",Rn,H,ss,Xn,Ss,Yo="Get frame from video.",En,P,ts,Gn,$s,Ho="Get the total number of unique object ids received so far in this session.",An,F,ns,Qn,zs,Po="Get output with smart device management.",Yn,D,os,Hn,Ws,Fo="Map object ID to index, creating new entry if needed.",Pn,L,as,Fn,Ns,Do="Map model-side object index to client-side object id.",Dn,q,rs,Ln,Bs,Lo="Remove mask inputs.",qn,K,is,Kn,Rs,qo="Remove point inputs.",On,O,ls,eo,Xs,Ko="Reset tracking data and cache.",so,ee,ms,to,Es,Oo="Reset tracking data but keep cache.",no,se,ds,oo,Gs,ea=`Store output with smart device management.
If output_key is None, the output is stored as a dictionary.`,Xt,cs,Et,w,ps,ao,As,sa="The bare Sam2 Video Model outputting raw hidden-states without any specific head on top.",ro,Qs,ta=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,io,Ys,na=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,lo,te,fs,mo,Hs,oa="Propagate the objects through a streamed video frame.",co,ne,_s,po,Ps,aa=`Propagate the objects through the video frames. Used when initializing an inference session with a whole video.
Yields Sam2VideoSegmentationOutput for each frame.`,Gt,gs,At,qs,Qt;return ae=new U({props:{title:"SAM2 Video",local:"sam2-video",headingTag:"h1"}}),re=new U({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ge=new U({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),he=new U({props:{title:"Video Segmentation and Tracking",local:"video-segmentation-and-tracking",headingTag:"h3"}}),ye=new U({props:{title:"Basic Video Tracking",local:"basic-video-tracking",headingTag:"h4"}}),Me=new Ls({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Sam2VideoModel, Sam2VideoProcessor, infer_device
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = infer_device()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Sam2VideoModel.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-tiny&quot;</span>).to(device, dtype=torch.bfloat16)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Sam2VideoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load video frames (example assumes you have a list of PIL Images)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># video_frames = [Image.open(f&quot;frame_{i:05d}.jpg&quot;) for i in range(num_frames)]</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For this example, we&#x27;ll use the video loading utility</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.video_utils <span class="hljs-keyword">import</span> load_video
<span class="hljs-meta">&gt;&gt;&gt; </span>video_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_frames, _ = load_video(video_url)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize video inference session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inference_session = processor.init_video_session(
<span class="hljs-meta">... </span>    video=video_frames,
<span class="hljs-meta">... </span>    inference_device=device,
<span class="hljs-meta">... </span>    dtype=torch.bfloat16,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add click on first frame to select object</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ann_frame_idx = <span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ann_obj_id = <span class="hljs-number">1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>points = [[[[<span class="hljs-number">210</span>, <span class="hljs-number">350</span>]]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [[[<span class="hljs-number">1</span>]]]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor.add_inputs_to_inference_session(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>    obj_ids=ann_obj_id,
<span class="hljs-meta">... </span>    input_points=points,
<span class="hljs-meta">... </span>    input_labels=labels,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Segment the object on the first frame</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>    [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Segmentation shape: <span class="hljs-subst">{video_res_masks.shape}</span>&quot;</span>)
Segmentation shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">480</span>, <span class="hljs-number">854</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Propagate through the entire video</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_segments = {}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> sam2_video_output <span class="hljs-keyword">in</span> model.propagate_in_video_iterator(inference_session):
<span class="hljs-meta">... </span>    video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>        [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>    )[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    video_segments[sam2_video_output.frame_idx] = video_res_masks

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tracked object through <span class="hljs-subst">{<span class="hljs-built_in">len</span>(video_segments)}</span> frames&quot;</span>)
Tracked <span class="hljs-built_in">object</span> through <span class="hljs-number">180</span> frames`,wrap:!1}}),ve=new U({props:{title:"Multi-Object Video Tracking",local:"multi-object-video-tracking",headingTag:"h4"}}),Te=new Ls({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Reset for new tracking session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inference_session.reset_inference_session()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add multiple objects on the first frame</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ann_frame_idx = <span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>obj_ids = [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">200</span>, <span class="hljs-number">300</span>]], [[<span class="hljs-number">400</span>, <span class="hljs-number">150</span>]]]]  <span class="hljs-comment"># Points for two objects (batched)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>]]]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor.add_inputs_to_inference_session(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>    obj_ids=obj_ids,
<span class="hljs-meta">... </span>    input_points=input_points,
<span class="hljs-meta">... </span>    input_labels=input_labels,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get masks for both objects on first frame</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Propagate both objects through video</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_segments = {}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> sam2_video_output <span class="hljs-keyword">in</span> model.propagate_in_video_iterator(inference_session):
<span class="hljs-meta">... </span>    video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>        [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>    )[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    video_segments[sam2_video_output.frame_idx] = {
<span class="hljs-meta">... </span>        obj_id: video_res_masks[i]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> i, obj_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inference_session.obj_ids)
<span class="hljs-meta">... </span>    }

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tracked <span class="hljs-subst">{<span class="hljs-built_in">len</span>(inference_session.obj_ids)}</span> objects through <span class="hljs-subst">{<span class="hljs-built_in">len</span>(video_segments)}</span> frames&quot;</span>)
Tracked <span class="hljs-number">2</span> objects through <span class="hljs-number">180</span> frames`,wrap:!1}}),je=new U({props:{title:"Refining Video Segmentation",local:"refining-video-segmentation",headingTag:"h4"}}),Ue=new Ls({props:{code:"JTIzJTIwQWRkJTIwcmVmaW5lbWVudCUyMGNsaWNrJTIwb24lMjBhJTIwbGF0ZXIlMjBmcmFtZSUwQXJlZmluZV9mcmFtZV9pZHglMjAlM0QlMjA1MCUwQWFubl9vYmpfaWQlMjAlM0QlMjAyJTIwJTIwJTIzJTIwUmVmaW5pbmclMjBmaXJzdCUyMG9iamVjdCUwQXBvaW50cyUyMCUzRCUyMCU1QiU1QiU1QiU1QjIyMCUyQyUyMDI4MCU1RCU1RCU1RCU1RCUyMCUyMCUyMyUyMEFkZGl0aW9uYWwlMjBwb2ludCUwQWxhYmVscyUyMCUzRCUyMCU1QiU1QiU1QjElNUQlNUQlNUQlMjAlMjAlMjMlMjBQb3NpdGl2ZSUyMGNsaWNrJTBBJTBBcHJvY2Vzc29yLmFkZF9pbnB1dHNfdG9faW5mZXJlbmNlX3Nlc3Npb24oJTBBJTIwJTIwJTIwJTIwaW5mZXJlbmNlX3Nlc3Npb24lM0RpbmZlcmVuY2Vfc2Vzc2lvbiUyQyUwQSUyMCUyMCUyMCUyMGZyYW1lX2lkeCUzRHJlZmluZV9mcmFtZV9pZHglMkMlMEElMjAlMjAlMjAlMjBvYmpfaWRzJTNEYW5uX29ial9pZCUyQyUwQSUyMCUyMCUyMCUyMGlucHV0X3BvaW50cyUzRHBvaW50cyUyQyUwQSUyMCUyMCUyMCUyMGlucHV0X2xhYmVscyUzRGxhYmVscyUyQyUwQSklMEElMEElMjMlMjBSZS1wcm9wYWdhdGUlMjB3aXRoJTIwdGhlJTIwYWRkaXRpb25hbCUyMGluZm9ybWF0aW9uJTBBdmlkZW9fc2VnbWVudHMlMjAlM0QlMjAlN0IlN0QlMEFmb3IlMjBzYW0yX3ZpZGVvX291dHB1dCUyMGluJTIwbW9kZWwucHJvcGFnYXRlX2luX3ZpZGVvX2l0ZXJhdG9yKGluZmVyZW5jZV9zZXNzaW9uKSUzQSUwQSUyMCUyMCUyMCUyMHZpZGVvX3Jlc19tYXNrcyUyMCUzRCUyMHByb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfbWFza3MoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCc2FtMl92aWRlb19vdXRwdXQucHJlZF9tYXNrcyU1RCUyQyUyMG9yaWdpbmFsX3NpemVzJTNEJTVCJTVCaW5mZXJlbmNlX3Nlc3Npb24udmlkZW9faGVpZ2h0JTJDJTIwaW5mZXJlbmNlX3Nlc3Npb24udmlkZW9fd2lkdGglNUQlNUQlMkMlMjBiaW5hcml6ZSUzREZhbHNlJTBBJTIwJTIwJTIwJTIwKSU1QjAlNUQlMEElMjAlMjAlMjAlMjB2aWRlb19zZWdtZW50cyU1QnNhbTJfdmlkZW9fb3V0cHV0LmZyYW1lX2lkeCU1RCUyMCUzRCUyMHZpZGVvX3Jlc19tYXNrcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add refinement click on a later frame</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>refine_frame_idx = <span class="hljs-number">50</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ann_obj_id = <span class="hljs-number">2</span>  <span class="hljs-comment"># Refining first object</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>points = [[[[<span class="hljs-number">220</span>, <span class="hljs-number">280</span>]]]]  <span class="hljs-comment"># Additional point</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [[[<span class="hljs-number">1</span>]]]  <span class="hljs-comment"># Positive click</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor.add_inputs_to_inference_session(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=refine_frame_idx,
<span class="hljs-meta">... </span>    obj_ids=ann_obj_id,
<span class="hljs-meta">... </span>    input_points=points,
<span class="hljs-meta">... </span>    input_labels=labels,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Re-propagate with the additional information</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_segments = {}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> sam2_video_output <span class="hljs-keyword">in</span> model.propagate_in_video_iterator(inference_session):
<span class="hljs-meta">... </span>    video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>        [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>    )[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    video_segments[sam2_video_output.frame_idx] = video_res_masks`,wrap:!1}}),Je=new U({props:{title:"Streaming Video Inference",local:"streaming-video-inference",headingTag:"h3"}}),Ce=new Ls({props:{code:"JTIzJTIwSW5pdGlhbGl6ZSUyMHNlc3Npb24lMjBmb3IlMjBzdHJlYW1pbmclMEFpbmZlcmVuY2Vfc2Vzc2lvbiUyMCUzRCUyMHByb2Nlc3Nvci5pbml0X3ZpZGVvX3Nlc3Npb24oJTBBJTIwJTIwJTIwJTIwaW5mZXJlbmNlX2RldmljZSUzRGRldmljZSUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEEpJTBBJTBBJTIzJTIwUHJvY2VzcyUyMGZyYW1lcyUyMG9uZSUyMGJ5JTIwb25lJTBBZm9yJTIwZnJhbWVfaWR4JTJDJTIwZnJhbWUlMjBpbiUyMGVudW1lcmF0ZSh2aWRlb19mcmFtZXMlNUIlM0ExMCU1RCklM0ElMjAlMjAlMjMlMjBQcm9jZXNzJTIwZmlyc3QlMjAxMCUyMGZyYW1lcyUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RmcmFtZSUyQyUyMGRldmljZSUzRGRldmljZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTIwJTIwJTIwJTIwaWYlMjBmcmFtZV9pZHglMjAlM0QlM0QlMjAwJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwQWRkJTIwcG9pbnQlMjBpbnB1dCUyMG9uJTIwZmlyc3QlMjBmcmFtZSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByb2Nlc3Nvci5hZGRfaW5wdXRzX3RvX2luZmVyZW5jZV9zZXNzaW9uKCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGluZmVyZW5jZV9zZXNzaW9uJTNEaW5mZXJlbmNlX3Nlc3Npb24lMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBmcmFtZV9pZHglM0QwJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb2JqX2lkcyUzRDElMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpbnB1dF9wb2ludHMlM0QlNUIlNUIlNUIlNUIyMTAlMkMlMjAzNTAlNUQlMkMlMjAlNUIyNTAlMkMlMjAyMjAlNUQlNUQlNUQlNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpbnB1dF9sYWJlbHMlM0QlNUIlNUIlNUIxJTJDJTIwMSU1RCU1RCU1RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG9yaWdpbmFsX3NpemUlM0RpbnB1dHMub3JpZ2luYWxfc2l6ZXMlNUIwJTVEJTJDJTIwJTIzJTIwbmVlZCUyMHRvJTIwYmUlMjBwcm92aWRlZCUyMHdoZW4lMjB1c2luZyUyMHN0cmVhbWluZyUyMHZpZGVvJTIwaW5mZXJlbmNlJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKSUwQSUyMCUyMCUyMCUyMCUyMyUyMFByb2Nlc3MlMjBjdXJyZW50JTIwZnJhbWUlMEElMjAlMjAlMjAlMjBzYW0yX3ZpZGVvX291dHB1dCUyMCUzRCUyMG1vZGVsKGluZmVyZW5jZV9zZXNzaW9uJTNEaW5mZXJlbmNlX3Nlc3Npb24lMkMlMjBmcmFtZSUzRGlucHV0cy5waXhlbF92YWx1ZXMlNUIwJTVEKSUwQSUyMCUyMCUyMCUyMHZpZGVvX3Jlc19tYXNrcyUyMCUzRCUyMHByb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfbWFza3MoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCc2FtMl92aWRlb19vdXRwdXQucHJlZF9tYXNrcyU1RCUyQyUyMG9yaWdpbmFsX3NpemVzJTNEaW5wdXRzLm9yaWdpbmFsX3NpemVzJTJDJTIwYmluYXJpemUlM0RGYWxzZSUwQSUyMCUyMCUyMCUyMCklNUIwJTVEJTBBJTIwJTIwJTIwJTIwcHJpbnQoZiUyMkZyYW1lJTIwJTdCZnJhbWVfaWR4JTdEJTNBJTIwbWFzayUyMHNoYXBlJTIwJTdCdmlkZW9fcmVzX21hc2tzLnNoYXBlJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize session for streaming</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inference_session = processor.init_video_session(
<span class="hljs-meta">... </span>    inference_device=device,
<span class="hljs-meta">... </span>    dtype=torch.bfloat16,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Process frames one by one</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> frame_idx, frame <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(video_frames[:<span class="hljs-number">10</span>]):  <span class="hljs-comment"># Process first 10 frames</span>
<span class="hljs-meta">... </span>    inputs = processor(images=frame, device=device, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> frame_idx == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># Add point input on first frame</span>
<span class="hljs-meta">... </span>        processor.add_inputs_to_inference_session(
<span class="hljs-meta">... </span>            inference_session=inference_session,
<span class="hljs-meta">... </span>            frame_idx=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>            obj_ids=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>            input_points=[[[[<span class="hljs-number">210</span>, <span class="hljs-number">350</span>], [<span class="hljs-number">250</span>, <span class="hljs-number">220</span>]]]],
<span class="hljs-meta">... </span>            input_labels=[[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]],
<span class="hljs-meta">... </span>            original_size=inputs.original_sizes[<span class="hljs-number">0</span>], <span class="hljs-comment"># need to be provided when using streaming video inference</span>
<span class="hljs-meta">... </span>        )
...
<span class="hljs-meta">... </span>    <span class="hljs-comment"># Process current frame</span>
<span class="hljs-meta">... </span>    sam2_video_output = model(inference_session=inference_session, frame=inputs.pixel_values[<span class="hljs-number">0</span>])
...
<span class="hljs-meta">... </span>    video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>        [sam2_video_output.pred_masks], original_sizes=inputs.original_sizes, binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>    )[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Frame <span class="hljs-subst">{frame_idx}</span>: mask shape <span class="hljs-subst">{video_res_masks.shape}</span>&quot;</span>)`,wrap:!1}}),xe=new U({props:{title:"Video Batch Processing for Multiple Objects",local:"video-batch-processing-for-multiple-objects",headingTag:"h4"}}),Ze=new Ls({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initialize video session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inference_session = processor.init_video_session(
<span class="hljs-meta">... </span>    video=video_frames,
<span class="hljs-meta">... </span>    inference_device=device,
<span class="hljs-meta">... </span>    dtype=torch.bfloat16,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add multiple objects on the first frame using batch processing</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ann_frame_idx = <span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>obj_ids = [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]  <span class="hljs-comment"># Track two different objects</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [
<span class="hljs-meta">... </span>    [[[<span class="hljs-number">200</span>, <span class="hljs-number">300</span>], [<span class="hljs-number">230</span>, <span class="hljs-number">250</span>], [<span class="hljs-number">275</span>, <span class="hljs-number">175</span>]], [[<span class="hljs-number">400</span>, <span class="hljs-number">150</span>]]]
<span class="hljs-meta">... </span>]  <span class="hljs-comment"># Object 2: 3 points (2 positive, 1 negative); Object 3: 1 point</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [
<span class="hljs-meta">... </span>    [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>]]
<span class="hljs-meta">... </span>]  <span class="hljs-comment"># Object 2: positive, positive, negative; Object 3: positive</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor.add_inputs_to_inference_session(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>    obj_ids=obj_ids,
<span class="hljs-meta">... </span>    input_points=input_points,
<span class="hljs-meta">... </span>    input_labels=input_labels,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get masks for all objects on the first frame</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    inference_session=inference_session,
<span class="hljs-meta">... </span>    frame_idx=ann_frame_idx,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>    [outputs.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated masks for <span class="hljs-subst">{video_res_masks.shape[<span class="hljs-number">0</span>]}</span> objects&quot;</span>)
Generated masks <span class="hljs-keyword">for</span> <span class="hljs-number">2</span> objects

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Propagate all objects through the video</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>video_segments = {}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> sam2_video_output <span class="hljs-keyword">in</span> model.propagate_in_video_iterator(inference_session):
<span class="hljs-meta">... </span>    video_res_masks = processor.post_process_masks(
<span class="hljs-meta">... </span>        [sam2_video_output.pred_masks], original_sizes=[[inference_session.video_height, inference_session.video_width]], binarize=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>    )[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    video_segments[sam2_video_output.frame_idx] = {
<span class="hljs-meta">... </span>        obj_id: video_res_masks[i]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> i, obj_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inference_session.obj_ids)
<span class="hljs-meta">... </span>    }

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Tracked <span class="hljs-subst">{<span class="hljs-built_in">len</span>(inference_session.obj_ids)}</span> objects through <span class="hljs-subst">{<span class="hljs-built_in">len</span>(video_segments)}</span> frames&quot;</span>)
Tracked <span class="hljs-number">2</span> objects through <span class="hljs-number">180</span> frames`,wrap:!1}}),Ie=new U({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ze=new U({props:{title:"Sam2VideoConfig",local:"transformers.Sam2VideoConfig",headingTag:"h2"}}),We=new v({props:{name:"class transformers.Sam2VideoConfig",anchor:"transformers.Sam2VideoConfig",parameters:[{name:"vision_config",val:" = None"},{name:"prompt_encoder_config",val:" = None"},{name:"mask_decoder_config",val:" = None"},{name:"initializer_range",val:" = 0.02"},{name:"num_maskmem",val:" = 7"},{name:"image_size",val:" = 1024"},{name:"sigmoid_scale_for_mem_enc",val:" = 20.0"},{name:"sigmoid_bias_for_mem_enc",val:" = -10.0"},{name:"enable_occlusion_spatial_embedding",val:" = True"},{name:"multimask_output_in_sam",val:" = True"},{name:"multimask_min_pt_num",val:" = 0"},{name:"multimask_max_pt_num",val:" = 1"},{name:"multimask_output_for_tracking",val:" = True"},{name:"max_object_pointers_in_encoder",val:" = 16"},{name:"enable_temporal_pos_encoding_for_object_pointers",val:" = True"},{name:"memory_attention_hidden_size",val:" = 256"},{name:"memory_attention_num_layers",val:" = 4"},{name:"memory_attention_num_attention_heads",val:" = 1"},{name:"memory_attention_downsample_rate",val:" = 1"},{name:"memory_attention_feed_forward_hidden_size",val:" = 2048"},{name:"memory_attention_feed_forward_hidden_act",val:" = 'relu'"},{name:"memory_attention_dropout",val:" = 0.1"},{name:"memory_attention_rope_theta",val:" = 10000"},{name:"memory_attention_rope_feat_sizes",val:" = None"},{name:"memory_attention_rope_dropout",val:" = 0.1"},{name:"memory_encoder_hidden_size",val:" = 256"},{name:"memory_encoder_output_channels",val:" = 64"},{name:"mask_downsampler_embed_dim",val:" = 256"},{name:"mask_downsampler_kernel_size",val:" = 3"},{name:"mask_downsampler_stride",val:" = 2"},{name:"mask_downsampler_padding",val:" = 1"},{name:"mask_downsampler_total_stride",val:" = 16"},{name:"mask_downsampler_hidden_act",val:" = 'gelu'"},{name:"memory_fuser_num_layers",val:" = 2"},{name:"memory_fuser_embed_dim",val:" = 256"},{name:"memory_fuser_intermediate_dim",val:" = 1024"},{name:"memory_fuser_kernel_size",val:" = 7"},{name:"memory_fuser_padding",val:" = 3"},{name:"memory_fuser_layer_scale_init_value",val:" = 1e-06"},{name:"memory_fuser_hidden_act",val:" = 'gelu'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoConfig.vision_config",description:`<strong>vision_config</strong> (Union[<code>dict</code>, <code>Sam2VisionConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionConfig">Sam2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.Sam2VideoConfig.prompt_encoder_config",description:`<strong>prompt_encoder_config</strong> (Union[<code>dict</code>, <code>Sam2PromptEncoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2PromptEncoderConfig">Sam2PromptEncoderConfig</a>.`,name:"prompt_encoder_config"},{anchor:"transformers.Sam2VideoConfig.mask_decoder_config",description:`<strong>mask_decoder_config</strong> (Union[<code>dict</code>, <code>Sam2MaskDecoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2MaskDecoderConfig">Sam2MaskDecoderConfig</a>.`,name:"mask_decoder_config"},{anchor:"transformers.Sam2VideoConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
Standard deviation for parameter initialization.`,name:"initializer_range"},{anchor:"transformers.Sam2VideoConfig.num_maskmem",description:`<strong>num_maskmem</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
The number of memory slots for the mask memory.`,name:"num_maskmem"},{anchor:"transformers.Sam2VideoConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The size of the input images.`,name:"image_size"},{anchor:"transformers.Sam2VideoConfig.sigmoid_scale_for_mem_enc",description:`<strong>sigmoid_scale_for_mem_enc</strong> (<code>float</code>, <em>optional</em>, defaults to 20.0) &#x2014;
Scale factor for the sigmoid function in the memory encoder.`,name:"sigmoid_scale_for_mem_enc"},{anchor:"transformers.Sam2VideoConfig.sigmoid_bias_for_mem_enc",description:`<strong>sigmoid_bias_for_mem_enc</strong> (<code>float</code>, <em>optional</em>, defaults to -10.0) &#x2014;
Bias for the sigmoid function in the memory encoder.`,name:"sigmoid_bias_for_mem_enc"},{anchor:"transformers.Sam2VideoConfig.enable_occlusion_spatial_embedding",description:`<strong>enable_occlusion_spatial_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to enable spatial embedding for occlusions.`,name:"enable_occlusion_spatial_embedding"},{anchor:"transformers.Sam2VideoConfig.multimask_output_in_sam",description:`<strong>multimask_output_in_sam</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to output multiple masks from the SAM head.`,name:"multimask_output_in_sam"},{anchor:"transformers.Sam2VideoConfig.multimask_min_pt_num",description:`<strong>multimask_min_pt_num</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The minimum number of points to trigger multimask output.`,name:"multimask_min_pt_num"},{anchor:"transformers.Sam2VideoConfig.multimask_max_pt_num",description:`<strong>multimask_max_pt_num</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The maximum number of points to trigger multimask output.`,name:"multimask_max_pt_num"},{anchor:"transformers.Sam2VideoConfig.multimask_output_for_tracking",description:`<strong>multimask_output_for_tracking</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use multimask output for tracking.`,name:"multimask_output_for_tracking"},{anchor:"transformers.Sam2VideoConfig.max_object_pointers_in_encoder",description:`<strong>max_object_pointers_in_encoder</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum number of object pointers in the encoder.`,name:"max_object_pointers_in_encoder"},{anchor:"transformers.Sam2VideoConfig.enable_temporal_pos_encoding_for_object_pointers",description:`<strong>enable_temporal_pos_encoding_for_object_pointers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to enable temporal positional encoding for object pointers.`,name:"enable_temporal_pos_encoding_for_object_pointers"},{anchor:"transformers.Sam2VideoConfig.memory_attention_hidden_size",description:`<strong>memory_attention_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the memory attention hidden states.`,name:"memory_attention_hidden_size"},{anchor:"transformers.Sam2VideoConfig.memory_attention_num_layers",description:`<strong>memory_attention_num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of layers in the memory attention module.`,name:"memory_attention_num_layers"},{anchor:"transformers.Sam2VideoConfig.memory_attention_num_attention_heads",description:`<strong>memory_attention_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of attention heads for each attention layer in the memory attention.`,name:"memory_attention_num_attention_heads"},{anchor:"transformers.Sam2VideoConfig.memory_attention_downsample_rate",description:`<strong>memory_attention_downsample_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The downsample rate for the attention layers.`,name:"memory_attention_downsample_rate"},{anchor:"transformers.Sam2VideoConfig.memory_attention_feed_forward_hidden_size",description:`<strong>memory_attention_feed_forward_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The dimension of the feedforward network in the memory attention module.`,name:"memory_attention_feed_forward_hidden_size"},{anchor:"transformers.Sam2VideoConfig.memory_attention_feed_forward_hidden_act",description:`<strong>memory_attention_feed_forward_hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function in the feedforward network in the memory attention module.`,name:"memory_attention_feed_forward_hidden_act"},{anchor:"transformers.Sam2VideoConfig.memory_attention_dropout",description:`<strong>memory_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout rate for the memory attention module.`,name:"memory_attention_dropout"},{anchor:"transformers.Sam2VideoConfig.memory_attention_rope_theta",description:`<strong>memory_attention_rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000) &#x2014;
The Rope theta parameter.`,name:"memory_attention_rope_theta"},{anchor:"transformers.Sam2VideoConfig.memory_attention_rope_feat_sizes",description:`<strong>memory_attention_rope_feat_sizes</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[64, 64]</code>) &#x2014;
The feature sizes for the Rope positional encoding.`,name:"memory_attention_rope_feat_sizes"},{anchor:"transformers.Sam2VideoConfig.memory_attention_rope_dropout",description:`<strong>memory_attention_rope_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout rate for the Rope positional encoding.`,name:"memory_attention_rope_dropout"},{anchor:"transformers.Sam2VideoConfig.memory_encoder_hidden_size",description:`<strong>memory_encoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the memory encoder hidden states.`,name:"memory_encoder_hidden_size"},{anchor:"transformers.Sam2VideoConfig.memory_encoder_output_channels",description:`<strong>memory_encoder_output_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of output channels for the memory encoder.`,name:"memory_encoder_output_channels"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_embed_dim",description:`<strong>mask_downsampler_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimension of the mask downsampler embedding.`,name:"mask_downsampler_embed_dim"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_kernel_size",description:`<strong>mask_downsampler_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The kernel size for the mask downsampler.`,name:"mask_downsampler_kernel_size"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_stride",description:`<strong>mask_downsampler_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The stride for the mask downsampler.`,name:"mask_downsampler_stride"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_padding",description:`<strong>mask_downsampler_padding</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The padding for the mask downsampler.`,name:"mask_downsampler_padding"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_total_stride",description:`<strong>mask_downsampler_total_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The total stride for the mask downsampler.`,name:"mask_downsampler_total_stride"},{anchor:"transformers.Sam2VideoConfig.mask_downsampler_hidden_act",description:`<strong>mask_downsampler_hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the mask downsampler.`,name:"mask_downsampler_hidden_act"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_num_layers",description:`<strong>memory_fuser_num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the memory fuser.`,name:"memory_fuser_num_layers"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_embed_dim",description:`<strong>memory_fuser_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The dimension of the embedding layer in the memory fuser.`,name:"memory_fuser_embed_dim"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_intermediate_dim",description:`<strong>memory_fuser_intermediate_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The dimension of the intermediate layer in the memory fuser.`,name:"memory_fuser_intermediate_dim"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_kernel_size",description:`<strong>memory_fuser_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
The kernel size for the memory fuser.`,name:"memory_fuser_kernel_size"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_padding",description:`<strong>memory_fuser_padding</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The padding for the memory fuser.`,name:"memory_fuser_padding"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_layer_scale_init_value",description:`<strong>memory_fuser_layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The initial value for the layer scale in the memory fuser.`,name:"memory_fuser_layer_scale_init_value"},{anchor:"transformers.Sam2VideoConfig.memory_fuser_hidden_act",description:`<strong>memory_fuser_hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the memory fuser.`,name:"memory_fuser_hidden_act"},{anchor:"transformers.Sam2VideoConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/configuration_sam2_video.py#L150"}}),N=new fa({props:{anchor:"transformers.Sam2VideoConfig.example",$$slots:{default:[ga]},$$scope:{ctx:Ks}}}),Ne=new U({props:{title:"Sam2VideoMaskDecoderConfig",local:"transformers.Sam2VideoMaskDecoderConfig",headingTag:"h2"}}),Be=new v({props:{name:"class transformers.Sam2VideoMaskDecoderConfig",anchor:"transformers.Sam2VideoMaskDecoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"hidden_act",val:" = 'gelu'"},{name:"mlp_dim",val:" = 2048"},{name:"num_hidden_layers",val:" = 2"},{name:"num_attention_heads",val:" = 8"},{name:"attention_downsample_rate",val:" = 2"},{name:"num_multimask_outputs",val:" = 3"},{name:"iou_head_depth",val:" = 3"},{name:"iou_head_hidden_dim",val:" = 256"},{name:"dynamic_multimask_via_stability",val:" = True"},{name:"dynamic_multimask_stability_delta",val:" = 0.05"},{name:"dynamic_multimask_stability_thresh",val:" = 0.98"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoMaskDecoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the SAM2_VIDEO mask decoder.`,name:"hidden_act"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.mlp_dim",description:`<strong>mlp_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The dimension of the MLP in the two-way transformer.`,name:"mlp_dim"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of hidden layers in the two-way transformer.`,name:"num_hidden_layers"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The number of attention heads in the two-way transformer.`,name:"num_attention_heads"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.attention_downsample_rate",description:`<strong>attention_downsample_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The downsample rate for the attention layers.`,name:"attention_downsample_rate"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.num_multimask_outputs",description:`<strong>num_multimask_outputs</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of multimask outputs.`,name:"num_multimask_outputs"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.iou_head_depth",description:`<strong>iou_head_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The depth of the IoU head.`,name:"iou_head_depth"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.iou_head_hidden_dim",description:`<strong>iou_head_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The hidden dimension of the IoU head.`,name:"iou_head_hidden_dim"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.dynamic_multimask_via_stability",description:`<strong>dynamic_multimask_via_stability</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use dynamic multimask via stability.`,name:"dynamic_multimask_via_stability"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.dynamic_multimask_stability_delta",description:`<strong>dynamic_multimask_stability_delta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
The stability delta for the dynamic multimask.`,name:"dynamic_multimask_stability_delta"},{anchor:"transformers.Sam2VideoMaskDecoderConfig.dynamic_multimask_stability_thresh",description:`<strong>dynamic_multimask_stability_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.98) &#x2014;
The stability threshold for the dynamic multimask.`,name:"dynamic_multimask_stability_thresh"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/configuration_sam2_video.py#L77"}}),Re=new U({props:{title:"Sam2VideoPromptEncoderConfig",local:"transformers.Sam2VideoPromptEncoderConfig",headingTag:"h2"}}),Xe=new v({props:{name:"class transformers.Sam2VideoPromptEncoderConfig",anchor:"transformers.Sam2VideoPromptEncoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"image_size",val:" = 1024"},{name:"patch_size",val:" = 16"},{name:"mask_input_channels",val:" = 16"},{name:"num_point_embeddings",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"scale",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoPromptEncoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The expected output resolution of the image.`,name:"image_size"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.mask_input_channels",description:`<strong>mask_input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The number of channels to be fed to the <code>MaskDecoder</code> module.`,name:"mask_input_channels"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.num_point_embeddings",description:`<strong>num_point_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of point embeddings to be used.`,name:"num_point_embeddings"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Sam2VideoPromptEncoderConfig.scale",description:`<strong>scale</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The scale factor for the prompt encoder.`,name:"scale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/configuration_sam2_video.py#L25"}}),Ee=new U({props:{title:"Sam2VideoProcessor",local:"transformers.Sam2VideoProcessor",headingTag:"h2"}}),Ge=new v({props:{name:"class transformers.Sam2VideoProcessor",anchor:"transformers.Sam2VideoProcessor",parameters:[{name:"image_processor",val:""},{name:"video_processor",val:""},{name:"target_size",val:": typing.Optional[int] = None"},{name:"point_pad_value",val:": int = -10"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoProcessor.image_processor",description:`<strong>image_processor</strong> (<code>Sam2ImageProcessorFast</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2ImageProcessorFast">Sam2ImageProcessorFast</a>.`,name:"image_processor"},{anchor:"transformers.Sam2VideoProcessor.video_processor",description:`<strong>video_processor</strong> (<code>Sam2VideoVideoProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoVideoProcessor">Sam2VideoVideoProcessor</a>.`,name:"video_processor"},{anchor:"transformers.Sam2VideoProcessor.target_size",description:`<strong>target_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The target size (target_size, target_size) to which the image will be resized.`,name:"target_size"},{anchor:"transformers.Sam2VideoProcessor.point_pad_value",description:`<strong>point_pad_value</strong> (<code>int</code>, <em>optional</em>, defaults to -10) &#x2014;
The value used for padding input points.`,name:"point_pad_value"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/processing_sam2_video.py#L37"}}),Ae=new v({props:{name:"__call__",anchor:"transformers.Sam2VideoProcessor.__call__",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None"},{name:"segmentation_maps",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None"},{name:"input_points",val:": typing.Union[list[list[list[list[float]]]], torch.Tensor, NoneType] = None"},{name:"input_labels",val:": typing.Union[list[list[list[int]]], torch.Tensor, NoneType] = None"},{name:"input_boxes",val:": typing.Union[list[list[list[float]]], torch.Tensor, NoneType] = None"},{name:"original_sizes",val:": typing.Union[list[list[float]], torch.Tensor, NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoProcessor.__call__.images",description:`<strong>images</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The image(s) to process.`,name:"images"},{anchor:"transformers.Sam2VideoProcessor.__call__.segmentation_maps",description:`<strong>segmentation_maps</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The segmentation maps to process.`,name:"segmentation_maps"},{anchor:"transformers.Sam2VideoProcessor.__call__.input_points",description:`<strong>input_points</strong> (<code>list[list[list[list[float]]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The points to add to the frame.`,name:"input_points"},{anchor:"transformers.Sam2VideoProcessor.__call__.input_labels",description:`<strong>input_labels</strong> (<code>list[list[list[int]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The labels for the points.`,name:"input_labels"},{anchor:"transformers.Sam2VideoProcessor.__call__.input_boxes",description:`<strong>input_boxes</strong> (<code>list[list[list[float]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The bounding boxes to add to the frame.`,name:"input_boxes"},{anchor:"transformers.Sam2VideoProcessor.__call__.original_sizes",description:`<strong>original_sizes</strong> (<code>list[list[float]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The original sizes of the images.`,name:"original_sizes"},{anchor:"transformers.Sam2VideoProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return.`,name:"return_tensors"},{anchor:"transformers.Sam2VideoProcessor.__call__.*kwargs",description:`*<strong>*kwargs</strong> &#x2014;
Additional keyword arguments to pass to the image processor.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/processing_sam2_video.py#L67",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><code>pixel_values</code> (<code>torch.Tensor</code>): The processed image(s).</li>
<li><code>original_sizes</code> (<code>list[list[float]]</code>): The original sizes of the images.</li>
<li><code>reshaped_input_sizes</code> (<code>torch.Tensor</code>): The reshaped input sizes of the images.</li>
<li><code>labels</code> (<code>torch.Tensor</code>): The processed segmentation maps (if provided).</li>
<li><code>input_points</code> (<code>torch.Tensor</code>): The processed points.</li>
<li><code>input_labels</code> (<code>torch.Tensor</code>): The processed labels.</li>
<li><code>input_boxes</code> (<code>torch.Tensor</code>): The processed bounding boxes.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields</p>
`}}),Qe=new v({props:{name:"post_process_masks",anchor:"transformers.Sam2VideoProcessor.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"max_hole_area",val:" = 0.0"},{name:"max_sprinkle_area",val:" = 0.0"},{name:"apply_non_overlapping_constraints",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VideoProcessor.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[List[torch.Tensor], List[np.ndarray]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold for binarization and post-processing operations.`,name:"mask_threshold"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.max_hole_area",description:`<strong>max_hole_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a hole to fill.`,name:"max_hole_area"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.max_sprinkle_area",description:`<strong>max_sprinkle_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a sprinkle to fill.`,name:"max_sprinkle_area"},{anchor:"transformers.Sam2VideoProcessor.post_process_masks.apply_non_overlapping_constraints",description:`<strong>apply_non_overlapping_constraints</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply non-overlapping constraints to the masks.`,name:"apply_non_overlapping_constraints"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/processing_sam2_video.py#L483",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where (height, width)
is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>torch.Tensor</code>)</p>
`}}),Ye=new v({props:{name:"init_video_session",anchor:"transformers.Sam2VideoProcessor.init_video_session",parameters:[{name:"video",val:": typing.Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]], NoneType] = None"},{name:"inference_device",val:": typing.Union[str, ForwardRef('torch.device')] = 'cpu'"},{name:"inference_state_device",val:": typing.Union[str, ForwardRef('torch.device')] = None"},{name:"processing_device",val:": typing.Union[str, ForwardRef('torch.device')] = None"},{name:"video_storage_device",val:": typing.Union[str, ForwardRef('torch.device')] = None"},{name:"max_vision_features_cache_size",val:": int = 1"},{name:"dtype",val:": dtype = torch.float32"}],parametersDescription:[{anchor:"transformers.Sam2VideoProcessor.init_video_session.video",description:`<strong>video</strong> (<code>VideoInput</code>, <em>optional</em>) &#x2014;
The video to process. No need to provide when streaming.`,name:"video"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.inference_device",description:`<strong>inference_device</strong> (<code>str</code> or <code>torch.device</code>, <em>optional</em>, defaults to &#x201C;cpu&#x201D;) &#x2014;
The device to use for inference.`,name:"inference_device"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.inference_state_device",description:`<strong>inference_state_device</strong> (<code>str</code> or <code>torch.device</code>, <em>optional</em>) &#x2014;
The device to store the inference state on.`,name:"inference_state_device"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.processing_device",description:`<strong>processing_device</strong> (<code>str</code> or <code>torch.device</code>, <em>optional</em>) &#x2014;
The device to use for video processing.`,name:"processing_device"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.video_storage_device",description:`<strong>video_storage_device</strong> (<code>str</code> or <code>torch.device</code>, <em>optional</em>) &#x2014;
The device to store the processed video frames on.`,name:"video_storage_device"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.max_vision_features_cache_size",description:`<strong>max_vision_features_cache_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The maximum number of vision features to cache.`,name:"max_vision_features_cache_size"},{anchor:"transformers.Sam2VideoProcessor.init_video_session.dtype",description:`<strong>dtype</strong> (<code>torch.dtype</code>, <em>optional</em>, defaults to <code>torch.float32</code>) &#x2014;
The torch dtype to use for the whole session.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/processing_sam2_video.py#L529"}}),He=new v({props:{name:"add_inputs_to_inference_session",anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session",parameters:[{name:"inference_session",val:": Sam2VideoInferenceSession"},{name:"frame_idx",val:": int"},{name:"obj_ids",val:": typing.Union[list[int], int]"},{name:"input_points",val:": typing.Union[list[list[list[list[float]]]], torch.Tensor, NoneType] = None"},{name:"input_labels",val:": typing.Union[list[list[list[int]]], torch.Tensor, NoneType] = None"},{name:"input_boxes",val:": typing.Union[list[list[list[float]]], torch.Tensor, NoneType] = None"},{name:"input_masks",val:": typing.Union[numpy.ndarray, torch.Tensor, list[numpy.ndarray], list[torch.Tensor], NoneType] = None"},{name:"original_size",val:": typing.Optional[tuple[int, int]] = None"},{name:"clear_old_inputs",val:": bool = True"}],parametersDescription:[{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.inference_session",description:`<strong>inference_session</strong> (<code>Sam2VideoInferenceSession</code>) &#x2014;
The inference session for the video.`,name:"inference_session"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.frame_idx",description:`<strong>frame_idx</strong> (<code>int</code>) &#x2014;
The index of the frame to process.`,name:"frame_idx"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.obj_ids",description:`<strong>obj_ids</strong> (<code>list[int]</code> or <code>int</code>) &#x2014;
The object ID(s) to associate with the points or box.
These can be any integers and can be reused later on to specify an object.`,name:"obj_ids"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.input_points",description:`<strong>input_points</strong> (<code>list[list[list[list[float]]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The points to add to the frame.`,name:"input_points"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.input_labels",description:`<strong>input_labels</strong> (<code>list[list[list[int]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The labels for the points.`,name:"input_labels"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.input_boxes",description:`<strong>input_boxes</strong> (<code>list[list[list[float]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The bounding boxes to add to the frame.`,name:"input_boxes"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.input_masks",description:`<strong>input_masks</strong> (<code>np.ndarray</code>, <code>torch.Tensor</code>, <code>list[np.ndarray]</code>, or <code>list[torch.Tensor]</code>, <em>optional</em>) &#x2014;
The mask(s) to add to the frame.`,name:"input_masks"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.original_size",description:`<strong>original_size</strong> (<code>tuple[int, int]</code>, <em>optional</em>) &#x2014;
The original size of the video. Provide when streaming.`,name:"original_size"},{anchor:"transformers.Sam2VideoProcessor.add_inputs_to_inference_session.clear_old_inputs",description:`<strong>clear_old_inputs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to clear old inputs for the object.`,name:"clear_old_inputs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/processing_sam2_video.py#L582"}}),Pe=new U({props:{title:"Sam2VideoVideoProcessor",local:"transformers.Sam2VideoVideoProcessor",headingTag:"h2"}}),Fe=new v({props:{name:"class transformers.Sam2VideoVideoProcessor",anchor:"transformers.Sam2VideoVideoProcessor",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.processing_utils.VideosKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/video_processing_sam2_video.py#L46"}}),De=new v({props:{name:"post_process_masks",anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"reshaped_input_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"pad_size",val:" = None"}],parametersDescription:[{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[List[torch.Tensor], List[np.ndarray]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.reshaped_input_sizes",description:`<strong>reshaped_input_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.`,name:"reshaped_input_sizes"},{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The threshold to use for binarizing the masks.`,name:"mask_threshold"},{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.Sam2VideoVideoProcessor.post_process_masks.pad_size",description:`<strong>pad_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.pad_size</code>) &#x2014;
The target size the images were padded to before being passed to the model. If None, the target size is
assumed to be the processor&#x2019;s <code>pad_size</code>.`,name:"pad_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/video_processing_sam2_video.py#L76",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where (height, width)
is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>torch.Tensor</code>)</p>
`}}),Le=new U({props:{title:"Sam2VideoInferenceSession",local:"transformers.Sam2VideoInferenceSession",headingTag:"h2"}}),qe=new v({props:{name:"class transformers.Sam2VideoInferenceSession",anchor:"transformers.Sam2VideoInferenceSession",parameters:[{name:"video",val:": FloatTensor = None"},{name:"video_height",val:": typing.Optional[int] = None"},{name:"video_width",val:": typing.Optional[int] = None"},{name:"inference_device",val:": typing.Union[torch.device, str] = 'cpu'"},{name:"inference_state_device",val:": typing.Union[torch.device, str] = 'cpu'"},{name:"video_storage_device",val:": typing.Union[torch.device, str] = 'cpu'"},{name:"dtype",val:": typing.Union[torch.dtype, str] = 'float32'"},{name:"max_vision_features_cache_size",val:": int = 1"}],parametersDescription:[{anchor:"transformers.Sam2VideoInferenceSession.video",description:`<strong>video</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
The video to process. No need to provide when streaming.`,name:"video"},{anchor:"transformers.Sam2VideoInferenceSession.video_height",description:`<strong>video_height</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The height of the video.`,name:"video_height"},{anchor:"transformers.Sam2VideoInferenceSession.video_width",description:`<strong>video_width</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The width of the video.`,name:"video_width"},{anchor:"transformers.Sam2VideoInferenceSession.inference_device",description:`<strong>inference_device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to <code>&quot;cpu&quot;</code>) &#x2014;
The device to use for inference.`,name:"inference_device"},{anchor:"transformers.Sam2VideoInferenceSession.inference_state_device",description:`<strong>inference_state_device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to <code>&quot;cpu&quot;</code>) &#x2014;
The device to store the inference state on.`,name:"inference_state_device"},{anchor:"transformers.Sam2VideoInferenceSession.video_storage_device",description:`<strong>video_storage_device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to <code>&quot;cpu&quot;</code>) &#x2014;
The device to store the video on.`,name:"video_storage_device"},{anchor:"transformers.Sam2VideoInferenceSession.dtype",description:`<strong>dtype</strong> (<code>torch.dtype</code>, <em>optional</em>, defaults to <code>&quot;float32&quot;</code>) &#x2014;
The dtype to use for the video.`,name:"dtype"},{anchor:"transformers.Sam2VideoInferenceSession.max_vision_features_cache_size",description:`<strong>max_vision_features_cache_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The maximum number of vision features to cache.`,name:"max_vision_features_cache_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L102"}}),Ke=new v({props:{name:"add_mask_inputs",anchor:"transformers.Sam2VideoInferenceSession.add_mask_inputs",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"},{name:"inputs",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L220"}}),Oe=new v({props:{name:"add_new_frame",anchor:"transformers.Sam2VideoInferenceSession.add_new_frame",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L295"}}),es=new v({props:{name:"add_point_inputs",anchor:"transformers.Sam2VideoInferenceSession.add_point_inputs",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"},{name:"inputs",val:": dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L206"}}),ss=new v({props:{name:"get_frame",anchor:"transformers.Sam2VideoInferenceSession.get_frame",parameters:[{name:"frame_idx",val:": int"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L308"}}),ts=new v({props:{name:"get_obj_num",anchor:"transformers.Sam2VideoInferenceSession.get_obj_num",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L201"}}),ns=new v({props:{name:"get_output",anchor:"transformers.Sam2VideoInferenceSession.get_output",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"},{name:"output_key",val:": str"},{name:"is_conditioning_frame",val:": bool = True"}],parametersDescription:[{anchor:"transformers.Sam2VideoInferenceSession.get_output.obj_idx",description:"<strong>obj_idx</strong> (int) &#x2014; The index of the object.",name:"obj_idx"},{anchor:"transformers.Sam2VideoInferenceSession.get_output.frame_idx",description:"<strong>frame_idx</strong> (int) &#x2014; The index of the frame.",name:"frame_idx"},{anchor:"transformers.Sam2VideoInferenceSession.get_output.output_key",description:"<strong>output_key</strong> (str) &#x2014; The key of the output.",name:"output_key"},{anchor:"transformers.Sam2VideoInferenceSession.get_output.is_conditioning_frame",description:"<strong>is_conditioning_frame</strong> (bool) &#x2014; Whether the output is for a conditioning frame.",name:"is_conditioning_frame"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L268"}}),os=new v({props:{name:"obj_id_to_idx",anchor:"transformers.Sam2VideoInferenceSession.obj_id_to_idx",parameters:[{name:"obj_id",val:": int"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L175"}}),as=new v({props:{name:"obj_idx_to_id",anchor:"transformers.Sam2VideoInferenceSession.obj_idx_to_id",parameters:[{name:"obj_idx",val:": int"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L197"}}),rs=new v({props:{name:"remove_mask_inputs",anchor:"transformers.Sam2VideoInferenceSession.remove_mask_inputs",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L226"}}),is=new v({props:{name:"remove_point_inputs",anchor:"transformers.Sam2VideoInferenceSession.remove_point_inputs",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L216"}}),ls=new v({props:{name:"reset_inference_session",anchor:"transformers.Sam2VideoInferenceSession.reset_inference_session",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L324"}}),ms=new v({props:{name:"reset_tracking_data",anchor:"transformers.Sam2VideoInferenceSession.reset_tracking_data",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L312"}}),ds=new v({props:{name:"store_output",anchor:"transformers.Sam2VideoInferenceSession.store_output",parameters:[{name:"obj_idx",val:": int"},{name:"frame_idx",val:": int"},{name:"output_key",val:": typing.Optional[str] = None"},{name:"output_value",val:": typing.Union[torch.Tensor, dict, NoneType] = None"},{name:"is_conditioning_frame",val:": bool = True"}],parametersDescription:[{anchor:"transformers.Sam2VideoInferenceSession.store_output.obj_idx",description:"<strong>obj_idx</strong> (int) &#x2014; The index of the object.",name:"obj_idx"},{anchor:"transformers.Sam2VideoInferenceSession.store_output.frame_idx",description:"<strong>frame_idx</strong> (int) &#x2014; The index of the frame.",name:"frame_idx"},{anchor:"transformers.Sam2VideoInferenceSession.store_output.output_key",description:"<strong>output_key</strong> (Optional[str]) &#x2014; The key of the output. If None, the output is stored as a dictionary.",name:"output_key"},{anchor:"transformers.Sam2VideoInferenceSession.store_output.output_value",description:"<strong>output_value</strong> (Optional[Union[torch.Tensor, dict]]) &#x2014; The value of the output.",name:"output_value"},{anchor:"transformers.Sam2VideoInferenceSession.store_output.is_conditioning_frame",description:"<strong>is_conditioning_frame</strong> (bool) &#x2014; Whether the output is for a conditioning frame.",name:"is_conditioning_frame"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L231"}}),cs=new U({props:{title:"Sam2VideoModel",local:"transformers.Sam2VideoModel",headingTag:"h2"}}),ps=new v({props:{name:"class transformers.Sam2VideoModel",anchor:"transformers.Sam2VideoModel",parameters:[{name:"config",val:": Sam2VideoConfig"}],parametersDescription:[{anchor:"transformers.Sam2VideoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoConfig">Sam2VideoConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L1556"}}),fs=new v({props:{name:"forward",anchor:"transformers.Sam2VideoModel.forward",parameters:[{name:"inference_session",val:": Sam2VideoInferenceSession"},{name:"frame_idx",val:": typing.Optional[int] = None"},{name:"frame",val:": typing.Optional[torch.Tensor] = None"},{name:"reverse",val:": bool = False"}],parametersDescription:[{anchor:"transformers.Sam2VideoModel.forward.inference_session",description:`<strong>inference_session</strong> (<code>~models.sam2_video.modeling_sam2_video.Sam2VideoInferenceSession</code>) &#x2014;
The video inference session object.`,name:"inference_session"},{anchor:"transformers.Sam2VideoModel.forward.frame_idx",description:`<strong>frame_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The index of the frame on which to run inference. No need to provide when inferring
on a new streamed frame.`,name:"frame_idx"},{anchor:"transformers.Sam2VideoModel.forward.frame",description:`<strong>frame</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The frame to process. Provide when streaming.`,name:"frame"},{anchor:"transformers.Sam2VideoModel.forward.reverse",description:`<strong>reverse</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to propagate in reverse.`,name:"reverse"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L1694",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.sam2_video.modeling_sam2_video.Sam2VideoSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoConfig"
>Sam2VideoConfig</a>) and inputs.</p>
<ul>
<li><strong>pred_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_masks, height, width)</code>) â€” The predicted masks stored at the modelâ€™s resolution.</li>
<li><strong>frame_idx</strong> (<code>&lt;class 'int'&gt;.frame_idx</code>, defaults to <code>None</code>) â€” The frame index of the video.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.sam2_video.modeling_sam2_video.Sam2VideoSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_s=new v({props:{name:"propagate_in_video_iterator",anchor:"transformers.Sam2VideoModel.propagate_in_video_iterator",parameters:[{name:"inference_session",val:": Sam2VideoInferenceSession"},{name:"start_frame_idx",val:": typing.Optional[int] = None"},{name:"max_frame_num_to_track",val:": typing.Optional[int] = None"},{name:"reverse",val:": bool = False"}],parametersDescription:[{anchor:"transformers.Sam2VideoModel.propagate_in_video_iterator.inference_session",description:`<strong>inference_session</strong> (<code>~models.sam2_video.modeling_sam2_video.Sam2VideoInferenceSession</code>) &#x2014;
The video inference session object.`,name:"inference_session"},{anchor:"transformers.Sam2VideoModel.propagate_in_video_iterator.start_frame_idx",description:`<strong>start_frame_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The starting frame index for propagation.
Need to be provided if <code>forward</code> hasn&#x2019;t been called on new inputs yet.
If not provided, the starting frame index will be the earliest frame with input points.`,name:"start_frame_idx"},{anchor:"transformers.Sam2VideoModel.propagate_in_video_iterator.max_frame_num_to_track",description:`<strong>max_frame_num_to_track</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum number of frames to track.`,name:"max_frame_num_to_track"},{anchor:"transformers.Sam2VideoModel.propagate_in_video_iterator.reverse",description:`<strong>reverse</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to propagate in reverse.`,name:"reverse"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2_video/modeling_sam2_video.py#L2491",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.sam2_video.modeling_sam2_video.Sam2VideoSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoConfig"
>Sam2VideoConfig</a>) and inputs.</p>
<ul>
<li><strong>pred_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_masks, height, width)</code>) â€” The predicted masks stored at the modelâ€™s resolution.</li>
<li><strong>frame_idx</strong> (<code>&lt;class 'int'&gt;.frame_idx</code>, defaults to <code>None</code>) â€” The frame index of the video.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.sam2_video.modeling_sam2_video.Sam2VideoSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),gs=new _a({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam2_video.md"}}),{c(){T=r("meta"),oe=o(),$=r("p"),x=o(),V=r("div"),V.innerHTML=j,W=o(),m(ae.$$.fragment),Os=o(),m(re.$$.fragment),et=o(),ie=r("p"),ie.innerHTML=fo,st=o(),le=r("p"),le.textContent=_o,tt=o(),me=r("p"),me.innerHTML=go,nt=o(),de=r("p"),de.textContent=ho,ot=o(),ce=r("p"),ce.innerHTML=uo,at=o(),pe=r("p"),pe.textContent=yo,rt=o(),fe=r("ul"),fe.innerHTML=Mo,it=o(),_e=r("p"),_e.innerHTML=vo,lt=o(),m(ge.$$.fragment),mt=o(),m(he.$$.fragment),dt=o(),ue=r("p"),ue.textContent=bo,ct=o(),m(ye.$$.fragment),pt=o(),m(Me.$$.fragment),ft=o(),m(ve.$$.fragment),_t=o(),be=r("p"),be.textContent=To,gt=o(),m(Te.$$.fragment),ht=o(),m(je.$$.fragment),ut=o(),we=r("p"),we.textContent=jo,yt=o(),m(Ue.$$.fragment),Mt=o(),m(Je.$$.fragment),vt=o(),Ve=r("p"),Ve.textContent=wo,bt=o(),m(Ce.$$.fragment),Tt=o(),m(xe.$$.fragment),jt=o(),ke=r("p"),ke.textContent=Uo,wt=o(),m(Ze.$$.fragment),Ut=o(),m(Ie.$$.fragment),Jt=o(),Se=r("p"),Se.textContent=Jo,Vt=o(),$e=r("ul"),$e.innerHTML=Vo,Ct=o(),m(ze.$$.fragment),xt=o(),C=r("div"),m(We.$$.fragment),fn=o(),hs=r("p"),hs.innerHTML=Co,_n=o(),us=r("p"),us.innerHTML=xo,gn=o(),m(N.$$.fragment),kt=o(),m(Ne.$$.fragment),Zt=o(),Z=r("div"),m(Be.$$.fragment),hn=o(),ys=r("p"),ys.innerHTML=ko,un=o(),Ms=r("p"),Ms.innerHTML=Zo,It=o(),m(Re.$$.fragment),St=o(),I=r("div"),m(Xe.$$.fragment),yn=o(),vs=r("p"),vs.innerHTML=Io,Mn=o(),bs=r("p"),bs.innerHTML=So,$t=o(),m(Ee.$$.fragment),zt=o(),b=r("div"),m(Ge.$$.fragment),vn=o(),Ts=r("p"),Ts.textContent=$o,bn=o(),js=r("p"),js.innerHTML=zo,Tn=o(),B=r("div"),m(Ae.$$.fragment),jn=o(),ws=r("p"),ws.innerHTML=Wo,wn=o(),R=r("div"),m(Qe.$$.fragment),Un=o(),Us=r("p"),Us.textContent=No,Jn=o(),X=r("div"),m(Ye.$$.fragment),Vn=o(),Js=r("p"),Js.innerHTML=Bo,Cn=o(),E=r("div"),m(He.$$.fragment),xn=o(),Vs=r("p"),Vs.textContent=Ro,Wt=o(),m(Pe.$$.fragment),Nt=o(),z=r("div"),m(Fe.$$.fragment),kn=o(),G=r("div"),m(De.$$.fragment),Zn=o(),Cs=r("p"),Cs.textContent=Xo,Bt=o(),m(Le.$$.fragment),Rt=o(),h=r("div"),m(qe.$$.fragment),In=o(),xs=r("p"),xs.textContent=Eo,Sn=o(),A=r("div"),m(Ke.$$.fragment),$n=o(),ks=r("p"),ks.textContent=Go,zn=o(),Q=r("div"),m(Oe.$$.fragment),Wn=o(),Zs=r("p"),Zs.textContent=Ao,Nn=o(),Y=r("div"),m(es.$$.fragment),Bn=o(),Is=r("p"),Is.textContent=Qo,Rn=o(),H=r("div"),m(ss.$$.fragment),Xn=o(),Ss=r("p"),Ss.textContent=Yo,En=o(),P=r("div"),m(ts.$$.fragment),Gn=o(),$s=r("p"),$s.textContent=Ho,An=o(),F=r("div"),m(ns.$$.fragment),Qn=o(),zs=r("p"),zs.textContent=Po,Yn=o(),D=r("div"),m(os.$$.fragment),Hn=o(),Ws=r("p"),Ws.textContent=Fo,Pn=o(),L=r("div"),m(as.$$.fragment),Fn=o(),Ns=r("p"),Ns.textContent=Do,Dn=o(),q=r("div"),m(rs.$$.fragment),Ln=o(),Bs=r("p"),Bs.textContent=Lo,qn=o(),K=r("div"),m(is.$$.fragment),Kn=o(),Rs=r("p"),Rs.textContent=qo,On=o(),O=r("div"),m(ls.$$.fragment),eo=o(),Xs=r("p"),Xs.textContent=Ko,so=o(),ee=r("div"),m(ms.$$.fragment),to=o(),Es=r("p"),Es.textContent=Oo,no=o(),se=r("div"),m(ds.$$.fragment),oo=o(),Gs=r("p"),Gs.textContent=ea,Xt=o(),m(cs.$$.fragment),Et=o(),w=r("div"),m(ps.$$.fragment),ao=o(),As=r("p"),As.textContent=sa,ro=o(),Qs=r("p"),Qs.innerHTML=ta,io=o(),Ys=r("p"),Ys.innerHTML=na,lo=o(),te=r("div"),m(fs.$$.fragment),mo=o(),Hs=r("p"),Hs.textContent=oa,co=o(),ne=r("div"),m(_s.$$.fragment),po=o(),Ps=r("p"),Ps.textContent=aa,Gt=o(),m(gs.$$.fragment),At=o(),qs=r("p"),this.h()},l(e){const n=ca("svelte-u9bgzb",document.head);T=i(n,"META",{name:!0,content:!0}),n.forEach(t),oe=a(e),$=i(e,"P",{}),y($).forEach(t),x=a(e),V=i(e,"DIV",{style:!0,"data-svelte-h":!0}),g(V)!=="svelte-1dwwnh7"&&(V.innerHTML=j),W=a(e),d(ae.$$.fragment,e),Os=a(e),d(re.$$.fragment,e),et=a(e),ie=i(e,"P",{"data-svelte-h":!0}),g(ie)!=="svelte-1yalyum"&&(ie.innerHTML=fo),st=a(e),le=i(e,"P",{"data-svelte-h":!0}),g(le)!=="svelte-sl1nob"&&(le.textContent=_o),tt=a(e),me=i(e,"P",{"data-svelte-h":!0}),g(me)!=="svelte-vowmqa"&&(me.innerHTML=go),nt=a(e),de=i(e,"P",{"data-svelte-h":!0}),g(de)!=="svelte-vfdo9a"&&(de.textContent=ho),ot=a(e),ce=i(e,"P",{"data-svelte-h":!0}),g(ce)!=="svelte-1grlhd0"&&(ce.innerHTML=uo),at=a(e),pe=i(e,"P",{"data-svelte-h":!0}),g(pe)!=="svelte-axv494"&&(pe.textContent=yo),rt=a(e),fe=i(e,"UL",{"data-svelte-h":!0}),g(fe)!=="svelte-htibf2"&&(fe.innerHTML=Mo),it=a(e),_e=i(e,"P",{"data-svelte-h":!0}),g(_e)!=="svelte-13u8sgy"&&(_e.innerHTML=vo),lt=a(e),d(ge.$$.fragment,e),mt=a(e),d(he.$$.fragment,e),dt=a(e),ue=i(e,"P",{"data-svelte-h":!0}),g(ue)!=="svelte-1uq71e6"&&(ue.textContent=bo),ct=a(e),d(ye.$$.fragment,e),pt=a(e),d(Me.$$.fragment,e),ft=a(e),d(ve.$$.fragment,e),_t=a(e),be=i(e,"P",{"data-svelte-h":!0}),g(be)!=="svelte-1ebwocb"&&(be.textContent=To),gt=a(e),d(Te.$$.fragment,e),ht=a(e),d(je.$$.fragment,e),ut=a(e),we=i(e,"P",{"data-svelte-h":!0}),g(we)!=="svelte-w6w9xe"&&(we.textContent=jo),yt=a(e),d(Ue.$$.fragment,e),Mt=a(e),d(Je.$$.fragment,e),vt=a(e),Ve=i(e,"P",{"data-svelte-h":!0}),g(Ve)!=="svelte-16ncsu"&&(Ve.textContent=wo),bt=a(e),d(Ce.$$.fragment,e),Tt=a(e),d(xe.$$.fragment,e),jt=a(e),ke=i(e,"P",{"data-svelte-h":!0}),g(ke)!=="svelte-cu4k4y"&&(ke.textContent=Uo),wt=a(e),d(Ze.$$.fragment,e),Ut=a(e),d(Ie.$$.fragment,e),Jt=a(e),Se=i(e,"P",{"data-svelte-h":!0}),g(Se)!=="svelte-r7mpci"&&(Se.textContent=Jo),Vt=a(e),$e=i(e,"UL",{"data-svelte-h":!0}),g($e)!=="svelte-yibdzd"&&($e.innerHTML=Vo),Ct=a(e),d(ze.$$.fragment,e),xt=a(e),C=i(e,"DIV",{class:!0});var S=y(C);d(We.$$.fragment,S),fn=a(S),hs=i(S,"P",{"data-svelte-h":!0}),g(hs)!=="svelte-18z3dap"&&(hs.innerHTML=Co),_n=a(S),us=i(S,"P",{"data-svelte-h":!0}),g(us)!=="svelte-1ek1ss9"&&(us.innerHTML=xo),gn=a(S),d(N.$$.fragment,S),S.forEach(t),kt=a(e),d(Ne.$$.fragment,e),Zt=a(e),Z=i(e,"DIV",{class:!0});var Fs=y(Z);d(Be.$$.fragment,Fs),hn=a(Fs),ys=i(Fs,"P",{"data-svelte-h":!0}),g(ys)!=="svelte-1a4bmye"&&(ys.innerHTML=ko),un=a(Fs),Ms=i(Fs,"P",{"data-svelte-h":!0}),g(Ms)!=="svelte-1ek1ss9"&&(Ms.innerHTML=Zo),Fs.forEach(t),It=a(e),d(Re.$$.fragment,e),St=a(e),I=i(e,"DIV",{class:!0});var Ds=y(I);d(Xe.$$.fragment,Ds),yn=a(Ds),vs=i(Ds,"P",{"data-svelte-h":!0}),g(vs)!=="svelte-1catk99"&&(vs.innerHTML=Io),Mn=a(Ds),bs=i(Ds,"P",{"data-svelte-h":!0}),g(bs)!=="svelte-1ek1ss9"&&(bs.innerHTML=So),Ds.forEach(t),$t=a(e),d(Ee.$$.fragment,e),zt=a(e),b=i(e,"DIV",{class:!0});var J=y(b);d(Ge.$$.fragment,J),vn=a(J),Ts=i(J,"P",{"data-svelte-h":!0}),g(Ts)!=="svelte-1er9ay4"&&(Ts.textContent=$o),bn=a(J),js=i(J,"P",{"data-svelte-h":!0}),g(js)!=="svelte-chdvof"&&(js.innerHTML=zo),Tn=a(J),B=i(J,"DIV",{class:!0});var Yt=y(B);d(Ae.$$.fragment,Yt),jn=a(Yt),ws=i(Yt,"P",{"data-svelte-h":!0}),g(ws)!=="svelte-gvuv6i"&&(ws.innerHTML=Wo),Yt.forEach(t),wn=a(J),R=i(J,"DIV",{class:!0});var Ht=y(R);d(Qe.$$.fragment,Ht),Un=a(Ht),Us=i(Ht,"P",{"data-svelte-h":!0}),g(Us)!=="svelte-juomob"&&(Us.textContent=No),Ht.forEach(t),Jn=a(J),X=i(J,"DIV",{class:!0});var Pt=y(X);d(Ye.$$.fragment,Pt),Vn=a(Pt),Js=i(Pt,"P",{"data-svelte-h":!0}),g(Js)!=="svelte-xjlnw9"&&(Js.innerHTML=Bo),Pt.forEach(t),Cn=a(J),E=i(J,"DIV",{class:!0});var Ft=y(E);d(He.$$.fragment,Ft),xn=a(Ft),Vs=i(Ft,"P",{"data-svelte-h":!0}),g(Vs)!=="svelte-6u1pfq"&&(Vs.textContent=Ro),Ft.forEach(t),J.forEach(t),Wt=a(e),d(Pe.$$.fragment,e),Nt=a(e),z=i(e,"DIV",{class:!0});var Dt=y(z);d(Fe.$$.fragment,Dt),kn=a(Dt),G=i(Dt,"DIV",{class:!0});var Lt=y(G);d(De.$$.fragment,Lt),Zn=a(Lt),Cs=i(Lt,"P",{"data-svelte-h":!0}),g(Cs)!=="svelte-juomob"&&(Cs.textContent=Xo),Lt.forEach(t),Dt.forEach(t),Bt=a(e),d(Le.$$.fragment,e),Rt=a(e),h=i(e,"DIV",{class:!0});var u=y(h);d(qe.$$.fragment,u),In=a(u),xs=i(u,"P",{"data-svelte-h":!0}),g(xs)!=="svelte-xz8zt4"&&(xs.textContent=Eo),Sn=a(u),A=i(u,"DIV",{class:!0});var qt=y(A);d(Ke.$$.fragment,qt),$n=a(qt),ks=i(qt,"P",{"data-svelte-h":!0}),g(ks)!=="svelte-1y09788"&&(ks.textContent=Go),qt.forEach(t),zn=a(u),Q=i(u,"DIV",{class:!0});var Kt=y(Q);d(Oe.$$.fragment,Kt),Wn=a(Kt),Zs=i(Kt,"P",{"data-svelte-h":!0}),g(Zs)!=="svelte-10znxuu"&&(Zs.textContent=Ao),Kt.forEach(t),Nn=a(u),Y=i(u,"DIV",{class:!0});var Ot=y(Y);d(es.$$.fragment,Ot),Bn=a(Ot),Is=i(Ot,"P",{"data-svelte-h":!0}),g(Is)!=="svelte-oj43zw"&&(Is.textContent=Qo),Ot.forEach(t),Rn=a(u),H=i(u,"DIV",{class:!0});var en=y(H);d(ss.$$.fragment,en),Xn=a(en),Ss=i(en,"P",{"data-svelte-h":!0}),g(Ss)!=="svelte-h5k05i"&&(Ss.textContent=Yo),en.forEach(t),En=a(u),P=i(u,"DIV",{class:!0});var sn=y(P);d(ts.$$.fragment,sn),Gn=a(sn),$s=i(sn,"P",{"data-svelte-h":!0}),g($s)!=="svelte-18oez56"&&($s.textContent=Ho),sn.forEach(t),An=a(u),F=i(u,"DIV",{class:!0});var tn=y(F);d(ns.$$.fragment,tn),Qn=a(tn),zs=i(tn,"P",{"data-svelte-h":!0}),g(zs)!=="svelte-ybxc4t"&&(zs.textContent=Po),tn.forEach(t),Yn=a(u),D=i(u,"DIV",{class:!0});var nn=y(D);d(os.$$.fragment,nn),Hn=a(nn),Ws=i(nn,"P",{"data-svelte-h":!0}),g(Ws)!=="svelte-hrd90g"&&(Ws.textContent=Fo),nn.forEach(t),Pn=a(u),L=i(u,"DIV",{class:!0});var on=y(L);d(as.$$.fragment,on),Fn=a(on),Ns=i(on,"P",{"data-svelte-h":!0}),g(Ns)!=="svelte-1dqwq2e"&&(Ns.textContent=Do),on.forEach(t),Dn=a(u),q=i(u,"DIV",{class:!0});var an=y(q);d(rs.$$.fragment,an),Ln=a(an),Bs=i(an,"P",{"data-svelte-h":!0}),g(Bs)!=="svelte-3cstnl"&&(Bs.textContent=Lo),an.forEach(t),qn=a(u),K=i(u,"DIV",{class:!0});var rn=y(K);d(is.$$.fragment,rn),Kn=a(rn),Rs=i(rn,"P",{"data-svelte-h":!0}),g(Rs)!=="svelte-xnda5h"&&(Rs.textContent=qo),rn.forEach(t),On=a(u),O=i(u,"DIV",{class:!0});var ln=y(O);d(ls.$$.fragment,ln),eo=a(ln),Xs=i(ln,"P",{"data-svelte-h":!0}),g(Xs)!=="svelte-ry9w8d"&&(Xs.textContent=Ko),ln.forEach(t),so=a(u),ee=i(u,"DIV",{class:!0});var mn=y(ee);d(ms.$$.fragment,mn),to=a(mn),Es=i(mn,"P",{"data-svelte-h":!0}),g(Es)!=="svelte-1erdffc"&&(Es.textContent=Oo),mn.forEach(t),no=a(u),se=i(u,"DIV",{class:!0});var dn=y(se);d(ds.$$.fragment,dn),oo=a(dn),Gs=i(dn,"P",{"data-svelte-h":!0}),g(Gs)!=="svelte-teudi0"&&(Gs.textContent=ea),dn.forEach(t),u.forEach(t),Xt=a(e),d(cs.$$.fragment,e),Et=a(e),w=i(e,"DIV",{class:!0});var k=y(w);d(ps.$$.fragment,k),ao=a(k),As=i(k,"P",{"data-svelte-h":!0}),g(As)!=="svelte-an35c"&&(As.textContent=sa),ro=a(k),Qs=i(k,"P",{"data-svelte-h":!0}),g(Qs)!=="svelte-q52n56"&&(Qs.innerHTML=ta),io=a(k),Ys=i(k,"P",{"data-svelte-h":!0}),g(Ys)!=="svelte-hswkmf"&&(Ys.innerHTML=na),lo=a(k),te=i(k,"DIV",{class:!0});var cn=y(te);d(fs.$$.fragment,cn),mo=a(cn),Hs=i(cn,"P",{"data-svelte-h":!0}),g(Hs)!=="svelte-1uac1up"&&(Hs.textContent=oa),cn.forEach(t),co=a(k),ne=i(k,"DIV",{class:!0});var pn=y(ne);d(_s.$$.fragment,pn),po=a(pn),Ps=i(pn,"P",{"data-svelte-h":!0}),g(Ps)!=="svelte-1wce5r2"&&(Ps.textContent=aa),pn.forEach(t),k.forEach(t),Gt=a(e),d(gs.$$.fragment,e),At=a(e),qs=i(e,"P",{}),y(qs).forEach(t),this.h()},h(){M(T,"name","hf:doc:metadata"),M(T,"content",ua),pa(V,"float","right"),M(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){s(document.head,T),l(e,oe,n),l(e,$,n),l(e,x,n),l(e,V,n),l(e,W,n),c(ae,e,n),l(e,Os,n),c(re,e,n),l(e,et,n),l(e,ie,n),l(e,st,n),l(e,le,n),l(e,tt,n),l(e,me,n),l(e,nt,n),l(e,de,n),l(e,ot,n),l(e,ce,n),l(e,at,n),l(e,pe,n),l(e,rt,n),l(e,fe,n),l(e,it,n),l(e,_e,n),l(e,lt,n),c(ge,e,n),l(e,mt,n),c(he,e,n),l(e,dt,n),l(e,ue,n),l(e,ct,n),c(ye,e,n),l(e,pt,n),c(Me,e,n),l(e,ft,n),c(ve,e,n),l(e,_t,n),l(e,be,n),l(e,gt,n),c(Te,e,n),l(e,ht,n),c(je,e,n),l(e,ut,n),l(e,we,n),l(e,yt,n),c(Ue,e,n),l(e,Mt,n),c(Je,e,n),l(e,vt,n),l(e,Ve,n),l(e,bt,n),c(Ce,e,n),l(e,Tt,n),c(xe,e,n),l(e,jt,n),l(e,ke,n),l(e,wt,n),c(Ze,e,n),l(e,Ut,n),c(Ie,e,n),l(e,Jt,n),l(e,Se,n),l(e,Vt,n),l(e,$e,n),l(e,Ct,n),c(ze,e,n),l(e,xt,n),l(e,C,n),c(We,C,null),s(C,fn),s(C,hs),s(C,_n),s(C,us),s(C,gn),c(N,C,null),l(e,kt,n),c(Ne,e,n),l(e,Zt,n),l(e,Z,n),c(Be,Z,null),s(Z,hn),s(Z,ys),s(Z,un),s(Z,Ms),l(e,It,n),c(Re,e,n),l(e,St,n),l(e,I,n),c(Xe,I,null),s(I,yn),s(I,vs),s(I,Mn),s(I,bs),l(e,$t,n),c(Ee,e,n),l(e,zt,n),l(e,b,n),c(Ge,b,null),s(b,vn),s(b,Ts),s(b,bn),s(b,js),s(b,Tn),s(b,B),c(Ae,B,null),s(B,jn),s(B,ws),s(b,wn),s(b,R),c(Qe,R,null),s(R,Un),s(R,Us),s(b,Jn),s(b,X),c(Ye,X,null),s(X,Vn),s(X,Js),s(b,Cn),s(b,E),c(He,E,null),s(E,xn),s(E,Vs),l(e,Wt,n),c(Pe,e,n),l(e,Nt,n),l(e,z,n),c(Fe,z,null),s(z,kn),s(z,G),c(De,G,null),s(G,Zn),s(G,Cs),l(e,Bt,n),c(Le,e,n),l(e,Rt,n),l(e,h,n),c(qe,h,null),s(h,In),s(h,xs),s(h,Sn),s(h,A),c(Ke,A,null),s(A,$n),s(A,ks),s(h,zn),s(h,Q),c(Oe,Q,null),s(Q,Wn),s(Q,Zs),s(h,Nn),s(h,Y),c(es,Y,null),s(Y,Bn),s(Y,Is),s(h,Rn),s(h,H),c(ss,H,null),s(H,Xn),s(H,Ss),s(h,En),s(h,P),c(ts,P,null),s(P,Gn),s(P,$s),s(h,An),s(h,F),c(ns,F,null),s(F,Qn),s(F,zs),s(h,Yn),s(h,D),c(os,D,null),s(D,Hn),s(D,Ws),s(h,Pn),s(h,L),c(as,L,null),s(L,Fn),s(L,Ns),s(h,Dn),s(h,q),c(rs,q,null),s(q,Ln),s(q,Bs),s(h,qn),s(h,K),c(is,K,null),s(K,Kn),s(K,Rs),s(h,On),s(h,O),c(ls,O,null),s(O,eo),s(O,Xs),s(h,so),s(h,ee),c(ms,ee,null),s(ee,to),s(ee,Es),s(h,no),s(h,se),c(ds,se,null),s(se,oo),s(se,Gs),l(e,Xt,n),c(cs,e,n),l(e,Et,n),l(e,w,n),c(ps,w,null),s(w,ao),s(w,As),s(w,ro),s(w,Qs),s(w,io),s(w,Ys),s(w,lo),s(w,te),c(fs,te,null),s(te,mo),s(te,Hs),s(w,co),s(w,ne),c(_s,ne,null),s(ne,po),s(ne,Ps),l(e,Gt,n),c(gs,e,n),l(e,At,n),l(e,qs,n),Qt=!0},p(e,[n]){const S={};n&2&&(S.$$scope={dirty:n,ctx:e}),N.$set(S)},i(e){Qt||(p(ae.$$.fragment,e),p(re.$$.fragment,e),p(ge.$$.fragment,e),p(he.$$.fragment,e),p(ye.$$.fragment,e),p(Me.$$.fragment,e),p(ve.$$.fragment,e),p(Te.$$.fragment,e),p(je.$$.fragment,e),p(Ue.$$.fragment,e),p(Je.$$.fragment,e),p(Ce.$$.fragment,e),p(xe.$$.fragment,e),p(Ze.$$.fragment,e),p(Ie.$$.fragment,e),p(ze.$$.fragment,e),p(We.$$.fragment,e),p(N.$$.fragment,e),p(Ne.$$.fragment,e),p(Be.$$.fragment,e),p(Re.$$.fragment,e),p(Xe.$$.fragment,e),p(Ee.$$.fragment,e),p(Ge.$$.fragment,e),p(Ae.$$.fragment,e),p(Qe.$$.fragment,e),p(Ye.$$.fragment,e),p(He.$$.fragment,e),p(Pe.$$.fragment,e),p(Fe.$$.fragment,e),p(De.$$.fragment,e),p(Le.$$.fragment,e),p(qe.$$.fragment,e),p(Ke.$$.fragment,e),p(Oe.$$.fragment,e),p(es.$$.fragment,e),p(ss.$$.fragment,e),p(ts.$$.fragment,e),p(ns.$$.fragment,e),p(os.$$.fragment,e),p(as.$$.fragment,e),p(rs.$$.fragment,e),p(is.$$.fragment,e),p(ls.$$.fragment,e),p(ms.$$.fragment,e),p(ds.$$.fragment,e),p(cs.$$.fragment,e),p(ps.$$.fragment,e),p(fs.$$.fragment,e),p(_s.$$.fragment,e),p(gs.$$.fragment,e),Qt=!0)},o(e){f(ae.$$.fragment,e),f(re.$$.fragment,e),f(ge.$$.fragment,e),f(he.$$.fragment,e),f(ye.$$.fragment,e),f(Me.$$.fragment,e),f(ve.$$.fragment,e),f(Te.$$.fragment,e),f(je.$$.fragment,e),f(Ue.$$.fragment,e),f(Je.$$.fragment,e),f(Ce.$$.fragment,e),f(xe.$$.fragment,e),f(Ze.$$.fragment,e),f(Ie.$$.fragment,e),f(ze.$$.fragment,e),f(We.$$.fragment,e),f(N.$$.fragment,e),f(Ne.$$.fragment,e),f(Be.$$.fragment,e),f(Re.$$.fragment,e),f(Xe.$$.fragment,e),f(Ee.$$.fragment,e),f(Ge.$$.fragment,e),f(Ae.$$.fragment,e),f(Qe.$$.fragment,e),f(Ye.$$.fragment,e),f(He.$$.fragment,e),f(Pe.$$.fragment,e),f(Fe.$$.fragment,e),f(De.$$.fragment,e),f(Le.$$.fragment,e),f(qe.$$.fragment,e),f(Ke.$$.fragment,e),f(Oe.$$.fragment,e),f(es.$$.fragment,e),f(ss.$$.fragment,e),f(ts.$$.fragment,e),f(ns.$$.fragment,e),f(os.$$.fragment,e),f(as.$$.fragment,e),f(rs.$$.fragment,e),f(is.$$.fragment,e),f(ls.$$.fragment,e),f(ms.$$.fragment,e),f(ds.$$.fragment,e),f(cs.$$.fragment,e),f(ps.$$.fragment,e),f(fs.$$.fragment,e),f(_s.$$.fragment,e),f(gs.$$.fragment,e),Qt=!1},d(e){e&&(t(oe),t($),t(x),t(V),t(W),t(Os),t(et),t(ie),t(st),t(le),t(tt),t(me),t(nt),t(de),t(ot),t(ce),t(at),t(pe),t(rt),t(fe),t(it),t(_e),t(lt),t(mt),t(dt),t(ue),t(ct),t(pt),t(ft),t(_t),t(be),t(gt),t(ht),t(ut),t(we),t(yt),t(Mt),t(vt),t(Ve),t(bt),t(Tt),t(jt),t(ke),t(wt),t(Ut),t(Jt),t(Se),t(Vt),t($e),t(Ct),t(xt),t(C),t(kt),t(Zt),t(Z),t(It),t(St),t(I),t($t),t(zt),t(b),t(Wt),t(Nt),t(z),t(Bt),t(Rt),t(h),t(Xt),t(Et),t(w),t(Gt),t(At),t(qs)),t(T),_(ae,e),_(re,e),_(ge,e),_(he,e),_(ye,e),_(Me,e),_(ve,e),_(Te,e),_(je,e),_(Ue,e),_(Je,e),_(Ce,e),_(xe,e),_(Ze,e),_(Ie,e),_(ze,e),_(We),_(N),_(Ne,e),_(Be),_(Re,e),_(Xe),_(Ee,e),_(Ge),_(Ae),_(Qe),_(Ye),_(He),_(Pe,e),_(Fe),_(De),_(Le,e),_(qe),_(Ke),_(Oe),_(es),_(ss),_(ts),_(ns),_(os),_(as),_(rs),_(is),_(ls),_(ms),_(ds),_(cs,e),_(ps),_(fs),_(_s),_(gs,e)}}}const ua='{"title":"SAM2 Video","local":"sam2-video","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[{"title":"Video Segmentation and Tracking","local":"video-segmentation-and-tracking","sections":[{"title":"Basic Video Tracking","local":"basic-video-tracking","sections":[],"depth":4},{"title":"Multi-Object Video Tracking","local":"multi-object-video-tracking","sections":[],"depth":4},{"title":"Refining Video Segmentation","local":"refining-video-segmentation","sections":[],"depth":4}],"depth":3},{"title":"Streaming Video Inference","local":"streaming-video-inference","sections":[{"title":"Video Batch Processing for Multiple Objects","local":"video-batch-processing-for-multiple-objects","sections":[],"depth":4}],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Sam2VideoConfig","local":"transformers.Sam2VideoConfig","sections":[],"depth":2},{"title":"Sam2VideoMaskDecoderConfig","local":"transformers.Sam2VideoMaskDecoderConfig","sections":[],"depth":2},{"title":"Sam2VideoPromptEncoderConfig","local":"transformers.Sam2VideoPromptEncoderConfig","sections":[],"depth":2},{"title":"Sam2VideoProcessor","local":"transformers.Sam2VideoProcessor","sections":[],"depth":2},{"title":"Sam2VideoVideoProcessor","local":"transformers.Sam2VideoVideoProcessor","sections":[],"depth":2},{"title":"Sam2VideoInferenceSession","local":"transformers.Sam2VideoInferenceSession","sections":[],"depth":2},{"title":"Sam2VideoModel","local":"transformers.Sam2VideoModel","sections":[],"depth":2}],"depth":1}';function ya(Ks){return ia(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ua extends ma{constructor(T){super(),da(this,T,ya,ha,ra,{})}}export{Ua as component};
