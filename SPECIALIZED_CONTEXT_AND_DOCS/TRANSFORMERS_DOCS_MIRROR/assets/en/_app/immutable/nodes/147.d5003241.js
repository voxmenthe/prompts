import{s as et,o as tt,n as re}from"../chunks/scheduler.18a86fab.js";import{S as nt,i as st,g as h,s as p,r as f,A as ot,h as m,f as o,c as d,j as oe,x as w,u,k as ae,l as at,y as g,a as i,v as b,d as y,t as _,w as M}from"../chunks/index.98837b22.js";import{T as Se}from"../chunks/Tip.77304350.js";import{D as $e}from"../chunks/Docstring.a1ef7999.js";import{C as ve}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ke}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as je,E as rt}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as it,a as Oe}from"../chunks/HfOption.6641485e.js";function lt(T){let t,c="Click on the Depth Anything models in the right sidebar for more examples of how to apply Depth Anything to different vision tasks.";return{c(){t=h("p"),t.textContent=c},l(n){t=m(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-abaip3"&&(t.textContent=c)},m(n,l){i(n,t,l)},p:re,d(n){n&&o(t)}}}function pt(T){let t,c;return t=new ve({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlJTIwJTNEJTIwcGlwZWxpbmUodGFzayUzRCUyMmRlcHRoLWVzdGltYXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMkxpaGVZb3VuZyUyRmRlcHRoLWFueXRoaW5nLWJhc2UtaGYlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwZGV2aWNlJTNEMCklMEFwaXBlKCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIpJTVCJTIyZGVwdGglMjIlNUQ=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipe = pipeline(task=<span class="hljs-string">&quot;depth-estimation&quot;</span>, model=<span class="hljs-string">&quot;LiheYoung/depth-anything-base-hf&quot;</span>, dtype=torch.bfloat16, device=<span class="hljs-number">0</span>)
pipe(<span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>)[<span class="hljs-string">&quot;depth&quot;</span>]`,wrap:!1}}),{c(){f(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,l){b(t,n,l),c=!0},p:re,i(n){c||(y(t.$$.fragment,n),c=!0)},o(n){_(t.$$.fragment,n),c=!1},d(n){M(t,n)}}}function dt(T){let t,c;return t=new ve({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWltcG9ydCUyMG51bXB5JTIwYXMlMjBucCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkxpaGVZb3VuZyUyRmRlcHRoLWFueXRoaW5nLWJhc2UtaGYlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMkxpaGVZb3VuZyUyRmRlcHRoLWFueXRoaW5nLWJhc2UtaGYlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2KSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBcG9zdF9wcm9jZXNzZWRfb3V0cHV0JTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19kZXB0aF9lc3RpbWF0aW9uKCUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMkMlMEElMjAlMjAlMjAlMjB0YXJnZXRfc2l6ZXMlM0QlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTVEJTJDJTBBKSUwQXByZWRpY3RlZF9kZXB0aCUyMCUzRCUyMHBvc3RfcHJvY2Vzc2VkX291dHB1dCU1QjAlNUQlNUIlMjJwcmVkaWN0ZWRfZGVwdGglMjIlNUQlMEFkZXB0aCUyMCUzRCUyMChwcmVkaWN0ZWRfZGVwdGglMjAtJTIwcHJlZGljdGVkX2RlcHRoLm1pbigpKSUyMCUyRiUyMChwcmVkaWN0ZWRfZGVwdGgubWF4KCklMjAtJTIwcHJlZGljdGVkX2RlcHRoLm1pbigpKSUwQWRlcHRoJTIwJTNEJTIwZGVwdGguZGV0YWNoKCkuY3B1KCkubnVtcHkoKSUyMColMjAyNTUlMEFJbWFnZS5mcm9tYXJyYXkoZGVwdGguYXN0eXBlKCUyMnVpbnQ4JTIyKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation

image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;LiheYoung/depth-anything-base-hf&quot;</span>)
model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;LiheYoung/depth-anything-base-hf&quot;</span>, dtype=torch.bfloat16)
url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

post_processed_output = image_processor.post_process_depth_estimation(
    outputs,
    target_sizes=[(image.height, image.width)],
)
predicted_depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
depth = (predicted_depth - predicted_depth.<span class="hljs-built_in">min</span>()) / (predicted_depth.<span class="hljs-built_in">max</span>() - predicted_depth.<span class="hljs-built_in">min</span>())
depth = depth.detach().cpu().numpy() * <span class="hljs-number">255</span>
Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint8&quot;</span>))`,wrap:!1}}),{c(){f(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,l){b(t,n,l),c=!0},p:re,i(n){c||(y(t.$$.fragment,n),c=!0)},o(n){_(t.$$.fragment,n),c=!1},d(n){M(t,n)}}}function ct(T){let t,c,n,l;return t=new Oe({props:{id:"usage",option:"Pipeline",$$slots:{default:[pt]},$$scope:{ctx:T}}}),n=new Oe({props:{id:"usage",option:"AutoModel",$$slots:{default:[dt]},$$scope:{ctx:T}}}),{c(){f(t.$$.fragment),c=p(),f(n.$$.fragment)},l(r){u(t.$$.fragment,r),c=d(r),u(n.$$.fragment,r)},m(r,a){b(t,r,a),i(r,c,a),b(n,r,a),l=!0},p(r,a){const $={};a&2&&($.$$scope={dirty:a,ctx:r}),t.$set($);const C={};a&2&&(C.$$scope={dirty:a,ctx:r}),n.$set(C)},i(r){l||(y(t.$$.fragment,r),y(n.$$.fragment,r),l=!0)},o(r){_(t.$$.fragment,r),_(n.$$.fragment,r),l=!1},d(r){r&&o(c),M(t,r),M(n,r)}}}function ht(T){let t,c="Example:",n,l,r;return l=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoQW55dGhpbmdDb25maWclMkMlMjBEZXB0aEFueXRoaW5nRm9yRGVwdGhFc3RpbWF0aW9uJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMERlcHRoQW55dGhpbmclMjBzbWFsbCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBEZXB0aEFueXRoaW5nQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwRGVwdGhBbnl0aGluZyUyMHNtYWxsJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBEZXB0aEFueXRoaW5nRm9yRGVwdGhFc3RpbWF0aW9uKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthAnythingConfig, DepthAnythingForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DepthAnything small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DepthAnythingConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the DepthAnything small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthAnythingForDepthEstimation(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=h("p"),t.textContent=c,n=p(),f(l.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-11lpom8"&&(t.textContent=c),n=d(a),u(l.$$.fragment,a)},m(a,$){i(a,t,$),i(a,n,$),b(l,a,$),r=!0},p:re,i(a){r||(y(l.$$.fragment,a),r=!0)},o(a){_(l.$$.fragment,a),r=!1},d(a){a&&(o(t),o(n)),M(l,a)}}}function mt(T){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=h("p"),t.innerHTML=c},l(n){t=m(n,"P",{"data-svelte-h":!0}),w(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,l){i(n,t,l)},p:re,d(n){n&&o(t)}}}function gt(T){let t,c="Examples:",n,l,r;return l=new ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBaW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJMaWhlWW91bmclMkZkZXB0aC1hbnl0aGluZy1zbWFsbC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyTGloZVlvdW5nJTJGZGVwdGgtYW55dGhpbmctc21hbGwtaGYlMjIpJTBBJTBBJTIzJTIwcHJlcGFyZSUyMGltYWdlJTIwZm9yJTIwdGhlJTIwbW9kZWwlMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwaW50ZXJwb2xhdGUlMjB0byUyMG9yaWdpbmFsJTIwc2l6ZSUwQXBvc3RfcHJvY2Vzc2VkX291dHB1dCUyMCUzRCUyMGltYWdlX3Byb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfZGVwdGhfZXN0aW1hdGlvbiglMEElMjAlMjAlMjAlMjBvdXRwdXRzJTJDJTBBJTIwJTIwJTIwJTIwdGFyZ2V0X3NpemVzJTNEJTVCKGltYWdlLmhlaWdodCUyQyUyMGltYWdlLndpZHRoKSU1RCUyQyUwQSklMEElMEElMjMlMjB2aXN1YWxpemUlMjB0aGUlMjBwcmVkaWN0aW9uJTBBcHJlZGljdGVkX2RlcHRoJTIwJTNEJTIwcG9zdF9wcm9jZXNzZWRfb3V0cHV0JTVCMCU1RCU1QiUyMnByZWRpY3RlZF9kZXB0aCUyMiU1RCUwQWRlcHRoJTIwJTNEJTIwcHJlZGljdGVkX2RlcHRoJTIwKiUyMDI1NSUyMCUyRiUyMHByZWRpY3RlZF9kZXB0aC5tYXgoKSUwQWRlcHRoJTIwJTNEJTIwZGVwdGguZGV0YWNoKCkuY3B1KCkubnVtcHkoKSUwQWRlcHRoJTIwJTNEJTIwSW1hZ2UuZnJvbWFycmF5KGRlcHRoLmFzdHlwZSglMjJ1aW50OCUyMikp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;LiheYoung/depth-anything-small-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;LiheYoung/depth-anything-small-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>post_processed_output = image_processor.post_process_depth_estimation(
<span class="hljs-meta">... </span>    outputs,
<span class="hljs-meta">... </span>    target_sizes=[(image.height, image.width)],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># visualize the prediction</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = predicted_depth * <span class="hljs-number">255</span> / predicted_depth.<span class="hljs-built_in">max</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth.detach().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint8&quot;</span>))`,wrap:!1}}),{c(){t=h("p"),t.textContent=c,n=p(),f(l.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),w(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),u(l.$$.fragment,a)},m(a,$){i(a,t,$),i(a,n,$),b(l,a,$),r=!0},p:re,i(a){r||(y(l.$$.fragment,a),r=!0)},o(a){_(l.$$.fragment,a),r=!1},d(a){a&&(o(t),o(n)),M(l,a)}}}function ft(T){let t,c,n,l,r,a="<em>This model was released on 2024-01-19 and added to Hugging Face Transformers on 2024-01-25.</em>",$,C,Ie='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',le,G,pe,I,Fe='<a href="https://huggingface.co/papers/2401.10891" rel="nofollow">Depth Anything</a> is designed to be a foundation model for monocular depth estimation (MDE). It is jointly trained on labeled and ~62M unlabeled images to enhance the dataset. It uses a pretrained <a href="./dinov2">DINOv2</a> model as an image encoder to inherit its existing rich semantic priors, and <a href="./dpt">DPT</a> as the decoder. A teacher model is trained on unlabeled images to create pseudo-labels. The student model is trained on a combination of the pseudo-labels and labeled images. To improve the student model’s performance, strong perturbations are added to the unlabeled images to challenge the student model to learn more visual knowledge from the image.',de,F,xe='You can find all the original Depth Anything checkpoints under the <a href="https://huggingface.co/collections/LiheYoung/depth-anything-release-65b317de04eec72abf6b55aa" rel="nofollow">Depth Anything</a> collection.',ce,B,he,x,ze='The example below demonstrates how to obtain a depth map with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',me,U,ge,z,fe,H,He='<li><a href="./depth_anything_v2">DepthAnythingV2</a>, released in June 2024, uses the same architecture as Depth Anything and is compatible with all code examples and existing workflows. It uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.</li>',ue,V,be,j,X,Je,S,Ve=`This is the configuration class to store the configuration of a <code>DepthAnythingModel</code>. It is used to instantiate a DepthAnything
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the DepthAnything
<a href="https://huggingface.co/LiheYoung/depth-anything-small-hf" rel="nofollow">LiheYoung/depth-anything-small-hf</a> architecture.`,ke,K,Xe=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,We,D,Ce,R,N,Ze,O,Ne=`Serializes this instance to a Python dictionary. Override the default <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict">to_dict()</a>. Returns:
<code>dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,`,ye,L,_e,v,Y,Be,ee,Le="Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.",Ue,te,Ye=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,De,ne,Pe=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Re,W,P,Ae,se,Qe='The <a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a> forward method, overrides the <code>__call__</code> special method.',Ee,A,Ge,E,Me,Q,we,ie,Te;return G=new je({props:{title:"Depth Anything",local:"depth-anything",headingTag:"h1"}}),B=new Se({props:{warning:!1,$$slots:{default:[lt]},$$scope:{ctx:T}}}),U=new it({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[ct]},$$scope:{ctx:T}}}),z=new je({props:{title:"Notes",local:"notes",headingTag:"h2"}}),V=new je({props:{title:"DepthAnythingConfig",local:"transformers.DepthAnythingConfig",headingTag:"h2"}}),X=new $e({props:{name:"class transformers.DepthAnythingConfig",anchor:"transformers.DepthAnythingConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"patch_size",val:" = 14"},{name:"initializer_range",val:" = 0.02"},{name:"reassemble_hidden_size",val:" = 384"},{name:"reassemble_factors",val:" = [4, 2, 1, 0.5]"},{name:"neck_hidden_sizes",val:" = [48, 96, 192, 384]"},{name:"fusion_hidden_size",val:" = 64"},{name:"head_in_index",val:" = -1"},{name:"head_hidden_size",val:" = 32"},{name:"depth_estimation_type",val:" = 'relative'"},{name:"max_depth",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DepthAnythingConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>Union[dict[str, Any], PretrainedConfig]</code>, <em>optional</em>) &#x2014;
The configuration of the backbone model. Only used in case <code>is_hybrid</code> is <code>True</code> or in case you want to
leverage the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a> API.`,name:"backbone_config"},{anchor:"transformers.DepthAnythingConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.DepthAnythingConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.DepthAnythingConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the <code>timm</code> library for the backbone. If set to <code>False</code>, will use the <a href="/docs/transformers/v4.56.2/en/main_classes/backbones#transformers.AutoBackbone">AutoBackbone</a>
API.`,name:"use_timm_backbone"},{anchor:"transformers.DepthAnythingConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.DepthAnythingConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size of the patches to extract from the backbone features.`,name:"patch_size"},{anchor:"transformers.DepthAnythingConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DepthAnythingConfig.reassemble_hidden_size",description:`<strong>reassemble_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
The number of input channels of the reassemble layers.`,name:"reassemble_hidden_size"},{anchor:"transformers.DepthAnythingConfig.reassemble_factors",description:`<strong>reassemble_factors</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 1, 0.5]</code>) &#x2014;
The up/downsampling factors of the reassemble layers.`,name:"reassemble_factors"},{anchor:"transformers.DepthAnythingConfig.neck_hidden_sizes",description:`<strong>neck_hidden_sizes</strong> (<code>list[str]</code>, <em>optional</em>, defaults to <code>[48, 96, 192, 384]</code>) &#x2014;
The hidden sizes to project to for the feature maps of the backbone.`,name:"neck_hidden_sizes"},{anchor:"transformers.DepthAnythingConfig.fusion_hidden_size",description:`<strong>fusion_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
The number of channels before fusion.`,name:"fusion_hidden_size"},{anchor:"transformers.DepthAnythingConfig.head_in_index",description:`<strong>head_in_index</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the features to use in the depth estimation head.`,name:"head_in_index"},{anchor:"transformers.DepthAnythingConfig.head_hidden_size",description:`<strong>head_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of output channels in the second convolution of the depth estimation head.`,name:"head_hidden_size"},{anchor:"transformers.DepthAnythingConfig.depth_estimation_type",description:`<strong>depth_estimation_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative&quot;</code>) &#x2014;
The type of depth estimation to use. Can be one of <code>[&quot;relative&quot;, &quot;metric&quot;]</code>.`,name:"depth_estimation_type"},{anchor:"transformers.DepthAnythingConfig.max_depth",description:`<strong>max_depth</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The maximum depth to use for the &#x201C;metric&#x201D; depth estimation head. 20 should be used for indoor models
and 80 for outdoor models. For &#x201C;relative&#x201D; depth estimation, this value is ignored.`,name:"max_depth"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_anything/configuration_depth_anything.py#L28"}}),D=new Ke({props:{anchor:"transformers.DepthAnythingConfig.example",$$slots:{default:[ht]},$$scope:{ctx:T}}}),N=new $e({props:{name:"to_dict",anchor:"transformers.DepthAnythingConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_anything/configuration_depth_anything.py#L162"}}),L=new je({props:{title:"DepthAnythingForDepthEstimation",local:"transformers.DepthAnythingForDepthEstimation",headingTag:"h2"}}),Y=new $e({props:{name:"class transformers.DepthAnythingForDepthEstimation",anchor:"transformers.DepthAnythingForDepthEstimation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.DepthAnythingForDepthEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_anything/modeling_depth_anything.py#L331"}}),P=new $e({props:{name:"forward",anchor:"transformers.DepthAnythingForDepthEstimation.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DepthAnythingForDepthEstimation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTFeatureExtractor.__call__">DPTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.DepthAnythingForDepthEstimation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth depth estimation maps for computing the loss.`,name:"labels"},{anchor:"transformers.DepthAnythingForDepthEstimation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DepthAnythingForDepthEstimation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DepthAnythingForDepthEstimation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_anything/modeling_depth_anything.py#L344",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/depth_anything#transformers.DepthAnythingConfig"
>DepthAnythingConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>) — Predicted depth for each pixel.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.DepthEstimatorOutput"
>transformers.modeling_outputs.DepthEstimatorOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),A=new Se({props:{$$slots:{default:[mt]},$$scope:{ctx:T}}}),E=new Ke({props:{anchor:"transformers.DepthAnythingForDepthEstimation.forward.example",$$slots:{default:[gt]},$$scope:{ctx:T}}}),Q=new rt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_anything.md"}}),{c(){t=h("meta"),c=p(),n=h("p"),l=p(),r=h("p"),r.innerHTML=a,$=p(),C=h("div"),C.innerHTML=Ie,le=p(),f(G.$$.fragment),pe=p(),I=h("p"),I.innerHTML=Fe,de=p(),F=h("p"),F.innerHTML=xe,ce=p(),f(B.$$.fragment),he=p(),x=h("p"),x.innerHTML=ze,me=p(),f(U.$$.fragment),ge=p(),f(z.$$.fragment),fe=p(),H=h("ul"),H.innerHTML=He,ue=p(),f(V.$$.fragment),be=p(),j=h("div"),f(X.$$.fragment),Je=p(),S=h("p"),S.innerHTML=Ve,ke=p(),K=h("p"),K.innerHTML=Xe,We=p(),f(D.$$.fragment),Ce=p(),R=h("div"),f(N.$$.fragment),Ze=p(),O=h("p"),O.innerHTML=Ne,ye=p(),f(L.$$.fragment),_e=p(),v=h("div"),f(Y.$$.fragment),Be=p(),ee=h("p"),ee.textContent=Le,Ue=p(),te=h("p"),te.innerHTML=Ye,De=p(),ne=h("p"),ne.innerHTML=Pe,Re=p(),W=h("div"),f(P.$$.fragment),Ae=p(),se=h("p"),se.innerHTML=Qe,Ee=p(),f(A.$$.fragment),Ge=p(),f(E.$$.fragment),Me=p(),f(Q.$$.fragment),we=p(),ie=h("p"),this.h()},l(e){const s=ot("svelte-u9bgzb",document.head);t=m(s,"META",{name:!0,content:!0}),s.forEach(o),c=d(e),n=m(e,"P",{}),oe(n).forEach(o),l=d(e),r=m(e,"P",{"data-svelte-h":!0}),w(r)!=="svelte-1pb2trh"&&(r.innerHTML=a),$=d(e),C=m(e,"DIV",{style:!0,"data-svelte-h":!0}),w(C)!=="svelte-wa5t4p"&&(C.innerHTML=Ie),le=d(e),u(G.$$.fragment,e),pe=d(e),I=m(e,"P",{"data-svelte-h":!0}),w(I)!=="svelte-19r763q"&&(I.innerHTML=Fe),de=d(e),F=m(e,"P",{"data-svelte-h":!0}),w(F)!=="svelte-1eiwyw0"&&(F.innerHTML=xe),ce=d(e),u(B.$$.fragment,e),he=d(e),x=m(e,"P",{"data-svelte-h":!0}),w(x)!=="svelte-15tgrhh"&&(x.innerHTML=ze),me=d(e),u(U.$$.fragment,e),ge=d(e),u(z.$$.fragment,e),fe=d(e),H=m(e,"UL",{"data-svelte-h":!0}),w(H)!=="svelte-1sfp44y"&&(H.innerHTML=He),ue=d(e),u(V.$$.fragment,e),be=d(e),j=m(e,"DIV",{class:!0});var J=oe(j);u(X.$$.fragment,J),Je=d(J),S=m(J,"P",{"data-svelte-h":!0}),w(S)!=="svelte-30ivap"&&(S.innerHTML=Ve),ke=d(J),K=m(J,"P",{"data-svelte-h":!0}),w(K)!=="svelte-1ek1ss9"&&(K.innerHTML=Xe),We=d(J),u(D.$$.fragment,J),Ce=d(J),R=m(J,"DIV",{class:!0});var q=oe(R);u(N.$$.fragment,q),Ze=d(q),O=m(q,"P",{"data-svelte-h":!0}),w(O)!=="svelte-1rfbzy4"&&(O.innerHTML=Ne),q.forEach(o),J.forEach(o),ye=d(e),u(L.$$.fragment,e),_e=d(e),v=m(e,"DIV",{class:!0});var k=oe(v);u(Y.$$.fragment,k),Be=d(k),ee=m(k,"P",{"data-svelte-h":!0}),w(ee)!=="svelte-1lyww6q"&&(ee.textContent=Le),Ue=d(k),te=m(k,"P",{"data-svelte-h":!0}),w(te)!=="svelte-q52n56"&&(te.innerHTML=Ye),De=d(k),ne=m(k,"P",{"data-svelte-h":!0}),w(ne)!=="svelte-hswkmf"&&(ne.innerHTML=Pe),Re=d(k),W=m(k,"DIV",{class:!0});var Z=oe(W);u(P.$$.fragment,Z),Ae=d(Z),se=m(Z,"P",{"data-svelte-h":!0}),w(se)!=="svelte-1je5ty7"&&(se.innerHTML=Qe),Ee=d(Z),u(A.$$.fragment,Z),Ge=d(Z),u(E.$$.fragment,Z),Z.forEach(o),k.forEach(o),Me=d(e),u(Q.$$.fragment,e),we=d(e),ie=m(e,"P",{}),oe(ie).forEach(o),this.h()},h(){ae(t,"name","hf:doc:metadata"),ae(t,"content",ut),at(C,"float","right"),ae(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ae(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ae(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ae(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){g(document.head,t),i(e,c,s),i(e,n,s),i(e,l,s),i(e,r,s),i(e,$,s),i(e,C,s),i(e,le,s),b(G,e,s),i(e,pe,s),i(e,I,s),i(e,de,s),i(e,F,s),i(e,ce,s),b(B,e,s),i(e,he,s),i(e,x,s),i(e,me,s),b(U,e,s),i(e,ge,s),b(z,e,s),i(e,fe,s),i(e,H,s),i(e,ue,s),b(V,e,s),i(e,be,s),i(e,j,s),b(X,j,null),g(j,Je),g(j,S),g(j,ke),g(j,K),g(j,We),b(D,j,null),g(j,Ce),g(j,R),b(N,R,null),g(R,Ze),g(R,O),i(e,ye,s),b(L,e,s),i(e,_e,s),i(e,v,s),b(Y,v,null),g(v,Be),g(v,ee),g(v,Ue),g(v,te),g(v,De),g(v,ne),g(v,Re),g(v,W),b(P,W,null),g(W,Ae),g(W,se),g(W,Ee),b(A,W,null),g(W,Ge),b(E,W,null),i(e,Me,s),b(Q,e,s),i(e,we,s),i(e,ie,s),Te=!0},p(e,[s]){const J={};s&2&&(J.$$scope={dirty:s,ctx:e}),B.$set(J);const q={};s&2&&(q.$$scope={dirty:s,ctx:e}),U.$set(q);const k={};s&2&&(k.$$scope={dirty:s,ctx:e}),D.$set(k);const Z={};s&2&&(Z.$$scope={dirty:s,ctx:e}),A.$set(Z);const qe={};s&2&&(qe.$$scope={dirty:s,ctx:e}),E.$set(qe)},i(e){Te||(y(G.$$.fragment,e),y(B.$$.fragment,e),y(U.$$.fragment,e),y(z.$$.fragment,e),y(V.$$.fragment,e),y(X.$$.fragment,e),y(D.$$.fragment,e),y(N.$$.fragment,e),y(L.$$.fragment,e),y(Y.$$.fragment,e),y(P.$$.fragment,e),y(A.$$.fragment,e),y(E.$$.fragment,e),y(Q.$$.fragment,e),Te=!0)},o(e){_(G.$$.fragment,e),_(B.$$.fragment,e),_(U.$$.fragment,e),_(z.$$.fragment,e),_(V.$$.fragment,e),_(X.$$.fragment,e),_(D.$$.fragment,e),_(N.$$.fragment,e),_(L.$$.fragment,e),_(Y.$$.fragment,e),_(P.$$.fragment,e),_(A.$$.fragment,e),_(E.$$.fragment,e),_(Q.$$.fragment,e),Te=!1},d(e){e&&(o(c),o(n),o(l),o(r),o($),o(C),o(le),o(pe),o(I),o(de),o(F),o(ce),o(he),o(x),o(me),o(ge),o(fe),o(H),o(ue),o(be),o(j),o(ye),o(_e),o(v),o(Me),o(we),o(ie)),o(t),M(G,e),M(B,e),M(U,e),M(z,e),M(V,e),M(X),M(D),M(N),M(L,e),M(Y),M(P),M(A),M(E),M(Q,e)}}}const ut='{"title":"Depth Anything","local":"depth-anything","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"DepthAnythingConfig","local":"transformers.DepthAnythingConfig","sections":[],"depth":2},{"title":"DepthAnythingForDepthEstimation","local":"transformers.DepthAnythingForDepthEstimation","sections":[],"depth":2}],"depth":1}';function bt(T){return tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Jt extends nt{constructor(t){super(),st(this,t,bt,ft,et,{})}}export{Jt as component};
