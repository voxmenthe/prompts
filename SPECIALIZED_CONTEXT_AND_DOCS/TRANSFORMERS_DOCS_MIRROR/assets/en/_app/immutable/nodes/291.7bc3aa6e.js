import{s as Br,o as Hr,n as S}from"../chunks/scheduler.18a86fab.js";import{S as Pr,i as Zr,g as i,s as n,r as p,A as Vr,h as l,f as o,c as s,j as x,x as c,u as m,k as M,l as Gr,y as a,a as r,v as u,d as h,t as f,w as g}from"../chunks/index.98837b22.js";import{T as Eo}from"../chunks/Tip.77304350.js";import{D as w}from"../chunks/Docstring.a1ef7999.js";import{C as be}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as ka}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as Er}from"../chunks/PipelineTag.7749150e.js";import{H as $,E as Ar}from"../chunks/getInferenceSnippets.06c2775f.js";function Rr(C){let d,k;return d=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1peHRyYWxNb2RlbCUyQyUyME1peHRyYWxDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWl4dHJhbCUyMDdCJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyME1peHRyYWxDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjBNaXh0cmFsJTIwN0IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME1peHRyYWxNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MixtralModel, MixtralConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Mixtral 7B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MixtralConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the Mixtral 7B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MixtralModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){p(d.$$.fragment)},l(_){m(d.$$.fragment,_)},m(_,T){u(d,_,T),k=!0},p:S,i(_){k||(h(d.$$.fragment,_),k=!0)},o(_){f(d.$$.fragment,_),k=!1},d(_){g(d,_)}}}function Xr(C){let d,k="<code>mistral-common</code> is the official tokenizer library for Mistral AI models. To use it, you need to install it with:",_,T,z;return T=new be({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyU1Qm1pc3RyYWwtY29tbW9uJTVE",highlighted:"pip install transformers[mistral-common]",wrap:!1}}),{c(){d=i("p"),d.innerHTML=k,_=n(),p(T.$$.fragment)},l(y){d=l(y,"P",{"data-svelte-h":!0}),c(d)!=="svelte-m7z88d"&&(d.innerHTML=k),_=s(y),m(T.$$.fragment,y)},m(y,F){r(y,d,F),r(y,_,F),u(T,y,F),z=!0},p:S,i(y){z||(h(T.$$.fragment,y),z=!0)},o(y){f(T.$$.fragment,y),z=!1},d(y){y&&(o(d),o(_)),g(T,y)}}}function Sr(C){let d,k=`If the <code>encoded_inputs</code> passed are dictionary of numpy arrays, PyTorch tensors, the
result will use the same type unless you provide a different tensor type with <code>return_tensors</code>. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-mer66"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function Dr(C){let d,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function Qr(C){let d,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function Yr(C){let d,k="Example:",_,T,z;return T=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNaXh0cmFsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbCUyMCUzRCUyME1peHRyYWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWl4dHJhbC04eDdCLXYwLjElMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWl4dHJhbC04eDdCLXYwLjElMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIySGV5JTJDJTIwYXJlJTIweW91JTIwY29uc2Npb3VzJTNGJTIwQ2FuJTIweW91JTIwdGFsayUyMHRvJTIwbWUlM0YlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIocHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBHZW5lcmF0ZSUwQWdlbmVyYXRlX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0cy5pbnB1dF9pZHMlMkMlMjBtYXhfbGVuZ3RoJTNEMzApJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZV9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSUyQyUyMGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMlM0RGYWxzZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MixtralForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MixtralForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){d=i("p"),d.textContent=k,_=n(),p(T.$$.fragment)},l(y){d=l(y,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=k),_=s(y),m(T.$$.fragment,y)},m(y,F){r(y,d,F),r(y,_,F),u(T,y,F),z=!0},p:S,i(y){z||(h(T.$$.fragment,y),z=!0)},o(y){f(T.$$.fragment,y),z=!1},d(y){y&&(o(d),o(_)),g(T,y)}}}function Or(C){let d,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function Kr(C){let d,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function ei(C){let d,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=k},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=k)},m(_,T){r(_,d,T)},p:S,d(_){_&&o(d)}}}function ti(C){let d,k,_,T,z,y="<em>This model was released on 2023-12-11 and added to Hugging Face Transformers on 2023-12-11.</em>",F,ve,Ro,D,ya='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="Tensor parallelism" src="https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&amp;logoColor=white"/>',Xo,Te,So,ke,Ma='<a href="https://huggingface.co/papers/2401.04088" rel="nofollow">Mixtral-8x7B</a> was introduced in the <a href="https://mistral.ai/news/mixtral-of-experts/" rel="nofollow">Mixtral of Experts blogpost</a> by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.',Do,ye,xa="The introduction of the blog post says:",Qo,Me,wa="<em>Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts models (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.</em>",Yo,xe,Ca='Mixtral-8x7B is the second large language model (LLM) released by <a href="https://mistral.ai/" rel="nofollow">mistral.ai</a>, after <a href="mistral">Mistral-7B</a>.',Oo,we,Ko,Ce,za="Mixtral-8x7B is a decoder-only Transformer with the following architectural choices:",en,ze,$a='<li>Mixtral is a Mixture of Experts (MoE) model with 8 experts per MLP, with a total of 45 billion parameters. To learn more about mixture-of-experts, refer to the <a href="https://huggingface.co/blog/moe" rel="nofollow">blog post</a>.</li> <li>Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length.</li>',tn,$e,Ja='The following implementation details are shared with Mistral AIâ€™s first model <a href="mistral">Mistral-7B</a>:',on,Je,qa="<li>Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens</li> <li>GQA (Grouped Query Attention) - allowing faster inference and lower cache size.</li> <li>Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens.</li>",nn,qe,Ia='For more details refer to the <a href="https://mistral.ai/news/mixtral-of-experts/" rel="nofollow">release blog post</a>.',sn,Ie,an,je,ja="<code>Mixtral-8x7B</code> is released under the Apache 2.0 license.",rn,Fe,ln,Le,Fa="The Mistral team has released 2 checkpoints:",dn,Ue,La='<li>a base model, <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" rel="nofollow">Mixtral-8x7B-v0.1</a>, which has been pre-trained to predict the next token on internet-scale data.</li> <li>an instruction tuned model, <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" rel="nofollow">Mixtral-8x7B-Instruct-v0.1</a>, which is the base model optimized for chat purposes using supervised fine-tuning (SFT) and direct preference optimization (DPO).</li>',cn,We,Ua="The base model can be used as follows:",pn,Ne,mn,Be,Wa="The instruction tuned model can be used as follows:",un,He,hn,Pe,Na='As can be seen, the instruction-tuned model requires a <a href="../chat_templating">chat template</a> to be applied to make sure the inputs are prepared in the right format.',fn,Ze,gn,Ve,Ba='The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging <a href="../perf_train_gpu_one#flash-attention-2">Flash Attention</a>, which is a faster implementation of the attention mechanism used inside the model.',_n,Ge,Ha="First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.",bn,Ee,vn,Ae,Pa='Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">flash attention repository</a>. Make also sure to load your model in half-precision (e.g. <code>torch.float16</code>)',Tn,Re,Za="To load and run a model using Flash Attention-2, refer to the snippet below:",kn,Xe,yn,Se,Mn,De,Va="Below is a expected speedup diagram that compares pure inference time between the native implementation in transformers using <code>mistralai/Mixtral-8x7B-v0.1</code> checkpoint and the Flash Attention 2 version of the model.",xn,Q,Ga='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png"/>',wn,Qe,Cn,Ye,Ea=`The current implementation supports the sliding window attention mechanism and memory efficient cache management.
To enable sliding window attention, just make sure to have a <code>flash-attn</code> version that is compatible with sliding window attention (<code>&gt;=2.3.0</code>).`,zn,Oe,Aa="The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (<code>self.config.sliding_window</code>), support batched generation only for <code>padding_side=&quot;left&quot;</code> and use the absolute position of the current token to compute the positional embedding.",$n,Ke,Jn,et,Ra='As the Mixtral model has 45 billion parameters, that would require about 90GB of GPU RAM in half precision (float16), since each parameter is stored in 2 bytes. However, one can shrink down the size of the model using <a href="../quantization">quantization</a>. If the model is quantized to 4 bits (or half a byte per parameter), a single A100 with 40GB of RAM is enough to fit the entire model, as in that case only about 27 GB of RAM is required.',qn,tt,Xa='Quantizing a model is as simple as passing a <code>quantization_config</code> to the model. Below, weâ€™ll leverage the bitsandbytes quantization library (but refer to <a href="../quantization">this page</a> for alternative quantization methods):',In,ot,jn,nt,Sa=`This model was contributed by <a href="https://huggingface.co/ybelkada" rel="nofollow">Younes Belkada</a> and <a href="https://huggingface.co/ArthurZ" rel="nofollow">Arthur Zucker</a> .
The original code can be found <a href="https://github.com/mistralai/mistral-src" rel="nofollow">here</a>.`,Fn,st,Ln,at,Da="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Mixtral. If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Un,rt,Wn,it,Qa='<li>A demo notebook to perform supervised fine-tuning (SFT) of Mixtral-8x7B can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb" rel="nofollow">here</a>. ðŸŒŽ</li> <li>A <a href="https://medium.com/@prakharsaxena11111/finetuning-mixtral-7bx8-6071b0ebf114" rel="nofollow">blog post</a> on fine-tuning Mixtral-8x7B using PEFT. ðŸŒŽ</li> <li>The <a href="https://github.com/huggingface/alignment-handbook" rel="nofollow">Alignment Handbook</a> by Hugging Face includes scripts and recipes to perform supervised fine-tuning (SFT) and direct preference optimization with Mistral-7B. This includes scripts for full fine-tuning, QLoRa on a single accelerator as well as multi-accelerator fine-tuning.</li> <li><a href="../tasks/language_modeling">Causal language modeling task guide</a></li>',Nn,lt,Bn,J,dt,rs,Dt,Ya=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a>. It is used to instantiate an
Mixtral model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Mixtral-7B-v0.1 or Mixtral-7B-Instruct-v0.1.`,is,Qt,Oa='<a href="https://huggingface.co/mixtralai/Mixtral-8x7B" rel="nofollow">mixtralai/Mixtral-8x7B</a> <a href="https://huggingface.co/mixtralai/Mixtral-7B-Instruct-v0.1" rel="nofollow">mixtralai/Mixtral-7B-Instruct-v0.1</a>',ls,Yt,Ka=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ds,Y,Hn,ct,Pn,b,pt,cs,Ot,er="Class to wrap <code>mistral-common</code> tokenizers.",ps,O,ms,Kt,tr="Otherwise the tokenizer falls back to the Transformers implementation of the tokenizer.",us,eo,or='For more info on <code>mistral-common</code>, see <a href="https://github.com/mistralai/mistral-common" rel="nofollow">mistral-common</a>.',hs,to,nr=`This class is a wrapper around a <code>mistral_common.tokens.tokenizers.mistral.MistralTokenizer</code>.
It provides a Hugging Face compatible interface to tokenize using the official mistral-common tokenizer.`,fs,oo,sr="Supports the following methods from the <code>PreTrainedTokenizerBase</code> class:",gs,no,ar='<li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.get_vocab">get_vocab()</a>: Returns the vocabulary as a dictionary of token to index.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.encode">encode()</a>: Encode a string to a list of integers.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.decode">decode()</a>: Decode a list of integers to a string.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.batch_decode">batch_decode()</a>: Decode a batch of list of integers to a list of strings.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.convert_tokens_to_ids">convert_tokens_to_ids()</a>: Convert a list of tokens to a list of integers.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.convert_ids_to_tokens">convert_ids_to_tokens()</a>: Convert a list of integers to a list of tokens.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.tokenize">tokenize()</a>: Tokenize a string.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.get_special_tokens_mask">get_special_tokens_mask()</a>: Get the special tokens mask for a list of tokens.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.prepare_for_model">prepare_for_model()</a>: Prepare a list of inputs for the model.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.pad">pad()</a>: Pad a list of inputs to the same length.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.truncate_sequences">truncate_sequences()</a>: Truncate a list of sequences to the same length.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.apply_chat_template">apply_chat_template()</a>: Apply a chat template to a list of messages.</li> <li><code>__call__()</code>: Tokenize a string or a list of strings.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.from_pretrained">from_pretrained()</a>: Download and cache a pretrained tokenizer from the Hugging Face model hub or local directory.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/pixtral#transformers.MistralCommonTokenizer.save_pretrained">save_pretrained()</a>: Save a tokenizer to a directory, so it can be reloaded using the <code>from_pretrained</code> class method.</li> <li><a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a>: Upload tokenizer to the Hugging Face model hub.</li>',_s,so,rr="Here are the key differences with the <code>PreTrainedTokenizerBase</code> class:",bs,ao,ir='<li>Pair of sequences are not supported. The signature have been kept for compatibility but all arguments related to pair of sequences are ignored. The return values of pairs are returned as <code>None</code>.</li> <li>The <code>is_split_into_words</code> argument is not supported.</li> <li>The <code>return_token_type_ids</code> argument is not supported.</li> <li>It is not possible to add new tokens to the tokenizer. Also the special tokens are handled differently from Transformers. In <code>mistral-common</code>, special tokens are never encoded directly. This means that: <code>tokenizer.encode(&quot;&lt;s&gt;&quot;)</code> will not return the ID of the <code>&lt;s&gt;</code> token. Instead, it will return a list of IDs corresponding to the tokenization of the string <code>&quot;&lt;s&gt;&quot;</code>. For more information, see the <a href="https://mistralai.github.io/mistral-common/usage/tokenizers/#special-tokens" rel="nofollow">mistral-common documentation</a>.</li>',vs,ro,lr='If you have suggestions to improve this class, please open an issue on the <a href="https://github.com/mistralai/mistral-common/issues" rel="nofollow">mistral-common GitHub repository</a> if it is related to the tokenizer or on the <a href="https://github.com/huggingface/transformers/issues" rel="nofollow">Transformers GitHub repository</a> if it is related to the Hugging Face interface.',Ts,K,mt,ks,io,dr=`Converts a list of dictionaries with <code>&quot;role&quot;</code> and <code>&quot;content&quot;</code> keys to a list of token
ids.`,ys,ee,ut,Ms,lo,cr="Convert a list of lists of token ids into a list of strings by calling decode.",xs,te,ht,ws,co,pr=`Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
added tokens.`,Cs,oe,ft,zs,po,mr=`Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
vocabulary.`,$s,ne,gt,Js,mo,ur=`Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.`,qs,se,_t,Is,uo,hr="Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.",js,ae,bt,Fs,ho,fr=`Instantiate a <code>MistralCommonTokenizer</code> from a predefined
tokenizer.`,Ls,re,vt,Us,fo,gr=`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.`,Ws,W,Tt,Ns,go,_r="Returns the vocabulary as a dictionary of token to index.",Bs,_o,br=`This is a lossy conversion. There may be multiple token ids that decode to the same
string due to partial UTF-8 byte sequences being converted to ï¿½.`,Hs,L,kt,Ps,bo,vr=`Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.`,Zs,vo,Tr=`Padding side (left/right) padding token ids are defined at the tokenizer level (with <code>self.padding_side</code>,
<code>self.pad_token_id</code>).`,Vs,ie,Gs,le,yt,Es,To,kr=`Prepares a sequence of input id so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens.`,As,N,Mt,Rs,ko,yr="Save the full tokenizer state.",Xs,yo,Mr=`This method make sure the full tokenizer can then be re-loaded using the
<code>~MistralCommonTokenizer.tokenization_mistral_common.from_pretrained</code> class method.`,Ss,B,xt,Ds,Mo,xr="Converts a string into a sequence of tokens, using the tokenizer.",Qs,xo,wr="Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies.",Ys,de,wt,Os,wo,Cr="Truncates a sequence pair in-place following the strategy.",Zn,Ct,Vn,q,zt,Ks,Co,zr="The bare Mixtral Model outputting raw hidden-states without any specific head on top.",ea,zo,$r=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ta,$o,Jr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,oa,H,$t,na,Jo,qr='The <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a> forward method, overrides the <code>__call__</code> special method.',sa,ce,Gn,Jt,En,I,qt,aa,qo,Ir="The Mixtral Model for causal language modeling.",ra,Io,jr=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ia,jo,Fr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,la,U,It,da,Fo,Lr='The <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',ca,pe,pa,me,An,jt,Rn,A,Ft,ma,P,Lt,ua,Lo,Ur="The <code>GenericForSequenceClassification</code> forward method, overrides the <code>__call__</code> special method.",ha,ue,Xn,Ut,Sn,R,Wt,fa,Z,Nt,ga,Uo,Wr="The <code>GenericForTokenClassification</code> forward method, overrides the <code>__call__</code> special method.",_a,he,Dn,Bt,Qn,X,Ht,ba,V,Pt,va,Wo,Nr="The <code>GenericForQuestionAnswering</code> forward method, overrides the <code>__call__</code> special method.",Ta,fe,Yn,Zt,On,Ao,Kn;return ve=new $({props:{title:"Mixtral",local:"mixtral",headingTag:"h1"}}),Te=new $({props:{title:"Overview",local:"overview",headingTag:"h2"}}),we=new $({props:{title:"Architectural details",local:"architectural-details",headingTag:"h3"}}),Ie=new $({props:{title:"License",local:"license",headingTag:"h3"}}),Fe=new $({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Ne=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1peHRyYWwtOHg3Qi12MC4xJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWl4dHJhbC04eDdCLXYwLjElMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyTXklMjBmYXZvdXJpdGUlMjBjb25kaW1lbnQlMjBpcyUyMiUwQSUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUJwcm9tcHQlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMDAlMkMlMjBkb19zYW1wbGUlM0RUcnVlKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;My favourite condiment is&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([prompt], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;My favourite condiment is to ...&quot;</span>`,wrap:!1}}),He=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1peHRyYWwtOHg3Qi1JbnN0cnVjdC12MC4xJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWl4dHJhbC04eDdCLUluc3RydWN0LXYwLjElMjIpJTBBJTBBbWVzc2FnZXMlMjAlM0QlMjAlNUIlMEElMjAlMjAlMjAlMjAlN0IlMjJyb2xlJTIyJTNBJTIwJTIydXNlciUyMiUyQyUyMCUyMmNvbnRlbnQlMjIlM0ElMjAlMjJXaGF0JTIwaXMlMjB5b3VyJTIwZmF2b3VyaXRlJTIwY29uZGltZW50JTNGJTIyJTdEJTJDJTBBJTIwJTIwJTIwJTIwJTdCJTIycm9sZSUyMiUzQSUyMCUyMmFzc2lzdGFudCUyMiUyQyUyMCUyMmNvbnRlbnQlMjIlM0ElMjAlMjJXZWxsJTJDJTIwSSdtJTIwcXVpdGUlMjBwYXJ0aWFsJTIwdG8lMjBhJTIwZ29vZCUyMHNxdWVlemUlMjBvZiUyMGZyZXNoJTIwbGVtb24lMjBqdWljZS4lMjBJdCUyMGFkZHMlMjBqdXN0JTIwdGhlJTIwcmlnaHQlMjBhbW91bnQlMjBvZiUyMHplc3R5JTIwZmxhdm91ciUyMHRvJTIwd2hhdGV2ZXIlMjBJJ20lMjBjb29raW5nJTIwdXAlMjBpbiUyMHRoZSUyMGtpdGNoZW4hJTIyJTdEJTJDJTBBJTIwJTIwJTIwJTIwJTdCJTIycm9sZSUyMiUzQSUyMCUyMnVzZXIlMjIlMkMlMjAlMjJjb250ZW50JTIyJTNBJTIwJTIyRG8lMjB5b3UlMjBoYXZlJTIwbWF5b25uYWlzZSUyMHJlY2lwZXMlM0YlMjIlN0QlMEElNUQlMEElMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYXBwbHlfY2hhdF90ZW1wbGF0ZShtZXNzYWdlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUobW9kZWxfaW5wdXRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QxMDAlMkMlMjBkb19zYW1wbGUlM0RUcnVlKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>messages = [
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What is your favourite condiment?&quot;</span>},
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Well, I&#x27;m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I&#x27;m cooking up in the kitchen!&quot;</span>},
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Do you have mayonnaise recipes?&quot;</span>}
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer.apply_chat_template(messages, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Mayonnaise can be made as follows: (...)&quot;</span>`,wrap:!1}}),Ze=new $({props:{title:"Speeding up Mixtral by using Flash Attention",local:"speeding-up-mixtral-by-using-flash-attention",headingTag:"h2"}}),Ee=new be({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),Xe=new be({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWl4dHJhbC04eDdCLXYwLjElMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjBhdHRuX2ltcGxlbWVudGF0aW9uJTNEJTIyZmxhc2hfYXR0ZW50aW9uXzIlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtaXN0cmFsYWklMkZNaXh0cmFsLTh4N0ItdjAuMSUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJNeSUyMGZhdm91cml0ZSUyMGNvbmRpbWVudCUyMGlzJTIyJTBBJTBBbW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QnByb21wdCU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKiptb2RlbF9pbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwMCUyQyUyMGRvX3NhbXBsZSUzRFRydWUpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>, dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;My favourite condiment is&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([prompt], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;The expected output&quot;</span>`,wrap:!1}}),Se=new $({props:{title:"Expected speedups",local:"expected-speedups",headingTag:"h3"}}),Qe=new $({props:{title:"Sliding window Attention",local:"sliding-window-attention",headingTag:"h3"}}),Ke=new $({props:{title:"Shrinking down Mixtral using quantization",local:"shrinking-down-mixtral-using-quantization",headingTag:"h2"}}),ot=new be({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># specify how to quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantization_config = BitsAndBytesConfig(
<span class="hljs-meta">... </span>        load_in_4bit=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
<span class="hljs-meta">... </span>        bnb_4bit_compute_dtype=<span class="hljs-string">&quot;torch.float16&quot;</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span>, quantization_config=<span class="hljs-literal">True</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;My favourite condiment is&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>messages = [
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What is your favourite condiment?&quot;</span>},
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Well, I&#x27;m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I&#x27;m cooking up in the kitchen!&quot;</span>},
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Do you have mayonnaise recipes?&quot;</span>}
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer.apply_chat_template(messages, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;The expected output&quot;</span>`,wrap:!1}}),st=new $({props:{title:"Resources",local:"resources",headingTag:"h2"}}),rt=new Er({props:{pipeline:"text-generation"}}),lt=new $({props:{title:"MixtralConfig",local:"transformers.MixtralConfig",headingTag:"h2"}}),dt=new w({props:{name:"class transformers.MixtralConfig",anchor:"transformers.MixtralConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 14336"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"num_key_value_heads",val:" = 8"},{name:"head_dim",val:" = None"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 131072"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-05"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = None"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 1000000.0"},{name:"sliding_window",val:" = None"},{name:"attention_dropout",val:" = 0.0"},{name:"num_experts_per_tok",val:" = 2"},{name:"num_local_experts",val:" = 8"},{name:"output_router_logits",val:" = False"},{name:"router_aux_loss_coef",val:" = 0.001"},{name:"router_jitter_noise",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MixtralConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the Mixtral model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a>`,name:"vocab_size"},{anchor:"transformers.MixtralConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.MixtralConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14336) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.MixtralConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MixtralConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MixtralConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA) otherwise GQA is used. When
converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
by meanpooling all the original heads within that group. For more details, check out <a href="https://huggingface.co/papers/2305.13245" rel="nofollow">this
paper</a>. If it is not specified, will default to <code>8</code>.`,name:"num_key_value_heads"},{anchor:"transformers.MixtralConfig.head_dim",description:`<strong>head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to <code>hidden_size // num_attention_heads</code>) &#x2014;
The attention head dimension.`,name:"head_dim"},{anchor:"transformers.MixtralConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.MixtralConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to <code>4096*32</code>) &#x2014;
The maximum sequence length that this model might ever be used with. Mixtral&#x2019;s sliding window attention
allows sequence of up to 4096*32 tokens.`,name:"max_position_embeddings"},{anchor:"transformers.MixtralConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MixtralConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.MixtralConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.MixtralConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the padding token.`,name:"pad_token_id"},{anchor:"transformers.MixtralConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The id of the &#x201C;beginning-of-sequence&#x201D; token.`,name:"bos_token_id"},{anchor:"transformers.MixtralConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the &#x201C;end-of-sequence&#x201D; token.`,name:"eos_token_id"},{anchor:"transformers.MixtralConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model&#x2019;s input and output word embeddings should be tied.`,name:"tie_word_embeddings"},{anchor:"transformers.MixtralConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 1000000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.MixtralConfig.sliding_window",description:`<strong>sliding_window</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Sliding window attention window size. If not specified, will default to <code>4096</code>.`,name:"sliding_window"},{anchor:"transformers.MixtralConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.MixtralConfig.num_experts_per_tok",description:`<strong>num_experts_per_tok</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of experts to route per-token, can be also interpreted as the <code>top-k</code> routing
parameter`,name:"num_experts_per_tok"},{anchor:"transformers.MixtralConfig.num_local_experts",description:`<strong>num_local_experts</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of experts per Sparse MLP layer.`,name:"num_local_experts"},{anchor:"transformers.MixtralConfig.output_router_logits",description:`<strong>output_router_logits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the router logits should be returned by the model. Enabling this will also
allow the model to output the auxiliary loss. See <a href>here</a> for more details`,name:"output_router_logits"},{anchor:"transformers.MixtralConfig.router_aux_loss_coef",description:`<strong>router_aux_loss_coef</strong> (<code>float</code>, <em>optional</em>, defaults to 0.001) &#x2014;
The aux loss factor for the total loss.`,name:"router_aux_loss_coef"},{anchor:"transformers.MixtralConfig.router_jitter_noise",description:`<strong>router_jitter_noise</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Amount of noise to add to the router.`,name:"router_jitter_noise"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/configuration_mixtral.py#L24"}}),Y=new ka({props:{anchor:"transformers.MixtralConfig.example",$$slots:{default:[Rr]},$$scope:{ctx:C}}}),ct=new $({props:{title:"MistralCommonTokenizer",local:"transformers.MistralCommonTokenizer",headingTag:"h2"}}),pt=new w({props:{name:"class transformers.MistralCommonTokenizer",anchor:"transformers.MistralCommonTokenizer",parameters:[{name:"tokenizer_path",val:": typing.Union[str, os.PathLike, pathlib.Path]"},{name:"mode",val:": ValidationMode = <ValidationMode.test: 'test'>"},{name:"model_max_length",val:": int = 1000000000000000019884624838656"},{name:"padding_side",val:": str = 'left'"},{name:"truncation_side",val:": str = 'right'"},{name:"model_input_names",val:": typing.Optional[list[str]] = None"},{name:"clean_up_tokenization_spaces",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L158"}}),O=new ka({props:{anchor:"transformers.MistralCommonTokenizer.example",$$slots:{default:[Xr]},$$scope:{ctx:C}}}),mt=new w({props:{name:"apply_chat_template",anchor:"transformers.MistralCommonTokenizer.apply_chat_template",parameters:[{name:"conversation",val:": typing.Union[list[dict[str, str]], list[list[dict[str, str]]]]"},{name:"tools",val:": typing.Optional[list[typing.Union[dict, typing.Callable]]] = None"},{name:"continue_final_message",val:": bool = False"},{name:"tokenize",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": bool = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_dict",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.conversation",description:`<strong>conversation</strong> (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]) &#x2014; A list of dicts
with &#x201C;role&#x201D; and &#x201C;content&#x201D; keys, representing the chat history so far.`,name:"conversation"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.tools",description:`<strong>tools</strong> (<code>List[Union[Dict, Callable]]</code>, <em>optional</em>) &#x2014;
A list of tools (callable functions) that will be accessible to the model. If the template does not
support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
giving the name, description and argument types for the tool. See our
<a href="https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use" rel="nofollow">chat templating guide</a>
for more information.`,name:"tools"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.continue_final_message",description:`<strong>continue_final_message</strong> (bool, <em>optional</em>) &#x2014;
If this is set, the chat will be formatted so that the final
message in the chat is open-ended, without any EOS tokens. The model will continue this message
rather than starting a new one. This allows you to &#x201C;prefill&#x201D; part of
the model&#x2019;s response for it. Cannot be used at the same time as <code>add_generation_prompt</code>.`,name:"continue_final_message"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.tokenize",description:`<strong>tokenize</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to tokenize the output. If <code>False</code>, the output will be a string.`,name:"tokenize"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.truncation",description:`<strong>truncation</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to truncate sequences at the maximum length. Has no effect if tokenize is <code>False</code>.`,name:"truncation"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is <code>False</code>. If
not specified, the tokenizer&#x2019;s <code>max_length</code> attribute will be used as a default.`,name:"max_length"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Has no effect if tokenize is <code>False</code>. Acceptable
values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to return a dictionary with named outputs. Has no effect if tokenize is <code>False</code>.
If at least one conversation contains an image, its pixel values will be returned in the <code>pixel_values</code> key.`,name:"return_dict"},{anchor:"transformers.MistralCommonTokenizer.apply_chat_template.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.apply_chat_template</code>.
Will raise an error if used.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L1368",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of token ids representing the tokenized chat so far, including control
tokens. This output is ready to pass to the model, either directly or via methods like <code>generate()</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Union[str, List[int], List[str], List[List[int]], BatchEncoding]</code></p>
`}}),ut=new w({props:{name:"batch_decode",anchor:"transformers.MistralCommonTokenizer.batch_decode",parameters:[{name:"sequences",val:": typing.Union[list[int], list[list[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.batch_decode.sequences",description:`<strong>sequences</strong> (<code>Union[List[int], List[List[int]], np.ndarray, torch.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"sequences"},{anchor:"transformers.MistralCommonTokenizer.batch_decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.MistralCommonTokenizer.batch_decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to clean up the tokenization spaces. If <code>None</code>, will default to
<code>self.clean_up_tokenization_spaces</code>.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.MistralCommonTokenizer.batch_decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.batch_decode</code>.
Will raise an error if used.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L476",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The list of decoded sentences.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[str]</code></p>
`}}),ht=new w({props:{name:"convert_ids_to_tokens",anchor:"transformers.MistralCommonTokenizer.convert_ids_to_tokens",parameters:[{name:"ids",val:": typing.Union[int, list[int]]"},{name:"skip_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.convert_ids_to_tokens.ids",description:`<strong>ids</strong> (<code>int</code> or <code>List[int]</code>) &#x2014;
The token id (or token ids) to convert to tokens.`,name:"ids"},{anchor:"transformers.MistralCommonTokenizer.convert_ids_to_tokens.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L523",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The decoded token(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>str</code> or <code>List[str]</code></p>
`}}),ft=new w({props:{name:"convert_tokens_to_ids",anchor:"transformers.MistralCommonTokenizer.convert_tokens_to_ids",parameters:[{name:"tokens",val:": typing.Union[str, list[str]]"}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.convert_tokens_to_ids.tokens",description:"<strong>tokens</strong> (<code>str</code> or <code>List[str]</code>) &#x2014; One or several token(s) to convert to token id(s).",name:"tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L571",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The token id or list of token ids.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>int</code> or <code>List[int]</code></p>
`}}),gt=new w({props:{name:"decode",anchor:"transformers.MistralCommonTokenizer.decode",parameters:[{name:"token_ids",val:": typing.Union[int, list[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.decode.token_ids",description:`<strong>token_ids</strong> (<code>Union[int, List[int], np.ndarray, torch.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"token_ids"},{anchor:"transformers.MistralCommonTokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.MistralCommonTokenizer.decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to clean up the tokenization spaces. If <code>None</code>, will default to
<code>self.clean_up_tokenization_spaces</code>.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.MistralCommonTokenizer.decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.decode</code>.
Will raise an error if used.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L434",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The decoded sentence.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>str</code></p>
`}}),_t=new w({props:{name:"encode",anchor:"transformers.MistralCommonTokenizer.encode",parameters:[{name:"text",val:": typing.Union[str, list[int]]"},{name:"text_pair",val:": None = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"verbose",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.encode.text",description:`<strong>text</strong> (<code>str</code> or <code>List[int]</code>) &#x2014;
The first sequence to be encoded. This can be a string or a list of integers (tokenized string ids).`,name:"text"},{anchor:"transformers.MistralCommonTokenizer.encode.text_pair",description:`<strong>text_pair</strong> (<code>None</code>, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.encode</code>. Kept to match <code>PreTrainedTokenizerBase.encode</code> signature.`,name:"text_pair"},{anchor:"transformers.MistralCommonTokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add special tokens when encoding the sequences. This will use the underlying
<code>PretrainedTokenizerBase.build_inputs_with_special_tokens</code> function, which defines which tokens are
automatically added to the input ids. This is useful if you want to add <code>bos</code> or <code>eos</code> tokens
automatically.`,name:"add_special_tokens"},{anchor:"transformers.MistralCommonTokenizer.encode.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.MistralCommonTokenizer.encode.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or
to the maximum acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.MistralCommonTokenizer.encode.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.MistralCommonTokenizer.encode.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to a number along with <code>max_length</code>, the overflowing tokens returned when
<code>return_overflowing_tokens=True</code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.`,name:"stride"},{anchor:"transformers.MistralCommonTokenizer.encode.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value. Requires <code>padding</code> to be activated.
This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.MistralCommonTokenizer.encode.padding_side",description:`<strong>padding_side</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The side on which the model should have padding applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"padding_side"},{anchor:"transformers.MistralCommonTokenizer.encode.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.MistralCommonTokenizer.encode.*kwargs",description:`*<strong>*kwargs</strong> &#x2014; Not supported by <code>MistralCommonTokenizer.encode</code>.
Will raise an error if used.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L367",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The tokenized ids of the text.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code>, <code>torch.Tensor</code></p>
`}}),bt=new w({props:{name:"from_pretrained",anchor:"transformers.MistralCommonTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"*init_inputs",val:""},{name:"mode",val:": ValidationMode = <ValidationMode.test: 'test'>"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"model_max_length",val:": int = 1000000000000000019884624838656"},{name:"padding_side",val:": str = 'left'"},{name:"truncation_side",val:": str = 'right'"},{name:"model_input_names",val:": typing.Optional[list[str]] = None"},{name:"clean_up_tokenization_spaces",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing the tokenizer config, for instance saved
using the <code>MistralCommonTokenizer.tokenization_mistral_common.save_pretrained</code> method, e.g.,
<code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.mode",description:`<strong>mode</strong> (<code>ValidationMode</code>, <em>optional</em>, defaults to <code>ValidationMode.test</code>) &#x2014;
Validation mode for the <code>MistralTokenizer</code> tokenizer.`,name:"mode"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the vocabulary files and override the cached versions if they
exist.`,name:"force_download"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>hf auth login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.local_files_only",description:`<strong>local_files_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only rely on local files and not to attempt to download any files.`,name:"local_files_only"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.padding_side",description:`<strong>padding_side</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;left&quot;</code>) &#x2014;
The side on which the model should have padding applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"padding_side"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.truncation_side",description:`<strong>truncation_side</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;right&quot;</code>) &#x2014;
The side on which the model should have truncation applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].`,name:"truncation_side"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.model_input_names",description:`<strong>model_input_names</strong> (<code>List[string]</code>, <em>optional</em>) &#x2014;
The list of inputs accepted by the forward pass of the model (like <code>&quot;token_type_ids&quot;</code> or
<code>&quot;attention_mask&quot;</code>). Default value is picked from the class attribute of the same name.`,name:"model_input_names"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the model should cleanup the spaces that were added when splitting the input text during the
tokenization process.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.MistralCommonTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.from_pretrained</code>.
Will raise an error if used.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L1689"}}),vt=new w({props:{name:"get_special_tokens_mask",anchor:"transformers.MistralCommonTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": None = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the sequence.`,name:"token_ids_0"},{anchor:"transformers.MistralCommonTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer</code>. Kept to match the interface of <code>PreTrainedTokenizerBase</code>.`,name:"token_ids_1"},{anchor:"transformers.MistralCommonTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L746",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]</p>
`}}),Tt=new w({props:{name:"get_vocab",anchor:"transformers.MistralCommonTokenizer.get_vocab",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L345",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The vocabulary.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, int]</code></p>
`}}),kt=new w({props:{name:"pad",anchor:"transformers.MistralCommonTokenizer.pad",parameters:[{name:"encoded_inputs",val:": typing.Union[transformers.tokenization_utils_base.BatchEncoding, list[transformers.tokenization_utils_base.BatchEncoding], dict[str, list[int]], dict[str, list[list[int]]], list[dict[str, list[int]]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"verbose",val:": bool = True"}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.pad.encoded_inputs",description:`<strong>encoded_inputs</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding">BatchEncoding</a>, list of <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding">BatchEncoding</a>, <code>Dict[str, List[int]]</code>, <code>Dict[str, List[List[int]]</code> or <code>List[Dict[str, List[int]]]</code>) &#x2014;
Tokenized inputs. Can represent one input (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding">BatchEncoding</a> or <code>Dict[str, List[int]]</code>) or a batch of
tokenized inputs (list of <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding">BatchEncoding</a>, <em>Dict[str, List[List[int]]]</em> or <em>List[Dict[str,
List[int]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[int]</code> you can have tensors (numpy arrays, PyTorch tensors), see
the note above for the return type.`,name:"encoded_inputs"},{anchor:"transformers.MistralCommonTokenizer.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code> (default): Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code>: No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.MistralCommonTokenizer.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.MistralCommonTokenizer.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.MistralCommonTokenizer.pad.padding_side",description:`<strong>padding_side</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The side on which the model should have padding applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"padding_side"},{anchor:"transformers.MistralCommonTokenizer.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.MistralCommonTokenizer.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.MistralCommonTokenizer.pad.verbose",description:`<strong>verbose</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to print more information and warnings.`,name:"verbose"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L1130"}}),ie=new Eo({props:{$$slots:{default:[Sr]},$$scope:{ctx:C}}}),yt=new w({props:{name:"prepare_for_model",anchor:"transformers.MistralCommonTokenizer.prepare_for_model",parameters:[{name:"ids",val:": list"},{name:"pair_ids",val:": None = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = False"},{name:"truncation",val:": typing.Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"stride",val:": int = 0"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"padding_side",val:": typing.Optional[str] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"prepend_batch_axis",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.ids",description:`<strong>ids</strong> (<code>List[int]</code>) &#x2014;
Tokenized input ids of the first sequence.`,name:"ids"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.pair_ids",description:`<strong>pair_ids</strong> (<code>None</code>, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer</code>. Kept to match the interface of <code>PreTrainedTokenizerBase</code>.`,name:"pair_ids"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add special tokens when encoding the sequences. This will use the underlying
<code>PretrainedTokenizerBase.build_inputs_with_special_tokens</code> function, which defines which tokens are
automatically added to the input ids. This is useful if you want to add <code>bos</code> or <code>eos</code> tokens
automatically.`,name:"add_special_tokens"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or
to the maximum acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to a number along with <code>max_length</code>, the overflowing tokens returned when
<code>return_overflowing_tokens=True</code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.`,name:"stride"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value. Requires <code>padding</code> to be activated.
This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.padding_side",description:`<strong>padding_side</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The side on which the model should have padding applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"padding_side"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_overflowing_tokens",description:`<strong>return_overflowing_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch
of pairs) is provided with <code>truncation_strategy = longest_first</code> or <code>True</code>, an error is raised instead
of returning overflowing tokens.`,name:"return_overflowing_tokens"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_special_tokens_mask",description:`<strong>return_special_tokens_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return special tokens mask information.`,name:"return_special_tokens_mask"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_offsets_mapping",description:`<strong>return_offsets_mapping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return <code>(char_start, char_end)</code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>, if using
Python&#x2019;s tokenizer, this method will raise <code>NotImplementedError</code>.`,name:"return_offsets_mapping"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.return_length",description:`<strong>return_length</strong>  (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the lengths of the encoded inputs.`,name:"return_length"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.verbose",description:`<strong>verbose</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to print more information and warnings.`,name:"verbose"},{anchor:"transformers.MistralCommonTokenizer.prepare_for_model.*kwargs",description:"*<strong>*kwargs</strong> &#x2014; passed to the <code>self.tokenize()</code> method",name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L842",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields:</p>
<ul>
<li>
<p><strong>input_ids</strong> â€” List of token ids to be fed to a model.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
</li>
<li>
<p><strong>attention_mask</strong> â€” List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>â€œattention_maskâ€</em> is in <code>self.model_input_names</code>).</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
</li>
<li>
<p><strong>overflowing_tokens</strong> â€” List of overflowing tokens sequences (when a <code>max_length</code> is specified and
<code>return_overflowing_tokens=True</code>).</p>
</li>
<li>
<p><strong>num_truncated_tokens</strong> â€” Number of tokens truncated (when a <code>max_length</code> is specified and
<code>return_overflowing_tokens=True</code>).</p>
</li>
<li>
<p><strong>special_tokens_mask</strong> â€” List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code>add_special_tokens=True</code> and <code>return_special_tokens_mask=True</code>).</p>
</li>
<li>
<p><strong>length</strong> â€” The length of the inputs (when <code>return_length=True</code>)</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),Mt=new w({props:{name:"save_pretrained",anchor:"transformers.MistralCommonTokenizer.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike, pathlib.Path]"},{name:"push_to_hub",val:": bool = False"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"repo_url",val:": typing.Optional[str] = None"},{name:"organization",val:": typing.Optional[str] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.save_pretrained.save_directory",description:"<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014; The path to a directory where the tokenizer will be saved.",name:"save_directory"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The token to use to push to the model hub. If <code>True</code>, will use the token in the <code>HF_TOKEN</code> environment
variable.`,name:"token"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.commit_message",description:"<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014; The commit message to use when pushing to the hub.",name:"commit_message"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.repo_id",description:"<strong>repo_id</strong> (<code>str</code>, <em>optional</em>) &#x2014; The name of the repository to which push to the Hub.",name:"repo_id"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.private",description:"<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014; Whether the model repository is private or not.",name:"private"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.repo_url",description:"<strong>repo_url</strong> (<code>str</code>, <em>optional</em>) &#x2014; The URL to the Git repository to which push to the Hub.",name:"repo_url"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.organization",description:"<strong>organization</strong> (<code>str</code>, <em>optional</em>) &#x2014; The name of the organization in which you would like to push your model.",name:"organization"},{anchor:"transformers.MistralCommonTokenizer.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer.save_pretrained</code>.
Will raise an error if used.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L1816",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The files saved.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tuple of <code>str</code></p>
`}}),xt=new w({props:{name:"tokenize",anchor:"transformers.MistralCommonTokenizer.tokenize",parameters:[{name:"text",val:": str"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.tokenize.text",description:`<strong>text</strong> (<code>str</code>) &#x2014;
The sequence to be encoded.`,name:"text"},{anchor:"transformers.MistralCommonTokenizer.tokenize.*kwargs",description:`*<strong>*kwargs</strong> (additional keyword arguments) &#x2014;
Not supported by <code>MistralCommonTokenizer.tokenize</code>.
Will raise an error if used.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L606",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The list of tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[str]</code></p>
`}}),wt=new w({props:{name:"truncate_sequences",anchor:"transformers.MistralCommonTokenizer.truncate_sequences",parameters:[{name:"ids",val:": list"},{name:"pair_ids",val:": None = None"},{name:"num_tokens_to_remove",val:": int = 0"},{name:"truncation_strategy",val:": typing.Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first'"},{name:"stride",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralCommonTokenizer.truncate_sequences.ids",description:`<strong>ids</strong> (<code>List[int]</code>) &#x2014;
Tokenized input ids. Can be obtained from a string by chaining the <code>tokenize</code> and
<code>convert_tokens_to_ids</code> methods.`,name:"ids"},{anchor:"transformers.MistralCommonTokenizer.truncate_sequences.pair_ids",description:`<strong>pair_ids</strong> (<code>None</code>, <em>optional</em>) &#x2014;
Not supported by <code>MistralCommonTokenizer</code>. Kept to match the signature of <code>PreTrainedTokenizerBase.truncate_sequences</code>.`,name:"pair_ids"},{anchor:"transformers.MistralCommonTokenizer.truncate_sequences.num_tokens_to_remove",description:`<strong>num_tokens_to_remove</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Number of tokens to remove using the truncation strategy.`,name:"num_tokens_to_remove"},{anchor:"transformers.MistralCommonTokenizer.truncate_sequences.truncation_strategy",description:`<strong>truncation_strategy</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>&apos;longest_first&apos;</code>) &#x2014;
The strategy to follow for truncation. Can be:</p>
<ul>
<li><code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the
maximum acceptable input length for the model if that argument is not provided.</li>
<li><code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths greater
than the model maximum admissible input size).</li>
</ul>`,name:"truncation_strategy"},{anchor:"transformers.MistralCommonTokenizer.truncate_sequences.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to a positive number, the overflowing tokens returned will contain some tokens from the main
sequence returned. The value of this argument defines the number of additional tokens.`,name:"stride"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/tokenization_mistral_common.py#L1293",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The truncated <code>ids</code> and the list of
overflowing tokens. <code>None</code> is returned to match Transformers signature.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Tuple[List[int], None, List[int]]</code></p>
`}}),Ct=new $({props:{title:"MixtralModel",local:"transformers.MixtralModel",headingTag:"h2"}}),zt=new w({props:{name:"class transformers.MixtralModel",anchor:"transformers.MixtralModel",parameters:[{name:"config",val:": MixtralConfig"}],parametersDescription:[{anchor:"transformers.MixtralModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L401"}}),$t=new w({props:{name:"forward",anchor:"transformers.MixtralModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.MixtralModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MixtralModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MixtralModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MixtralModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MixtralModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MixtralModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.MixtralModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L418",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.MoeModelOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig"
>MixtralConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>router_logits</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_router_probs=True</code> and <code>config.add_router_probs=True</code> is passed or when <code>config.output_router_probs=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, sequence_length, num_experts)</code>.</p>
<p>Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary
loss for Mixture of Experts models.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.MoeModelOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ce=new Eo({props:{$$slots:{default:[Dr]},$$scope:{ctx:C}}}),Jt=new $({props:{title:"MixtralForCausalLM",local:"transformers.MixtralForCausalLM",headingTag:"h2"}}),qt=new w({props:{name:"class transformers.MixtralForCausalLM",anchor:"transformers.MixtralForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MixtralForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L566"}}),It=new w({props:{name:"forward",anchor:"transformers.MixtralForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_router_logits",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"logits_to_keep",val:": typing.Union[int, torch.Tensor] = 0"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.MixtralForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MixtralForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MixtralForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MixtralForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MixtralForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MixtralForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.MixtralForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.MixtralForCausalLM.forward.output_router_logits",description:`<strong>output_router_logits</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the logits of all the routers. They are useful for computing the router loss, and
should not be returned during inference.`,name:"output_router_logits"},{anchor:"transformers.MixtralForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"},{anchor:"transformers.MixtralForCausalLM.forward.logits_to_keep",description:`<strong>logits_to_keep</strong> (<code>Union[int, torch.Tensor]</code>, defaults to <code>0</code>) &#x2014;
If an <code>int</code>, compute logits for the last <code>logits_to_keep</code> tokens. If <code>0</code>, calculate logits for all
<code>input_ids</code> (special case). Only last token logits are needed for generation, and calculating them only for that
token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
If a <code>torch.Tensor</code>, must be 1D corresponding to the indices to keep in the sequence length dimension.
This is useful when using packed tensor format (single dimension for batch and sequence length).`,name:"logits_to_keep"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L583",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.MoeCausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mixtral#transformers.MixtralConfig"
>MixtralConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>aux_loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” aux_loss for the sparse modules.</p>
</li>
<li>
<p><strong>router_logits</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_router_probs=True</code> and <code>config.add_router_probs=True</code> is passed or when <code>config.output_router_probs=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, sequence_length, num_experts)</code>.</p>
<p>Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary
loss for Mixture of Experts models.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.MoeCausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new Eo({props:{$$slots:{default:[Qr]},$$scope:{ctx:C}}}),me=new ka({props:{anchor:"transformers.MixtralForCausalLM.forward.example",$$slots:{default:[Yr]},$$scope:{ctx:C}}}),jt=new $({props:{title:"MixtralForSequenceClassification",local:"transformers.MixtralForSequenceClassification",headingTag:"h2"}}),Ft=new w({props:{name:"class transformers.MixtralForSequenceClassification",anchor:"transformers.MixtralForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L670"}}),Lt=new w({props:{name:"forward",anchor:"transformers.MixtralForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.MixtralForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MixtralForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MixtralForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MixtralForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MixtralForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MixtralForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.MixtralForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_layers.py#L111",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>None</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new Eo({props:{$$slots:{default:[Or]},$$scope:{ctx:C}}}),Ut=new $({props:{title:"MixtralForTokenClassification",local:"transformers.MixtralForTokenClassification",headingTag:"h2"}}),Wt=new w({props:{name:"class transformers.MixtralForTokenClassification",anchor:"transformers.MixtralForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L674"}}),Nt=new w({props:{name:"forward",anchor:"transformers.MixtralForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MixtralForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MixtralForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MixtralForTokenClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MixtralForTokenClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MixtralForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MixtralForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.MixtralForTokenClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_layers.py#L254",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>None</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  â€” Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) â€” Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new Eo({props:{$$slots:{default:[Kr]},$$scope:{ctx:C}}}),Bt=new $({props:{title:"MixtralForQuestionAnswering",local:"transformers.MixtralForQuestionAnswering",headingTag:"h2"}}),Ht=new w({props:{name:"class transformers.MixtralForQuestionAnswering",anchor:"transformers.MixtralForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mixtral/modeling_mixtral.py#L678"}}),Pt=new w({props:{name:"forward",anchor:"transformers.MixtralForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"end_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.MixtralForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MixtralForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MixtralForQuestionAnswering.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MixtralForQuestionAnswering.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>~cache_utils.Cache</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MixtralForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MixtralForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.MixtralForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_layers.py#L191",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>None</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) â€” Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) â€” Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Eo({props:{$$slots:{default:[ei]},$$scope:{ctx:C}}}),Zt=new Ar({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mixtral.md"}}),{c(){d=i("meta"),k=n(),_=i("p"),T=n(),z=i("p"),z.innerHTML=y,F=n(),p(ve.$$.fragment),Ro=n(),D=i("div"),D.innerHTML=ya,Xo=n(),p(Te.$$.fragment),So=n(),ke=i("p"),ke.innerHTML=Ma,Do=n(),ye=i("p"),ye.textContent=xa,Qo=n(),Me=i("p"),Me.innerHTML=wa,Yo=n(),xe=i("p"),xe.innerHTML=Ca,Oo=n(),p(we.$$.fragment),Ko=n(),Ce=i("p"),Ce.textContent=za,en=n(),ze=i("ul"),ze.innerHTML=$a,tn=n(),$e=i("p"),$e.innerHTML=Ja,on=n(),Je=i("ul"),Je.innerHTML=qa,nn=n(),qe=i("p"),qe.innerHTML=Ia,sn=n(),p(Ie.$$.fragment),an=n(),je=i("p"),je.innerHTML=ja,rn=n(),p(Fe.$$.fragment),ln=n(),Le=i("p"),Le.textContent=Fa,dn=n(),Ue=i("ul"),Ue.innerHTML=La,cn=n(),We=i("p"),We.textContent=Ua,pn=n(),p(Ne.$$.fragment),mn=n(),Be=i("p"),Be.textContent=Wa,un=n(),p(He.$$.fragment),hn=n(),Pe=i("p"),Pe.innerHTML=Na,fn=n(),p(Ze.$$.fragment),gn=n(),Ve=i("p"),Ve.innerHTML=Ba,_n=n(),Ge=i("p"),Ge.textContent=Ha,bn=n(),p(Ee.$$.fragment),vn=n(),Ae=i("p"),Ae.innerHTML=Pa,Tn=n(),Re=i("p"),Re.textContent=Za,kn=n(),p(Xe.$$.fragment),yn=n(),p(Se.$$.fragment),Mn=n(),De=i("p"),De.innerHTML=Va,xn=n(),Q=i("div"),Q.innerHTML=Ga,wn=n(),p(Qe.$$.fragment),Cn=n(),Ye=i("p"),Ye.innerHTML=Ea,zn=n(),Oe=i("p"),Oe.innerHTML=Aa,$n=n(),p(Ke.$$.fragment),Jn=n(),et=i("p"),et.innerHTML=Ra,qn=n(),tt=i("p"),tt.innerHTML=Xa,In=n(),p(ot.$$.fragment),jn=n(),nt=i("p"),nt.innerHTML=Sa,Fn=n(),p(st.$$.fragment),Ln=n(),at=i("p"),at.textContent=Da,Un=n(),p(rt.$$.fragment),Wn=n(),it=i("ul"),it.innerHTML=Qa,Nn=n(),p(lt.$$.fragment),Bn=n(),J=i("div"),p(dt.$$.fragment),rs=n(),Dt=i("p"),Dt.innerHTML=Ya,is=n(),Qt=i("p"),Qt.innerHTML=Oa,ls=n(),Yt=i("p"),Yt.innerHTML=Ka,ds=n(),p(Y.$$.fragment),Hn=n(),p(ct.$$.fragment),Pn=n(),b=i("div"),p(pt.$$.fragment),cs=n(),Ot=i("p"),Ot.innerHTML=er,ps=n(),p(O.$$.fragment),ms=n(),Kt=i("p"),Kt.textContent=tr,us=n(),eo=i("p"),eo.innerHTML=or,hs=n(),to=i("p"),to.innerHTML=nr,fs=n(),oo=i("p"),oo.innerHTML=sr,gs=n(),no=i("ul"),no.innerHTML=ar,_s=n(),so=i("p"),so.innerHTML=rr,bs=n(),ao=i("ul"),ao.innerHTML=ir,vs=n(),ro=i("p"),ro.innerHTML=lr,Ts=n(),K=i("div"),p(mt.$$.fragment),ks=n(),io=i("p"),io.innerHTML=dr,ys=n(),ee=i("div"),p(ut.$$.fragment),Ms=n(),lo=i("p"),lo.textContent=cr,xs=n(),te=i("div"),p(ht.$$.fragment),ws=n(),co=i("p"),co.textContent=pr,Cs=n(),oe=i("div"),p(ft.$$.fragment),zs=n(),po=i("p"),po.textContent=mr,$s=n(),ne=i("div"),p(gt.$$.fragment),Js=n(),mo=i("p"),mo.textContent=ur,qs=n(),se=i("div"),p(_t.$$.fragment),Is=n(),uo=i("p"),uo.textContent=hr,js=n(),ae=i("div"),p(bt.$$.fragment),Fs=n(),ho=i("p"),ho.innerHTML=fr,Ls=n(),re=i("div"),p(vt.$$.fragment),Us=n(),fo=i("p"),fo.innerHTML=gr,Ws=n(),W=i("div"),p(Tt.$$.fragment),Ns=n(),go=i("p"),go.textContent=_r,Bs=n(),_o=i("p"),_o.textContent=br,Hs=n(),L=i("div"),p(kt.$$.fragment),Ps=n(),bo=i("p"),bo.textContent=vr,Zs=n(),vo=i("p"),vo.innerHTML=Tr,Vs=n(),p(ie.$$.fragment),Gs=n(),le=i("div"),p(yt.$$.fragment),Es=n(),To=i("p"),To.textContent=kr,As=n(),N=i("div"),p(Mt.$$.fragment),Rs=n(),ko=i("p"),ko.textContent=yr,Xs=n(),yo=i("p"),yo.innerHTML=Mr,Ss=n(),B=i("div"),p(xt.$$.fragment),Ds=n(),Mo=i("p"),Mo.textContent=xr,Qs=n(),xo=i("p"),xo.textContent=wr,Ys=n(),de=i("div"),p(wt.$$.fragment),Os=n(),wo=i("p"),wo.textContent=Cr,Zn=n(),p(Ct.$$.fragment),Vn=n(),q=i("div"),p(zt.$$.fragment),Ks=n(),Co=i("p"),Co.textContent=zr,ea=n(),zo=i("p"),zo.innerHTML=$r,ta=n(),$o=i("p"),$o.innerHTML=Jr,oa=n(),H=i("div"),p($t.$$.fragment),na=n(),Jo=i("p"),Jo.innerHTML=qr,sa=n(),p(ce.$$.fragment),Gn=n(),p(Jt.$$.fragment),En=n(),I=i("div"),p(qt.$$.fragment),aa=n(),qo=i("p"),qo.textContent=Ir,ra=n(),Io=i("p"),Io.innerHTML=jr,ia=n(),jo=i("p"),jo.innerHTML=Fr,la=n(),U=i("div"),p(It.$$.fragment),da=n(),Fo=i("p"),Fo.innerHTML=Lr,ca=n(),p(pe.$$.fragment),pa=n(),p(me.$$.fragment),An=n(),p(jt.$$.fragment),Rn=n(),A=i("div"),p(Ft.$$.fragment),ma=n(),P=i("div"),p(Lt.$$.fragment),ua=n(),Lo=i("p"),Lo.innerHTML=Ur,ha=n(),p(ue.$$.fragment),Xn=n(),p(Ut.$$.fragment),Sn=n(),R=i("div"),p(Wt.$$.fragment),fa=n(),Z=i("div"),p(Nt.$$.fragment),ga=n(),Uo=i("p"),Uo.innerHTML=Wr,_a=n(),p(he.$$.fragment),Dn=n(),p(Bt.$$.fragment),Qn=n(),X=i("div"),p(Ht.$$.fragment),ba=n(),V=i("div"),p(Pt.$$.fragment),va=n(),Wo=i("p"),Wo.innerHTML=Nr,Ta=n(),p(fe.$$.fragment),Yn=n(),p(Zt.$$.fragment),On=n(),Ao=i("p"),this.h()},l(e){const t=Vr("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),k=s(e),_=l(e,"P",{}),x(_).forEach(o),T=s(e),z=l(e,"P",{"data-svelte-h":!0}),c(z)!=="svelte-8vo2ki"&&(z.innerHTML=y),F=s(e),m(ve.$$.fragment,e),Ro=s(e),D=l(e,"DIV",{class:!0,"data-svelte-h":!0}),c(D)!=="svelte-is43db"&&(D.innerHTML=ya),Xo=s(e),m(Te.$$.fragment,e),So=s(e),ke=l(e,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-1udwfr5"&&(ke.innerHTML=Ma),Do=s(e),ye=l(e,"P",{"data-svelte-h":!0}),c(ye)!=="svelte-mxegxd"&&(ye.textContent=xa),Qo=s(e),Me=l(e,"P",{"data-svelte-h":!0}),c(Me)!=="svelte-1xjcuti"&&(Me.innerHTML=wa),Yo=s(e),xe=l(e,"P",{"data-svelte-h":!0}),c(xe)!=="svelte-1421v88"&&(xe.innerHTML=Ca),Oo=s(e),m(we.$$.fragment,e),Ko=s(e),Ce=l(e,"P",{"data-svelte-h":!0}),c(Ce)!=="svelte-3oc3cl"&&(Ce.textContent=za),en=s(e),ze=l(e,"UL",{"data-svelte-h":!0}),c(ze)!=="svelte-1id2ggu"&&(ze.innerHTML=$a),tn=s(e),$e=l(e,"P",{"data-svelte-h":!0}),c($e)!=="svelte-13twqsf"&&($e.innerHTML=Ja),on=s(e),Je=l(e,"UL",{"data-svelte-h":!0}),c(Je)!=="svelte-hqpplt"&&(Je.innerHTML=qa),nn=s(e),qe=l(e,"P",{"data-svelte-h":!0}),c(qe)!=="svelte-diqoxa"&&(qe.innerHTML=Ia),sn=s(e),m(Ie.$$.fragment,e),an=s(e),je=l(e,"P",{"data-svelte-h":!0}),c(je)!=="svelte-ne80pp"&&(je.innerHTML=ja),rn=s(e),m(Fe.$$.fragment,e),ln=s(e),Le=l(e,"P",{"data-svelte-h":!0}),c(Le)!=="svelte-1yo8vts"&&(Le.textContent=Fa),dn=s(e),Ue=l(e,"UL",{"data-svelte-h":!0}),c(Ue)!=="svelte-pt1rre"&&(Ue.innerHTML=La),cn=s(e),We=l(e,"P",{"data-svelte-h":!0}),c(We)!=="svelte-18cq7lr"&&(We.textContent=Ua),pn=s(e),m(Ne.$$.fragment,e),mn=s(e),Be=l(e,"P",{"data-svelte-h":!0}),c(Be)!=="svelte-14idvxg"&&(Be.textContent=Wa),un=s(e),m(He.$$.fragment,e),hn=s(e),Pe=l(e,"P",{"data-svelte-h":!0}),c(Pe)!=="svelte-11xhi4l"&&(Pe.innerHTML=Na),fn=s(e),m(Ze.$$.fragment,e),gn=s(e),Ve=l(e,"P",{"data-svelte-h":!0}),c(Ve)!=="svelte-uke0yx"&&(Ve.innerHTML=Ba),_n=s(e),Ge=l(e,"P",{"data-svelte-h":!0}),c(Ge)!=="svelte-o3pzzu"&&(Ge.textContent=Ha),bn=s(e),m(Ee.$$.fragment,e),vn=s(e),Ae=l(e,"P",{"data-svelte-h":!0}),c(Ae)!=="svelte-1ehkek3"&&(Ae.innerHTML=Pa),Tn=s(e),Re=l(e,"P",{"data-svelte-h":!0}),c(Re)!=="svelte-15f76ro"&&(Re.textContent=Za),kn=s(e),m(Xe.$$.fragment,e),yn=s(e),m(Se.$$.fragment,e),Mn=s(e),De=l(e,"P",{"data-svelte-h":!0}),c(De)!=="svelte-1jfvy1t"&&(De.innerHTML=Va),xn=s(e),Q=l(e,"DIV",{style:!0,"data-svelte-h":!0}),c(Q)!=="svelte-lcfs6y"&&(Q.innerHTML=Ga),wn=s(e),m(Qe.$$.fragment,e),Cn=s(e),Ye=l(e,"P",{"data-svelte-h":!0}),c(Ye)!=="svelte-10i6fhp"&&(Ye.innerHTML=Ea),zn=s(e),Oe=l(e,"P",{"data-svelte-h":!0}),c(Oe)!=="svelte-1bvsrfr"&&(Oe.innerHTML=Aa),$n=s(e),m(Ke.$$.fragment,e),Jn=s(e),et=l(e,"P",{"data-svelte-h":!0}),c(et)!=="svelte-18wreyn"&&(et.innerHTML=Ra),qn=s(e),tt=l(e,"P",{"data-svelte-h":!0}),c(tt)!=="svelte-8bbp20"&&(tt.innerHTML=Xa),In=s(e),m(ot.$$.fragment,e),jn=s(e),nt=l(e,"P",{"data-svelte-h":!0}),c(nt)!=="svelte-1wxpeky"&&(nt.innerHTML=Sa),Fn=s(e),m(st.$$.fragment,e),Ln=s(e),at=l(e,"P",{"data-svelte-h":!0}),c(at)!=="svelte-1ci19ty"&&(at.textContent=Da),Un=s(e),m(rt.$$.fragment,e),Wn=s(e),it=l(e,"UL",{"data-svelte-h":!0}),c(it)!=="svelte-xldioh"&&(it.innerHTML=Qa),Nn=s(e),m(lt.$$.fragment,e),Bn=s(e),J=l(e,"DIV",{class:!0});var j=x(J);m(dt.$$.fragment,j),rs=s(j),Dt=l(j,"P",{"data-svelte-h":!0}),c(Dt)!=="svelte-1c0ul7j"&&(Dt.innerHTML=Ya),is=s(j),Qt=l(j,"P",{"data-svelte-h":!0}),c(Qt)!=="svelte-1nulibo"&&(Qt.innerHTML=Oa),ls=s(j),Yt=l(j,"P",{"data-svelte-h":!0}),c(Yt)!=="svelte-1ek1ss9"&&(Yt.innerHTML=Ka),ds=s(j),m(Y.$$.fragment,j),j.forEach(o),Hn=s(e),m(ct.$$.fragment,e),Pn=s(e),b=l(e,"DIV",{class:!0});var v=x(b);m(pt.$$.fragment,v),cs=s(v),Ot=l(v,"P",{"data-svelte-h":!0}),c(Ot)!=="svelte-iuk2y8"&&(Ot.innerHTML=er),ps=s(v),m(O.$$.fragment,v),ms=s(v),Kt=l(v,"P",{"data-svelte-h":!0}),c(Kt)!=="svelte-kud278"&&(Kt.textContent=tr),us=s(v),eo=l(v,"P",{"data-svelte-h":!0}),c(eo)!=="svelte-ifzpy9"&&(eo.innerHTML=or),hs=s(v),to=l(v,"P",{"data-svelte-h":!0}),c(to)!=="svelte-ktmcb2"&&(to.innerHTML=nr),fs=s(v),oo=l(v,"P",{"data-svelte-h":!0}),c(oo)!=="svelte-mzof2m"&&(oo.innerHTML=sr),gs=s(v),no=l(v,"UL",{"data-svelte-h":!0}),c(no)!=="svelte-1hlq74o"&&(no.innerHTML=ar),_s=s(v),so=l(v,"P",{"data-svelte-h":!0}),c(so)!=="svelte-k8piyc"&&(so.innerHTML=rr),bs=s(v),ao=l(v,"UL",{"data-svelte-h":!0}),c(ao)!=="svelte-mjbefh"&&(ao.innerHTML=ir),vs=s(v),ro=l(v,"P",{"data-svelte-h":!0}),c(ro)!=="svelte-18hne1"&&(ro.innerHTML=lr),Ts=s(v),K=l(v,"DIV",{class:!0});var Vt=x(K);m(mt.$$.fragment,Vt),ks=s(Vt),io=l(Vt,"P",{"data-svelte-h":!0}),c(io)!=="svelte-sr2voc"&&(io.innerHTML=dr),Vt.forEach(o),ys=s(v),ee=l(v,"DIV",{class:!0});var Gt=x(ee);m(ut.$$.fragment,Gt),Ms=s(Gt),lo=l(Gt,"P",{"data-svelte-h":!0}),c(lo)!=="svelte-1deng2j"&&(lo.textContent=cr),Gt.forEach(o),xs=s(v),te=l(v,"DIV",{class:!0});var Et=x(te);m(ht.$$.fragment,Et),ws=s(Et),co=l(Et,"P",{"data-svelte-h":!0}),c(co)!=="svelte-cx157h"&&(co.textContent=pr),Et.forEach(o),Cs=s(v),oe=l(v,"DIV",{class:!0});var At=x(oe);m(ft.$$.fragment,At),zs=s(At),po=l(At,"P",{"data-svelte-h":!0}),c(po)!=="svelte-1urz5jj"&&(po.textContent=mr),At.forEach(o),$s=s(v),ne=l(v,"DIV",{class:!0});var Rt=x(ne);m(gt.$$.fragment,Rt),Js=s(Rt),mo=l(Rt,"P",{"data-svelte-h":!0}),c(mo)!=="svelte-vbfkpu"&&(mo.textContent=ur),Rt.forEach(o),qs=s(v),se=l(v,"DIV",{class:!0});var Xt=x(se);m(_t.$$.fragment,Xt),Is=s(Xt),uo=l(Xt,"P",{"data-svelte-h":!0}),c(uo)!=="svelte-12b8hzo"&&(uo.textContent=hr),Xt.forEach(o),js=s(v),ae=l(v,"DIV",{class:!0});var St=x(ae);m(bt.$$.fragment,St),Fs=s(St),ho=l(St,"P",{"data-svelte-h":!0}),c(ho)!=="svelte-5j01oy"&&(ho.innerHTML=fr),St.forEach(o),Ls=s(v),re=l(v,"DIV",{class:!0});var es=x(re);m(vt.$$.fragment,es),Us=s(es),fo=l(es,"P",{"data-svelte-h":!0}),c(fo)!=="svelte-1wmjg8a"&&(fo.innerHTML=gr),es.forEach(o),Ws=s(v),W=l(v,"DIV",{class:!0});var No=x(W);m(Tt.$$.fragment,No),Ns=s(No),go=l(No,"P",{"data-svelte-h":!0}),c(go)!=="svelte-1gbatu6"&&(go.textContent=_r),Bs=s(No),_o=l(No,"P",{"data-svelte-h":!0}),c(_o)!=="svelte-1d4v47d"&&(_o.textContent=br),No.forEach(o),Hs=s(v),L=l(v,"DIV",{class:!0});var ge=x(L);m(kt.$$.fragment,ge),Ps=s(ge),bo=l(ge,"P",{"data-svelte-h":!0}),c(bo)!=="svelte-1n892mi"&&(bo.textContent=vr),Zs=s(ge),vo=l(ge,"P",{"data-svelte-h":!0}),c(vo)!=="svelte-954lq4"&&(vo.innerHTML=Tr),Vs=s(ge),m(ie.$$.fragment,ge),ge.forEach(o),Gs=s(v),le=l(v,"DIV",{class:!0});var ts=x(le);m(yt.$$.fragment,ts),Es=s(ts),To=l(ts,"P",{"data-svelte-h":!0}),c(To)!=="svelte-15kr77e"&&(To.textContent=kr),ts.forEach(o),As=s(v),N=l(v,"DIV",{class:!0});var Bo=x(N);m(Mt.$$.fragment,Bo),Rs=s(Bo),ko=l(Bo,"P",{"data-svelte-h":!0}),c(ko)!=="svelte-u73u19"&&(ko.textContent=yr),Xs=s(Bo),yo=l(Bo,"P",{"data-svelte-h":!0}),c(yo)!=="svelte-oagoqu"&&(yo.innerHTML=Mr),Bo.forEach(o),Ss=s(v),B=l(v,"DIV",{class:!0});var Ho=x(B);m(xt.$$.fragment,Ho),Ds=s(Ho),Mo=l(Ho,"P",{"data-svelte-h":!0}),c(Mo)!=="svelte-sso1qb"&&(Mo.textContent=xr),Qs=s(Ho),xo=l(Ho,"P",{"data-svelte-h":!0}),c(xo)!=="svelte-46tdba"&&(xo.textContent=wr),Ho.forEach(o),Ys=s(v),de=l(v,"DIV",{class:!0});var os=x(de);m(wt.$$.fragment,os),Os=s(os),wo=l(os,"P",{"data-svelte-h":!0}),c(wo)!=="svelte-fkofn"&&(wo.textContent=Cr),os.forEach(o),v.forEach(o),Zn=s(e),m(Ct.$$.fragment,e),Vn=s(e),q=l(e,"DIV",{class:!0});var G=x(q);m(zt.$$.fragment,G),Ks=s(G),Co=l(G,"P",{"data-svelte-h":!0}),c(Co)!=="svelte-ydij5d"&&(Co.textContent=zr),ea=s(G),zo=l(G,"P",{"data-svelte-h":!0}),c(zo)!=="svelte-q52n56"&&(zo.innerHTML=$r),ta=s(G),$o=l(G,"P",{"data-svelte-h":!0}),c($o)!=="svelte-hswkmf"&&($o.innerHTML=Jr),oa=s(G),H=l(G,"DIV",{class:!0});var Po=x(H);m($t.$$.fragment,Po),na=s(Po),Jo=l(Po,"P",{"data-svelte-h":!0}),c(Jo)!=="svelte-y55rss"&&(Jo.innerHTML=qr),sa=s(Po),m(ce.$$.fragment,Po),Po.forEach(o),G.forEach(o),Gn=s(e),m(Jt.$$.fragment,e),En=s(e),I=l(e,"DIV",{class:!0});var E=x(I);m(qt.$$.fragment,E),aa=s(E),qo=l(E,"P",{"data-svelte-h":!0}),c(qo)!=="svelte-8psjxm"&&(qo.textContent=Ir),ra=s(E),Io=l(E,"P",{"data-svelte-h":!0}),c(Io)!=="svelte-q52n56"&&(Io.innerHTML=jr),ia=s(E),jo=l(E,"P",{"data-svelte-h":!0}),c(jo)!=="svelte-hswkmf"&&(jo.innerHTML=Fr),la=s(E),U=l(E,"DIV",{class:!0});var _e=x(U);m(It.$$.fragment,_e),da=s(_e),Fo=l(_e,"P",{"data-svelte-h":!0}),c(Fo)!=="svelte-kbqf6k"&&(Fo.innerHTML=Lr),ca=s(_e),m(pe.$$.fragment,_e),pa=s(_e),m(me.$$.fragment,_e),_e.forEach(o),E.forEach(o),An=s(e),m(jt.$$.fragment,e),Rn=s(e),A=l(e,"DIV",{class:!0});var ns=x(A);m(Ft.$$.fragment,ns),ma=s(ns),P=l(ns,"DIV",{class:!0});var Zo=x(P);m(Lt.$$.fragment,Zo),ua=s(Zo),Lo=l(Zo,"P",{"data-svelte-h":!0}),c(Lo)!=="svelte-1sal4ui"&&(Lo.innerHTML=Ur),ha=s(Zo),m(ue.$$.fragment,Zo),Zo.forEach(o),ns.forEach(o),Xn=s(e),m(Ut.$$.fragment,e),Sn=s(e),R=l(e,"DIV",{class:!0});var ss=x(R);m(Wt.$$.fragment,ss),fa=s(ss),Z=l(ss,"DIV",{class:!0});var Vo=x(Z);m(Nt.$$.fragment,Vo),ga=s(Vo),Uo=l(Vo,"P",{"data-svelte-h":!0}),c(Uo)!=="svelte-1py4aay"&&(Uo.innerHTML=Wr),_a=s(Vo),m(he.$$.fragment,Vo),Vo.forEach(o),ss.forEach(o),Dn=s(e),m(Bt.$$.fragment,e),Qn=s(e),X=l(e,"DIV",{class:!0});var as=x(X);m(Ht.$$.fragment,as),ba=s(as),V=l(as,"DIV",{class:!0});var Go=x(V);m(Pt.$$.fragment,Go),va=s(Go),Wo=l(Go,"P",{"data-svelte-h":!0}),c(Wo)!=="svelte-dyrov9"&&(Wo.innerHTML=Nr),Ta=s(Go),m(fe.$$.fragment,Go),Go.forEach(o),as.forEach(o),Yn=s(e),m(Zt.$$.fragment,e),On=s(e),Ao=l(e,"P",{}),x(Ao).forEach(o),this.h()},h(){M(d,"name","hf:doc:metadata"),M(d,"content",oi),M(D,"class","flex flex-wrap space-x-1"),Gr(Q,"text-align","center"),M(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){a(document.head,d),r(e,k,t),r(e,_,t),r(e,T,t),r(e,z,t),r(e,F,t),u(ve,e,t),r(e,Ro,t),r(e,D,t),r(e,Xo,t),u(Te,e,t),r(e,So,t),r(e,ke,t),r(e,Do,t),r(e,ye,t),r(e,Qo,t),r(e,Me,t),r(e,Yo,t),r(e,xe,t),r(e,Oo,t),u(we,e,t),r(e,Ko,t),r(e,Ce,t),r(e,en,t),r(e,ze,t),r(e,tn,t),r(e,$e,t),r(e,on,t),r(e,Je,t),r(e,nn,t),r(e,qe,t),r(e,sn,t),u(Ie,e,t),r(e,an,t),r(e,je,t),r(e,rn,t),u(Fe,e,t),r(e,ln,t),r(e,Le,t),r(e,dn,t),r(e,Ue,t),r(e,cn,t),r(e,We,t),r(e,pn,t),u(Ne,e,t),r(e,mn,t),r(e,Be,t),r(e,un,t),u(He,e,t),r(e,hn,t),r(e,Pe,t),r(e,fn,t),u(Ze,e,t),r(e,gn,t),r(e,Ve,t),r(e,_n,t),r(e,Ge,t),r(e,bn,t),u(Ee,e,t),r(e,vn,t),r(e,Ae,t),r(e,Tn,t),r(e,Re,t),r(e,kn,t),u(Xe,e,t),r(e,yn,t),u(Se,e,t),r(e,Mn,t),r(e,De,t),r(e,xn,t),r(e,Q,t),r(e,wn,t),u(Qe,e,t),r(e,Cn,t),r(e,Ye,t),r(e,zn,t),r(e,Oe,t),r(e,$n,t),u(Ke,e,t),r(e,Jn,t),r(e,et,t),r(e,qn,t),r(e,tt,t),r(e,In,t),u(ot,e,t),r(e,jn,t),r(e,nt,t),r(e,Fn,t),u(st,e,t),r(e,Ln,t),r(e,at,t),r(e,Un,t),u(rt,e,t),r(e,Wn,t),r(e,it,t),r(e,Nn,t),u(lt,e,t),r(e,Bn,t),r(e,J,t),u(dt,J,null),a(J,rs),a(J,Dt),a(J,is),a(J,Qt),a(J,ls),a(J,Yt),a(J,ds),u(Y,J,null),r(e,Hn,t),u(ct,e,t),r(e,Pn,t),r(e,b,t),u(pt,b,null),a(b,cs),a(b,Ot),a(b,ps),u(O,b,null),a(b,ms),a(b,Kt),a(b,us),a(b,eo),a(b,hs),a(b,to),a(b,fs),a(b,oo),a(b,gs),a(b,no),a(b,_s),a(b,so),a(b,bs),a(b,ao),a(b,vs),a(b,ro),a(b,Ts),a(b,K),u(mt,K,null),a(K,ks),a(K,io),a(b,ys),a(b,ee),u(ut,ee,null),a(ee,Ms),a(ee,lo),a(b,xs),a(b,te),u(ht,te,null),a(te,ws),a(te,co),a(b,Cs),a(b,oe),u(ft,oe,null),a(oe,zs),a(oe,po),a(b,$s),a(b,ne),u(gt,ne,null),a(ne,Js),a(ne,mo),a(b,qs),a(b,se),u(_t,se,null),a(se,Is),a(se,uo),a(b,js),a(b,ae),u(bt,ae,null),a(ae,Fs),a(ae,ho),a(b,Ls),a(b,re),u(vt,re,null),a(re,Us),a(re,fo),a(b,Ws),a(b,W),u(Tt,W,null),a(W,Ns),a(W,go),a(W,Bs),a(W,_o),a(b,Hs),a(b,L),u(kt,L,null),a(L,Ps),a(L,bo),a(L,Zs),a(L,vo),a(L,Vs),u(ie,L,null),a(b,Gs),a(b,le),u(yt,le,null),a(le,Es),a(le,To),a(b,As),a(b,N),u(Mt,N,null),a(N,Rs),a(N,ko),a(N,Xs),a(N,yo),a(b,Ss),a(b,B),u(xt,B,null),a(B,Ds),a(B,Mo),a(B,Qs),a(B,xo),a(b,Ys),a(b,de),u(wt,de,null),a(de,Os),a(de,wo),r(e,Zn,t),u(Ct,e,t),r(e,Vn,t),r(e,q,t),u(zt,q,null),a(q,Ks),a(q,Co),a(q,ea),a(q,zo),a(q,ta),a(q,$o),a(q,oa),a(q,H),u($t,H,null),a(H,na),a(H,Jo),a(H,sa),u(ce,H,null),r(e,Gn,t),u(Jt,e,t),r(e,En,t),r(e,I,t),u(qt,I,null),a(I,aa),a(I,qo),a(I,ra),a(I,Io),a(I,ia),a(I,jo),a(I,la),a(I,U),u(It,U,null),a(U,da),a(U,Fo),a(U,ca),u(pe,U,null),a(U,pa),u(me,U,null),r(e,An,t),u(jt,e,t),r(e,Rn,t),r(e,A,t),u(Ft,A,null),a(A,ma),a(A,P),u(Lt,P,null),a(P,ua),a(P,Lo),a(P,ha),u(ue,P,null),r(e,Xn,t),u(Ut,e,t),r(e,Sn,t),r(e,R,t),u(Wt,R,null),a(R,fa),a(R,Z),u(Nt,Z,null),a(Z,ga),a(Z,Uo),a(Z,_a),u(he,Z,null),r(e,Dn,t),u(Bt,e,t),r(e,Qn,t),r(e,X,t),u(Ht,X,null),a(X,ba),a(X,V),u(Pt,V,null),a(V,va),a(V,Wo),a(V,Ta),u(fe,V,null),r(e,Yn,t),u(Zt,e,t),r(e,On,t),r(e,Ao,t),Kn=!0},p(e,[t]){const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),Y.$set(j);const v={};t&2&&(v.$$scope={dirty:t,ctx:e}),O.$set(v);const Vt={};t&2&&(Vt.$$scope={dirty:t,ctx:e}),ie.$set(Vt);const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),ce.$set(Gt);const Et={};t&2&&(Et.$$scope={dirty:t,ctx:e}),pe.$set(Et);const At={};t&2&&(At.$$scope={dirty:t,ctx:e}),me.$set(At);const Rt={};t&2&&(Rt.$$scope={dirty:t,ctx:e}),ue.$set(Rt);const Xt={};t&2&&(Xt.$$scope={dirty:t,ctx:e}),he.$set(Xt);const St={};t&2&&(St.$$scope={dirty:t,ctx:e}),fe.$set(St)},i(e){Kn||(h(ve.$$.fragment,e),h(Te.$$.fragment,e),h(we.$$.fragment,e),h(Ie.$$.fragment,e),h(Fe.$$.fragment,e),h(Ne.$$.fragment,e),h(He.$$.fragment,e),h(Ze.$$.fragment,e),h(Ee.$$.fragment,e),h(Xe.$$.fragment,e),h(Se.$$.fragment,e),h(Qe.$$.fragment,e),h(Ke.$$.fragment,e),h(ot.$$.fragment,e),h(st.$$.fragment,e),h(rt.$$.fragment,e),h(lt.$$.fragment,e),h(dt.$$.fragment,e),h(Y.$$.fragment,e),h(ct.$$.fragment,e),h(pt.$$.fragment,e),h(O.$$.fragment,e),h(mt.$$.fragment,e),h(ut.$$.fragment,e),h(ht.$$.fragment,e),h(ft.$$.fragment,e),h(gt.$$.fragment,e),h(_t.$$.fragment,e),h(bt.$$.fragment,e),h(vt.$$.fragment,e),h(Tt.$$.fragment,e),h(kt.$$.fragment,e),h(ie.$$.fragment,e),h(yt.$$.fragment,e),h(Mt.$$.fragment,e),h(xt.$$.fragment,e),h(wt.$$.fragment,e),h(Ct.$$.fragment,e),h(zt.$$.fragment,e),h($t.$$.fragment,e),h(ce.$$.fragment,e),h(Jt.$$.fragment,e),h(qt.$$.fragment,e),h(It.$$.fragment,e),h(pe.$$.fragment,e),h(me.$$.fragment,e),h(jt.$$.fragment,e),h(Ft.$$.fragment,e),h(Lt.$$.fragment,e),h(ue.$$.fragment,e),h(Ut.$$.fragment,e),h(Wt.$$.fragment,e),h(Nt.$$.fragment,e),h(he.$$.fragment,e),h(Bt.$$.fragment,e),h(Ht.$$.fragment,e),h(Pt.$$.fragment,e),h(fe.$$.fragment,e),h(Zt.$$.fragment,e),Kn=!0)},o(e){f(ve.$$.fragment,e),f(Te.$$.fragment,e),f(we.$$.fragment,e),f(Ie.$$.fragment,e),f(Fe.$$.fragment,e),f(Ne.$$.fragment,e),f(He.$$.fragment,e),f(Ze.$$.fragment,e),f(Ee.$$.fragment,e),f(Xe.$$.fragment,e),f(Se.$$.fragment,e),f(Qe.$$.fragment,e),f(Ke.$$.fragment,e),f(ot.$$.fragment,e),f(st.$$.fragment,e),f(rt.$$.fragment,e),f(lt.$$.fragment,e),f(dt.$$.fragment,e),f(Y.$$.fragment,e),f(ct.$$.fragment,e),f(pt.$$.fragment,e),f(O.$$.fragment,e),f(mt.$$.fragment,e),f(ut.$$.fragment,e),f(ht.$$.fragment,e),f(ft.$$.fragment,e),f(gt.$$.fragment,e),f(_t.$$.fragment,e),f(bt.$$.fragment,e),f(vt.$$.fragment,e),f(Tt.$$.fragment,e),f(kt.$$.fragment,e),f(ie.$$.fragment,e),f(yt.$$.fragment,e),f(Mt.$$.fragment,e),f(xt.$$.fragment,e),f(wt.$$.fragment,e),f(Ct.$$.fragment,e),f(zt.$$.fragment,e),f($t.$$.fragment,e),f(ce.$$.fragment,e),f(Jt.$$.fragment,e),f(qt.$$.fragment,e),f(It.$$.fragment,e),f(pe.$$.fragment,e),f(me.$$.fragment,e),f(jt.$$.fragment,e),f(Ft.$$.fragment,e),f(Lt.$$.fragment,e),f(ue.$$.fragment,e),f(Ut.$$.fragment,e),f(Wt.$$.fragment,e),f(Nt.$$.fragment,e),f(he.$$.fragment,e),f(Bt.$$.fragment,e),f(Ht.$$.fragment,e),f(Pt.$$.fragment,e),f(fe.$$.fragment,e),f(Zt.$$.fragment,e),Kn=!1},d(e){e&&(o(k),o(_),o(T),o(z),o(F),o(Ro),o(D),o(Xo),o(So),o(ke),o(Do),o(ye),o(Qo),o(Me),o(Yo),o(xe),o(Oo),o(Ko),o(Ce),o(en),o(ze),o(tn),o($e),o(on),o(Je),o(nn),o(qe),o(sn),o(an),o(je),o(rn),o(ln),o(Le),o(dn),o(Ue),o(cn),o(We),o(pn),o(mn),o(Be),o(un),o(hn),o(Pe),o(fn),o(gn),o(Ve),o(_n),o(Ge),o(bn),o(vn),o(Ae),o(Tn),o(Re),o(kn),o(yn),o(Mn),o(De),o(xn),o(Q),o(wn),o(Cn),o(Ye),o(zn),o(Oe),o($n),o(Jn),o(et),o(qn),o(tt),o(In),o(jn),o(nt),o(Fn),o(Ln),o(at),o(Un),o(Wn),o(it),o(Nn),o(Bn),o(J),o(Hn),o(Pn),o(b),o(Zn),o(Vn),o(q),o(Gn),o(En),o(I),o(An),o(Rn),o(A),o(Xn),o(Sn),o(R),o(Dn),o(Qn),o(X),o(Yn),o(On),o(Ao)),o(d),g(ve,e),g(Te,e),g(we,e),g(Ie,e),g(Fe,e),g(Ne,e),g(He,e),g(Ze,e),g(Ee,e),g(Xe,e),g(Se,e),g(Qe,e),g(Ke,e),g(ot,e),g(st,e),g(rt,e),g(lt,e),g(dt),g(Y),g(ct,e),g(pt),g(O),g(mt),g(ut),g(ht),g(ft),g(gt),g(_t),g(bt),g(vt),g(Tt),g(kt),g(ie),g(yt),g(Mt),g(xt),g(wt),g(Ct,e),g(zt),g($t),g(ce),g(Jt,e),g(qt),g(It),g(pe),g(me),g(jt,e),g(Ft),g(Lt),g(ue),g(Ut,e),g(Wt),g(Nt),g(he),g(Bt,e),g(Ht),g(Pt),g(fe),g(Zt,e)}}}const oi='{"title":"Mixtral","local":"mixtral","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Architectural details","local":"architectural-details","sections":[],"depth":3},{"title":"License","local":"license","sections":[],"depth":3}],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Speeding up Mixtral by using Flash Attention","local":"speeding-up-mixtral-by-using-flash-attention","sections":[{"title":"Expected speedups","local":"expected-speedups","sections":[],"depth":3},{"title":"Sliding window Attention","local":"sliding-window-attention","sections":[],"depth":3}],"depth":2},{"title":"Shrinking down Mixtral using quantization","local":"shrinking-down-mixtral-using-quantization","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"MixtralConfig","local":"transformers.MixtralConfig","sections":[],"depth":2},{"title":"MistralCommonTokenizer","local":"transformers.MistralCommonTokenizer","sections":[],"depth":2},{"title":"MixtralModel","local":"transformers.MixtralModel","sections":[],"depth":2},{"title":"MixtralForCausalLM","local":"transformers.MixtralForCausalLM","sections":[],"depth":2},{"title":"MixtralForSequenceClassification","local":"transformers.MixtralForSequenceClassification","sections":[],"depth":2},{"title":"MixtralForTokenClassification","local":"transformers.MixtralForTokenClassification","sections":[],"depth":2},{"title":"MixtralForQuestionAnswering","local":"transformers.MixtralForQuestionAnswering","sections":[],"depth":2}],"depth":1}';function ni(C){return Hr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class mi extends Pr{constructor(d){super(),Zr(this,d,ni,ti,Br,{})}}export{mi as component};
