import{s as At,o as Gt,n as Qt}from"../chunks/scheduler.18a86fab.js";import{S as Wt,i as Bt,g as o,s as a,r as p,A as Zt,h as r,f as s,c as l,j as xt,u as m,x as h,k as ot,y as Ht,a as n,v as u,d as c,t as f,w as d}from"../chunks/index.98837b22.js";import{T as It}from"../chunks/Tip.77304350.js";import{Y as Vt}from"../chunks/Youtube.14fb207c.js";import{C as y}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ue,E as Et}from"../chunks/getInferenceSnippets.06c2775f.js";function Rt(ce){let i,$='Refer to the Performance <a href="performance">guide</a> for more details about memory-saving techniques.';return{c(){i=o("p"),i.innerHTML=$},l(g){i=r(g,"P",{"data-svelte-h":!0}),h(i)!=="svelte-xgjmzv"&&(i.innerHTML=$)},m(g,b){n(g,i,b)},p:Qt,d(g){g&&s(i)}}}function Nt(ce){let i,$="By default, the tokenizer creates an <code>attention_mask</code> for you based on your specific tokenizerâ€™s defaults.";return{c(){i=o("p"),i.innerHTML=$},l(g){i=r(g,"P",{"data-svelte-h":!0}),h(i)!=="svelte-fvd42m"&&(i.innerHTML=$)},m(g,b){n(g,i,b)},p:Qt,d(g){g&&s(i)}}}function Ft(ce){let i,$,g,b,T,de,v,rt="Sometimes errors occur, but we are here to help! This guide covers some of the most common issues weâ€™ve seen and how you can resolve them. However, this guide isnâ€™t meant to be a comprehensive collection of every ðŸ¤— Transformers issue. For more help with troubleshooting your issue, try:",he,k,ge,C,it='<li>Asking for help on the <a href="https://discuss.huggingface.co/" rel="nofollow">forums</a>. There are specific categories you can post your question to, like <a href="https://discuss.huggingface.co/c/beginners/5" rel="nofollow">Beginners</a> or <a href="https://discuss.huggingface.co/c/transformers/9" rel="nofollow">ðŸ¤— Transformers</a>. Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!</li>',ye,U,$e,M,pt='<li><p>Create an <a href="https://github.com/huggingface/transformers/issues/new/choose" rel="nofollow">Issue</a> on the ðŸ¤— Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out whatâ€™s wrong and how we can fix it.</p></li> <li><p>Check the <a href="migration">Migration</a> guide if you use an older version of ðŸ¤— Transformers since some important changes have been introduced between versions.</p></li>',be,J,mt='For more details about troubleshooting and getting help, take a look at <a href="https://huggingface.co/course/chapter8/1?fw=pt" rel="nofollow">Chapter 8</a> of the Hugging Face course.',Me,_,we,x,ut="Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:",je,I,Te,V,ct='In this case, you should try to run ðŸ¤— Transformers on <a href="installation#offline-mode">offline mode</a> to avoid the connection error.',ve,Q,ke,A,ft="Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:",Ce,G,Ue,W,dt="Here are some potential solutions you can try to lessen memory use:",Je,B,ht='<li>Reduce the <a href="main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size"><code>per_device_train_batch_size</code></a> value in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>.</li> <li>Try using <a href="main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps"><code>gradient_accumulation_steps</code></a> in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> to effectively increase overall batch size.</li>',_e,w,xe,Z,Ie,H,gt="Another common error you may encounter, especially if it is a newly released model, is <code>ImportError</code>:",Ve,E,Qe,R,yt="For these error types, check to make sure you have the latest version of ðŸ¤— Transformers installed to access the most recent models:",Ae,N,Ge,F,We,L,$t="Sometimes you may run into a generic CUDA error about an error in the device code.",Be,P,Ze,z,bt="You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to switch to a CPU:",He,X,Ee,S,Mt="Another option is to get a better traceback from the GPU. Add the following environment variable to the beginning of your code to get the traceback to point to the source of the error:",Re,Y,Ne,D,Fe,q,wt="In some cases, the output <code>hidden_state</code> may be incorrect if the <code>input_ids</code> include padding tokens. To demonstrate, load a model and tokenizer. You can access a modelâ€™s <code>pad_token_id</code> to see its value. The <code>pad_token_id</code> may be <code>None</code> for some models, but you can always manually set it.",Le,O,Pe,K,jt="The following example shows the output without masking the padding tokens:",ze,ee,Xe,te,Tt="Here is the actual output of the second sequence:",Se,se,Ye,ne,vt="Most of the time, you should provide an <code>attention_mask</code> to your model to ignore the padding tokens to avoid this silent error. Now the output of the second sequence matches its actual output:",De,j,qe,ae,Oe,le,kt="ðŸ¤— Transformers doesnâ€™t automatically create an <code>attention_mask</code> to mask a padding token if it is provided because:",Ke,oe,Ct="<li>Some models donâ€™t have a padding token.</li> <li>For some use-cases, users want a model to attend to a padding token.</li>",et,re,tt,ie,Ut=`Generally, we recommend using the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class to load pretrained instances of models. This class
can automatically infer and load the correct architecture from a given checkpoint based on the configuration. If you see
this <code>ValueError</code> when loading a model from a checkpoint, this means the Auto class couldnâ€™t find a mapping from
the configuration in the given checkpoint to the kind of model you are trying to load. Most commonly, this happens when a
checkpoint doesnâ€™t support a given task.
For instance, youâ€™ll see this error in the following example because there is no GPT2 for question answering:`,st,pe,nt,me,at,fe,lt;return T=new ue({props:{title:"Troubleshoot",local:"troubleshoot",headingTag:"h1"}}),k=new Vt({props:{id:"S2EEG3JIt2A"}}),U=new Vt({props:{id:"_PAli-V4wj0"}}),_=new ue({props:{title:"Firewalled environments",local:"firewalled-environments",headingTag:"h2"}}),I=new y({props:{code:"VmFsdWVFcnJvciUzQSUyMENvbm5lY3Rpb24lMjBlcnJvciUyQyUyMGFuZCUyMHdlJTIwY2Fubm90JTIwZmluZCUyMHRoZSUyMHJlcXVlc3RlZCUyMGZpbGVzJTIwaW4lMjB0aGUlMjBjYWNoZWQlMjBwYXRoLiUwQVBsZWFzZSUyMHRyeSUyMGFnYWluJTIwb3IlMjBtYWtlJTIwc3VyZSUyMHlvdXIlMjBJbnRlcm5ldCUyMGNvbm5lY3Rpb24lMjBpcyUyMG9uLg==",highlighted:`ValueError: Connection error, <span class="hljs-built_in">and</span> we cannot <span class="hljs-keyword">find</span> the requested <span class="hljs-keyword">files</span> in the cached path.
Please <span class="hljs-keyword">try</span> again <span class="hljs-built_in">or</span> <span class="hljs-keyword">make</span> sure your Internet connection <span class="hljs-keyword">is</span> <span class="hljs-keyword">on</span>.`,wrap:!1}}),Q=new ue({props:{title:"CUDA out of memory",local:"cuda-out-of-memory",headingTag:"h2"}}),G=new y({props:{code:"Q1VEQSUyMG91dCUyMG9mJTIwbWVtb3J5LiUyMFRyaWVkJTIwdG8lMjBhbGxvY2F0ZSUyMDI1Ni4wMCUyME1pQiUyMChHUFUlMjAwJTNCJTIwMTEuMTclMjBHaUIlMjB0b3RhbCUyMGNhcGFjaXR5JTNCJTIwOS43MCUyMEdpQiUyMGFscmVhZHklMjBhbGxvY2F0ZWQlM0IlMjAxNzkuODElMjBNaUIlMjBmcmVlJTNCJTIwOS44NSUyMEdpQiUyMHJlc2VydmVkJTIwaW4lMjB0b3RhbCUyMGJ5JTIwUHlUb3JjaCk=",highlighted:'<span class="hljs-attribute">CUDA</span> out of memory. Tried to allocate <span class="hljs-number">256</span>.<span class="hljs-number">00</span> MiB (GPU <span class="hljs-number">0</span>; <span class="hljs-number">11</span>.<span class="hljs-number">17</span> GiB total capacity; <span class="hljs-number">9</span>.<span class="hljs-number">70</span> GiB already allocated; <span class="hljs-number">179</span>.<span class="hljs-number">81</span> MiB free; <span class="hljs-number">9</span>.<span class="hljs-number">85</span> GiB reserved in total by PyTorch)',wrap:!1}}),w=new It({props:{$$slots:{default:[Rt]},$$scope:{ctx:ce}}}),Z=new ue({props:{title:"ImportError",local:"importerror",headingTag:"h2"}}),E=new y({props:{code:"SW1wb3J0RXJyb3IlM0ElMjBjYW5ub3QlMjBpbXBvcnQlMjBuYW1lJTIwJ0ltYWdlR1BUSW1hZ2VQcm9jZXNzb3InJTIwZnJvbSUyMCd0cmFuc2Zvcm1lcnMnJTIwKHVua25vd24lMjBsb2NhdGlvbik=",highlighted:'ImportError: cannot <span class="hljs-keyword">import</span> <span class="hljs-type">name</span> <span class="hljs-string">&#x27;ImageGPTImageProcessor&#x27;</span> <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;transformers&#x27;</span> (<span class="hljs-type">unknown</span> <span class="hljs-keyword">location</span>)',wrap:!1}}),N=new y({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMC0tdXBncmFkZQ==",highlighted:"pip install transformers --upgrade",wrap:!1}}),F=new ue({props:{title:"CUDA error: device-side assert triggered",local:"cuda-error-device-side-assert-triggered",headingTag:"h2"}}),P=new y({props:{code:"UnVudGltZUVycm9yJTNBJTIwQ1VEQSUyMGVycm9yJTNBJTIwZGV2aWNlLXNpZGUlMjBhc3NlcnQlMjB0cmlnZ2VyZWQ=",highlighted:'RuntimeError: CUDA <span class="hljs-literal">error</span>: device-<span class="hljs-literal">side</span> <span class="hljs-keyword">assert</span> triggered',wrap:!1}}),X=new y({props:{code:"aW1wb3J0JTIwb3MlMEElMEFvcy5lbnZpcm9uJTVCJTIyQ1VEQV9WSVNJQkxFX0RFVklDRVMlMjIlNUQlMjAlM0QlMjAlMjIlMjI=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;&quot;</span>`,wrap:!1}}),Y=new y({props:{code:"aW1wb3J0JTIwb3MlMEElMEFvcy5lbnZpcm9uJTVCJTIyQ1VEQV9MQVVOQ0hfQkxPQ0tJTkclMjIlNUQlMjAlM0QlMjAlMjIxJTIy",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_LAUNCH_BLOCKING&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span>`,wrap:!1}}),D=new ue({props:{title:"Incorrect output when padding tokens arenâ€™t masked",local:"incorrect-output-when-padding-tokens-arent-masked",headingTag:"h2"}}),O=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiklMEFtb2RlbC5jb25maWcucGFkX3Rva2VuX2lk",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id
<span class="hljs-number">0</span>`,wrap:!1}}),ee=new y({props:{code:"aW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QiU1Qjc1OTIlMkMlMjAyMDU3JTJDJTIwMjA5NyUyQyUyMDIzOTMlMkMlMjA5NjExJTJDJTIwMjExNSU1RCUyQyUyMCU1Qjc1OTIlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCU1RCU1RCklMEFvdXRwdXQlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMpJTBBcHJpbnQob3V0cHV0LmxvZ2l0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([[<span class="hljs-number">7592</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2097</span>, <span class="hljs-number">2393</span>, <span class="hljs-number">9611</span>, <span class="hljs-number">2115</span>], [<span class="hljs-number">7592</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[ <span class="hljs-number">0.0082</span>, -<span class="hljs-number">0.2307</span>],
        [ <span class="hljs-number">0.1317</span>, -<span class="hljs-number">0.1683</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),se=new y({props:{code:"aW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QiU1Qjc1OTIlNUQlNUQpJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwoaW5wdXRfaWRzKSUwQXByaW50KG91dHB1dC5sb2dpdHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([[<span class="hljs-number">7592</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[-<span class="hljs-number">0.1008</span>, -<span class="hljs-number">0.4061</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),j=new It({props:{$$slots:{default:[Nt]},$$scope:{ctx:ce}}}),ae=new y({props:{code:"YXR0ZW50aW9uX21hc2slMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCJTVCMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTJDJTIwJTVCMSUyQyUyMDAlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTVEJTVEKSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcyUyQyUyMGF0dGVudGlvbl9tYXNrJTNEYXR0ZW50aW9uX21hc2spJTBBcHJpbnQob3V0cHV0LmxvZ2l0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>attention_mask = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids, attention_mask=attention_mask)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[ <span class="hljs-number">0.0082</span>, -<span class="hljs-number">0.2307</span>],
        [-<span class="hljs-number">0.1008</span>, -<span class="hljs-number">0.4061</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),re=new ue({props:{title:"ValueError: Unrecognized configuration class XYZ for this kind of AutoModel",local:"valueerror-unrecognized-configuration-class-xyz-for-this-kind-of-automodel",headingTag:"h2"}}),pe=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZncHQyLW1lZGl1bSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGZ3B0Mi1tZWRpdW0lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2-medium&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2-medium&quot;</span>)
ValueError: Unrecognized configuration <span class="hljs-keyword">class</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;transformers.models.gpt2.configuration_gpt2.GPT2Config&#x27;</span>&gt; <span class="hljs-keyword">for</span> this kind of AutoModel: AutoModelForQuestionAnswering.
Model <span class="hljs-built_in">type</span> should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...`,wrap:!1}}),me=new Et({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md"}}),{c(){i=o("meta"),$=a(),g=o("p"),b=a(),p(T.$$.fragment),de=a(),v=o("p"),v.textContent=rt,he=a(),p(k.$$.fragment),ge=a(),C=o("ol"),C.innerHTML=it,ye=a(),p(U.$$.fragment),$e=a(),M=o("ol"),M.innerHTML=pt,be=a(),J=o("p"),J.innerHTML=mt,Me=a(),p(_.$$.fragment),we=a(),x=o("p"),x.textContent=ut,je=a(),p(I.$$.fragment),Te=a(),V=o("p"),V.innerHTML=ct,ve=a(),p(Q.$$.fragment),ke=a(),A=o("p"),A.textContent=ft,Ce=a(),p(G.$$.fragment),Ue=a(),W=o("p"),W.textContent=dt,Je=a(),B=o("ul"),B.innerHTML=ht,_e=a(),p(w.$$.fragment),xe=a(),p(Z.$$.fragment),Ie=a(),H=o("p"),H.innerHTML=gt,Ve=a(),p(E.$$.fragment),Qe=a(),R=o("p"),R.textContent=yt,Ae=a(),p(N.$$.fragment),Ge=a(),p(F.$$.fragment),We=a(),L=o("p"),L.textContent=$t,Be=a(),p(P.$$.fragment),Ze=a(),z=o("p"),z.textContent=bt,He=a(),p(X.$$.fragment),Ee=a(),S=o("p"),S.textContent=Mt,Re=a(),p(Y.$$.fragment),Ne=a(),p(D.$$.fragment),Fe=a(),q=o("p"),q.innerHTML=wt,Le=a(),p(O.$$.fragment),Pe=a(),K=o("p"),K.textContent=jt,ze=a(),p(ee.$$.fragment),Xe=a(),te=o("p"),te.textContent=Tt,Se=a(),p(se.$$.fragment),Ye=a(),ne=o("p"),ne.innerHTML=vt,De=a(),p(j.$$.fragment),qe=a(),p(ae.$$.fragment),Oe=a(),le=o("p"),le.innerHTML=kt,Ke=a(),oe=o("ul"),oe.innerHTML=Ct,et=a(),p(re.$$.fragment),tt=a(),ie=o("p"),ie.innerHTML=Ut,st=a(),p(pe.$$.fragment),nt=a(),p(me.$$.fragment),at=a(),fe=o("p"),this.h()},l(e){const t=Zt("svelte-u9bgzb",document.head);i=r(t,"META",{name:!0,content:!0}),t.forEach(s),$=l(e),g=r(e,"P",{}),xt(g).forEach(s),b=l(e),m(T.$$.fragment,e),de=l(e),v=r(e,"P",{"data-svelte-h":!0}),h(v)!=="svelte-1cx76wd"&&(v.textContent=rt),he=l(e),m(k.$$.fragment,e),ge=l(e),C=r(e,"OL",{"data-svelte-h":!0}),h(C)!=="svelte-wyi91t"&&(C.innerHTML=it),ye=l(e),m(U.$$.fragment,e),$e=l(e),M=r(e,"OL",{start:!0,"data-svelte-h":!0}),h(M)!=="svelte-l1kxjh"&&(M.innerHTML=pt),be=l(e),J=r(e,"P",{"data-svelte-h":!0}),h(J)!=="svelte-c2f5jg"&&(J.innerHTML=mt),Me=l(e),m(_.$$.fragment,e),we=l(e),x=r(e,"P",{"data-svelte-h":!0}),h(x)!=="svelte-lpgdhl"&&(x.textContent=ut),je=l(e),m(I.$$.fragment,e),Te=l(e),V=r(e,"P",{"data-svelte-h":!0}),h(V)!=="svelte-170em04"&&(V.innerHTML=ct),ve=l(e),m(Q.$$.fragment,e),ke=l(e),A=r(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-28duqs"&&(A.textContent=ft),Ce=l(e),m(G.$$.fragment,e),Ue=l(e),W=r(e,"P",{"data-svelte-h":!0}),h(W)!=="svelte-1wd6425"&&(W.textContent=dt),Je=l(e),B=r(e,"UL",{"data-svelte-h":!0}),h(B)!=="svelte-1jcnr7c"&&(B.innerHTML=ht),_e=l(e),m(w.$$.fragment,e),xe=l(e),m(Z.$$.fragment,e),Ie=l(e),H=r(e,"P",{"data-svelte-h":!0}),h(H)!=="svelte-1yk3oka"&&(H.innerHTML=gt),Ve=l(e),m(E.$$.fragment,e),Qe=l(e),R=r(e,"P",{"data-svelte-h":!0}),h(R)!=="svelte-4f4dig"&&(R.textContent=yt),Ae=l(e),m(N.$$.fragment,e),Ge=l(e),m(F.$$.fragment,e),We=l(e),L=r(e,"P",{"data-svelte-h":!0}),h(L)!=="svelte-1u0kwcf"&&(L.textContent=$t),Be=l(e),m(P.$$.fragment,e),Ze=l(e),z=r(e,"P",{"data-svelte-h":!0}),h(z)!=="svelte-aeboz0"&&(z.textContent=bt),He=l(e),m(X.$$.fragment,e),Ee=l(e),S=r(e,"P",{"data-svelte-h":!0}),h(S)!=="svelte-ctfxbh"&&(S.textContent=Mt),Re=l(e),m(Y.$$.fragment,e),Ne=l(e),m(D.$$.fragment,e),Fe=l(e),q=r(e,"P",{"data-svelte-h":!0}),h(q)!=="svelte-1hu02cc"&&(q.innerHTML=wt),Le=l(e),m(O.$$.fragment,e),Pe=l(e),K=r(e,"P",{"data-svelte-h":!0}),h(K)!=="svelte-i1hsji"&&(K.textContent=jt),ze=l(e),m(ee.$$.fragment,e),Xe=l(e),te=r(e,"P",{"data-svelte-h":!0}),h(te)!=="svelte-101z1g5"&&(te.textContent=Tt),Se=l(e),m(se.$$.fragment,e),Ye=l(e),ne=r(e,"P",{"data-svelte-h":!0}),h(ne)!=="svelte-1p57h5c"&&(ne.innerHTML=vt),De=l(e),m(j.$$.fragment,e),qe=l(e),m(ae.$$.fragment,e),Oe=l(e),le=r(e,"P",{"data-svelte-h":!0}),h(le)!=="svelte-150uvc6"&&(le.innerHTML=kt),Ke=l(e),oe=r(e,"UL",{"data-svelte-h":!0}),h(oe)!=="svelte-qm9a5i"&&(oe.innerHTML=Ct),et=l(e),m(re.$$.fragment,e),tt=l(e),ie=r(e,"P",{"data-svelte-h":!0}),h(ie)!=="svelte-jk2zy2"&&(ie.innerHTML=Ut),st=l(e),m(pe.$$.fragment,e),nt=l(e),m(me.$$.fragment,e),at=l(e),fe=r(e,"P",{}),xt(fe).forEach(s),this.h()},h(){ot(i,"name","hf:doc:metadata"),ot(i,"content",Lt),ot(M,"start","2")},m(e,t){Ht(document.head,i),n(e,$,t),n(e,g,t),n(e,b,t),u(T,e,t),n(e,de,t),n(e,v,t),n(e,he,t),u(k,e,t),n(e,ge,t),n(e,C,t),n(e,ye,t),u(U,e,t),n(e,$e,t),n(e,M,t),n(e,be,t),n(e,J,t),n(e,Me,t),u(_,e,t),n(e,we,t),n(e,x,t),n(e,je,t),u(I,e,t),n(e,Te,t),n(e,V,t),n(e,ve,t),u(Q,e,t),n(e,ke,t),n(e,A,t),n(e,Ce,t),u(G,e,t),n(e,Ue,t),n(e,W,t),n(e,Je,t),n(e,B,t),n(e,_e,t),u(w,e,t),n(e,xe,t),u(Z,e,t),n(e,Ie,t),n(e,H,t),n(e,Ve,t),u(E,e,t),n(e,Qe,t),n(e,R,t),n(e,Ae,t),u(N,e,t),n(e,Ge,t),u(F,e,t),n(e,We,t),n(e,L,t),n(e,Be,t),u(P,e,t),n(e,Ze,t),n(e,z,t),n(e,He,t),u(X,e,t),n(e,Ee,t),n(e,S,t),n(e,Re,t),u(Y,e,t),n(e,Ne,t),u(D,e,t),n(e,Fe,t),n(e,q,t),n(e,Le,t),u(O,e,t),n(e,Pe,t),n(e,K,t),n(e,ze,t),u(ee,e,t),n(e,Xe,t),n(e,te,t),n(e,Se,t),u(se,e,t),n(e,Ye,t),n(e,ne,t),n(e,De,t),u(j,e,t),n(e,qe,t),u(ae,e,t),n(e,Oe,t),n(e,le,t),n(e,Ke,t),n(e,oe,t),n(e,et,t),u(re,e,t),n(e,tt,t),n(e,ie,t),n(e,st,t),u(pe,e,t),n(e,nt,t),u(me,e,t),n(e,at,t),n(e,fe,t),lt=!0},p(e,[t]){const Jt={};t&2&&(Jt.$$scope={dirty:t,ctx:e}),w.$set(Jt);const _t={};t&2&&(_t.$$scope={dirty:t,ctx:e}),j.$set(_t)},i(e){lt||(c(T.$$.fragment,e),c(k.$$.fragment,e),c(U.$$.fragment,e),c(_.$$.fragment,e),c(I.$$.fragment,e),c(Q.$$.fragment,e),c(G.$$.fragment,e),c(w.$$.fragment,e),c(Z.$$.fragment,e),c(E.$$.fragment,e),c(N.$$.fragment,e),c(F.$$.fragment,e),c(P.$$.fragment,e),c(X.$$.fragment,e),c(Y.$$.fragment,e),c(D.$$.fragment,e),c(O.$$.fragment,e),c(ee.$$.fragment,e),c(se.$$.fragment,e),c(j.$$.fragment,e),c(ae.$$.fragment,e),c(re.$$.fragment,e),c(pe.$$.fragment,e),c(me.$$.fragment,e),lt=!0)},o(e){f(T.$$.fragment,e),f(k.$$.fragment,e),f(U.$$.fragment,e),f(_.$$.fragment,e),f(I.$$.fragment,e),f(Q.$$.fragment,e),f(G.$$.fragment,e),f(w.$$.fragment,e),f(Z.$$.fragment,e),f(E.$$.fragment,e),f(N.$$.fragment,e),f(F.$$.fragment,e),f(P.$$.fragment,e),f(X.$$.fragment,e),f(Y.$$.fragment,e),f(D.$$.fragment,e),f(O.$$.fragment,e),f(ee.$$.fragment,e),f(se.$$.fragment,e),f(j.$$.fragment,e),f(ae.$$.fragment,e),f(re.$$.fragment,e),f(pe.$$.fragment,e),f(me.$$.fragment,e),lt=!1},d(e){e&&(s($),s(g),s(b),s(de),s(v),s(he),s(ge),s(C),s(ye),s($e),s(M),s(be),s(J),s(Me),s(we),s(x),s(je),s(Te),s(V),s(ve),s(ke),s(A),s(Ce),s(Ue),s(W),s(Je),s(B),s(_e),s(xe),s(Ie),s(H),s(Ve),s(Qe),s(R),s(Ae),s(Ge),s(We),s(L),s(Be),s(Ze),s(z),s(He),s(Ee),s(S),s(Re),s(Ne),s(Fe),s(q),s(Le),s(Pe),s(K),s(ze),s(Xe),s(te),s(Se),s(Ye),s(ne),s(De),s(qe),s(Oe),s(le),s(Ke),s(oe),s(et),s(tt),s(ie),s(st),s(nt),s(at),s(fe)),s(i),d(T,e),d(k,e),d(U,e),d(_,e),d(I,e),d(Q,e),d(G,e),d(w,e),d(Z,e),d(E,e),d(N,e),d(F,e),d(P,e),d(X,e),d(Y,e),d(D,e),d(O,e),d(ee,e),d(se,e),d(j,e),d(ae,e),d(re,e),d(pe,e),d(me,e)}}}const Lt='{"title":"Troubleshoot","local":"troubleshoot","sections":[{"title":"Firewalled environments","local":"firewalled-environments","sections":[],"depth":2},{"title":"CUDA out of memory","local":"cuda-out-of-memory","sections":[],"depth":2},{"title":"ImportError","local":"importerror","sections":[],"depth":2},{"title":"CUDA error: device-side assert triggered","local":"cuda-error-device-side-assert-triggered","sections":[],"depth":2},{"title":"Incorrect output when padding tokens arenâ€™t masked","local":"incorrect-output-when-padding-tokens-arent-masked","sections":[],"depth":2},{"title":"ValueError: Unrecognized configuration class XYZ for this kind of AutoModel","local":"valueerror-unrecognized-configuration-class-xyz-for-this-kind-of-automodel","sections":[],"depth":2}],"depth":1}';function Pt(ce){return Gt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ot extends Wt{constructor(i){super(),Bt(this,i,Pt,Ft,At,{})}}export{Ot as component};
