import{s as cs,o as ps,n as j}from"../chunks/scheduler.18a86fab.js";import{S as ms,i as fs,g as m,s as r,r as g,A as hs,h as f,f as l,c as i,j as v,x as T,u,k as $,l as gs,y as d,a as p,v as M,d as _,t as b,w as y}from"../chunks/index.98837b22.js";import{T as ft}from"../chunks/Tip.77304350.js";import{D as x}from"../chunks/Docstring.a1ef7999.js";import{C as H}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as re}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ie,E as us}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Ms,a as ds}from"../chunks/HfOption.6641485e.js";function _s(w){let t,h="Click on the MetaCLIP 2 models in the right sidebar for more examples of how to apply MetaCLIP 2 to different image and language tasks.";return{c(){t=m("p"),t.textContent=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-45ehwp"&&(t.textContent=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function bs(w){let t,h;return t=new H({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFjbGlwJTIwJTNEJTIwcGlwZWxpbmUoJTBBJTIwJTIwJTIwdGFzayUzRCUyMnplcm8tc2hvdC1pbWFnZS1jbGFzc2lmaWNhdGlvbiUyMiUyQyUwQSUyMCUyMCUyMG1vZGVsJTNEJTIyZmFjZWJvb2slMkZtZXRhY2xpcC0yLXdvcmxkd2lkZS1odWdlLXF1aWNrZ2VsdSUyMiUyQyUwQSUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEElMjAlMjAlMjBkZXZpY2UlM0QwJTBBKSUwQWxhYmVscyUyMCUzRCUyMCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhciUyMiU1RCUwQWNsaXAoJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUyQyUyMGNhbmRpZGF0ZV9sYWJlbHMlM0RsYWJlbHMp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

clip = pipeline(
   task=<span class="hljs-string">&quot;zero-shot-image-classification&quot;</span>,
   model=<span class="hljs-string">&quot;facebook/metaclip-2-worldwide-huge-quickgelu&quot;</span>,
   dtype=torch.bfloat16,
   device=<span class="hljs-number">0</span>
)
labels = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>, <span class="hljs-string">&quot;a photo of a car&quot;</span>]
clip(<span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>, candidate_labels=labels)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,s){M(t,n,s),h=!0},p:j,i(n){h||(_(t.$$.fragment,n),h=!0)},o(n){b(t.$$.fragment,n),h=!1},d(n){y(t,n)}}}function ys(w){let t,h;return t=new H({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwQXV0b01vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGbWV0YWNsaXAtMi13b3JsZHdpZGUtaHVnZS1xdWlja2dlbHUlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMnNkcGElMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZtZXRhY2xpcC0yLXdvcmxkd2lkZS1odWdlLXF1aWNrZ2VsdSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBbGFiZWxzJTIwJTNEJTIwJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2FyJTIyJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0RsYWJlbHMlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzX3Blcl9pbWFnZSUyMCUzRCUyMG91dHB1dHMubG9naXRzX3Blcl9pbWFnZSUwQXByb2JzJTIwJTNEJTIwbG9naXRzX3Blcl9pbWFnZS5zb2Z0bWF4KGRpbSUzRDEpJTBBbW9zdF9saWtlbHlfaWR4JTIwJTNEJTIwcHJvYnMuYXJnbWF4KGRpbSUzRDEpLml0ZW0oKSUwQW1vc3RfbGlrZWx5X2xhYmVsJTIwJTNEJTIwbGFiZWxzJTVCbW9zdF9saWtlbHlfaWR4JTVEJTBBcHJpbnQoZiUyMk1vc3QlMjBsaWtlbHklMjBsYWJlbCUzQSUyMCU3Qm1vc3RfbGlrZWx5X2xhYmVsJTdEJTIwd2l0aCUyMHByb2JhYmlsaXR5JTNBJTIwJTdCcHJvYnMlNUIwJTVEJTVCbW9zdF9saWtlbHlfaWR4JTVELml0ZW0oKSUzQS4zZiU3RCUyMik=",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">&quot;facebook/metaclip-2-worldwide-huge-quickgelu&quot;</span>, dtype=torch.bfloat16, attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>)
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/metaclip-2-worldwide-huge-quickgelu&quot;</span>)

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
labels = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>, <span class="hljs-string">&quot;a photo of a car&quot;</span>]

inputs = processor(text=labels, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)
most_likely_idx = probs.argmax(dim=<span class="hljs-number">1</span>).item()
most_likely_label = labels[most_likely_idx]
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Most likely label: <span class="hljs-subst">{most_likely_label}</span> with probability: <span class="hljs-subst">{probs[<span class="hljs-number">0</span>][most_likely_idx].item():<span class="hljs-number">.3</span>f}</span>&quot;</span>)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){u(t.$$.fragment,n)},m(n,s){M(t,n,s),h=!0},p:j,i(n){h||(_(t.$$.fragment,n),h=!0)},o(n){b(t.$$.fragment,n),h=!1},d(n){y(t,n)}}}function Ts(w){let t,h,n,s;return t=new ds({props:{id:"usage",option:"Pipeline",$$slots:{default:[bs]},$$scope:{ctx:w}}}),n=new ds({props:{id:"usage",option:"AutoModel",$$slots:{default:[ys]},$$scope:{ctx:w}}}),{c(){g(t.$$.fragment),h=r(),g(n.$$.fragment)},l(c){u(t.$$.fragment,c),h=i(c),u(n.$$.fragment,c)},m(c,o){M(t,c,o),p(c,h,o),M(n,c,o),s=!0},p(c,o){const C={};o&2&&(C.$$scope={dirty:o,ctx:c}),t.$set(C);const A={};o&2&&(A.$$scope={dirty:o,ctx:c}),n.$set(A)},i(c){s||(_(t.$$.fragment,c),_(n.$$.fragment,c),s=!0)},o(c){b(t.$$.fragment,c),b(n.$$.fragment,c),s=!1},d(c){c&&l(h),y(t,c),y(n,c)}}}function Cs(w){let t,h="Example:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1ldGFDbGlwMkNvbmZpZyUyQyUyME1ldGFDbGlwMk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME1ldGFDbGlwMkNvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTWV0YUNsaXAyQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWV0YUNsaXAyTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMG9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwTWV0YUNsaXAyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBNZXRhQ2xpcDJDb25maWclMjBmcm9tJTIwYSUyME1ldGFDbGlwMlRleHRDb25maWclMjBhbmQlMjBhJTIwTWV0YUNsaXAyVmlzaW9uQ29uZmlnJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1ldGFDbGlwMlRleHRDb25maWclMkMlMjBNZXRhQ2xpcDJWaXNpb25Db25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWV0YUNsaXAyVGV4dCUyMGFuZCUyME1ldGFDbGlwMlZpc2lvbiUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWdfdGV4dCUyMCUzRCUyME1ldGFDbGlwMlRleHRDb25maWcoKSUwQWNvbmZpZ192aXNpb24lMjAlM0QlMjBNZXRhQ2xpcDJWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyME1ldGFDbGlwMkNvbmZpZy5mcm9tX3RleHRfdmlzaW9uX2NvbmZpZ3MoY29uZmlnX3RleHQlMkMlMjBjb25maWdfdmlzaW9uKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MetaClip2Config, MetaClip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2Config with openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MetaClip2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2Model (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a MetaClip2Config from a MetaClip2TextConfig and a MetaClip2VisionConfig</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MetaClip2TextConfig, MetaClip2VisionConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2Text and MetaClip2Vision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = MetaClip2TextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = MetaClip2VisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = MetaClip2Config.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function ws(w){let t,h="Example:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1ldGFDbGlwMlRleHRDb25maWclMkMlMjBNZXRhQ2xpcDJUZXh0TW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWV0YUNsaXAyVGV4dENvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTWV0YUNsaXAyVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME1ldGFDbGlwMlRleHRNb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwb3BlbmFpJTJGbWV0YWNsaXBfMi12aXQtYmFzZS1wYXRjaDMyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBNZXRhQ2xpcDJUZXh0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MetaClip2TextConfig, MetaClip2TextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2TextConfig with openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MetaClip2TextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2TextModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2TextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function vs(w){let t,h="Example:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1ldGFDbGlwMlZpc2lvbkNvbmZpZyUyQyUyME1ldGFDbGlwMlZpc2lvbk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME1ldGFDbGlwMlZpc2lvbkNvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTWV0YUNsaXAyVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWV0YUNsaXAyVmlzaW9uTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMG9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwTWV0YUNsaXAyVmlzaW9uTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MetaClip2VisionConfig, MetaClip2VisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2VisionConfig with openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MetaClip2VisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MetaClip2VisionModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2VisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function $s(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function js(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1ldGFDbGlwMk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBNZXRhQ2xpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGbWV0YWNsaXBfMi12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUwQSUyMCUyMCUyMCUyMHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUwQSklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzX3Blcl9pbWFnZSUyMCUzRCUyMG91dHB1dHMubG9naXRzX3Blcl9pbWFnZSUyMCUyMCUyMyUyMHRoaXMlMjBpcyUyMHRoZSUyMGltYWdlLXRleHQlMjBzaW1pbGFyaXR5JTIwc2NvcmUlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUyMCUyMCUyMyUyMHdlJTIwY2FuJTIwdGFrZSUyMHRoZSUyMHNvZnRtYXglMjB0byUyMGdldCUyMHRoZSUyMGxhYmVsJTIwcHJvYmFiaWxpdGllcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MetaClip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2Model.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Js(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNZXRhQ2xpcDJNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwTWV0YUNsaXAyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF0ZXh0X2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X3RleHRfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MetaClip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2Model.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function xs(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1ldGFDbGlwMk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBNZXRhQ2xpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGbWV0YWNsaXBfMi12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFpbWFnZV9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF9pbWFnZV9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MetaClip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2Model.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function ks(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function Is(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNZXRhQ2xpcDJUZXh0TW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyME1ldGFDbGlwMlRleHRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGbWV0YWNsaXBfMi12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwKEVPUyUyMHRva2VuKSUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MetaClip2TextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2TextModel.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Us(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function Ws(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNZXRhQ2xpcDJUZXh0TW9kZWxXaXRoUHJvamVjdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwTWV0YUNsaXAyVGV4dE1vZGVsV2l0aFByb2plY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBdGV4dF9lbWJlZHMlMjAlM0QlMjBvdXRwdXRzLnRleHRfZW1iZWRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MetaClip2TextModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2TextModelWithProjection.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeds = outputs.text_embeds`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Zs(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function zs(w){let t,h="Examples:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1ldGFDbGlwMlZpc2lvbk1vZGVsV2l0aFByb2plY3Rpb24lMEElMEFtb2RlbCUyMCUzRCUyME1ldGFDbGlwMlZpc2lvbk1vZGVsV2l0aFByb2plY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZtZXRhY2xpcF8yLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWltYWdlX2VtYmVkcyUyMCUzRCUyMG91dHB1dHMuaW1hZ2VfZW1iZWRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MetaClip2VisionModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2VisionModelWithProjection.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_embeds = outputs.image_embeds`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Bs(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function Vs(w){let t,h="Example:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyME1ldGFDbGlwMlZpc2lvbk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBNZXRhQ2xpcDJWaXNpb25Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGbWV0YWNsaXBfMi12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMENMUyUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, MetaClip2VisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2VisionModel.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Gs(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=f(n,"P",{"data-svelte-h":!0}),T(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,s){p(n,t,s)},p:j,d(n){n&&l(t)}}}function Fs(w){let t,h="Example:",n,s,c;return s=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME1ldGFDbGlwMkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEFtb2RlbCUyMCUzRCUyME1ldGFDbGlwMkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRm1ldGFjbGlwXzItdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MetaClip2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MetaClip2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;openai/metaclip_2-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
...`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=r(),g(s.$$.fragment)},l(o){t=f(o,"P",{"data-svelte-h":!0}),T(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),u(s.$$.fragment,o)},m(o,C){p(o,t,C),p(o,n,C),M(s,o,C),c=!0},p:j,i(o){c||(_(s.$$.fragment,o),c=!0)},o(o){b(s.$$.fragment,o),c=!1},d(o){o&&(l(t),l(n)),y(s,o)}}}function Ps(w){let t,h,n,s,c,o="<em>This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-20.</em>",C,A,jn='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Yt,Ie,qt,Ue,St,We,Jn='MetaCLIP 2 is a replication of the original CLIP model trained on 300+ languages. It achieves state-of-the-art (SOTA) results on multilingual benchmarks (e.g., XM3600, CVQA, Babel‑ImageNet), surpassing previous SOTA such as <a href="siglip">mSigLIP</a> and <a href="siglip2">SigLIP‑2</a>. The authors show that English and non-English worlds can mutually benefit and elevate each other.',Qt,Ze,xn=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/facebookresearch/MetaCLIP" rel="nofollow">here</a>.`,Dt,ze,kn='You can find all the MetaCLIP 2 checkpoints under the <a href="https://huggingface.co/facebook?search_models=metaclip-2" rel="nofollow">Meta</a> organization.',At,le,Ot,Be,In='The example below demonstrates how to calculate similarity scores between multiple text descriptions and an image with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class. Usage of the MetaCLIP 2 models is identical to the CLIP models, you just need the <code>MetaClip2Model</code> class instead of <code>CLIPModel</code>.',Kt,de,eo,Ve,to,I,Ge,Co,ht,Un=`<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a>. It is used to instantiate
a METACLIP_2 model according to the specified arguments, defining the text model and vision model configs. Instantiating
a configuration with the defaults will yield a similar configuration to that of the METACLIP_2
<a href="https://huggingface.co/openai/metaclip_2-vit-base-patch32" rel="nofollow">openai/metaclip_2-vit-base-patch32</a> architecture.`,wo,gt,Wn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,vo,ce,$o,pe,Fe,jo,ut,Zn=`Instantiate a model config (or a derived class) from text model configuration and vision model
configuration.`,oo,Pe,no,V,Re,Jo,Mt,zn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModel">MetaClip2TextModel</a>. It is used to instantiate a METACLIP_2
text encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the text encoder of the METACLIP_2
<a href="https://huggingface.co/openai/metaclip_2-vit-base-patch32" rel="nofollow">openai/metaclip_2-vit-base-patch32</a> architecture.`,xo,_t,Bn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ko,me,so,Xe,ao,G,Ee,Io,bt,Vn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModel">MetaClip2VisionModel</a>. It is used to instantiate a
METACLIP_2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the vision encoder of the METACLIP_2
<a href="https://huggingface.co/openai/metaclip_2-vit-base-patch32" rel="nofollow">openai/metaclip_2-vit-base-patch32</a> architecture.`,Uo,yt,Gn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Wo,fe,ro,Ne,io,J,He,Zo,Tt,Fn="The bare Metaclip 2 Model outputting raw hidden-states without any specific head on top.",zo,Ct,Pn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Bo,wt,Rn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Vo,L,Le,Go,vt,Xn='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a> forward method, overrides the <code>__call__</code> special method.',Fo,he,Po,ge,Ro,ue,Ye,Xo,Me,Eo,_e,qe,No,be,lo,Se,co,U,Qe,Ho,$t,En="The text model from METACLIP_2 without any head or projection on top.",Lo,jt,Nn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Yo,Jt,Hn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qo,Y,De,So,xt,Ln='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModel">MetaClip2TextModel</a> forward method, overrides the <code>__call__</code> special method.',Qo,ye,Do,Te,po,Ae,mo,W,Oe,Ao,kt,Yn="The Metaclip 2 Model with a projection layer on top (a linear layer on top of the pooled output).",Oo,It,qn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ko,Ut,Sn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,en,q,Ke,tn,Wt,Qn='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModelWithProjection">MetaClip2TextModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',on,Ce,nn,we,fo,et,ho,Z,tt,sn,Zt,Dn="The Metaclip 2 Model with a projection layer on top (a linear layer on top of the pooled output).",an,zt,An=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,rn,Bt,On=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ln,S,ot,dn,Vt,Kn='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModelWithProjection">MetaClip2VisionModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',cn,ve,pn,$e,go,nt,uo,z,st,mn,Gt,es="The vision model from METACLIP_2 without any head or projection on top.",fn,Ft,ts=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,hn,Pt,os=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,gn,Q,at,un,Rt,ns='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModel">MetaClip2VisionModel</a> forward method, overrides the <code>__call__</code> special method.',Mn,je,_n,Je,Mo,rt,_o,B,it,bn,Xt,ss=`METACLIP_2 vision encoder with an image classification head on top (a linear layer on top of the pooled final hidden states of
the patch tokens) e.g. for ImageNet.`,yn,Et,as=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Tn,Nt,rs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Cn,D,lt,wn,Ht,is='The <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2ForImageClassification">MetaClip2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',vn,xe,$n,ke,bo,dt,yo,Lt,To;return Ie=new ie({props:{title:"MetaCLIP 2",local:"metaclip-2",headingTag:"h1"}}),Ue=new ie({props:{title:"Overview",local:"overview",headingTag:"h2"}}),le=new ft({props:{warning:!1,$$slots:{default:[_s]},$$scope:{ctx:w}}}),de=new Ms({props:{id:"usage",options:["Pipeline","AutoModel"],$$slots:{default:[Ts]},$$scope:{ctx:w}}}),Ve=new ie({props:{title:"MetaClip2Config",local:"transformers.MetaClip2Config",headingTag:"h2"}}),Ge=new x({props:{name:"class transformers.MetaClip2Config",anchor:"transformers.MetaClip2Config",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MetaClip2Config.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextConfig">MetaClip2TextConfig</a>.`,name:"text_config"},{anchor:"transformers.MetaClip2Config.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionConfig">MetaClip2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.MetaClip2Config.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.MetaClip2Config.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The initial value of the <em>logit_scale</em> parameter. Default is used as per the original METACLIP_2 implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.MetaClip2Config.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/configuration_metaclip_2.py#L208"}}),ce=new re({props:{anchor:"transformers.MetaClip2Config.example",$$slots:{default:[Cs]},$$scope:{ctx:w}}}),Fe=new x({props:{name:"from_text_vision_configs",anchor:"transformers.MetaClip2Config.from_text_vision_configs",parameters:[{name:"text_config",val:""},{name:"vision_config",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/configuration_utils.py#L1254",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>PreTrainedConfig</code></p>
`}}),Pe=new ie({props:{title:"MetaClip2TextConfig",local:"transformers.MetaClip2TextConfig",headingTag:"h2"}}),Re=new x({props:{name:"class transformers.MetaClip2TextConfig",anchor:"transformers.MetaClip2TextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MetaClip2TextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the METACLIP_2 text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Model">MetaClip2Model</a>.`,name:"vocab_size"},{anchor:"transformers.MetaClip2TextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.MetaClip2TextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.MetaClip2TextConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.MetaClip2TextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MetaClip2TextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MetaClip2TextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.MetaClip2TextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.MetaClip2TextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MetaClip2TextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.MetaClip2TextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MetaClip2TextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MetaClip2TextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.MetaClip2TextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.MetaClip2TextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/configuration_metaclip_2.py#L15"}}),me=new re({props:{anchor:"transformers.MetaClip2TextConfig.example",$$slots:{default:[ws]},$$scope:{ctx:w}}}),Xe=new ie({props:{title:"MetaClip2VisionConfig",local:"transformers.MetaClip2VisionConfig",headingTag:"h2"}}),Ee=new x({props:{name:"class transformers.MetaClip2VisionConfig",anchor:"transformers.MetaClip2VisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MetaClip2VisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.MetaClip2VisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.MetaClip2VisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.MetaClip2VisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MetaClip2VisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MetaClip2VisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.MetaClip2VisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.MetaClip2VisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.MetaClip2VisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.MetaClip2VisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MetaClip2VisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.MetaClip2VisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MetaClip2VisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/configuration_metaclip_2.py#L116"}}),fe=new re({props:{anchor:"transformers.MetaClip2VisionConfig.example",$$slots:{default:[vs]},$$scope:{ctx:w}}}),Ne=new ie({props:{title:"MetaClip2Model",local:"transformers.MetaClip2Model",headingTag:"h2"}}),He=new x({props:{name:"class transformers.MetaClip2Model",anchor:"transformers.MetaClip2Model",parameters:[{name:"config",val:": MetaClip2Config"}],parametersDescription:[{anchor:"transformers.MetaClip2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L742"}}),Le=new x({props:{name:"forward",anchor:"transformers.MetaClip2Model.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MetaClip2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MetaClip2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MetaClip2Model.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MetaClip2Model.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MetaClip2Model.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.MetaClip2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MetaClip2Model.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L874",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2Output</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModel"
>MetaClip2TextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModel"
>MetaClip2VisionModel</a>.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModel"
>MetaClip2TextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>) — The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModel"
>MetaClip2VisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2Output</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new ft({props:{$$slots:{default:[$s]},$$scope:{ctx:w}}}),ge=new re({props:{anchor:"transformers.MetaClip2Model.forward.example",$$slots:{default:[js]},$$scope:{ctx:w}}}),Ye=new x({props:{name:"get_text_features",anchor:"transformers.MetaClip2Model.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MetaClip2Model.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MetaClip2Model.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MetaClip2Model.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MetaClip2Model.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2Model.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L782",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextModel"
>MetaClip2TextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Me=new re({props:{anchor:"transformers.MetaClip2Model.get_text_features.example",$$slots:{default:[Js]},$$scope:{ctx:w}}}),qe=new x({props:{name:"get_image_features",anchor:"transformers.MetaClip2Model.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MetaClip2Model.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MetaClip2Model.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2Model.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MetaClip2Model.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L826",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionModel"
>MetaClip2VisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),be=new re({props:{anchor:"transformers.MetaClip2Model.get_image_features.example",$$slots:{default:[xs]},$$scope:{ctx:w}}}),Se=new ie({props:{title:"MetaClip2TextModel",local:"transformers.MetaClip2TextModel",headingTag:"h2"}}),Qe=new x({props:{name:"class transformers.MetaClip2TextModel",anchor:"transformers.MetaClip2TextModel",parameters:[{name:"config",val:": MetaClip2TextConfig"}],parametersDescription:[{anchor:"transformers.MetaClip2TextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextConfig">MetaClip2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L544"}}),De=new x({props:{name:"forward",anchor:"transformers.MetaClip2TextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MetaClip2TextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MetaClip2TextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MetaClip2TextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MetaClip2TextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2TextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L562",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ye=new ft({props:{$$slots:{default:[ks]},$$scope:{ctx:w}}}),Te=new re({props:{anchor:"transformers.MetaClip2TextModel.forward.example",$$slots:{default:[Is]},$$scope:{ctx:w}}}),Ae=new ie({props:{title:"MetaClip2TextModelWithProjection",local:"transformers.MetaClip2TextModelWithProjection",headingTag:"h2"}}),Oe=new x({props:{name:"class transformers.MetaClip2TextModelWithProjection",anchor:"transformers.MetaClip2TextModelWithProjection",parameters:[{name:"config",val:": MetaClip2TextConfig"}],parametersDescription:[{anchor:"transformers.MetaClip2TextModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2TextConfig">MetaClip2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L616"}}),Ke=new x({props:{name:"forward",anchor:"transformers.MetaClip2TextModelWithProjection.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MetaClip2TextModelWithProjection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MetaClip2TextModelWithProjection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MetaClip2TextModelWithProjection.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MetaClip2TextModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2TextModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L638",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2TextModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) — The text embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2TextModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ce=new ft({props:{$$slots:{default:[Us]},$$scope:{ctx:w}}}),we=new re({props:{anchor:"transformers.MetaClip2TextModelWithProjection.forward.example",$$slots:{default:[Ws]},$$scope:{ctx:w}}}),et=new ie({props:{title:"MetaClip2VisionModelWithProjection",local:"transformers.MetaClip2VisionModelWithProjection",headingTag:"h2"}}),tt=new x({props:{name:"class transformers.MetaClip2VisionModelWithProjection",anchor:"transformers.MetaClip2VisionModelWithProjection",parameters:[{name:"config",val:": MetaClip2VisionConfig"}],parametersDescription:[{anchor:"transformers.MetaClip2VisionModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionConfig">MetaClip2VisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1088"}}),ot=new x({props:{name:"forward",anchor:"transformers.MetaClip2VisionModelWithProjection.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MetaClip2VisionModelWithProjection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MetaClip2VisionModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2VisionModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MetaClip2VisionModelWithProjection.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1106",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2VisionModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) — The image embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.metaclip_2.modeling_metaclip_2.MetaClip2VisionModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new ft({props:{$$slots:{default:[Zs]},$$scope:{ctx:w}}}),$e=new re({props:{anchor:"transformers.MetaClip2VisionModelWithProjection.forward.example",$$slots:{default:[zs]},$$scope:{ctx:w}}}),nt=new ie({props:{title:"MetaClip2VisionModel",local:"transformers.MetaClip2VisionModel",headingTag:"h2"}}),st=new x({props:{name:"class transformers.MetaClip2VisionModel",anchor:"transformers.MetaClip2VisionModel",parameters:[{name:"config",val:": MetaClip2VisionConfig"}],parametersDescription:[{anchor:"transformers.MetaClip2VisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2VisionConfig">MetaClip2VisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1017"}}),at=new x({props:{name:"forward",anchor:"transformers.MetaClip2VisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.MetaClip2VisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MetaClip2VisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2VisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MetaClip2VisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1031",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),je=new ft({props:{$$slots:{default:[Bs]},$$scope:{ctx:w}}}),Je=new re({props:{anchor:"transformers.MetaClip2VisionModel.forward.example",$$slots:{default:[Vs]},$$scope:{ctx:w}}}),rt=new ie({props:{title:"MetaClip2ForImageClassification",local:"transformers.MetaClip2ForImageClassification",headingTag:"h2"}}),it=new x({props:{name:"class transformers.MetaClip2ForImageClassification",anchor:"transformers.MetaClip2ForImageClassification",parameters:[{name:"config",val:": MetaClip2Config"}],parametersDescription:[{anchor:"transformers.MetaClip2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config">MetaClip2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1158"}}),lt=new x({props:{name:"forward",anchor:"transformers.MetaClip2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.MetaClip2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MetaClip2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.MetaClip2ForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MetaClip2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/metaclip_2/modeling_metaclip_2.py#L1176",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/metaclip_2#transformers.MetaClip2Config"
>MetaClip2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),xe=new ft({props:{$$slots:{default:[Gs]},$$scope:{ctx:w}}}),ke=new re({props:{anchor:"transformers.MetaClip2ForImageClassification.forward.example",$$slots:{default:[Fs]},$$scope:{ctx:w}}}),dt=new us({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/metaclip_2.md"}}),{c(){t=m("meta"),h=r(),n=m("p"),s=r(),c=m("p"),c.innerHTML=o,C=r(),A=m("div"),A.innerHTML=jn,Yt=r(),g(Ie.$$.fragment),qt=r(),g(Ue.$$.fragment),St=r(),We=m("p"),We.innerHTML=Jn,Qt=r(),Ze=m("p"),Ze.innerHTML=xn,Dt=r(),ze=m("p"),ze.innerHTML=kn,At=r(),g(le.$$.fragment),Ot=r(),Be=m("p"),Be.innerHTML=In,Kt=r(),g(de.$$.fragment),eo=r(),g(Ve.$$.fragment),to=r(),I=m("div"),g(Ge.$$.fragment),Co=r(),ht=m("p"),ht.innerHTML=Un,wo=r(),gt=m("p"),gt.innerHTML=Wn,vo=r(),g(ce.$$.fragment),$o=r(),pe=m("div"),g(Fe.$$.fragment),jo=r(),ut=m("p"),ut.textContent=Zn,oo=r(),g(Pe.$$.fragment),no=r(),V=m("div"),g(Re.$$.fragment),Jo=r(),Mt=m("p"),Mt.innerHTML=zn,xo=r(),_t=m("p"),_t.innerHTML=Bn,ko=r(),g(me.$$.fragment),so=r(),g(Xe.$$.fragment),ao=r(),G=m("div"),g(Ee.$$.fragment),Io=r(),bt=m("p"),bt.innerHTML=Vn,Uo=r(),yt=m("p"),yt.innerHTML=Gn,Wo=r(),g(fe.$$.fragment),ro=r(),g(Ne.$$.fragment),io=r(),J=m("div"),g(He.$$.fragment),Zo=r(),Tt=m("p"),Tt.textContent=Fn,zo=r(),Ct=m("p"),Ct.innerHTML=Pn,Bo=r(),wt=m("p"),wt.innerHTML=Rn,Vo=r(),L=m("div"),g(Le.$$.fragment),Go=r(),vt=m("p"),vt.innerHTML=Xn,Fo=r(),g(he.$$.fragment),Po=r(),g(ge.$$.fragment),Ro=r(),ue=m("div"),g(Ye.$$.fragment),Xo=r(),g(Me.$$.fragment),Eo=r(),_e=m("div"),g(qe.$$.fragment),No=r(),g(be.$$.fragment),lo=r(),g(Se.$$.fragment),co=r(),U=m("div"),g(Qe.$$.fragment),Ho=r(),$t=m("p"),$t.textContent=En,Lo=r(),jt=m("p"),jt.innerHTML=Nn,Yo=r(),Jt=m("p"),Jt.innerHTML=Hn,qo=r(),Y=m("div"),g(De.$$.fragment),So=r(),xt=m("p"),xt.innerHTML=Ln,Qo=r(),g(ye.$$.fragment),Do=r(),g(Te.$$.fragment),po=r(),g(Ae.$$.fragment),mo=r(),W=m("div"),g(Oe.$$.fragment),Ao=r(),kt=m("p"),kt.textContent=Yn,Oo=r(),It=m("p"),It.innerHTML=qn,Ko=r(),Ut=m("p"),Ut.innerHTML=Sn,en=r(),q=m("div"),g(Ke.$$.fragment),tn=r(),Wt=m("p"),Wt.innerHTML=Qn,on=r(),g(Ce.$$.fragment),nn=r(),g(we.$$.fragment),fo=r(),g(et.$$.fragment),ho=r(),Z=m("div"),g(tt.$$.fragment),sn=r(),Zt=m("p"),Zt.textContent=Dn,an=r(),zt=m("p"),zt.innerHTML=An,rn=r(),Bt=m("p"),Bt.innerHTML=On,ln=r(),S=m("div"),g(ot.$$.fragment),dn=r(),Vt=m("p"),Vt.innerHTML=Kn,cn=r(),g(ve.$$.fragment),pn=r(),g($e.$$.fragment),go=r(),g(nt.$$.fragment),uo=r(),z=m("div"),g(st.$$.fragment),mn=r(),Gt=m("p"),Gt.textContent=es,fn=r(),Ft=m("p"),Ft.innerHTML=ts,hn=r(),Pt=m("p"),Pt.innerHTML=os,gn=r(),Q=m("div"),g(at.$$.fragment),un=r(),Rt=m("p"),Rt.innerHTML=ns,Mn=r(),g(je.$$.fragment),_n=r(),g(Je.$$.fragment),Mo=r(),g(rt.$$.fragment),_o=r(),B=m("div"),g(it.$$.fragment),bn=r(),Xt=m("p"),Xt.textContent=ss,yn=r(),Et=m("p"),Et.innerHTML=as,Tn=r(),Nt=m("p"),Nt.innerHTML=rs,Cn=r(),D=m("div"),g(lt.$$.fragment),wn=r(),Ht=m("p"),Ht.innerHTML=is,vn=r(),g(xe.$$.fragment),$n=r(),g(ke.$$.fragment),bo=r(),g(dt.$$.fragment),yo=r(),Lt=m("p"),this.h()},l(e){const a=hs("svelte-u9bgzb",document.head);t=f(a,"META",{name:!0,content:!0}),a.forEach(l),h=i(e),n=f(e,"P",{}),v(n).forEach(l),s=i(e),c=f(e,"P",{"data-svelte-h":!0}),T(c)!=="svelte-1gh4xkk"&&(c.innerHTML=o),C=i(e),A=f(e,"DIV",{style:!0,"data-svelte-h":!0}),T(A)!=="svelte-2m0t7r"&&(A.innerHTML=jn),Yt=i(e),u(Ie.$$.fragment,e),qt=i(e),u(Ue.$$.fragment,e),St=i(e),We=f(e,"P",{"data-svelte-h":!0}),T(We)!=="svelte-1qff1uc"&&(We.innerHTML=Jn),Qt=i(e),Ze=f(e,"P",{"data-svelte-h":!0}),T(Ze)!=="svelte-16zb2ff"&&(Ze.innerHTML=xn),Dt=i(e),ze=f(e,"P",{"data-svelte-h":!0}),T(ze)!=="svelte-1nw8qu3"&&(ze.innerHTML=kn),At=i(e),u(le.$$.fragment,e),Ot=i(e),Be=f(e,"P",{"data-svelte-h":!0}),T(Be)!=="svelte-lcboc4"&&(Be.innerHTML=In),Kt=i(e),u(de.$$.fragment,e),eo=i(e),u(Ve.$$.fragment,e),to=i(e),I=f(e,"DIV",{class:!0});var F=v(I);u(Ge.$$.fragment,F),Co=i(F),ht=f(F,"P",{"data-svelte-h":!0}),T(ht)!=="svelte-1vazocl"&&(ht.innerHTML=Un),wo=i(F),gt=f(F,"P",{"data-svelte-h":!0}),T(gt)!=="svelte-1ek1ss9"&&(gt.innerHTML=Wn),vo=i(F),u(ce.$$.fragment,F),$o=i(F),pe=f(F,"DIV",{class:!0});var ct=v(pe);u(Fe.$$.fragment,ct),jo=i(ct),ut=f(ct,"P",{"data-svelte-h":!0}),T(ut)!=="svelte-w5eluu"&&(ut.textContent=Zn),ct.forEach(l),F.forEach(l),oo=i(e),u(Pe.$$.fragment,e),no=i(e),V=f(e,"DIV",{class:!0});var O=v(V);u(Re.$$.fragment,O),Jo=i(O),Mt=f(O,"P",{"data-svelte-h":!0}),T(Mt)!=="svelte-m7hxk4"&&(Mt.innerHTML=zn),xo=i(O),_t=f(O,"P",{"data-svelte-h":!0}),T(_t)!=="svelte-1ek1ss9"&&(_t.innerHTML=Bn),ko=i(O),u(me.$$.fragment,O),O.forEach(l),so=i(e),u(Xe.$$.fragment,e),ao=i(e),G=f(e,"DIV",{class:!0});var K=v(G);u(Ee.$$.fragment,K),Io=i(K),bt=f(K,"P",{"data-svelte-h":!0}),T(bt)!=="svelte-1rab5ey"&&(bt.innerHTML=Vn),Uo=i(K),yt=f(K,"P",{"data-svelte-h":!0}),T(yt)!=="svelte-1ek1ss9"&&(yt.innerHTML=Gn),Wo=i(K),u(fe.$$.fragment,K),K.forEach(l),ro=i(e),u(Ne.$$.fragment,e),io=i(e),J=f(e,"DIV",{class:!0});var k=v(J);u(He.$$.fragment,k),Zo=i(k),Tt=f(k,"P",{"data-svelte-h":!0}),T(Tt)!=="svelte-1c5w01x"&&(Tt.textContent=Fn),zo=i(k),Ct=f(k,"P",{"data-svelte-h":!0}),T(Ct)!=="svelte-q52n56"&&(Ct.innerHTML=Pn),Bo=i(k),wt=f(k,"P",{"data-svelte-h":!0}),T(wt)!=="svelte-hswkmf"&&(wt.innerHTML=Rn),Vo=i(k),L=f(k,"DIV",{class:!0});var ee=v(L);u(Le.$$.fragment,ee),Go=i(ee),vt=f(ee,"P",{"data-svelte-h":!0}),T(vt)!=="svelte-sjnd3p"&&(vt.innerHTML=Xn),Fo=i(ee),u(he.$$.fragment,ee),Po=i(ee),u(ge.$$.fragment,ee),ee.forEach(l),Ro=i(k),ue=f(k,"DIV",{class:!0});var pt=v(ue);u(Ye.$$.fragment,pt),Xo=i(pt),u(Me.$$.fragment,pt),pt.forEach(l),Eo=i(k),_e=f(k,"DIV",{class:!0});var mt=v(_e);u(qe.$$.fragment,mt),No=i(mt),u(be.$$.fragment,mt),mt.forEach(l),k.forEach(l),lo=i(e),u(Se.$$.fragment,e),co=i(e),U=f(e,"DIV",{class:!0});var P=v(U);u(Qe.$$.fragment,P),Ho=i(P),$t=f(P,"P",{"data-svelte-h":!0}),T($t)!=="svelte-5y07d1"&&($t.textContent=En),Lo=i(P),jt=f(P,"P",{"data-svelte-h":!0}),T(jt)!=="svelte-q52n56"&&(jt.innerHTML=Nn),Yo=i(P),Jt=f(P,"P",{"data-svelte-h":!0}),T(Jt)!=="svelte-hswkmf"&&(Jt.innerHTML=Hn),qo=i(P),Y=f(P,"DIV",{class:!0});var te=v(Y);u(De.$$.fragment,te),So=i(te),xt=f(te,"P",{"data-svelte-h":!0}),T(xt)!=="svelte-yl6p17"&&(xt.innerHTML=Ln),Qo=i(te),u(ye.$$.fragment,te),Do=i(te),u(Te.$$.fragment,te),te.forEach(l),P.forEach(l),po=i(e),u(Ae.$$.fragment,e),mo=i(e),W=f(e,"DIV",{class:!0});var R=v(W);u(Oe.$$.fragment,R),Ao=i(R),kt=f(R,"P",{"data-svelte-h":!0}),T(kt)!=="svelte-1nuv8yk"&&(kt.textContent=Yn),Oo=i(R),It=f(R,"P",{"data-svelte-h":!0}),T(It)!=="svelte-q52n56"&&(It.innerHTML=qn),Ko=i(R),Ut=f(R,"P",{"data-svelte-h":!0}),T(Ut)!=="svelte-hswkmf"&&(Ut.innerHTML=Sn),en=i(R),q=f(R,"DIV",{class:!0});var oe=v(q);u(Ke.$$.fragment,oe),tn=i(oe),Wt=f(oe,"P",{"data-svelte-h":!0}),T(Wt)!=="svelte-1s6ixx1"&&(Wt.innerHTML=Qn),on=i(oe),u(Ce.$$.fragment,oe),nn=i(oe),u(we.$$.fragment,oe),oe.forEach(l),R.forEach(l),fo=i(e),u(et.$$.fragment,e),ho=i(e),Z=f(e,"DIV",{class:!0});var X=v(Z);u(tt.$$.fragment,X),sn=i(X),Zt=f(X,"P",{"data-svelte-h":!0}),T(Zt)!=="svelte-1nuv8yk"&&(Zt.textContent=Dn),an=i(X),zt=f(X,"P",{"data-svelte-h":!0}),T(zt)!=="svelte-q52n56"&&(zt.innerHTML=An),rn=i(X),Bt=f(X,"P",{"data-svelte-h":!0}),T(Bt)!=="svelte-hswkmf"&&(Bt.innerHTML=On),ln=i(X),S=f(X,"DIV",{class:!0});var ne=v(S);u(ot.$$.fragment,ne),dn=i(ne),Vt=f(ne,"P",{"data-svelte-h":!0}),T(Vt)!=="svelte-6s4zif"&&(Vt.innerHTML=Kn),cn=i(ne),u(ve.$$.fragment,ne),pn=i(ne),u($e.$$.fragment,ne),ne.forEach(l),X.forEach(l),go=i(e),u(nt.$$.fragment,e),uo=i(e),z=f(e,"DIV",{class:!0});var E=v(z);u(st.$$.fragment,E),mn=i(E),Gt=f(E,"P",{"data-svelte-h":!0}),T(Gt)!=="svelte-19wkk0o"&&(Gt.textContent=es),fn=i(E),Ft=f(E,"P",{"data-svelte-h":!0}),T(Ft)!=="svelte-q52n56"&&(Ft.innerHTML=ts),hn=i(E),Pt=f(E,"P",{"data-svelte-h":!0}),T(Pt)!=="svelte-hswkmf"&&(Pt.innerHTML=os),gn=i(E),Q=f(E,"DIV",{class:!0});var se=v(Q);u(at.$$.fragment,se),un=i(se),Rt=f(se,"P",{"data-svelte-h":!0}),T(Rt)!=="svelte-1v1bqi5"&&(Rt.innerHTML=ns),Mn=i(se),u(je.$$.fragment,se),_n=i(se),u(Je.$$.fragment,se),se.forEach(l),E.forEach(l),Mo=i(e),u(rt.$$.fragment,e),_o=i(e),B=f(e,"DIV",{class:!0});var N=v(B);u(it.$$.fragment,N),bn=i(N),Xt=f(N,"P",{"data-svelte-h":!0}),T(Xt)!=="svelte-pl55zw"&&(Xt.textContent=ss),yn=i(N),Et=f(N,"P",{"data-svelte-h":!0}),T(Et)!=="svelte-q52n56"&&(Et.innerHTML=as),Tn=i(N),Nt=f(N,"P",{"data-svelte-h":!0}),T(Nt)!=="svelte-hswkmf"&&(Nt.innerHTML=rs),Cn=i(N),D=f(N,"DIV",{class:!0});var ae=v(D);u(lt.$$.fragment,ae),wn=i(ae),Ht=f(ae,"P",{"data-svelte-h":!0}),T(Ht)!=="svelte-o7wfx"&&(Ht.innerHTML=is),vn=i(ae),u(xe.$$.fragment,ae),$n=i(ae),u(ke.$$.fragment,ae),ae.forEach(l),N.forEach(l),bo=i(e),u(dt.$$.fragment,e),yo=i(e),Lt=f(e,"P",{}),v(Lt).forEach(l),this.h()},h(){$(t,"name","hf:doc:metadata"),$(t,"content",Rs),gs(A,"float","right"),$(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){d(document.head,t),p(e,h,a),p(e,n,a),p(e,s,a),p(e,c,a),p(e,C,a),p(e,A,a),p(e,Yt,a),M(Ie,e,a),p(e,qt,a),M(Ue,e,a),p(e,St,a),p(e,We,a),p(e,Qt,a),p(e,Ze,a),p(e,Dt,a),p(e,ze,a),p(e,At,a),M(le,e,a),p(e,Ot,a),p(e,Be,a),p(e,Kt,a),M(de,e,a),p(e,eo,a),M(Ve,e,a),p(e,to,a),p(e,I,a),M(Ge,I,null),d(I,Co),d(I,ht),d(I,wo),d(I,gt),d(I,vo),M(ce,I,null),d(I,$o),d(I,pe),M(Fe,pe,null),d(pe,jo),d(pe,ut),p(e,oo,a),M(Pe,e,a),p(e,no,a),p(e,V,a),M(Re,V,null),d(V,Jo),d(V,Mt),d(V,xo),d(V,_t),d(V,ko),M(me,V,null),p(e,so,a),M(Xe,e,a),p(e,ao,a),p(e,G,a),M(Ee,G,null),d(G,Io),d(G,bt),d(G,Uo),d(G,yt),d(G,Wo),M(fe,G,null),p(e,ro,a),M(Ne,e,a),p(e,io,a),p(e,J,a),M(He,J,null),d(J,Zo),d(J,Tt),d(J,zo),d(J,Ct),d(J,Bo),d(J,wt),d(J,Vo),d(J,L),M(Le,L,null),d(L,Go),d(L,vt),d(L,Fo),M(he,L,null),d(L,Po),M(ge,L,null),d(J,Ro),d(J,ue),M(Ye,ue,null),d(ue,Xo),M(Me,ue,null),d(J,Eo),d(J,_e),M(qe,_e,null),d(_e,No),M(be,_e,null),p(e,lo,a),M(Se,e,a),p(e,co,a),p(e,U,a),M(Qe,U,null),d(U,Ho),d(U,$t),d(U,Lo),d(U,jt),d(U,Yo),d(U,Jt),d(U,qo),d(U,Y),M(De,Y,null),d(Y,So),d(Y,xt),d(Y,Qo),M(ye,Y,null),d(Y,Do),M(Te,Y,null),p(e,po,a),M(Ae,e,a),p(e,mo,a),p(e,W,a),M(Oe,W,null),d(W,Ao),d(W,kt),d(W,Oo),d(W,It),d(W,Ko),d(W,Ut),d(W,en),d(W,q),M(Ke,q,null),d(q,tn),d(q,Wt),d(q,on),M(Ce,q,null),d(q,nn),M(we,q,null),p(e,fo,a),M(et,e,a),p(e,ho,a),p(e,Z,a),M(tt,Z,null),d(Z,sn),d(Z,Zt),d(Z,an),d(Z,zt),d(Z,rn),d(Z,Bt),d(Z,ln),d(Z,S),M(ot,S,null),d(S,dn),d(S,Vt),d(S,cn),M(ve,S,null),d(S,pn),M($e,S,null),p(e,go,a),M(nt,e,a),p(e,uo,a),p(e,z,a),M(st,z,null),d(z,mn),d(z,Gt),d(z,fn),d(z,Ft),d(z,hn),d(z,Pt),d(z,gn),d(z,Q),M(at,Q,null),d(Q,un),d(Q,Rt),d(Q,Mn),M(je,Q,null),d(Q,_n),M(Je,Q,null),p(e,Mo,a),M(rt,e,a),p(e,_o,a),p(e,B,a),M(it,B,null),d(B,bn),d(B,Xt),d(B,yn),d(B,Et),d(B,Tn),d(B,Nt),d(B,Cn),d(B,D),M(lt,D,null),d(D,wn),d(D,Ht),d(D,vn),M(xe,D,null),d(D,$n),M(ke,D,null),p(e,bo,a),M(dt,e,a),p(e,yo,a),p(e,Lt,a),To=!0},p(e,[a]){const F={};a&2&&(F.$$scope={dirty:a,ctx:e}),le.$set(F);const ct={};a&2&&(ct.$$scope={dirty:a,ctx:e}),de.$set(ct);const O={};a&2&&(O.$$scope={dirty:a,ctx:e}),ce.$set(O);const K={};a&2&&(K.$$scope={dirty:a,ctx:e}),me.$set(K);const k={};a&2&&(k.$$scope={dirty:a,ctx:e}),fe.$set(k);const ee={};a&2&&(ee.$$scope={dirty:a,ctx:e}),he.$set(ee);const pt={};a&2&&(pt.$$scope={dirty:a,ctx:e}),ge.$set(pt);const mt={};a&2&&(mt.$$scope={dirty:a,ctx:e}),Me.$set(mt);const P={};a&2&&(P.$$scope={dirty:a,ctx:e}),be.$set(P);const te={};a&2&&(te.$$scope={dirty:a,ctx:e}),ye.$set(te);const R={};a&2&&(R.$$scope={dirty:a,ctx:e}),Te.$set(R);const oe={};a&2&&(oe.$$scope={dirty:a,ctx:e}),Ce.$set(oe);const X={};a&2&&(X.$$scope={dirty:a,ctx:e}),we.$set(X);const ne={};a&2&&(ne.$$scope={dirty:a,ctx:e}),ve.$set(ne);const E={};a&2&&(E.$$scope={dirty:a,ctx:e}),$e.$set(E);const se={};a&2&&(se.$$scope={dirty:a,ctx:e}),je.$set(se);const N={};a&2&&(N.$$scope={dirty:a,ctx:e}),Je.$set(N);const ae={};a&2&&(ae.$$scope={dirty:a,ctx:e}),xe.$set(ae);const ls={};a&2&&(ls.$$scope={dirty:a,ctx:e}),ke.$set(ls)},i(e){To||(_(Ie.$$.fragment,e),_(Ue.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(Ve.$$.fragment,e),_(Ge.$$.fragment,e),_(ce.$$.fragment,e),_(Fe.$$.fragment,e),_(Pe.$$.fragment,e),_(Re.$$.fragment,e),_(me.$$.fragment,e),_(Xe.$$.fragment,e),_(Ee.$$.fragment,e),_(fe.$$.fragment,e),_(Ne.$$.fragment,e),_(He.$$.fragment,e),_(Le.$$.fragment,e),_(he.$$.fragment,e),_(ge.$$.fragment,e),_(Ye.$$.fragment,e),_(Me.$$.fragment,e),_(qe.$$.fragment,e),_(be.$$.fragment,e),_(Se.$$.fragment,e),_(Qe.$$.fragment,e),_(De.$$.fragment,e),_(ye.$$.fragment,e),_(Te.$$.fragment,e),_(Ae.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(Ce.$$.fragment,e),_(we.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(ot.$$.fragment,e),_(ve.$$.fragment,e),_($e.$$.fragment,e),_(nt.$$.fragment,e),_(st.$$.fragment,e),_(at.$$.fragment,e),_(je.$$.fragment,e),_(Je.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(xe.$$.fragment,e),_(ke.$$.fragment,e),_(dt.$$.fragment,e),To=!0)},o(e){b(Ie.$$.fragment,e),b(Ue.$$.fragment,e),b(le.$$.fragment,e),b(de.$$.fragment,e),b(Ve.$$.fragment,e),b(Ge.$$.fragment,e),b(ce.$$.fragment,e),b(Fe.$$.fragment,e),b(Pe.$$.fragment,e),b(Re.$$.fragment,e),b(me.$$.fragment,e),b(Xe.$$.fragment,e),b(Ee.$$.fragment,e),b(fe.$$.fragment,e),b(Ne.$$.fragment,e),b(He.$$.fragment,e),b(Le.$$.fragment,e),b(he.$$.fragment,e),b(ge.$$.fragment,e),b(Ye.$$.fragment,e),b(Me.$$.fragment,e),b(qe.$$.fragment,e),b(be.$$.fragment,e),b(Se.$$.fragment,e),b(Qe.$$.fragment,e),b(De.$$.fragment,e),b(ye.$$.fragment,e),b(Te.$$.fragment,e),b(Ae.$$.fragment,e),b(Oe.$$.fragment,e),b(Ke.$$.fragment,e),b(Ce.$$.fragment,e),b(we.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(ot.$$.fragment,e),b(ve.$$.fragment,e),b($e.$$.fragment,e),b(nt.$$.fragment,e),b(st.$$.fragment,e),b(at.$$.fragment,e),b(je.$$.fragment,e),b(Je.$$.fragment,e),b(rt.$$.fragment,e),b(it.$$.fragment,e),b(lt.$$.fragment,e),b(xe.$$.fragment,e),b(ke.$$.fragment,e),b(dt.$$.fragment,e),To=!1},d(e){e&&(l(h),l(n),l(s),l(c),l(C),l(A),l(Yt),l(qt),l(St),l(We),l(Qt),l(Ze),l(Dt),l(ze),l(At),l(Ot),l(Be),l(Kt),l(eo),l(to),l(I),l(oo),l(no),l(V),l(so),l(ao),l(G),l(ro),l(io),l(J),l(lo),l(co),l(U),l(po),l(mo),l(W),l(fo),l(ho),l(Z),l(go),l(uo),l(z),l(Mo),l(_o),l(B),l(bo),l(yo),l(Lt)),l(t),y(Ie,e),y(Ue,e),y(le,e),y(de,e),y(Ve,e),y(Ge),y(ce),y(Fe),y(Pe,e),y(Re),y(me),y(Xe,e),y(Ee),y(fe),y(Ne,e),y(He),y(Le),y(he),y(ge),y(Ye),y(Me),y(qe),y(be),y(Se,e),y(Qe),y(De),y(ye),y(Te),y(Ae,e),y(Oe),y(Ke),y(Ce),y(we),y(et,e),y(tt),y(ot),y(ve),y($e),y(nt,e),y(st),y(at),y(je),y(Je),y(rt,e),y(it),y(lt),y(xe),y(ke),y(dt,e)}}}const Rs='{"title":"MetaCLIP 2","local":"metaclip-2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"MetaClip2Config","local":"transformers.MetaClip2Config","sections":[],"depth":2},{"title":"MetaClip2TextConfig","local":"transformers.MetaClip2TextConfig","sections":[],"depth":2},{"title":"MetaClip2VisionConfig","local":"transformers.MetaClip2VisionConfig","sections":[],"depth":2},{"title":"MetaClip2Model","local":"transformers.MetaClip2Model","sections":[],"depth":2},{"title":"MetaClip2TextModel","local":"transformers.MetaClip2TextModel","sections":[],"depth":2},{"title":"MetaClip2TextModelWithProjection","local":"transformers.MetaClip2TextModelWithProjection","sections":[],"depth":2},{"title":"MetaClip2VisionModelWithProjection","local":"transformers.MetaClip2VisionModelWithProjection","sections":[],"depth":2},{"title":"MetaClip2VisionModel","local":"transformers.MetaClip2VisionModel","sections":[],"depth":2},{"title":"MetaClip2ForImageClassification","local":"transformers.MetaClip2ForImageClassification","sections":[],"depth":2}],"depth":1}';function Xs(w){return ps(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ds extends ms{constructor(t){super(),fs(this,t,Xs,Ps,cs,{})}}export{Ds as component};
