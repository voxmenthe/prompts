import{s as ye,o as Je,n as Yl}from"../chunks/scheduler.18a86fab.js";import{S as ce,i as Ce,g as o,s as r,r as C,A as me,h as w,f as t,c as p,j as Ue,u as m,x as c,k as oe,y as je,a as s,v as j,d as I,t as h,w as u}from"../chunks/index.98837b22.js";import{T as Ll}from"../chunks/Tip.77304350.js";import{C as _}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as wl,E as Ie}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as he,a as we}from"../chunks/HfOption.6641485e.js";function ue(A){let n,y='Refer to the oneCCL <a href="https://github.com/intel/torch-ccl#installation" rel="nofollow">installation</a> for more details.';return{c(){n=o("p"),n.innerHTML=y},l(a){n=w(a,"P",{"data-svelte-h":!0}),c(n)!=="svelte-1iyq1qq"&&(n.innerHTML=y)},m(a,T){s(a,n,T)},p:Yl,d(a){a&&t(n)}}}function de(A){let n,y='Refer to the IPEX <a href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation" rel="nofollow">installation</a> for more details.';return{c(){n=o("p"),n.innerHTML=y},l(a){n=w(a,"P",{"data-svelte-h":!0}),c(n)!=="svelte-88tmbr"&&(n.innerHTML=y)},m(a,T){s(a,n,T)},p:Yl,d(a){a&&t(n)}}}function fe(A){let n,y="Tune the variable <code>OMP_NUM_THREADS/CCL_WORKER_COUNT</code> for optimal performance.";return{c(){n=o("p"),n.innerHTML=y},l(a){n=w(a,"P",{"data-svelte-h":!0}),c(n)!=="svelte-4dbzn8"&&(n.innerHTML=y)},m(a,T){s(a,n,T)},p:Yl,d(a){a&&t(n)}}}function be(A){let n,y='The example below demonstrates the <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering" rel="nofollow">run_qa.py</a> script. It enables training with two processes on one Xeon CPU, with one process running per socket.',a,T,i,J,d;return T=new Ll({props:{warning:!1,$$slots:{default:[fe]},$$scope:{ctx:A}}}),J=new _({props:{code:"ZXhwb3J0JTIwQ0NMX1dPUktFUl9DT1VOVCUzRDElMEFleHBvcnQlMjBNQVNURVJfQUREUiUzRDEyNy4wLjAuMSUwQW1waXJ1biUyMC1uJTIwMiUyMC1nZW52JTIwT01QX05VTV9USFJFQURTJTNEMjMlMjAlNUMlMEFweXRob24zJTIwcnVuX3FhLnB5JTIwJTVDJTBBJTIwLS1tb2RlbF9uYW1lX29yX3BhdGglMjBnb29nbGUtYmVydCUyRmJlcnQtbGFyZ2UtdW5jYXNlZCUyMCU1QyUwQSUyMC0tZGF0YXNldF9uYW1lJTIwc3F1YWQlMjAlNUMlMEElMjAtLWRvX3RyYWluJTIwJTVDJTBBJTIwLS1kb19ldmFsJTIwJTVDJTBBJTIwLS1wZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlMjAxMiUyMCUyMCU1QyUwQSUyMC0tbGVhcm5pbmdfcmF0ZSUyMDNlLTUlMjAlMjAlNUMlMEElMjAtLW51bV90cmFpbl9lcG9jaHMlMjAyJTIwJTIwJTVDJTBBJTIwLS1tYXhfc2VxX2xlbmd0aCUyMDM4NCUyMCU1QyUwQSUyMC0tZG9jX3N0cmlkZSUyMDEyOCUyMCUyMCU1QyUwQSUyMC0tb3V0cHV0X2RpciUyMCUyRnRtcCUyRmRlYnVnX3NxdWFkJTJGJTIwJTVDJTBBJTIwLS1ub19jdWRhJTIwJTVDJTBBJTIwLS1kZHBfYmFja2VuZCUyMGNjbA==",highlighted:`<span class="hljs-built_in">export</span> CCL_WORKER_COUNT=1
<span class="hljs-built_in">export</span> MASTER_ADDR=127.0.0.1
mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
python3 run_qa.py \\
 --model_name_or_path google-bert/bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --ddp_backend ccl`,wrap:!1}}),{c(){n=o("p"),n.innerHTML=y,a=r(),C(T.$$.fragment),i=r(),C(J.$$.fragment)},l(U){n=w(U,"P",{"data-svelte-h":!0}),c(n)!=="svelte-6lyzik"&&(n.innerHTML=y),a=p(U),m(T.$$.fragment,U),i=p(U),m(J.$$.fragment,U)},m(U,f){s(U,n,f),s(U,a,f),j(T,U,f),s(U,i,f),j(J,U,f),d=!0},p(U,f){const $={};f&2&&($.$$scope={dirty:f,ctx:U}),T.$set($)},i(U){d||(I(T.$$.fragment,U),I(J.$$.fragment,U),d=!0)},o(U){h(T.$$.fragment,U),h(J.$$.fragment,U),d=!1},d(U){U&&(t(n),t(a),t(i)),u(T,U),u(J,U)}}}function Ae(A){let n,y="Tune the variable <code>OMP_NUM_THREADS/CCL_WORKER_COUNT</code> for optimal performance.";return{c(){n=o("p"),n.innerHTML=y},l(a){n=w(a,"P",{"data-svelte-h":!0}),c(n)!=="svelte-4dbzn8"&&(n.innerHTML=y)},m(a,T){s(a,n,T)},p:Yl,d(a){a&&t(n)}}}function $e(A){let n,y="Scale the training script to four processes on two Xeon CPUs (<code>node0</code> and <code>node1</code>) by setting <code>-n 4</code> and <code>ppn 2</code>. The <code>ppn</code> parameter specifies the number of processes per node, with one process running per socket.",a,T,i="Assume <code>node0</code> is the main process and create a configuration file containing the IP addresses of each node (for example, hostfile) and pass the configuration file path as an argument.",J,d,U,f,$="Run the script below on <code>node0</code> to enable DDP on <code>node0</code> and <code>node1</code> and train with bf16 auto mixed precision.",S,g,B,G,v;return d=new _({props:{code:"Y2F0JTIwaG9zdGZpbGUlMEF4eHgueHh4Lnh4eC54eHglMjAlMjNub2RlMCUyMGlwJTBBeHh4Lnh4eC54eHgueHh4JTIwJTIzbm9kZTElMjBpcA==",highlighted:`<span class="hljs-built_in">cat</span> hostfile
xxx.xxx.xxx.xxx <span class="hljs-comment">#node0 ip</span>
xxx.xxx.xxx.xxx <span class="hljs-comment">#node1 ip</span>`,wrap:!1}}),g=new Ll({props:{warning:!1,$$slots:{default:[Ae]},$$scope:{ctx:A}}}),G=new _({props:{code:"ZXhwb3J0JTIwQ0NMX1dPUktFUl9DT1VOVCUzRDElMEFleHBvcnQlMjBNQVNURVJfQUREUiUzRHh4eC54eHgueHh4Lnh4eCUyMCUyM25vZGUwJTIwaXAlMEFtcGlydW4lMjAtZiUyMGhvc3RmaWxlJTIwLW4lMjA0JTIwLXBwbiUyMDIlMjAlNUMlMEElMjAtZ2VudiUyME9NUF9OVU1fVEhSRUFEUyUzRDIzJTIwJTVDJTBBcHl0aG9uMyUyMHJ1bl9xYS5weSUyMCU1QyUwQSUyMC0tbW9kZWxfbmFtZV9vcl9wYXRoJTIwZ29vZ2xlLWJlcnQlMkZiZXJ0LWxhcmdlLXVuY2FzZWQlMjAlNUMlMEElMjAtLWRhdGFzZXRfbmFtZSUyMHNxdWFkJTIwJTVDJTBBJTIwLS1kb190cmFpbiUyMCU1QyUwQSUyMC0tZG9fZXZhbCUyMCU1QyUwQSUyMC0tcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTIwMTIlMjAlMjAlNUMlMEElMjAtLWxlYXJuaW5nX3JhdGUlMjAzZS01JTIwJTIwJTVDJTBBJTIwLS1udW1fdHJhaW5fZXBvY2hzJTIwMiUyMCUyMCU1QyUwQSUyMC0tbWF4X3NlcV9sZW5ndGglMjAzODQlMjAlNUMlMEElMjAtLWRvY19zdHJpZGUlMjAxMjglMjAlMjAlNUMlMEElMjAtLW91dHB1dF9kaXIlMjAlMkZ0bXAlMkZkZWJ1Z19zcXVhZCUyRiUyMCU1QyUwQSUyMC0tbm9fY3VkYSUyMCU1QyUwQSUyMC0tZGRwX2JhY2tlbmQlMjBjY2wlMjAlNUMlMEElMjAtLWJmMTY=",highlighted:`<span class="hljs-built_in">export</span> CCL_WORKER_COUNT=1
<span class="hljs-built_in">export</span> MASTER_ADDR=xxx.xxx.xxx.xxx <span class="hljs-comment">#node0 ip</span>
mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
python3 run_qa.py \\
 --model_name_or_path google-bert/bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --ddp_backend ccl \\
 --bf16`,wrap:!1}}),{c(){n=o("p"),n.innerHTML=y,a=r(),T=o("p"),T.innerHTML=i,J=r(),C(d.$$.fragment),U=r(),f=o("p"),f.innerHTML=$,S=r(),C(g.$$.fragment),B=r(),C(G.$$.fragment)},l(M){n=w(M,"P",{"data-svelte-h":!0}),c(n)!=="svelte-ba432r"&&(n.innerHTML=y),a=p(M),T=w(M,"P",{"data-svelte-h":!0}),c(T)!=="svelte-1mhbkfw"&&(T.innerHTML=i),J=p(M),m(d.$$.fragment,M),U=p(M),f=w(M,"P",{"data-svelte-h":!0}),c(f)!=="svelte-1hmztf"&&(f.innerHTML=$),S=p(M),m(g.$$.fragment,M),B=p(M),m(G.$$.fragment,M)},m(M,b){s(M,n,b),s(M,a,b),s(M,T,b),s(M,J,b),j(d,M,b),s(M,U,b),s(M,f,b),s(M,S,b),j(g,M,b),s(M,B,b),j(G,M,b),v=!0},p(M,b){const W={};b&2&&(W.$$scope={dirty:b,ctx:M}),g.$set(W)},i(M){v||(I(d.$$.fragment,M),I(g.$$.fragment,M),I(G.$$.fragment,M),v=!0)},o(M){h(d.$$.fragment,M),h(g.$$.fragment,M),h(G.$$.fragment,M),v=!1},d(M){M&&(t(n),t(a),t(T),t(J),t(U),t(f),t(S),t(B)),u(d,M),u(g,M),u(G,M)}}}function ge(A){let n,y,a,T;return n=new we({props:{id:"distrib-cpu",option:"single node",$$slots:{default:[be]},$$scope:{ctx:A}}}),a=new we({props:{id:"distrib-cpu",option:"multiple nodes",$$slots:{default:[$e]},$$scope:{ctx:A}}}),{c(){C(n.$$.fragment),y=r(),C(a.$$.fragment)},l(i){m(n.$$.fragment,i),y=p(i),m(a.$$.fragment,i)},m(i,J){j(n,i,J),s(i,y,J),j(a,i,J),T=!0},p(i,J){const d={};J&2&&(d.$$scope={dirty:J,ctx:i}),n.$set(d);const U={};J&2&&(U.$$scope={dirty:J,ctx:i}),a.$set(U)},i(i){T||(I(n.$$.fragment,i),I(a.$$.fragment,i),T=!0)},o(i){h(n.$$.fragment,i),h(a.$$.fragment,i),T=!1},d(i){i&&t(y),u(n,i),u(a,i)}}}function Be(A){let n,y,a,T,i,J,d,U="CPUs are commonly available and can be a cost-effective training option when GPUs are unavailable. When training large models or if a single CPU is too slow, distributed training with CPUs can help speed up training.",f,$,S='This guide demonstrates how to perform distributed training with multiple CPUs using a <a href="./perf_train_gpu_many#distributeddataparallel">DistributedDataParallel (DDP)</a> strategy on bare metal with <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> and a Kubernetes cluster. All examples shown in this guide depend on the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit.html" rel="nofollow">Intel oneAPI HPC Toolkit</a>.',g,B,G="There are two toolkits you’ll need from Intel oneAPI.",v,M,b='<li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html" rel="nofollow">oneCCL</a> includes efficient implementations of collectives commonly used in deep learning such as all-gather, all-reduce, and reduce-scatter. To install from a prebuilt wheel, make sure you always use the latest release. Refer to the table <a href="https://github.com/intel/torch-ccl#install-prebuilt-wheel" rel="nofollow">here</a> to check if a version of oneCCL is supported for a Python and PyTorch version.</li>',W,E,yl,N,Jl,x,zl='<li><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html" rel="nofollow">MPI</a> is a message-passing interface for communications between hardware and networks. The oneCCL toolkit is installed along with MPI, but you need to source the environment as shown below before using it.</li>',cl,V,Cl,k,Pl='Lastly, install the <a href="https://intel.github.io/intel-extension-for-pytorch/index.html" rel="nofollow">Intex Extension for PyTorch (IPEX)</a> which enables additional performance optimizations for Intel hardware such as weight sharing and better thread runtime control.',ml,X,jl,R,Il,Q,hl,H,Dl='<a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> supports distributed training with CPUs with the oneCCL backend. Add the <code>--ddp_backend ccl</code> parameter in the command arguments to enable it.',ul,Z,dl,F,fl,L,ql='Distributed training with CPUs can also be deployed to a Kubernetes cluster with <a href="https://www.kubeflow.org/docs/components/training/user-guides/pytorch/" rel="nofollow">PyTorchJob</a>. Before you get started, you should perform the following setup steps.',bl,Y,Ol='<li>Ensure you have access to a Kubernetes cluster with <a href="https://www.kubeflow.org/docs/started/installing-kubeflow/" rel="nofollow">Kubeflow</a> installed.</li> <li>Install and configure <a href="https://kubernetes.io/docs/tasks/tools" rel="nofollow">kubectl</a> to interact with the cluster.</li> <li>Set up a <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" rel="nofollow">PersistentVolumeClaim (PVC)</a> to store datasets and model files. There are multiple options to choose from, including a <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" rel="nofollow">StorageClass</a> or a cloud storage bucket.</li> <li>Set up a Docker container for the training script and all required dependencies such as PyTorch, Transformers, IPEX, oneCCL, and OpenSSH to facilitate communicattion between containers.</li>',Al,z,Kl="The example Dockerfile below uses a base image that supports distributed training with CPUs, and extracts Transformers to the <code>/workspace</code> directory to include the training scripts in the image. The image needs to be built and copied to the clusters nodes or pushed to a container registry prior to deployment.",$l,P,gl,D,Bl,q,le='<a href="https://www.kubeflow.org/docs/components/training/user-guides/pytorch/" rel="nofollow">PyTorchJob</a> is an extension of the Kubernetes API for running PyTorch training jobs on Kubernetes. It includes a yaml file that defines the training jobs parameters such as the name of the PyTorchJob, number of workers, types of resources for each worker, and more.',_l,O,ee="The volume mount parameter is a path to where the PVC is mounted in the container for each worker pod. The PVC is typically used to hold the dataset, checkpoint files, and the model after it has finished training.",Gl,K,te='The example yaml file below sets up four workers on the <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering" rel="nofollow">run_qa.py</a> script. Adapt the yaml file based on your training script and number of nodes in your cluster.',vl,ll,se='The CPU resource limits and requests are defined in <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu" rel="nofollow">CPU units</a>. One CPU unit is equivalent to one physical CPU core or virtual core. The CPU units defined in the yaml file should be less than the amount of available CPU and memory capacity of a single machine in order to leave some resources for kubelet and the system. For a <code>Guaranteed</code> <a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod" rel="nofollow">quality of service</a>, set the same CPU and memory amounts for both the resource limits and requests.',Nl,el,Rl,tl,Zl,sl,ne="After you’ve setup the PyTorchJob yaml file with the appropriate settings for your cluster and training job, deploy it to the cluster with the command below.",Sl,nl,Wl,al,ae="List the pods in the namespace with <code>kubectl get pods -n ${NAMESPACE}</code>. At first, the status may be “Pending” but it should change to “Running” once the containers are pulled and created.",El,Ml,xl,rl,Me="Inspect the logs for each worker with the following command. Add <code>-f</code> to stream the logs.",Vl,pl,kl,il,re="Once training is complete, the trained model can be copied from the PVC or storage location. Delete the PyTorchJob resource from the cluster with the command below.",Xl,Tl,Ql,Ul,Hl,ol,Fl;return i=new wl({props:{title:"Distributed CPUs",local:"distributed-cpus",headingTag:"h1"}}),E=new _({props:{code:"JTIzJTIwaW5zdGFsbHMlMjBvbmVDQ0wlMjBmb3IlMjBQeVRvcmNoJTIwMi40LjAlMEFwaXAlMjBpbnN0YWxsJTIwb25lY2NsX2JpbmRfcHQlM0QlM0QyLjQuMCUyMC1mJTIwaHR0cHMlM0ElMkYlMkZkZXZlbG9wZXIuaW50ZWwuY29tJTJGaXBleC13aGwtc3RhYmxlLWNwdQ==",highlighted:`<span class="hljs-comment"># installs oneCCL for PyTorch 2.4.0</span>
pip install oneccl_bind_pt==2.4.0 -f https://developer.intel.com/ipex-whl-stable-cpu`,wrap:!1}}),N=new Ll({props:{warning:!1,$$slots:{default:[ue]},$$scope:{ctx:A}}}),V=new _({props:{code:"b25lY2NsX2JpbmRpbmdzX2Zvcl9weXRvcmNoX3BhdGglM0QlMjQocHl0aG9uJTIwLWMlMjAlMjJmcm9tJTIwb25lY2NsX2JpbmRpbmdzX2Zvcl9weXRvcmNoJTIwaW1wb3J0JTIwY3dkJTNCJTIwcHJpbnQoY3dkKSUyMiklMEFzb3VyY2UlMjAlMjRvbmVjY2xfYmluZGluZ3NfZm9yX3B5dG9yY2hfcGF0aCUyRmVudiUyRnNldHZhcnMuc2g=",highlighted:`oneccl_bindings_for_pytorch_path=$(python -c <span class="hljs-string">&quot;from oneccl_bindings_for_pytorch import cwd; print(cwd)&quot;</span>)
<span class="hljs-built_in">source</span> <span class="hljs-variable">$oneccl_bindings_for_pytorch_path</span>/env/setvars.sh`,wrap:!1}}),X=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMGludGVsX2V4dGVuc2lvbl9mb3JfcHl0b3JjaCUzRCUzRCUzQ3ZlcnNpb25fbmFtZSUzRSUyMC1mJTIwaHR0cHMlM0ElMkYlMkZkZXZlbG9wZXIuaW50ZWwuY29tJTJGaXBleC13aGwtc3RhYmxlLWNwdQ==",highlighted:"pip install intel_extension_for_pytorch==&lt;version_name&gt; -f https://developer.intel.com/ipex-whl-stable-cpu",wrap:!1}}),R=new Ll({props:{warning:!1,$$slots:{default:[de]},$$scope:{ctx:A}}}),Q=new wl({props:{title:"Trainer",local:"trainer",headingTag:"h2"}}),Z=new he({props:{id:"distrib-cpu",options:["single node","multiple nodes"],$$slots:{default:[ge]},$$scope:{ctx:A}}}),F=new wl({props:{title:"Kubernetes",local:"kubernetes",headingTag:"h2"}}),P=new _({props:{code:"RlJPTSUyMGludGVsJTJGaW50ZWwtb3B0aW1pemVkLXB5dG9yY2glM0EyLjQuMC1waXAtbXVsdGlub2RlJTBBJTBBUlVOJTIwYXB0LWdldCUyMHVwZGF0ZSUyMC15JTIwJTI2JTI2JTIwJTVDJTBBJTIwJTIwJTIwJTIwYXB0LWdldCUyMGluc3RhbGwlMjAteSUyMC0tbm8taW5zdGFsbC1yZWNvbW1lbmRzJTIwLS1maXgtbWlzc2luZyUyMCU1QyUwQSUyMCUyMCUyMCUyMGdvb2dsZS1wZXJmdG9vbHMlMjAlNUMlMEElMjAlMjAlMjAlMjBsaWJvbXAtZGV2JTBBJTBBV09SS0RJUiUyMCUyRndvcmtzcGFjZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwYW5kJTIwZXh0cmFjdCUyMHRoZSUyMHRyYW5zZm9ybWVycyUyMGNvZGUlMEFBUkclMjBIRl9UUkFOU0ZPUk1FUlNfVkVSJTNEJTIyNC40Ni4wJTIyJTBBUlVOJTIwcGlwJTIwaW5zdGFsbCUyMC0tbm8tY2FjaGUtZGlyJTIwJTVDJTBBJTIwJTIwJTIwJTIwdHJhbnNmb3JtZXJzJTNEJTNEJTI0JTdCSEZfVFJBTlNGT1JNRVJTX1ZFUiU3RCUyMCUyNiUyNiUyMCU1QyUwQSUyMCUyMCUyMCUyMG1rZGlyJTIwdHJhbnNmb3JtZXJzJTIwJTI2JTI2JTIwJTVDJTBBJTIwJTIwJTIwJTIwY3VybCUyMC1zU0wlMjAtLXJldHJ5JTIwNSUyMGh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmh1Z2dpbmdmYWNlJTJGdHJhbnNmb3JtZXJzJTJGYXJjaGl2ZSUyRnJlZnMlMkZ0YWdzJTJGdiUyNCU3QkhGX1RSQU5TRk9STUVSU19WRVIlN0QudGFyLmd6JTIwJTdDJTIwdGFyJTIwLUMlMjB0cmFuc2Zvcm1lcnMlMjAtLXN0cmlwLWNvbXBvbmVudHMlM0QxJTIwLXh6ZiUyMC0=",highlighted:`<span class="hljs-keyword">FROM</span> intel/intel-optimized-pytorch:<span class="hljs-number">2.4</span>.<span class="hljs-number">0</span>-pip-multinode

<span class="hljs-keyword">RUN</span><span class="language-bash"> apt-get update -y &amp;&amp; \\
    apt-get install -y --no-install-recommends --fix-missing \\
    google-perftools \\
    libomp-dev</span>

<span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /workspace</span>

<span class="hljs-comment"># Download and extract the transformers code</span>
<span class="hljs-keyword">ARG</span> HF_TRANSFORMERS_VER=<span class="hljs-string">&quot;4.46.0&quot;</span>
<span class="hljs-keyword">RUN</span><span class="language-bash"> pip install --no-cache-dir \\
    transformers==<span class="hljs-variable">\${HF_TRANSFORMERS_VER}</span> &amp;&amp; \\
    <span class="hljs-built_in">mkdir</span> transformers &amp;&amp; \\
    curl -sSL --retry 5 https://github.com/huggingface/transformers/archive/refs/tags/v<span class="hljs-variable">\${HF_TRANSFORMERS_VER}</span>.tar.gz | tar -C transformers --strip-components=1 -xzf -</span>`,wrap:!1}}),D=new wl({props:{title:"PyTorchJob",local:"pytorchjob",headingTag:"h3"}}),el=new _({props:{code:"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",highlighted:`<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">&quot;kubeflow.org/v1&quot;</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PyTorchJob</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">transformers-pytorchjob</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">elasticPolicy:</span>
    <span class="hljs-attr">rdzvBackend:</span> <span class="hljs-string">c10d</span>
    <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">1</span>
    <span class="hljs-attr">maxReplicas:</span> <span class="hljs-number">4</span>
    <span class="hljs-attr">maxRestarts:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">pytorchReplicaSpecs:</span>
    <span class="hljs-attr">Worker:</span>
      <span class="hljs-attr">replicas:</span> <span class="hljs-number">4</span>  <span class="hljs-comment"># The number of worker pods</span>
      <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">OnFailure</span>
      <span class="hljs-attr">template:</span>
        <span class="hljs-attr">spec:</span>
          <span class="hljs-attr">containers:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pytorch</span>
              <span class="hljs-attr">image:</span> <span class="hljs-string">&lt;image</span> <span class="hljs-string">name&gt;:&lt;tag&gt;</span>  <span class="hljs-comment"># Specify the docker image to use for the worker pods</span>
              <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>
              <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/bash&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]
              <span class="hljs-attr">args:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">&gt;-
                  cd /workspace/transformers;
                  pip install -r /workspace/transformers/examples/pytorch/question-answering/requirements.txt;
                  source /usr/local/lib/python3.10/dist-packages/oneccl_bindings_for_pytorch/env/setvars.sh;
                  torchrun /workspace/transformers/examples/pytorch/question-answering/run_qa.py \\
                    --model_name_or_path distilbert/distilbert-base-uncased \\
                    --dataset_name squad \\
                    --do_train \\
                    --do_eval \\
                    --per_device_train_batch_size 12 \\
                    --learning_rate 3e-5 \\
                    --num_train_epochs 2 \\
                    --max_seq_length 384 \\
                    --doc_stride 128 \\
                    --output_dir /tmp/pvc-mount/output_$(date +%Y%m%d_%H%M%S) \\
                    --no_cuda \\
                    --ddp_backend ccl \\
                    --bf16;
</span>              <span class="hljs-attr">env:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">LD_PRELOAD</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9:/usr/local/lib/libiomp5.so&quot;</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">TRANSFORMERS_CACHE</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;/tmp/pvc-mount/transformers_cache&quot;</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">HF_DATASETS_CACHE</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;/tmp/pvc-mount/hf_datasets_cache&quot;</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">LOGLEVEL</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;INFO&quot;</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CCL_WORKER_COUNT</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;1&quot;</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">OMP_NUM_THREADS</span>  <span class="hljs-comment"># Can be tuned for optimal performance</span>
                <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;240&quot;</span>
              <span class="hljs-attr">resources:</span>
                <span class="hljs-attr">limits:</span>
                  <span class="hljs-attr">cpu:</span> <span class="hljs-number">240</span>  <span class="hljs-comment"># Update the CPU and memory limit values based on your nodes</span>
                  <span class="hljs-attr">memory:</span> <span class="hljs-string">128Gi</span>
                <span class="hljs-attr">requests:</span>
                  <span class="hljs-attr">cpu:</span> <span class="hljs-number">240</span>  <span class="hljs-comment"># Update the CPU and memory request values based on your nodes</span>
                  <span class="hljs-attr">memory:</span> <span class="hljs-string">128Gi</span>
              <span class="hljs-attr">volumeMounts:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pvc-volume</span>
                <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/tmp/pvc-mount</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/dev/shm</span>
                <span class="hljs-attr">name:</span> <span class="hljs-string">dshm</span>
          <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span>
          <span class="hljs-attr">nodeSelector:</span>  <span class="hljs-comment"># Optionally use nodeSelector to match a certain node label for the worker pods</span>
            <span class="hljs-attr">node-type:</span> <span class="hljs-string">gnr</span>
          <span class="hljs-attr">volumes:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pvc-volume</span>
            <span class="hljs-attr">persistentVolumeClaim:</span>
              <span class="hljs-attr">claimName:</span> <span class="hljs-string">transformers-pvc</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dshm</span>
            <span class="hljs-attr">emptyDir:</span>
              <span class="hljs-attr">medium:</span> <span class="hljs-string">Memory</span>`,wrap:!1}}),tl=new wl({props:{title:"Deploy",local:"deploy",headingTag:"h3"}}),nl=new _({props:{code:"ZXhwb3J0JTIwTkFNRVNQQUNFJTNEJTNDc3BlY2lmeSUyMHlvdXIlMjBuYW1lc3BhY2UlM0UlMEElMEFrdWJlY3RsJTIwY3JlYXRlJTIwLWYlMjBweXRvcmNoam9iLnlhbWwlMjAtbiUyMCUyNCU3Qk5BTUVTUEFDRSU3RA==",highlighted:`<span class="hljs-built_in">export</span> NAMESPACE=&lt;specify your namespace&gt;

kubectl create -f pytorchjob.yaml -n <span class="hljs-variable">\${NAMESPACE}</span>`,wrap:!1}}),Ml=new _({props:{code:"",highlighted:`kubectl get pods -n <span class="hljs-variable">\${NAMESPACE}</span>

NAME                                                     READY   STATUS                  RESTARTS          AGE
...
transformers-pytorchjob-worker-0                         1/1     Running                 0                 7m37s
transformers-pytorchjob-worker-1                         1/1     Running                 0                 7m37s
transformers-pytorchjob-worker-2                         1/1     Running                 0                 7m37s
transformers-pytorchjob-worker-3                         1/1     Running                 0                 7m37s
...`,wrap:!1}}),pl=new _({props:{code:"a3ViZWN0bCUyMGxvZ3MlMjB0cmFuc2Zvcm1lcnMtcHl0b3JjaGpvYi13b3JrZXItMCUyMC1uJTIwJTI0JTdCTkFNRVNQQUNFJTdEJTIwLWY=",highlighted:'kubectl logs transformers-pytorchjob-worker-0 -n <span class="hljs-variable">${NAMESPACE}</span> -f',wrap:!1}}),Tl=new _({props:{code:"a3ViZWN0bCUyMGRlbGV0ZSUyMC1mJTIwcHl0b3JjaGpvYi55YW1sJTIwLW4lMjAlMjQlN0JOQU1FU1BBQ0UlN0Q=",highlighted:'kubectl delete -f pytorchjob.yaml -n <span class="hljs-variable">${NAMESPACE}</span>',wrap:!1}}),Ul=new Ie({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu_many.md"}}),{c(){n=o("meta"),y=r(),a=o("p"),T=r(),C(i.$$.fragment),J=r(),d=o("p"),d.textContent=U,f=r(),$=o("p"),$.innerHTML=S,g=r(),B=o("p"),B.textContent=G,v=r(),M=o("ol"),M.innerHTML=b,W=r(),C(E.$$.fragment),yl=r(),C(N.$$.fragment),Jl=r(),x=o("ol"),x.innerHTML=zl,cl=r(),C(V.$$.fragment),Cl=r(),k=o("p"),k.innerHTML=Pl,ml=r(),C(X.$$.fragment),jl=r(),C(R.$$.fragment),Il=r(),C(Q.$$.fragment),hl=r(),H=o("p"),H.innerHTML=Dl,ul=r(),C(Z.$$.fragment),dl=r(),C(F.$$.fragment),fl=r(),L=o("p"),L.innerHTML=ql,bl=r(),Y=o("ol"),Y.innerHTML=Ol,Al=r(),z=o("p"),z.innerHTML=Kl,$l=r(),C(P.$$.fragment),gl=r(),C(D.$$.fragment),Bl=r(),q=o("p"),q.innerHTML=le,_l=r(),O=o("p"),O.textContent=ee,Gl=r(),K=o("p"),K.innerHTML=te,vl=r(),ll=o("p"),ll.innerHTML=se,Nl=r(),C(el.$$.fragment),Rl=r(),C(tl.$$.fragment),Zl=r(),sl=o("p"),sl.textContent=ne,Sl=r(),C(nl.$$.fragment),Wl=r(),al=o("p"),al.innerHTML=ae,El=r(),C(Ml.$$.fragment),xl=r(),rl=o("p"),rl.innerHTML=Me,Vl=r(),C(pl.$$.fragment),kl=r(),il=o("p"),il.textContent=re,Xl=r(),C(Tl.$$.fragment),Ql=r(),C(Ul.$$.fragment),Hl=r(),ol=o("p"),this.h()},l(l){const e=me("svelte-u9bgzb",document.head);n=w(e,"META",{name:!0,content:!0}),e.forEach(t),y=p(l),a=w(l,"P",{}),Ue(a).forEach(t),T=p(l),m(i.$$.fragment,l),J=p(l),d=w(l,"P",{"data-svelte-h":!0}),c(d)!=="svelte-1db6toe"&&(d.textContent=U),f=p(l),$=w(l,"P",{"data-svelte-h":!0}),c($)!=="svelte-bg5ywc"&&($.innerHTML=S),g=p(l),B=w(l,"P",{"data-svelte-h":!0}),c(B)!=="svelte-z73c5n"&&(B.textContent=G),v=p(l),M=w(l,"OL",{"data-svelte-h":!0}),c(M)!=="svelte-90fu2y"&&(M.innerHTML=b),W=p(l),m(E.$$.fragment,l),yl=p(l),m(N.$$.fragment,l),Jl=p(l),x=w(l,"OL",{"data-svelte-h":!0}),c(x)!=="svelte-1batrdf"&&(x.innerHTML=zl),cl=p(l),m(V.$$.fragment,l),Cl=p(l),k=w(l,"P",{"data-svelte-h":!0}),c(k)!=="svelte-1otdpgb"&&(k.innerHTML=Pl),ml=p(l),m(X.$$.fragment,l),jl=p(l),m(R.$$.fragment,l),Il=p(l),m(Q.$$.fragment,l),hl=p(l),H=w(l,"P",{"data-svelte-h":!0}),c(H)!=="svelte-4lo5uu"&&(H.innerHTML=Dl),ul=p(l),m(Z.$$.fragment,l),dl=p(l),m(F.$$.fragment,l),fl=p(l),L=w(l,"P",{"data-svelte-h":!0}),c(L)!=="svelte-1pbm113"&&(L.innerHTML=ql),bl=p(l),Y=w(l,"OL",{"data-svelte-h":!0}),c(Y)!=="svelte-2pmwlj"&&(Y.innerHTML=Ol),Al=p(l),z=w(l,"P",{"data-svelte-h":!0}),c(z)!=="svelte-qbn6pm"&&(z.innerHTML=Kl),$l=p(l),m(P.$$.fragment,l),gl=p(l),m(D.$$.fragment,l),Bl=p(l),q=w(l,"P",{"data-svelte-h":!0}),c(q)!=="svelte-1m9zjwg"&&(q.innerHTML=le),_l=p(l),O=w(l,"P",{"data-svelte-h":!0}),c(O)!=="svelte-wrjz37"&&(O.textContent=ee),Gl=p(l),K=w(l,"P",{"data-svelte-h":!0}),c(K)!=="svelte-sjxwh"&&(K.innerHTML=te),vl=p(l),ll=w(l,"P",{"data-svelte-h":!0}),c(ll)!=="svelte-1d9s969"&&(ll.innerHTML=se),Nl=p(l),m(el.$$.fragment,l),Rl=p(l),m(tl.$$.fragment,l),Zl=p(l),sl=w(l,"P",{"data-svelte-h":!0}),c(sl)!=="svelte-1b7ewcg"&&(sl.textContent=ne),Sl=p(l),m(nl.$$.fragment,l),Wl=p(l),al=w(l,"P",{"data-svelte-h":!0}),c(al)!=="svelte-d7wzxc"&&(al.innerHTML=ae),El=p(l),m(Ml.$$.fragment,l),xl=p(l),rl=w(l,"P",{"data-svelte-h":!0}),c(rl)!=="svelte-fir50r"&&(rl.innerHTML=Me),Vl=p(l),m(pl.$$.fragment,l),kl=p(l),il=w(l,"P",{"data-svelte-h":!0}),c(il)!=="svelte-16x58zj"&&(il.textContent=re),Xl=p(l),m(Tl.$$.fragment,l),Ql=p(l),m(Ul.$$.fragment,l),Hl=p(l),ol=w(l,"P",{}),Ue(ol).forEach(t),this.h()},h(){oe(n,"name","hf:doc:metadata"),oe(n,"content",_e)},m(l,e){je(document.head,n),s(l,y,e),s(l,a,e),s(l,T,e),j(i,l,e),s(l,J,e),s(l,d,e),s(l,f,e),s(l,$,e),s(l,g,e),s(l,B,e),s(l,v,e),s(l,M,e),s(l,W,e),j(E,l,e),s(l,yl,e),j(N,l,e),s(l,Jl,e),s(l,x,e),s(l,cl,e),j(V,l,e),s(l,Cl,e),s(l,k,e),s(l,ml,e),j(X,l,e),s(l,jl,e),j(R,l,e),s(l,Il,e),j(Q,l,e),s(l,hl,e),s(l,H,e),s(l,ul,e),j(Z,l,e),s(l,dl,e),j(F,l,e),s(l,fl,e),s(l,L,e),s(l,bl,e),s(l,Y,e),s(l,Al,e),s(l,z,e),s(l,$l,e),j(P,l,e),s(l,gl,e),j(D,l,e),s(l,Bl,e),s(l,q,e),s(l,_l,e),s(l,O,e),s(l,Gl,e),s(l,K,e),s(l,vl,e),s(l,ll,e),s(l,Nl,e),j(el,l,e),s(l,Rl,e),j(tl,l,e),s(l,Zl,e),s(l,sl,e),s(l,Sl,e),j(nl,l,e),s(l,Wl,e),s(l,al,e),s(l,El,e),j(Ml,l,e),s(l,xl,e),s(l,rl,e),s(l,Vl,e),j(pl,l,e),s(l,kl,e),s(l,il,e),s(l,Xl,e),j(Tl,l,e),s(l,Ql,e),j(Ul,l,e),s(l,Hl,e),s(l,ol,e),Fl=!0},p(l,[e]){const pe={};e&2&&(pe.$$scope={dirty:e,ctx:l}),N.$set(pe);const ie={};e&2&&(ie.$$scope={dirty:e,ctx:l}),R.$set(ie);const Te={};e&2&&(Te.$$scope={dirty:e,ctx:l}),Z.$set(Te)},i(l){Fl||(I(i.$$.fragment,l),I(E.$$.fragment,l),I(N.$$.fragment,l),I(V.$$.fragment,l),I(X.$$.fragment,l),I(R.$$.fragment,l),I(Q.$$.fragment,l),I(Z.$$.fragment,l),I(F.$$.fragment,l),I(P.$$.fragment,l),I(D.$$.fragment,l),I(el.$$.fragment,l),I(tl.$$.fragment,l),I(nl.$$.fragment,l),I(Ml.$$.fragment,l),I(pl.$$.fragment,l),I(Tl.$$.fragment,l),I(Ul.$$.fragment,l),Fl=!0)},o(l){h(i.$$.fragment,l),h(E.$$.fragment,l),h(N.$$.fragment,l),h(V.$$.fragment,l),h(X.$$.fragment,l),h(R.$$.fragment,l),h(Q.$$.fragment,l),h(Z.$$.fragment,l),h(F.$$.fragment,l),h(P.$$.fragment,l),h(D.$$.fragment,l),h(el.$$.fragment,l),h(tl.$$.fragment,l),h(nl.$$.fragment,l),h(Ml.$$.fragment,l),h(pl.$$.fragment,l),h(Tl.$$.fragment,l),h(Ul.$$.fragment,l),Fl=!1},d(l){l&&(t(y),t(a),t(T),t(J),t(d),t(f),t($),t(g),t(B),t(v),t(M),t(W),t(yl),t(Jl),t(x),t(cl),t(Cl),t(k),t(ml),t(jl),t(Il),t(hl),t(H),t(ul),t(dl),t(fl),t(L),t(bl),t(Y),t(Al),t(z),t($l),t(gl),t(Bl),t(q),t(_l),t(O),t(Gl),t(K),t(vl),t(ll),t(Nl),t(Rl),t(Zl),t(sl),t(Sl),t(Wl),t(al),t(El),t(xl),t(rl),t(Vl),t(kl),t(il),t(Xl),t(Ql),t(Hl),t(ol)),t(n),u(i,l),u(E,l),u(N,l),u(V,l),u(X,l),u(R,l),u(Q,l),u(Z,l),u(F,l),u(P,l),u(D,l),u(el,l),u(tl,l),u(nl,l),u(Ml,l),u(pl,l),u(Tl,l),u(Ul,l)}}}const _e='{"title":"Distributed CPUs","local":"distributed-cpus","sections":[{"title":"Trainer","local":"trainer","sections":[],"depth":2},{"title":"Kubernetes","local":"kubernetes","sections":[{"title":"PyTorchJob","local":"pytorchjob","sections":[],"depth":3},{"title":"Deploy","local":"deploy","sections":[],"depth":3}],"depth":2}],"depth":1}';function Ge(A){return Je(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ee extends ce{constructor(n){super(),Ce(this,n,Ge,Be,ye,{})}}export{Ee as component};
