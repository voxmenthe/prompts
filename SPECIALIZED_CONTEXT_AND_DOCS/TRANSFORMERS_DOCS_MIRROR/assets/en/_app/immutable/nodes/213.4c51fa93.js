import{s as sn,o as rn,n as Z}from"../chunks/scheduler.18a86fab.js";import{S as ln,i as dn,g as m,s as l,r as g,A as cn,h as u,f as a,c as i,j as R,x as J,u as _,k as I,l as pn,y as h,a as r,v as T,d as b,t as y,w as M}from"../chunks/index.98837b22.js";import{T as Gt}from"../chunks/Tip.77304350.js";import{D as ve}from"../chunks/Docstring.a1ef7999.js";import{C as W}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as pt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Pe,E as mn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as un,a as zt}from"../chunks/HfOption.6641485e.js";function hn(v){let t,p='This model was contributed by <a href="https://github.com/SO0529" rel="nofollow">Shinya Otani</a>, <a href="https://github.com/spider-man-tm" rel="nofollow">Takayoshi Makabe</a>, <a href="https://github.com/Anuj040" rel="nofollow">Anuj Arora</a>, and <a href="https://github.com/go5paopao" rel="nofollow">Kyo Hattori</a> from <a href="https://www.abejainc.com/" rel="nofollow">ABEJA, Inc.</a>.',n,d,f="Click on the GPT-NeoX-Japanese models in the right sidebar for more examples of how to apply GPT-NeoX-Japanese to different language tasks.";return{c(){t=m("p"),t.innerHTML=p,n=l(),d=m("p"),d.textContent=f},l(s){t=u(s,"P",{"data-svelte-h":!0}),J(t)!=="svelte-12wvpip"&&(t.innerHTML=p),n=i(s),d=u(s,"P",{"data-svelte-h":!0}),J(d)!=="svelte-1l0y4wx"&&(d.textContent=f)},m(s,c){r(s,t,c),r(s,n,c),r(s,d,c)},p:Z,d(s){s&&(a(t),a(n),a(d))}}}function fn(v){let t,p;return t=new W({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKHRhc2slM0QlMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMjAlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBtb2RlbCUzRCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiUyQyUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMGRldmljZSUzRDApJTBBcGlwZWxpbmUoJTIyJUU0JUJBJUJBJUUzJTgxJUE4QUklRTMlODElOEMlRTUlOEQlOTQlRTglQUElQkYlRTMlODElOTklRTMlODIlOEIlRTMlODElOUYlRTMlODIlODElRTMlODElQUIlRTMlODElQUYlRTMlODAlODElMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
pipeline = pipeline(task=<span class="hljs-string">&quot;text-generation&quot;</span>, 
                    model=<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>, dtype=torch.float16, device=<span class="hljs-number">0</span>)
pipeline(<span class="hljs-string">&quot;‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ&quot;</span>)`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,d){T(t,n,d),p=!0},p:Z,i(n){p||(b(t.$$.fragment,n),p=!0)},o(n){y(t.$$.fragment,n),p=!1},d(n){M(t,n)}}}function gn(v){let t,p;return t=new W({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQWlucHV0X2lkcyUyMCUzRCUyMHRva2VuaXplciglMjIlRTQlQkElQkElRTMlODElQThBSSVFMyU4MSU4QyVFNSU4RCU5NCVFOCVBQSVCRiVFMyU4MSU5OSVFMyU4MiU4QiVFMyU4MSU5RiVFMyU4MiU4MSVFMyU4MSVBQiVFMyU4MSVBRiVFMyU4MCU4MSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLmlucHV0X2lkcy50byhtb2RlbC5kZXZpY2UpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0X2lkcyklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKG91dHB1dHMlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
input_ids = tokenizer(<span class="hljs-string">&quot;‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids.to(model.device)
outputs = model.generate(input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,d){T(t,n,d),p=!0},p:Z,i(n){p||(b(t.$$.fragment,n),p=!0)},o(n){y(t.$$.fragment,n),p=!1},d(n){M(t,n)}}}function _n(v){let t,p;return t=new W({props:{code:"ZWNobyUyMC1lJTIwJTIyJUU0JUJBJUJBJUUzJTgxJUE4QUklRTMlODElOEMlRTUlOEQlOTQlRTglQUElQkYlRTMlODElOTklRTMlODIlOEIlRTMlODElOUYlRTMlODIlODElRTMlODElQUIlRTMlODElQUYlRTMlODAlODElMjIlMjAlN0MlMjB0cmFuc2Zvcm1lcnMlMjBydW4lMjAtLXRhc2slMjB0ZXh0LWdlbmVyYXRpb24lMjAtLW1vZGVsJTIwYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIwLS1kZXZpY2UlMjAw",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ&quot;</span> | transformers run --task text-generation --model abeja/gpt-neox-japanese-2.7b --device 0',wrap:!1}}),{c(){g(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,d){T(t,n,d),p=!0},p:Z,i(n){p||(b(t.$$.fragment,n),p=!0)},o(n){y(t.$$.fragment,n),p=!1},d(n){M(t,n)}}}function Tn(v){let t,p,n,d,f,s;return t=new zt({props:{id:"usage",option:"Pipeline",$$slots:{default:[fn]},$$scope:{ctx:v}}}),n=new zt({props:{id:"usage",option:"AutoModel",$$slots:{default:[gn]},$$scope:{ctx:v}}}),f=new zt({props:{id:"usage",option:"transformers CLI",$$slots:{default:[_n]},$$scope:{ctx:v}}}),{c(){g(t.$$.fragment),p=l(),g(n.$$.fragment),d=l(),g(f.$$.fragment)},l(c){_(t.$$.fragment,c),p=i(c),_(n.$$.fragment,c),d=i(c),_(f.$$.fragment,c)},m(c,U){T(t,c,U),r(c,p,U),T(n,c,U),r(c,d,U),T(f,c,U),s=!0},p(c,U){const Ie={};U&2&&(Ie.$$scope={dirty:U,ctx:c}),t.$set(Ie);const D={};U&2&&(D.$$scope={dirty:U,ctx:c}),n.$set(D);const P={};U&2&&(P.$$scope={dirty:U,ctx:c}),f.$set(P)},i(c){s||(b(t.$$.fragment,c),b(n.$$.fragment,c),b(f.$$.fragment,c),s=!0)},o(c){y(t.$$.fragment,c),y(n.$$.fragment,c),y(f.$$.fragment,c),s=!1},d(c){c&&(a(p),a(d)),M(t,c),M(n,c),M(f,c)}}}function bn(v){let t,p;return t=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZyUyQyUyMEdQVE5lb1hKYXBhbmVzZU1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdQVE5lb1hKYXBhbmVzZSUyMGdwdC1uZW94LWphcGFuZXNlLTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGdwdC1uZW94LWphcGFuZXNlLTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoXJapaneseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,d){T(t,n,d),p=!0},p:Z,i(n){p||(b(t.$$.fragment,n),p=!0)},o(n){y(t.$$.fragment,n),p=!1},d(n){M(t,n)}}}function yn(v){let t,p="Example:",n,d,f;return d=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVE5lb1hKYXBhbmVzZVRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZVRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQSUyMyUyMFlvdSUyMGNhbiUyMGNvbmZpcm0lMjBib3RoJTIwJUU2JTg1JUI2JUU1JUJGJTlDJTIwYW5kJTIwJUU2JTg1JUI2JUU2JTg3JTg5JTIwYXJlJTIwZW5jb2RlZCUyMHRvJTIwMTc3NDklMEF0b2tlbml6ZXIoJTIyJUU1JTkwJUJFJUU4JUJDJUE5JUUzJTgxJUFGJUU3JThDJUFCJUUzJTgxJUE3JUUzJTgxJTgyJUUzJTgyJThCJUYwJTlGJTkwJUFGJUUzJTgwJTgyJUU1JUFFJTlGJUUzJTgxJUFGJUU2JTg1JUI2JUU1JUJGJTlDKCVFNiU4NSVCNiVFNiU4NyU4OSklRTUlQTQlQTclRTUlQUQlQTYlRTUlODclQkElRTglQkElQUIlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBJTBBJTIzJTIwQm90aCUyMCVFNiU4NSVCNiVFNSVCRiU5QyUyMGFuZCUyMCVFNiU4NSVCNiVFNiU4NyU4OSUyMGFyZSUyMGRlY29kZWQlMjB0byUyMCVFNiU4NSVCNiVFNSVCRiU5QyUwQXRva2VuaXplci5kZWNvZGUodG9rZW5pemVyKCUyMiVFNSU5MCVCRSVFOCVCQyVBOSVFMyU4MSVBRiVFNyU4QyVBQiVFMyU4MSVBNyVFMyU4MSU4MiVFMyU4MiU4QiVGMCU5RiU5MCVBRiVFMyU4MCU4MiVFNSVBRSU5RiVFMyU4MSVBRiVFNiU4NSVCNiVFNSVCRiU5QyglRTYlODUlQjYlRTYlODclODkpJUU1JUE0JUE3JUU1JUFEJUE2JUU1JTg3JUJBJUU4JUJBJUFCJTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># You can confirm both ÊÖ∂Âøú and ÊÖ∂Êáâ are encoded to 17749</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">30014</span>, <span class="hljs-number">26883</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">27228</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26650</span>, <span class="hljs-number">31732</span>, <span class="hljs-number">31679</span>, <span class="hljs-number">27809</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31592</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31593</span>, <span class="hljs-number">321</span>, <span class="hljs-number">1281</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Both ÊÖ∂Âøú and ÊÖ∂Êáâ are decoded to ÊÖ∂Âøú</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Âøú)Â§ßÂ≠¶Âá∫Ë∫´&#x27;</span>`,wrap:!1}}),{c(){t=m("p"),t.textContent=p,n=l(),g(d.$$.fragment)},l(s){t=u(s,"P",{"data-svelte-h":!0}),J(t)!=="svelte-11lpom8"&&(t.textContent=p),n=i(s),_(d.$$.fragment,s)},m(s,c){r(s,t,c),r(s,n,c),T(d,s,c),f=!0},p:Z,i(s){f||(b(d.$$.fragment,s),f=!0)},o(s){y(d.$$.fragment,s),f=!1},d(s){s&&(a(t),a(n)),M(d,s)}}}function Mn(v){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=p},l(n){t=u(n,"P",{"data-svelte-h":!0}),J(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(n,d){r(n,t,d)},p:Z,d(n){n&&a(t)}}}function Jn(v){let t,p="Example:",n,d,f;return d=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHUFROZW9YSmFwYW5lc2VNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyJUU2JTk3JUE1JUU2JTlDJUFDJUU4JUFBJTlFJUUzJTgxJUFFR1BULW5lb3glRTMlODElOENIdWdnaW5nJTIwRmFjZSVFMyU4MSVBNyVFNCVCRCVCRiVFMyU4MSU4OCVFMyU4MSVCRSVFMyU4MSU5OSVGMCU5RiU5OCU4MCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPTNeoXJapaneseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Êó•Êú¨Ë™û„ÅÆGPT-neox„ÅåHugging Face„Åß‰Ωø„Åà„Åæ„ÅôüòÄ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){t=m("p"),t.textContent=p,n=l(),g(d.$$.fragment)},l(s){t=u(s,"P",{"data-svelte-h":!0}),J(t)!=="svelte-11lpom8"&&(t.textContent=p),n=i(s),_(d.$$.fragment,s)},m(s,c){r(s,t,c),r(s,n,c),T(d,s,c),f=!0},p:Z,i(s){f||(b(d.$$.fragment,s),f=!0)},o(s){y(d.$$.fragment,s),f=!1},d(s){s&&(a(t),a(n)),M(d,s)}}}function vn(v){let t,p=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=p},l(n){t=u(n,"P",{"data-svelte-h":!0}),J(t)!=="svelte-fincs2"&&(t.innerHTML=p)},m(n,d){r(n,t,d)},p:Z,d(n){n&&a(t)}}}function Un(v){let t,p="Example:",n,d,f;return d=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHUFROZW9YSmFwYW5lc2VGb3JDYXVzYWxMTSUyQyUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZyUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQWNvbmZpZyUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQWNvbmZpZy5pc19kZWNvZGVyJTIwJTNEJTIwVHJ1ZSUwQW1vZGVsJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiUyQyUyMGNvbmZpZyUzRGNvbmZpZyklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyJUU2JTk3JUE1JUU2JTlDJUFDJUU4JUFBJTlFJUUzJTgxJUFFR1BULW5lb3glRTMlODElOENIdWdnaW5nJTIwRmFjZSVFMyU4MSVBNyVFNCVCRCVCRiVFMyU4MSU4OCVFMyU4MSVCRSVFMyU4MSU5OSVGMCU5RiU5OCU4MCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQXByZWRpY3Rpb25fbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config = GPTNeoXJapaneseConfig.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>, config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Êó•Êú¨Ë™û„ÅÆGPT-neox„ÅåHugging Face„Åß‰Ωø„Åà„Åæ„ÅôüòÄ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>prediction_logits = outputs.logits`,wrap:!1}}),{c(){t=m("p"),t.textContent=p,n=l(),g(d.$$.fragment)},l(s){t=u(s,"P",{"data-svelte-h":!0}),J(t)!=="svelte-11lpom8"&&(t.textContent=p),n=i(s),_(d.$$.fragment,s)},m(s,c){r(s,t,c),r(s,n,c),T(d,s,c),f=!0},p:Z,i(s){f||(b(d.$$.fragment,s),f=!0)},o(s){y(d.$$.fragment,s),f=!1},d(s){s&&(a(t),a(n)),M(d,s)}}}function wn(v){let t,p,n,d,f,s='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColorF=white"/></div>',c,U,Ie="<em>This model was released on 2022-07-27 and added to Hugging Face Transformers on 2022-09-14.</em>",D,P,Re,K,Ft=`GPT-NeoX-Japanese, a Japanese language model based on <a href="./gpt_neox">GPT-NeoX</a>.
Japanese uses three types of characters (hiragana, katakana, kanji) and has a huge vocabulary. This model uses <a href="https://github.com/tanreinama/Japanese-BPEEncoder_V2" rel="nofollow">BPEEncoder V2</a>, a sub-word tokenizer to handle the different characters.`,We,ee,Pt="The model also removes some bias parameters for better performance.",Ee,te,It='You can find all the original GPT-NeoX-Japanese checkpoints under the <a href="https://huggingface.co/abeja/models?search=gpt-neo-x" rel="nofollow">ABEJA</a> organization.',Be,E,Le,ne,Zt='The example below demonstrates how to generate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> or the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',qe,B,Qe,oe,Rt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',He,ae,Wt='The example below uses <a href="../quantization/bitsandbytes">bitsandbytes</a> to only quantize the weights to 4-bits.',Se,se,Oe,re,Et='Use the <a href="https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139" rel="nofollow">AttentionMaskVisualizer</a> to better understand what tokens the model can and cannot attend to.',Ye,le,Ae,L,Bt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/gpt_neox_japanese-attn-mask.png"/>',De,ie,Ke,de,Lt='Refer to the <a href="https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4" rel="nofollow">Training a better GPT model: Learnings from PaLM</a> blog post for more details about how ABEJA trained GPT-NeoX-Japanese.',et,ce,tt,j,pe,mt,Ue,qt=`This is the configuration class to store the configuration of a <code>GPTNeoXModelJapanese</code>. It is used to instantiate
a GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeoXJapanese
<a href="https://huggingface.co/abeja/gpt-neox-japanese-2.7b" rel="nofollow">abeja/gpt-neox-japanese-2.7b</a> architecture.`,ut,we,Qt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information. Default configs is set as 2.7B model`,ht,q,nt,me,ot,w,ue,ft,ke,Ht=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> and is based on Japanese special Sub-Word-Encoding that is
used in this repository (<a href="https://github.com/tanreinama/Japanese-BPEEncoder_V2" rel="nofollow">https://github.com/tanreinama/Japanese-BPEEncoder_V2</a>). Check the repository for details.
Japanese has a relatively large vocabulary and there is no separation between words. Furthermore, the language is a
combination of hiragana, katakana, and kanji, and variants such as ‚Äú1‚Äù and ‚Äú‚ë†‚Äù are often used. In order to cope
with these, this tokenizer has the following features`,gt,$e,St=`<li>Subword-by-subword segmentation, which is intermediate between byte strings and morphological analysis.</li> <li>BPEs are created for each Kanji, Hiragana, and Katakana character, and there are no BPEs that cross character
types, such as Kanji + Hiragana or Hiragana + Katakana.</li> <li>All-byte encoding that does not require &lt;unk&gt;.</li> <li>Independent of UTF codes such as 2-byte and 3-byte characters</li> <li>Conversion of heterographs to the same token_id</li> <li>Emoji and Emoticon are grouped into 12 types as special tags.</li>`,_t,Q,Tt,H,he,bt,je,Ot="Converts a sequence of tokens (string) in a single string.",at,fe,st,k,ge,yt,xe,Yt="The bare Gpt Neox Japanese Model outputting raw hidden-states without any specific head on top.",Mt,Ce,At=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Jt,Ne,Dt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,vt,X,_e,Ut,Xe,Kt='The <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> forward method, overrides the <code>__call__</code> special method.',wt,S,kt,O,rt,Te,lt,$,be,$t,Ve,en="GPTNeoXJapanese Model with a <code>language modeling</code> head on top for Classifier Model fine-tuning.",jt,Ge,tn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xt,ze,nn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ct,V,ye,Nt,Fe,on='The <a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',Xt,Y,Vt,A,it,Me,dt,Ze,ct;return P=new Pe({props:{title:"GPT-NeoX-Japanese",local:"gpt-neox-japanese",headingTag:"h1"}}),E=new Gt({props:{warning:!1,$$slots:{default:[hn]},$$scope:{ctx:v}}}),B=new un({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Tn]},$$scope:{ctx:v}}}),se=new W({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyglMEElMjAlMjAlMjAlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYm5iXzRiaXRfdXNlX2RvdWJsZV9xdWFudCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBibmJfNGJpdF9xdWFudF90eXBlJTNEJTIybmY0JTIyJTJDJTBBJTIwJTIwJTIwJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRCUyMmZsb2F0MTYlMjIlMEEpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUwQSklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJhYmVqYSUyRmdwdC1uZW94LWphcGFuZXNlLTIuN2IlMjIpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyLmVuY29kZSglMjIlRTQlQkElQkElRTMlODElQThBSSVFMyU4MSU4QyVFNSU4RCU5NCVFOCVBQSVCRiVFMyU4MSU5OSVFMyU4MiU4QiVFMyU4MSU5RiVFMyU4MiU4MSVFMyU4MSVBQiVFMyU4MSVBRiVFMyU4MCU4MSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEFvdXRwdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dF9pZHMpJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
    bnb_4bit_compute_dtype=<span class="hljs-string">&quot;float16&quot;</span>
)
model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>,
    quantization_config=quantization_config,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
input_ids = tokenizer.encode(<span class="hljs-string">&quot;‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
output = model.generate(input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),le=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy51dGlscy5hdHRlbnRpb25fdmlzdWFsaXplciUyMGltcG9ydCUyMEF0dGVudGlvbk1hc2tWaXN1YWxpemVyJTBBJTBBdmlzdWFsaXplciUyMCUzRCUyMEF0dGVudGlvbk1hc2tWaXN1YWxpemVyKCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiklMEF2aXN1YWxpemVyKCUyMiUzQ2ltZyUzRVdoYXQlMjBpcyUyMHNob3duJTIwaW4lMjB0aGlzJTIwaW1hZ2UlM0YlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers.utils.attention_visualizer <span class="hljs-keyword">import</span> AttentionMaskVisualizer

visualizer = AttentionMaskVisualizer(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
visualizer(<span class="hljs-string">&quot;&lt;img&gt;What is shown in this image?&quot;</span>)`,wrap:!1}}),ie=new Pe({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ce=new Pe({props:{title:"GPTNeoXJapaneseConfig",local:"transformers.GPTNeoXJapaneseConfig",headingTag:"h2"}}),pe=new ve({props:{name:"class transformers.GPTNeoXJapaneseConfig",anchor:"transformers.GPTNeoXJapaneseConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 2560"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"intermediate_multiple_size",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"rotary_pct",val:" = 1.0"},{name:"rotary_emb_base",val:" = 10000"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 31996"},{name:"eos_token_id",val:" = 31999"},{name:"rope_scaling",val:" = None"},{name:"attention_dropout",val:" = 0.1"},{name:"hidden_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the GPTNeoXJapanese model. Defines the number of different tokens that can be
represented by the <code>inputs_ids</code> passed when calling <code>GPTNeoXJapanese</code>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2560) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GPTNeoXJapaneseConfig.intermediate_multiple_size",description:`<strong>intermediate_multiple_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; layer in the Transformer encoder is calculated by hidden_size *
intermediate_multiple_size.`,name:"intermediate_multiple_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_pct",description:`<strong>rotary_pct</strong> (<code>float</code>, <em>optional</em>, defaults to 1.00) &#x2014;
percentage of hidden dimensions to allocate to rotary embeddings`,name:"rotary_pct"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_emb_base",description:`<strong>rotary_emb_base</strong> (<code>int</code>, <em>optional</em>, defaults to 10000) &#x2014;
base for computing rotary embeddings frequency`,name:"rotary_emb_base"},{anchor:"transformers.GPTNeoXJapaneseConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with.`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoXJapaneseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoXJapaneseConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GPTNeoXJapaneseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.GPTNeoXJapaneseConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
and you expect the model to work on longer <code>max_position_embeddings</code>, we recommend you to update this value
accordingly.
Expected contents:
<code>rope_type</code> (<code>str</code>):
The sub-variant of RoPE to use. Can be one of [&#x2018;default&#x2019;, &#x2018;linear&#x2019;, &#x2018;dynamic&#x2019;, &#x2018;yarn&#x2019;, &#x2018;longrope&#x2019;,
&#x2018;llama3&#x2019;], with &#x2018;default&#x2019; being the original RoPE implementation.
<code>factor</code> (<code>float</code>, <em>optional</em>):
Used with all rope types except &#x2018;default&#x2019;. The scaling factor to apply to the RoPE embeddings. In
most scaling types, a <code>factor</code> of x will enable the model to handle sequences of length x <em>
original maximum pre-trained length.
<code>original_max_position_embeddings</code> (<code>int</code>, </em>optional<em>):
Used with &#x2018;dynamic&#x2019;, &#x2018;longrope&#x2019; and &#x2018;llama3&#x2019;. The original max position embeddings used during
pretraining.
<code>attention_factor</code> (<code>float</code>, </em>optional<em>):
Used with &#x2018;yarn&#x2019; and &#x2018;longrope&#x2019;. The scaling factor to be applied on the attention
computation. If unspecified, it defaults to value recommended by the implementation, using the
<code>factor</code> field to infer the suggested value.
<code>beta_fast</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;yarn&#x2019;. Parameter to set the boundary for extrapolation (only) in the linear
ramp function. If unspecified, it defaults to 32.
<code>beta_slow</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;yarn&#x2019;. Parameter to set the boundary for interpolation (only) in the linear
ramp function. If unspecified, it defaults to 1.
<code>short_factor</code> (<code>list[float]</code>, </em>optional<em>):
Only used with &#x2018;longrope&#x2019;. The scaling factor to be applied to short contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>long_factor</code> (<code>list[float]</code>, </em>optional<em>):
Only used with &#x2018;longrope&#x2019;. The scaling factor to be applied to long contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>low_freq_factor</code> (<code>float</code>, </em>optional<em>):
Only used with &#x2018;llama3&#x2019;. Scaling factor applied to low frequency components of the RoPE
<code>high_freq_factor</code> (<code>float</code>, </em>optional*):
Only used with &#x2018;llama3&#x2019;. Scaling factor applied to high frequency components of the RoPE`,name:"rope_scaling"},{anchor:"transformers.GPTNeoXJapaneseConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the hidden layer.`,name:"hidden_dropout"},{anchor:"transformers.GPTNeoXJapaneseConfig.Example",description:"<strong>Example</strong> &#x2014;",name:"Example"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py#L25"}}),q=new pt({props:{anchor:"transformers.GPTNeoXJapaneseConfig.example",$$slots:{default:[bn]},$$scope:{ctx:v}}}),me=new Pe({props:{title:"GPTNeoXJapaneseTokenizer",local:"transformers.GPTNeoXJapaneseTokenizer",headingTag:"h2"}}),ue=new ve({props:{name:"class transformers.GPTNeoXJapaneseTokenizer",anchor:"transformers.GPTNeoXJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"emoji_file",val:""},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"pad_token",val:" = '<|endoftext|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"do_clean_text",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.emoji_file",description:`<strong>emoji_file</strong> (<code>str</code>) &#x2014;
File containing the emoji.`,name:"emoji_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The token used for padding`,name:"pad_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.do_clean_text",description:`<strong>do_clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.`,name:"do_clean_text"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L55"}}),Q=new pt({props:{anchor:"transformers.GPTNeoXJapaneseTokenizer.example",$$slots:{default:[yn]},$$scope:{ctx:v}}}),he=new ve({props:{name:"convert_tokens_to_string",anchor:"transformers.GPTNeoXJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L160"}}),fe=new Pe({props:{title:"GPTNeoXJapaneseModel",local:"transformers.GPTNeoXJapaneseModel",headingTag:"h2"}}),ge=new ve({props:{name:"class transformers.GPTNeoXJapaneseModel",anchor:"transformers.GPTNeoXJapaneseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L388"}}),_e=new ve({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Union[transformers.cache_utils.Cache, tuple[tuple[torch.FloatTensor]], NoneType] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Union[~cache_utils.Cache, tuple[tuple[torch.FloatTensor]], NoneType]</code>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L409",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new Gt({props:{$$slots:{default:[Mn]},$$scope:{ctx:v}}}),O=new pt({props:{anchor:"transformers.GPTNeoXJapaneseModel.forward.example",$$slots:{default:[Jn]},$$scope:{ctx:v}}}),Te=new Pe({props:{title:"GPTNeoXJapaneseForCausalLM",local:"transformers.GPTNeoXJapaneseForCausalLM",headingTag:"h2"}}),be=new ve({props:{name:"class transformers.GPTNeoXJapaneseForCausalLM",anchor:"transformers.GPTNeoXJapaneseForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L652"}}),ye=new ve({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Union[transformers.cache_utils.Cache, tuple[tuple[torch.FloatTensor]], NoneType] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Union[~cache_utils.Cache, tuple[tuple[torch.FloatTensor]], NoneType]</code>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L671",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Y=new Gt({props:{$$slots:{default:[vn]},$$scope:{ctx:v}}}),A=new pt({props:{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.example",$$slots:{default:[Un]},$$scope:{ctx:v}}}),Me=new mn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox_japanese.md"}}),{c(){t=m("meta"),p=l(),n=m("p"),d=l(),f=m("div"),f.innerHTML=s,c=l(),U=m("p"),U.innerHTML=Ie,D=l(),g(P.$$.fragment),Re=l(),K=m("p"),K.innerHTML=Ft,We=l(),ee=m("p"),ee.textContent=Pt,Ee=l(),te=m("p"),te.innerHTML=It,Be=l(),g(E.$$.fragment),Le=l(),ne=m("p"),ne.innerHTML=Zt,qe=l(),g(B.$$.fragment),Qe=l(),oe=m("p"),oe.innerHTML=Rt,He=l(),ae=m("p"),ae.innerHTML=Wt,Se=l(),g(se.$$.fragment),Oe=l(),re=m("p"),re.innerHTML=Et,Ye=l(),g(le.$$.fragment),Ae=l(),L=m("div"),L.innerHTML=Bt,De=l(),g(ie.$$.fragment),Ke=l(),de=m("p"),de.innerHTML=Lt,et=l(),g(ce.$$.fragment),tt=l(),j=m("div"),g(pe.$$.fragment),mt=l(),Ue=m("p"),Ue.innerHTML=qt,ut=l(),we=m("p"),we.innerHTML=Qt,ht=l(),g(q.$$.fragment),nt=l(),g(me.$$.fragment),ot=l(),w=m("div"),g(ue.$$.fragment),ft=l(),ke=m("p"),ke.innerHTML=Ht,gt=l(),$e=m("ul"),$e.innerHTML=St,_t=l(),g(Q.$$.fragment),Tt=l(),H=m("div"),g(he.$$.fragment),bt=l(),je=m("p"),je.textContent=Ot,at=l(),g(fe.$$.fragment),st=l(),k=m("div"),g(ge.$$.fragment),yt=l(),xe=m("p"),xe.textContent=Yt,Mt=l(),Ce=m("p"),Ce.innerHTML=At,Jt=l(),Ne=m("p"),Ne.innerHTML=Dt,vt=l(),X=m("div"),g(_e.$$.fragment),Ut=l(),Xe=m("p"),Xe.innerHTML=Kt,wt=l(),g(S.$$.fragment),kt=l(),g(O.$$.fragment),rt=l(),g(Te.$$.fragment),lt=l(),$=m("div"),g(be.$$.fragment),$t=l(),Ve=m("p"),Ve.innerHTML=en,jt=l(),Ge=m("p"),Ge.innerHTML=tn,xt=l(),ze=m("p"),ze.innerHTML=nn,Ct=l(),V=m("div"),g(ye.$$.fragment),Nt=l(),Fe=m("p"),Fe.innerHTML=on,Xt=l(),g(Y.$$.fragment),Vt=l(),g(A.$$.fragment),it=l(),g(Me.$$.fragment),dt=l(),Ze=m("p"),this.h()},l(e){const o=cn("svelte-u9bgzb",document.head);t=u(o,"META",{name:!0,content:!0}),o.forEach(a),p=i(e),n=u(e,"P",{}),R(n).forEach(a),d=i(e),f=u(e,"DIV",{style:!0,"data-svelte-h":!0}),J(f)!=="svelte-to8ak1"&&(f.innerHTML=s),c=i(e),U=u(e,"P",{"data-svelte-h":!0}),J(U)!=="svelte-2kl3km"&&(U.innerHTML=Ie),D=i(e),_(P.$$.fragment,e),Re=i(e),K=u(e,"P",{"data-svelte-h":!0}),J(K)!=="svelte-8dtue"&&(K.innerHTML=Ft),We=i(e),ee=u(e,"P",{"data-svelte-h":!0}),J(ee)!=="svelte-12ne38i"&&(ee.textContent=Pt),Ee=i(e),te=u(e,"P",{"data-svelte-h":!0}),J(te)!=="svelte-1wua3c7"&&(te.innerHTML=It),Be=i(e),_(E.$$.fragment,e),Le=i(e),ne=u(e,"P",{"data-svelte-h":!0}),J(ne)!=="svelte-x9rs6r"&&(ne.innerHTML=Zt),qe=i(e),_(B.$$.fragment,e),Qe=i(e),oe=u(e,"P",{"data-svelte-h":!0}),J(oe)!=="svelte-nf5ooi"&&(oe.innerHTML=Rt),He=i(e),ae=u(e,"P",{"data-svelte-h":!0}),J(ae)!=="svelte-60nsd0"&&(ae.innerHTML=Wt),Se=i(e),_(se.$$.fragment,e),Oe=i(e),re=u(e,"P",{"data-svelte-h":!0}),J(re)!=="svelte-w3z5ks"&&(re.innerHTML=Et),Ye=i(e),_(le.$$.fragment,e),Ae=i(e),L=u(e,"DIV",{class:!0,"data-svelte-h":!0}),J(L)!=="svelte-zmxho4"&&(L.innerHTML=Bt),De=i(e),_(ie.$$.fragment,e),Ke=i(e),de=u(e,"P",{"data-svelte-h":!0}),J(de)!=="svelte-10cwthc"&&(de.innerHTML=Lt),et=i(e),_(ce.$$.fragment,e),tt=i(e),j=u(e,"DIV",{class:!0});var G=R(j);_(pe.$$.fragment,G),mt=i(G),Ue=u(G,"P",{"data-svelte-h":!0}),J(Ue)!=="svelte-6p1ri7"&&(Ue.innerHTML=qt),ut=i(G),we=u(G,"P",{"data-svelte-h":!0}),J(we)!=="svelte-1fysx6b"&&(we.innerHTML=Qt),ht=i(G),_(q.$$.fragment,G),G.forEach(a),nt=i(e),_(me.$$.fragment,e),ot=i(e),w=u(e,"DIV",{class:!0});var x=R(w);_(ue.$$.fragment,x),ft=i(x),ke=u(x,"P",{"data-svelte-h":!0}),J(ke)!=="svelte-imevdk"&&(ke.innerHTML=Ht),gt=i(x),$e=u(x,"UL",{"data-svelte-h":!0}),J($e)!=="svelte-9px4ih"&&($e.innerHTML=St),_t=i(x),_(Q.$$.fragment,x),Tt=i(x),H=u(x,"DIV",{class:!0});var Je=R(H);_(he.$$.fragment,Je),bt=i(Je),je=u(Je,"P",{"data-svelte-h":!0}),J(je)!=="svelte-b3k2yi"&&(je.textContent=Ot),Je.forEach(a),x.forEach(a),at=i(e),_(fe.$$.fragment,e),st=i(e),k=u(e,"DIV",{class:!0});var C=R(k);_(ge.$$.fragment,C),yt=i(C),xe=u(C,"P",{"data-svelte-h":!0}),J(xe)!=="svelte-1cum3sy"&&(xe.textContent=Yt),Mt=i(C),Ce=u(C,"P",{"data-svelte-h":!0}),J(Ce)!=="svelte-q52n56"&&(Ce.innerHTML=At),Jt=i(C),Ne=u(C,"P",{"data-svelte-h":!0}),J(Ne)!=="svelte-hswkmf"&&(Ne.innerHTML=Dt),vt=i(C),X=u(C,"DIV",{class:!0});var z=R(X);_(_e.$$.fragment,z),Ut=i(z),Xe=u(z,"P",{"data-svelte-h":!0}),J(Xe)!=="svelte-7d8npr"&&(Xe.innerHTML=Kt),wt=i(z),_(S.$$.fragment,z),kt=i(z),_(O.$$.fragment,z),z.forEach(a),C.forEach(a),rt=i(e),_(Te.$$.fragment,e),lt=i(e),$=u(e,"DIV",{class:!0});var N=R($);_(be.$$.fragment,N),$t=i(N),Ve=u(N,"P",{"data-svelte-h":!0}),J(Ve)!=="svelte-lpe5g3"&&(Ve.innerHTML=en),jt=i(N),Ge=u(N,"P",{"data-svelte-h":!0}),J(Ge)!=="svelte-q52n56"&&(Ge.innerHTML=tn),xt=i(N),ze=u(N,"P",{"data-svelte-h":!0}),J(ze)!=="svelte-hswkmf"&&(ze.innerHTML=nn),Ct=i(N),V=u(N,"DIV",{class:!0});var F=R(V);_(ye.$$.fragment,F),Nt=i(F),Fe=u(F,"P",{"data-svelte-h":!0}),J(Fe)!=="svelte-11czcbn"&&(Fe.innerHTML=on),Xt=i(F),_(Y.$$.fragment,F),Vt=i(F),_(A.$$.fragment,F),F.forEach(a),N.forEach(a),it=i(e),_(Me.$$.fragment,e),dt=i(e),Ze=u(e,"P",{}),R(Ze).forEach(a),this.h()},h(){I(t,"name","hf:doc:metadata"),I(t,"content",kn),pn(f,"float","right"),I(L,"class","flex justify-center"),I(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){h(document.head,t),r(e,p,o),r(e,n,o),r(e,d,o),r(e,f,o),r(e,c,o),r(e,U,o),r(e,D,o),T(P,e,o),r(e,Re,o),r(e,K,o),r(e,We,o),r(e,ee,o),r(e,Ee,o),r(e,te,o),r(e,Be,o),T(E,e,o),r(e,Le,o),r(e,ne,o),r(e,qe,o),T(B,e,o),r(e,Qe,o),r(e,oe,o),r(e,He,o),r(e,ae,o),r(e,Se,o),T(se,e,o),r(e,Oe,o),r(e,re,o),r(e,Ye,o),T(le,e,o),r(e,Ae,o),r(e,L,o),r(e,De,o),T(ie,e,o),r(e,Ke,o),r(e,de,o),r(e,et,o),T(ce,e,o),r(e,tt,o),r(e,j,o),T(pe,j,null),h(j,mt),h(j,Ue),h(j,ut),h(j,we),h(j,ht),T(q,j,null),r(e,nt,o),T(me,e,o),r(e,ot,o),r(e,w,o),T(ue,w,null),h(w,ft),h(w,ke),h(w,gt),h(w,$e),h(w,_t),T(Q,w,null),h(w,Tt),h(w,H),T(he,H,null),h(H,bt),h(H,je),r(e,at,o),T(fe,e,o),r(e,st,o),r(e,k,o),T(ge,k,null),h(k,yt),h(k,xe),h(k,Mt),h(k,Ce),h(k,Jt),h(k,Ne),h(k,vt),h(k,X),T(_e,X,null),h(X,Ut),h(X,Xe),h(X,wt),T(S,X,null),h(X,kt),T(O,X,null),r(e,rt,o),T(Te,e,o),r(e,lt,o),r(e,$,o),T(be,$,null),h($,$t),h($,Ve),h($,jt),h($,Ge),h($,xt),h($,ze),h($,Ct),h($,V),T(ye,V,null),h(V,Nt),h(V,Fe),h(V,Xt),T(Y,V,null),h(V,Vt),T(A,V,null),r(e,it,o),T(Me,e,o),r(e,dt,o),r(e,Ze,o),ct=!0},p(e,[o]){const G={};o&2&&(G.$$scope={dirty:o,ctx:e}),E.$set(G);const x={};o&2&&(x.$$scope={dirty:o,ctx:e}),B.$set(x);const Je={};o&2&&(Je.$$scope={dirty:o,ctx:e}),q.$set(Je);const C={};o&2&&(C.$$scope={dirty:o,ctx:e}),Q.$set(C);const z={};o&2&&(z.$$scope={dirty:o,ctx:e}),S.$set(z);const N={};o&2&&(N.$$scope={dirty:o,ctx:e}),O.$set(N);const F={};o&2&&(F.$$scope={dirty:o,ctx:e}),Y.$set(F);const an={};o&2&&(an.$$scope={dirty:o,ctx:e}),A.$set(an)},i(e){ct||(b(P.$$.fragment,e),b(E.$$.fragment,e),b(B.$$.fragment,e),b(se.$$.fragment,e),b(le.$$.fragment,e),b(ie.$$.fragment,e),b(ce.$$.fragment,e),b(pe.$$.fragment,e),b(q.$$.fragment,e),b(me.$$.fragment,e),b(ue.$$.fragment,e),b(Q.$$.fragment,e),b(he.$$.fragment,e),b(fe.$$.fragment,e),b(ge.$$.fragment,e),b(_e.$$.fragment,e),b(S.$$.fragment,e),b(O.$$.fragment,e),b(Te.$$.fragment,e),b(be.$$.fragment,e),b(ye.$$.fragment,e),b(Y.$$.fragment,e),b(A.$$.fragment,e),b(Me.$$.fragment,e),ct=!0)},o(e){y(P.$$.fragment,e),y(E.$$.fragment,e),y(B.$$.fragment,e),y(se.$$.fragment,e),y(le.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(pe.$$.fragment,e),y(q.$$.fragment,e),y(me.$$.fragment,e),y(ue.$$.fragment,e),y(Q.$$.fragment,e),y(he.$$.fragment,e),y(fe.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(S.$$.fragment,e),y(O.$$.fragment,e),y(Te.$$.fragment,e),y(be.$$.fragment,e),y(ye.$$.fragment,e),y(Y.$$.fragment,e),y(A.$$.fragment,e),y(Me.$$.fragment,e),ct=!1},d(e){e&&(a(p),a(n),a(d),a(f),a(c),a(U),a(D),a(Re),a(K),a(We),a(ee),a(Ee),a(te),a(Be),a(Le),a(ne),a(qe),a(Qe),a(oe),a(He),a(ae),a(Se),a(Oe),a(re),a(Ye),a(Ae),a(L),a(De),a(Ke),a(de),a(et),a(tt),a(j),a(nt),a(ot),a(w),a(at),a(st),a(k),a(rt),a(lt),a($),a(it),a(dt),a(Ze)),a(t),M(P,e),M(E,e),M(B,e),M(se,e),M(le,e),M(ie,e),M(ce,e),M(pe),M(q),M(me,e),M(ue),M(Q),M(he),M(fe,e),M(ge),M(_e),M(S),M(O),M(Te,e),M(be),M(ye),M(Y),M(A),M(Me,e)}}}const kn='{"title":"GPT-NeoX-Japanese","local":"gpt-neox-japanese","sections":[{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"GPTNeoXJapaneseConfig","local":"transformers.GPTNeoXJapaneseConfig","sections":[],"depth":2},{"title":"GPTNeoXJapaneseTokenizer","local":"transformers.GPTNeoXJapaneseTokenizer","sections":[],"depth":2},{"title":"GPTNeoXJapaneseModel","local":"transformers.GPTNeoXJapaneseModel","sections":[],"depth":2},{"title":"GPTNeoXJapaneseForCausalLM","local":"transformers.GPTNeoXJapaneseForCausalLM","sections":[],"depth":2}],"depth":1}';function $n(v){return rn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fn extends ln{constructor(t){super(),dn(this,t,$n,wn,sn,{})}}export{Fn as component};
