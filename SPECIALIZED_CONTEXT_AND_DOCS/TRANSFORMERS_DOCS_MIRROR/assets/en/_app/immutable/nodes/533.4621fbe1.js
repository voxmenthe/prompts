import{s as ma,o as ca,n as oa}from"../chunks/scheduler.18a86fab.js";import{S as ra,i as da,g as i,s as l,r as c,A as Ma,h as p,f as s,c as n,j as pa,u as r,x as o,k as x,y as ha,a,v as d,d as M,t as h,w as g,m as ga,n as ua}from"../chunks/index.98837b22.js";import{T as rs}from"../chunks/Tip.77304350.js";import{C as f}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as ya}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as w,E as fa}from"../chunks/getInferenceSnippets.06c2775f.js";function wa(J){let m;return{c(){m=ga("To run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.")},l(y){m=ua(y,"To run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.")},m(y,u){a(y,m,u)},d(y){y&&s(m)}}}function Ja(J){let m,y=`It is a good idea to include the <code>bad_words_ids</code> in the call to <code>generate</code> to avoid errors arising when increasing
the <code>max_new_tokens</code>: the model will want to generate a new <code>&lt;image&gt;</code> or <code>&lt;fake_token_around_image&gt;</code> token when there
is no image being generated by the model.
You can set it on-the-fly as in this guide, or store in the <code>GenerationConfig</code> as described in the <a href="../generation_strategies">Text generation strategies</a> guide.`;return{c(){m=i("p"),m.innerHTML=y},l(u){m=p(u,"P",{"data-svelte-h":!0}),o(m)!=="svelte-uwc51d"&&(m.innerHTML=y)},m(u,j){a(u,m,j)},p:oa,d(u){u&&s(m)}}}function ja(J){let m,y=`For longer outputs like this, you will greatly benefit from tweaking the text generation strategy. This can help
you significantly improve the quality of the generated output. Check out <a href="../generation_strategies">Text generation strategies</a>
to learn more.`;return{c(){m=i("p"),m.innerHTML=y},l(u){m=p(u,"P",{"data-svelte-h":!0}),o(m)!=="svelte-79vr3a"&&(m.innerHTML=y)},m(u,j){a(u,m,j)},p:oa,d(u){u&&s(m)}}}function Ta(J){let m,y,u,j,N,$t,B,Yt,_,ds=`While individual tasks can be tackled by fine-tuning specialized models, an alternative approach
that has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning.
For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more.
This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can
solve image-text tasks with a large multimodal model called IDEFICS.`,St,G,Ms=`<a href="../model_doc/idefics">IDEFICS</a> is an open-access vision and language model based on <a href="https://huggingface.co/papers/2204.14198" rel="nofollow">Flamingo</a>,
a state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image
and text inputs and generates coherent text as output. It can answer questions about images, describe visual content,
create stories grounded in multiple images, and so on. IDEFICS comes in two variants - <a href="https://huggingface.co/HuggingFaceM4/idefics-80b" rel="nofollow">80 billion parameters</a>
and <a href="https://huggingface.co/HuggingFaceM4/idefics-9b" rel="nofollow">9 billion parameters</a>, both of which are available on the ü§ó Hub. For each variant, you can also find fine-tuned instructed
versions of the model adapted for conversational use cases.`,qt,W,hs=`This model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However,
being a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether
this approach suits your use case better than fine-tuning specialized models for each individual task.`,At,z,gs="In this guide, you‚Äôll learn how to:",Dt,X,us='<li><a href="#loading-the-model">Load IDEFICS</a> and <a href="#quantized-model">load the quantized version of the model</a></li> <li>Use IDEFICS for: <ul><li><a href="#image-captioning">Image captioning</a></li> <li><a href="#prompted-image-captioning">Prompted image captioning</a></li> <li><a href="#few-shot-prompting">Few-shot prompting</a></li> <li><a href="#visual-question-answering">Visual question answering</a></li> <li><a href="#image-classification">Image classification</a></li> <li><a href="#image-guided-text-generation">Image-guided text generation</a></li></ul></li> <li><a href="#running-inference-in-batch-mode">Run inference in batch mode</a></li> <li><a href="#idefics-instruct-for-conversational-use">Run IDEFICS instruct for conversational use</a></li>',Lt,V,ys="Before you begin, make sure you have all the necessary libraries installed.",Pt,H,Ot,T,Kt,F,te,Q,fs="Let‚Äôs start by loading the model‚Äôs 9 billion parameters checkpoint:",ee,E,se,$,ws=`Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint.
The IDEFICS processor wraps a <a href="/docs/transformers/v4.56.2/en/model_doc/llama#transformers.LlamaTokenizer">LlamaTokenizer</a> and IDEFICS image processor into a single processor to take care of
preparing text and image inputs for the model.`,ae,Y,le,S,Js=`Setting <code>device_map</code> to <code>&quot;auto&quot;</code> will automatically determine how to load and store the model weights in the most optimized
manner given existing devices.`,ne,q,ie,A,js=`If high-memory device availability is an issue, you can load the quantized version of the model. To load the model and the
processor in 4bit precision, pass a <code>BitsAndBytesConfig</code> to the <code>from_pretrained</code> method and the model will be compressed
on the fly while loading.`,pe,D,oe,L,Ts="Now that you have the model loaded in one of the suggested ways, let‚Äôs move on to exploring tasks that you can use IDEFICS for.",me,P,ce,O,bs=`Image captioning is the task of predicting a caption for a given image. A common application is to aid visually impaired
people navigate through different situations, for instance, explore image content online.`,re,K,Us="To illustrate the task, get an image to be captioned, e.g.:",de,b,Zs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-im-captioning.jpg" alt="Image of a puppy in a flower bed"/>',Me,tt,Is='Photo by <a href="https://unsplash.com/@hendoo" rel="nofollow">Hendo Wang</a>.',he,et,ks=`IDEFICS accepts text and image prompts. However, to caption an image, you do not have to provide a text prompt to the
model, only the preprocessed input image. Without a text prompt, the model will start generating text from the
BOS (beginning-of-sequence) token thus creating a caption.`,ge,st,Cs="As image input to the model, you can use either an image object (<code>PIL.Image</code>) or a url from which the image can be retrieved.",ue,at,ye,U,fe,lt,we,nt,Rs=`You can extend image captioning by providing a text prompt, which the model will continue given the image. Let‚Äôs take
another image to illustrate:`,Je,Z,vs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-prompted-im-captioning.jpg" alt="Image of the Eiffel Tower at night"/>',je,it,xs='Photo by <a href="https://unsplash.com/@dnevozhai" rel="nofollow">Denys Nevozhai</a>.',Te,pt,Ns="Textual and image prompts can be passed to the model‚Äôs processor as a single list to create appropriate inputs.",be,ot,Ue,mt,Ze,ct,Bs=`While IDEFICS demonstrates great zero-shot results, your task may require a certain format of the caption, or come with
other restrictions or requirements that increase task‚Äôs complexity. Few-shot prompting can be used to enable in-context learning.
By providing examples in the prompt, you can steer the model to generate results that mimic the format of given examples.`,Ie,rt,_s=`Let‚Äôs use the previous image of the Eiffel Tower as an example for the model and build a prompt that demonstrates to the model
that in addition to learning what the object in an image is, we would also like to get some interesting information about it.
Then, let‚Äôs see, if we can get the same response format for an image of the Statue of Liberty:`,ke,I,Gs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg" alt="Image of the Statue of Liberty"/>',Ce,dt,Ws='Photo by <a href="https://unsplash.com/@jmayobres" rel="nofollow">Juan Mayobre</a>.',Re,Mt,ve,ht,zs=`Notice that just from a single example (i.e., 1-shot) the model has learned how to perform the task. For more complex tasks,
feel free to experiment with a larger number of examples (e.g., 3-shot, 5-shot, etc.).`,xe,gt,Ne,ut,Xs=`Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. Similar to image
captioning it can be used in accessibility applications, but also in education (reasoning about visual materials), customer
service (questions about products based on images), and image retrieval.`,Be,yt,Vs="Let‚Äôs get a new image for this task:",_e,k,Hs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-vqa.jpg" alt="Image of a couple having a picnic"/>',Ge,ft,Fs='Photo by <a href="https://unsplash.com/@jarritos" rel="nofollow">Jarritos Mexican Soda</a>.',We,wt,Qs="You can steer the model from image captioning to visual question answering by prompting it with appropriate instructions:",ze,Jt,Xe,jt,Ve,Tt,Es=`IDEFICS is capable of classifying images into different categories without being explicitly trained on data containing
labeled examples from those specific categories. Given a list of categories and using its image and text understanding
capabilities, the model can infer which category the image likely belongs to.`,He,bt,$s="Say, we have this image of a vegetable stand:",Fe,C,Ys='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-classification.jpg" alt="Image of a vegetable stand"/>',Qe,Ut,Ss='Photo by <a href="https://unsplash.com/@peterwendt" rel="nofollow">Peter Wendt</a>.',Ee,Zt,qs="We can instruct the model to classify the image into one of the categories that we have:",$e,It,Ye,kt,As="In the example above we instruct the model to classify the image into a single category, however, you can also prompt the model to do rank classification.",Se,Ct,qe,Rt,Ds=`For more creative applications, you can use image-guided text generation to generate text based on an image. This can be
useful to create descriptions of products, ads, descriptions of a scene, etc.`,Ae,vt,Ls="Let‚Äôs prompt IDEFICS to write a story based on a simple image of a red door:",De,R,Ps='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-story-generation.jpg" alt="Image of a red door with a pumpkin on the steps"/>',Le,xt,Os='Photo by <a href="https://unsplash.com/@devonshiremedia" rel="nofollow">Craig Tidball</a>.',Pe,Nt,Oe,Bt,Ks="Looks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story about a ghost.",Ke,v,ts,_t,es,Gt,ta=`All of the earlier sections illustrated IDEFICS for a single example. In a very similar fashion, you can run inference
for a batch of examples by passing a list of prompts:`,ss,Wt,as,zt,ls,Xt,ea=`For conversational use cases, you can find fine-tuned instructed versions of the model on the ü§ó Hub:
<code>HuggingFaceM4/idefics-80b-instruct</code> and <code>HuggingFaceM4/idefics-9b-instruct</code>.`,ns,Vt,sa=`These checkpoints are the result of fine-tuning the respective base models on a mixture of supervised and instruction
fine-tuning datasets, which boosts the downstream performance while making the models more usable in conversational settings.`,is,Ht,aa="The use and prompting for the conversational use is very similar to using the base models:",ps,Ft,os,Qt,ms,Et,cs;return N=new w({props:{title:"Image tasks with IDEFICS",local:"image-tasks-with-idefics",headingTag:"h1"}}),B=new ya({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/idefics.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/idefics.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/idefics.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/idefics.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/idefics.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/idefics.ipynb"}]}}),H=new f({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1xJTIwYml0c2FuZGJ5dGVzJTIwc2VudGVuY2VwaWVjZSUyMGFjY2VsZXJhdGUlMjB0cmFuc2Zvcm1lcnM=",highlighted:"pip install -q bitsandbytes sentencepiece accelerate transformers",wrap:!1}}),T=new rs({props:{$$slots:{default:[wa]},$$scope:{ctx:J}}}),F=new w({props:{title:"Loading the model",local:"loading-the-model",headingTag:"h2"}}),E=new f({props:{code:"Y2hlY2twb2ludCUyMCUzRCUyMCUyMkh1Z2dpbmdGYWNlTTQlMkZpZGVmaWNzLTliJTIy",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;HuggingFaceM4/idefics-9b&quot;</span>',wrap:!1}}),Y=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwSWRlZmljc0ZvclZpc2lvblRleHQyVGV4dCUyQyUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KSUwQSUwQW1vZGVsJTIwJTNEJTIwSWRlZmljc0ZvclZpc2lvblRleHQyVGV4dC5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCUyQyUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> IdeficsForVisionText2Text, AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),q=new w({props:{title:"Quantized model",local:"quantized-model",headingTag:"h3"}}),D=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwSWRlZmljc0ZvclZpc2lvblRleHQyVGV4dCUyQyUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCaXRzQW5kQnl0ZXNDb25maWclMEElMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwQml0c0FuZEJ5dGVzQ29uZmlnKCUwQSUyMCUyMCUyMCUyMGxvYWRfaW5fNGJpdCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBibmJfNGJpdF9jb21wdXRlX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUwQSklMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KSUwQSUwQW1vZGVsJTIwJTNEJTIwSWRlZmljc0ZvclZpc2lvblRleHQyVGV4dC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwY2hlY2twb2ludCUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>quantization_config = BitsAndBytesConfig(
<span class="hljs-meta">... </span>    load_in_4bit=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    bnb_4bit_compute_dtype=torch.float16,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = IdeficsForVisionText2Text.from_pretrained(
<span class="hljs-meta">... </span>    checkpoint,
<span class="hljs-meta">... </span>    quantization_config=quantization_config,
<span class="hljs-meta">... </span>    device_map=<span class="hljs-string">&quot;auto&quot;</span>
<span class="hljs-meta">... </span>)`,wrap:!1}}),P=new w({props:{title:"Image captioning",local:"image-captioning",headingTag:"h2"}}),at=new f({props:{code:"cHJvbXB0JTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIyaHR0cHMlM0ElMkYlMkZpbWFnZXMudW5zcGxhc2guY29tJTJGcGhvdG8tMTU4MzE2MDI0NzcxMS0yMTkxNzc2YjRiOTElM0ZpeGxpYiUzRHJiLTQuMC4zJTI2aXhpZCUzRE0zd3hNakEzZkRCOE1IeHdhRzkwYnkxd1lXZGxmSHg4ZkdWdWZEQjhmSHg4ZkElMjUzRCUyNTNEJTI2YXV0byUzRGZvcm1hdCUyNmZpdCUzRGNyb3AlMjZ3JTNEMzU0MiUyNnElM0Q4MCUyMiUyQyUwQSU1RCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBYmFkX3dvcmRzX2lkcyUyMCUzRCUyMHByb2Nlc3Nvci50b2tlbml6ZXIoJTVCJTIyJTNDaW1hZ2UlM0UlMjIlMkMlMjAlMjIlM0NmYWtlX3Rva2VuX2Fyb3VuZF9pbWFnZSUzRSUyMiU1RCUyQyUyMGFkZF9zcGVjaWFsX3Rva2VucyUzREZhbHNlKS5pbnB1dF9pZHMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwJTJDJTIwYmFkX3dvcmRzX2lkcyUzRGJhZF93b3Jkc19pZHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludChnZW5lcmF0ZWRfdGV4dCU1QjAlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3542&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">10</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])
A puppy <span class="hljs-keyword">in</span> a flower bed`,wrap:!1}}),U=new rs({props:{$$slots:{default:[Ja]},$$scope:{ctx:J}}}),lt=new w({props:{title:"Prompted image captioning",local:"prompted-image-captioning",headingTag:"h2"}}),ot=new f({props:{code:"cHJvbXB0JTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIyaHR0cHMlM0ElMkYlMkZpbWFnZXMudW5zcGxhc2guY29tJTJGcGhvdG8tMTU0MzM0OTY4OS05YTRkNDI2YmVlOGUlM0ZpeGxpYiUzRHJiLTQuMC4zJTI2aXhpZCUzRE0zd3hNakEzZkRCOE1IeHdhRzkwYnkxd1lXZGxmSHg4ZkdWdWZEQjhmSHg4ZkElMjUzRCUyNTNEJTI2YXV0byUzRGZvcm1hdCUyNmZpdCUzRGNyb3AlMjZ3JTNEMzUwMSUyNnElM0Q4MCUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMlRoaXMlMjBpcyUyMGFuJTIwaW1hZ2UlMjBvZiUyMCUyMiUyQyUwQSU1RCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBYmFkX3dvcmRzX2lkcyUyMCUzRCUyMHByb2Nlc3Nvci50b2tlbml6ZXIoJTVCJTIyJTNDaW1hZ2UlM0UlMjIlMkMlMjAlMjIlM0NmYWtlX3Rva2VuX2Fyb3VuZF9pbWFnZSUzRSUyMiU1RCUyQyUyMGFkZF9zcGVjaWFsX3Rva2VucyUzREZhbHNlKS5pbnB1dF9pZHMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwJTJDJTIwYmFkX3dvcmRzX2lkcyUzRGJhZF93b3Jkc19pZHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludChnZW5lcmF0ZWRfdGV4dCU1QjAlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3501&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;This is an image of &quot;</span>,
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">10</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])
This <span class="hljs-keyword">is</span> an image of the Eiffel Tower <span class="hljs-keyword">in</span> Paris, France.`,wrap:!1}}),mt=new w({props:{title:"Few-shot prompting",local:"few-shot-prompting",headingTag:"h2"}}),Mt=new f({props:{code:"cHJvbXB0JTIwJTNEJTIwJTVCJTIyVXNlciUzQSUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaW1hZ2VzLnVuc3BsYXNoLmNvbSUyRnBob3RvLTE1NDMzNDk2ODktOWE0ZDQyNmJlZThlJTNGaXhsaWIlM0RyYi00LjAuMyUyNml4aWQlM0RNM3d4TWpBM2ZEQjhNSHh3YUc5MGJ5MXdZV2RsZkh4OGZHVnVmREI4Zkh4OGZBJTI1M0QlMjUzRCUyNmF1dG8lM0Rmb3JtYXQlMjZmaXQlM0Rjcm9wJTI2dyUzRDM1MDElMjZxJTNEODAlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJEZXNjcmliZSUyMHRoaXMlMjBpbWFnZS4lNUNuQXNzaXN0YW50JTNBJTIwQW4lMjBpbWFnZSUyMG9mJTIwdGhlJTIwRWlmZmVsJTIwVG93ZXIlMjBhdCUyMG5pZ2h0LiUyMEZ1biUyMGZhY3QlM0ElMjB0aGUlMjBFaWZmZWwlMjBUb3dlciUyMGlzJTIwdGhlJTIwc2FtZSUyMGhlaWdodCUyMGFzJTIwYW4lMjA4MS1zdG9yZXklMjBidWlsZGluZy4lNUNuJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyVXNlciUzQSUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaW1hZ2VzLnVuc3BsYXNoLmNvbSUyRnBob3RvLTE1MjQwOTkxNjMyNTMtMzJiN2YwMjU2ODY4JTNGaXhsaWIlM0RyYi00LjAuMyUyNml4aWQlM0RNM3d4TWpBM2ZEQjhNSHh3YUc5MGJ5MXdZV2RsZkh4OGZHVnVmREI4Zkh4OGZBJTI1M0QlMjUzRCUyNmF1dG8lM0Rmb3JtYXQlMjZmaXQlM0Rjcm9wJTI2dyUzRDMzODclMjZxJTNEODAlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJEZXNjcmliZSUyMHRoaXMlMjBpbWFnZS4lNUNuQXNzaXN0YW50JTNBJTIyJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEFiYWRfd29yZHNfaWRzJTIwJTNEJTIwcHJvY2Vzc29yLnRva2VuaXplciglNUIlMjIlM0NpbWFnZSUzRSUyMiUyQyUyMCUyMiUzQ2Zha2VfdG9rZW5fYXJvdW5kX2ltYWdlJTNFJTIyJTVEJTJDJTIwYWRkX3NwZWNpYWxfdG9rZW5zJTNERmFsc2UpLmlucHV0X2lkcyUwQSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMzAlMkMlMjBiYWRfd29yZHNfaWRzJTNEYmFkX3dvcmRzX2lkcyklMEFnZW5lcmF0ZWRfdGV4dCUyMCUzRCUyMHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQXByaW50KGdlbmVyYXRlZF90ZXh0JTVCMCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [<span class="hljs-string">&quot;User:&quot;</span>,
<span class="hljs-meta">... </span>           <span class="hljs-string">&quot;https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3501&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>           <span class="hljs-string">&quot;Describe this image.\\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\\n&quot;</span>,
<span class="hljs-meta">... </span>           <span class="hljs-string">&quot;User:&quot;</span>,
<span class="hljs-meta">... </span>           <span class="hljs-string">&quot;https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3387&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>           <span class="hljs-string">&quot;Describe this image.\\nAssistant:&quot;</span>
<span class="hljs-meta">... </span>           ]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">30</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])
User: Describe this image.
Assistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower <span class="hljs-keyword">is</span> the same height <span class="hljs-keyword">as</span> an <span class="hljs-number">81</span>-storey building. 
User: Describe this image.
Assistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty <span class="hljs-keyword">is</span> <span class="hljs-number">151</span> feet tall.`,wrap:!1}}),gt=new w({props:{title:"Visual question answering",local:"visual-question-answering",headingTag:"h2"}}),Jt=new f({props:{code:"cHJvbXB0JTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIySW5zdHJ1Y3Rpb24lM0ElMjBQcm92aWRlJTIwYW4lMjBhbnN3ZXIlMjB0byUyMHRoZSUyMHF1ZXN0aW9uLiUyMFVzZSUyMHRoZSUyMGltYWdlJTIwdG8lMjBhbnN3ZXIuJTVDbiUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaW1hZ2VzLnVuc3BsYXNoLmNvbSUyRnBob3RvLTE2MjM5NDQ4ODkyODgtY2QxNDdkYmI1MTdjJTNGaXhsaWIlM0RyYi00LjAuMyUyNml4aWQlM0RNM3d4TWpBM2ZEQjhNSHh3YUc5MGJ5MXdZV2RsZkh4OGZHVnVmREI4Zkh4OGZBJTI1M0QlMjUzRCUyNmF1dG8lM0Rmb3JtYXQlMjZmaXQlM0Rjcm9wJTI2dyUzRDM1NDAlMjZxJTNEODAlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJRdWVzdGlvbiUzQSUyMFdoZXJlJTIwYXJlJTIwdGhlc2UlMjBwZW9wbGUlMjBhbmQlMjB3aGF0J3MlMjB0aGUlMjB3ZWF0aGVyJTIwbGlrZSUzRiUyMEFuc3dlciUzQSUyMiUwQSU1RCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBYmFkX3dvcmRzX2lkcyUyMCUzRCUyMHByb2Nlc3Nvci50b2tlbml6ZXIoJTVCJTIyJTNDaW1hZ2UlM0UlMjIlMkMlMjAlMjIlM0NmYWtlX3Rva2VuX2Fyb3VuZF9pbWFnZSUzRSUyMiU1RCUyQyUyMGFkZF9zcGVjaWFsX3Rva2VucyUzREZhbHNlKS5pbnB1dF9pZHMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDIwJTJDJTIwYmFkX3dvcmRzX2lkcyUzRGJhZF93b3Jkc19pZHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludChnZW5lcmF0ZWRfdGV4dCU1QjAlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Instruction: Provide an answer to the question. Use the image to answer.\\n&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3540&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Question: Where are these people and what&#x27;s the weather like? Answer:&quot;</span>
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">20</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])
Instruction: Provide an answer to the question. Use the image to answer.
 Question: Where are these people <span class="hljs-keyword">and</span> what<span class="hljs-string">&#x27;s the weather like? Answer: They&#x27;</span>re <span class="hljs-keyword">in</span> a park <span class="hljs-keyword">in</span> New York City, <span class="hljs-keyword">and</span> it<span class="hljs-string">&#x27;s a beautiful day.</span>`,wrap:!1}}),jt=new w({props:{title:"Image classification",local:"image-classification",headingTag:"h2"}}),It=new f({props:{code:"Y2F0ZWdvcmllcyUyMCUzRCUyMCU1QidhbmltYWxzJyUyQyd2ZWdldGFibGVzJyUyQyUyMCdjaXR5JTIwbGFuZHNjYXBlJyUyQyUyMCdjYXJzJyUyQyUyMCdvZmZpY2UnJTVEJTBBcHJvbXB0JTIwJTNEJTIwJTVCZiUyMkluc3RydWN0aW9uJTNBJTIwQ2xhc3NpZnklMjB0aGUlMjBmb2xsb3dpbmclMjBpbWFnZSUyMGludG8lMjBhJTIwc2luZ2xlJTIwY2F0ZWdvcnklMjBmcm9tJTIwdGhlJTIwZm9sbG93aW5nJTIwbGlzdCUzQSUyMCU3QmNhdGVnb3JpZXMlN0QuJTVDbiUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaW1hZ2VzLnVuc3BsYXNoLmNvbSUyRnBob3RvLTE0NzExOTM5NDU1MDktOWFkMDYxN2FmYWJmJTNGaXhsaWIlM0RyYi00LjAuMyUyNml4aWQlM0RNM3d4TWpBM2ZEQjhNSHh3YUc5MGJ5MXdZV2RsZkh4OGZHVnVmREI4Zkh4OGZBJTI1M0QlMjUzRCUyNmF1dG8lM0Rmb3JtYXQlMjZmaXQlM0Rjcm9wJTI2dyUzRDM1NDAlMjZxJTNEODAlMjIlMkMlMjAlMjAlMjAlMjAlMEElMjAlMjAlMjAlMjAlMjJDYXRlZ29yeSUzQSUyMCUyMiUwQSU1RCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBYmFkX3dvcmRzX2lkcyUyMCUzRCUyMHByb2Nlc3Nvci50b2tlbml6ZXIoJTVCJTIyJTNDaW1hZ2UlM0UlMjIlMkMlMjAlMjIlM0NmYWtlX3Rva2VuX2Fyb3VuZF9pbWFnZSUzRSUyMiU1RCUyQyUyMGFkZF9zcGVjaWFsX3Rva2VucyUzREZhbHNlKS5pbnB1dF9pZHMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDYlMkMlMjBiYWRfd29yZHNfaWRzJTNEYmFkX3dvcmRzX2lkcyklMEFnZW5lcmF0ZWRfdGV4dCUyMCUzRCUyMHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQXByaW50KGdlbmVyYXRlZF90ZXh0JTVCMCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>categories = [<span class="hljs-string">&#x27;animals&#x27;</span>,<span class="hljs-string">&#x27;vegetables&#x27;</span>, <span class="hljs-string">&#x27;city landscape&#x27;</span>, <span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;office&#x27;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [<span class="hljs-string">f&quot;Instruction: Classify the following image into a single category from the following list: <span class="hljs-subst">{categories}</span>.\\n&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3540&amp;q=80&quot;</span>,    
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Category: &quot;</span>
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">6</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>])
Instruction: Classify the following image into a single category <span class="hljs-keyword">from</span> the following <span class="hljs-built_in">list</span>: [<span class="hljs-string">&#x27;animals&#x27;</span>, <span class="hljs-string">&#x27;vegetables&#x27;</span>, <span class="hljs-string">&#x27;city landscape&#x27;</span>, <span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;office&#x27;</span>].
Category: Vegetables`,wrap:!1}}),Ct=new w({props:{title:"Image-guided text generation",local:"image-guided-text-generation",headingTag:"h2"}}),Nt=new f({props:{code:"cHJvbXB0JTIwJTNEJTIwJTVCJTIySW5zdHJ1Y3Rpb24lM0ElMjBVc2UlMjB0aGUlMjBpbWFnZSUyMHRvJTIwd3JpdGUlMjBhJTIwc3RvcnkuJTIwJTVDbiUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaW1hZ2VzLnVuc3BsYXNoLmNvbSUyRnBob3RvLTE1MTcwODY4MjIxNTctMmIwMzU4ZTc2ODRhJTNGaXhsaWIlM0RyYi00LjAuMyUyNml4aWQlM0RNM3d4TWpBM2ZEQjhNSHh3YUc5MGJ5MXdZV2RsZkh4OGZHVnVmREI4Zkh4OGZBJTI1M0QlMjUzRCUyNmF1dG8lM0Rmb3JtYXQlMjZmaXQlM0Rjcm9wJTI2dyUzRDIyMDMlMjZxJTNEODAlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJTdG9yeSUzQSUyMCU1Q24lMjIlNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IocHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQWJhZF93b3Jkc19pZHMlMjAlM0QlMjBwcm9jZXNzb3IudG9rZW5pemVyKCU1QiUyMiUzQ2ltYWdlJTNFJTIyJTJDJTIwJTIyJTNDZmFrZV90b2tlbl9hcm91bmRfaW1hZ2UlM0UlMjIlNUQlMkMlMjBhZGRfc3BlY2lhbF90b2tlbnMlM0RGYWxzZSkuaW5wdXRfaWRzJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwbnVtX2JlYW1zJTNEMiUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMjAwJTJDJTIwYmFkX3dvcmRzX2lkcyUzRGJhZF93b3Jkc19pZHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludChnZW5lcmF0ZWRfdGV4dCU1QjAlNUQpJTIwJTBBJTBBJTBBJTBBJTBBJTBBJTBBJTBB",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = [<span class="hljs-string">&quot;Instruction: Use the image to write a story. \\n&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2203&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Story: \\n&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, num_beams=<span class="hljs-number">2</span>, max_new_tokens=<span class="hljs-number">200</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text[<span class="hljs-number">0</span>]) 
Instruction: Use the image to write a story. 
 Story: 
Once upon a time, there was a little girl who lived <span class="hljs-keyword">in</span> a house <span class="hljs-keyword">with</span> a red door.  She loved her red door.  It was the prettiest door <span class="hljs-keyword">in</span> the whole world.

One day, the little girl was playing <span class="hljs-keyword">in</span> her yard when she noticed a man standing on her doorstep.  He was wearing a long black coat <span class="hljs-keyword">and</span> a top hat.

The little girl ran inside <span class="hljs-keyword">and</span> told her mother about the man.

Her mother said, ‚ÄúDon‚Äôt worry, honey.  He‚Äôs just a friendly ghost.‚Äù

The little girl wasn‚Äôt sure <span class="hljs-keyword">if</span> she believed her mother, but she went outside anyway.

When she got to the door, the man was gone.

The <span class="hljs-built_in">next</span> day, the little girl was playing <span class="hljs-keyword">in</span> her yard again when she noticed the man standing on her doorstep.

He was wearing a long black coat <span class="hljs-keyword">and</span> a top hat.

The little girl ran`,wrap:!1}}),v=new rs({props:{$$slots:{default:[ja]},$$scope:{ctx:J}}}),_t=new w({props:{title:"Running inference in batch mode",local:"running-inference-in-batch-mode",headingTag:"h2"}}),Wt=new f({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompts = [
<span class="hljs-meta">... </span>    [   <span class="hljs-string">&quot;https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3501&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;This is an image of &quot;</span>,
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>    [   <span class="hljs-string">&quot;https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3540&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;This is an image of &quot;</span>,
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>    [   <span class="hljs-string">&quot;https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=3540&amp;q=80&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;This is an image of &quot;</span>,
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">10</span>, bad_words_ids=bad_words_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i,t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(generated_text):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{i}</span>:\\n<span class="hljs-subst">{t}</span>\\n&quot;</span>) 
<span class="hljs-number">0</span>:
This <span class="hljs-keyword">is</span> an image of the Eiffel Tower <span class="hljs-keyword">in</span> Paris, France.

<span class="hljs-number">1</span>:
This <span class="hljs-keyword">is</span> an image of a couple on a picnic blanket.

<span class="hljs-number">2</span>:
This <span class="hljs-keyword">is</span> an image of a vegetable stand.`,wrap:!1}}),zt=new w({props:{title:"IDEFICS instruct for conversational use",local:"idefics-instruct-for-conversational-use",headingTag:"h2"}}),Ft=new f({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> IdeficsForVisionText2Text, AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;HuggingFaceM4/idefics-9b-instruct&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = IdeficsForVisionText2Text.from_pretrained(checkpoint, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompts = [
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;User: What is in this image?&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;&lt;end_of_utterance&gt;&quot;</span>,

<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.&lt;end_of_utterance&gt;&quot;</span>,

<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;\\nUser:&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052&quot;</span>,
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;And who is that?&lt;end_of_utterance&gt;&quot;</span>,

<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;\\nAssistant:&quot;</span>,
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># --batched mode</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(prompts, add_end_of_utterance_token=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># --single sample mode</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inputs = processor(prompts[0], return_tensors=&quot;pt&quot;).to(model.device)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generation args</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>exit_condition = processor.tokenizer(<span class="hljs-string">&quot;&lt;end_of_utterance&gt;&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>bad_words_ids = processor.tokenizer([<span class="hljs-string">&quot;&lt;image&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;fake_token_around_image&gt;&quot;</span>], add_special_tokens=<span class="hljs-literal">False</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(generated_text):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{i}</span>:\\n<span class="hljs-subst">{t}</span>\\n&quot;</span>)`,wrap:!1}}),Qt=new fa({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md"}}),{c(){m=i("meta"),y=l(),u=i("p"),j=l(),c(N.$$.fragment),$t=l(),c(B.$$.fragment),Yt=l(),_=i("p"),_.textContent=ds,St=l(),G=i("p"),G.innerHTML=Ms,qt=l(),W=i("p"),W.textContent=hs,At=l(),z=i("p"),z.textContent=gs,Dt=l(),X=i("ul"),X.innerHTML=us,Lt=l(),V=i("p"),V.textContent=ys,Pt=l(),c(H.$$.fragment),Ot=l(),c(T.$$.fragment),Kt=l(),c(F.$$.fragment),te=l(),Q=i("p"),Q.textContent=fs,ee=l(),c(E.$$.fragment),se=l(),$=i("p"),$.innerHTML=ws,ae=l(),c(Y.$$.fragment),le=l(),S=i("p"),S.innerHTML=Js,ne=l(),c(q.$$.fragment),ie=l(),A=i("p"),A.innerHTML=js,pe=l(),c(D.$$.fragment),oe=l(),L=i("p"),L.textContent=Ts,me=l(),c(P.$$.fragment),ce=l(),O=i("p"),O.textContent=bs,re=l(),K=i("p"),K.textContent=Us,de=l(),b=i("div"),b.innerHTML=Zs,Me=l(),tt=i("p"),tt.innerHTML=Is,he=l(),et=i("p"),et.textContent=ks,ge=l(),st=i("p"),st.innerHTML=Cs,ue=l(),c(at.$$.fragment),ye=l(),c(U.$$.fragment),fe=l(),c(lt.$$.fragment),we=l(),nt=i("p"),nt.textContent=Rs,Je=l(),Z=i("div"),Z.innerHTML=vs,je=l(),it=i("p"),it.innerHTML=xs,Te=l(),pt=i("p"),pt.textContent=Ns,be=l(),c(ot.$$.fragment),Ue=l(),c(mt.$$.fragment),Ze=l(),ct=i("p"),ct.textContent=Bs,Ie=l(),rt=i("p"),rt.textContent=_s,ke=l(),I=i("div"),I.innerHTML=Gs,Ce=l(),dt=i("p"),dt.innerHTML=Ws,Re=l(),c(Mt.$$.fragment),ve=l(),ht=i("p"),ht.textContent=zs,xe=l(),c(gt.$$.fragment),Ne=l(),ut=i("p"),ut.textContent=Xs,Be=l(),yt=i("p"),yt.textContent=Vs,_e=l(),k=i("div"),k.innerHTML=Hs,Ge=l(),ft=i("p"),ft.innerHTML=Fs,We=l(),wt=i("p"),wt.textContent=Qs,ze=l(),c(Jt.$$.fragment),Xe=l(),c(jt.$$.fragment),Ve=l(),Tt=i("p"),Tt.textContent=Es,He=l(),bt=i("p"),bt.textContent=$s,Fe=l(),C=i("div"),C.innerHTML=Ys,Qe=l(),Ut=i("p"),Ut.innerHTML=Ss,Ee=l(),Zt=i("p"),Zt.textContent=qs,$e=l(),c(It.$$.fragment),Ye=l(),kt=i("p"),kt.textContent=As,Se=l(),c(Ct.$$.fragment),qe=l(),Rt=i("p"),Rt.textContent=Ds,Ae=l(),vt=i("p"),vt.textContent=Ls,De=l(),R=i("div"),R.innerHTML=Ps,Le=l(),xt=i("p"),xt.innerHTML=Os,Pe=l(),c(Nt.$$.fragment),Oe=l(),Bt=i("p"),Bt.textContent=Ks,Ke=l(),c(v.$$.fragment),ts=l(),c(_t.$$.fragment),es=l(),Gt=i("p"),Gt.textContent=ta,ss=l(),c(Wt.$$.fragment),as=l(),c(zt.$$.fragment),ls=l(),Xt=i("p"),Xt.innerHTML=ea,ns=l(),Vt=i("p"),Vt.textContent=sa,is=l(),Ht=i("p"),Ht.textContent=aa,ps=l(),c(Ft.$$.fragment),os=l(),c(Qt.$$.fragment),ms=l(),Et=i("p"),this.h()},l(t){const e=Ma("svelte-u9bgzb",document.head);m=p(e,"META",{name:!0,content:!0}),e.forEach(s),y=n(t),u=p(t,"P",{}),pa(u).forEach(s),j=n(t),r(N.$$.fragment,t),$t=n(t),r(B.$$.fragment,t),Yt=n(t),_=p(t,"P",{"data-svelte-h":!0}),o(_)!=="svelte-11wnsr"&&(_.textContent=ds),St=n(t),G=p(t,"P",{"data-svelte-h":!0}),o(G)!=="svelte-1nt7nk6"&&(G.innerHTML=Ms),qt=n(t),W=p(t,"P",{"data-svelte-h":!0}),o(W)!=="svelte-1k5g1sg"&&(W.textContent=hs),At=n(t),z=p(t,"P",{"data-svelte-h":!0}),o(z)!=="svelte-fp1hdi"&&(z.textContent=gs),Dt=n(t),X=p(t,"UL",{"data-svelte-h":!0}),o(X)!=="svelte-lzrpc8"&&(X.innerHTML=us),Lt=n(t),V=p(t,"P",{"data-svelte-h":!0}),o(V)!=="svelte-qn4ey1"&&(V.textContent=ys),Pt=n(t),r(H.$$.fragment,t),Ot=n(t),r(T.$$.fragment,t),Kt=n(t),r(F.$$.fragment,t),te=n(t),Q=p(t,"P",{"data-svelte-h":!0}),o(Q)!=="svelte-pbsmkm"&&(Q.textContent=fs),ee=n(t),r(E.$$.fragment,t),se=n(t),$=p(t,"P",{"data-svelte-h":!0}),o($)!=="svelte-1hei291"&&($.innerHTML=ws),ae=n(t),r(Y.$$.fragment,t),le=n(t),S=p(t,"P",{"data-svelte-h":!0}),o(S)!=="svelte-10jwul3"&&(S.innerHTML=Js),ne=n(t),r(q.$$.fragment,t),ie=n(t),A=p(t,"P",{"data-svelte-h":!0}),o(A)!=="svelte-1r1m2bi"&&(A.innerHTML=js),pe=n(t),r(D.$$.fragment,t),oe=n(t),L=p(t,"P",{"data-svelte-h":!0}),o(L)!=="svelte-vovwl0"&&(L.textContent=Ts),me=n(t),r(P.$$.fragment,t),ce=n(t),O=p(t,"P",{"data-svelte-h":!0}),o(O)!=="svelte-195xyco"&&(O.textContent=bs),re=n(t),K=p(t,"P",{"data-svelte-h":!0}),o(K)!=="svelte-syqfn9"&&(K.textContent=Us),de=n(t),b=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(b)!=="svelte-t8y7db"&&(b.innerHTML=Zs),Me=n(t),tt=p(t,"P",{"data-svelte-h":!0}),o(tt)!=="svelte-knozkn"&&(tt.innerHTML=Is),he=n(t),et=p(t,"P",{"data-svelte-h":!0}),o(et)!=="svelte-10bztc1"&&(et.textContent=ks),ge=n(t),st=p(t,"P",{"data-svelte-h":!0}),o(st)!=="svelte-sk4o55"&&(st.innerHTML=Cs),ue=n(t),r(at.$$.fragment,t),ye=n(t),r(U.$$.fragment,t),fe=n(t),r(lt.$$.fragment,t),we=n(t),nt=p(t,"P",{"data-svelte-h":!0}),o(nt)!=="svelte-15l3yis"&&(nt.textContent=Rs),Je=n(t),Z=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(Z)!=="svelte-1ritb1k"&&(Z.innerHTML=vs),je=n(t),it=p(t,"P",{"data-svelte-h":!0}),o(it)!=="svelte-1uz68x8"&&(it.innerHTML=xs),Te=n(t),pt=p(t,"P",{"data-svelte-h":!0}),o(pt)!=="svelte-cncrxa"&&(pt.textContent=Ns),be=n(t),r(ot.$$.fragment,t),Ue=n(t),r(mt.$$.fragment,t),Ze=n(t),ct=p(t,"P",{"data-svelte-h":!0}),o(ct)!=="svelte-fytdtn"&&(ct.textContent=Bs),Ie=n(t),rt=p(t,"P",{"data-svelte-h":!0}),o(rt)!=="svelte-148v07a"&&(rt.textContent=_s),ke=n(t),I=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(I)!=="svelte-gin1vp"&&(I.innerHTML=Gs),Ce=n(t),dt=p(t,"P",{"data-svelte-h":!0}),o(dt)!=="svelte-s7b8dy"&&(dt.innerHTML=Ws),Re=n(t),r(Mt.$$.fragment,t),ve=n(t),ht=p(t,"P",{"data-svelte-h":!0}),o(ht)!=="svelte-mpygyg"&&(ht.textContent=zs),xe=n(t),r(gt.$$.fragment,t),Ne=n(t),ut=p(t,"P",{"data-svelte-h":!0}),o(ut)!=="svelte-cnfg69"&&(ut.textContent=Xs),Be=n(t),yt=p(t,"P",{"data-svelte-h":!0}),o(yt)!=="svelte-nptt59"&&(yt.textContent=Vs),_e=n(t),k=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(k)!=="svelte-1j2xr8e"&&(k.innerHTML=Hs),Ge=n(t),ft=p(t,"P",{"data-svelte-h":!0}),o(ft)!=="svelte-1aj82zb"&&(ft.innerHTML=Fs),We=n(t),wt=p(t,"P",{"data-svelte-h":!0}),o(wt)!=="svelte-tdtrto"&&(wt.textContent=Qs),ze=n(t),r(Jt.$$.fragment,t),Xe=n(t),r(jt.$$.fragment,t),Ve=n(t),Tt=p(t,"P",{"data-svelte-h":!0}),o(Tt)!=="svelte-a4cfdv"&&(Tt.textContent=Es),He=n(t),bt=p(t,"P",{"data-svelte-h":!0}),o(bt)!=="svelte-1xbkffx"&&(bt.textContent=$s),Fe=n(t),C=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(C)!=="svelte-g02ga3"&&(C.innerHTML=Ys),Qe=n(t),Ut=p(t,"P",{"data-svelte-h":!0}),o(Ut)!=="svelte-17q4ltv"&&(Ut.innerHTML=Ss),Ee=n(t),Zt=p(t,"P",{"data-svelte-h":!0}),o(Zt)!=="svelte-13lz1gw"&&(Zt.textContent=qs),$e=n(t),r(It.$$.fragment,t),Ye=n(t),kt=p(t,"P",{"data-svelte-h":!0}),o(kt)!=="svelte-571z8p"&&(kt.textContent=As),Se=n(t),r(Ct.$$.fragment,t),qe=n(t),Rt=p(t,"P",{"data-svelte-h":!0}),o(Rt)!=="svelte-1cpi6ml"&&(Rt.textContent=Ds),Ae=n(t),vt=p(t,"P",{"data-svelte-h":!0}),o(vt)!=="svelte-bxfm2h"&&(vt.textContent=Ls),De=n(t),R=p(t,"DIV",{class:!0,"data-svelte-h":!0}),o(R)!=="svelte-1mf93u3"&&(R.innerHTML=Ps),Le=n(t),xt=p(t,"P",{"data-svelte-h":!0}),o(xt)!=="svelte-75pbgi"&&(xt.innerHTML=Os),Pe=n(t),r(Nt.$$.fragment,t),Oe=n(t),Bt=p(t,"P",{"data-svelte-h":!0}),o(Bt)!=="svelte-eumo01"&&(Bt.textContent=Ks),Ke=n(t),r(v.$$.fragment,t),ts=n(t),r(_t.$$.fragment,t),es=n(t),Gt=p(t,"P",{"data-svelte-h":!0}),o(Gt)!=="svelte-2k4kpw"&&(Gt.textContent=ta),ss=n(t),r(Wt.$$.fragment,t),as=n(t),r(zt.$$.fragment,t),ls=n(t),Xt=p(t,"P",{"data-svelte-h":!0}),o(Xt)!=="svelte-ay5071"&&(Xt.innerHTML=ea),ns=n(t),Vt=p(t,"P",{"data-svelte-h":!0}),o(Vt)!=="svelte-8tbmhu"&&(Vt.textContent=sa),is=n(t),Ht=p(t,"P",{"data-svelte-h":!0}),o(Ht)!=="svelte-ccrnle"&&(Ht.textContent=aa),ps=n(t),r(Ft.$$.fragment,t),os=n(t),r(Qt.$$.fragment,t),ms=n(t),Et=p(t,"P",{}),pa(Et).forEach(s),this.h()},h(){x(m,"name","hf:doc:metadata"),x(m,"content",ba),x(b,"class","flex justify-center"),x(Z,"class","flex justify-center"),x(I,"class","flex justify-center"),x(k,"class","flex justify-center"),x(C,"class","flex justify-center"),x(R,"class","flex justify-center")},m(t,e){ha(document.head,m),a(t,y,e),a(t,u,e),a(t,j,e),d(N,t,e),a(t,$t,e),d(B,t,e),a(t,Yt,e),a(t,_,e),a(t,St,e),a(t,G,e),a(t,qt,e),a(t,W,e),a(t,At,e),a(t,z,e),a(t,Dt,e),a(t,X,e),a(t,Lt,e),a(t,V,e),a(t,Pt,e),d(H,t,e),a(t,Ot,e),d(T,t,e),a(t,Kt,e),d(F,t,e),a(t,te,e),a(t,Q,e),a(t,ee,e),d(E,t,e),a(t,se,e),a(t,$,e),a(t,ae,e),d(Y,t,e),a(t,le,e),a(t,S,e),a(t,ne,e),d(q,t,e),a(t,ie,e),a(t,A,e),a(t,pe,e),d(D,t,e),a(t,oe,e),a(t,L,e),a(t,me,e),d(P,t,e),a(t,ce,e),a(t,O,e),a(t,re,e),a(t,K,e),a(t,de,e),a(t,b,e),a(t,Me,e),a(t,tt,e),a(t,he,e),a(t,et,e),a(t,ge,e),a(t,st,e),a(t,ue,e),d(at,t,e),a(t,ye,e),d(U,t,e),a(t,fe,e),d(lt,t,e),a(t,we,e),a(t,nt,e),a(t,Je,e),a(t,Z,e),a(t,je,e),a(t,it,e),a(t,Te,e),a(t,pt,e),a(t,be,e),d(ot,t,e),a(t,Ue,e),d(mt,t,e),a(t,Ze,e),a(t,ct,e),a(t,Ie,e),a(t,rt,e),a(t,ke,e),a(t,I,e),a(t,Ce,e),a(t,dt,e),a(t,Re,e),d(Mt,t,e),a(t,ve,e),a(t,ht,e),a(t,xe,e),d(gt,t,e),a(t,Ne,e),a(t,ut,e),a(t,Be,e),a(t,yt,e),a(t,_e,e),a(t,k,e),a(t,Ge,e),a(t,ft,e),a(t,We,e),a(t,wt,e),a(t,ze,e),d(Jt,t,e),a(t,Xe,e),d(jt,t,e),a(t,Ve,e),a(t,Tt,e),a(t,He,e),a(t,bt,e),a(t,Fe,e),a(t,C,e),a(t,Qe,e),a(t,Ut,e),a(t,Ee,e),a(t,Zt,e),a(t,$e,e),d(It,t,e),a(t,Ye,e),a(t,kt,e),a(t,Se,e),d(Ct,t,e),a(t,qe,e),a(t,Rt,e),a(t,Ae,e),a(t,vt,e),a(t,De,e),a(t,R,e),a(t,Le,e),a(t,xt,e),a(t,Pe,e),d(Nt,t,e),a(t,Oe,e),a(t,Bt,e),a(t,Ke,e),d(v,t,e),a(t,ts,e),d(_t,t,e),a(t,es,e),a(t,Gt,e),a(t,ss,e),d(Wt,t,e),a(t,as,e),d(zt,t,e),a(t,ls,e),a(t,Xt,e),a(t,ns,e),a(t,Vt,e),a(t,is,e),a(t,Ht,e),a(t,ps,e),d(Ft,t,e),a(t,os,e),d(Qt,t,e),a(t,ms,e),a(t,Et,e),cs=!0},p(t,[e]){const la={};e&2&&(la.$$scope={dirty:e,ctx:t}),T.$set(la);const na={};e&2&&(na.$$scope={dirty:e,ctx:t}),U.$set(na);const ia={};e&2&&(ia.$$scope={dirty:e,ctx:t}),v.$set(ia)},i(t){cs||(M(N.$$.fragment,t),M(B.$$.fragment,t),M(H.$$.fragment,t),M(T.$$.fragment,t),M(F.$$.fragment,t),M(E.$$.fragment,t),M(Y.$$.fragment,t),M(q.$$.fragment,t),M(D.$$.fragment,t),M(P.$$.fragment,t),M(at.$$.fragment,t),M(U.$$.fragment,t),M(lt.$$.fragment,t),M(ot.$$.fragment,t),M(mt.$$.fragment,t),M(Mt.$$.fragment,t),M(gt.$$.fragment,t),M(Jt.$$.fragment,t),M(jt.$$.fragment,t),M(It.$$.fragment,t),M(Ct.$$.fragment,t),M(Nt.$$.fragment,t),M(v.$$.fragment,t),M(_t.$$.fragment,t),M(Wt.$$.fragment,t),M(zt.$$.fragment,t),M(Ft.$$.fragment,t),M(Qt.$$.fragment,t),cs=!0)},o(t){h(N.$$.fragment,t),h(B.$$.fragment,t),h(H.$$.fragment,t),h(T.$$.fragment,t),h(F.$$.fragment,t),h(E.$$.fragment,t),h(Y.$$.fragment,t),h(q.$$.fragment,t),h(D.$$.fragment,t),h(P.$$.fragment,t),h(at.$$.fragment,t),h(U.$$.fragment,t),h(lt.$$.fragment,t),h(ot.$$.fragment,t),h(mt.$$.fragment,t),h(Mt.$$.fragment,t),h(gt.$$.fragment,t),h(Jt.$$.fragment,t),h(jt.$$.fragment,t),h(It.$$.fragment,t),h(Ct.$$.fragment,t),h(Nt.$$.fragment,t),h(v.$$.fragment,t),h(_t.$$.fragment,t),h(Wt.$$.fragment,t),h(zt.$$.fragment,t),h(Ft.$$.fragment,t),h(Qt.$$.fragment,t),cs=!1},d(t){t&&(s(y),s(u),s(j),s($t),s(Yt),s(_),s(St),s(G),s(qt),s(W),s(At),s(z),s(Dt),s(X),s(Lt),s(V),s(Pt),s(Ot),s(Kt),s(te),s(Q),s(ee),s(se),s($),s(ae),s(le),s(S),s(ne),s(ie),s(A),s(pe),s(oe),s(L),s(me),s(ce),s(O),s(re),s(K),s(de),s(b),s(Me),s(tt),s(he),s(et),s(ge),s(st),s(ue),s(ye),s(fe),s(we),s(nt),s(Je),s(Z),s(je),s(it),s(Te),s(pt),s(be),s(Ue),s(Ze),s(ct),s(Ie),s(rt),s(ke),s(I),s(Ce),s(dt),s(Re),s(ve),s(ht),s(xe),s(Ne),s(ut),s(Be),s(yt),s(_e),s(k),s(Ge),s(ft),s(We),s(wt),s(ze),s(Xe),s(Ve),s(Tt),s(He),s(bt),s(Fe),s(C),s(Qe),s(Ut),s(Ee),s(Zt),s($e),s(Ye),s(kt),s(Se),s(qe),s(Rt),s(Ae),s(vt),s(De),s(R),s(Le),s(xt),s(Pe),s(Oe),s(Bt),s(Ke),s(ts),s(es),s(Gt),s(ss),s(as),s(ls),s(Xt),s(ns),s(Vt),s(is),s(Ht),s(ps),s(os),s(ms),s(Et)),s(m),g(N,t),g(B,t),g(H,t),g(T,t),g(F,t),g(E,t),g(Y,t),g(q,t),g(D,t),g(P,t),g(at,t),g(U,t),g(lt,t),g(ot,t),g(mt,t),g(Mt,t),g(gt,t),g(Jt,t),g(jt,t),g(It,t),g(Ct,t),g(Nt,t),g(v,t),g(_t,t),g(Wt,t),g(zt,t),g(Ft,t),g(Qt,t)}}}const ba='{"title":"Image tasks with IDEFICS","local":"image-tasks-with-idefics","sections":[{"title":"Loading the model","local":"loading-the-model","sections":[{"title":"Quantized model","local":"quantized-model","sections":[],"depth":3}],"depth":2},{"title":"Image captioning","local":"image-captioning","sections":[],"depth":2},{"title":"Prompted image captioning","local":"prompted-image-captioning","sections":[],"depth":2},{"title":"Few-shot prompting","local":"few-shot-prompting","sections":[],"depth":2},{"title":"Visual question answering","local":"visual-question-answering","sections":[],"depth":2},{"title":"Image classification","local":"image-classification","sections":[],"depth":2},{"title":"Image-guided text generation","local":"image-guided-text-generation","sections":[],"depth":2},{"title":"Running inference in batch mode","local":"running-inference-in-batch-mode","sections":[],"depth":2},{"title":"IDEFICS instruct for conversational use","local":"idefics-instruct-for-conversational-use","sections":[],"depth":2}],"depth":1}';function Ua(J){return ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xa extends ra{constructor(m){super(),da(this,m,Ua,Ta,ma,{})}}export{xa as component};
