import{s as Mn,z as Cn,o as vn,n as k}from"../chunks/scheduler.18a86fab.js";import{S as In,i as wn,g as m,s as a,r as f,A as $n,h as p,f as s,c as i,j as P,x as M,u,k as I,y as c,a as l,v as h,d as _,t as T,w as b}from"../chunks/index.98837b22.js";import{T as so}from"../chunks/Tip.77304350.js";import{D as L}from"../chunks/Docstring.a1ef7999.js";import{C as Y}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as A}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as Pn}from"../chunks/PipelineTag.7749150e.js";import{H as B,E as xn}from"../chunks/getInferenceSnippets.06c2775f.js";function Sn(v){let o,y="Example:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdDb25maWclMkMlMjBDTElQU2VnTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ01vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMGZyb20lMjBhJTIwQ0xJUFNlZ1RleHRDb25maWclMjBhbmQlMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dCUyMGFuZCUyMENMSVBTZWdWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMENMSVBTZWdWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMENMSVBTZWdDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegConfig, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegText and CLIPSegVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPSegTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-11lpom8"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function Ln(v){let o,y="Example:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdUZXh0Q29uZmlnJTJDJTIwQ0xJUFNlZ1RleHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dENvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBTZWdUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMENJREFTJTJGY2xpcHNlZy1yZDY0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegTextConfig, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-11lpom8"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function jn(v){let o,y="Example:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdWaXNpb25Db25maWclMkMlMjBDTElQU2VnVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegVisionConfig, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-11lpom8"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function Jn(v){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(d){o=p(d,"P",{"data-svelte-h":!0}),M(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(d,r){l(d,o,r)},p:k,d(d){d&&s(o)}}}function Zn(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMEElMjAlMjAlMjAlMjB0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUlMEEpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBsb2dpdHNfcGVyX2ltYWdlLnNvZnRtYXgoZGltJTNEMSklMjAlMjAlMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function kn(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function Un(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBaW1hZ2VfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfaW1hZ2VfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function zn(v){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(d){o=p(d,"P",{"data-svelte-h":!0}),M(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(d,r){l(d,o,r)},p:k,d(d){d&&s(o)}}}function Wn(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnVGV4dE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyQ0lEQVMlMkZjbGlwc2VnLXJkNjQtcmVmaW5lZCUyMiklMEFtb2RlbCUyMCUzRCUyMENMSVBTZWdUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMChFT1MlMjB0b2tlbiklMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function Vn(v){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(d){o=p(d,"P",{"data-svelte-h":!0}),M(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(d,r){l(d,o,r)},p:k,d(d){d&&s(o)}}}function Bn(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdWaXNpb25Nb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjBDTFMlMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function Nn(v){let o,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=y},l(d){o=p(d,"P",{"data-svelte-h":!0}),M(o)!=="svelte-fincs2"&&(o.innerHTML=y)},m(d,r){l(d,o,r)},p:k,d(d){d&&s(o)}}}function Fn(v){let o,y="Examples:",d,r,g;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXRleHRzJTIwJTNEJTIwJTVCJTIyYSUyMGNhdCUyMiUyQyUyMCUyMmElMjByZW1vdGUlMjIlMkMlMjAlMjJhJTIwYmxhbmtldCUyMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEdGV4dHMlMkMlMjBpbWFnZXMlM0QlNUJpbWFnZSU1RCUyMColMjBsZW4odGV4dHMpJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQXByaW50KGxvZ2l0cy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote&quot;</span>, <span class="hljs-string">&quot;a blanket&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=[image] * <span class="hljs-built_in">len</span>(texts), padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(logits.shape)
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">352</span>, <span class="hljs-number">352</span>])`,wrap:!1}}),{c(){o=m("p"),o.textContent=y,d=a(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),M(o)!=="svelte-kvfsh7"&&(o.textContent=y),d=i(t),u(r.$$.fragment,t)},m(t,C){l(t,o,C),l(t,d,C),h(r,t,C),g=!0},p:k,i(t){g||(_(r.$$.fragment,t),g=!0)},o(t){T(r.$$.fragment,t),g=!1},d(t){t&&(s(o),s(d)),b(r,t)}}}function En(v){let o,y,d,r,g,t="<em>This model was released on 2021-12-18 and added to Hugging Face Transformers on 2022-11-08.</em>",C,_e,wt,O,Ho='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',$t,Te,Pt,be,Go=`The CLIPSeg model was proposed in <a href="https://huggingface.co/papers/2112.10003" rel="nofollow">Image Segmentation Using Text and Image Prompts</a> by Timo LÃ¼ddecke
and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen <a href="clip">CLIP</a> model for zero-shot and one-shot image segmentation.`,xt,ye,Ro="The abstract from the paper is the following:",St,Me,Qo=`<em>Image segmentation is usually addressed by training a
model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive
as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system
that can generate image segmentations based on arbitrary
prompts at test time. A prompt can be either a text or an
image. This approach enables us to create a unified model
(trained once) for three common segmentation tasks, which
come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.
We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense
prediction. After training on an extended version of the
PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on
an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.
This novel hybrid input allows for dynamic adaptation not
only to the three segmentation tasks mentioned above, but
to any binary segmentation task where a text or image query
can be formulated. Finally, we find our system to adapt well
to generalized queries involving affordances or properties</em>`,Lt,K,Do,jt,Ce,Ao='CLIPSeg overview. Taken from the <a href="https://huggingface.co/papers/2112.10003">original paper.</a>',Jt,ve,Yo=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/timojl/clipseg" rel="nofollow">here</a>.`,Zt,Ie,kt,we,Oo=`<li><a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> adds a decoder on top of <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. The latter is identical to <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a>.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text
(provided to the model as <code>input_ids</code>) or an image (provided to the model as <code>conditional_pixel_values</code>). One can also provide custom
conditional embeddings (provided to the model as <code>conditional_embeddings</code>).</li>`,Ut,$e,zt,Pe,Ko="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with CLIPSeg. If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Wt,xe,Vt,Se,en='<li>A notebook that illustrates <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb" rel="nofollow">zero-shot image segmentation with CLIPSeg</a>.</li>',Bt,Le,Nt,x,je,ro,rt,tn=`<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,ao,at,on=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,io,ee,lo,te,Je,co,it,nn=`Instantiate a model config (or a derived class) from text model configuration and vision model
configuration.`,Ft,Ze,Et,j,ke,mo,lt,sn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,po,dt,rn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,go,oe,qt,Ue,Xt,J,ze,fo,ct,an=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,uo,mt,ln=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ho,ne,Ht,We,Gt,N,Ve,_o,pt,dn="Constructs a CLIPSeg processor which wraps a CLIPSeg image processor and a CLIP tokenizer into a single processor.",To,gt,cn=`<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Rt,Be,Qt,w,Ne,bo,ft,mn="The bare Clipseg Model outputting raw hidden-states without any specific head on top.",yo,ut,pn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Mo,ht,gn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Co,U,Fe,vo,_t,fn='The <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Io,se,wo,re,$o,ae,Ee,Po,ie,xo,le,qe,So,de,Dt,Xe,At,R,He,Lo,z,Ge,jo,Tt,un='The <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel">CLIPSegTextModel</a> forward method, overrides the <code>__call__</code> special method.',Jo,ce,Zo,me,Yt,Re,Ot,Q,Qe,ko,W,De,Uo,bt,hn='The <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel">CLIPSegVisionModel</a> forward method, overrides the <code>__call__</code> special method.',zo,pe,Wo,ge,Kt,Ae,eo,S,Ye,Vo,yt,_n="CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.",Bo,Mt,Tn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,No,Ct,bn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Fo,V,Oe,Eo,vt,yn='The <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> forward method, overrides the <code>__call__</code> special method.',qo,fe,Xo,ue,to,Ke,oo,It,no;return _e=new B({props:{title:"CLIPSeg",local:"clipseg",headingTag:"h1"}}),Te=new B({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ie=new B({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),$e=new B({props:{title:"Resources",local:"resources",headingTag:"h2"}}),xe=new Pn({props:{pipeline:"image-segmentation"}}),Le=new B({props:{title:"CLIPSegConfig",local:"transformers.CLIPSegConfig",headingTag:"h2"}}),je=new L({props:{name:"class transformers.CLIPSegConfig",anchor:"transformers.CLIPSegConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"extract_layers",val:" = [3, 6, 9]"},{name:"reduce_dim",val:" = 64"},{name:"decoder_num_attention_heads",val:" = 4"},{name:"decoder_attention_dropout",val:" = 0.0"},{name:"decoder_hidden_act",val:" = 'quick_gelu'"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"conditional_layer",val:" = 0"},{name:"use_complex_transposed_convolution",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextConfig">CLIPSegTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPSegConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionConfig">CLIPSegVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPSegConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPSegConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The initial value of the <em>logit_scale</em> parameter. Default is used as per the original CLIPSeg implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPSegConfig.extract_layers",description:`<strong>extract_layers</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 6, 9]</code>) &#x2014;
Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.`,name:"extract_layers"},{anchor:"transformers.CLIPSegConfig.reduce_dim",description:`<strong>reduce_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality to reduce the CLIP vision embedding.`,name:"reduce_dim"},{anchor:"transformers.CLIPSegConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention heads in the decoder of CLIPSeg.`,name:"decoder_num_attention_heads"},{anchor:"transformers.CLIPSegConfig.decoder_attention_dropout",description:`<strong>decoder_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"decoder_attention_dropout"},{anchor:"transformers.CLIPSegConfig.decoder_hidden_act",description:`<strong>decoder_hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"decoder_hidden_act"},{anchor:"transformers.CLIPSegConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layers in the Transformer decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.CLIPSegConfig.conditional_layer",description:`<strong>conditional_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The layer to use of the Transformer encoder whose activations will be combined with the condition
embeddings using FiLM (Feature-wise Linear Modulation). If 0, the last layer is used.`,name:"conditional_layer"},{anchor:"transformers.CLIPSegConfig.use_complex_transposed_convolution",description:`<strong>use_complex_transposed_convolution</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a more complex transposed convolution in the decoder, enabling more fine-grained
segmentation.`,name:"use_complex_transposed_convolution"},{anchor:"transformers.CLIPSegConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/configuration_clipseg.py#L207"}}),ee=new A({props:{anchor:"transformers.CLIPSegConfig.example",$$slots:{default:[Sn]},$$scope:{ctx:v}}}),Je=new L({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPSegConfig.from_text_vision_configs",parameters:[{name:"text_config",val:""},{name:"vision_config",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/configuration_utils.py#L1254",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>PreTrainedConfig</code></p>
`}}),Ze=new B({props:{title:"CLIPSegTextConfig",local:"transformers.CLIPSegTextConfig",headingTag:"h2"}}),ke=new L({props:{name:"class transformers.CLIPSegTextConfig",anchor:"transformers.CLIPSegTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIPSeg text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPSegTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPSegTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.CLIPSegTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.CLIPSegTextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.CLIPSegTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/configuration_clipseg.py#L24"}}),oe=new A({props:{anchor:"transformers.CLIPSegTextConfig.example",$$slots:{default:[Ln]},$$scope:{ctx:v}}}),Ue=new B({props:{title:"CLIPSegVisionConfig",local:"transformers.CLIPSegVisionConfig",headingTag:"h2"}}),ze=new L({props:{name:"class transformers.CLIPSegVisionConfig",anchor:"transformers.CLIPSegVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CLIPSegVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPSegVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPSegVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/configuration_clipseg.py#L119"}}),ne=new A({props:{anchor:"transformers.CLIPSegVisionConfig.example",$$slots:{default:[jn]},$$scope:{ctx:v}}}),We=new B({props:{title:"CLIPSegProcessor",local:"transformers.CLIPSegProcessor",headingTag:"h2"}}),Ve=new L({props:{name:"class transformers.CLIPSegProcessor",anchor:"transformers.CLIPSegProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.CLIPSegProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/processing_clipseg.py#L25"}}),Be=new B({props:{title:"CLIPSegModel",local:"transformers.CLIPSegModel",headingTag:"h2"}}),Ne=new L({props:{name:"class transformers.CLIPSegModel",anchor:"transformers.CLIPSegModel",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L811"}}),Fe=new L({props:{name:"forward",anchor:"transformers.CLIPSegModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.CLIPSegModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L948",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) â€” Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) â€” The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) â€” The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new so({props:{$$slots:{default:[Jn]},$$scope:{ctx:v}}}),re=new A({props:{anchor:"transformers.CLIPSegModel.forward.example",$$slots:{default:[Zn]},$$scope:{ctx:v}}}),Ee=new L({props:{name:"get_text_features",anchor:"transformers.CLIPSegModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L850",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ie=new A({props:{anchor:"transformers.CLIPSegModel.get_text_features.example",$$slots:{default:[kn]},$$scope:{ctx:v}}}),qe=new L({props:{name:"get_image_features",anchor:"transformers.CLIPSegModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.CLIPSegModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L897",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),de=new A({props:{anchor:"transformers.CLIPSegModel.get_image_features.example",$$slots:{default:[Un]},$$scope:{ctx:v}}}),Xe=new B({props:{title:"CLIPSegTextModel",local:"transformers.CLIPSegTextModel",headingTag:"h2"}}),He=new L({props:{name:"class transformers.CLIPSegTextModel",anchor:"transformers.CLIPSegTextModel",parameters:[{name:"config",val:": CLIPSegTextConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L655"}}),Ge=new L({props:{name:"forward",anchor:"transformers.CLIPSegTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L672",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ce=new so({props:{$$slots:{default:[zn]},$$scope:{ctx:v}}}),me=new A({props:{anchor:"transformers.CLIPSegTextModel.forward.example",$$slots:{default:[Wn]},$$scope:{ctx:v}}}),Re=new B({props:{title:"CLIPSegVisionModel",local:"transformers.CLIPSegVisionModel",headingTag:"h2"}}),Qe=new L({props:{name:"class transformers.CLIPSegVisionModel",anchor:"transformers.CLIPSegVisionModel",parameters:[{name:"config",val:": CLIPSegVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L759"}}),De=new L({props:{name:"forward",anchor:"transformers.CLIPSegVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.CLIPSegVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegVisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.CLIPSegVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L772",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new so({props:{$$slots:{default:[Vn]},$$scope:{ctx:v}}}),ge=new A({props:{anchor:"transformers.CLIPSegVisionModel.forward.example",$$slots:{default:[Bn]},$$scope:{ctx:v}}}),Ae=new B({props:{title:"CLIPSegForImageSegmentation",local:"transformers.CLIPSegForImageSegmentation",headingTag:"h2"}}),Ye=new L({props:{name:"class transformers.CLIPSegForImageSegmentation",anchor:"transformers.CLIPSegForImageSegmentation",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L1206"}}),Oe=new L({props:{name:"forward",anchor:"transformers.CLIPSegForImageSegmentation.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"conditional_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"conditional_embeddings",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.conditional_pixel_values",description:`<strong>conditional_pixel_values</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
The pixel values of the conditional images.`,name:"conditional_pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.conditional_embeddings",description:`<strong>conditional_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.projection_dim)</code>, <em>optional</em>) &#x2014;
The conditional embeddings for the query images. If provided, the model will use this instead of computing
the embeddings from the conditional_pixel_values.`,name:"conditional_embeddings"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/clipseg/modeling_clipseg.py#L1251",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) â€” Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) â€” The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) â€” The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) â€” The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>) â€” The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new so({props:{$$slots:{default:[Nn]},$$scope:{ctx:v}}}),ue=new A({props:{anchor:"transformers.CLIPSegForImageSegmentation.forward.example",$$slots:{default:[Fn]},$$scope:{ctx:v}}}),Ke=new xn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/clipseg.md"}}),{c(){o=m("meta"),y=a(),d=m("p"),r=a(),g=m("p"),g.innerHTML=t,C=a(),f(_e.$$.fragment),wt=a(),O=m("div"),O.innerHTML=Ho,$t=a(),f(Te.$$.fragment),Pt=a(),be=m("p"),be.innerHTML=Go,xt=a(),ye=m("p"),ye.textContent=Ro,St=a(),Me=m("p"),Me.innerHTML=Qo,Lt=a(),K=m("img"),jt=a(),Ce=m("small"),Ce.innerHTML=Ao,Jt=a(),ve=m("p"),ve.innerHTML=Yo,Zt=a(),f(Ie.$$.fragment),kt=a(),we=m("ul"),we.innerHTML=Oo,Ut=a(),f($e.$$.fragment),zt=a(),Pe=m("p"),Pe.textContent=Ko,Wt=a(),f(xe.$$.fragment),Vt=a(),Se=m("ul"),Se.innerHTML=en,Bt=a(),f(Le.$$.fragment),Nt=a(),x=m("div"),f(je.$$.fragment),ro=a(),rt=m("p"),rt.innerHTML=tn,ao=a(),at=m("p"),at.innerHTML=on,io=a(),f(ee.$$.fragment),lo=a(),te=m("div"),f(Je.$$.fragment),co=a(),it=m("p"),it.textContent=nn,Ft=a(),f(Ze.$$.fragment),Et=a(),j=m("div"),f(ke.$$.fragment),mo=a(),lt=m("p"),lt.innerHTML=sn,po=a(),dt=m("p"),dt.innerHTML=rn,go=a(),f(oe.$$.fragment),qt=a(),f(Ue.$$.fragment),Xt=a(),J=m("div"),f(ze.$$.fragment),fo=a(),ct=m("p"),ct.innerHTML=an,uo=a(),mt=m("p"),mt.innerHTML=ln,ho=a(),f(ne.$$.fragment),Ht=a(),f(We.$$.fragment),Gt=a(),N=m("div"),f(Ve.$$.fragment),_o=a(),pt=m("p"),pt.textContent=dn,To=a(),gt=m("p"),gt.innerHTML=cn,Rt=a(),f(Be.$$.fragment),Qt=a(),w=m("div"),f(Ne.$$.fragment),bo=a(),ft=m("p"),ft.textContent=mn,yo=a(),ut=m("p"),ut.innerHTML=pn,Mo=a(),ht=m("p"),ht.innerHTML=gn,Co=a(),U=m("div"),f(Fe.$$.fragment),vo=a(),_t=m("p"),_t.innerHTML=fn,Io=a(),f(se.$$.fragment),wo=a(),f(re.$$.fragment),$o=a(),ae=m("div"),f(Ee.$$.fragment),Po=a(),f(ie.$$.fragment),xo=a(),le=m("div"),f(qe.$$.fragment),So=a(),f(de.$$.fragment),Dt=a(),f(Xe.$$.fragment),At=a(),R=m("div"),f(He.$$.fragment),Lo=a(),z=m("div"),f(Ge.$$.fragment),jo=a(),Tt=m("p"),Tt.innerHTML=un,Jo=a(),f(ce.$$.fragment),Zo=a(),f(me.$$.fragment),Yt=a(),f(Re.$$.fragment),Ot=a(),Q=m("div"),f(Qe.$$.fragment),ko=a(),W=m("div"),f(De.$$.fragment),Uo=a(),bt=m("p"),bt.innerHTML=hn,zo=a(),f(pe.$$.fragment),Wo=a(),f(ge.$$.fragment),Kt=a(),f(Ae.$$.fragment),eo=a(),S=m("div"),f(Ye.$$.fragment),Vo=a(),yt=m("p"),yt.textContent=_n,Bo=a(),Mt=m("p"),Mt.innerHTML=Tn,No=a(),Ct=m("p"),Ct.innerHTML=bn,Fo=a(),V=m("div"),f(Oe.$$.fragment),Eo=a(),vt=m("p"),vt.innerHTML=yn,qo=a(),f(fe.$$.fragment),Xo=a(),f(ue.$$.fragment),to=a(),f(Ke.$$.fragment),oo=a(),It=m("p"),this.h()},l(e){const n=$n("svelte-u9bgzb",document.head);o=p(n,"META",{name:!0,content:!0}),n.forEach(s),y=i(e),d=p(e,"P",{}),P(d).forEach(s),r=i(e),g=p(e,"P",{"data-svelte-h":!0}),M(g)!=="svelte-eba6s5"&&(g.innerHTML=t),C=i(e),u(_e.$$.fragment,e),wt=i(e),O=p(e,"DIV",{class:!0,"data-svelte-h":!0}),M(O)!=="svelte-13t8s2t"&&(O.innerHTML=Ho),$t=i(e),u(Te.$$.fragment,e),Pt=i(e),be=p(e,"P",{"data-svelte-h":!0}),M(be)!=="svelte-krih9e"&&(be.innerHTML=Go),xt=i(e),ye=p(e,"P",{"data-svelte-h":!0}),M(ye)!=="svelte-vfdo9a"&&(ye.textContent=Ro),St=i(e),Me=p(e,"P",{"data-svelte-h":!0}),M(Me)!=="svelte-2767bu"&&(Me.innerHTML=Qo),Lt=i(e),K=p(e,"IMG",{src:!0,alt:!0,width:!0}),jt=i(e),Ce=p(e,"SMALL",{"data-svelte-h":!0}),M(Ce)!=="svelte-jxty9y"&&(Ce.innerHTML=Ao),Jt=i(e),ve=p(e,"P",{"data-svelte-h":!0}),M(ve)!=="svelte-14bl8yt"&&(ve.innerHTML=Yo),Zt=i(e),u(Ie.$$.fragment,e),kt=i(e),we=p(e,"UL",{"data-svelte-h":!0}),M(we)!=="svelte-nzzs6l"&&(we.innerHTML=Oo),Ut=i(e),u($e.$$.fragment,e),zt=i(e),Pe=p(e,"P",{"data-svelte-h":!0}),M(Pe)!=="svelte-i5etz8"&&(Pe.textContent=Ko),Wt=i(e),u(xe.$$.fragment,e),Vt=i(e),Se=p(e,"UL",{"data-svelte-h":!0}),M(Se)!=="svelte-ksa1e9"&&(Se.innerHTML=en),Bt=i(e),u(Le.$$.fragment,e),Nt=i(e),x=p(e,"DIV",{class:!0});var Z=P(x);u(je.$$.fragment,Z),ro=i(Z),rt=p(Z,"P",{"data-svelte-h":!0}),M(rt)!=="svelte-1y7sjf7"&&(rt.innerHTML=tn),ao=i(Z),at=p(Z,"P",{"data-svelte-h":!0}),M(at)!=="svelte-1ek1ss9"&&(at.innerHTML=on),io=i(Z),u(ee.$$.fragment,Z),lo=i(Z),te=p(Z,"DIV",{class:!0});var et=P(te);u(Je.$$.fragment,et),co=i(et),it=p(et,"P",{"data-svelte-h":!0}),M(it)!=="svelte-w5eluu"&&(it.textContent=nn),et.forEach(s),Z.forEach(s),Ft=i(e),u(Ze.$$.fragment,e),Et=i(e),j=p(e,"DIV",{class:!0});var F=P(j);u(ke.$$.fragment,F),mo=i(F),lt=p(F,"P",{"data-svelte-h":!0}),M(lt)!=="svelte-180rr8g"&&(lt.innerHTML=sn),po=i(F),dt=p(F,"P",{"data-svelte-h":!0}),M(dt)!=="svelte-1ek1ss9"&&(dt.innerHTML=rn),go=i(F),u(oe.$$.fragment,F),F.forEach(s),qt=i(e),u(Ue.$$.fragment,e),Xt=i(e),J=p(e,"DIV",{class:!0});var E=P(J);u(ze.$$.fragment,E),fo=i(E),ct=p(E,"P",{"data-svelte-h":!0}),M(ct)!=="svelte-180rr8g"&&(ct.innerHTML=an),uo=i(E),mt=p(E,"P",{"data-svelte-h":!0}),M(mt)!=="svelte-1ek1ss9"&&(mt.innerHTML=ln),ho=i(E),u(ne.$$.fragment,E),E.forEach(s),Ht=i(e),u(We.$$.fragment,e),Gt=i(e),N=p(e,"DIV",{class:!0});var D=P(N);u(Ve.$$.fragment,D),_o=i(D),pt=p(D,"P",{"data-svelte-h":!0}),M(pt)!=="svelte-9hszhx"&&(pt.textContent=dn),To=i(D),gt=p(D,"P",{"data-svelte-h":!0}),M(gt)!=="svelte-18k4lpw"&&(gt.innerHTML=cn),D.forEach(s),Rt=i(e),u(Be.$$.fragment,e),Qt=i(e),w=p(e,"DIV",{class:!0});var $=P(w);u(Ne.$$.fragment,$),bo=i($),ft=p($,"P",{"data-svelte-h":!0}),M(ft)!=="svelte-9mbg47"&&(ft.textContent=mn),yo=i($),ut=p($,"P",{"data-svelte-h":!0}),M(ut)!=="svelte-q52n56"&&(ut.innerHTML=pn),Mo=i($),ht=p($,"P",{"data-svelte-h":!0}),M(ht)!=="svelte-hswkmf"&&(ht.innerHTML=gn),Co=i($),U=p($,"DIV",{class:!0});var q=P(U);u(Fe.$$.fragment,q),vo=i(q),_t=p(q,"P",{"data-svelte-h":!0}),M(_t)!=="svelte-gj5qby"&&(_t.innerHTML=fn),Io=i(q),u(se.$$.fragment,q),wo=i(q),u(re.$$.fragment,q),q.forEach(s),$o=i($),ae=p($,"DIV",{class:!0});var tt=P(ae);u(Ee.$$.fragment,tt),Po=i(tt),u(ie.$$.fragment,tt),tt.forEach(s),xo=i($),le=p($,"DIV",{class:!0});var ot=P(le);u(qe.$$.fragment,ot),So=i(ot),u(de.$$.fragment,ot),ot.forEach(s),$.forEach(s),Dt=i(e),u(Xe.$$.fragment,e),At=i(e),R=p(e,"DIV",{class:!0});var nt=P(R);u(He.$$.fragment,nt),Lo=i(nt),z=p(nt,"DIV",{class:!0});var X=P(z);u(Ge.$$.fragment,X),jo=i(X),Tt=p(X,"P",{"data-svelte-h":!0}),M(Tt)!=="svelte-1hkz9g0"&&(Tt.innerHTML=un),Jo=i(X),u(ce.$$.fragment,X),Zo=i(X),u(me.$$.fragment,X),X.forEach(s),nt.forEach(s),Yt=i(e),u(Re.$$.fragment,e),Ot=i(e),Q=p(e,"DIV",{class:!0});var st=P(Q);u(Qe.$$.fragment,st),ko=i(st),W=p(st,"DIV",{class:!0});var H=P(W);u(De.$$.fragment,H),Uo=i(H),bt=p(H,"P",{"data-svelte-h":!0}),M(bt)!=="svelte-1f7v6ym"&&(bt.innerHTML=hn),zo=i(H),u(pe.$$.fragment,H),Wo=i(H),u(ge.$$.fragment,H),H.forEach(s),st.forEach(s),Kt=i(e),u(Ae.$$.fragment,e),eo=i(e),S=p(e,"DIV",{class:!0});var G=P(S);u(Ye.$$.fragment,G),Vo=i(G),yt=p(G,"P",{"data-svelte-h":!0}),M(yt)!=="svelte-qndran"&&(yt.textContent=_n),Bo=i(G),Mt=p(G,"P",{"data-svelte-h":!0}),M(Mt)!=="svelte-q52n56"&&(Mt.innerHTML=Tn),No=i(G),Ct=p(G,"P",{"data-svelte-h":!0}),M(Ct)!=="svelte-hswkmf"&&(Ct.innerHTML=bn),Fo=i(G),V=p(G,"DIV",{class:!0});var he=P(V);u(Oe.$$.fragment,he),Eo=i(he),vt=p(he,"P",{"data-svelte-h":!0}),M(vt)!=="svelte-1h2hn8a"&&(vt.innerHTML=yn),qo=i(he),u(fe.$$.fragment,he),Xo=i(he),u(ue.$$.fragment,he),he.forEach(s),G.forEach(s),to=i(e),u(Ke.$$.fragment,e),oo=i(e),It=p(e,"P",{}),P(It).forEach(s),this.h()},h(){I(o,"name","hf:doc:metadata"),I(o,"content",qn),I(O,"class","flex flex-wrap space-x-1"),Cn(K.src,Do="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png")||I(K,"src",Do),I(K,"alt","drawing"),I(K,"width","600"),I(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){c(document.head,o),l(e,y,n),l(e,d,n),l(e,r,n),l(e,g,n),l(e,C,n),h(_e,e,n),l(e,wt,n),l(e,O,n),l(e,$t,n),h(Te,e,n),l(e,Pt,n),l(e,be,n),l(e,xt,n),l(e,ye,n),l(e,St,n),l(e,Me,n),l(e,Lt,n),l(e,K,n),l(e,jt,n),l(e,Ce,n),l(e,Jt,n),l(e,ve,n),l(e,Zt,n),h(Ie,e,n),l(e,kt,n),l(e,we,n),l(e,Ut,n),h($e,e,n),l(e,zt,n),l(e,Pe,n),l(e,Wt,n),h(xe,e,n),l(e,Vt,n),l(e,Se,n),l(e,Bt,n),h(Le,e,n),l(e,Nt,n),l(e,x,n),h(je,x,null),c(x,ro),c(x,rt),c(x,ao),c(x,at),c(x,io),h(ee,x,null),c(x,lo),c(x,te),h(Je,te,null),c(te,co),c(te,it),l(e,Ft,n),h(Ze,e,n),l(e,Et,n),l(e,j,n),h(ke,j,null),c(j,mo),c(j,lt),c(j,po),c(j,dt),c(j,go),h(oe,j,null),l(e,qt,n),h(Ue,e,n),l(e,Xt,n),l(e,J,n),h(ze,J,null),c(J,fo),c(J,ct),c(J,uo),c(J,mt),c(J,ho),h(ne,J,null),l(e,Ht,n),h(We,e,n),l(e,Gt,n),l(e,N,n),h(Ve,N,null),c(N,_o),c(N,pt),c(N,To),c(N,gt),l(e,Rt,n),h(Be,e,n),l(e,Qt,n),l(e,w,n),h(Ne,w,null),c(w,bo),c(w,ft),c(w,yo),c(w,ut),c(w,Mo),c(w,ht),c(w,Co),c(w,U),h(Fe,U,null),c(U,vo),c(U,_t),c(U,Io),h(se,U,null),c(U,wo),h(re,U,null),c(w,$o),c(w,ae),h(Ee,ae,null),c(ae,Po),h(ie,ae,null),c(w,xo),c(w,le),h(qe,le,null),c(le,So),h(de,le,null),l(e,Dt,n),h(Xe,e,n),l(e,At,n),l(e,R,n),h(He,R,null),c(R,Lo),c(R,z),h(Ge,z,null),c(z,jo),c(z,Tt),c(z,Jo),h(ce,z,null),c(z,Zo),h(me,z,null),l(e,Yt,n),h(Re,e,n),l(e,Ot,n),l(e,Q,n),h(Qe,Q,null),c(Q,ko),c(Q,W),h(De,W,null),c(W,Uo),c(W,bt),c(W,zo),h(pe,W,null),c(W,Wo),h(ge,W,null),l(e,Kt,n),h(Ae,e,n),l(e,eo,n),l(e,S,n),h(Ye,S,null),c(S,Vo),c(S,yt),c(S,Bo),c(S,Mt),c(S,No),c(S,Ct),c(S,Fo),c(S,V),h(Oe,V,null),c(V,Eo),c(V,vt),c(V,qo),h(fe,V,null),c(V,Xo),h(ue,V,null),l(e,to,n),h(Ke,e,n),l(e,oo,n),l(e,It,n),no=!0},p(e,[n]){const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),ee.$set(Z);const et={};n&2&&(et.$$scope={dirty:n,ctx:e}),oe.$set(et);const F={};n&2&&(F.$$scope={dirty:n,ctx:e}),ne.$set(F);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),se.$set(E);const D={};n&2&&(D.$$scope={dirty:n,ctx:e}),re.$set(D);const $={};n&2&&($.$$scope={dirty:n,ctx:e}),ie.$set($);const q={};n&2&&(q.$$scope={dirty:n,ctx:e}),de.$set(q);const tt={};n&2&&(tt.$$scope={dirty:n,ctx:e}),ce.$set(tt);const ot={};n&2&&(ot.$$scope={dirty:n,ctx:e}),me.$set(ot);const nt={};n&2&&(nt.$$scope={dirty:n,ctx:e}),pe.$set(nt);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),ge.$set(X);const st={};n&2&&(st.$$scope={dirty:n,ctx:e}),fe.$set(st);const H={};n&2&&(H.$$scope={dirty:n,ctx:e}),ue.$set(H)},i(e){no||(_(_e.$$.fragment,e),_(Te.$$.fragment,e),_(Ie.$$.fragment,e),_($e.$$.fragment,e),_(xe.$$.fragment,e),_(Le.$$.fragment,e),_(je.$$.fragment,e),_(ee.$$.fragment,e),_(Je.$$.fragment,e),_(Ze.$$.fragment,e),_(ke.$$.fragment,e),_(oe.$$.fragment,e),_(Ue.$$.fragment,e),_(ze.$$.fragment,e),_(ne.$$.fragment,e),_(We.$$.fragment,e),_(Ve.$$.fragment,e),_(Be.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(Ee.$$.fragment,e),_(ie.$$.fragment,e),_(qe.$$.fragment,e),_(de.$$.fragment,e),_(Xe.$$.fragment,e),_(He.$$.fragment,e),_(Ge.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(Re.$$.fragment,e),_(Qe.$$.fragment,e),_(De.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(Ae.$$.fragment,e),_(Ye.$$.fragment,e),_(Oe.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(Ke.$$.fragment,e),no=!0)},o(e){T(_e.$$.fragment,e),T(Te.$$.fragment,e),T(Ie.$$.fragment,e),T($e.$$.fragment,e),T(xe.$$.fragment,e),T(Le.$$.fragment,e),T(je.$$.fragment,e),T(ee.$$.fragment,e),T(Je.$$.fragment,e),T(Ze.$$.fragment,e),T(ke.$$.fragment,e),T(oe.$$.fragment,e),T(Ue.$$.fragment,e),T(ze.$$.fragment,e),T(ne.$$.fragment,e),T(We.$$.fragment,e),T(Ve.$$.fragment,e),T(Be.$$.fragment,e),T(Ne.$$.fragment,e),T(Fe.$$.fragment,e),T(se.$$.fragment,e),T(re.$$.fragment,e),T(Ee.$$.fragment,e),T(ie.$$.fragment,e),T(qe.$$.fragment,e),T(de.$$.fragment,e),T(Xe.$$.fragment,e),T(He.$$.fragment,e),T(Ge.$$.fragment,e),T(ce.$$.fragment,e),T(me.$$.fragment,e),T(Re.$$.fragment,e),T(Qe.$$.fragment,e),T(De.$$.fragment,e),T(pe.$$.fragment,e),T(ge.$$.fragment,e),T(Ae.$$.fragment,e),T(Ye.$$.fragment,e),T(Oe.$$.fragment,e),T(fe.$$.fragment,e),T(ue.$$.fragment,e),T(Ke.$$.fragment,e),no=!1},d(e){e&&(s(y),s(d),s(r),s(g),s(C),s(wt),s(O),s($t),s(Pt),s(be),s(xt),s(ye),s(St),s(Me),s(Lt),s(K),s(jt),s(Ce),s(Jt),s(ve),s(Zt),s(kt),s(we),s(Ut),s(zt),s(Pe),s(Wt),s(Vt),s(Se),s(Bt),s(Nt),s(x),s(Ft),s(Et),s(j),s(qt),s(Xt),s(J),s(Ht),s(Gt),s(N),s(Rt),s(Qt),s(w),s(Dt),s(At),s(R),s(Yt),s(Ot),s(Q),s(Kt),s(eo),s(S),s(to),s(oo),s(It)),s(o),b(_e,e),b(Te,e),b(Ie,e),b($e,e),b(xe,e),b(Le,e),b(je),b(ee),b(Je),b(Ze,e),b(ke),b(oe),b(Ue,e),b(ze),b(ne),b(We,e),b(Ve),b(Be,e),b(Ne),b(Fe),b(se),b(re),b(Ee),b(ie),b(qe),b(de),b(Xe,e),b(He),b(Ge),b(ce),b(me),b(Re,e),b(Qe),b(De),b(pe),b(ge),b(Ae,e),b(Ye),b(Oe),b(fe),b(ue),b(Ke,e)}}}const qn='{"title":"CLIPSeg","local":"clipseg","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CLIPSegConfig","local":"transformers.CLIPSegConfig","sections":[],"depth":2},{"title":"CLIPSegTextConfig","local":"transformers.CLIPSegTextConfig","sections":[],"depth":2},{"title":"CLIPSegVisionConfig","local":"transformers.CLIPSegVisionConfig","sections":[],"depth":2},{"title":"CLIPSegProcessor","local":"transformers.CLIPSegProcessor","sections":[],"depth":2},{"title":"CLIPSegModel","local":"transformers.CLIPSegModel","sections":[],"depth":2},{"title":"CLIPSegTextModel","local":"transformers.CLIPSegTextModel","sections":[],"depth":2},{"title":"CLIPSegVisionModel","local":"transformers.CLIPSegVisionModel","sections":[],"depth":2},{"title":"CLIPSegForImageSegmentation","local":"transformers.CLIPSegForImageSegmentation","sections":[],"depth":2}],"depth":1}';function Xn(v){return vn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kn extends In{constructor(o){super(),wn(this,o,Xn,En,Mn,{})}}export{Kn as component};
