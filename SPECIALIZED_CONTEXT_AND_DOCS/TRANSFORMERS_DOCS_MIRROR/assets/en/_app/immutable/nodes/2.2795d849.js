import{s as $e,n as Ce,o as We}from"../chunks/scheduler.18a86fab.js";import{S as ke,i as Fe,g as r,s as t,r as p,A as Ne,h as c,f as s,c as n,j as Ae,u as i,x as w,k as ve,y as ze,a as l,v as o,d,t as h,w as m}from"../chunks/index.98837b22.js";import{C as T}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as fe,E as Ve}from"../chunks/getInferenceSnippets.06c2775f.js";function Ye(ye){let J,S,Y,H,M,Q,f,ue='<a href="https://hf.co/docs/accelerate/index" rel="nofollow">Accelerate</a> is a library designed to simplify distributed training on any type of setup with PyTorch by uniting the most common frameworks (<a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/" rel="nofollow">Fully Sharded Data Parallel (FSDP)</a> and <a href="https://www.deepspeed.ai/" rel="nofollow">DeepSpeed</a>) for it into a single interface. <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> is powered by Accelerate under the hood, enabling loading big models and distributed training.',x,y,je='This guide will show you two ways to use Accelerate with Transformers, using FSDP as the backend. The first method demonstrates distributed training with <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>, and the second method demonstrates adapting a PyTorch training loop. For more detailed information about Accelerate, please refer to the <a href="https://hf.co/docs/accelerate/index" rel="nofollow">documentation</a>.',D,u,P,j,Ue='Start by running <a href="https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-config" rel="nofollow">accelerate config</a> in the command line to answer a series of prompts about your training system. This creates and saves a configuration file to help Accelerate correctly set up training based on your setup.',L,U,q,b,be="Depending on your setup and the answers you provide, an example configuration file for distributing training with FSDP on one machine with two GPUs may look like the following.",K,g,O,I,ee,Z,ge='Pass the path to the saved configuration file to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>, and from there, pass your <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>.',ae,_,se,B,le,G,Ie='Accelerate can also be added to any PyTorch training loop to enable distributed training. The <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/accelerator#accelerate.Accelerator" rel="nofollow">Accelerator</a> is the main entry point for adapting your PyTorch code to work with Accelerate. It automatically detects your distributed training setup and initializes all the necessary components for training. You donâ€™t need to explicitly place your model on a device because <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/accelerator#accelerate.Accelerator" rel="nofollow">Accelerator</a> knows which device to move your model to.',te,R,ne,X,Ze='All PyTorch objects (model, optimizer, scheduler, dataloaders) should be passed to the <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/accelerator#accelerate.Accelerator.prepare" rel="nofollow">prepare</a> method now. This method moves your model to the appropriate device or devices, adapts the optimizer and scheduler to use <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/torch_wrappers#accelerate.optimizer.AcceleratedOptimizer" rel="nofollow">AcceleratedOptimizer</a> and <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/torch_wrappers#accelerate.scheduler.AcceleratedScheduler" rel="nofollow">AcceleratedScheduler</a>, and creates a new shardable dataloader.',re,A,ce,v,_e='Replace <code>loss.backward</code> in your training loop with Accelerates <a href="https://huggingface.co/docs/accelerate/v1.10.1/en/package_reference/accelerator#accelerate.Accelerator.backward" rel="nofollow">backward</a> method to scale the gradients and determine the appropriate <code>backward</code> method to use depending on your framework (for example, DeepSpeed or Megatron).',pe,$,ie,C,Be="Combine everything into a function and make it callable as a script.",oe,W,de,k,Ge='From the command line, call <a href="https://hf.co/docs/accelerate/main/en/package_reference/cli#accelerate-launch" rel="nofollow">accelerate launch</a> to run your training script. Any additional arguments or parameters can be passed here as well.',he,F,Re="To launch your training script on two GPUs, add the <code>--num_processes</code> argument.",me,N,we,z,Xe='Refer to the <a href="https://hf.co/docs/accelerate/main/en/basic_tutorials/launch" rel="nofollow">Launching Accelerate scripts</a> for more details.',Je,V,Te,E,Me;return M=new fe({props:{title:"Accelerate",local:"accelerate",headingTag:"h1"}}),u=new T({props:{code:"cGlwJTIwaW5zdGFsbCUyMGFjY2VsZXJhdGU=",highlighted:"pip install accelerate",wrap:!1}}),U=new T({props:{code:"YWNjZWxlcmF0ZSUyMGNvbmZpZw==",highlighted:"accelerate config",wrap:!1}}),g=new T({props:{code:"Y29tcHV0ZV9lbnZpcm9ubWVudCUzQSUyMExPQ0FMX01BQ0hJTkUlMEFkZWJ1ZyUzQSUyMGZhbHNlJTBBZGlzdHJpYnV0ZWRfdHlwZSUzQSUyMEZTRFAlMEFkb3duY2FzdF9iZjE2JTNBJTIwJ25vJyUwQWZzZHBfY29uZmlnJTNBJTBBJTIwJTIwZnNkcF9hdXRvX3dyYXBfcG9saWN5JTNBJTIwVFJBTlNGT1JNRVJfQkFTRURfV1JBUCUwQSUyMCUyMGZzZHBfYmFja3dhcmRfcHJlZmV0Y2hfcG9saWN5JTNBJTIwQkFDS1dBUkRfUFJFJTBBJTIwJTIwZnNkcF9mb3J3YXJkX3ByZWZldGNoJTNBJTIwZmFsc2UlMEElMjAlMjBmc2RwX2NwdV9yYW1fZWZmaWNpZW50X2xvYWRpbmclM0ElMjB0cnVlJTBBJTIwJTIwZnNkcF9vZmZsb2FkX3BhcmFtcyUzQSUyMGZhbHNlJTBBJTIwJTIwZnNkcF9zaGFyZGluZ19zdHJhdGVneSUzQSUyMEZVTExfU0hBUkQlMEElMjAlMjBmc2RwX3N0YXRlX2RpY3RfdHlwZSUzQSUyMFNIQVJERURfU1RBVEVfRElDVCUwQSUyMCUyMGZzZHBfc3luY19tb2R1bGVfc3RhdGVzJTNBJTIwdHJ1ZSUwQSUyMCUyMGZzZHBfdHJhbnNmb3JtZXJfbGF5ZXJfY2xzX3RvX3dyYXAlM0ElMjBCZXJ0TGF5ZXIlMEElMjAlMjBmc2RwX3VzZV9vcmlnX3BhcmFtcyUzQSUyMHRydWUlMEFtYWNoaW5lX3JhbmslM0ElMjAwJTBBbWFpbl90cmFpbmluZ19mdW5jdGlvbiUzQSUyMG1haW4lMEFtaXhlZF9wcmVjaXNpb24lM0ElMjBiZjE2JTBBbnVtX21hY2hpbmVzJTNBJTIwMSUwQW51bV9wcm9jZXNzZXMlM0ElMjAyJTBBcmR6dl9iYWNrZW5kJTNBJTIwc3RhdGljJTBBc2FtZV9uZXR3b3JrJTNBJTIwdHJ1ZSUwQXRwdV9lbnYlM0ElMjAlNUIlNUQlMEF0cHVfdXNlX2NsdXN0ZXIlM0ElMjBmYWxzZSUwQXRwdV91c2Vfc3VkbyUzQSUyMGZhbHNlJTBBdXNlX2NwdSUzQSUyMGZhbHNl",highlighted:`<span class="hljs-attr">compute_environment:</span> <span class="hljs-string">LOCAL_MACHINE</span>
<span class="hljs-attr">debug:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">distributed_type:</span> <span class="hljs-string">FSDP</span>
<span class="hljs-attr">downcast_bf16:</span> <span class="hljs-string">&#x27;no&#x27;</span>
<span class="hljs-attr">fsdp_config:</span>
  <span class="hljs-attr">fsdp_auto_wrap_policy:</span> <span class="hljs-string">TRANSFORMER_BASED_WRAP</span>
  <span class="hljs-attr">fsdp_backward_prefetch_policy:</span> <span class="hljs-string">BACKWARD_PRE</span>
  <span class="hljs-attr">fsdp_forward_prefetch:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">fsdp_cpu_ram_efficient_loading:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">fsdp_offload_params:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">fsdp_sharding_strategy:</span> <span class="hljs-string">FULL_SHARD</span>
  <span class="hljs-attr">fsdp_state_dict_type:</span> <span class="hljs-string">SHARDED_STATE_DICT</span>
  <span class="hljs-attr">fsdp_sync_module_states:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">fsdp_transformer_layer_cls_to_wrap:</span> <span class="hljs-string">BertLayer</span>
  <span class="hljs-attr">fsdp_use_orig_params:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">machine_rank:</span> <span class="hljs-number">0</span>
<span class="hljs-attr">main_training_function:</span> <span class="hljs-string">main</span>
<span class="hljs-attr">mixed_precision:</span> <span class="hljs-string">bf16</span>
<span class="hljs-attr">num_machines:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">num_processes:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">rdzv_backend:</span> <span class="hljs-string">static</span>
<span class="hljs-attr">same_network:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">tpu_env:</span> []
<span class="hljs-attr">tpu_use_cluster:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">tpu_use_sudo:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">use_cpu:</span> <span class="hljs-literal">false</span>`,wrap:!1}}),I=new fe({props:{title:"Trainer",local:"trainer",headingTag:"h2"}}),_=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQXRyYWluaW5nX2FyZ3MlMjAlM0QlMjBUcmFpbmluZ0FyZ3VtZW50cyglMEElMjAlMjAlMjAlMjBvdXRwdXRfZGlyJTNEJTIyeW91ci1tb2RlbCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX2V2YWxfYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX2Vwb2NocyUzRDIlMkMlMEElMjAlMjAlMjAlMjBmc2RwX2NvbmZpZyUzRCUyMnBhdGglMkZ0byUyRmZzZHBfY29uZmlnJTIyJTJDJTBBJTIwJTIwJTIwJTIwZnNkcCUzRCUyMmZ1bGxfc2hhcmQlMjIlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbG9hZF9iZXN0X21vZGVsX2F0X2VuZCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBwdXNoX3RvX2h1YiUzRFRydWUlMkMlMEEpJTBBJTBBdHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRGRhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcHJvY2Vzc2luZ19jbGFzcyUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">&quot;your-model&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">16</span>,
    per_device_eval_batch_size=<span class="hljs-number">16</span>,
    num_train_epochs=<span class="hljs-number">2</span>,
    fsdp_config=<span class="hljs-string">&quot;path/to/fsdp_config&quot;</span>,
    fsdp=<span class="hljs-string">&quot;full_shard&quot;</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
    eval_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    load_best_model_at_end=<span class="hljs-literal">True</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=dataset[<span class="hljs-string">&quot;test&quot;</span>],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()`,wrap:!1}}),B=new fe({props:{title:"Native PyTorch",local:"native-pytorch",headingTag:"h2"}}),R=new T({props:{code:"ZnJvbSUyMGFjY2VsZXJhdGUlMjBpbXBvcnQlMjBBY2NlbGVyYXRvciUwQSUwQWFjY2VsZXJhdG9yJTIwJTNEJTIwQWNjZWxlcmF0b3IoKSUwQWRldmljZSUyMCUzRCUyMGFjY2VsZXJhdG9yLmRldmljZQ==",highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
device = accelerator.device`,wrap:!1}}),A=new T({props:{code:"dHJhaW5fZGF0YWxvYWRlciUyQyUyMGV2YWxfZGF0YWxvYWRlciUyQyUyMG1vZGVsJTJDJTIwb3B0aW1pemVyJTIwJTNEJTIwYWNjZWxlcmF0b3IucHJlcGFyZSglMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhbG9hZGVyJTJDJTIwZXZhbF9kYXRhbG9hZGVyJTJDJTIwbW9kZWwlMkMlMjBvcHRpbWl6ZXIlMEEp",highlighted:`train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)`,wrap:!1}}),$=new T({props:{code:"Zm9yJTIwZXBvY2glMjBpbiUyMHJhbmdlKG51bV9lcG9jaHMpJTNBJTBBJTIwJTIwJTIwJTIwZm9yJTIwYmF0Y2glMjBpbiUyMHRyYWluX2RhdGFsb2FkZXIlM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb3NzJTIwJTNEJTIwb3V0cHV0cy5sb3NzJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYWNjZWxlcmF0b3IuYmFja3dhcmQobG9zcyklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvcHRpbWl6ZXIuc3RlcCgpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbHJfc2NoZWR1bGVyLnN0ZXAoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG9wdGltaXplci56ZXJvX2dyYWQoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByb2dyZXNzX2Jhci51cGRhdGUoMSk=",highlighted:`<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)`,wrap:!1}}),W=new T({props:{code:"ZnJvbSUyMGFjY2VsZXJhdGUlMjBpbXBvcnQlMjBBY2NlbGVyYXRvciUwQSUyMCUyMCUwQWRlZiUyMG1haW4oKSUzQSUwQSUyMCUyMGFjY2VsZXJhdG9yJTIwJTNEJTIwQWNjZWxlcmF0b3IoKSUwQSUwQSUyMCUyMG1vZGVsJTJDJTIwb3B0aW1pemVyJTJDJTIwdHJhaW5pbmdfZGF0YWxvYWRlciUyQyUyMHNjaGVkdWxlciUyMCUzRCUyMGFjY2VsZXJhdG9yLnByZXBhcmUoJTBBJTIwJTIwJTIwJTIwJTIwJTIwbW9kZWwlMkMlMjBvcHRpbWl6ZXIlMkMlMjB0cmFpbmluZ19kYXRhbG9hZGVyJTJDJTIwc2NoZWR1bGVyJTBBJTIwJTIwKSUwQSUwQSUyMCUyMGZvciUyMGJhdGNoJTIwaW4lMjB0cmFpbmluZ19kYXRhbG9hZGVyJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwb3B0aW1pemVyLnplcm9fZ3JhZCgpJTBBJTIwJTIwJTIwJTIwJTIwJTIwaW5wdXRzJTJDJTIwdGFyZ2V0cyUyMCUzRCUyMGJhdGNoJTBBJTIwJTIwJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKGlucHV0cyklMEElMjAlMjAlMjAlMjAlMjAlMjBsb3NzJTIwJTNEJTIwbG9zc19mdW5jdGlvbihvdXRwdXRzJTJDJTIwdGFyZ2V0cyklMEElMjAlMjAlMjAlMjAlMjAlMjBhY2NlbGVyYXRvci5iYWNrd2FyZChsb3NzKSUwQSUyMCUyMCUyMCUyMCUyMCUyMG9wdGltaXplci5zdGVwKCklMEElMjAlMjAlMjAlMjAlMjAlMjBzY2hlZHVsZXIuc3RlcCgpJTBBJTBBaWYlMjBfX25hbWVfXyUyMCUzRCUzRCUyMCUyMl9fbWFpbl9fJTIyJTNBJTBBJTIwJTIwJTIwJTIwbWFpbigp",highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
  
<span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
  accelerator = Accelerator()

  model, optimizer, training_dataloader, scheduler = accelerator.prepare(
      model, optimizer, training_dataloader, scheduler
  )

  <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> training_dataloader:
      optimizer.zero_grad()
      inputs, targets = batch
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      accelerator.backward(loss)
      optimizer.step()
      scheduler.step()

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    main()`,wrap:!1}}),N=new T({props:{code:"YWNjZWxlcmF0ZSUyMGxhdW5jaCUyMC0tbnVtX3Byb2Nlc3NlcyUzRDIlMjB5b3VyX3NjcmlwdC5weQ==",highlighted:"accelerate launch --num_processes=2 your_script.py",wrap:!1}}),V=new Ve({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md"}}),{c(){J=r("meta"),S=t(),Y=r("p"),H=t(),p(M.$$.fragment),Q=t(),f=r("p"),f.innerHTML=ue,x=t(),y=r("p"),y.innerHTML=je,D=t(),p(u.$$.fragment),P=t(),j=r("p"),j.innerHTML=Ue,L=t(),p(U.$$.fragment),q=t(),b=r("p"),b.textContent=be,K=t(),p(g.$$.fragment),O=t(),p(I.$$.fragment),ee=t(),Z=r("p"),Z.innerHTML=ge,ae=t(),p(_.$$.fragment),se=t(),p(B.$$.fragment),le=t(),G=r("p"),G.innerHTML=Ie,te=t(),p(R.$$.fragment),ne=t(),X=r("p"),X.innerHTML=Ze,re=t(),p(A.$$.fragment),ce=t(),v=r("p"),v.innerHTML=_e,pe=t(),p($.$$.fragment),ie=t(),C=r("p"),C.textContent=Be,oe=t(),p(W.$$.fragment),de=t(),k=r("p"),k.innerHTML=Ge,he=t(),F=r("p"),F.innerHTML=Re,me=t(),p(N.$$.fragment),we=t(),z=r("p"),z.innerHTML=Xe,Je=t(),p(V.$$.fragment),Te=t(),E=r("p"),this.h()},l(e){const a=Ne("svelte-u9bgzb",document.head);J=c(a,"META",{name:!0,content:!0}),a.forEach(s),S=n(e),Y=c(e,"P",{}),Ae(Y).forEach(s),H=n(e),i(M.$$.fragment,e),Q=n(e),f=c(e,"P",{"data-svelte-h":!0}),w(f)!=="svelte-d60n1r"&&(f.innerHTML=ue),x=n(e),y=c(e,"P",{"data-svelte-h":!0}),w(y)!=="svelte-1budo8w"&&(y.innerHTML=je),D=n(e),i(u.$$.fragment,e),P=n(e),j=c(e,"P",{"data-svelte-h":!0}),w(j)!=="svelte-rt6hpx"&&(j.innerHTML=Ue),L=n(e),i(U.$$.fragment,e),q=n(e),b=c(e,"P",{"data-svelte-h":!0}),w(b)!=="svelte-1444ksv"&&(b.textContent=be),K=n(e),i(g.$$.fragment,e),O=n(e),i(I.$$.fragment,e),ee=n(e),Z=c(e,"P",{"data-svelte-h":!0}),w(Z)!=="svelte-1k8gfb1"&&(Z.innerHTML=ge),ae=n(e),i(_.$$.fragment,e),se=n(e),i(B.$$.fragment,e),le=n(e),G=c(e,"P",{"data-svelte-h":!0}),w(G)!=="svelte-1tr0xn7"&&(G.innerHTML=Ie),te=n(e),i(R.$$.fragment,e),ne=n(e),X=c(e,"P",{"data-svelte-h":!0}),w(X)!=="svelte-8mikv3"&&(X.innerHTML=Ze),re=n(e),i(A.$$.fragment,e),ce=n(e),v=c(e,"P",{"data-svelte-h":!0}),w(v)!=="svelte-1ft6z9j"&&(v.innerHTML=_e),pe=n(e),i($.$$.fragment,e),ie=n(e),C=c(e,"P",{"data-svelte-h":!0}),w(C)!=="svelte-1b3w16l"&&(C.textContent=Be),oe=n(e),i(W.$$.fragment,e),de=n(e),k=c(e,"P",{"data-svelte-h":!0}),w(k)!=="svelte-igpm6b"&&(k.innerHTML=Ge),he=n(e),F=c(e,"P",{"data-svelte-h":!0}),w(F)!=="svelte-1wafclc"&&(F.innerHTML=Re),me=n(e),i(N.$$.fragment,e),we=n(e),z=c(e,"P",{"data-svelte-h":!0}),w(z)!=="svelte-1gxoe3z"&&(z.innerHTML=Xe),Je=n(e),i(V.$$.fragment,e),Te=n(e),E=c(e,"P",{}),Ae(E).forEach(s),this.h()},h(){ve(J,"name","hf:doc:metadata"),ve(J,"content",Ee)},m(e,a){ze(document.head,J),l(e,S,a),l(e,Y,a),l(e,H,a),o(M,e,a),l(e,Q,a),l(e,f,a),l(e,x,a),l(e,y,a),l(e,D,a),o(u,e,a),l(e,P,a),l(e,j,a),l(e,L,a),o(U,e,a),l(e,q,a),l(e,b,a),l(e,K,a),o(g,e,a),l(e,O,a),o(I,e,a),l(e,ee,a),l(e,Z,a),l(e,ae,a),o(_,e,a),l(e,se,a),o(B,e,a),l(e,le,a),l(e,G,a),l(e,te,a),o(R,e,a),l(e,ne,a),l(e,X,a),l(e,re,a),o(A,e,a),l(e,ce,a),l(e,v,a),l(e,pe,a),o($,e,a),l(e,ie,a),l(e,C,a),l(e,oe,a),o(W,e,a),l(e,de,a),l(e,k,a),l(e,he,a),l(e,F,a),l(e,me,a),o(N,e,a),l(e,we,a),l(e,z,a),l(e,Je,a),o(V,e,a),l(e,Te,a),l(e,E,a),Me=!0},p:Ce,i(e){Me||(d(M.$$.fragment,e),d(u.$$.fragment,e),d(U.$$.fragment,e),d(g.$$.fragment,e),d(I.$$.fragment,e),d(_.$$.fragment,e),d(B.$$.fragment,e),d(R.$$.fragment,e),d(A.$$.fragment,e),d($.$$.fragment,e),d(W.$$.fragment,e),d(N.$$.fragment,e),d(V.$$.fragment,e),Me=!0)},o(e){h(M.$$.fragment,e),h(u.$$.fragment,e),h(U.$$.fragment,e),h(g.$$.fragment,e),h(I.$$.fragment,e),h(_.$$.fragment,e),h(B.$$.fragment,e),h(R.$$.fragment,e),h(A.$$.fragment,e),h($.$$.fragment,e),h(W.$$.fragment,e),h(N.$$.fragment,e),h(V.$$.fragment,e),Me=!1},d(e){e&&(s(S),s(Y),s(H),s(Q),s(f),s(x),s(y),s(D),s(P),s(j),s(L),s(q),s(b),s(K),s(O),s(ee),s(Z),s(ae),s(se),s(le),s(G),s(te),s(ne),s(X),s(re),s(ce),s(v),s(pe),s(ie),s(C),s(oe),s(de),s(k),s(he),s(F),s(me),s(we),s(z),s(Je),s(Te),s(E)),s(J),m(M,e),m(u,e),m(U,e),m(g,e),m(I,e),m(_,e),m(B,e),m(R,e),m(A,e),m($,e),m(W,e),m(N,e),m(V,e)}}}const Ee='{"title":"Accelerate","local":"accelerate","sections":[{"title":"Trainer","local":"trainer","sections":[],"depth":2},{"title":"Native PyTorch","local":"native-pytorch","sections":[],"depth":2}],"depth":1}';function Se(ye){return We(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pe extends ke{constructor(J){super(),Fe(this,J,Se,Ye,$e,{})}}export{Pe as component};
