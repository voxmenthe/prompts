import{s as kt,o as Zt,n as it}from"../chunks/scheduler.18a86fab.js";import{S as Ct,i as Gt,g as i,s as n,r as h,A as Vt,h as c,f as a,c as o,j as k,x as p,u as g,k as W,y as d,a as s,v as f,d as _,t as b,w as y}from"../chunks/index.98837b22.js";import{T as Dt}from"../chunks/Tip.77304350.js";import{D as de}from"../chunks/Docstring.a1ef7999.js";import{C as ct}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Wt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as me,E as It}from"../chunks/getInferenceSnippets.06c2775f.js";function Xt(Z){let r,T="Example:",u,m,w;return m=new ct({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhY01vZGVsJTJDJTIwRGFjQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMCUyMmRlc2NyaXB0JTJGZGFjXzE2a2h6JTIyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMERhY0NvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjAlMjJkZXNjcmlwdCUyRmRhY18xNmtoeiUyMiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRGFjTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DacModel, DacConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a &quot;descript/dac_16khz&quot; style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DacConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the &quot;descript/dac_16khz&quot; style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DacModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=i("p"),r.textContent=T,u=n(),h(m.$$.fragment)},l(l){r=c(l,"P",{"data-svelte-h":!0}),p(r)!=="svelte-11lpom8"&&(r.textContent=T),u=o(l),g(m.$$.fragment,l)},m(l,j){s(l,r,j),s(l,u,j),f(m,l,j),w=!0},p:it,i(l){w||(_(m.$$.fragment,l),w=!0)},o(l){b(m.$$.fragment,l),w=!1},d(l){l&&(a(r),a(u)),y(m,l)}}}function Bt(Z){let r,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=T},l(u){r=c(u,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=T)},m(u,m){s(u,r,m)},p:it,d(u){u&&a(r)}}}function qt(Z){let r,T="Examples:",u,m,w;return m=new ct({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwRGFjTW9kZWwlMkMlMjBBdXRvUHJvY2Vzc29yJTBBbGlicmlzcGVlY2hfZHVtbXklMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQSUwQW1vZGVsJTIwJTNEJTIwRGFjTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmRlc2NyaXB0JTJGZGFjXzE2a2h6JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmRlc2NyaXB0JTJGZGFjXzE2a2h6JTIyKSUwQWxpYnJpc3BlZWNoX2R1bW15JTIwJTNEJTIwbGlicmlzcGVlY2hfZHVtbXkuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEcHJvY2Vzc29yLnNhbXBsaW5nX3JhdGUpKSUwQWF1ZGlvX3NhbXBsZSUyMCUzRCUyMGxpYnJpc3BlZWNoX2R1bW15JTVCLTElNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHJhd19hdWRpbyUzRGF1ZGlvX3NhbXBsZSUyQyUyMHNhbXBsaW5nX3JhdGUlM0Rwcm9jZXNzb3Iuc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBZW5jb2Rlcl9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZW5jb2RlKGlucHV0cyU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCklMEElMjMlMjBHZXQlMjB0aGUlMjBpbnRlcm1lZGlhdGUlMjBhdWRpbyUyMGNvZGVzJTBBYXVkaW9fY29kZXMlMjAlM0QlMjBlbmNvZGVyX291dHB1dHMuYXVkaW9fY29kZXMlMEElMjMlMjBSZWNvbnN0cnVjdCUyMHRoZSUyMGF1ZGlvJTIwZnJvbSUyMGl0cyUyMHF1YW50aXplZCUyMHJlcHJlc2VudGF0aW9uJTBBYXVkaW9fdmFsdWVzJTIwJTNEJTIwbW9kZWwuZGVjb2RlKGVuY29kZXJfb3V0cHV0cy5xdWFudGl6ZWRfcmVwcmVzZW50YXRpb24pJTBBJTIzJTIwb3IlMjB0aGUlMjBlcXVpdmFsZW50JTIwd2l0aCUyMGElMjBmb3J3YXJkJTIwcGFzcyUwQWF1ZGlvX3ZhbHVlcyUyMCUzRCUyMG1vZGVsKGlucHV0cyU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCkuYXVkaW9fdmFsdWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DacModel, AutoProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = DacModel.from_pretrained(<span class="hljs-string">&quot;descript/dac_16khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;descript/dac_16khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = librispeech_dummy.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=processor.sampling_rate))
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_sample = librispeech_dummy[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_outputs = model.encode(inputs[<span class="hljs-string">&quot;input_values&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get the intermediate audio codes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_codes = encoder_outputs.audio_codes
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Reconstruct the audio from its quantized representation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model.decode(encoder_outputs.quantized_representation)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or the equivalent with a forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model(inputs[<span class="hljs-string">&quot;input_values&quot;</span>]).audio_values`,wrap:!1}}),{c(){r=i("p"),r.textContent=T,u=n(),h(m.$$.fragment)},l(l){r=c(l,"P",{"data-svelte-h":!0}),p(r)!=="svelte-kvfsh7"&&(r.textContent=T),u=o(l),g(m.$$.fragment,l)},m(l,j){s(l,r,j),s(l,u,j),f(m,l,j),w=!0},p:it,i(l){w||(_(m.$$.fragment,l),w=!0)},o(l){b(m.$$.fragment,l),w=!1},d(l){l&&(a(r),a(u)),y(m,l)}}}function Ft(Z){let r,T,u,m,w,l="<em>This model was released on 2023-06-11 and added to Hugging Face Transformers on 2024-08-19.</em>",j,B,Je,C,dt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',$e,q,je,F,mt='The DAC model was proposed in <a href="https://huggingface.co/papers/2306.06546" rel="nofollow">Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN</a> by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.',xe,R,pt="The Descript Audio Codec (DAC) model is a powerful tool for compressing audio data, making it highly efficient for storage and transmission. By compressing 44.1 KHz audio into tokens at just 8kbps bandwidth, the DAC model enables high-quality audio processing while significantly reducing the data footprint. This is particularly useful in scenarios where bandwidth is limited or storage space is at a premium, such as in streaming applications, remote conferencing, and archiving large audio datasets.",ze,H,ut="The abstract from the paper is the following:",Ue,N,ht="<em>Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.</em>",We,E,gt=`This model was contributed by <a href="https://huggingface.co/kamilakesbi" rel="nofollow">Kamil Akesbi</a>.
The original code can be found <a href="https://github.com/descriptinc/descript-audio-codec/tree/main?tab=readme-ov-file" rel="nofollow">here</a>.`,ke,Y,Ze,Q,ft="The Descript Audio Codec (DAC) model is structured into three distinct stages:",Ce,P,_t="<li>Encoder Model: This stage compresses the input audio, reducing its size while retaining essential information.</li> <li>Residual Vector Quantizer (RVQ) Model: Working in tandem with the encoder, this model quantizes the latent codes of the audio, refining the compression and ensuring high-quality reconstruction.</li> <li>Decoder Model: This final stage reconstructs the audio from its compressed form, restoring it to a state that closely resembles the original input.</li>",Ge,S,Ve,L,bt="Here is a quick example of how to encode and decode an audio using this model:",De,A,Ie,K,Xe,J,O,Ye,pe,yt=`This is the configuration class to store the configuration of an <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a>. It is used to instantiate a
Dac model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/descript/dac_16khz" rel="nofollow">descript/dac_16khz</a> architecture.`,Qe,ue,vt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Pe,G,Be,ee,qe,$,te,Se,he,wt="Constructs an Dac feature extractor.",Le,ge,Mt=`This feature extractor inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods.`,Ae,V,ae,Ke,fe,Tt="Main method to featurize and prepare for the model one or several sequence(s).",Fe,se,Re,v,ne,Oe,_e,Jt="The DAC (Descript Audio Codec) model.",et,be,$t=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,tt,ye,jt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,at,ve,oe,st,we,re,nt,x,le,ot,Me,xt='The <a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacModel">DacModel</a> forward method, overrides the <code>__call__</code> special method.',rt,D,lt,I,He,ie,Ne,Te,Ee;return B=new me({props:{title:"DAC",local:"dac",headingTag:"h1"}}),q=new me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Y=new me({props:{title:"Model structure",local:"model-structure",headingTag:"h2"}}),S=new me({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),A=new ct({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwRGFjTW9kZWwlMkMlMjBBdXRvUHJvY2Vzc29yJTBBbGlicmlzcGVlY2hfZHVtbXklMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxpYnJpc3BlZWNoX2Fzcl9kdW1teSUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQSUwQW1vZGVsJTIwJTNEJTIwRGFjTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmRlc2NyaXB0JTJGZGFjXzE2a2h6JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmRlc2NyaXB0JTJGZGFjXzE2a2h6JTIyKSUwQWxpYnJpc3BlZWNoX2R1bW15JTIwJTNEJTIwbGlicmlzcGVlY2hfZHVtbXkuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEcHJvY2Vzc29yLnNhbXBsaW5nX3JhdGUpKSUwQWF1ZGlvX3NhbXBsZSUyMCUzRCUyMGxpYnJpc3BlZWNoX2R1bW15JTVCLTElNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVEJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHJhd19hdWRpbyUzRGF1ZGlvX3NhbXBsZSUyQyUyMHNhbXBsaW5nX3JhdGUlM0Rwcm9jZXNzb3Iuc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBZW5jb2Rlcl9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZW5jb2RlKGlucHV0cyU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCklMEElMjMlMjBHZXQlMjB0aGUlMjBpbnRlcm1lZGlhdGUlMjBhdWRpbyUyMGNvZGVzJTBBYXVkaW9fY29kZXMlMjAlM0QlMjBlbmNvZGVyX291dHB1dHMuYXVkaW9fY29kZXMlMEElMjMlMjBSZWNvbnN0cnVjdCUyMHRoZSUyMGF1ZGlvJTIwZnJvbSUyMGl0cyUyMHF1YW50aXplZCUyMHJlcHJlc2VudGF0aW9uJTBBYXVkaW9fdmFsdWVzJTIwJTNEJTIwbW9kZWwuZGVjb2RlKGVuY29kZXJfb3V0cHV0cy5xdWFudGl6ZWRfcmVwcmVzZW50YXRpb24pJTBBJTIzJTIwb3IlMjB0aGUlMjBlcXVpdmFsZW50JTIwd2l0aCUyMGElMjBmb3J3YXJkJTIwcGFzcyUwQWF1ZGlvX3ZhbHVlcyUyMCUzRCUyMG1vZGVsKGlucHV0cyU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCkuYXVkaW9fdmFsdWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DacModel, AutoProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model = DacModel.from_pretrained(<span class="hljs-string">&quot;descript/dac_16khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;descript/dac_16khz&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_dummy = librispeech_dummy.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=processor.sampling_rate))
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_sample = librispeech_dummy[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_outputs = model.encode(inputs[<span class="hljs-string">&quot;input_values&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get the intermediate audio codes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_codes = encoder_outputs.audio_codes
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Reconstruct the audio from its quantized representation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model.decode(encoder_outputs.quantized_representation)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or the equivalent with a forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_values = model(inputs[<span class="hljs-string">&quot;input_values&quot;</span>]).audio_values`,wrap:!1}}),K=new me({props:{title:"DacConfig",local:"transformers.DacConfig",headingTag:"h2"}}),O=new de({props:{name:"class transformers.DacConfig",anchor:"transformers.DacConfig",parameters:[{name:"encoder_hidden_size",val:" = 64"},{name:"downsampling_ratios",val:" = [2, 4, 8, 8]"},{name:"decoder_hidden_size",val:" = 1536"},{name:"n_codebooks",val:" = 9"},{name:"codebook_size",val:" = 1024"},{name:"codebook_dim",val:" = 8"},{name:"quantizer_dropout",val:" = 0"},{name:"commitment_loss_weight",val:" = 0.25"},{name:"codebook_loss_weight",val:" = 1.0"},{name:"sampling_rate",val:" = 16000"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DacConfig.encoder_hidden_size",description:`<strong>encoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Intermediate representation dimension for the encoder.`,name:"encoder_hidden_size"},{anchor:"transformers.DacConfig.downsampling_ratios",description:`<strong>downsampling_ratios</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 4, 8, 8]</code>) &#x2014;
Ratios for downsampling in the encoder. These are used in reverse order for upsampling in the decoder.`,name:"downsampling_ratios"},{anchor:"transformers.DacConfig.decoder_hidden_size",description:`<strong>decoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1536) &#x2014;
Intermediate representation dimension for the decoder.`,name:"decoder_hidden_size"},{anchor:"transformers.DacConfig.n_codebooks",description:`<strong>n_codebooks</strong> (<code>int</code>, <em>optional</em>, defaults to 9) &#x2014;
Number of codebooks in the VQVAE.`,name:"n_codebooks"},{anchor:"transformers.DacConfig.codebook_size",description:`<strong>codebook_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Number of discrete codes in each codebook.`,name:"codebook_size"},{anchor:"transformers.DacConfig.codebook_dim",description:`<strong>codebook_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Dimension of the codebook vectors. If not defined, uses <code>encoder_hidden_size</code>.`,name:"codebook_dim"},{anchor:"transformers.DacConfig.quantizer_dropout",description:`<strong>quantizer_dropout</strong> (<code>bool</code>, <em>optional</em>, defaults to 0) &#x2014;
Whether to apply dropout to the quantizer.`,name:"quantizer_dropout"},{anchor:"transformers.DacConfig.commitment_loss_weight",description:`<strong>commitment_loss_weight</strong> (float, <em>optional</em>, defaults to 0.25) &#x2014;
Weight of the commitment loss term in the VQVAE loss function.`,name:"commitment_loss_weight"},{anchor:"transformers.DacConfig.codebook_loss_weight",description:`<strong>codebook_loss_weight</strong> (float, <em>optional</em>, defaults to 1.0) &#x2014;
Weight of the codebook loss term in the VQVAE loss function.`,name:"codebook_loss_weight"},{anchor:"transformers.DacConfig.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 16000) &#x2014;
The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/configuration_dac.py#L28"}}),G=new Wt({props:{anchor:"transformers.DacConfig.example",$$slots:{default:[Xt]},$$scope:{ctx:Z}}}),ee=new me({props:{title:"DacFeatureExtractor",local:"transformers.DacFeatureExtractor",headingTag:"h2"}}),te=new de({props:{name:"class transformers.DacFeatureExtractor",anchor:"transformers.DacFeatureExtractor",parameters:[{name:"feature_size",val:": int = 1"},{name:"sampling_rate",val:": int = 16000"},{name:"padding_value",val:": float = 0.0"},{name:"hop_length",val:": int = 512"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DacFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The feature dimension of the extracted features. Use 1 for mono, 2 for stereo.`,name:"feature_size"},{anchor:"transformers.DacFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 16000) &#x2014;
The sampling rate at which the audio waveform should be digitalized, expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.DacFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The value that is used for padding.`,name:"padding_value"},{anchor:"transformers.DacFeatureExtractor.hop_length",description:`<strong>hop_length</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Overlap length between successive windows.`,name:"hop_length"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/feature_extraction_dac.py#L29"}}),ae=new de({props:{name:"__call__",anchor:"transformers.DacFeatureExtractor.__call__",parameters:[{name:"raw_audio",val:": typing.Union[numpy.ndarray, list[float], list[numpy.ndarray], list[list[float]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy, NoneType] = None"},{name:"truncation",val:": typing.Optional[bool] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"sampling_rate",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.DacFeatureExtractor.__call__.raw_audio",description:`<strong>raw_audio</strong> (<code>np.ndarray</code>, <code>list[float]</code>, <code>list[np.ndarray]</code>, <code>list[list[float]]</code>) &#x2014;
The sequence or batch of sequences to be processed. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values. The numpy array must be of shape
<code>(num_samples,)</code> for mono audio (<code>feature_size = 1</code>), or <code>(2, num_samples)</code> for stereo audio
(<code>feature_size = 2</code>).`,name:"raw_audio"},{anchor:"transformers.DacFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DacFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.DacFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.DacFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, default to &#x2018;pt&#x2019;) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DacFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>audio</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/feature_extraction_dac.py#L60"}}),se=new me({props:{title:"DacModel",local:"transformers.DacModel",headingTag:"h2"}}),ne=new de({props:{name:"class transformers.DacModel",anchor:"transformers.DacModel",parameters:[{name:"config",val:": DacConfig"}],parametersDescription:[{anchor:"transformers.DacModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/dac#transformers.DacConfig">DacConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/modeling_dac.py#L558"}}),oe=new de({props:{name:"decode",anchor:"transformers.DacModel.decode",parameters:[{name:"quantized_representation",val:": typing.Optional[torch.Tensor] = None"},{name:"audio_codes",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DacModel.decode.quantized_representation",description:`<strong>quantized_representation</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, dimension, time_steps)</code>, <em>optional</em>) &#x2014;
Quantized continuous representation of input.`,name:"quantized_representation"},{anchor:"transformers.DacModel.decode.audio_codes",description:`<strong>audio_codes</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_codebooks, time_steps)</code>, <em>optional</em>) &#x2014;
The codebook indices for each codebook, representing the quantized discrete
representation of the input. This parameter should be provided if you want
to decode directly from the audio codes (it will overwrite quantized_representation).`,name:"audio_codes"},{anchor:"transformers.DacModel.decode.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return a <code>DacDecoderOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/modeling_dac.py#L602"}}),re=new de({props:{name:"encode",anchor:"transformers.DacModel.encode",parameters:[{name:"input_values",val:": Tensor"},{name:"n_quantizers",val:": typing.Optional[int] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DacModel.encode.input_values",description:"<strong>input_values</strong> (<code>torch.Tensor of shape </code>(batch_size, 1, time_steps)`) &#x2014;\nInput audio data to encode,",name:"input_values"},{anchor:"transformers.DacModel.encode.n_quantizers",description:`<strong>n_quantizers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of quantizers to use. If None, all quantizers are used. Default is None.`,name:"n_quantizers"},{anchor:"transformers.DacModel.encode.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/modeling_dac.py#L575"}}),le=new de({props:{name:"forward",anchor:"transformers.DacModel.forward",parameters:[{name:"input_values",val:": Tensor"},{name:"n_quantizers",val:": typing.Optional[int] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DacModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, 1, time_steps)</code>) &#x2014;
Audio data to encode.`,name:"input_values"},{anchor:"transformers.DacModel.forward.n_quantizers",description:`<strong>n_quantizers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of quantizers to use. If <code>None</code>, all quantizers are used. Default is <code>None</code>.`,name:"n_quantizers"},{anchor:"transformers.DacModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/dac/modeling_dac.py#L635"}}),D=new Dt({props:{$$slots:{default:[Bt]},$$scope:{ctx:Z}}}),I=new Wt({props:{anchor:"transformers.DacModel.forward.example",$$slots:{default:[qt]},$$scope:{ctx:Z}}}),ie=new It({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dac.md"}}),{c(){r=i("meta"),T=n(),u=i("p"),m=n(),w=i("p"),w.innerHTML=l,j=n(),h(B.$$.fragment),Je=n(),C=i("div"),C.innerHTML=dt,$e=n(),h(q.$$.fragment),je=n(),F=i("p"),F.innerHTML=mt,xe=n(),R=i("p"),R.textContent=pt,ze=n(),H=i("p"),H.textContent=ut,Ue=n(),N=i("p"),N.innerHTML=ht,We=n(),E=i("p"),E.innerHTML=gt,ke=n(),h(Y.$$.fragment),Ze=n(),Q=i("p"),Q.textContent=ft,Ce=n(),P=i("ol"),P.innerHTML=_t,Ge=n(),h(S.$$.fragment),Ve=n(),L=i("p"),L.textContent=bt,De=n(),h(A.$$.fragment),Ie=n(),h(K.$$.fragment),Xe=n(),J=i("div"),h(O.$$.fragment),Ye=n(),pe=i("p"),pe.innerHTML=yt,Qe=n(),ue=i("p"),ue.innerHTML=vt,Pe=n(),h(G.$$.fragment),Be=n(),h(ee.$$.fragment),qe=n(),$=i("div"),h(te.$$.fragment),Se=n(),he=i("p"),he.textContent=wt,Le=n(),ge=i("p"),ge.innerHTML=Mt,Ae=n(),V=i("div"),h(ae.$$.fragment),Ke=n(),fe=i("p"),fe.textContent=Tt,Fe=n(),h(se.$$.fragment),Re=n(),v=i("div"),h(ne.$$.fragment),Oe=n(),_e=i("p"),_e.textContent=Jt,et=n(),be=i("p"),be.innerHTML=$t,tt=n(),ye=i("p"),ye.innerHTML=jt,at=n(),ve=i("div"),h(oe.$$.fragment),st=n(),we=i("div"),h(re.$$.fragment),nt=n(),x=i("div"),h(le.$$.fragment),ot=n(),Me=i("p"),Me.innerHTML=xt,rt=n(),h(D.$$.fragment),lt=n(),h(I.$$.fragment),He=n(),h(ie.$$.fragment),Ne=n(),Te=i("p"),this.h()},l(e){const t=Vt("svelte-u9bgzb",document.head);r=c(t,"META",{name:!0,content:!0}),t.forEach(a),T=o(e),u=c(e,"P",{}),k(u).forEach(a),m=o(e),w=c(e,"P",{"data-svelte-h":!0}),p(w)!=="svelte-8eskj5"&&(w.innerHTML=l),j=o(e),g(B.$$.fragment,e),Je=o(e),C=c(e,"DIV",{class:!0,"data-svelte-h":!0}),p(C)!=="svelte-13t8s2t"&&(C.innerHTML=dt),$e=o(e),g(q.$$.fragment,e),je=o(e),F=c(e,"P",{"data-svelte-h":!0}),p(F)!=="svelte-gj1goy"&&(F.innerHTML=mt),xe=o(e),R=c(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-18inimd"&&(R.textContent=pt),ze=o(e),H=c(e,"P",{"data-svelte-h":!0}),p(H)!=="svelte-vfdo9a"&&(H.textContent=ut),Ue=o(e),N=c(e,"P",{"data-svelte-h":!0}),p(N)!=="svelte-105iw56"&&(N.innerHTML=ht),We=o(e),E=c(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-4oznky"&&(E.innerHTML=gt),ke=o(e),g(Y.$$.fragment,e),Ze=o(e),Q=c(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-kluisz"&&(Q.textContent=ft),Ce=o(e),P=c(e,"OL",{"data-svelte-h":!0}),p(P)!=="svelte-gw5kk9"&&(P.innerHTML=_t),Ge=o(e),g(S.$$.fragment,e),Ve=o(e),L=c(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-1itt6cz"&&(L.textContent=bt),De=o(e),g(A.$$.fragment,e),Ie=o(e),g(K.$$.fragment,e),Xe=o(e),J=c(e,"DIV",{class:!0});var z=k(J);g(O.$$.fragment,z),Ye=o(z),pe=c(z,"P",{"data-svelte-h":!0}),p(pe)!=="svelte-pvz8ef"&&(pe.innerHTML=yt),Qe=o(z),ue=c(z,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-1ek1ss9"&&(ue.innerHTML=vt),Pe=o(z),g(G.$$.fragment,z),z.forEach(a),Be=o(e),g(ee.$$.fragment,e),qe=o(e),$=c(e,"DIV",{class:!0});var U=k($);g(te.$$.fragment,U),Se=o(U),he=c(U,"P",{"data-svelte-h":!0}),p(he)!=="svelte-f7d725"&&(he.textContent=wt),Le=o(U),ge=c(U,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-ue5gbv"&&(ge.innerHTML=Mt),Ae=o(U),V=c(U,"DIV",{class:!0});var ce=k(V);g(ae.$$.fragment,ce),Ke=o(ce),fe=c(ce,"P",{"data-svelte-h":!0}),p(fe)!=="svelte-1a6wgfx"&&(fe.textContent=Tt),ce.forEach(a),U.forEach(a),Fe=o(e),g(se.$$.fragment,e),Re=o(e),v=c(e,"DIV",{class:!0});var M=k(v);g(ne.$$.fragment,M),Oe=o(M),_e=c(M,"P",{"data-svelte-h":!0}),p(_e)!=="svelte-1orproz"&&(_e.textContent=Jt),et=o(M),be=c(M,"P",{"data-svelte-h":!0}),p(be)!=="svelte-q52n56"&&(be.innerHTML=$t),tt=o(M),ye=c(M,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-hswkmf"&&(ye.innerHTML=jt),at=o(M),ve=c(M,"DIV",{class:!0});var zt=k(ve);g(oe.$$.fragment,zt),zt.forEach(a),st=o(M),we=c(M,"DIV",{class:!0});var Ut=k(we);g(re.$$.fragment,Ut),Ut.forEach(a),nt=o(M),x=c(M,"DIV",{class:!0});var X=k(x);g(le.$$.fragment,X),ot=o(X),Me=c(X,"P",{"data-svelte-h":!0}),p(Me)!=="svelte-736iax"&&(Me.innerHTML=xt),rt=o(X),g(D.$$.fragment,X),lt=o(X),g(I.$$.fragment,X),X.forEach(a),M.forEach(a),He=o(e),g(ie.$$.fragment,e),Ne=o(e),Te=c(e,"P",{}),k(Te).forEach(a),this.h()},h(){W(r,"name","hf:doc:metadata"),W(r,"content",Rt),W(C,"class","flex flex-wrap space-x-1"),W(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),W(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){d(document.head,r),s(e,T,t),s(e,u,t),s(e,m,t),s(e,w,t),s(e,j,t),f(B,e,t),s(e,Je,t),s(e,C,t),s(e,$e,t),f(q,e,t),s(e,je,t),s(e,F,t),s(e,xe,t),s(e,R,t),s(e,ze,t),s(e,H,t),s(e,Ue,t),s(e,N,t),s(e,We,t),s(e,E,t),s(e,ke,t),f(Y,e,t),s(e,Ze,t),s(e,Q,t),s(e,Ce,t),s(e,P,t),s(e,Ge,t),f(S,e,t),s(e,Ve,t),s(e,L,t),s(e,De,t),f(A,e,t),s(e,Ie,t),f(K,e,t),s(e,Xe,t),s(e,J,t),f(O,J,null),d(J,Ye),d(J,pe),d(J,Qe),d(J,ue),d(J,Pe),f(G,J,null),s(e,Be,t),f(ee,e,t),s(e,qe,t),s(e,$,t),f(te,$,null),d($,Se),d($,he),d($,Le),d($,ge),d($,Ae),d($,V),f(ae,V,null),d(V,Ke),d(V,fe),s(e,Fe,t),f(se,e,t),s(e,Re,t),s(e,v,t),f(ne,v,null),d(v,Oe),d(v,_e),d(v,et),d(v,be),d(v,tt),d(v,ye),d(v,at),d(v,ve),f(oe,ve,null),d(v,st),d(v,we),f(re,we,null),d(v,nt),d(v,x),f(le,x,null),d(x,ot),d(x,Me),d(x,rt),f(D,x,null),d(x,lt),f(I,x,null),s(e,He,t),f(ie,e,t),s(e,Ne,t),s(e,Te,t),Ee=!0},p(e,[t]){const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),G.$set(z);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),D.$set(U);const ce={};t&2&&(ce.$$scope={dirty:t,ctx:e}),I.$set(ce)},i(e){Ee||(_(B.$$.fragment,e),_(q.$$.fragment,e),_(Y.$$.fragment,e),_(S.$$.fragment,e),_(A.$$.fragment,e),_(K.$$.fragment,e),_(O.$$.fragment,e),_(G.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(D.$$.fragment,e),_(I.$$.fragment,e),_(ie.$$.fragment,e),Ee=!0)},o(e){b(B.$$.fragment,e),b(q.$$.fragment,e),b(Y.$$.fragment,e),b(S.$$.fragment,e),b(A.$$.fragment,e),b(K.$$.fragment,e),b(O.$$.fragment,e),b(G.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(ne.$$.fragment,e),b(oe.$$.fragment,e),b(re.$$.fragment,e),b(le.$$.fragment,e),b(D.$$.fragment,e),b(I.$$.fragment,e),b(ie.$$.fragment,e),Ee=!1},d(e){e&&(a(T),a(u),a(m),a(w),a(j),a(Je),a(C),a($e),a(je),a(F),a(xe),a(R),a(ze),a(H),a(Ue),a(N),a(We),a(E),a(ke),a(Ze),a(Q),a(Ce),a(P),a(Ge),a(Ve),a(L),a(De),a(Ie),a(Xe),a(J),a(Be),a(qe),a($),a(Fe),a(Re),a(v),a(He),a(Ne),a(Te)),a(r),y(B,e),y(q,e),y(Y,e),y(S,e),y(A,e),y(K,e),y(O),y(G),y(ee,e),y(te),y(ae),y(se,e),y(ne),y(oe),y(re),y(le),y(D),y(I),y(ie,e)}}}const Rt='{"title":"DAC","local":"dac","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Model structure","local":"model-structure","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"DacConfig","local":"transformers.DacConfig","sections":[],"depth":2},{"title":"DacFeatureExtractor","local":"transformers.DacFeatureExtractor","sections":[],"depth":2},{"title":"DacModel","local":"transformers.DacModel","sections":[],"depth":2}],"depth":1}';function Ht(Z){return Zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class At extends Ct{constructor(r){super(),Gt(this,r,Ht,Ft,kt,{})}}export{At as component};
