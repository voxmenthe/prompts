import{s as me,o as ge,n as de}from"../chunks/scheduler.18a86fab.js";import{S as ue,i as he,g as s,s as o,r as H,A as ce,h as r,f as l,c as i,j as fe,x as f,u as L,k as z,y as ve,a as n,v as P,d as U,t as G,w as Z}from"../chunks/index.98837b22.js";import{T as $e}from"../chunks/Tip.77304350.js";import{C as we}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as K,E as xe}from"../chunks/getInferenceSnippets.06c2775f.js";function Te(W){let a,d='Refer to <a href="t5">T5’s documentation page</a> for all API reference, tips, code examples and notebooks.';return{c(){a=s("p"),a.innerHTML=d},l(p){a=r(p,"P",{"data-svelte-h":!0}),f(a)!=="svelte-10ysmkr"&&(a.innerHTML=d)},m(p,k){n(p,a,k)},p:de,d(p){p&&l(a)}}}function _e(W){let a,d,p,k,u,ee="<em>This model was released on 2020-02-12 and added to Hugging Face Transformers on 2023-06-20.</em>",B,h,R,m,te='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',E,c,J,v,le=`T5v1.1 was released in the <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511" rel="nofollow">google-research/text-to-text-transfer-transformer</a>
repository by Colin Raffel et al. It’s an improved version of the original T5 model.
This model was contributed by <a href="https://huggingface.co/patrickvonplaten" rel="nofollow">patrickvonplaten</a>. The original code can be
found <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511" rel="nofollow">here</a>.`,F,$,S,w,ne="One can directly plug in the weights of T5v1.1 into a T5 model, like so:",I,x,q,T,ae="T5 Version 1.1 includes the following improvements compared to the original T5 model:",D,_,oe=`<li><p>GEGLU activation in the feed-forward hidden layer, rather than ReLU. See <a href="https://huggingface.co/papers/2002.05202" rel="nofollow">this paper</a>.</p></li> <li><p>Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.</p></li> <li><p>Pre-trained on C4 only without mixing in the downstream tasks.</p></li> <li><p>No parameter sharing between the embedding and classifier layer.</p></li> <li><p>“xl” and “xxl” replace “3B” and “11B”. The model shapes are a bit different - larger <code>d_model</code> and smaller
<code>num_heads</code> and <code>d_ff</code>.</p></li>`,N,y,ie=`Note: T5 Version 1.1 was only pre-trained on <a href="https://huggingface.co/datasets/c4" rel="nofollow">C4</a> excluding any supervised
training. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5
model. Since t5v1.1 was pre-trained unsupervisedly, there’s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`,Q,b,se="Google has released the following variants:",V,C,re='<li><p><a href="https://huggingface.co/google/t5-v1_1-small" rel="nofollow">google/t5-v1_1-small</a></p></li> <li><p><a href="https://huggingface.co/google/t5-v1_1-base" rel="nofollow">google/t5-v1_1-base</a></p></li> <li><p><a href="https://huggingface.co/google/t5-v1_1-large" rel="nofollow">google/t5-v1_1-large</a></p></li> <li><p><a href="https://huggingface.co/google/t5-v1_1-xl" rel="nofollow">google/t5-v1_1-xl</a></p></li> <li><p><a href="https://huggingface.co/google/t5-v1_1-xxl" rel="nofollow">google/t5-v1_1-xxl</a>.</p></li>',A,g,O,M,X,j,Y;return h=new K({props:{title:"T5v1.1",local:"t5v11",headingTag:"h1"}}),c=new K({props:{title:"Overview",local:"overview",headingTag:"h2"}}),$=new K({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),x=new we({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFQ1Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBUNUZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdDUtdjFfMS1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> T5ForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>model = T5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/t5-v1_1-base&quot;</span>)`,wrap:!1}}),g=new $e({props:{$$slots:{default:[Te]},$$scope:{ctx:W}}}),M=new xe({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5v1.1.md"}}),{c(){a=s("meta"),d=o(),p=s("p"),k=o(),u=s("p"),u.innerHTML=ee,B=o(),H(h.$$.fragment),R=o(),m=s("div"),m.innerHTML=te,E=o(),H(c.$$.fragment),J=o(),v=s("p"),v.innerHTML=le,F=o(),H($.$$.fragment),S=o(),w=s("p"),w.textContent=ne,I=o(),H(x.$$.fragment),q=o(),T=s("p"),T.textContent=ae,D=o(),_=s("ul"),_.innerHTML=oe,N=o(),y=s("p"),y.innerHTML=ie,Q=o(),b=s("p"),b.textContent=se,V=o(),C=s("ul"),C.innerHTML=re,A=o(),H(g.$$.fragment),O=o(),H(M.$$.fragment),X=o(),j=s("p"),this.h()},l(e){const t=ce("svelte-u9bgzb",document.head);a=r(t,"META",{name:!0,content:!0}),t.forEach(l),d=i(e),p=r(e,"P",{}),fe(p).forEach(l),k=i(e),u=r(e,"P",{"data-svelte-h":!0}),f(u)!=="svelte-iucyxw"&&(u.innerHTML=ee),B=i(e),L(h.$$.fragment,e),R=i(e),m=r(e,"DIV",{class:!0,"data-svelte-h":!0}),f(m)!=="svelte-13t8s2t"&&(m.innerHTML=te),E=i(e),L(c.$$.fragment,e),J=i(e),v=r(e,"P",{"data-svelte-h":!0}),f(v)!=="svelte-1s19v3y"&&(v.innerHTML=le),F=i(e),L($.$$.fragment,e),S=i(e),w=r(e,"P",{"data-svelte-h":!0}),f(w)!=="svelte-vf87kd"&&(w.textContent=ne),I=i(e),L(x.$$.fragment,e),q=i(e),T=r(e,"P",{"data-svelte-h":!0}),f(T)!=="svelte-x07s8t"&&(T.textContent=ae),D=i(e),_=r(e,"UL",{"data-svelte-h":!0}),f(_)!=="svelte-tpmfce"&&(_.innerHTML=oe),N=i(e),y=r(e,"P",{"data-svelte-h":!0}),f(y)!=="svelte-7k8pn0"&&(y.innerHTML=ie),Q=i(e),b=r(e,"P",{"data-svelte-h":!0}),f(b)!=="svelte-1p0jqca"&&(b.textContent=se),V=i(e),C=r(e,"UL",{"data-svelte-h":!0}),f(C)!=="svelte-1rz8hcf"&&(C.innerHTML=re),A=i(e),L(g.$$.fragment,e),O=i(e),L(M.$$.fragment,e),X=i(e),j=r(e,"P",{}),fe(j).forEach(l),this.h()},h(){z(a,"name","hf:doc:metadata"),z(a,"content",ye),z(m,"class","flex flex-wrap space-x-1")},m(e,t){ve(document.head,a),n(e,d,t),n(e,p,t),n(e,k,t),n(e,u,t),n(e,B,t),P(h,e,t),n(e,R,t),n(e,m,t),n(e,E,t),P(c,e,t),n(e,J,t),n(e,v,t),n(e,F,t),P($,e,t),n(e,S,t),n(e,w,t),n(e,I,t),P(x,e,t),n(e,q,t),n(e,T,t),n(e,D,t),n(e,_,t),n(e,N,t),n(e,y,t),n(e,Q,t),n(e,b,t),n(e,V,t),n(e,C,t),n(e,A,t),P(g,e,t),n(e,O,t),P(M,e,t),n(e,X,t),n(e,j,t),Y=!0},p(e,[t]){const pe={};t&2&&(pe.$$scope={dirty:t,ctx:e}),g.$set(pe)},i(e){Y||(U(h.$$.fragment,e),U(c.$$.fragment,e),U($.$$.fragment,e),U(x.$$.fragment,e),U(g.$$.fragment,e),U(M.$$.fragment,e),Y=!0)},o(e){G(h.$$.fragment,e),G(c.$$.fragment,e),G($.$$.fragment,e),G(x.$$.fragment,e),G(g.$$.fragment,e),G(M.$$.fragment,e),Y=!1},d(e){e&&(l(d),l(p),l(k),l(u),l(B),l(R),l(m),l(E),l(J),l(v),l(F),l(S),l(w),l(I),l(q),l(T),l(D),l(_),l(N),l(y),l(Q),l(b),l(V),l(C),l(A),l(O),l(X),l(j)),l(a),Z(h,e),Z(c,e),Z($,e),Z(x,e),Z(g,e),Z(M,e)}}}const ye='{"title":"T5v1.1","local":"t5v11","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2}],"depth":1}';function be(W){return ge(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pe extends ue{constructor(a){super(),he(this,a,be,_e,me,{})}}export{Pe as component};
