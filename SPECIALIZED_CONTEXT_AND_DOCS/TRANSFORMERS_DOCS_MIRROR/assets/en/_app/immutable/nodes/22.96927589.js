import{s as $,n as N,o as Q}from"../chunks/scheduler.18a86fab.js";import{S,i as z,g as d,s as o,r as E,A,h as u,f as a,c as n,j as F,u as x,x as k,k as C,y as q,a as l,v as I,d as R,t as X,w as W}from"../chunks/index.98837b22.js";import{C as H}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as L,E as P}from"../chunks/getInferenceSnippets.06c2775f.js";function D(B){let s,J,y,f,r,b,c,V='<a href="https://pytorch.org/executorch/stable/index.html" rel="nofollow">ExecuTorch</a> is a platform that enables PyTorch training and inference programs to be run on mobile and edge devices. It is powered by <a href="https://pytorch.org/docs/stable/torch.compiler.html" rel="nofollow">torch.compile</a> and <a href="https://pytorch.org/docs/main/export.html" rel="nofollow">torch.export</a> for performance and deployment.',w,p,Y='You can use ExecuTorch with Transformers with <a href="https://pytorch.org/docs/main/export.html" rel="nofollow">torch.export</a>. The <a href="/docs/transformers/v4.56.2/en/main_classes/executorch#transformers.convert_and_export_with_cache">convert_and_export_with_cache()</a> method converts a <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> into an exportable module. Under the hood, it uses <a href="https://pytorch.org/docs/main/export.html" rel="nofollow">torch.export</a> to export the model, ensuring compatibility with ExecuTorch.',j,i,U,h,v='The exported PyTorch model is now ready to be used with ExecuTorch. Wrap the model with <a href="/docs/transformers/v4.56.2/en/main_classes/executorch#transformers.TorchExportableModuleWithStaticCache">TorchExportableModuleWithStaticCache</a> to generate text.',_,m,G,M,Z,T,g;return r=new L({props:{title:"ExecuTorch",local:"executorch",headingTag:"h1"}}),i=new H({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwTGxhbWFGb3JDYXVzYWxMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHZW5lcmF0aW9uQ29uZmlnJTBBZnJvbSUyMHRyYW5zZm9ybWVycy5pbnRlZ3JhdGlvbnMuZXhlY3V0b3JjaCUyMGltcG9ydCglMEElMjAlMjAlMjAlMjBUb3JjaEV4cG9ydGFibGVNb2R1bGVXaXRoU3RhdGljQ2FjaGUlMkMlMEElMjAlMjAlMjAlMjBjb252ZXJ0X2FuZF9leHBvcnRfd2l0aF9jYWNoZSUwQSklMEElMEFnZW5lcmF0aW9uX2NvbmZpZyUyMCUzRCUyMEdlbmVyYXRpb25Db25maWcoJTBBJTIwJTIwJTIwJTIwdXNlX2NhY2hlJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIyc3RhdGljJTIyJTJDJTBBJTIwJTIwJTIwJTIwY2FjaGVfY29uZmlnJTNEJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyYmF0Y2hfc2l6ZSUyMiUzQSUyMDElMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJtYXhfY2FjaGVfbGVuJTIyJTNBJTIwMjAlMkMlMEElMjAlMjAlMjAlMjAlN0QlMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMi0xQiUyMiUyQyUyMHBhZF90b2tlbiUzRCUyMiUzQyUyRnMlM0UlMjIlMkMlMjBwYWRkaW5nX3NpZGUlM0QlMjJyaWdodCUyMiklMEFtb2RlbCUyMCUzRCUyMExsYW1hRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0zLjItMUIlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMjBhdHRuX2ltcGxlbWVudGF0aW9uJTNEJTIyc2RwYSUyMiUyQyUyMGdlbmVyYXRpb25fY29uZmlnJTNEZ2VuZXJhdGlvbl9jb25maWcpJTBBJTBBZXhwb3J0ZWRfcHJvZ3JhbSUyMCUzRCUyMGNvbnZlcnRfYW5kX2V4cG9ydF93aXRoX2NhY2hlKG1vZGVsKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaForCausalLM, AutoTokenizer, GenerationConfig
<span class="hljs-keyword">from</span> transformers.integrations.executorch <span class="hljs-keyword">import</span>(
    TorchExportableModuleWithStaticCache,
    convert_and_export_with_cache
)

generation_config = GenerationConfig(
    use_cache=<span class="hljs-literal">True</span>,
    cache_implementation=<span class="hljs-string">&quot;static&quot;</span>,
    cache_config={
        <span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">1</span>,
        <span class="hljs-string">&quot;max_cache_len&quot;</span>: <span class="hljs-number">20</span>,
    }
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.2-1B&quot;</span>, pad_token=<span class="hljs-string">&quot;&lt;/s&gt;&quot;</span>, padding_side=<span class="hljs-string">&quot;right&quot;</span>)
model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-3.2-1B&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, dtype=torch.bfloat16, attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>, generation_config=generation_config)

exported_program = convert_and_export_with_cache(model)`,wrap:!1}}),m=new H({props:{code:"cHJvbXB0cyUyMCUzRCUyMCU1QiUyMlNpbXBseSUyMHB1dCUyQyUyMHRoZSUyMHRoZW9yeSUyMG9mJTIwcmVsYXRpdml0eSUyMHN0YXRlcyUyMHRoYXQlMjAlMjIlNUQlMEFwcm9tcHRfdG9rZW5zJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdHMlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUpLnRvKG1vZGVsLmRldmljZSklMEFwcm9tcHRfdG9rZW5faWRzJTIwJTNEJTIwcHJvbXB0X3Rva2VucyU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBUb3JjaEV4cG9ydGFibGVNb2R1bGVXaXRoU3RhdGljQ2FjaGUuZ2VuZXJhdGUoJTBBJTIwJTIwJTIwJTIwZXhwb3J0ZWRfcHJvZ3JhbSUzRGV4cG9ydGVkX3Byb2dyYW0lMkMlMjBwcm9tcHRfdG9rZW5faWRzJTNEcHJvbXB0X3Rva2VuX2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMjAlMkMlMEEpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFwcmludChnZW5lcmF0ZWRfdGV4dCklMEElNUInU2ltcGx5JTIwcHV0JTJDJTIwdGhlJTIwdGhlb3J5JTIwb2YlMjByZWxhdGl2aXR5JTIwc3RhdGVzJTIwdGhhdCUyMDEpJTIwdGhlJTIwc3BlZWQlMjBvZiUyMGxpZ2h0JTIwaXMlMjB0aGUnJTVE",highlighted:`prompts = [<span class="hljs-string">&quot;Simply put, the theory of relativity states that &quot;</span>]
prompt_tokens = tokenizer(prompts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>).to(model.device)
prompt_token_ids = prompt_tokens[<span class="hljs-string">&quot;input_ids&quot;</span>]

generated_ids = TorchExportableModuleWithStaticCache.generate(
    exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=<span class="hljs-number">20</span>,
)
generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(generated_text)
[<span class="hljs-string">&#x27;Simply put, the theory of relativity states that 1) the speed of light is the&#x27;</span>]`,wrap:!1}}),M=new P({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/executorch.md"}}),{c(){s=d("meta"),J=o(),y=d("p"),f=o(),E(r.$$.fragment),b=o(),c=d("p"),c.innerHTML=V,w=o(),p=d("p"),p.innerHTML=Y,j=o(),E(i.$$.fragment),U=o(),h=d("p"),h.innerHTML=v,_=o(),E(m.$$.fragment),G=o(),E(M.$$.fragment),Z=o(),T=d("p"),this.h()},l(t){const e=A("svelte-u9bgzb",document.head);s=u(e,"META",{name:!0,content:!0}),e.forEach(a),J=n(t),y=u(t,"P",{}),F(y).forEach(a),f=n(t),x(r.$$.fragment,t),b=n(t),c=u(t,"P",{"data-svelte-h":!0}),k(c)!=="svelte-uwjuic"&&(c.innerHTML=V),w=n(t),p=u(t,"P",{"data-svelte-h":!0}),k(p)!=="svelte-knhh82"&&(p.innerHTML=Y),j=n(t),x(i.$$.fragment,t),U=n(t),h=u(t,"P",{"data-svelte-h":!0}),k(h)!=="svelte-1ur85mq"&&(h.innerHTML=v),_=n(t),x(m.$$.fragment,t),G=n(t),x(M.$$.fragment,t),Z=n(t),T=u(t,"P",{}),F(T).forEach(a),this.h()},h(){C(s,"name","hf:doc:metadata"),C(s,"content",K)},m(t,e){q(document.head,s),l(t,J,e),l(t,y,e),l(t,f,e),I(r,t,e),l(t,b,e),l(t,c,e),l(t,w,e),l(t,p,e),l(t,j,e),I(i,t,e),l(t,U,e),l(t,h,e),l(t,_,e),I(m,t,e),l(t,G,e),I(M,t,e),l(t,Z,e),l(t,T,e),g=!0},p:N,i(t){g||(R(r.$$.fragment,t),R(i.$$.fragment,t),R(m.$$.fragment,t),R(M.$$.fragment,t),g=!0)},o(t){X(r.$$.fragment,t),X(i.$$.fragment,t),X(m.$$.fragment,t),X(M.$$.fragment,t),g=!1},d(t){t&&(a(J),a(y),a(f),a(b),a(c),a(w),a(p),a(j),a(U),a(h),a(_),a(G),a(Z),a(T)),a(s),W(r,t),W(i,t),W(m,t),W(M,t)}}}const K='{"title":"ExecuTorch","local":"executorch","sections":[],"depth":1}';function O(B){return Q(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class st extends S{constructor(s){super(),z(this,s,O,D,$,{})}}export{st as component};
