import{s as go,o as To,n as B}from"../chunks/scheduler.18a86fab.js";import{S as _o,i as bo,g as p,s as r,r as g,A as Mo,h as m,f as s,c as l,j as A,x as f,u as T,k as C,l as yo,y as i,a as d,v as _,d as b,t as M,w as y}from"../chunks/index.98837b22.js";import{T as Tt}from"../chunks/Tip.77304350.js";import{D as V}from"../chunks/Docstring.a1ef7999.js";import{C as pe}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as _t}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Q,E as wo}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as vo,a as zn}from"../chunks/HfOption.6641485e.js";function ko(v){let t,u="Click on the GPT models in the right sidebar for more examples of how to apply GPT to different language tasks.";return{c(){t=p("p"),t.textContent=u},l(n){t=m(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-3bi4th"&&(t.textContent=u)},m(n,c){d(n,t,c)},p:B,d(n){n&&s(t)}}}function $o(v){let t,u;return t=new pe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFnZW5lcmF0b3IlMjAlM0QlMjBwaXBlbGluZSh0YXNrJTNEJTIydGV4dC1nZW5lcmF0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJvcGVuYWktY29tbXVuaXR5JTJGZ3B0JTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlJTNEMCklMEFvdXRwdXQlMjAlM0QlMjBnZW5lcmF0b3IoJTIyVGhlJTIwZnV0dXJlJTIwb2YlMjBBSSUyMGlzJTIyJTJDJTIwbWF4X2xlbmd0aCUzRDUwJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSklMEFwcmludChvdXRwdXQlNUIwJTVEJTVCJTIyZ2VuZXJhdGVkX3RleHQlMjIlNUQp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

generator = pipeline(task=<span class="hljs-string">&quot;text-generation&quot;</span>, model=<span class="hljs-string">&quot;openai-community/gpt&quot;</span>, dtype=torch.float16, device=<span class="hljs-number">0</span>)
output = generator(<span class="hljs-string">&quot;The future of AI is&quot;</span>, max_length=<span class="hljs-number">50</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>])`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){T(t.$$.fragment,n)},m(n,c){_(t,n,c),u=!0},p:B,i(n){u||(b(t.$$.fragment,n),u=!0)},o(n){M(t.$$.fragment,n),u=!1},d(n){y(t,n)}}}function Io(v){let t,u;return t=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZncHQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpLWNvbW11bml0eSUyRm9wZW5haS1ncHQlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMlRoZSUyMGZ1dHVyZSUyMG9mJTIwQUklMjBpcyUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwbWF4X2xlbmd0aCUzRDUwKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>, dtype=torch.float16)

inputs = tokenizer(<span class="hljs-string">&quot;The future of AI is&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model.generate(**inputs, max_length=<span class="hljs-number">50</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){T(t.$$.fragment,n)},m(n,c){_(t,n,c),u=!0},p:B,i(n){u||(b(t.$$.fragment,n),u=!0)},o(n){M(t.$$.fragment,n),u=!1},d(n){y(t,n)}}}function Go(v){let t,u;return t=new pe({props:{code:"ZWNobyUyMC1lJTIwJTIyVGhlJTIwZnV0dXJlJTIwb2YlMjBBSSUyMGlzJTIyJTIwJTdDJTIwdHJhbnNmb3JtZXJzJTIwcnVuJTIwLS10YXNrJTIwdGV4dC1nZW5lcmF0aW9uJTIwLS1tb2RlbCUyMG9wZW5haS1jb21tdW5pdHklMkZvcGVuYWktZ3B0JTIwLS1kZXZpY2UlMjAwJTBB",highlighted:`<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;The future of AI is&quot;</span> | transformers run --task text-generation --model openai-community/openai-gpt --device 0
`,wrap:!1}}),{c(){g(t.$$.fragment)},l(n){T(t.$$.fragment,n)},m(n,c){_(t,n,c),u=!0},p:B,i(n){u||(b(t.$$.fragment,n),u=!0)},o(n){M(t.$$.fragment,n),u=!1},d(n){y(t,n)}}}function Jo(v){let t,u,n,c,w,a;return t=new zn({props:{id:"usage",option:"Pipeline",$$slots:{default:[$o]},$$scope:{ctx:v}}}),n=new zn({props:{id:"usage",option:"AutoModel",$$slots:{default:[Io]},$$scope:{ctx:v}}}),w=new zn({props:{id:"usage",option:"transformers CLI",$$slots:{default:[Go]},$$scope:{ctx:v}}}),{c(){g(t.$$.fragment),u=r(),g(n.$$.fragment),c=r(),g(w.$$.fragment)},l(h){T(t.$$.fragment,h),u=l(h),T(n.$$.fragment,h),c=l(h),T(w.$$.fragment,h)},m(h,k){_(t,h,k),d(h,u,k),_(n,h,k),d(h,c,k),_(w,h,k),a=!0},p(h,k){const ft={};k&2&&(ft.$$scope={dirty:k,ctx:h}),t.$set(ft);const me={};k&2&&(me.$$scope={dirty:k,ctx:h}),n.$set(me);const R={};k&2&&(R.$$scope={dirty:k,ctx:h}),w.$set(R)},i(h){a||(b(t.$$.fragment,h),b(n.$$.fragment,h),b(w.$$.fragment,h),a=!0)},o(h){M(t.$$.fragment,h),M(n.$$.fragment,h),M(w.$$.fragment,h),a=!1},d(h){h&&(s(u),s(c)),y(t,h),y(n,h),y(w,h)}}}function Po(v){let t,u="Examples:",n,c,w;return c=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME9wZW5BSUdQVENvbmZpZyUyQyUyME9wZW5BSUdQVE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdQVCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwT3BlbkFJR1BUQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME9wZW5BSUdQVE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OpenAIGPTConfig, OpenAIGPTModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OpenAIGPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=p("p"),t.textContent=u,n=r(),g(c.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=u),n=l(a),T(c.$$.fragment,a)},m(a,h){d(a,t,h),d(a,n,h),_(c,a,h),w=!0},p:B,i(a){w||(b(c.$$.fragment,a),w=!0)},o(a){M(c.$$.fragment,a),w=!1},d(a){a&&(s(t),s(n)),y(c,a)}}}function jo(v){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=u},l(n){t=m(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(n,c){d(n,t,c)},p:B,d(n){n&&s(t)}}}function Ao(v){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=u},l(n){t=m(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(n,c){d(n,t,c)},p:B,d(n){n&&s(t)}}}function Co(v){let t,u="Example:",n,c,w;return c=new pe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyME9wZW5BSUdQVExNSGVhZE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpLWNvbW11bml0eSUyRm9wZW5haS1ncHQlMjIpJTBBbW9kZWwlMjAlM0QlMjBPcGVuQUlHUFRMTUhlYWRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpLWNvbW11bml0eSUyRm9wZW5haS1ncHQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGlucHV0cyU1QiUyMmlucHV0X2lkcyUyMiU1RCklMEFsb3NzJTIwJTNEJTIwb3V0cHV0cy5sb3NzJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenAIGPTLMHeadModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTLMHeadModel.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=p("p"),t.textContent=u,n=r(),g(c.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=u),n=l(a),T(c.$$.fragment,a)},m(a,h){d(a,t,h),d(a,n,h),_(c,a,h),w=!0},p:B,i(a){w||(b(c.$$.fragment,a),w=!0)},o(a){M(c.$$.fragment,a),w=!1},d(a){a&&(s(t),s(n)),y(c,a)}}}function xo(v){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=u},l(n){t=m(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(n,c){d(n,t,c)},p:B,d(n){n&&s(t)}}}function zo(v){let t,u="Examples:",n,c,w;return c=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBPcGVuQUlHUFREb3VibGVIZWFkc01vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGb3BlbmFpLWdwdCUyMiklMEFtb2RlbCUyMCUzRCUyME9wZW5BSUdQVERvdWJsZUhlYWRzTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZvcGVuYWktZ3B0JTIyKSUwQXRva2VuaXplci5hZGRfc3BlY2lhbF90b2tlbnMoJTBBJTIwJTIwJTIwJTIwJTdCJTIyY2xzX3Rva2VuJTIyJTNBJTIwJTIyJTVCQ0xTJTVEJTIyJTdEJTBBKSUyMCUyMCUyMyUyMEFkZCUyMGElMjAlNUJDTFMlNUQlMjB0byUyMHRoZSUyMHZvY2FidWxhcnklMjAod2UlMjBzaG91bGQlMjB0cmFpbiUyMGl0JTIwYWxzbyEpJTBBbW9kZWwucmVzaXplX3Rva2VuX2VtYmVkZGluZ3MobGVuKHRva2VuaXplcikpJTBBJTBBY2hvaWNlcyUyMCUzRCUyMCU1QiUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjAlNUJDTFMlNUQlMjIlMkMlMjAlMjJIZWxsbyUyQyUyMG15JTIwY2F0JTIwaXMlMjBjdXRlJTIwJTVCQ0xTJTVEJTIyJTVEJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QnRva2VuaXplci5lbmNvZGUocyklMjBmb3IlMjBzJTIwaW4lMjBjaG9pY2VzJTVEKS51bnNxdWVlemUoMCklMjAlMjAlMjMlMjBCYXRjaCUyMHNpemUlMjAxJTJDJTIwMiUyMGNob2ljZXMlMEFtY190b2tlbl9pZHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCaW5wdXRfaWRzLnNpemUoLTEpJTIwLSUyMDElMkMlMjBpbnB1dF9pZHMuc2l6ZSgtMSklMjAtJTIwMSU1RCkudW5zcXVlZXplKDApJTIwJTIwJTIzJTIwQmF0Y2glMjBzaXplJTIwMSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMlMkMlMjBtY190b2tlbl9pZHMlM0RtY190b2tlbl9pZHMpJTBBbG1fbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEFtY19sb2dpdHMlMjAlM0QlMjBvdXRwdXRzLm1jX2xvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenAIGPTDoubleHeadsModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTDoubleHeadsModel.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.add_special_tokens(
<span class="hljs-meta">... </span>    {<span class="hljs-string">&quot;cls_token&quot;</span>: <span class="hljs-string">&quot;[CLS]&quot;</span>}
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># Add a [CLS] to the vocabulary (we should train it also!)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))

<span class="hljs-meta">&gt;&gt;&gt; </span>choices = [<span class="hljs-string">&quot;Hello, my dog is cute [CLS]&quot;</span>, <span class="hljs-string">&quot;Hello, my cat is cute [CLS]&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(s) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> choices]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1, 2 choices</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mc_token_ids = torch.tensor([input_ids.size(-<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>, input_ids.size(-<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids, mc_token_ids=mc_token_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>lm_logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span>mc_logits = outputs.mc_logits`,wrap:!1}}),{c(){t=p("p"),t.textContent=u,n=r(),g(c.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=u),n=l(a),T(c.$$.fragment,a)},m(a,h){d(a,t,h),d(a,n,h),_(c,a,h),w=!0},p:B,i(a){w||(b(c.$$.fragment,a),w=!0)},o(a){M(c.$$.fragment,a),w=!1},d(a){a&&(s(t),s(n)),y(c,a)}}}function Oo(v){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=u},l(n){t=m(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(n,c){d(n,t,c)},p:B,d(n){n&&s(t)}}}function Ho(v){let t,u="Example of single-label classification:",n,c,w;return c=new pe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyME9wZW5BSUdQVEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGb3BlbmFpLWdwdCUyMiklMEFtb2RlbCUyMCUzRCUyME9wZW5BSUdQVEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZvcGVuYWktZ3B0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQXByZWRpY3RlZF9jbGFzc19pZCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoKS5pdGVtKCklMEFtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfY2xhc3NfaWQlNUQlMEElMEElMjMlMjBUbyUyMHRyYWluJTIwYSUyMG1vZGVsJTIwb24lMjAlNjBudW1fbGFiZWxzJTYwJTIwY2xhc3NlcyUyQyUyMHlvdSUyMGNhbiUyMHBhc3MlMjAlNjBudW1fbGFiZWxzJTNEbnVtX2xhYmVscyU2MCUyMHRvJTIwJTYwLmZyb21fcHJldHJhaW5lZCguLi4pJTYwJTBBbnVtX2xhYmVscyUyMCUzRCUyMGxlbihtb2RlbC5jb25maWcuaWQybGFiZWwpJTBBbW9kZWwlMjAlM0QlMjBPcGVuQUlHUFRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGb3BlbmFpLWdwdCUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzKSUwQSUwQWxhYmVscyUyMCUzRCUyMHRvcmNoLnRlbnNvciglNUIxJTVEKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenAIGPTForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
...

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
...`,wrap:!1}}),{c(){t=p("p"),t.textContent=u,n=r(),g(c.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),f(t)!=="svelte-ykxpe4"&&(t.textContent=u),n=l(a),T(c.$$.fragment,a)},m(a,h){d(a,t,h),d(a,n,h),_(c,a,h),w=!0},p:B,i(a){w||(b(c.$$.fragment,a),w=!0)},o(a){M(c.$$.fragment,a),w=!1},d(a){a&&(s(t),s(n)),y(c,a)}}}function Uo(v){let t,u="Example of multi-label classification:",n,c,w;return c=new pe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyME9wZW5BSUdQVEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGb3BlbmFpLWdwdCUyMiklMEFtb2RlbCUyMCUzRCUyME9wZW5BSUdQVEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZvcGVuYWktZ3B0JTIyJTJDJTIwcHJvYmxlbV90eXBlJTNEJTIybXVsdGlfbGFiZWxfY2xhc3NpZmljYXRpb24lMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBcHJlZGljdGVkX2NsYXNzX2lkcyUyMCUzRCUyMHRvcmNoLmFyYW5nZSgwJTJDJTIwbG9naXRzLnNoYXBlJTVCLTElNUQpJTVCdG9yY2guc2lnbW9pZChsb2dpdHMpLnNxdWVlemUoZGltJTNEMCklMjAlM0UlMjAwLjUlNUQlMEElMEElMjMlMjBUbyUyMHRyYWluJTIwYSUyMG1vZGVsJTIwb24lMjAlNjBudW1fbGFiZWxzJTYwJTIwY2xhc3NlcyUyQyUyMHlvdSUyMGNhbiUyMHBhc3MlMjAlNjBudW1fbGFiZWxzJTNEbnVtX2xhYmVscyU2MCUyMHRvJTIwJTYwLmZyb21fcHJldHJhaW5lZCguLi4pJTYwJTBBbnVtX2xhYmVscyUyMCUzRCUyMGxlbihtb2RlbC5jb25maWcuaWQybGFiZWwpJTBBbW9kZWwlMjAlM0QlMjBPcGVuQUlHUFRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWktY29tbXVuaXR5JTJGb3BlbmFpLWdwdCUyMiUyQyUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTJDJTIwcHJvYmxlbV90eXBlJTNEJTIybXVsdGlfbGFiZWxfY2xhc3NpZmljYXRpb24lMjIlMEEpJTBBJTBBbGFiZWxzJTIwJTNEJTIwdG9yY2guc3VtKCUwQSUyMCUyMCUyMCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwub25lX2hvdChwcmVkaWN0ZWRfY2xhc3NfaWRzJTVCTm9uZSUyQyUyMCUzQSU1RC5jbG9uZSgpJTJDJTIwbnVtX2NsYXNzZXMlM0RudW1fbGFiZWxzKSUyQyUyMGRpbSUzRDElMEEpLnRvKHRvcmNoLmZsb2F0KSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscykubG9zcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenAIGPTForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.arange(<span class="hljs-number">0</span>, logits.shape[-<span class="hljs-number">1</span>])[torch.sigmoid(logits).squeeze(dim=<span class="hljs-number">0</span>) &gt; <span class="hljs-number">0.5</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenAIGPTForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;openai-community/openai-gpt&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.<span class="hljs-built_in">sum</span>(
<span class="hljs-meta">... </span>    torch.nn.functional.one_hot(predicted_class_ids[<span class="hljs-literal">None</span>, :].clone(), num_classes=num_labels), dim=<span class="hljs-number">1</span>
<span class="hljs-meta">... </span>).to(torch.<span class="hljs-built_in">float</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss`,wrap:!1}}),{c(){t=p("p"),t.textContent=u,n=r(),g(c.$$.fragment)},l(a){t=m(a,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1l8e32d"&&(t.textContent=u),n=l(a),T(c.$$.fragment,a)},m(a,h){d(a,t,h),d(a,n,h),_(c,a,h),w=!0},p:B,i(a){w||(b(c.$$.fragment,a),w=!0)},o(a){M(c.$$.fragment,a),w=!1},d(a){a&&(s(t),s(n)),y(c,a)}}}function Wo(v){let t,u,n,c,w,a="<em>This model was released on 2018-06-11 and added to Hugging Face Transformers on 2023-06-20.</em>",h,k,ft='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/></div>',me,R,bt,ue,On='<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">GPT (Generative Pre-trained Transformer)</a> (<a href="https://openai.com/index/language-unsupervised/" rel="nofollow">blog post</a>) focuses on effectively learning text representations and transferring them to tasks. This model trains the Transformer decoder to predict the next word, and then fine-tuned on labeled data.',Mt,he,Hn="GPT can generate high-quality text, making it well-suited for a variety of natural language understanding tasks such as textual entailment, question answering, semantic similarity, and document classification.",yt,fe,Un='You can find all the original GPT checkpoints under the <a href="https://huggingface.co/openai-community/openai-gpt" rel="nofollow">OpenAI community</a> organization.',wt,Y,vt,ge,Wn='The example below demonstrates how to generate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',kt,K,$t,Te,It,_e,Zn="<li>Inputs should be padded on the right because GPT uses absolute position embeddings.</li>",Gt,be,Jt,x,Me,Rt,Ve,qn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> or a <code>TFOpenAIGPTModel</code>. It is
used to instantiate a GPT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the GPT
<a href="https://huggingface.co/openai-community/openai-gpt" rel="nofollow">openai-community/openai-gpt</a> architecture from OpenAI.`,Xt,Ne,Bn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Dt,ee,Pt,ye,jt,$,we,Qt,Se,Ln="The bare Openai Model outputting raw hidden-states without any specific head on top.",Yt,Ee,Fn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Kt,Re,Vn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,en,X,ve,tn,Xe,Nn='The <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> forward method, overrides the <code>__call__</code> special method.',nn,te,At,ke,Sn="<li>forward</li>",Ct,$e,xt,I,Ie,on,De,En=`OpenAI GPT Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`,sn,Qe,Rn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,an,Ye,Xn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,rn,L,Ge,ln,Ke,Dn='The <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> forward method, overrides the <code>__call__</code> special method.',dn,ne,cn,oe,zt,Je,Qn="<li>forward</li>",Ot,Pe,Ht,G,je,pn,et,Yn=`OpenAI GPT Model transformer with a language modeling and a multiple-choice classification head on top e.g. for
RocStories/SWAG tasks. The two heads are two linear layers. The language modeling head has its weights tied to the
input embeddings, the classification head takes as input the input of a specified classification token index in the
input sequence).`,mn,tt,Kn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,un,nt,eo=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,hn,F,Ae,fn,ot,to='The <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> forward method, overrides the <code>__call__</code> special method.',gn,se,Tn,ae,Ut,Ce,no="<li>forward</li>",Wt,xe,Zt,J,ze,_n,st,oo=`The Original OpenAI GPT Model transformer with a sequence classification head on top (linear layer).
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> uses the last token in order to do the classification, as other causal
models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the
last token. If a <code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding
token in each row. If no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since
it cannot guess the padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take
the last value in each row of the batch).`,bn,at,so=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Mn,rt,ao=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,yn,j,Oe,wn,lt,ro='The <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',vn,re,kn,le,$n,ie,qt,He,lo="<li>forward</li>",Bt,Ue,Lt,P,We,In,it,io="Construct a GPT Tokenizer. Based on Byte-Pair-Encoding with the following peculiarities:",Gn,dt,co=`<li>lowercases all inputs,</li> <li>uses <code>SpaCy</code> tokenizer and <code>ftfy</code> for pre-BPE tokenization if they are installed, fallback to BERT’s
<code>BasicTokenizer</code> if not.</li>`,Jn,ct,po=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Pn,de,Ze,jn,pt,mo="Converts a sequence of tokens (string) in a single string.",Ft,qe,Vt,z,Be,An,mt,uo=`Construct a “fast” GPT Tokenizer (backed by HuggingFace’s <em>tokenizers</em> library). Based on Byte-Pair-Encoding with
the following peculiarities:`,Cn,ut,ho="<li>lower case all inputs</li> <li>uses BERT’s BasicTokenizer for pre-BPE tokenization</li>",xn,ht,fo=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Nt,Le,St,gt,Et;return R=new Q({props:{title:"GPT",local:"gpt",headingTag:"h1"}}),Y=new Tt({props:{warning:!1,$$slots:{default:[ko]},$$scope:{ctx:v}}}),K=new vo({props:{id:"usage",options:["Pipeline","AutoModel","transformers CLI"],$$slots:{default:[Jo]},$$scope:{ctx:v}}}),Te=new Q({props:{title:"Notes",local:"notes",headingTag:"h2"}}),be=new Q({props:{title:"OpenAIGPTConfig",local:"transformers.OpenAIGPTConfig",headingTag:"h2"}}),Me=new V({props:{name:"class transformers.OpenAIGPTConfig",anchor:"transformers.OpenAIGPTConfig",parameters:[{name:"vocab_size",val:" = 40478"},{name:"n_positions",val:" = 512"},{name:"n_embd",val:" = 768"},{name:"n_layer",val:" = 12"},{name:"n_head",val:" = 12"},{name:"afn",val:" = 'gelu'"},{name:"resid_pdrop",val:" = 0.1"},{name:"embd_pdrop",val:" = 0.1"},{name:"attn_pdrop",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 40478) &#x2014;
Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> or <code>TFOpenAIGPTModel</code>.`,name:"vocab_size"},{anchor:"transformers.OpenAIGPTConfig.n_positions",description:`<strong>n_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"n_positions"},{anchor:"transformers.OpenAIGPTConfig.n_embd",description:`<strong>n_embd</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the embeddings and hidden states.`,name:"n_embd"},{anchor:"transformers.OpenAIGPTConfig.n_layer",description:`<strong>n_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layer"},{anchor:"transformers.OpenAIGPTConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.OpenAIGPTConfig.afn",description:`<strong>afn</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"afn"},{anchor:"transformers.OpenAIGPTConfig.resid_pdrop",description:`<strong>resid_pdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"resid_pdrop"},{anchor:"transformers.OpenAIGPTConfig.embd_pdrop",description:`<strong>embd_pdrop</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the embeddings.`,name:"embd_pdrop"},{anchor:"transformers.OpenAIGPTConfig.attn_pdrop",description:`<strong>attn_pdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attn_pdrop"},{anchor:"transformers.OpenAIGPTConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon to use in the layer normalization layers`,name:"layer_norm_epsilon"},{anchor:"transformers.OpenAIGPTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OpenAIGPTConfig.summary_type",description:`<strong>summary_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;cls_index&quot;</code>) &#x2014;
Argument used when doing sequence summary, used in the models <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> and
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>.</p>
<p>Has to be one of the following options:</p>
<ul>
<li><code>&quot;last&quot;</code>: Take the last token hidden state (like XLNet).</li>
<li><code>&quot;first&quot;</code>: Take the first token hidden state (like BERT).</li>
<li><code>&quot;mean&quot;</code>: Take the mean of all tokens hidden states.</li>
<li><code>&quot;cls_index&quot;</code>: Supply a Tensor of classification token position (like GPT/GPT-2).</li>
<li><code>&quot;attn&quot;</code>: Not implemented now, use multi-head attention.</li>
</ul>`,name:"summary_type"},{anchor:"transformers.OpenAIGPTConfig.summary_use_proj",description:`<strong>summary_use_proj</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Argument used when doing sequence summary, used in the models <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> and
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>.</p>
<p>Whether or not to add a projection after the vector extraction.`,name:"summary_use_proj"},{anchor:"transformers.OpenAIGPTConfig.summary_activation",description:`<strong>summary_activation</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Argument used when doing sequence summary, used in the models <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> and
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>.</p>
<p>Pass <code>&quot;tanh&quot;</code> for a tanh activation to the output, any other value will result in no activation.`,name:"summary_activation"},{anchor:"transformers.OpenAIGPTConfig.summary_proj_to_labels",description:`<strong>summary_proj_to_labels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Argument used when doing sequence summary, used in the models <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> and
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>.</p>
<p>Whether the projection outputs should have <code>config.num_labels</code> or <code>config.hidden_size</code> classes.`,name:"summary_proj_to_labels"},{anchor:"transformers.OpenAIGPTConfig.summary_first_dropout",description:`<strong>summary_first_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Argument used when doing sequence summary, used in the models <a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a> and
<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>.</p>
<p>The dropout ratio to be used after the projection and activation.`,name:"summary_first_dropout"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/configuration_openai.py#L25"}}),ee=new _t({props:{anchor:"transformers.OpenAIGPTConfig.example",$$slots:{default:[Po]},$$scope:{ctx:v}}}),ye=new Q({props:{title:"OpenAIGPTModel",local:"transformers.OpenAIGPTModel",headingTag:"h2"}}),we=new V({props:{name:"class transformers.OpenAIGPTModel",anchor:"transformers.OpenAIGPTModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L409"}}),ve=new V({props:{name:"forward",anchor:"transformers.OpenAIGPTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenAIGPTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenAIGPTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.OpenAIGPTModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.OpenAIGPTModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenAIGPTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.OpenAIGPTModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenAIGPTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenAIGPTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenAIGPTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L435",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"
>OpenAIGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),te=new Tt({props:{$$slots:{default:[jo]},$$scope:{ctx:v}}}),$e=new Q({props:{title:"OpenAIGPTLMHeadModel",local:"transformers.OpenAIGPTLMHeadModel",headingTag:"h2"}}),Ie=new V({props:{name:"class transformers.OpenAIGPTLMHeadModel",anchor:"transformers.OpenAIGPTLMHeadModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTLMHeadModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L534"}}),Ge=new V({props:{name:"forward",anchor:"transformers.OpenAIGPTLMHeadModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTLMHeadModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenAIGPTLMHeadModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L545",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"
>OpenAIGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ne=new Tt({props:{$$slots:{default:[Ao]},$$scope:{ctx:v}}}),oe=new _t({props:{anchor:"transformers.OpenAIGPTLMHeadModel.forward.example",$$slots:{default:[Co]},$$scope:{ctx:v}}}),Pe=new Q({props:{title:"OpenAIGPTDoubleHeadsModel",local:"transformers.OpenAIGPTDoubleHeadsModel",headingTag:"h2"}}),je=new V({props:{name:"class transformers.OpenAIGPTDoubleHeadsModel",anchor:"transformers.OpenAIGPTDoubleHeadsModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTDoubleHeadsModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel">OpenAIGPTDoubleHeadsModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L616"}}),Ae=new V({props:{name:"forward",anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"mc_token_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"mc_labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.mc_token_ids",description:`<strong>mc_token_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>, default to index of the last token of the input) &#x2014;
Index of the classification token in each input sequence. Selected in the range <code>[0, input_ids.size(-1) - 1]</code>.`,name:"mc_token_ids"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-1, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code> are
ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.mc_labels",description:`<strong>mc_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <em>num_choices</em> is the size of the second dimension of the input tensors. (see <em>input_ids</em> above)`,name:"mc_labels"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L630",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"
>OpenAIGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss.</p>
</li>
<li>
<p><strong>mc_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>mc_labels</code> is provided) — Multiple choice classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>mc_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) — Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new Tt({props:{$$slots:{default:[xo]},$$scope:{ctx:v}}}),ae=new _t({props:{anchor:"transformers.OpenAIGPTDoubleHeadsModel.forward.example",$$slots:{default:[zo]},$$scope:{ctx:v}}}),xe=new Q({props:{title:"OpenAIGPTForSequenceClassification",local:"transformers.OpenAIGPTForSequenceClassification",headingTag:"h2"}}),ze=new V({props:{name:"class transformers.OpenAIGPTForSequenceClassification",anchor:"transformers.OpenAIGPTForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L734"}}),Oe=new V({props:{name:"forward",anchor:"transformers.OpenAIGPTForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/modeling_openai.py#L744",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"
>OpenAIGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new Tt({props:{$$slots:{default:[Oo]},$$scope:{ctx:v}}}),le=new _t({props:{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.example",$$slots:{default:[Ho]},$$scope:{ctx:v}}}),ie=new _t({props:{anchor:"transformers.OpenAIGPTForSequenceClassification.forward.example-2",$$slots:{default:[Uo]},$$scope:{ctx:v}}}),Ue=new Q({props:{title:"OpenAIGPTTokenizer",local:"transformers.OpenAIGPTTokenizer",headingTag:"h2"}}),We=new V({props:{name:"class transformers.OpenAIGPTTokenizer",anchor:"transformers.OpenAIGPTTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.OpenAIGPTTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.OpenAIGPTTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/tokenization_openai.py#L235"}}),Ze=new V({props:{name:"convert_tokens_to_string",anchor:"transformers.OpenAIGPTTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/tokenization_openai.py#L361"}}),qe=new Q({props:{title:"OpenAIGPTTokenizerFast",local:"transformers.OpenAIGPTTokenizerFast",headingTag:"h2"}}),Be=new V({props:{name:"class transformers.OpenAIGPTTokenizerFast",anchor:"transformers.OpenAIGPTTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenAIGPTTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.OpenAIGPTTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.OpenAIGPTTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/openai/tokenization_openai_fast.py#L29"}}),Le=new wo({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md"}}),{c(){t=p("meta"),u=r(),n=p("p"),c=r(),w=p("p"),w.innerHTML=a,h=r(),k=p("div"),k.innerHTML=ft,me=r(),g(R.$$.fragment),bt=r(),ue=p("p"),ue.innerHTML=On,Mt=r(),he=p("p"),he.textContent=Hn,yt=r(),fe=p("p"),fe.innerHTML=Un,wt=r(),g(Y.$$.fragment),vt=r(),ge=p("p"),ge.innerHTML=Wn,kt=r(),g(K.$$.fragment),$t=r(),g(Te.$$.fragment),It=r(),_e=p("ul"),_e.innerHTML=Zn,Gt=r(),g(be.$$.fragment),Jt=r(),x=p("div"),g(Me.$$.fragment),Rt=r(),Ve=p("p"),Ve.innerHTML=qn,Xt=r(),Ne=p("p"),Ne.innerHTML=Bn,Dt=r(),g(ee.$$.fragment),Pt=r(),g(ye.$$.fragment),jt=r(),$=p("div"),g(we.$$.fragment),Qt=r(),Se=p("p"),Se.textContent=Ln,Yt=r(),Ee=p("p"),Ee.innerHTML=Fn,Kt=r(),Re=p("p"),Re.innerHTML=Vn,en=r(),X=p("div"),g(ve.$$.fragment),tn=r(),Xe=p("p"),Xe.innerHTML=Nn,nn=r(),g(te.$$.fragment),At=r(),ke=p("ul"),ke.innerHTML=Sn,Ct=r(),g($e.$$.fragment),xt=r(),I=p("div"),g(Ie.$$.fragment),on=r(),De=p("p"),De.textContent=En,sn=r(),Qe=p("p"),Qe.innerHTML=Rn,an=r(),Ye=p("p"),Ye.innerHTML=Xn,rn=r(),L=p("div"),g(Ge.$$.fragment),ln=r(),Ke=p("p"),Ke.innerHTML=Dn,dn=r(),g(ne.$$.fragment),cn=r(),g(oe.$$.fragment),zt=r(),Je=p("ul"),Je.innerHTML=Qn,Ot=r(),g(Pe.$$.fragment),Ht=r(),G=p("div"),g(je.$$.fragment),pn=r(),et=p("p"),et.textContent=Yn,mn=r(),tt=p("p"),tt.innerHTML=Kn,un=r(),nt=p("p"),nt.innerHTML=eo,hn=r(),F=p("div"),g(Ae.$$.fragment),fn=r(),ot=p("p"),ot.innerHTML=to,gn=r(),g(se.$$.fragment),Tn=r(),g(ae.$$.fragment),Ut=r(),Ce=p("ul"),Ce.innerHTML=no,Wt=r(),g(xe.$$.fragment),Zt=r(),J=p("div"),g(ze.$$.fragment),_n=r(),st=p("p"),st.innerHTML=oo,bn=r(),at=p("p"),at.innerHTML=so,Mn=r(),rt=p("p"),rt.innerHTML=ao,yn=r(),j=p("div"),g(Oe.$$.fragment),wn=r(),lt=p("p"),lt.innerHTML=ro,vn=r(),g(re.$$.fragment),kn=r(),g(le.$$.fragment),$n=r(),g(ie.$$.fragment),qt=r(),He=p("ul"),He.innerHTML=lo,Bt=r(),g(Ue.$$.fragment),Lt=r(),P=p("div"),g(We.$$.fragment),In=r(),it=p("p"),it.textContent=io,Gn=r(),dt=p("ul"),dt.innerHTML=co,Jn=r(),ct=p("p"),ct.innerHTML=po,Pn=r(),de=p("div"),g(Ze.$$.fragment),jn=r(),pt=p("p"),pt.textContent=mo,Ft=r(),g(qe.$$.fragment),Vt=r(),z=p("div"),g(Be.$$.fragment),An=r(),mt=p("p"),mt.innerHTML=uo,Cn=r(),ut=p("ul"),ut.innerHTML=ho,xn=r(),ht=p("p"),ht.innerHTML=fo,Nt=r(),g(Le.$$.fragment),St=r(),gt=p("p"),this.h()},l(e){const o=Mo("svelte-u9bgzb",document.head);t=m(o,"META",{name:!0,content:!0}),o.forEach(s),u=l(e),n=m(e,"P",{}),A(n).forEach(s),c=l(e),w=m(e,"P",{"data-svelte-h":!0}),f(w)!=="svelte-t6knqi"&&(w.innerHTML=a),h=l(e),k=m(e,"DIV",{style:!0,"data-svelte-h":!0}),f(k)!=="svelte-rxr0ez"&&(k.innerHTML=ft),me=l(e),T(R.$$.fragment,e),bt=l(e),ue=m(e,"P",{"data-svelte-h":!0}),f(ue)!=="svelte-11fej68"&&(ue.innerHTML=On),Mt=l(e),he=m(e,"P",{"data-svelte-h":!0}),f(he)!=="svelte-17279vc"&&(he.textContent=Hn),yt=l(e),fe=m(e,"P",{"data-svelte-h":!0}),f(fe)!=="svelte-yxcj4s"&&(fe.innerHTML=Un),wt=l(e),T(Y.$$.fragment,e),vt=l(e),ge=m(e,"P",{"data-svelte-h":!0}),f(ge)!=="svelte-17pa8jt"&&(ge.innerHTML=Wn),kt=l(e),T(K.$$.fragment,e),$t=l(e),T(Te.$$.fragment,e),It=l(e),_e=m(e,"UL",{"data-svelte-h":!0}),f(_e)!=="svelte-1feg9pv"&&(_e.innerHTML=Zn),Gt=l(e),T(be.$$.fragment,e),Jt=l(e),x=m(e,"DIV",{class:!0});var N=A(x);T(Me.$$.fragment,N),Rt=l(N),Ve=m(N,"P",{"data-svelte-h":!0}),f(Ve)!=="svelte-1mtq74i"&&(Ve.innerHTML=qn),Xt=l(N),Ne=m(N,"P",{"data-svelte-h":!0}),f(Ne)!=="svelte-1ek1ss9"&&(Ne.innerHTML=Bn),Dt=l(N),T(ee.$$.fragment,N),N.forEach(s),Pt=l(e),T(ye.$$.fragment,e),jt=l(e),$=m(e,"DIV",{class:!0});var O=A($);T(we.$$.fragment,O),Qt=l(O),Se=m(O,"P",{"data-svelte-h":!0}),f(Se)!=="svelte-1y38yiy"&&(Se.textContent=Ln),Yt=l(O),Ee=m(O,"P",{"data-svelte-h":!0}),f(Ee)!=="svelte-q52n56"&&(Ee.innerHTML=Fn),Kt=l(O),Re=m(O,"P",{"data-svelte-h":!0}),f(Re)!=="svelte-hswkmf"&&(Re.innerHTML=Vn),en=l(O),X=m(O,"DIV",{class:!0});var D=A(X);T(ve.$$.fragment,D),tn=l(D),Xe=m(D,"P",{"data-svelte-h":!0}),f(Xe)!=="svelte-r5lqld"&&(Xe.innerHTML=Nn),nn=l(D),T(te.$$.fragment,D),D.forEach(s),O.forEach(s),At=l(e),ke=m(e,"UL",{"data-svelte-h":!0}),f(ke)!=="svelte-n3ow4o"&&(ke.innerHTML=Sn),Ct=l(e),T($e.$$.fragment,e),xt=l(e),I=m(e,"DIV",{class:!0});var H=A(I);T(Ie.$$.fragment,H),on=l(H),De=m(H,"P",{"data-svelte-h":!0}),f(De)!=="svelte-w06qmh"&&(De.textContent=En),sn=l(H),Qe=m(H,"P",{"data-svelte-h":!0}),f(Qe)!=="svelte-q52n56"&&(Qe.innerHTML=Rn),an=l(H),Ye=m(H,"P",{"data-svelte-h":!0}),f(Ye)!=="svelte-hswkmf"&&(Ye.innerHTML=Xn),rn=l(H),L=m(H,"DIV",{class:!0});var S=A(L);T(Ge.$$.fragment,S),ln=l(S),Ke=m(S,"P",{"data-svelte-h":!0}),f(Ke)!=="svelte-6qhtln"&&(Ke.innerHTML=Dn),dn=l(S),T(ne.$$.fragment,S),cn=l(S),T(oe.$$.fragment,S),S.forEach(s),H.forEach(s),zt=l(e),Je=m(e,"UL",{"data-svelte-h":!0}),f(Je)!=="svelte-n3ow4o"&&(Je.innerHTML=Qn),Ot=l(e),T(Pe.$$.fragment,e),Ht=l(e),G=m(e,"DIV",{class:!0});var U=A(G);T(je.$$.fragment,U),pn=l(U),et=m(U,"P",{"data-svelte-h":!0}),f(et)!=="svelte-1iaobcj"&&(et.textContent=Yn),mn=l(U),tt=m(U,"P",{"data-svelte-h":!0}),f(tt)!=="svelte-q52n56"&&(tt.innerHTML=Kn),un=l(U),nt=m(U,"P",{"data-svelte-h":!0}),f(nt)!=="svelte-hswkmf"&&(nt.innerHTML=eo),hn=l(U),F=m(U,"DIV",{class:!0});var E=A(F);T(Ae.$$.fragment,E),fn=l(E),ot=m(E,"P",{"data-svelte-h":!0}),f(ot)!=="svelte-1hada1l"&&(ot.innerHTML=to),gn=l(E),T(se.$$.fragment,E),Tn=l(E),T(ae.$$.fragment,E),E.forEach(s),U.forEach(s),Ut=l(e),Ce=m(e,"UL",{"data-svelte-h":!0}),f(Ce)!=="svelte-n3ow4o"&&(Ce.innerHTML=no),Wt=l(e),T(xe.$$.fragment,e),Zt=l(e),J=m(e,"DIV",{class:!0});var W=A(J);T(ze.$$.fragment,W),_n=l(W),st=m(W,"P",{"data-svelte-h":!0}),f(st)!=="svelte-wg4wtk"&&(st.innerHTML=oo),bn=l(W),at=m(W,"P",{"data-svelte-h":!0}),f(at)!=="svelte-q52n56"&&(at.innerHTML=so),Mn=l(W),rt=m(W,"P",{"data-svelte-h":!0}),f(rt)!=="svelte-hswkmf"&&(rt.innerHTML=ao),yn=l(W),j=m(W,"DIV",{class:!0});var Z=A(j);T(Oe.$$.fragment,Z),wn=l(Z),lt=m(Z,"P",{"data-svelte-h":!0}),f(lt)!=="svelte-1u8zgwn"&&(lt.innerHTML=ro),vn=l(Z),T(re.$$.fragment,Z),kn=l(Z),T(le.$$.fragment,Z),$n=l(Z),T(ie.$$.fragment,Z),Z.forEach(s),W.forEach(s),qt=l(e),He=m(e,"UL",{"data-svelte-h":!0}),f(He)!=="svelte-n3ow4o"&&(He.innerHTML=lo),Bt=l(e),T(Ue.$$.fragment,e),Lt=l(e),P=m(e,"DIV",{class:!0});var q=A(P);T(We.$$.fragment,q),In=l(q),it=m(q,"P",{"data-svelte-h":!0}),f(it)!=="svelte-1mapzk"&&(it.textContent=io),Gn=l(q),dt=m(q,"UL",{"data-svelte-h":!0}),f(dt)!=="svelte-1ob4ey9"&&(dt.innerHTML=co),Jn=l(q),ct=m(q,"P",{"data-svelte-h":!0}),f(ct)!=="svelte-ntrhio"&&(ct.innerHTML=po),Pn=l(q),de=m(q,"DIV",{class:!0});var Fe=A(de);T(Ze.$$.fragment,Fe),jn=l(Fe),pt=m(Fe,"P",{"data-svelte-h":!0}),f(pt)!=="svelte-b3k2yi"&&(pt.textContent=mo),Fe.forEach(s),q.forEach(s),Ft=l(e),T(qe.$$.fragment,e),Vt=l(e),z=m(e,"DIV",{class:!0});var ce=A(z);T(Be.$$.fragment,ce),An=l(ce),mt=m(ce,"P",{"data-svelte-h":!0}),f(mt)!=="svelte-1ktb3mv"&&(mt.innerHTML=uo),Cn=l(ce),ut=m(ce,"UL",{"data-svelte-h":!0}),f(ut)!=="svelte-ls1v46"&&(ut.innerHTML=ho),xn=l(ce),ht=m(ce,"P",{"data-svelte-h":!0}),f(ht)!=="svelte-gxzj9w"&&(ht.innerHTML=fo),ce.forEach(s),Nt=l(e),T(Le.$$.fragment,e),St=l(e),gt=m(e,"P",{}),A(gt).forEach(s),this.h()},h(){C(t,"name","hf:doc:metadata"),C(t,"content",Zo),yo(k,"float","right"),C(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),C(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){i(document.head,t),d(e,u,o),d(e,n,o),d(e,c,o),d(e,w,o),d(e,h,o),d(e,k,o),d(e,me,o),_(R,e,o),d(e,bt,o),d(e,ue,o),d(e,Mt,o),d(e,he,o),d(e,yt,o),d(e,fe,o),d(e,wt,o),_(Y,e,o),d(e,vt,o),d(e,ge,o),d(e,kt,o),_(K,e,o),d(e,$t,o),_(Te,e,o),d(e,It,o),d(e,_e,o),d(e,Gt,o),_(be,e,o),d(e,Jt,o),d(e,x,o),_(Me,x,null),i(x,Rt),i(x,Ve),i(x,Xt),i(x,Ne),i(x,Dt),_(ee,x,null),d(e,Pt,o),_(ye,e,o),d(e,jt,o),d(e,$,o),_(we,$,null),i($,Qt),i($,Se),i($,Yt),i($,Ee),i($,Kt),i($,Re),i($,en),i($,X),_(ve,X,null),i(X,tn),i(X,Xe),i(X,nn),_(te,X,null),d(e,At,o),d(e,ke,o),d(e,Ct,o),_($e,e,o),d(e,xt,o),d(e,I,o),_(Ie,I,null),i(I,on),i(I,De),i(I,sn),i(I,Qe),i(I,an),i(I,Ye),i(I,rn),i(I,L),_(Ge,L,null),i(L,ln),i(L,Ke),i(L,dn),_(ne,L,null),i(L,cn),_(oe,L,null),d(e,zt,o),d(e,Je,o),d(e,Ot,o),_(Pe,e,o),d(e,Ht,o),d(e,G,o),_(je,G,null),i(G,pn),i(G,et),i(G,mn),i(G,tt),i(G,un),i(G,nt),i(G,hn),i(G,F),_(Ae,F,null),i(F,fn),i(F,ot),i(F,gn),_(se,F,null),i(F,Tn),_(ae,F,null),d(e,Ut,o),d(e,Ce,o),d(e,Wt,o),_(xe,e,o),d(e,Zt,o),d(e,J,o),_(ze,J,null),i(J,_n),i(J,st),i(J,bn),i(J,at),i(J,Mn),i(J,rt),i(J,yn),i(J,j),_(Oe,j,null),i(j,wn),i(j,lt),i(j,vn),_(re,j,null),i(j,kn),_(le,j,null),i(j,$n),_(ie,j,null),d(e,qt,o),d(e,He,o),d(e,Bt,o),_(Ue,e,o),d(e,Lt,o),d(e,P,o),_(We,P,null),i(P,In),i(P,it),i(P,Gn),i(P,dt),i(P,Jn),i(P,ct),i(P,Pn),i(P,de),_(Ze,de,null),i(de,jn),i(de,pt),d(e,Ft,o),_(qe,e,o),d(e,Vt,o),d(e,z,o),_(Be,z,null),i(z,An),i(z,mt),i(z,Cn),i(z,ut),i(z,xn),i(z,ht),d(e,Nt,o),_(Le,e,o),d(e,St,o),d(e,gt,o),Et=!0},p(e,[o]){const N={};o&2&&(N.$$scope={dirty:o,ctx:e}),Y.$set(N);const O={};o&2&&(O.$$scope={dirty:o,ctx:e}),K.$set(O);const D={};o&2&&(D.$$scope={dirty:o,ctx:e}),ee.$set(D);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),te.$set(H);const S={};o&2&&(S.$$scope={dirty:o,ctx:e}),ne.$set(S);const U={};o&2&&(U.$$scope={dirty:o,ctx:e}),oe.$set(U);const E={};o&2&&(E.$$scope={dirty:o,ctx:e}),se.$set(E);const W={};o&2&&(W.$$scope={dirty:o,ctx:e}),ae.$set(W);const Z={};o&2&&(Z.$$scope={dirty:o,ctx:e}),re.$set(Z);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),le.$set(q);const Fe={};o&2&&(Fe.$$scope={dirty:o,ctx:e}),ie.$set(Fe)},i(e){Et||(b(R.$$.fragment,e),b(Y.$$.fragment,e),b(K.$$.fragment,e),b(Te.$$.fragment,e),b(be.$$.fragment,e),b(Me.$$.fragment,e),b(ee.$$.fragment,e),b(ye.$$.fragment,e),b(we.$$.fragment,e),b(ve.$$.fragment,e),b(te.$$.fragment,e),b($e.$$.fragment,e),b(Ie.$$.fragment,e),b(Ge.$$.fragment,e),b(ne.$$.fragment,e),b(oe.$$.fragment,e),b(Pe.$$.fragment,e),b(je.$$.fragment,e),b(Ae.$$.fragment,e),b(se.$$.fragment,e),b(ae.$$.fragment,e),b(xe.$$.fragment,e),b(ze.$$.fragment,e),b(Oe.$$.fragment,e),b(re.$$.fragment,e),b(le.$$.fragment,e),b(ie.$$.fragment,e),b(Ue.$$.fragment,e),b(We.$$.fragment,e),b(Ze.$$.fragment,e),b(qe.$$.fragment,e),b(Be.$$.fragment,e),b(Le.$$.fragment,e),Et=!0)},o(e){M(R.$$.fragment,e),M(Y.$$.fragment,e),M(K.$$.fragment,e),M(Te.$$.fragment,e),M(be.$$.fragment,e),M(Me.$$.fragment,e),M(ee.$$.fragment,e),M(ye.$$.fragment,e),M(we.$$.fragment,e),M(ve.$$.fragment,e),M(te.$$.fragment,e),M($e.$$.fragment,e),M(Ie.$$.fragment,e),M(Ge.$$.fragment,e),M(ne.$$.fragment,e),M(oe.$$.fragment,e),M(Pe.$$.fragment,e),M(je.$$.fragment,e),M(Ae.$$.fragment,e),M(se.$$.fragment,e),M(ae.$$.fragment,e),M(xe.$$.fragment,e),M(ze.$$.fragment,e),M(Oe.$$.fragment,e),M(re.$$.fragment,e),M(le.$$.fragment,e),M(ie.$$.fragment,e),M(Ue.$$.fragment,e),M(We.$$.fragment,e),M(Ze.$$.fragment,e),M(qe.$$.fragment,e),M(Be.$$.fragment,e),M(Le.$$.fragment,e),Et=!1},d(e){e&&(s(u),s(n),s(c),s(w),s(h),s(k),s(me),s(bt),s(ue),s(Mt),s(he),s(yt),s(fe),s(wt),s(vt),s(ge),s(kt),s($t),s(It),s(_e),s(Gt),s(Jt),s(x),s(Pt),s(jt),s($),s(At),s(ke),s(Ct),s(xt),s(I),s(zt),s(Je),s(Ot),s(Ht),s(G),s(Ut),s(Ce),s(Wt),s(Zt),s(J),s(qt),s(He),s(Bt),s(Lt),s(P),s(Ft),s(Vt),s(z),s(Nt),s(St),s(gt)),s(t),y(R,e),y(Y,e),y(K,e),y(Te,e),y(be,e),y(Me),y(ee),y(ye,e),y(we),y(ve),y(te),y($e,e),y(Ie),y(Ge),y(ne),y(oe),y(Pe,e),y(je),y(Ae),y(se),y(ae),y(xe,e),y(ze),y(Oe),y(re),y(le),y(ie),y(Ue,e),y(We),y(Ze),y(qe,e),y(Be),y(Le,e)}}}const Zo='{"title":"GPT","local":"gpt","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"OpenAIGPTConfig","local":"transformers.OpenAIGPTConfig","sections":[],"depth":2},{"title":"OpenAIGPTModel","local":"transformers.OpenAIGPTModel","sections":[],"depth":2},{"title":"OpenAIGPTLMHeadModel","local":"transformers.OpenAIGPTLMHeadModel","sections":[],"depth":2},{"title":"OpenAIGPTDoubleHeadsModel","local":"transformers.OpenAIGPTDoubleHeadsModel","sections":[],"depth":2},{"title":"OpenAIGPTForSequenceClassification","local":"transformers.OpenAIGPTForSequenceClassification","sections":[],"depth":2},{"title":"OpenAIGPTTokenizer","local":"transformers.OpenAIGPTTokenizer","sections":[],"depth":2},{"title":"OpenAIGPTTokenizerFast","local":"transformers.OpenAIGPTTokenizerFast","sections":[],"depth":2}],"depth":1}';function qo(v){return To(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xo extends _o{constructor(t){super(),bo(this,t,qo,Wo,go,{})}}export{Xo as component};
