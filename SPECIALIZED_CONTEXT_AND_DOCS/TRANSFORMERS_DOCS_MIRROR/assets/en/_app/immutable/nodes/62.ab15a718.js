import{s as cn,n as gn,o as un}from"../chunks/scheduler.18a86fab.js";import{S as hn,i as xn,g as s,s as r,r as f,A as _n,h as i,f as n,c as o,j as _,u as l,x as u,k as v,y as t,a as m,v as d,d as p,t as c,w as g}from"../chunks/index.98837b22.js";import{D as $}from"../chunks/Docstring.a1ef7999.js";import{H as he,E as vn}from"../chunks/getInferenceSnippets.06c2775f.js";function $n(Rt){let O,Ie,Le,Se,z,We,V,At=`ðŸ¤— Transformers provides a <code>transformers.onnx</code> package that enables you to
convert model checkpoints to an ONNX graph by leveraging configuration objects.`,ze,X,jt=`See the <a href="../serialization">guide</a> on exporting ðŸ¤— Transformers models for more
details.`,Ve,H,Xe,R,Bt=`We provide three abstract classes that you should inherit from, depending on the
type of model architecture you wish to export:`,He,A,Gt='<li>Encoder-based models inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/onnx#transformers.onnx.OnnxConfig">OnnxConfig</a></li> <li>Decoder-based models inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/onnx#transformers.onnx.OnnxConfigWithPast">OnnxConfigWithPast</a></li> <li>Encoder-decoder models inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/onnx#transformers.onnx.OnnxSeq2SeqConfigWithPast">OnnxSeq2SeqConfigWithPast</a></li>',Re,j,Ae,h,B,gt,xe,Ut="Base class for ONNX exportable model describing metadata on how to export the model through the ONNX format.",ut,T,G,ht,_e,Jt=`Flatten any potential nested structure expanding the name of the field with the index of the element within the
structure.`,xt,k,U,_t,ve,Kt="Instantiate a OnnxConfig for a specific model",vt,P,J,$t,$e,Qt="Generate inputs to provide to the ONNX exporter for the specific framework",bt,M,K,yt,be,Yt=`Generate inputs for ONNX Runtime using the reference model inputs. Override this to run inference with seq2seq
models which have the encoder and decoder exported as separate ONNX files.`,Ct,D,Q,wt,ye,Zt="Flag indicating if the model requires using external data format",je,Y,Be,w,Z,Ot,F,ee,Tt,Ce,en="Fill the input_or_outputs mapping with past_key_values dynamic axes considering.",kt,N,te,Pt,we,tn="Instantiate a OnnxConfig with <code>use_past</code> attribute set to True",Ge,ne,Ue,re,oe,Je,ae,Ke,se,nn=`Each ONNX configuration is associated with a set of <em>features</em> that enable you
to export models for different types of topologies or tasks.`,Qe,ie,Ye,x,me,Mt,q,fe,Dt,Oe,rn="Check whether or not the model has the requested features.",Ft,C,le,Nt,Te,on="Determines the framework to use for the export.",qt,ke,an="The priority is in the following order:",Lt,Pe,sn="<li>User input via <code>framework</code>.</li> <li>If local checkpoint is provided, use the same framework as the checkpoint.</li> <li>Available framework in environment, with priority given to PyTorch</li>",Et,L,de,It,Me,mn="Gets the OnnxConfig for a model_type and feature combination.",St,E,pe,Wt,De,fn="Attempts to retrieve an AutoModel class from a feature name.",zt,I,ce,Vt,Fe,ln="Attempts to retrieve a model from a modelâ€™s name and the feature to be enabled.",Xt,S,ge,Ht,Ne,dn="Tries to retrieve the feature -> OnnxConfig constructor map from the model type.",Ze,ue,et,Ee,tt;return z=new he({props:{title:"Exporting ðŸ¤— Transformers models to ONNX",local:"exporting--transformers-models-to-onnx",headingTag:"h1"}}),H=new he({props:{title:"ONNX Configurations",local:"onnx-configurations",headingTag:"h2"}}),j=new he({props:{title:"OnnxConfig",local:"transformers.onnx.OnnxConfig",headingTag:"h3"}}),B=new $({props:{name:"class transformers.onnx.OnnxConfig",anchor:"transformers.onnx.OnnxConfig",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": typing.Optional[list[transformers.onnx.config.PatchingSpec]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L69"}}),G=new $({props:{name:"flatten_output_collection_property",anchor:"transformers.onnx.OnnxConfig.flatten_output_collection_property",parameters:[{name:"name",val:": str"},{name:"field",val:": Iterable"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.flatten_output_collection_property.name",description:"<strong>name</strong> &#x2014; The name of the nested structure",name:"name"},{anchor:"transformers.onnx.OnnxConfig.flatten_output_collection_property.field",description:"<strong>field</strong> &#x2014; The structure to, potentially, be flattened",name:"field"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L427",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Outputs with flattened structure and key mapping this new structure.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(dict[str, Any])</p>
`}}),U=new $({props:{name:"from_model_config",anchor:"transformers.onnx.OnnxConfig.from_model_config",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.from_model_config.config",description:"<strong>config</strong> &#x2014; The model&#x2019;s configuration to use when exporting to ONNX",name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L130",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>OnnxConfig for this model</p>
`}}),J=new $({props:{name:"generate_dummy_inputs",anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs",parameters:[{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin'), ForwardRef('ImageProcessingMixin')]"},{name:"batch_size",val:": int = -1"},{name:"seq_length",val:": int = -1"},{name:"num_choices",val:": int = -1"},{name:"is_pair",val:": bool = False"},{name:"framework",val:": typing.Optional[transformers.utils.generic.TensorType] = None"},{name:"num_channels",val:": int = 3"},{name:"image_width",val:": int = 40"},{name:"image_height",val:": int = 40"},{name:"sampling_rate",val:": int = 22050"},{name:"time_duration",val:": float = 5.0"},{name:"frequency",val:": int = 220"},{name:"tokenizer",val:": typing.Optional[ForwardRef('PreTrainedTokenizerBase')] = None"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.preprocessor",description:`<strong>preprocessor</strong> &#x2014; (<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase">PreTrainedTokenizerBase</a>, <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin">FeatureExtractionMixin</a>, or <a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a>):
The preprocessor associated with this model configuration.`,name:"preprocessor"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The batch size to export the model for (-1 means dynamic axis).`,name:"batch_size"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.num_choices",description:`<strong>num_choices</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The number of candidate answers provided for multiple choice task (-1 means dynamic axis).`,name:"num_choices"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.seq_length",description:`<strong>seq_length</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The sequence length to export the model for (-1 means dynamic axis).`,name:"seq_length"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.is_pair",description:`<strong>is_pair</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Indicate if the input is a pair (sentence 1, sentence 2)`,name:"is_pair"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.framework",description:`<strong>framework</strong> (<code>TensorType</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework (PyTorch or TensorFlow) that the tokenizer will generate tensors for.`,name:"framework"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of channels of the generated images.`,name:"num_channels"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.image_width",description:`<strong>image_width</strong> (<code>int</code>, <em>optional</em>, defaults to 40) &#x2014;
The width of the generated images.`,name:"image_width"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.image_height",description:`<strong>image_height</strong> (<code>int</code>, <em>optional</em>, defaults to 40) &#x2014;
The height of the generated images.`,name:"image_height"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em> defaults to 22050) &#x2014;
The sampling rate for audio data generation.`,name:"sampling_rate"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.time_duration",description:`<strong>time_duration</strong> (<code>float</code>, <em>optional</em> defaults to 5.0) &#x2014;
Total seconds of sampling for audio data generation.`,name:"time_duration"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.frequency",description:`<strong>frequency</strong> (<code>int</code>, <em>optional</em> defaults to 220) &#x2014;
The desired natural frequency of generated audio.`,name:"frequency"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L283",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Mapping[str, Tensor] holding the kwargs to provide to the modelâ€™s forward function</p>
`}}),K=new $({props:{name:"generate_dummy_inputs_onnxruntime",anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs_onnxruntime",parameters:[{name:"reference_model_inputs",val:": Mapping"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs_onnxruntime.reference_model_inputs",description:`<strong>reference_model_inputs</strong> ([<code>Mapping[str, Tensor]</code>) &#x2014;
Reference inputs for the model.`,name:"reference_model_inputs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L403",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The mapping holding the kwargs to provide to the modelâ€™s forward function</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Mapping[str, Tensor]</code></p>
`}}),Q=new $({props:{name:"use_external_data_format",anchor:"transformers.onnx.OnnxConfig.use_external_data_format",parameters:[{name:"num_parameters",val:": int"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.use_external_data_format.num_parameters",description:"<strong>num_parameters</strong> &#x2014; Number of parameter on the model",name:"num_parameters"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L244",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>True if model.num_parameters() * size_of(float32) >= 2Gb False otherwise</p>
`}}),Y=new he({props:{title:"OnnxConfigWithPast",local:"transformers.onnx.OnnxConfigWithPast",headingTag:"h3"}}),Z=new $({props:{name:"class transformers.onnx.OnnxConfigWithPast",anchor:"transformers.onnx.OnnxConfigWithPast",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": typing.Optional[list[transformers.onnx.config.PatchingSpec]] = None"},{name:"use_past",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L446"}}),ee=new $({props:{name:"fill_with_past_key_values_",anchor:"transformers.onnx.OnnxConfigWithPast.fill_with_past_key_values_",parameters:[{name:"inputs_or_outputs",val:": Mapping"},{name:"direction",val:": str"},{name:"inverted_values_shape",val:": bool = False"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfigWithPast.fill_with_past_key_values_.inputs_or_outputs",description:"<strong>inputs_or_outputs</strong> &#x2014; The mapping to fill.",name:"inputs_or_outputs"},{anchor:"transformers.onnx.OnnxConfigWithPast.fill_with_past_key_values_.direction",description:`<strong>direction</strong> &#x2014; either &#x201C;inputs&#x201D; or &#x201C;outputs&#x201D;, it specifies whether input_or_outputs is the input mapping or the
output mapping, this is important for axes naming.`,name:"direction"},{anchor:"transformers.onnx.OnnxConfigWithPast.fill_with_past_key_values_.inverted_values_shape",description:`<strong>inverted_values_shape</strong> &#x2014;
If <code>True</code>, store values on dynamic axis 1, else on axis 2.`,name:"inverted_values_shape"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L553"}}),te=new $({props:{name:"with_past",anchor:"transformers.onnx.OnnxConfigWithPast.with_past",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfigWithPast.with_past.config",description:"<strong>config</strong> &#x2014; The underlying model&#x2019;s config to use when exporting to ONNX",name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L457",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>OnnxConfig with <code>.use_past = True</code></p>
`}}),ne=new he({props:{title:"OnnxSeq2SeqConfigWithPast",local:"transformers.onnx.OnnxSeq2SeqConfigWithPast",headingTag:"h3"}}),oe=new $({props:{name:"class transformers.onnx.OnnxSeq2SeqConfigWithPast",anchor:"transformers.onnx.OnnxSeq2SeqConfigWithPast",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": typing.Optional[list[transformers.onnx.config.PatchingSpec]] = None"},{name:"use_past",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/config.py#L593"}}),ae=new he({props:{title:"ONNX Features",local:"onnx-features",headingTag:"h2"}}),ie=new he({props:{title:"FeaturesManager",local:"transformers.onnx.FeaturesManager",headingTag:"h3"}}),me=new $({props:{name:"class transformers.onnx.FeaturesManager",anchor:"transformers.onnx.FeaturesManager",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L85"}}),fe=new $({props:{name:"check_supported_model_or_raise",anchor:"transformers.onnx.FeaturesManager.check_supported_model_or_raise",parameters:[{name:"model",val:": typing.Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"feature",val:": str = 'default'"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.check_supported_model_or_raise.model",description:"<strong>model</strong> &#x2014; The model to export.",name:"model"},{anchor:"transformers.onnx.FeaturesManager.check_supported_model_or_raise.feature",description:"<strong>feature</strong> &#x2014; The name of the feature to check if it is available.",name:"feature"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L711",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(str) The type of the model (OnnxConfig) The OnnxConfig instance holding the model export properties.</p>
`}}),le=new $({props:{name:"determine_framework",anchor:"transformers.onnx.FeaturesManager.determine_framework",parameters:[{name:"model",val:": str"},{name:"framework",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.determine_framework.model",description:`<strong>model</strong> (<code>str</code>) &#x2014;
The name of the model to export.`,name:"model"},{anchor:"transformers.onnx.FeaturesManager.determine_framework.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework to use for the export. See above for priority if none provided.`,name:"framework"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L628",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The framework to use for the export.</p>
`}}),de=new $({props:{name:"get_config",anchor:"transformers.onnx.FeaturesManager.get_config",parameters:[{name:"model_type",val:": str"},{name:"feature",val:": str"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_config.model_type",description:`<strong>model_type</strong> (<code>str</code>) &#x2014;
The model type to retrieve the config for.`,name:"model_type"},{anchor:"transformers.onnx.FeaturesManager.get_config.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature to retrieve the config for.`,name:"feature"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L736",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>config for the combination</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>OnnxConfig</code></p>
`}}),pe=new $({props:{name:"get_model_class_for_feature",anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature",parameters:[{name:"feature",val:": str"},{name:"framework",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature required.`,name:"feature"},{anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The framework to use for the export.`,name:"framework"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L601",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The AutoModel class corresponding to the feature.</p>
`}}),ce=new $({props:{name:"get_model_from_feature",anchor:"transformers.onnx.FeaturesManager.get_model_from_feature",parameters:[{name:"feature",val:": str"},{name:"model",val:": str"},{name:"framework",val:": typing.Optional[str] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature required.`,name:"feature"},{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.model",description:`<strong>model</strong> (<code>str</code>) &#x2014;
The name of the model to export.`,name:"model"},{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework to use for the export. See <code>FeaturesManager.determine_framework</code> for the priority should
none be provided.`,name:"framework"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L678",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The instance of the model.</p>
`}}),ge=new $({props:{name:"get_supported_features_for_model_type",anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type",parameters:[{name:"model_type",val:": str"},{name:"model_name",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type.model_type",description:`<strong>model_type</strong> (<code>str</code>) &#x2014;
The model type to retrieve the supported features for.`,name:"model_type"},{anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type.model_name",description:`<strong>model_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name attribute of the model object, only used for the exception message.`,name:"model_name"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/onnx/features.py#L556",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The dictionary mapping each feature to a corresponding OnnxConfig constructor.</p>
`}}),ue=new vn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/onnx.md"}}),{c(){O=s("meta"),Ie=r(),Le=s("p"),Se=r(),f(z.$$.fragment),We=r(),V=s("p"),V.innerHTML=At,ze=r(),X=s("p"),X.innerHTML=jt,Ve=r(),f(H.$$.fragment),Xe=r(),R=s("p"),R.textContent=Bt,He=r(),A=s("ul"),A.innerHTML=Gt,Re=r(),f(j.$$.fragment),Ae=r(),h=s("div"),f(B.$$.fragment),gt=r(),xe=s("p"),xe.textContent=Ut,ut=r(),T=s("div"),f(G.$$.fragment),ht=r(),_e=s("p"),_e.textContent=Jt,xt=r(),k=s("div"),f(U.$$.fragment),_t=r(),ve=s("p"),ve.textContent=Kt,vt=r(),P=s("div"),f(J.$$.fragment),$t=r(),$e=s("p"),$e.textContent=Qt,bt=r(),M=s("div"),f(K.$$.fragment),yt=r(),be=s("p"),be.textContent=Yt,Ct=r(),D=s("div"),f(Q.$$.fragment),wt=r(),ye=s("p"),ye.textContent=Zt,je=r(),f(Y.$$.fragment),Be=r(),w=s("div"),f(Z.$$.fragment),Ot=r(),F=s("div"),f(ee.$$.fragment),Tt=r(),Ce=s("p"),Ce.textContent=en,kt=r(),N=s("div"),f(te.$$.fragment),Pt=r(),we=s("p"),we.innerHTML=tn,Ge=r(),f(ne.$$.fragment),Ue=r(),re=s("div"),f(oe.$$.fragment),Je=r(),f(ae.$$.fragment),Ke=r(),se=s("p"),se.innerHTML=nn,Qe=r(),f(ie.$$.fragment),Ye=r(),x=s("div"),f(me.$$.fragment),Mt=r(),q=s("div"),f(fe.$$.fragment),Dt=r(),Oe=s("p"),Oe.textContent=rn,Ft=r(),C=s("div"),f(le.$$.fragment),Nt=r(),Te=s("p"),Te.textContent=on,qt=r(),ke=s("p"),ke.textContent=an,Lt=r(),Pe=s("ol"),Pe.innerHTML=sn,Et=r(),L=s("div"),f(de.$$.fragment),It=r(),Me=s("p"),Me.textContent=mn,St=r(),E=s("div"),f(pe.$$.fragment),Wt=r(),De=s("p"),De.textContent=fn,zt=r(),I=s("div"),f(ce.$$.fragment),Vt=r(),Fe=s("p"),Fe.textContent=ln,Xt=r(),S=s("div"),f(ge.$$.fragment),Ht=r(),Ne=s("p"),Ne.textContent=dn,Ze=r(),f(ue.$$.fragment),et=r(),Ee=s("p"),this.h()},l(e){const a=_n("svelte-u9bgzb",document.head);O=i(a,"META",{name:!0,content:!0}),a.forEach(n),Ie=o(e),Le=i(e,"P",{}),_(Le).forEach(n),Se=o(e),l(z.$$.fragment,e),We=o(e),V=i(e,"P",{"data-svelte-h":!0}),u(V)!=="svelte-4jsgzi"&&(V.innerHTML=At),ze=o(e),X=i(e,"P",{"data-svelte-h":!0}),u(X)!=="svelte-1nfy7l0"&&(X.innerHTML=jt),Ve=o(e),l(H.$$.fragment,e),Xe=o(e),R=i(e,"P",{"data-svelte-h":!0}),u(R)!=="svelte-10ifyz5"&&(R.textContent=Bt),He=o(e),A=i(e,"UL",{"data-svelte-h":!0}),u(A)!=="svelte-16c3y70"&&(A.innerHTML=Gt),Re=o(e),l(j.$$.fragment,e),Ae=o(e),h=i(e,"DIV",{class:!0});var b=_(h);l(B.$$.fragment,b),gt=o(b),xe=i(b,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-1gqzpaz"&&(xe.textContent=Ut),ut=o(b),T=i(b,"DIV",{class:!0});var nt=_(T);l(G.$$.fragment,nt),ht=o(nt),_e=i(nt,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-1lfqihc"&&(_e.textContent=Jt),nt.forEach(n),xt=o(b),k=i(b,"DIV",{class:!0});var rt=_(k);l(U.$$.fragment,rt),_t=o(rt),ve=i(rt,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-1u54gj1"&&(ve.textContent=Kt),rt.forEach(n),vt=o(b),P=i(b,"DIV",{class:!0});var ot=_(P);l(J.$$.fragment,ot),$t=o(ot),$e=i(ot,"P",{"data-svelte-h":!0}),u($e)!=="svelte-1oyyynq"&&($e.textContent=Qt),ot.forEach(n),bt=o(b),M=i(b,"DIV",{class:!0});var at=_(M);l(K.$$.fragment,at),yt=o(at),be=i(at,"P",{"data-svelte-h":!0}),u(be)!=="svelte-tenqiw"&&(be.textContent=Yt),at.forEach(n),Ct=o(b),D=i(b,"DIV",{class:!0});var st=_(D);l(Q.$$.fragment,st),wt=o(st),ye=i(st,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-coevd"&&(ye.textContent=Zt),st.forEach(n),b.forEach(n),je=o(e),l(Y.$$.fragment,e),Be=o(e),w=i(e,"DIV",{class:!0});var qe=_(w);l(Z.$$.fragment,qe),Ot=o(qe),F=i(qe,"DIV",{class:!0});var it=_(F);l(ee.$$.fragment,it),Tt=o(it),Ce=i(it,"P",{"data-svelte-h":!0}),u(Ce)!=="svelte-1bh80il"&&(Ce.textContent=en),it.forEach(n),kt=o(qe),N=i(qe,"DIV",{class:!0});var mt=_(N);l(te.$$.fragment,mt),Pt=o(mt),we=i(mt,"P",{"data-svelte-h":!0}),u(we)!=="svelte-1kg2yu2"&&(we.innerHTML=tn),mt.forEach(n),qe.forEach(n),Ge=o(e),l(ne.$$.fragment,e),Ue=o(e),re=i(e,"DIV",{class:!0});var pn=_(re);l(oe.$$.fragment,pn),pn.forEach(n),Je=o(e),l(ae.$$.fragment,e),Ke=o(e),se=i(e,"P",{"data-svelte-h":!0}),u(se)!=="svelte-a66ofi"&&(se.innerHTML=nn),Qe=o(e),l(ie.$$.fragment,e),Ye=o(e),x=i(e,"DIV",{class:!0});var y=_(x);l(me.$$.fragment,y),Mt=o(y),q=i(y,"DIV",{class:!0});var ft=_(q);l(fe.$$.fragment,ft),Dt=o(ft),Oe=i(ft,"P",{"data-svelte-h":!0}),u(Oe)!=="svelte-rlyyl3"&&(Oe.textContent=rn),ft.forEach(n),Ft=o(y),C=i(y,"DIV",{class:!0});var W=_(C);l(le.$$.fragment,W),Nt=o(W),Te=i(W,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-rshgf5"&&(Te.textContent=on),qt=o(W),ke=i(W,"P",{"data-svelte-h":!0}),u(ke)!=="svelte-1wbth9c"&&(ke.textContent=an),Lt=o(W),Pe=i(W,"OL",{"data-svelte-h":!0}),u(Pe)!=="svelte-qby6wj"&&(Pe.innerHTML=sn),W.forEach(n),Et=o(y),L=i(y,"DIV",{class:!0});var lt=_(L);l(de.$$.fragment,lt),It=o(lt),Me=i(lt,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-il0adz"&&(Me.textContent=mn),lt.forEach(n),St=o(y),E=i(y,"DIV",{class:!0});var dt=_(E);l(pe.$$.fragment,dt),Wt=o(dt),De=i(dt,"P",{"data-svelte-h":!0}),u(De)!=="svelte-k5ftuy"&&(De.textContent=fn),dt.forEach(n),zt=o(y),I=i(y,"DIV",{class:!0});var pt=_(I);l(ce.$$.fragment,pt),Vt=o(pt),Fe=i(pt,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-auvjm5"&&(Fe.textContent=ln),pt.forEach(n),Xt=o(y),S=i(y,"DIV",{class:!0});var ct=_(S);l(ge.$$.fragment,ct),Ht=o(ct),Ne=i(ct,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-tbalh5"&&(Ne.textContent=dn),ct.forEach(n),y.forEach(n),Ze=o(e),l(ue.$$.fragment,e),et=o(e),Ee=i(e,"P",{}),_(Ee).forEach(n),this.h()},h(){v(O,"name","hf:doc:metadata"),v(O,"content",bn),v(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){t(document.head,O),m(e,Ie,a),m(e,Le,a),m(e,Se,a),d(z,e,a),m(e,We,a),m(e,V,a),m(e,ze,a),m(e,X,a),m(e,Ve,a),d(H,e,a),m(e,Xe,a),m(e,R,a),m(e,He,a),m(e,A,a),m(e,Re,a),d(j,e,a),m(e,Ae,a),m(e,h,a),d(B,h,null),t(h,gt),t(h,xe),t(h,ut),t(h,T),d(G,T,null),t(T,ht),t(T,_e),t(h,xt),t(h,k),d(U,k,null),t(k,_t),t(k,ve),t(h,vt),t(h,P),d(J,P,null),t(P,$t),t(P,$e),t(h,bt),t(h,M),d(K,M,null),t(M,yt),t(M,be),t(h,Ct),t(h,D),d(Q,D,null),t(D,wt),t(D,ye),m(e,je,a),d(Y,e,a),m(e,Be,a),m(e,w,a),d(Z,w,null),t(w,Ot),t(w,F),d(ee,F,null),t(F,Tt),t(F,Ce),t(w,kt),t(w,N),d(te,N,null),t(N,Pt),t(N,we),m(e,Ge,a),d(ne,e,a),m(e,Ue,a),m(e,re,a),d(oe,re,null),m(e,Je,a),d(ae,e,a),m(e,Ke,a),m(e,se,a),m(e,Qe,a),d(ie,e,a),m(e,Ye,a),m(e,x,a),d(me,x,null),t(x,Mt),t(x,q),d(fe,q,null),t(q,Dt),t(q,Oe),t(x,Ft),t(x,C),d(le,C,null),t(C,Nt),t(C,Te),t(C,qt),t(C,ke),t(C,Lt),t(C,Pe),t(x,Et),t(x,L),d(de,L,null),t(L,It),t(L,Me),t(x,St),t(x,E),d(pe,E,null),t(E,Wt),t(E,De),t(x,zt),t(x,I),d(ce,I,null),t(I,Vt),t(I,Fe),t(x,Xt),t(x,S),d(ge,S,null),t(S,Ht),t(S,Ne),m(e,Ze,a),d(ue,e,a),m(e,et,a),m(e,Ee,a),tt=!0},p:gn,i(e){tt||(p(z.$$.fragment,e),p(H.$$.fragment,e),p(j.$$.fragment,e),p(B.$$.fragment,e),p(G.$$.fragment,e),p(U.$$.fragment,e),p(J.$$.fragment,e),p(K.$$.fragment,e),p(Q.$$.fragment,e),p(Y.$$.fragment,e),p(Z.$$.fragment,e),p(ee.$$.fragment,e),p(te.$$.fragment,e),p(ne.$$.fragment,e),p(oe.$$.fragment,e),p(ae.$$.fragment,e),p(ie.$$.fragment,e),p(me.$$.fragment,e),p(fe.$$.fragment,e),p(le.$$.fragment,e),p(de.$$.fragment,e),p(pe.$$.fragment,e),p(ce.$$.fragment,e),p(ge.$$.fragment,e),p(ue.$$.fragment,e),tt=!0)},o(e){c(z.$$.fragment,e),c(H.$$.fragment,e),c(j.$$.fragment,e),c(B.$$.fragment,e),c(G.$$.fragment,e),c(U.$$.fragment,e),c(J.$$.fragment,e),c(K.$$.fragment,e),c(Q.$$.fragment,e),c(Y.$$.fragment,e),c(Z.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),c(ne.$$.fragment,e),c(oe.$$.fragment,e),c(ae.$$.fragment,e),c(ie.$$.fragment,e),c(me.$$.fragment,e),c(fe.$$.fragment,e),c(le.$$.fragment,e),c(de.$$.fragment,e),c(pe.$$.fragment,e),c(ce.$$.fragment,e),c(ge.$$.fragment,e),c(ue.$$.fragment,e),tt=!1},d(e){e&&(n(Ie),n(Le),n(Se),n(We),n(V),n(ze),n(X),n(Ve),n(Xe),n(R),n(He),n(A),n(Re),n(Ae),n(h),n(je),n(Be),n(w),n(Ge),n(Ue),n(re),n(Je),n(Ke),n(se),n(Qe),n(Ye),n(x),n(Ze),n(et),n(Ee)),n(O),g(z,e),g(H,e),g(j,e),g(B),g(G),g(U),g(J),g(K),g(Q),g(Y,e),g(Z),g(ee),g(te),g(ne,e),g(oe),g(ae,e),g(ie,e),g(me),g(fe),g(le),g(de),g(pe),g(ce),g(ge),g(ue,e)}}}const bn='{"title":"Exporting ðŸ¤— Transformers models to ONNX","local":"exporting--transformers-models-to-onnx","sections":[{"title":"ONNX Configurations","local":"onnx-configurations","sections":[{"title":"OnnxConfig","local":"transformers.onnx.OnnxConfig","sections":[],"depth":3},{"title":"OnnxConfigWithPast","local":"transformers.onnx.OnnxConfigWithPast","sections":[],"depth":3},{"title":"OnnxSeq2SeqConfigWithPast","local":"transformers.onnx.OnnxSeq2SeqConfigWithPast","sections":[],"depth":3}],"depth":2},{"title":"ONNX Features","local":"onnx-features","sections":[{"title":"FeaturesManager","local":"transformers.onnx.FeaturesManager","sections":[],"depth":3}],"depth":2}],"depth":1}';function yn(Rt){return un(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kn extends hn{constructor(O){super(),xn(this,O,yn,$n,cn,{})}}export{kn as component};
