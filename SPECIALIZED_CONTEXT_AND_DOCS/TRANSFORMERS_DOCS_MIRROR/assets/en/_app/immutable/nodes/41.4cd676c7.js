import{s as Yt,o as Nt,n as Bt}from"../chunks/scheduler.18a86fab.js";import{S as qt,i as Qt,g as i,s as r,r as p,A as St,h as l,f as t,c as s,j as k,u as m,x as _,k as T,y as a,a as o,v as f,d as u,t as h,w as g}from"../chunks/index.98837b22.js";import{D as M}from"../chunks/Docstring.a1ef7999.js";import{C as Wt}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Xt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ie,E as Kt}from"../chunks/getInferenceSnippets.06c2775f.js";function Ot(le){let c,V="Example:",b,y,$;return y=new Wt({props:{code:"JTIzJTIwQ29ycmVjdCUyMC0lMjBoaWRkZW5fc3RhdGVzJTIwcGFzc2VkJTIwYXMlMjBwb3NpdGlvbmFsJTIwYXJnJTBBb3V0JTIwJTNEJTIwc2VsZi5sYXllcihoaWRkZW5fc3RhdGVzJTJDJTIwYXR0ZW50aW9uX21hc2slM0RhdHRlbnRpb25fbWFzayklMEElMEElMjMlMjBJbmNvcnJlY3QlMjAtJTIwaGlkZGVuX3N0YXRlcyUyMHBhc3NlZCUyMGFzJTIwa2V5d29yZCUyMGFyZyUwQW91dCUyMCUzRCUyMHNlbGYubGF5ZXIoaGlkZGVuX3N0YXRlcyUzRGhpZGRlbl9zdGF0ZXMlMkMlMjBhdHRlbnRpb25fbWFzayUzRGF0dGVudGlvbl9tYXNrKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Correct - hidden_states passed as positional arg</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>out = self.layer(hidden_states, attention_mask=attention_mask)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Incorrect - hidden_states passed as keyword arg</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>out = self.layer(hidden_states=hidden_states, attention_mask=attention_mask)`,wrap:!1}}),{c(){c=i("p"),c.textContent=V,b=r(),p(y.$$.fragment)},l(d){c=l(d,"P",{"data-svelte-h":!0}),_(c)!=="svelte-11lpom8"&&(c.textContent=V),b=s(d),m(y.$$.fragment,d)},m(d,x){o(d,c,x),o(d,b,x),f(y,d,x),$=!0},p:Bt,i(d){$||(u(y.$$.fragment,d),$=!0)},o(d){h(y.$$.fragment,d),$=!1},d(d){d&&(t(c),t(b)),g(y,d)}}}function en(le){let c,V="Examples:",b,y,$;return y=new Wt({props:{code:"JTIzJTIwcmVuYW1lJTIwdGhlJTIwdXN1YWwlMjBmb3J3YXJkKCklMjBmbiUyMHRvJTIwZm9yd2FyZF9jaHVuaygpJTBBZGVmJTIwZm9yd2FyZF9jaHVuayhzZWxmJTJDJTIwaGlkZGVuX3N0YXRlcyklM0ElMEElMjAlMjAlMjAlMjBoaWRkZW5fc3RhdGVzJTIwJTNEJTIwc2VsZi5kZWNvZGVyKGhpZGRlbl9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaGlkZGVuX3N0YXRlcyUwQSUwQSUwQSUyMyUyMGltcGxlbWVudCUyMGElMjBjaHVua2VkJTIwZm9yd2FyZCUyMGZ1bmN0aW9uJTBBZGVmJTIwZm9yd2FyZChzZWxmJTJDJTIwaGlkZGVuX3N0YXRlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjBhcHBseV9jaHVua2luZ190b19mb3J3YXJkKHNlbGYuZm9yd2FyZF9jaHVuayUyQyUyMHNlbGYuY2h1bmtfc2l6ZV9sbV9oZWFkJTJDJTIwc2VsZi5zZXFfbGVuX2RpbSUyQyUyMGhpZGRlbl9zdGF0ZXMp",highlighted:`<span class="hljs-comment"># rename the usual forward() fn to forward_chunk()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_chunk</span>(<span class="hljs-params">self, hidden_states</span>):
    hidden_states = self.decoder(hidden_states)
    <span class="hljs-keyword">return</span> hidden_states


<span class="hljs-comment"># implement a chunked forward function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
    <span class="hljs-keyword">return</span> apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)`,wrap:!1}}),{c(){c=i("p"),c.textContent=V,b=r(),p(y.$$.fragment)},l(d){c=l(d,"P",{"data-svelte-h":!0}),_(c)!=="svelte-kvfsh7"&&(c.textContent=V),b=s(d),m(y.$$.fragment,d)},m(d,x){o(d,c,x),o(d,b,x),f(y,d,x),$=!0},p:Bt,i(d){$||(u(y.$$.fragment,d),$=!0)},o(d){h(y.$$.fragment,d),$=!1},d(d){d&&(t(c),t(b)),g(y,d)}}}function tn(le){let c,V,b,y,$,d,x,wt="This page lists all the custom layers used by the library, as well as the utility functions and classes it provides for modeling.",Ve,H,kt="Most of those are only useful if you are studying the code of the models in the library.",Ze,U,je,v,A,nt,de,Tt="Base class for layers with gradient checkpointing.",rt,ce,Ct=`This class enables gradient checkpointing functionality for a layer. By default, gradient checkpointing is disabled
(<code>gradient_checkpointing = False</code>). When <code>model.set_gradient_checkpointing()</code> is called, gradient checkpointing is
enabled by setting <code>gradient_checkpointing = True</code> and assigning a checkpointing function to <code>_gradient_checkpointing_func</code>.`,st,pe,Mt="Important:",at,me,It=`When using gradient checkpointing with <code>use_reentrant=True</code>, inputs that require gradients (e.g. hidden states)
must be passed as positional arguments (<code>*args</code>) rather than keyword arguments to properly propagate gradients.`,ot,z,Ee,X,Pe,I,B,it,fe,Dt=`Dict-like object keeping track of allowed attention functions. You can easily add a new attention function
with a call to <code>register()</code>. If a model needs to locally overwrite an existing attention function, say <code>sdpa</code>,
it needs to declare a new instance of this class inside the <code>modeling_&lt;model&gt;.py</code>, and declare it on that instance.`,lt,ue,W,ze,Y,Re,Z,N,dt,he,q,Fe,Q,He,j,S,ct,ge,Gt=`Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE
(i.e. a RoPE implementation that may recompute its frequencies in the forward pass).`,Ue,K,Ae,D,O,pt,_e,Jt="1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).",mt,ye,Lt="Basically works like a linear layer but the weights are transposed.",Xe,ee,Be,C,te,ft,$e,Vt=`This function chunks the <code>input_tensors</code> into smaller input tensor parts of size <code>chunk_size</code> over the dimension
<code>chunk_dim</code>. It then applies a layer <code>forward_fn</code> to each chunk independently to save memory.`,ut,ve,Zt=`If the <code>forward_fn</code> is independent across the <code>chunk_dim</code> this function will yield the same result as directly
applying <code>forward_fn</code> to <code>input_tensors</code>.`,ht,R,We,E,ne,gt,be,jt="Finds the heads and their indices taking <code>already_pruned_heads</code> into account.",Ye,G,re,_t,xe,Et="Prune a Conv1D or linear layer to keep only entries in index.",yt,we,Pt="Used to remove heads.",Ne,J,se,$t,ke,zt=`Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights
are transposed.`,vt,Te,Rt="Used to remove heads.",qe,L,ae,bt,Ce,Ft="Prune a linear layer to keep only entries in index.",xt,Me,Ht="Used to remove heads.",Qe,oe,Se,Le,Ke;return $=new ie({props:{title:"Custom Layers and Utilities",local:"custom-layers-and-utilities",headingTag:"h1"}}),U=new ie({props:{title:"Layers",local:"transformers.GradientCheckpointingLayer",headingTag:"h2"}}),A=new M({props:{name:"class transformers.GradientCheckpointingLayer",anchor:"transformers.GradientCheckpointingLayer",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_layers.py#L35"}}),z=new Xt({props:{anchor:"transformers.GradientCheckpointingLayer.example",$$slots:{default:[Ot]},$$scope:{ctx:le}}}),X=new ie({props:{title:"Attention Functions",local:"transformers.AttentionInterface",headingTag:"h2"}}),B=new M({props:{name:"class transformers.AttentionInterface",anchor:"transformers.AttentionInterface",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_utils.py#L6250"}}),W=new M({props:{name:"register",anchor:"transformers.AttentionInterface.register",parameters:[{name:"key",val:": str"},{name:"value",val:": typing.Callable"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/utils/generic.py#L1127"}}),Y=new ie({props:{title:"Attention Mask Functions",local:"transformers.AttentionMaskInterface",headingTag:"h2"}}),N=new M({props:{name:"class transformers.AttentionMaskInterface",anchor:"transformers.AttentionMaskInterface",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/masking_utils.py#L621"}}),q=new M({props:{name:"register",anchor:"transformers.AttentionMaskInterface.register",parameters:[{name:"key",val:": str"},{name:"value",val:": typing.Callable"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/utils/generic.py#L1127"}}),Q=new ie({props:{title:"Rotary Position Embedding Functions",local:"transformers.dynamic_rope_update",headingTag:"h2"}}),S=new M({props:{name:"transformers.dynamic_rope_update",anchor:"transformers.dynamic_rope_update",parameters:[{name:"rope_forward",val:""}],parametersDescription:[{anchor:"transformers.dynamic_rope_update.rope_forward",description:`<strong>rope_forward</strong> (Callable) &#x2014;
The forward pass of the RoPE implementation.`,name:"rope_forward"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_rope_utils.py#L30",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The decorated forward pass.</p>
`}}),K=new ie({props:{title:"Pytorch custom modules",local:"transformers.Conv1D",headingTag:"h2"}}),O=new M({props:{name:"class transformers.Conv1D",anchor:"transformers.Conv1D",parameters:[{name:"nf",val:""},{name:"nx",val:""}],parametersDescription:[{anchor:"transformers.Conv1D.nf",description:"<strong>nf</strong> (<code>int</code>) &#x2014; The number of output features.",name:"nf"},{anchor:"transformers.Conv1D.nx",description:"<strong>nx</strong> (<code>int</code>) &#x2014; The number of input features.",name:"nx"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L98"}}),ee=new ie({props:{title:"PyTorch Helper Functions",local:"transformers.apply_chunking_to_forward",headingTag:"h2"}}),te=new M({props:{name:"transformers.apply_chunking_to_forward",anchor:"transformers.apply_chunking_to_forward",parameters:[{name:"forward_fn",val:": Callable[..., torch.Tensor]"},{name:"chunk_size",val:": int"},{name:"chunk_dim",val:": int"},{name:"*input_tensors",val:""}],parametersDescription:[{anchor:"transformers.apply_chunking_to_forward.forward_fn",description:`<strong>forward_fn</strong> (<code>Callable[..., torch.Tensor]</code>) &#x2014;
The forward function of the model.`,name:"forward_fn"},{anchor:"transformers.apply_chunking_to_forward.chunk_size",description:`<strong>chunk_size</strong> (<code>int</code>) &#x2014;
The chunk size of a chunked tensor: <code>num_chunks = len(input_tensors[0]) / chunk_size</code>.`,name:"chunk_size"},{anchor:"transformers.apply_chunking_to_forward.chunk_dim",description:`<strong>chunk_dim</strong> (<code>int</code>) &#x2014;
The dimension over which the <code>input_tensors</code> should be chunked.`,name:"chunk_dim"},{anchor:"transformers.apply_chunking_to_forward.input_tensors",description:`<strong>input_tensors</strong> (<code>tuple[torch.Tensor]</code>) &#x2014;
The input tensors of <code>forward_fn</code> which will be chunked`,name:"input_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L182",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tensor with the same shape as the <code>forward_fn</code> would have given if applied\`.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.Tensor</code></p>
`}}),R=new Xt({props:{anchor:"transformers.apply_chunking_to_forward.example",$$slots:{default:[en]},$$scope:{ctx:le}}}),ne=new M({props:{name:"transformers.pytorch_utils.find_pruneable_heads_and_indices",anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices",parameters:[{name:"heads",val:": list[int]"},{name:"n_heads",val:": int"},{name:"head_size",val:": int"},{name:"already_pruned_heads",val:": set[int]"}],parametersDescription:[{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.heads",description:"<strong>heads</strong> (<code>list[int]</code>) &#x2014; List of the indices of heads to prune.",name:"heads"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.n_heads",description:"<strong>n_heads</strong> (<code>int</code>) &#x2014; The number of heads in the model.",name:"n_heads"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.head_size",description:"<strong>head_size</strong> (<code>int</code>) &#x2014; The size of each head.",name:"head_size"},{anchor:"transformers.pytorch_utils.find_pruneable_heads_and_indices.already_pruned_heads",description:"<strong>already_pruned_heads</strong> (<code>Set[int]</code>) &#x2014; A set of already pruned heads.",name:"already_pruned_heads"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L260",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tuple with the indices of heads to prune taking <code>already_pruned_heads</code>
into account and the indices of rows/columns to keep in the layer weight.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>tuple[Set[int], torch.LongTensor]</code></p>
`}}),re=new M({props:{name:"transformers.prune_layer",anchor:"transformers.prune_layer",parameters:[{name:"layer",val:": nn.Linear | Conv1D"},{name:"index",val:": torch.LongTensor"},{name:"dim",val:": int | None = None"}],parametersDescription:[{anchor:"transformers.prune_layer.layer",description:"<strong>layer</strong> (<code>Union[torch.nn.Linear, Conv1D]</code>) &#x2014; The layer to prune.",name:"layer"},{anchor:"transformers.prune_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.prune_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L160",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.nn.Linear</code> or <a
  href="/docs/transformers/v4.56.2/en/internal/modeling_utils#transformers.Conv1D"
>Conv1D</a></p>
`}}),se=new M({props:{name:"transformers.pytorch_utils.prune_conv1d_layer",anchor:"transformers.pytorch_utils.prune_conv1d_layer",parameters:[{name:"layer",val:": Conv1D"},{name:"index",val:": torch.LongTensor"},{name:"dim",val:": int = 1"}],parametersDescription:[{anchor:"transformers.pytorch_utils.prune_conv1d_layer.layer",description:'<strong>layer</strong> (<a href="/docs/transformers/v4.56.2/en/internal/modeling_utils#transformers.Conv1D">Conv1D</a>) &#x2014; The layer to prune.',name:"layer"},{anchor:"transformers.pytorch_utils.prune_conv1d_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.pytorch_utils.prune_conv1d_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L127",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/internal/modeling_utils#transformers.Conv1D"
>Conv1D</a></p>
`}}),ae=new M({props:{name:"transformers.pytorch_utils.prune_linear_layer",anchor:"transformers.pytorch_utils.prune_linear_layer",parameters:[{name:"layer",val:": nn.Linear"},{name:"index",val:": torch.LongTensor"},{name:"dim",val:": int = 0"}],parametersDescription:[{anchor:"transformers.pytorch_utils.prune_linear_layer.layer",description:"<strong>layer</strong> (<code>torch.nn.Linear</code>) &#x2014; The layer to prune.",name:"layer"},{anchor:"transformers.pytorch_utils.prune_linear_layer.index",description:"<strong>index</strong> (<code>torch.LongTensor</code>) &#x2014; The indices to keep in the layer.",name:"index"},{anchor:"transformers.pytorch_utils.prune_linear_layer.dim",description:"<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The dimension on which to keep the indices.",name:"dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/pytorch_utils.py#L64",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The pruned layer as a new layer with <code>requires_grad=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.nn.Linear</code></p>
`}}),oe=new Kt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/internal/modeling_utils.md"}}),{c(){c=i("meta"),V=r(),b=i("p"),y=r(),p($.$$.fragment),d=r(),x=i("p"),x.textContent=wt,Ve=r(),H=i("p"),H.textContent=kt,Ze=r(),p(U.$$.fragment),je=r(),v=i("div"),p(A.$$.fragment),nt=r(),de=i("p"),de.textContent=Tt,rt=r(),ce=i("p"),ce.innerHTML=Ct,st=r(),pe=i("p"),pe.textContent=Mt,at=r(),me=i("p"),me.innerHTML=It,ot=r(),p(z.$$.fragment),Ee=r(),p(X.$$.fragment),Pe=r(),I=i("div"),p(B.$$.fragment),it=r(),fe=i("p"),fe.innerHTML=Dt,lt=r(),ue=i("div"),p(W.$$.fragment),ze=r(),p(Y.$$.fragment),Re=r(),Z=i("div"),p(N.$$.fragment),dt=r(),he=i("div"),p(q.$$.fragment),Fe=r(),p(Q.$$.fragment),He=r(),j=i("div"),p(S.$$.fragment),ct=r(),ge=i("p"),ge.textContent=Gt,Ue=r(),p(K.$$.fragment),Ae=r(),D=i("div"),p(O.$$.fragment),pt=r(),_e=i("p"),_e.textContent=Jt,mt=r(),ye=i("p"),ye.textContent=Lt,Xe=r(),p(ee.$$.fragment),Be=r(),C=i("div"),p(te.$$.fragment),ft=r(),$e=i("p"),$e.innerHTML=Vt,ut=r(),ve=i("p"),ve.innerHTML=Zt,ht=r(),p(R.$$.fragment),We=r(),E=i("div"),p(ne.$$.fragment),gt=r(),be=i("p"),be.innerHTML=jt,Ye=r(),G=i("div"),p(re.$$.fragment),_t=r(),xe=i("p"),xe.textContent=Et,yt=r(),we=i("p"),we.textContent=Pt,Ne=r(),J=i("div"),p(se.$$.fragment),$t=r(),ke=i("p"),ke.textContent=zt,vt=r(),Te=i("p"),Te.textContent=Rt,qe=r(),L=i("div"),p(ae.$$.fragment),bt=r(),Ce=i("p"),Ce.textContent=Ft,xt=r(),Me=i("p"),Me.textContent=Ht,Qe=r(),p(oe.$$.fragment),Se=r(),Le=i("p"),this.h()},l(e){const n=St("svelte-u9bgzb",document.head);c=l(n,"META",{name:!0,content:!0}),n.forEach(t),V=s(e),b=l(e,"P",{}),k(b).forEach(t),y=s(e),m($.$$.fragment,e),d=s(e),x=l(e,"P",{"data-svelte-h":!0}),_(x)!=="svelte-1xex0ip"&&(x.textContent=wt),Ve=s(e),H=l(e,"P",{"data-svelte-h":!0}),_(H)!=="svelte-1ui7w53"&&(H.textContent=kt),Ze=s(e),m(U.$$.fragment,e),je=s(e),v=l(e,"DIV",{class:!0});var w=k(v);m(A.$$.fragment,w),nt=s(w),de=l(w,"P",{"data-svelte-h":!0}),_(de)!=="svelte-bfm9hg"&&(de.textContent=Tt),rt=s(w),ce=l(w,"P",{"data-svelte-h":!0}),_(ce)!=="svelte-1pvdiq"&&(ce.innerHTML=Ct),st=s(w),pe=l(w,"P",{"data-svelte-h":!0}),_(pe)!=="svelte-1sypzf4"&&(pe.textContent=Mt),at=s(w),me=l(w,"P",{"data-svelte-h":!0}),_(me)!=="svelte-mc40q1"&&(me.innerHTML=It),ot=s(w),m(z.$$.fragment,w),w.forEach(t),Ee=s(e),m(X.$$.fragment,e),Pe=s(e),I=l(e,"DIV",{class:!0});var P=k(I);m(B.$$.fragment,P),it=s(P),fe=l(P,"P",{"data-svelte-h":!0}),_(fe)!=="svelte-1md8jok"&&(fe.innerHTML=Dt),lt=s(P),ue=l(P,"DIV",{class:!0});var Ut=k(ue);m(W.$$.fragment,Ut),Ut.forEach(t),P.forEach(t),ze=s(e),m(Y.$$.fragment,e),Re=s(e),Z=l(e,"DIV",{class:!0});var Oe=k(Z);m(N.$$.fragment,Oe),dt=s(Oe),he=l(Oe,"DIV",{class:!0});var At=k(he);m(q.$$.fragment,At),At.forEach(t),Oe.forEach(t),Fe=s(e),m(Q.$$.fragment,e),He=s(e),j=l(e,"DIV",{class:!0});var et=k(j);m(S.$$.fragment,et),ct=s(et),ge=l(et,"P",{"data-svelte-h":!0}),_(ge)!=="svelte-199au1f"&&(ge.textContent=Gt),et.forEach(t),Ue=s(e),m(K.$$.fragment,e),Ae=s(e),D=l(e,"DIV",{class:!0});var Ie=k(D);m(O.$$.fragment,Ie),pt=s(Ie),_e=l(Ie,"P",{"data-svelte-h":!0}),_(_e)!=="svelte-1lta4gb"&&(_e.textContent=Jt),mt=s(Ie),ye=l(Ie,"P",{"data-svelte-h":!0}),_(ye)!=="svelte-6u0wx8"&&(ye.textContent=Lt),Ie.forEach(t),Xe=s(e),m(ee.$$.fragment,e),Be=s(e),C=l(e,"DIV",{class:!0});var F=k(C);m(te.$$.fragment,F),ft=s(F),$e=l(F,"P",{"data-svelte-h":!0}),_($e)!=="svelte-16kfis2"&&($e.innerHTML=Vt),ut=s(F),ve=l(F,"P",{"data-svelte-h":!0}),_(ve)!=="svelte-1ufp6tv"&&(ve.innerHTML=Zt),ht=s(F),m(R.$$.fragment,F),F.forEach(t),We=s(e),E=l(e,"DIV",{class:!0});var tt=k(E);m(ne.$$.fragment,tt),gt=s(tt),be=l(tt,"P",{"data-svelte-h":!0}),_(be)!=="svelte-o28a6l"&&(be.innerHTML=jt),tt.forEach(t),Ye=s(e),G=l(e,"DIV",{class:!0});var De=k(G);m(re.$$.fragment,De),_t=s(De),xe=l(De,"P",{"data-svelte-h":!0}),_(xe)!=="svelte-by2rhc"&&(xe.textContent=Et),yt=s(De),we=l(De,"P",{"data-svelte-h":!0}),_(we)!=="svelte-14zrzk1"&&(we.textContent=Pt),De.forEach(t),Ne=s(e),J=l(e,"DIV",{class:!0});var Ge=k(J);m(se.$$.fragment,Ge),$t=s(Ge),ke=l(Ge,"P",{"data-svelte-h":!0}),_(ke)!=="svelte-1xogpwx"&&(ke.textContent=zt),vt=s(Ge),Te=l(Ge,"P",{"data-svelte-h":!0}),_(Te)!=="svelte-14zrzk1"&&(Te.textContent=Rt),Ge.forEach(t),qe=s(e),L=l(e,"DIV",{class:!0});var Je=k(L);m(ae.$$.fragment,Je),bt=s(Je),Ce=l(Je,"P",{"data-svelte-h":!0}),_(Ce)!=="svelte-1edz4x0"&&(Ce.textContent=Ft),xt=s(Je),Me=l(Je,"P",{"data-svelte-h":!0}),_(Me)!=="svelte-14zrzk1"&&(Me.textContent=Ht),Je.forEach(t),Qe=s(e),m(oe.$$.fragment,e),Se=s(e),Le=l(e,"P",{}),k(Le).forEach(t),this.h()},h(){T(c,"name","hf:doc:metadata"),T(c,"content",nn),T(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){a(document.head,c),o(e,V,n),o(e,b,n),o(e,y,n),f($,e,n),o(e,d,n),o(e,x,n),o(e,Ve,n),o(e,H,n),o(e,Ze,n),f(U,e,n),o(e,je,n),o(e,v,n),f(A,v,null),a(v,nt),a(v,de),a(v,rt),a(v,ce),a(v,st),a(v,pe),a(v,at),a(v,me),a(v,ot),f(z,v,null),o(e,Ee,n),f(X,e,n),o(e,Pe,n),o(e,I,n),f(B,I,null),a(I,it),a(I,fe),a(I,lt),a(I,ue),f(W,ue,null),o(e,ze,n),f(Y,e,n),o(e,Re,n),o(e,Z,n),f(N,Z,null),a(Z,dt),a(Z,he),f(q,he,null),o(e,Fe,n),f(Q,e,n),o(e,He,n),o(e,j,n),f(S,j,null),a(j,ct),a(j,ge),o(e,Ue,n),f(K,e,n),o(e,Ae,n),o(e,D,n),f(O,D,null),a(D,pt),a(D,_e),a(D,mt),a(D,ye),o(e,Xe,n),f(ee,e,n),o(e,Be,n),o(e,C,n),f(te,C,null),a(C,ft),a(C,$e),a(C,ut),a(C,ve),a(C,ht),f(R,C,null),o(e,We,n),o(e,E,n),f(ne,E,null),a(E,gt),a(E,be),o(e,Ye,n),o(e,G,n),f(re,G,null),a(G,_t),a(G,xe),a(G,yt),a(G,we),o(e,Ne,n),o(e,J,n),f(se,J,null),a(J,$t),a(J,ke),a(J,vt),a(J,Te),o(e,qe,n),o(e,L,n),f(ae,L,null),a(L,bt),a(L,Ce),a(L,xt),a(L,Me),o(e,Qe,n),f(oe,e,n),o(e,Se,n),o(e,Le,n),Ke=!0},p(e,[n]){const w={};n&2&&(w.$$scope={dirty:n,ctx:e}),z.$set(w);const P={};n&2&&(P.$$scope={dirty:n,ctx:e}),R.$set(P)},i(e){Ke||(u($.$$.fragment,e),u(U.$$.fragment,e),u(A.$$.fragment,e),u(z.$$.fragment,e),u(X.$$.fragment,e),u(B.$$.fragment,e),u(W.$$.fragment,e),u(Y.$$.fragment,e),u(N.$$.fragment,e),u(q.$$.fragment,e),u(Q.$$.fragment,e),u(S.$$.fragment,e),u(K.$$.fragment,e),u(O.$$.fragment,e),u(ee.$$.fragment,e),u(te.$$.fragment,e),u(R.$$.fragment,e),u(ne.$$.fragment,e),u(re.$$.fragment,e),u(se.$$.fragment,e),u(ae.$$.fragment,e),u(oe.$$.fragment,e),Ke=!0)},o(e){h($.$$.fragment,e),h(U.$$.fragment,e),h(A.$$.fragment,e),h(z.$$.fragment,e),h(X.$$.fragment,e),h(B.$$.fragment,e),h(W.$$.fragment,e),h(Y.$$.fragment,e),h(N.$$.fragment,e),h(q.$$.fragment,e),h(Q.$$.fragment,e),h(S.$$.fragment,e),h(K.$$.fragment,e),h(O.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(R.$$.fragment,e),h(ne.$$.fragment,e),h(re.$$.fragment,e),h(se.$$.fragment,e),h(ae.$$.fragment,e),h(oe.$$.fragment,e),Ke=!1},d(e){e&&(t(V),t(b),t(y),t(d),t(x),t(Ve),t(H),t(Ze),t(je),t(v),t(Ee),t(Pe),t(I),t(ze),t(Re),t(Z),t(Fe),t(He),t(j),t(Ue),t(Ae),t(D),t(Xe),t(Be),t(C),t(We),t(E),t(Ye),t(G),t(Ne),t(J),t(qe),t(L),t(Qe),t(Se),t(Le)),t(c),g($,e),g(U,e),g(A),g(z),g(X,e),g(B),g(W),g(Y,e),g(N),g(q),g(Q,e),g(S),g(K,e),g(O),g(ee,e),g(te),g(R),g(ne),g(re),g(se),g(ae),g(oe,e)}}}const nn='{"title":"Custom Layers and Utilities","local":"custom-layers-and-utilities","sections":[{"title":"Layers","local":"transformers.GradientCheckpointingLayer","sections":[],"depth":2},{"title":"Attention Functions","local":"transformers.AttentionInterface","sections":[],"depth":2},{"title":"Attention Mask Functions","local":"transformers.AttentionMaskInterface","sections":[],"depth":2},{"title":"Rotary Position Embedding Functions","local":"transformers.dynamic_rope_update","sections":[],"depth":2},{"title":"Pytorch custom modules","local":"transformers.Conv1D","sections":[],"depth":2},{"title":"PyTorch Helper Functions","local":"transformers.apply_chunking_to_forward","sections":[],"depth":2}],"depth":1}';function rn(le){return Nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pn extends qt{constructor(c){super(),Qt(this,c,rn,tn,Yt,{})}}export{pn as component};
