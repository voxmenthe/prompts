import{s as bs,o as Ms,n as Xt}from"../chunks/scheduler.18a86fab.js";import{S as $s,i as ws,g as o,s as l,r,A as Ts,h as p,f as s,c as n,j as ys,u as m,x as d,k as gs,y as js,a,v as c,d as f,t as h,w as u}from"../chunks/index.98837b22.js";import{T as Vt}from"../chunks/Tip.77304350.js";import{Y as Us}from"../chunks/Youtube.14fb207c.js";import{C as g}from"../chunks/CodeBlock.8d0c2e8a.js";import{D as vs}from"../chunks/DocNotebookDropdown.a04a6b2a.js";import{H as ke,E as Js}from"../chunks/getInferenceSnippets.06c2775f.js";function ks($){let i,b='To see all architectures and checkpoints compatible with this task, we recommend checking the <a href="https://huggingface.co/tasks/text-classification" rel="nofollow">task-page</a>.';return{c(){i=o("p"),i.innerHTML=b},l(y){i=p(y,"P",{"data-svelte-h":!0}),d(i)!=="svelte-1cytdew"&&(i.innerHTML=b)},m(y,M){a(y,i,M)},p:Xt,d(y){y&&s(i)}}}function _s($){let i,b='If you aren‚Äôt familiar with finetuning a model with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#train-with-pytorch-trainer">here</a>!';return{c(){i=o("p"),i.innerHTML=b},l(y){i=p(y,"P",{"data-svelte-h":!0}),d(i)!=="svelte-p303g8"&&(i.innerHTML=b)},m(y,M){a(y,i,M)},p:Xt,d(y){y&&s(i)}}}function Cs($){let i,b='<a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> applies dynamic padding by default when you pass <code>tokenizer</code> to it. In this case, you don‚Äôt need to specify a data collator explicitly.';return{c(){i=o("p"),i.innerHTML=b},l(y){i=p(y,"P",{"data-svelte-h":!0}),d(i)!=="svelte-1x81xrq"&&(i.innerHTML=b)},m(y,M){a(y,i,M)},p:Xt,d(y){y&&s(i)}}}function xs($){let i,b=`For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb" rel="nofollow">PyTorch notebook</a>.`;return{c(){i=o("p"),i.innerHTML=b},l(y){i=p(y,"P",{"data-svelte-h":!0}),d(i)!=="svelte-fkbkol"&&(i.innerHTML=b)},m(y,M){a(y,i,M)},p:Xt,d(y){y&&s(i)}}}function Zs($){let i,b,y,M,v,Ce,J,xe,k,Ze,_,Et="Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.",Ge,C,Yt="This guide will show you how to:",We,x,Ht='<li>Finetune <a href="https://huggingface.co/distilbert/distilbert-base-uncased" rel="nofollow">DistilBERT</a> on the <a href="https://huggingface.co/datasets/imdb" rel="nofollow">IMDb</a> dataset to determine whether a movie review is positive or negative.</li> <li>Use your finetuned model for inference.</li>',Re,w,Ie,Z,Nt="Before you begin, make sure you have all the necessary libraries installed:",Ve,G,Xe,W,Bt="We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:",Ee,R,Ye,I,He,V,zt="Start by loading the IMDb dataset from the ü§ó Datasets library:",Ne,X,Be,E,Ft="Then take a look at an example:",ze,Y,Fe,H,qt="There are two fields in this dataset:",qe,N,Lt="<li><code>text</code>: the movie review text.</li> <li><code>label</code>: a value that is either <code>0</code> for a negative review or <code>1</code> for a positive review.</li>",Le,B,Ae,z,At="The next step is to load a DistilBERT tokenizer to preprocess the <code>text</code> field:",Qe,F,Se,q,Qt="Create a preprocessing function to tokenize <code>text</code> and truncate sequences to be no longer than DistilBERT‚Äôs maximum input length:",Pe,L,De,A,St='To apply the preprocessing function over the entire dataset, use ü§ó Datasets <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> function. You can speed up <code>map</code> by setting <code>batched=True</code> to process multiple elements of the dataset at once:',Ke,Q,Oe,S,Pt='Now create a batch of examples using <a href="/docs/transformers/v4.56.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a>. It‚Äôs more efficient to <em>dynamically pad</em> the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.',et,P,tt,D,st,K,Dt='Including a metric during training is often helpful for evaluating your model‚Äôs performance. You can quickly load a evaluation method with the ü§ó <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> library. For this task, load the <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy</a> metric (see the ü§ó Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a> to learn more about how to load and compute a metric):',at,O,lt,ee,Kt='Then create a function that passes your predictions and labels to <a href="https://huggingface.co/docs/evaluate/v0.4.5/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a> to calculate the accuracy:',nt,te,it,se,Ot="Your <code>compute_metrics</code> function is ready to go now, and you‚Äôll return to it when you setup your training.",ot,ae,pt,le,es="Before you start training your model, create a map of the expected ids to their labels with <code>id2label</code> and <code>label2id</code>:",rt,ne,mt,T,ct,ie,ts='You‚Äôre ready to start training your model now! Load DistilBERT with <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification">AutoModelForSequenceClassification</a> along with the number of expected labels, and the label mappings:',ft,oe,ht,pe,ss="At this point, only three steps remain:",ut,re,as='<li>Define your training hyperparameters in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. The only required parameter is <code>output_dir</code> which specifies where to save your model. You‚Äôll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> will evaluate the accuracy and save the training checkpoint.</li> <li>Pass the training arguments to <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, data collator, and <code>compute_metrics</code> function.</li> <li>Call <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.train">train()</a> to finetune your model.</li>',dt,me,yt,j,gt,ce,ls='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so everyone can use your model:',bt,fe,Mt,U,$t,he,wt,ue,ns="Great, now that you‚Äôve finetuned a model, you can use it for inference!",Tt,de,is="Grab some text you‚Äôd like to run inference on:",jt,ye,Ut,ge,os='The simplest way to try out your finetuned model for inference is to use it in a <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>. Instantiate a <code>pipeline</code> for sentiment analysis with your model, and pass your text to it:',vt,be,Jt,Me,ps="You can also manually replicate the results of the <code>pipeline</code> if you‚Äôd like:",kt,$e,rs="Tokenize the text and return PyTorch tensors:",_t,we,Ct,Te,ms="Pass your inputs to the model and return the <code>logits</code>:",xt,je,Zt,Ue,cs="Get the class with the highest probability, and use the model‚Äôs <code>id2label</code> mapping to convert it to a text label:",Gt,ve,Wt,Je,Rt,_e,It;return v=new ke({props:{title:"Text classification",local:"text-classification",headingTag:"h1"}}),J=new vs({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/sequence_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/sequence_classification.ipynb"}]}}),k=new Us({props:{id:"leNG9fN9FQU"}}),w=new Vt({props:{$$slots:{default:[ks]},$$scope:{ctx:$}}}),G=new g({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBhY2NlbGVyYXRl",highlighted:"pip install transformers datasets evaluate accelerate",wrap:!1}}),R=new g({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),I=new ke({props:{title:"Load IMDb dataset",local:"load-imdb-dataset",headingTag:"h2"}}),X=new g({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBaW1kYiUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJpbWRiJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`,wrap:!1}}),Y=new g({props:{code:"aW1kYiU1QiUyMnRlc3QlMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`,wrap:!1}}),B=new ke({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),F=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),L=new g({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoZXhhbXBsZXMlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`,wrap:!1}}),Q=new g({props:{code:"dG9rZW5pemVkX2ltZGIlMjAlM0QlMjBpbWRiLm1hcChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwYmF0Y2hlZCUzRFRydWUp",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),P=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),D=new ke({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),O=new g({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),te=new g({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwbnAuYXJnbWF4KHByZWRpY3Rpb25zJTJDJTIwYXhpcyUzRDEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYWNjdXJhY3kuY29tcHV0ZShwcmVkaWN0aW9ucyUzRHByZWRpY3Rpb25zJTJDJTIwcmVmZXJlbmNlcyUzRGxhYmVscyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=labels)`,wrap:!1}}),ae=new ke({props:{title:"Train",local:"train",headingTag:"h2"}}),ne=new g({props:{code:"aWQybGFiZWwlMjAlM0QlMjAlN0IwJTNBJTIwJTIyTkVHQVRJVkUlMjIlMkMlMjAxJTNBJTIwJTIyUE9TSVRJVkUlMjIlN0QlMEFsYWJlbDJpZCUyMCUzRCUyMCU3QiUyMk5FR0FUSVZFJTIyJTNBJTIwMCUyQyUyMCUyMlBPU0lUSVZFJTIyJTNBJTIwMSU3RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;NEGATIVE&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;POSITIVE&quot;</span>: <span class="hljs-number">1</span>}`,wrap:!1}}),T=new Vt({props:{$$slots:{default:[_s]},$$scope:{ctx:$}}}),oe=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG51bV9sYWJlbHMlM0QyJTJDJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUyQyUyMGxhYmVsMmlkJTNEbGFiZWwyaWQlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`,wrap:!1}}),me=new g({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMHdlaWdodF9kZWNheSUzRDAuMDElMkMlMEElMjAlMjAlMjAlMjBldmFsX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEdG9rZW5pemVkX2ltZGIlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRHRva2VuaXplZF9pbWRiJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHByb2Nlc3NpbmdfY2xhc3MlM0R0b2tlbml6ZXIlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_model&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    eval_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    processing_class=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),j=new Vt({props:{$$slots:{default:[Cs]},$$scope:{ctx:$}}}),fe=new g({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),U=new Vt({props:{$$slots:{default:[xs]},$$scope:{ctx:$}}}),he=new ke({props:{title:"Inference",local:"inference",headingTag:"h2"}}),ye=new g({props:{code:"dGV4dCUyMCUzRCUyMCUyMlRoaXMlMjB3YXMlMjBhJTIwbWFzdGVycGllY2UuJTIwTm90JTIwY29tcGxldGVseSUyMGZhaXRoZnVsJTIwdG8lMjB0aGUlMjBib29rcyUyQyUyMGJ1dCUyMGVudGhyYWxsaW5nJTIwZnJvbSUyMGJlZ2lubmluZyUyMHRvJTIwZW5kLiUyME1pZ2h0JTIwYmUlMjBteSUyMGZhdm9yaXRlJTIwb2YlMjB0aGUlMjB0aHJlZS4lMjI=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.&quot;</span>',wrap:!1}}),be=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEJTIyc3RldmhsaXUlMkZteV9hd2Vzb21lX21vZGVsJTIyKSUwQWNsYXNzaWZpZXIodGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(text)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994940757751465</span>}]`,wrap:!1}}),we=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),je=new g({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnN0ZXZobGl1JTJGbXlfYXdlc29tZV9tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),ve=new g({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heCgpLml0ZW0oKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;POSITIVE&#x27;</span>`,wrap:!1}}),Je=new Js({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md"}}),{c(){i=o("meta"),b=l(),y=o("p"),M=l(),r(v.$$.fragment),Ce=l(),r(J.$$.fragment),xe=l(),r(k.$$.fragment),Ze=l(),_=o("p"),_.textContent=Et,Ge=l(),C=o("p"),C.textContent=Yt,We=l(),x=o("ol"),x.innerHTML=Ht,Re=l(),r(w.$$.fragment),Ie=l(),Z=o("p"),Z.textContent=Nt,Ve=l(),r(G.$$.fragment),Xe=l(),W=o("p"),W.textContent=Bt,Ee=l(),r(R.$$.fragment),Ye=l(),r(I.$$.fragment),He=l(),V=o("p"),V.textContent=zt,Ne=l(),r(X.$$.fragment),Be=l(),E=o("p"),E.textContent=Ft,ze=l(),r(Y.$$.fragment),Fe=l(),H=o("p"),H.textContent=qt,qe=l(),N=o("ul"),N.innerHTML=Lt,Le=l(),r(B.$$.fragment),Ae=l(),z=o("p"),z.innerHTML=At,Qe=l(),r(F.$$.fragment),Se=l(),q=o("p"),q.innerHTML=Qt,Pe=l(),r(L.$$.fragment),De=l(),A=o("p"),A.innerHTML=St,Ke=l(),r(Q.$$.fragment),Oe=l(),S=o("p"),S.innerHTML=Pt,et=l(),r(P.$$.fragment),tt=l(),r(D.$$.fragment),st=l(),K=o("p"),K.innerHTML=Dt,at=l(),r(O.$$.fragment),lt=l(),ee=o("p"),ee.innerHTML=Kt,nt=l(),r(te.$$.fragment),it=l(),se=o("p"),se.innerHTML=Ot,ot=l(),r(ae.$$.fragment),pt=l(),le=o("p"),le.innerHTML=es,rt=l(),r(ne.$$.fragment),mt=l(),r(T.$$.fragment),ct=l(),ie=o("p"),ie.innerHTML=ts,ft=l(),r(oe.$$.fragment),ht=l(),pe=o("p"),pe.textContent=ss,ut=l(),re=o("ol"),re.innerHTML=as,dt=l(),r(me.$$.fragment),yt=l(),r(j.$$.fragment),gt=l(),ce=o("p"),ce.innerHTML=ls,bt=l(),r(fe.$$.fragment),Mt=l(),r(U.$$.fragment),$t=l(),r(he.$$.fragment),wt=l(),ue=o("p"),ue.textContent=ns,Tt=l(),de=o("p"),de.textContent=is,jt=l(),r(ye.$$.fragment),Ut=l(),ge=o("p"),ge.innerHTML=os,vt=l(),r(be.$$.fragment),Jt=l(),Me=o("p"),Me.innerHTML=ps,kt=l(),$e=o("p"),$e.textContent=rs,_t=l(),r(we.$$.fragment),Ct=l(),Te=o("p"),Te.innerHTML=ms,xt=l(),r(je.$$.fragment),Zt=l(),Ue=o("p"),Ue.innerHTML=cs,Gt=l(),r(ve.$$.fragment),Wt=l(),r(Je.$$.fragment),Rt=l(),_e=o("p"),this.h()},l(e){const t=Ts("svelte-u9bgzb",document.head);i=p(t,"META",{name:!0,content:!0}),t.forEach(s),b=n(e),y=p(e,"P",{}),ys(y).forEach(s),M=n(e),m(v.$$.fragment,e),Ce=n(e),m(J.$$.fragment,e),xe=n(e),m(k.$$.fragment,e),Ze=n(e),_=p(e,"P",{"data-svelte-h":!0}),d(_)!=="svelte-a44la3"&&(_.textContent=Et),Ge=n(e),C=p(e,"P",{"data-svelte-h":!0}),d(C)!=="svelte-1aff4p7"&&(C.textContent=Yt),We=n(e),x=p(e,"OL",{"data-svelte-h":!0}),d(x)!=="svelte-g478nj"&&(x.innerHTML=Ht),Re=n(e),m(w.$$.fragment,e),Ie=n(e),Z=p(e,"P",{"data-svelte-h":!0}),d(Z)!=="svelte-1c9nexd"&&(Z.textContent=Nt),Ve=n(e),m(G.$$.fragment,e),Xe=n(e),W=p(e,"P",{"data-svelte-h":!0}),d(W)!=="svelte-k76o1m"&&(W.textContent=Bt),Ee=n(e),m(R.$$.fragment,e),Ye=n(e),m(I.$$.fragment,e),He=n(e),V=p(e,"P",{"data-svelte-h":!0}),d(V)!=="svelte-cx4bj0"&&(V.textContent=zt),Ne=n(e),m(X.$$.fragment,e),Be=n(e),E=p(e,"P",{"data-svelte-h":!0}),d(E)!=="svelte-1m91ua0"&&(E.textContent=Ft),ze=n(e),m(Y.$$.fragment,e),Fe=n(e),H=p(e,"P",{"data-svelte-h":!0}),d(H)!=="svelte-q802b4"&&(H.textContent=qt),qe=n(e),N=p(e,"UL",{"data-svelte-h":!0}),d(N)!=="svelte-4d5l79"&&(N.innerHTML=Lt),Le=n(e),m(B.$$.fragment,e),Ae=n(e),z=p(e,"P",{"data-svelte-h":!0}),d(z)!=="svelte-1gepr51"&&(z.innerHTML=At),Qe=n(e),m(F.$$.fragment,e),Se=n(e),q=p(e,"P",{"data-svelte-h":!0}),d(q)!=="svelte-1xsc197"&&(q.innerHTML=Qt),Pe=n(e),m(L.$$.fragment,e),De=n(e),A=p(e,"P",{"data-svelte-h":!0}),d(A)!=="svelte-1v6tgeo"&&(A.innerHTML=St),Ke=n(e),m(Q.$$.fragment,e),Oe=n(e),S=p(e,"P",{"data-svelte-h":!0}),d(S)!=="svelte-3kf86p"&&(S.innerHTML=Pt),et=n(e),m(P.$$.fragment,e),tt=n(e),m(D.$$.fragment,e),st=n(e),K=p(e,"P",{"data-svelte-h":!0}),d(K)!=="svelte-j1ipe9"&&(K.innerHTML=Dt),at=n(e),m(O.$$.fragment,e),lt=n(e),ee=p(e,"P",{"data-svelte-h":!0}),d(ee)!=="svelte-16da6d3"&&(ee.innerHTML=Kt),nt=n(e),m(te.$$.fragment,e),it=n(e),se=p(e,"P",{"data-svelte-h":!0}),d(se)!=="svelte-183aynn"&&(se.innerHTML=Ot),ot=n(e),m(ae.$$.fragment,e),pt=n(e),le=p(e,"P",{"data-svelte-h":!0}),d(le)!=="svelte-18c6io4"&&(le.innerHTML=es),rt=n(e),m(ne.$$.fragment,e),mt=n(e),m(T.$$.fragment,e),ct=n(e),ie=p(e,"P",{"data-svelte-h":!0}),d(ie)!=="svelte-wrxxn8"&&(ie.innerHTML=ts),ft=n(e),m(oe.$$.fragment,e),ht=n(e),pe=p(e,"P",{"data-svelte-h":!0}),d(pe)!=="svelte-l42k0i"&&(pe.textContent=ss),ut=n(e),re=p(e,"OL",{"data-svelte-h":!0}),d(re)!=="svelte-swqbmp"&&(re.innerHTML=as),dt=n(e),m(me.$$.fragment,e),yt=n(e),m(j.$$.fragment,e),gt=n(e),ce=p(e,"P",{"data-svelte-h":!0}),d(ce)!=="svelte-yclg6q"&&(ce.innerHTML=ls),bt=n(e),m(fe.$$.fragment,e),Mt=n(e),m(U.$$.fragment,e),$t=n(e),m(he.$$.fragment,e),wt=n(e),ue=p(e,"P",{"data-svelte-h":!0}),d(ue)!=="svelte-633ppb"&&(ue.textContent=ns),Tt=n(e),de=p(e,"P",{"data-svelte-h":!0}),d(de)!=="svelte-o1jbfg"&&(de.textContent=is),jt=n(e),m(ye.$$.fragment,e),Ut=n(e),ge=p(e,"P",{"data-svelte-h":!0}),d(ge)!=="svelte-1c7zqjr"&&(ge.innerHTML=os),vt=n(e),m(be.$$.fragment,e),Jt=n(e),Me=p(e,"P",{"data-svelte-h":!0}),d(Me)!=="svelte-1njl8vm"&&(Me.innerHTML=ps),kt=n(e),$e=p(e,"P",{"data-svelte-h":!0}),d($e)!=="svelte-1qcz1wr"&&($e.textContent=rs),_t=n(e),m(we.$$.fragment,e),Ct=n(e),Te=p(e,"P",{"data-svelte-h":!0}),d(Te)!=="svelte-f3g043"&&(Te.innerHTML=ms),xt=n(e),m(je.$$.fragment,e),Zt=n(e),Ue=p(e,"P",{"data-svelte-h":!0}),d(Ue)!=="svelte-6mgrol"&&(Ue.innerHTML=cs),Gt=n(e),m(ve.$$.fragment,e),Wt=n(e),m(Je.$$.fragment,e),Rt=n(e),_e=p(e,"P",{}),ys(_e).forEach(s),this.h()},h(){gs(i,"name","hf:doc:metadata"),gs(i,"content",Gs)},m(e,t){js(document.head,i),a(e,b,t),a(e,y,t),a(e,M,t),c(v,e,t),a(e,Ce,t),c(J,e,t),a(e,xe,t),c(k,e,t),a(e,Ze,t),a(e,_,t),a(e,Ge,t),a(e,C,t),a(e,We,t),a(e,x,t),a(e,Re,t),c(w,e,t),a(e,Ie,t),a(e,Z,t),a(e,Ve,t),c(G,e,t),a(e,Xe,t),a(e,W,t),a(e,Ee,t),c(R,e,t),a(e,Ye,t),c(I,e,t),a(e,He,t),a(e,V,t),a(e,Ne,t),c(X,e,t),a(e,Be,t),a(e,E,t),a(e,ze,t),c(Y,e,t),a(e,Fe,t),a(e,H,t),a(e,qe,t),a(e,N,t),a(e,Le,t),c(B,e,t),a(e,Ae,t),a(e,z,t),a(e,Qe,t),c(F,e,t),a(e,Se,t),a(e,q,t),a(e,Pe,t),c(L,e,t),a(e,De,t),a(e,A,t),a(e,Ke,t),c(Q,e,t),a(e,Oe,t),a(e,S,t),a(e,et,t),c(P,e,t),a(e,tt,t),c(D,e,t),a(e,st,t),a(e,K,t),a(e,at,t),c(O,e,t),a(e,lt,t),a(e,ee,t),a(e,nt,t),c(te,e,t),a(e,it,t),a(e,se,t),a(e,ot,t),c(ae,e,t),a(e,pt,t),a(e,le,t),a(e,rt,t),c(ne,e,t),a(e,mt,t),c(T,e,t),a(e,ct,t),a(e,ie,t),a(e,ft,t),c(oe,e,t),a(e,ht,t),a(e,pe,t),a(e,ut,t),a(e,re,t),a(e,dt,t),c(me,e,t),a(e,yt,t),c(j,e,t),a(e,gt,t),a(e,ce,t),a(e,bt,t),c(fe,e,t),a(e,Mt,t),c(U,e,t),a(e,$t,t),c(he,e,t),a(e,wt,t),a(e,ue,t),a(e,Tt,t),a(e,de,t),a(e,jt,t),c(ye,e,t),a(e,Ut,t),a(e,ge,t),a(e,vt,t),c(be,e,t),a(e,Jt,t),a(e,Me,t),a(e,kt,t),a(e,$e,t),a(e,_t,t),c(we,e,t),a(e,Ct,t),a(e,Te,t),a(e,xt,t),c(je,e,t),a(e,Zt,t),a(e,Ue,t),a(e,Gt,t),c(ve,e,t),a(e,Wt,t),c(Je,e,t),a(e,Rt,t),a(e,_e,t),It=!0},p(e,[t]){const fs={};t&2&&(fs.$$scope={dirty:t,ctx:e}),w.$set(fs);const hs={};t&2&&(hs.$$scope={dirty:t,ctx:e}),T.$set(hs);const us={};t&2&&(us.$$scope={dirty:t,ctx:e}),j.$set(us);const ds={};t&2&&(ds.$$scope={dirty:t,ctx:e}),U.$set(ds)},i(e){It||(f(v.$$.fragment,e),f(J.$$.fragment,e),f(k.$$.fragment,e),f(w.$$.fragment,e),f(G.$$.fragment,e),f(R.$$.fragment,e),f(I.$$.fragment,e),f(X.$$.fragment,e),f(Y.$$.fragment,e),f(B.$$.fragment,e),f(F.$$.fragment,e),f(L.$$.fragment,e),f(Q.$$.fragment,e),f(P.$$.fragment,e),f(D.$$.fragment,e),f(O.$$.fragment,e),f(te.$$.fragment,e),f(ae.$$.fragment,e),f(ne.$$.fragment,e),f(T.$$.fragment,e),f(oe.$$.fragment,e),f(me.$$.fragment,e),f(j.$$.fragment,e),f(fe.$$.fragment,e),f(U.$$.fragment,e),f(he.$$.fragment,e),f(ye.$$.fragment,e),f(be.$$.fragment,e),f(we.$$.fragment,e),f(je.$$.fragment,e),f(ve.$$.fragment,e),f(Je.$$.fragment,e),It=!0)},o(e){h(v.$$.fragment,e),h(J.$$.fragment,e),h(k.$$.fragment,e),h(w.$$.fragment,e),h(G.$$.fragment,e),h(R.$$.fragment,e),h(I.$$.fragment,e),h(X.$$.fragment,e),h(Y.$$.fragment,e),h(B.$$.fragment,e),h(F.$$.fragment,e),h(L.$$.fragment,e),h(Q.$$.fragment,e),h(P.$$.fragment,e),h(D.$$.fragment,e),h(O.$$.fragment,e),h(te.$$.fragment,e),h(ae.$$.fragment,e),h(ne.$$.fragment,e),h(T.$$.fragment,e),h(oe.$$.fragment,e),h(me.$$.fragment,e),h(j.$$.fragment,e),h(fe.$$.fragment,e),h(U.$$.fragment,e),h(he.$$.fragment,e),h(ye.$$.fragment,e),h(be.$$.fragment,e),h(we.$$.fragment,e),h(je.$$.fragment,e),h(ve.$$.fragment,e),h(Je.$$.fragment,e),It=!1},d(e){e&&(s(b),s(y),s(M),s(Ce),s(xe),s(Ze),s(_),s(Ge),s(C),s(We),s(x),s(Re),s(Ie),s(Z),s(Ve),s(Xe),s(W),s(Ee),s(Ye),s(He),s(V),s(Ne),s(Be),s(E),s(ze),s(Fe),s(H),s(qe),s(N),s(Le),s(Ae),s(z),s(Qe),s(Se),s(q),s(Pe),s(De),s(A),s(Ke),s(Oe),s(S),s(et),s(tt),s(st),s(K),s(at),s(lt),s(ee),s(nt),s(it),s(se),s(ot),s(pt),s(le),s(rt),s(mt),s(ct),s(ie),s(ft),s(ht),s(pe),s(ut),s(re),s(dt),s(yt),s(gt),s(ce),s(bt),s(Mt),s($t),s(wt),s(ue),s(Tt),s(de),s(jt),s(Ut),s(ge),s(vt),s(Jt),s(Me),s(kt),s($e),s(_t),s(Ct),s(Te),s(xt),s(Zt),s(Ue),s(Gt),s(Wt),s(Rt),s(_e)),s(i),u(v,e),u(J,e),u(k,e),u(w,e),u(G,e),u(R,e),u(I,e),u(X,e),u(Y,e),u(B,e),u(F,e),u(L,e),u(Q,e),u(P,e),u(D,e),u(O,e),u(te,e),u(ae,e),u(ne,e),u(T,e),u(oe,e),u(me,e),u(j,e),u(fe,e),u(U,e),u(he,e),u(ye,e),u(be,e),u(we,e),u(je,e),u(ve,e),u(Je,e)}}}const Gs='{"title":"Text classification","local":"text-classification","sections":[{"title":"Load IMDb dataset","local":"load-imdb-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Ws($){return Ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ns extends $s{constructor(i){super(),ws(this,i,Ws,Zs,bs,{})}}export{Ns as component};
