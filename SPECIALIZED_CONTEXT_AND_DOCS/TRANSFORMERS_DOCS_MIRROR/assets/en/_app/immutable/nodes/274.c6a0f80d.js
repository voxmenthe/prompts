import{s as Wt,o as Ft,n as $e}from"../chunks/scheduler.18a86fab.js";import{S as Bt,i as Lt,g as d,s,r as f,m as Qe,A as Rt,h as m,f as o,c as r,j as de,x as h,u,n as De,k as me,l as Gt,y as c,a as n,v as g,d as b,t as _,w as M}from"../chunks/index.98837b22.js";import{T as ft}from"../chunks/Tip.77304350.js";import{D as ve}from"../chunks/Docstring.a1ef7999.js";import{C as ce}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Zt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Je,E as Ht}from"../chunks/getInferenceSnippets.06c2775f.js";function Xt(k){let a,T=`This model was contributed by <a href="https://huggingface.co/ArthurZ" rel="nofollow">ArthurZ</a>.
Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language tasks.`;return{c(){a=d("p"),a.innerHTML=T},l(i){a=m(i,"P",{"data-svelte-h":!0}),h(a)!=="svelte-7dldjo"&&(a.innerHTML=T)},m(i,p){n(i,a,p)},p:$e,d(i){i&&o(a)}}}function Vt(k){let a,T="Example:",i,p,y;return p=new ce({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1hbWJhMkNvbmZpZyUyQyUyME1hbWJhMk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME1hbWJhMiUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTWFtYmEyQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME1hbWJhMk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Mamba2Config, Mamba2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Mamba2 configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Mamba2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Mamba2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){a=d("p"),a.textContent=T,i=s(),f(p.$$.fragment)},l(l){a=m(l,"P",{"data-svelte-h":!0}),h(a)!=="svelte-11lpom8"&&(a.textContent=T),i=r(l),u(p.$$.fragment,l)},m(l,x){n(l,a,x),n(l,i,x),g(p,l,x),y=!0},p:$e,i(l){y||(b(p.$$.fragment,l),y=!0)},o(l){_(p.$$.fragment,l),y=!1},d(l){l&&(o(a),o(i)),M(p,l)}}}function qt(k){let a,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=d("p"),a.innerHTML=T},l(i){a=m(i,"P",{"data-svelte-h":!0}),h(a)!=="svelte-fincs2"&&(a.innerHTML=T)},m(i,p){n(i,a,p)},p:$e,d(i){i&&o(a)}}}function Et(k){let a,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=d("p"),a.innerHTML=T},l(i){a=m(i,"P",{"data-svelte-h":!0}),h(a)!=="svelte-fincs2"&&(a.innerHTML=T)},m(i,p){n(i,a,p)},p:$e,d(i){i&&o(a)}}}function Nt(k){let a,T="Example:",i,p,y;return p=new ce({props:{code:"",highlighted:"",wrap:!1}}),{c(){a=d("p"),a.textContent=T,i=s(),f(p.$$.fragment)},l(l){a=m(l,"P",{"data-svelte-h":!0}),h(a)!=="svelte-11lpom8"&&(a.textContent=T),i=r(l),u(p.$$.fragment,l)},m(l,x){n(l,a,x),n(l,i,x),g(p,l,x),y=!0},p:$e,i(l){y||(b(p.$$.fragment,l),y=!0)},o(l){_(p.$$.fragment,l),y=!1},d(l){l&&(o(a),o(i)),M(p,l)}}}function Yt(k){let a,T,i,p,y,l="<em>This model was released on 2024-05-31 and added to Hugging Face Transformers on 2024-08-06.</em>",x,W,ut='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Ce,H,ke,X,gt='<a href="https://huggingface.co/papers/2405.21060" rel="nofollow">Mamba 2</a> is based on the state space duality (SSD) framework which connects structured state space models (SSMs) and attention variants. It uses a more efficient SSD algorithm that is 2-8x faster than Mamba and modifies the architecture to enable tensor parallelism and a grouped-value attention (GVA) head structure.',xe,V,bt='You can find all the original Mamba 2 checkpoints under the <a href="https://huggingface.co/state-spaces" rel="nofollow">State Space Models</a> organization, but the examples shown below use <a href="https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1" rel="nofollow">mistralai/Mamba-Codestral-7B-v0.1</a> because a Hugging Face implementation isn’t supported yet for the original checkpoints.',je,F,ze,q,_t='The example below demonstrates how to generate text with <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a>, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and from the command line.',Ue,E,Mt="hfoptions id=“usage”>",Ie,N,Ze,Y,We,P,Fe,A,Tt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',Be,S,yt='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to 4-bit integers.',Le,Q,Re,D,Ge,O,wt='<li><p>Codestral Mamba has <code>groups=8</code> which are similar to the number of kv heads in an attention-based model.</p></li> <li><p>Codestral Mamba has two different forward passes, <code>torch_forward</code> or <code>cuda_kernels_forward</code>, and their results are expected to be slightly different.</p> <ul><li><code>torch_forward</code> without compilation is 3-4x faster than <code>cuda_kernels_forward</code>.</li> <li><code>cuda_kernels_forward</code> uses the original CUDA kernels if they’re available in your environment. It is slower during prefill because it requires a “warmup run” due to the higher CPU overhead (see <a href="https://github.com/state-spaces/mamba/issues/389#issuecomment-2171755306" rel="nofollow">these</a> <a href="https://github.com/state-spaces/mamba/issues/355#issuecomment-2147597457" rel="nofollow">comments</a> for more details).</li></ul></li> <li><p>There are no positional embeddings in this model, but there is an <code>attention_mask</code> and a specific logic to mask out hidden states in two places in the case of batched generation (see this <a href="https://github.com/state-spaces/mamba/issues/66#issuecomment-1863563829" rel="nofollow">comment</a> for more details). This (and the addition of the reimplemented Mamba 2 kernels) results in a slight discrepancy between batched and cached generation.</p></li> <li><p>The SSM algorithm heavily relies on tensor contractions, which have matmul equivalents but the order of operations is slightly different. This makes the difference greater at smaller precisions.</p></li> <li><p>Hidden states that correspond to padding tokens is shutdown in 2 places and is mostly tested with left-padding. Right-padding propagates noise down the line and is not guaranteed to yield satisfactory results. <code>tokenizer.padding_side = &quot;left&quot;</code> ensures you are using the correct padding side.</p></li> <li><p>The example below demonstrates how to fine-tune Mamba 2 with <a href="https://huggingface.co/docs/peft" rel="nofollow">PEFT</a>.</p></li>',He,K,Xe,ee,Ve,J,te,Oe,pe,vt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a>. It is used to instantiate a MAMBA2
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the MAMBA2
<a href="https://huggingface.co/state-spaces/mamba2-2.8b" rel="nofollow">state-spaces/mamba2-2.8b</a> architecture.`,Ke,he,Jt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,et,B,qe,oe,Ee,w,ae,tt,fe,$t="The bare Mamba2 Model outputting raw hidden-states without any specific head on top.",ot,ue,Ct=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,at,ge,kt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,nt,I,ne,st,be,xt='The <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a> forward method, overrides the <code>__call__</code> special method.',rt,L,Ne,se,Ye,v,re,it,_e,jt=`The MAMBA2 Model transformer with a language modeling head on top (linear layer with weights not tied to the input
embeddings).`,lt,Me,zt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,dt,Te,Ut=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,mt,j,ie,ct,ye,It='The <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',pt,R,ht,G,Pe,le,Ae,we,Se;return H=new Je({props:{title:"Mamba 2",local:"mamba-2",headingTag:"h1"}}),F=new ft({props:{warning:!1,$$slots:{default:[Xt]},$$scope:{ctx:k}}}),N=new ce({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFwaXBlbGluZSUyMCUzRCUyMHBpcGVsaW5lKCUwQSUyMCUyMCUyMCUyMHRhc2slM0QlMjJ0ZXh0LWdlbmVyYXRpb24lMjIlMkMlMEElMjAlMjAlMjAlMjBtb2RlbCUzRCUyMm1pc3RyYWxhaSUyRk1hbWJhLUNvZGVzdHJhbC03Qi12MC4xJTIyJTJDJTBBJTIwJTIwJTIwJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUwQSUyMCUyMCUyMCUyMGRldmljZSUzRDAlMEEpJTBBcGlwZWxpbmUoJTIyUGxhbnRzJTIwY3JlYXRlJTIwZW5lcmd5JTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMik=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

pipeline = pipeline(
    task=<span class="hljs-string">&quot;text-generation&quot;</span>,
    model=<span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>,
    dtype=torch.bfloat16,
    device=<span class="hljs-number">0</span>
)
pipeline(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>)`,wrap:!1}}),Y=new ce({props:{code:"aW1wb3J0JTIwdG9yY2glMjAlMjAlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTIwJTIwJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWFtYmEtQ29kZXN0cmFsLTdCLXYwLjElMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWFtYmEtQ29kZXN0cmFsLTdCLXYwLjElMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTIwJTIwJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMlBsYW50cyUyMGNyZWF0ZSUyMGVuZXJneSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTIwJTIwJTBBJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMpJTIwJTIwJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKQ==",highlighted:`<span class="hljs-keyword">import</span> torch  
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer  

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>, dtype=torch.bfloat16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)  
input_ids = tokenizer(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)  

output = model.generate(**input_ids)  
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),P=new ce({props:{code:"ZWNobyUyMC1lJTIwJTIyUGxhbnRzJTIwY3JlYXRlJTIwZW5lcmd5JTIwdGhyb3VnaCUyMGElMjBwcm9jZXNzJTIwa25vd24lMjBhcyUyMiUyMCU3QyUyMHRyYW5zZm9ybWVycy1jbGklMjBydW4lMjAtLXRhc2slMjB0ZXh0LWdlbmVyYXRpb24lMjAtLW1vZGVsJTIwbWlzdHJhbGFpJTJGTWFtYmEtQ29kZXN0cmFsLTdCLXYwLjElMjAtLWRldmljZSUyMDA=",highlighted:'<span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span> | transformers-cli run --task text-generation --model mistralai/Mamba-Codestral-7B-v0.1 --device 0',wrap:!1}}),Q=new ce({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwVG9yY2hBb0NvbmZpZyUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBUb3JjaEFvQ29uZmlnKCUyMmludDRfd2VpZ2h0X29ubHklMjIlMkMlMjBncm91cF9zaXplJTNEMTI4KSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1hbWJhLUNvZGVzdHJhbC03Qi12MC4xJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1hbWJhLUNvZGVzdHJhbC03Qi12MC4xJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RxdWFudGl6YXRpb25fY29uZmlnJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMlBsYW50cyUyMGNyZWF0ZSUyMGVuZXJneSUyMHRocm91Z2glMjBhJTIwcHJvY2VzcyUyMGtub3duJTIwYXMlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMpJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShvdXRwdXQlNUIwJTVEJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

quantization_config = TorchAoConfig(<span class="hljs-string">&quot;int4_weight_only&quot;</span>, group_size=<span class="hljs-number">128</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>, dtype=torch.bfloat16, quantization_config=quantization_config, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
input_ids = tokenizer(<span class="hljs-string">&quot;Plants create energy through a process known as&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

output = model.generate(**input_ids)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),D=new Je({props:{title:"Notes",local:"notes",headingTag:"h2"}}),K=new ce({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHBlZnQlMjBpbXBvcnQlMjBMb3JhQ29uZmlnJTBBZnJvbSUyMHRybCUyMGltcG9ydCUyMFNGVENvbmZpZyUyQyUyMFNGVFRyYWluZXIlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMm1pc3RyYWxhaSUyRk1hbWJhLUNvZGVzdHJhbC03Qi12MC4xJTIyJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJBYmlyYXRlJTJGZW5nbGlzaF9xdW90ZXMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQXRyYWluaW5nX2FyZ3MlMjAlM0QlMjBTRlRDb25maWcoZGF0YXNldF90ZXh0X2ZpZWxkJTNEJTIycXVvdGUlMjIlMkMlMjBncmFkaWVudF9jaGVja3BvaW50aW5nJTNEVHJ1ZSUyQyUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRDQpJTBBbG9yYV9jb25maWclMjAlM0QlMjAlMjBMb3JhQ29uZmlnKHRhcmdldF9tb2R1bGVzJTNEJTVCJTIyeF9wcm9qJTIyJTJDJTIwJTIyZW1iZWRkaW5ncyUyMiUyQyUyMCUyMmluX3Byb2olMjIlMkMlMjAlMjJvdXRfcHJvaiUyMiU1RCklMEF0cmFpbmVyJTIwJTNEJTIwU0ZUVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCUyQyUwQSUyMCUyMCUyMCUyMHBlZnRfY29uZmlnJTNEbG9yYV9jb25maWclMkMlMEEpJTBBdHJhaW5lci50cmFpbigp",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig
<span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> SFTConfig, SFTTrainer

model_id = <span class="hljs-string">&quot;mistralai/Mamba-Codestral-7B-v0.1&quot;</span>
dataset = load_dataset(<span class="hljs-string">&quot;Abirate/english_quotes&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
training_args = SFTConfig(dataset_text_field=<span class="hljs-string">&quot;quote&quot;</span>, gradient_checkpointing=<span class="hljs-literal">True</span>, per_device_train_batch_size=<span class="hljs-number">4</span>)
lora_config =  LoraConfig(target_modules=[<span class="hljs-string">&quot;x_proj&quot;</span>, <span class="hljs-string">&quot;embeddings&quot;</span>, <span class="hljs-string">&quot;in_proj&quot;</span>, <span class="hljs-string">&quot;out_proj&quot;</span>])
trainer = SFTTrainer(
    model=model_id,
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,
)
trainer.train()`,wrap:!1}}),ee=new Je({props:{title:"Mamba2Config",local:"transformers.Mamba2Config",headingTag:"h2"}}),te=new ve({props:{name:"class transformers.Mamba2Config",anchor:"transformers.Mamba2Config",parameters:[{name:"num_heads",val:" = 128"},{name:"head_dim",val:" = 64"},{name:"vocab_size",val:" = 32768"},{name:"hidden_size",val:" = 4096"},{name:"state_size",val:" = 128"},{name:"num_hidden_layers",val:" = 64"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"expand",val:" = 2"},{name:"conv_kernel",val:" = 4"},{name:"n_groups",val:" = 8"},{name:"use_bias",val:" = False"},{name:"use_conv_bias",val:" = True"},{name:"hidden_act",val:" = 'silu'"},{name:"initializer_range",val:" = 0.1"},{name:"residual_in_fp32",val:" = True"},{name:"time_step_rank",val:" = 'auto'"},{name:"time_step_min",val:" = 0.001"},{name:"time_step_max",val:" = 0.1"},{name:"time_step_floor",val:" = 0.0001"},{name:"time_step_limit",val:" = (0.0, inf)"},{name:"rescale_prenorm_residual",val:" = False"},{name:"use_cache",val:" = True"},{name:"rms_norm",val:" = True"},{name:"chunk_size",val:" = 256"},{name:"tie_word_embeddings",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Mamba2Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of heads for the evolution matrices of mamba 2.`,name:"num_heads"},{anchor:"transformers.Mamba2Config.head_dim",description:`<strong>head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimension of each head.`,name:"head_dim"},{anchor:"transformers.Mamba2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32768) &#x2014;
Vocabulary size of the MAMBA2 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a>.`,name:"vocab_size"},{anchor:"transformers.Mamba2Config.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the embeddings and hidden states.`,name:"hidden_size"},{anchor:"transformers.Mamba2Config.state_size",description:"<strong>state_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014; shape of the state space latents.",name:"state_size"},{anchor:"transformers.Mamba2Config.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Number of hidden layers in the model.`,name:"num_hidden_layers"},{anchor:"transformers.Mamba2Config.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon to use in the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.Mamba2Config.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.Mamba2Config.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The id of the beginning of sentence token in the vocabulary.`,name:"bos_token_id"},{anchor:"transformers.Mamba2Config.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the end of sentence token in the vocabulary.`,name:"eos_token_id"},{anchor:"transformers.Mamba2Config.expand",description:"<strong>expand</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; Expanding factor used to determine the intermediate size.",name:"expand"},{anchor:"transformers.Mamba2Config.conv_kernel",description:"<strong>conv_kernel</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; Size of the convolution kernel.",name:"conv_kernel"},{anchor:"transformers.Mamba2Config.n_groups",description:`<strong>n_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of groups for the evolution matrices of mamba 2.`,name:"n_groups"},{anchor:"transformers.Mamba2Config.use_bias",description:`<strong>use_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use bias in [&#x201C;in_proj&#x201D;, &#x201C;out_proj&#x201D;] of the mixer block`,name:"use_bias"},{anchor:"transformers.Mamba2Config.use_conv_bias",description:`<strong>use_conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the convolution layer of the mixer block.`,name:"use_conv_bias"},{anchor:"transformers.Mamba2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.Mamba2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Mamba2Config.residual_in_fp32",description:`<strong>residual_in_fp32</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not residuals should be in <code>float32</code>. If set to <code>False</code> residuals will keep the same <code>dtype</code> as the rest of the model`,name:"residual_in_fp32"},{anchor:"transformers.Mamba2Config.time_step_rank",description:`<strong>time_step_rank</strong> (<code>Union[int,str]</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
Rank of the discretization projection matrix. <code>&quot;auto&quot;</code> means that it will default to <code>math.ceil(self.hidden_size / 16)</code>`,name:"time_step_rank"},{anchor:"transformers.Mamba2Config.time_step_min",description:`<strong>time_step_min</strong> (<code>float</code>, <em>optional</em>, defaults to 0.001) &#x2014;
Minimum <code>time_step</code> used to bound <code>dt_proj.bias</code>.`,name:"time_step_min"},{anchor:"transformers.Mamba2Config.time_step_max",description:`<strong>time_step_max</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Maximum <code>time_step</code> used to bound <code>dt_proj.bias</code>.`,name:"time_step_max"},{anchor:"transformers.Mamba2Config.time_step_floor",description:`<strong>time_step_floor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0001) &#x2014;
Minimum clamping value of the <code>dt_proj.bias</code> layer initialization.`,name:"time_step_floor"},{anchor:"transformers.Mamba2Config.time_step_limit",description:`<strong>time_step_limit</strong> (<code>tuple</code>, <em>optional</em>, defaults to <code>(0.0, inf)</code>) &#x2014;
Accepted range of time step values.`,name:"time_step_limit"},{anchor:"transformers.Mamba2Config.rescale_prenorm_residual",description:`<strong>rescale_prenorm_residual</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale <code>out_proj</code> weights when initializing.`,name:"rescale_prenorm_residual"},{anchor:"transformers.Mamba2Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the cache should be used.`,name:"use_cache"},{anchor:"transformers.Mamba2Config.rms_norm",description:`<strong>rms_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use RMS norm or not.`,name:"rms_norm"},{anchor:"transformers.Mamba2Config.chunk_size",description:`<strong>chunk_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Size of the chunks that will comprise the sequence.`,name:"chunk_size"},{anchor:"transformers.Mamba2Config.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie word embeddings or not.`,name:"tie_word_embeddings"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba2/configuration_mamba2.py#L26"}}),B=new Zt({props:{anchor:"transformers.Mamba2Config.example",$$slots:{default:[Vt]},$$scope:{ctx:k}}}),oe=new Je({props:{title:"Mamba2Model",local:"transformers.Mamba2Model",headingTag:"h2"}}),ae=new ve({props:{name:"class transformers.Mamba2Model",anchor:"transformers.Mamba2Model",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Mamba2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Model">Mamba2Model</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba2/modeling_mamba2.py#L825"}}),ne=new ve({props:{name:"forward",anchor:"transformers.Mamba2Model.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.LongTensor] = None"},{name:"cache_params",val:": typing.Optional[transformers.models.mamba2.modeling_mamba2.Mamba2Cache] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Mamba2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Mamba2Model.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Mamba2Model.forward.cache_params",description:`<strong>cache_params</strong> (<code>Mamba2Cache</code>, <em>optional</em>) &#x2014;
If passed along, the model uses the previous state in all the blocks (which will give the output for the
<code>input_ids</code> provided as if the model add <code>state_input_ids + input_ids</code> as context).`,name:"cache_params"},{anchor:"transformers.Mamba2Model.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the <code>cache_params</code> is returned and can be used to quickly generate the next logits.`,name:"use_cache"},{anchor:"transformers.Mamba2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Mamba2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Mamba2Model.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
The position of the current input in the cache. This is used to ensure that the cache is correctly updated.
If <code>cache_params</code> is passed, <code>cache_position</code> should also be passed.`,name:"cache_position"},{anchor:"transformers.Mamba2Model.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba2/modeling_mamba2.py#L850",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.mamba2.modeling_mamba2.Mamba2Output</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config"
>Mamba2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>, defaults to <code>None</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>cache_params</strong> (<code>~models.mamba2.modeling_mamba2.Mamba2Cache</code>, <em>optional</em>, defaults to <code>None</code>) — The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.mamba2.modeling_mamba2.Mamba2Output</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),L=new ft({props:{$$slots:{default:[qt]},$$scope:{ctx:k}}}),se=new Je({props:{title:"Mamba2LMHeadModel",local:"transformers.Mamba2ForCausalLM",headingTag:"h2"}}),re=new ve({props:{name:"class transformers.Mamba2ForCausalLM",anchor:"transformers.Mamba2ForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Mamba2ForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2ForCausalLM">Mamba2ForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba2/modeling_mamba2.py#L940"}}),ie=new ve({props:{name:"forward",anchor:"transformers.Mamba2ForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"cache_params",val:": typing.Optional[transformers.models.mamba2.modeling_mamba2.Mamba2Cache] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Mamba2ForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Mamba2ForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.Mamba2ForCausalLM.forward.cache_params",description:`<strong>cache_params</strong> (<code>Mamba2Cache</code>, <em>optional</em>) &#x2014;
If passed along, the model uses the previous state in all the blocks (which will give the output for the
<code>input_ids</code> provided as if the model add <code>state_input_ids + input_ids</code> as context).`,name:"cache_params"},{anchor:"transformers.Mamba2ForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.Mamba2ForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Mamba2ForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Mamba2ForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the <code>cache_params</code> is returned and can be used to quickly generate the next logits.`,name:"use_cache"},{anchor:"transformers.Mamba2ForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
The position of the current input in the cache. This is used to ensure that the cache is correctly updated.
If <code>cache_params</code> is passed, <code>cache_position</code> should also be passed.`,name:"cache_position"},{anchor:"transformers.Mamba2ForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mamba2/modeling_mamba2.py#L998",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.mamba2.modeling_mamba2.Mamba2CausalLMOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/mamba2#transformers.Mamba2Config"
>Mamba2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>cache_params</strong> (<code>~models.mamba2.modeling_mamba2.Mamba2Cache</code>, <em>optional</em>, defaults to <code>None</code>) — The state of the model at the last time step. Can be used in a forward method with the next <code>input_ids</code> to
avoid providing the old <code>input_ids</code>.</p>
<p>Includes both the State space model state matrices after the selective scan, and the Convolutional states</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.mamba2.modeling_mamba2.Mamba2CausalLMOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new ft({props:{$$slots:{default:[Et]},$$scope:{ctx:k}}}),G=new Zt({props:{anchor:"transformers.Mamba2ForCausalLM.forward.example",$$slots:{default:[Nt]},$$scope:{ctx:k}}}),le=new Ht({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba2.md"}}),{c(){a=d("meta"),T=s(),i=d("p"),p=s(),y=d("p"),y.innerHTML=l,x=s(),W=d("div"),W.innerHTML=ut,Ce=s(),f(H.$$.fragment),ke=s(),X=d("p"),X.innerHTML=gt,xe=s(),V=d("p"),V.innerHTML=bt,je=s(),f(F.$$.fragment),ze=s(),q=d("p"),q.innerHTML=_t,Ue=s(),E=d("p"),E.textContent=Mt,Ie=Qe(`
<hfoption id="Pipeline">

	`),f(N.$$.fragment),Ze=Qe(`
</hfoption>
<hfoption id="AutoModel">

	`),f(Y.$$.fragment),We=Qe(`
</hfoption>
<hfoption id="transformers CLI">

	`),f(P.$$.fragment),Fe=Qe(`
</hfoption>
</hfoptions>
`),A=d("p"),A.innerHTML=Tt,Be=s(),S=d("p"),S.innerHTML=yt,Le=s(),f(Q.$$.fragment),Re=s(),f(D.$$.fragment),Ge=s(),O=d("ul"),O.innerHTML=wt,He=s(),f(K.$$.fragment),Xe=s(),f(ee.$$.fragment),Ve=s(),J=d("div"),f(te.$$.fragment),Oe=s(),pe=d("p"),pe.innerHTML=vt,Ke=s(),he=d("p"),he.innerHTML=Jt,et=s(),f(B.$$.fragment),qe=s(),f(oe.$$.fragment),Ee=s(),w=d("div"),f(ae.$$.fragment),tt=s(),fe=d("p"),fe.textContent=$t,ot=s(),ue=d("p"),ue.innerHTML=Ct,at=s(),ge=d("p"),ge.innerHTML=kt,nt=s(),I=d("div"),f(ne.$$.fragment),st=s(),be=d("p"),be.innerHTML=xt,rt=s(),f(L.$$.fragment),Ne=s(),f(se.$$.fragment),Ye=s(),v=d("div"),f(re.$$.fragment),it=s(),_e=d("p"),_e.textContent=jt,lt=s(),Me=d("p"),Me.innerHTML=zt,dt=s(),Te=d("p"),Te.innerHTML=Ut,mt=s(),j=d("div"),f(ie.$$.fragment),ct=s(),ye=d("p"),ye.innerHTML=It,pt=s(),f(R.$$.fragment),ht=s(),f(G.$$.fragment),Pe=s(),f(le.$$.fragment),Ae=s(),we=d("p"),this.h()},l(e){const t=Rt("svelte-u9bgzb",document.head);a=m(t,"META",{name:!0,content:!0}),t.forEach(o),T=r(e),i=m(e,"P",{}),de(i).forEach(o),p=r(e),y=m(e,"P",{"data-svelte-h":!0}),h(y)!=="svelte-1a374yh"&&(y.innerHTML=l),x=r(e),W=m(e,"DIV",{style:!0,"data-svelte-h":!0}),h(W)!=="svelte-ceaj4l"&&(W.innerHTML=ut),Ce=r(e),u(H.$$.fragment,e),ke=r(e),X=m(e,"P",{"data-svelte-h":!0}),h(X)!=="svelte-1hbt8th"&&(X.innerHTML=gt),xe=r(e),V=m(e,"P",{"data-svelte-h":!0}),h(V)!=="svelte-16w2cju"&&(V.innerHTML=bt),je=r(e),u(F.$$.fragment,e),ze=r(e),q=m(e,"P",{"data-svelte-h":!0}),h(q)!=="svelte-17pa8jt"&&(q.innerHTML=_t),Ue=r(e),E=m(e,"P",{"data-svelte-h":!0}),h(E)!=="svelte-4wh1t4"&&(E.textContent=Mt),Ie=De(e,`
<hfoption id="Pipeline">

	`),u(N.$$.fragment,e),Ze=De(e,`
</hfoption>
<hfoption id="AutoModel">

	`),u(Y.$$.fragment,e),We=De(e,`
</hfoption>
<hfoption id="transformers CLI">

	`),u(P.$$.fragment,e),Fe=De(e,`
</hfoption>
</hfoptions>
`),A=m(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-nf5ooi"&&(A.innerHTML=Tt),Be=r(e),S=m(e,"P",{"data-svelte-h":!0}),h(S)!=="svelte-1bwmz6y"&&(S.innerHTML=yt),Le=r(e),u(Q.$$.fragment,e),Re=r(e),u(D.$$.fragment,e),Ge=r(e),O=m(e,"UL",{"data-svelte-h":!0}),h(O)!=="svelte-1qw28oj"&&(O.innerHTML=wt),He=r(e),u(K.$$.fragment,e),Xe=r(e),u(ee.$$.fragment,e),Ve=r(e),J=m(e,"DIV",{class:!0});var z=de(J);u(te.$$.fragment,z),Oe=r(z),pe=m(z,"P",{"data-svelte-h":!0}),h(pe)!=="svelte-95v3a1"&&(pe.innerHTML=vt),Ke=r(z),he=m(z,"P",{"data-svelte-h":!0}),h(he)!=="svelte-1ek1ss9"&&(he.innerHTML=Jt),et=r(z),u(B.$$.fragment,z),z.forEach(o),qe=r(e),u(oe.$$.fragment,e),Ee=r(e),w=m(e,"DIV",{class:!0});var $=de(w);u(ae.$$.fragment,$),tt=r($),fe=m($,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-ey9rgy"&&(fe.textContent=$t),ot=r($),ue=m($,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-q52n56"&&(ue.innerHTML=Ct),at=r($),ge=m($,"P",{"data-svelte-h":!0}),h(ge)!=="svelte-hswkmf"&&(ge.innerHTML=kt),nt=r($),I=m($,"DIV",{class:!0});var Z=de(I);u(ne.$$.fragment,Z),st=r(Z),be=m(Z,"P",{"data-svelte-h":!0}),h(be)!=="svelte-gzwnvh"&&(be.innerHTML=xt),rt=r(Z),u(L.$$.fragment,Z),Z.forEach(o),$.forEach(o),Ne=r(e),u(se.$$.fragment,e),Ye=r(e),v=m(e,"DIV",{class:!0});var C=de(v);u(re.$$.fragment,C),it=r(C),_e=m(C,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-14gyjie"&&(_e.textContent=jt),lt=r(C),Me=m(C,"P",{"data-svelte-h":!0}),h(Me)!=="svelte-q52n56"&&(Me.innerHTML=zt),dt=r(C),Te=m(C,"P",{"data-svelte-h":!0}),h(Te)!=="svelte-hswkmf"&&(Te.innerHTML=Ut),mt=r(C),j=m(C,"DIV",{class:!0});var U=de(j);u(ie.$$.fragment,U),ct=r(U),ye=m(U,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-752vbd"&&(ye.innerHTML=It),pt=r(U),u(R.$$.fragment,U),ht=r(U),u(G.$$.fragment,U),U.forEach(o),C.forEach(o),Pe=r(e),u(le.$$.fragment,e),Ae=r(e),we=m(e,"P",{}),de(we).forEach(o),this.h()},h(){me(a,"name","hf:doc:metadata"),me(a,"content",Pt),Gt(W,"float","right"),me(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),me(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),me(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),me(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),me(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,a),n(e,T,t),n(e,i,t),n(e,p,t),n(e,y,t),n(e,x,t),n(e,W,t),n(e,Ce,t),g(H,e,t),n(e,ke,t),n(e,X,t),n(e,xe,t),n(e,V,t),n(e,je,t),g(F,e,t),n(e,ze,t),n(e,q,t),n(e,Ue,t),n(e,E,t),n(e,Ie,t),g(N,e,t),n(e,Ze,t),g(Y,e,t),n(e,We,t),g(P,e,t),n(e,Fe,t),n(e,A,t),n(e,Be,t),n(e,S,t),n(e,Le,t),g(Q,e,t),n(e,Re,t),g(D,e,t),n(e,Ge,t),n(e,O,t),n(e,He,t),g(K,e,t),n(e,Xe,t),g(ee,e,t),n(e,Ve,t),n(e,J,t),g(te,J,null),c(J,Oe),c(J,pe),c(J,Ke),c(J,he),c(J,et),g(B,J,null),n(e,qe,t),g(oe,e,t),n(e,Ee,t),n(e,w,t),g(ae,w,null),c(w,tt),c(w,fe),c(w,ot),c(w,ue),c(w,at),c(w,ge),c(w,nt),c(w,I),g(ne,I,null),c(I,st),c(I,be),c(I,rt),g(L,I,null),n(e,Ne,t),g(se,e,t),n(e,Ye,t),n(e,v,t),g(re,v,null),c(v,it),c(v,_e),c(v,lt),c(v,Me),c(v,dt),c(v,Te),c(v,mt),c(v,j),g(ie,j,null),c(j,ct),c(j,ye),c(j,pt),g(R,j,null),c(j,ht),g(G,j,null),n(e,Pe,t),g(le,e,t),n(e,Ae,t),n(e,we,t),Se=!0},p(e,[t]){const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),F.$set(z);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),B.$set($);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),L.$set(Z);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),R.$set(C);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),G.$set(U)},i(e){Se||(b(H.$$.fragment,e),b(F.$$.fragment,e),b(N.$$.fragment,e),b(Y.$$.fragment,e),b(P.$$.fragment,e),b(Q.$$.fragment,e),b(D.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(B.$$.fragment,e),b(oe.$$.fragment,e),b(ae.$$.fragment,e),b(ne.$$.fragment,e),b(L.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(R.$$.fragment,e),b(G.$$.fragment,e),b(le.$$.fragment,e),Se=!0)},o(e){_(H.$$.fragment,e),_(F.$$.fragment,e),_(N.$$.fragment,e),_(Y.$$.fragment,e),_(P.$$.fragment,e),_(Q.$$.fragment,e),_(D.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(B.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(ne.$$.fragment,e),_(L.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(R.$$.fragment,e),_(G.$$.fragment,e),_(le.$$.fragment,e),Se=!1},d(e){e&&(o(T),o(i),o(p),o(y),o(x),o(W),o(Ce),o(ke),o(X),o(xe),o(V),o(je),o(ze),o(q),o(Ue),o(E),o(Ie),o(Ze),o(We),o(Fe),o(A),o(Be),o(S),o(Le),o(Re),o(Ge),o(O),o(He),o(Xe),o(Ve),o(J),o(qe),o(Ee),o(w),o(Ne),o(Ye),o(v),o(Pe),o(Ae),o(we)),o(a),M(H,e),M(F,e),M(N,e),M(Y,e),M(P,e),M(Q,e),M(D,e),M(K,e),M(ee,e),M(te),M(B),M(oe,e),M(ae),M(ne),M(L),M(se,e),M(re),M(ie),M(R),M(G),M(le,e)}}}const Pt='{"title":"Mamba 2","local":"mamba-2","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Mamba2Config","local":"transformers.Mamba2Config","sections":[],"depth":2},{"title":"Mamba2Model","local":"transformers.Mamba2Model","sections":[],"depth":2},{"title":"Mamba2LMHeadModel","local":"transformers.Mamba2ForCausalLM","sections":[],"depth":2}],"depth":1}';function At(k){return Ft(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class oo extends Bt{constructor(a){super(),Lt(this,a,At,Yt,Wt,{})}}export{oo as component};
