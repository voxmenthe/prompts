import{s as Ga,o as Xa,n as va}from"../chunks/scheduler.18a86fab.js";import{S as Ja,i as Qa,g as l,s as n,r as p,A as Ya,h as s,f as t,c as r,j as $,u as c,x as d,k as b,y as a,a as i,v as g,d as f,t as u,w as h,m as Za,n as eo}from"../chunks/index.98837b22.js";import{T as ba}from"../chunks/Tip.77304350.js";import{D as C}from"../chunks/Docstring.a1ef7999.js";import{H as A,E as to}from"../chunks/getInferenceSnippets.06c2775f.js";function ao(U){let m,P=`For best performance, this data collator should be used with a dataset having items that are dictionaries or
BatchEncoding, with the <code>&quot;special_tokens_mask&quot;</code> key, as returned by a <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or a
<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> with the argument <code>return_special_tokens_mask=True</code>.`,_,x,I="<li><p>Default Behavior:</p> <ul><li><code>mask_replace_prob=0.8</code>, <code>random_replace_prob=0.1</code>.</li> <li>Expect 80% of masked tokens replaced with <code>[MASK]</code>, 10% replaced with random tokens, and 10% left unchanged.</li></ul></li> <li><p>All masked tokens replaced by <code>[MASK]</code>:</p> <ul><li><code>mask_replace_prob=1.0</code>, <code>random_replace_prob=0.0</code>.</li> <li>Expect all masked tokens to be replaced with <code>[MASK]</code>. No tokens are left unchanged or replaced with random tokens.</li></ul></li> <li><p>No <code>[MASK]</code> replacement, only random tokens:</p> <ul><li><code>mask_replace_prob=0.0</code>, <code>random_replace_prob=1.0</code>.</li> <li>Expect all masked tokens to be replaced with random tokens. No <code>[MASK]</code> replacements or unchanged tokens.</li></ul></li> <li><p>Balanced replacement:</p> <ul><li><code>mask_replace_prob=0.5</code>, <code>random_replace_prob=0.4</code>.</li> <li>Expect 50% of masked tokens replaced with <code>[MASK]</code>, 40% replaced with random tokens, and 10% left unchanged.</li></ul></li>",O,y,nt=`Note:
The sum of <code>mask_replace_prob</code> and <code>random_replace_prob</code> must not exceed 1. If their sum is less than 1, the
remaining proportion will consist of masked tokens left unchanged.`;return{c(){m=l("p"),m.innerHTML=P,_=Za(`
<Example Options and Expectations>
`),x=l("ol"),x.innerHTML=I,O=n(),y=l("p"),y.innerHTML=nt},l(v){m=s(v,"P",{"data-svelte-h":!0}),d(m)!=="svelte-1s3ii3q"&&(m.innerHTML=P),_=eo(v,`
<Example Options and Expectations>
`),x=s(v,"OL",{"data-svelte-h":!0}),d(x)!=="svelte-14xwpjc"&&(x.innerHTML=I),O=r(v),y=s(v,"P",{"data-svelte-h":!0}),d(y)!=="svelte-10bg7dp"&&(y.innerHTML=nt)},m(v,F){i(v,m,F),i(v,_,F),i(v,x,F),i(v,O,F),i(v,y,F)},p:va,d(v){v&&(t(m),t(_),t(x),t(O),t(y))}}}function oo(U){let m,P=`This collator relies on details of the implementation of subword tokenization by <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>, specifically
that subword tokens are prefixed with <em>##</em>. For tokenizers that do not adhere to this scheme, this collator will
produce an output that is roughly equivalent to <code>.DataCollatorForLanguageModeling</code>.`;return{c(){m=l("p"),m.innerHTML=P},l(_){m=s(_,"P",{"data-svelte-h":!0}),d(m)!=="svelte-1pyjk9r"&&(m.innerHTML=P)},m(_,x){i(_,m,x)},p:va,d(_){_&&t(m)}}}function no(U){let m,P=`Using <code>DataCollatorWithFlattening</code> will flatten the entire mini batch into single long sequence.
Make sure your attention computation is able to handle it!`;return{c(){m=l("p"),m.innerHTML=P},l(_){m=s(_,"P",{"data-svelte-h":!0}),d(m)!=="svelte-ai37qf"&&(m.innerHTML=P)},m(_,x){i(_,m,x)},p:va,d(_){_&&t(m)}}}function ro(U){let m,P,_,x,I,O,y,nt=`Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of
the same type as the elements of <code>train_dataset</code> or <code>eval_dataset</code>.`,v,F,$a=`To be able to build batches, data collators may apply some processing (like padding). Some of them (like
<a href="/docs/transformers/v4.56.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling">DataCollatorForLanguageModeling</a>) also apply some random data augmentation (like random masking)
on the formed batch.`,lt,te,ka='Examples of use can be found in the <a href="../examples">example scripts</a> or <a href="../notebooks">example notebooks</a>.',st,ae,it,M,oe,Wt,Le,xa=`Very simple data collator that simply collates batches of dict-like objects and performs special handling for
potential keys named:`,Et,ze,Ca="<li><code>label</code>: handles a single value (int or float) per object</li> <li><code>label_ids</code>: handles a list of values per object</li>",Ht,Se,ya=`Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs
to the model. See glue and ner for example of how it’s useful.`,dt,ne,mt,T,re,Nt,Ae,Ta=`Very simple data collator that simply collates batches of dict-like objects and performs special handling for
potential keys named:`,Ot,Ie,Da="<li><code>label</code>: handles a single value (int or float) per object</li> <li><code>label_ids</code>: handles a list of values per object</li>",Vt,We,wa=`Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs
to the model. See glue and ner for example of how it’s useful.`,jt,Ee,Pa=`This is an object (like other data collators) rather than a pure function like default_data_collator. This can be
helpful if you need to set a return_tensors value at initialization.`,pt,le,ct,V,se,Kt,He,Fa="Data collator that will dynamically pad the inputs received.",gt,ie,ft,j,de,Bt,Ne,Ma="Data collator that will dynamically pad the inputs received, as well as the labels.",ut,me,ht,K,pe,Ut,Oe,qa="Data collator that will dynamically pad the inputs received, as well as the labels.",_t,ce,bt,D,ge,Rt,Ve,La=`Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they
are not all of the same length.`,Gt,R,Xt,G,fe,Jt,je,za="Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.",Qt,X,ue,Yt,Ke,Sa="Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.",vt,he,$t,k,_e,Zt,Be,Aa="Data collator used for language modeling that masks entire words.",ea,Ue,Ia="<li>collates batches of tensors, honoring their tokenizer’s pad_token</li> <li>preprocesses batches for masked language modeling</li>",ta,J,aa,Q,be,oa,Re,Wa=`Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set
‘mask_labels’ means we use whole word mask (wwm), we directly mask idxs according to it’s ref.`,na,Y,ve,ra,Ge,Ea=`Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set
‘mask_labels’ means we use whole word mask (wwm), we directly mask idxs according to it’s ref.`,kt,$e,xt,w,ke,la,Xe,Ha="Data collator used for permutation language modeling.",sa,Je,Na="<li>collates batches of tensors, honoring their tokenizer’s pad_token</li> <li>preprocesses batches for permutation language modeling with procedures specific to XLNet</li>",ia,W,xe,da,Qe,Oa="The masked tokens to be predicted for a particular sequence are determined by the following algorithm:",ma,Ce,Va=`<li>Start from the beginning of the sequence by setting <code>cur_len = 0</code> (number of tokens processed so far).</li> <li>Sample a <code>span_length</code> from the interval <code>[1, max_span_length]</code> (length of span of tokens to be masked)</li> <li>Reserve a context of length <code>context_length = span_length / plm_probability</code> to surround span to be
masked</li> <li>Sample a starting point <code>start_index</code> from the interval <code>[cur_len, cur_len + context_length - span_length]</code> and mask tokens <code>start_index:start_index + span_length</code></li> <li>Set <code>cur_len = cur_len + context_length</code>. If <code>cur_len &lt; max_len</code> (i.e. there are tokens remaining in the
sequence to be processed), repeat from Step 1.</li>`,pa,E,ye,ca,Ye,ja="The masked tokens to be predicted for a particular sequence are determined by the following algorithm:",ga,Te,Ka=`<li>Start from the beginning of the sequence by setting <code>cur_len = 0</code> (number of tokens processed so far).</li> <li>Sample a <code>span_length</code> from the interval <code>[1, max_span_length]</code> (length of span of tokens to be masked)</li> <li>Reserve a context of length <code>context_length = span_length / plm_probability</code> to surround span to be
masked</li> <li>Sample a starting point <code>start_index</code> from the interval <code>[cur_len, cur_len + context_length - span_length]</code> and mask tokens <code>start_index:start_index + span_length</code></li> <li>Set <code>cur_len = cur_len + context_length</code>. If <code>cur_len &lt; max_len</code> (i.e. there are tokens remaining in the
sequence to be processed), repeat from Step 1.</li>`,Ct,De,yt,q,we,fa,Ze,Ba="Data collator used for padding free approach. Does the following:",ua,et,Ua="<li>concatenates the entire mini batch into single long sequence of shape [1, total_tokens]</li> <li>uses <code>separator_id</code> to separate sequences within the concatenated <code>labels</code>, default value is -100</li> <li>no padding will be added, returns <code>input_ids</code>, <code>labels</code> and <code>position_ids</code> by default</li> <li>optionally returns the kwargs contained in FlashAttentionKwargs</li> <li>optionally returns seq_idx indicating which sequence each token belongs to</li>",ha,Z,Tt,Pe,Dt,B,Fe,_a,tt,Ra=`Data collator that dynamically pads a batch of nested examples for multiple choice, so that all choices
of all examples have the same length.`,wt,Me,Pt,rt,Ft;return I=new A({props:{title:"Data Collator",local:"data-collator",headingTag:"h1"}}),ae=new A({props:{title:"Default data collator",local:"transformers.default_data_collator",headingTag:"h2"}}),oe=new C({props:{name:"transformers.default_data_collator",anchor:"transformers.default_data_collator",parameters:[{name:"features",val:": list"},{name:"return_tensors",val:" = 'pt'"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L75"}}),ne=new A({props:{title:"DefaultDataCollator",local:"transformers.DefaultDataCollator",headingTag:"h2"}}),re=new C({props:{name:"class transformers.DefaultDataCollator",anchor:"transformers.DefaultDataCollator",parameters:[{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.DefaultDataCollator.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L101"}}),le=new A({props:{title:"DataCollatorWithPadding",local:"transformers.DataCollatorWithPadding",headingTag:"h2"}}),se=new C({props:{name:"class transformers.DataCollatorWithPadding",anchor:"transformers.DataCollatorWithPadding",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.DataCollatorWithPadding.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>) &#x2014;
The tokenizer used for encoding the data.`,name:"tokenizer"},{anchor:"transformers.DataCollatorWithPadding.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding index)
among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code> (default): Pad to the longest sequence in the batch (or no padding if only a single
sequence is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code>: No padding (i.e., can output a batch with sequences of different lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DataCollatorWithPadding.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.DataCollatorWithPadding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability &gt;=
7.0 (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.DataCollatorWithPadding.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L238"}}),ie=new A({props:{title:"DataCollatorForTokenClassification",local:"transformers.DataCollatorForTokenClassification",headingTag:"h2"}}),de=new C({props:{name:"class transformers.DataCollatorForTokenClassification",anchor:"transformers.DataCollatorForTokenClassification",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"label_pad_token_id",val:": int = -100"},{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.DataCollatorForTokenClassification.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>) &#x2014;
The tokenizer used for encoding the data.`,name:"tokenizer"},{anchor:"transformers.DataCollatorForTokenClassification.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding index)
among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code> (default): Pad to the longest sequence in the batch (or no padding if only a single
sequence is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code>: No padding (i.e., can output a batch with sequences of different lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DataCollatorForTokenClassification.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.DataCollatorForTokenClassification.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability &gt;=
7.0 (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.DataCollatorForTokenClassification.label_pad_token_id",description:`<strong>label_pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).`,name:"label_pad_token_id"},{anchor:"transformers.DataCollatorForTokenClassification.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L290"}}),me=new A({props:{title:"DataCollatorForSeq2Seq",local:"transformers.DataCollatorForSeq2Seq",headingTag:"h2"}}),pe=new C({props:{name:"class transformers.DataCollatorForSeq2Seq",anchor:"transformers.DataCollatorForSeq2Seq",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"model",val:": typing.Optional[typing.Any] = None"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"label_pad_token_id",val:": int = -100"},{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.DataCollatorForSeq2Seq.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>) &#x2014;
The tokenizer used for encoding the data.`,name:"tokenizer"},{anchor:"transformers.DataCollatorForSeq2Seq.model",description:`<strong>model</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>, <em>optional</em>) &#x2014;
The model that is being trained. If set and has the <em>prepare_decoder_input_ids_from_labels</em>, use it to
prepare the <em>decoder_input_ids</em></p>
<p>This is useful when using <em>label_smoothing</em> to avoid calculating loss twice.`,name:"model"},{anchor:"transformers.DataCollatorForSeq2Seq.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding index)
among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code> (default): Pad to the longest sequence in the batch (or no padding if only a single
sequence is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code>: No padding (i.e., can output a batch with sequences of different lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DataCollatorForSeq2Seq.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.DataCollatorForSeq2Seq.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability &gt;=
7.0 (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.DataCollatorForSeq2Seq.label_pad_token_id",description:`<strong>label_pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).`,name:"label_pad_token_id"},{anchor:"transformers.DataCollatorForSeq2Seq.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L628"}}),ce=new A({props:{title:"DataCollatorForLanguageModeling",local:"transformers.DataCollatorForLanguageModeling",headingTag:"h2"}}),ge=new C({props:{name:"class transformers.DataCollatorForLanguageModeling",anchor:"transformers.DataCollatorForLanguageModeling",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"mlm",val:": bool = True"},{name:"mlm_probability",val:": typing.Optional[float] = 0.15"},{name:"mask_replace_prob",val:": float = 0.8"},{name:"random_replace_prob",val:": float = 0.1"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"tf_experimental_compile",val:": bool = False"},{name:"return_tensors",val:": str = 'pt'"},{name:"seed",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"transformers.DataCollatorForLanguageModeling.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>) &#x2014;
The tokenizer used for encoding the data.`,name:"tokenizer"},{anchor:"transformers.DataCollatorForLanguageModeling.mlm",description:`<strong>mlm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use masked language modeling. If set to <code>False</code>, the labels are the same as the inputs
with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked
tokens and the value to predict for the masked token.`,name:"mlm"},{anchor:"transformers.DataCollatorForLanguageModeling.mlm_probability",description:`<strong>mlm_probability</strong> (<code>float</code>, <em>optional</em>, defaults to 0.15) &#x2014;
The probability with which to (randomly) mask tokens in the input, when <code>mlm</code> is set to <code>True</code>.`,name:"mlm_probability"},{anchor:"transformers.DataCollatorForLanguageModeling.mask_replace_prob",description:`<strong>mask_replace_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.8) &#x2014;
The probability with which masked tokens are replaced by the tokenizer&#x2019;s mask token (e.g., <code>[MASK]</code>).
Defaults to 0.8, meaning 80% of the masked tokens will be replaced with <code>[MASK]</code>.
Only works when <code>mlm</code> is set to <code>True</code>.`,name:"mask_replace_prob"},{anchor:"transformers.DataCollatorForLanguageModeling.random_replace_prob",description:`<strong>random_replace_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The probability with which masked tokens are replaced by random tokens from the tokenizer&#x2019;s vocabulary.
Defaults to 0.1, meaning 10% of the masked tokens will be replaced with random tokens. The remaining
masked tokens (1 - mask_replace_prob - random_replace_prob) are left unchanged.
Only works when <code>mlm</code> is set to <code>True</code>.`,name:"random_replace_prob"},{anchor:"transformers.DataCollatorForLanguageModeling.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set, will pad the sequence to a multiple of the provided value.`,name:"pad_to_multiple_of"},{anchor:"transformers.DataCollatorForLanguageModeling.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"},{anchor:"transformers.DataCollatorForLanguageModeling.seed",description:`<strong>seed</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The seed to use for the random number generator for masking. If not provided, the global RNG will be used.`,name:"seed"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L764"}}),R=new ba({props:{$$slots:{default:[ao]},$$scope:{ctx:U}}}),fe=new C({props:{name:"numpy_mask_tokens",anchor:"transformers.DataCollatorForLanguageModeling.numpy_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"},{name:"special_tokens_mask",val:": typing.Optional[typing.Any] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1114"}}),ue=new C({props:{name:"torch_mask_tokens",anchor:"transformers.DataCollatorForLanguageModeling.torch_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"},{name:"special_tokens_mask",val:": typing.Optional[typing.Any] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1035"}}),he=new A({props:{title:"DataCollatorForWholeWordMask",local:"transformers.DataCollatorForWholeWordMask",headingTag:"h2"}}),_e=new C({props:{name:"class transformers.DataCollatorForWholeWordMask",anchor:"transformers.DataCollatorForWholeWordMask",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"mlm",val:": bool = True"},{name:"mlm_probability",val:": typing.Optional[float] = 0.15"},{name:"mask_replace_prob",val:": float = 0.8"},{name:"random_replace_prob",val:": float = 0.1"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"tf_experimental_compile",val:": bool = False"},{name:"return_tensors",val:": str = 'pt'"},{name:"seed",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1181"}}),J=new ba({props:{$$slots:{default:[oo]},$$scope:{ctx:U}}}),be=new C({props:{name:"numpy_mask_tokens",anchor:"transformers.DataCollatorForWholeWordMask.numpy_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"},{name:"mask_labels",val:": typing.Any"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1477"}}),ve=new C({props:{name:"torch_mask_tokens",anchor:"transformers.DataCollatorForWholeWordMask.torch_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"},{name:"mask_labels",val:": typing.Any"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1361"}}),$e=new A({props:{title:"DataCollatorForPermutationLanguageModeling",local:"transformers.DataCollatorForPermutationLanguageModeling",headingTag:"h2"}}),ke=new C({props:{name:"class transformers.DataCollatorForPermutationLanguageModeling",anchor:"transformers.DataCollatorForPermutationLanguageModeling",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"plm_probability",val:": float = 0.16666666666666666"},{name:"max_span_length",val:": int = 5"},{name:"return_tensors",val:": str = 'pt'"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1635"}}),xe=new C({props:{name:"numpy_mask_tokens",anchor:"transformers.DataCollatorForPermutationLanguageModeling.numpy_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1875"}}),ye=new C({props:{name:"torch_mask_tokens",anchor:"transformers.DataCollatorForPermutationLanguageModeling.torch_mask_tokens",parameters:[{name:"inputs",val:": typing.Any"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1669"}}),De=new A({props:{title:"DataCollatorWithFlattening",local:"transformers.DataCollatorWithFlattening",headingTag:"h2"}}),we=new C({props:{name:"class transformers.DataCollatorWithFlattening",anchor:"transformers.DataCollatorWithFlattening",parameters:[{name:"*args",val:""},{name:"return_position_ids",val:" = True"},{name:"separator_id",val:" = -100"},{name:"return_flash_attn_kwargs",val:" = False"},{name:"return_seq_idx",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L1974"}}),Z=new ba({props:{warning:!0,$$slots:{default:[no]},$$scope:{ctx:U}}}),Pe=new A({props:{title:"DataCollatorForMultipleChoice",local:"transformers.DataCollatorForMultipleChoice",headingTag:"h1"}}),Fe=new C({props:{name:"class transformers.DataCollatorForMultipleChoice",anchor:"transformers.DataCollatorForMultipleChoice",parameters:[{name:"tokenizer",val:": PreTrainedTokenizerBase"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.DataCollatorForMultipleChoice.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>) &#x2014;
The tokenizer used for encoding the data.`,name:"tokenizer"},{anchor:"transformers.DataCollatorForMultipleChoice.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences according to the model&#x2019;s padding side and padding index
among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
is provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DataCollatorForMultipleChoice.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.DataCollatorForMultipleChoice.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability &gt;=
7.5 (Volta).`,name:"pad_to_multiple_of"},{anchor:"transformers.DataCollatorForMultipleChoice.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The type of Tensor to return. Allowable values are &#x201C;np&#x201D;, &#x201C;pt&#x201D; and &#x201C;tf&#x201D;.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/data/data_collator.py#L537"}}),Me=new to({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/data_collator.md"}}),{c(){m=l("meta"),P=n(),_=l("p"),x=n(),p(I.$$.fragment),O=n(),y=l("p"),y.innerHTML=nt,v=n(),F=l("p"),F.innerHTML=$a,lt=n(),te=l("p"),te.innerHTML=ka,st=n(),p(ae.$$.fragment),it=n(),M=l("div"),p(oe.$$.fragment),Wt=n(),Le=l("p"),Le.textContent=xa,Et=n(),ze=l("ul"),ze.innerHTML=Ca,Ht=n(),Se=l("p"),Se.textContent=ya,dt=n(),p(ne.$$.fragment),mt=n(),T=l("div"),p(re.$$.fragment),Nt=n(),Ae=l("p"),Ae.textContent=Ta,Ot=n(),Ie=l("ul"),Ie.innerHTML=Da,Vt=n(),We=l("p"),We.textContent=wa,jt=n(),Ee=l("p"),Ee.textContent=Pa,pt=n(),p(le.$$.fragment),ct=n(),V=l("div"),p(se.$$.fragment),Kt=n(),He=l("p"),He.textContent=Fa,gt=n(),p(ie.$$.fragment),ft=n(),j=l("div"),p(de.$$.fragment),Bt=n(),Ne=l("p"),Ne.textContent=Ma,ut=n(),p(me.$$.fragment),ht=n(),K=l("div"),p(pe.$$.fragment),Ut=n(),Oe=l("p"),Oe.textContent=qa,_t=n(),p(ce.$$.fragment),bt=n(),D=l("div"),p(ge.$$.fragment),Rt=n(),Ve=l("p"),Ve.textContent=La,Gt=n(),p(R.$$.fragment),Xt=n(),G=l("div"),p(fe.$$.fragment),Jt=n(),je=l("p"),je.textContent=za,Qt=n(),X=l("div"),p(ue.$$.fragment),Yt=n(),Ke=l("p"),Ke.textContent=Sa,vt=n(),p(he.$$.fragment),$t=n(),k=l("div"),p(_e.$$.fragment),Zt=n(),Be=l("p"),Be.textContent=Aa,ea=n(),Ue=l("ul"),Ue.innerHTML=Ia,ta=n(),p(J.$$.fragment),aa=n(),Q=l("div"),p(be.$$.fragment),oa=n(),Re=l("p"),Re.textContent=Wa,na=n(),Y=l("div"),p(ve.$$.fragment),ra=n(),Ge=l("p"),Ge.textContent=Ea,kt=n(),p($e.$$.fragment),xt=n(),w=l("div"),p(ke.$$.fragment),la=n(),Xe=l("p"),Xe.textContent=Ha,sa=n(),Je=l("ul"),Je.innerHTML=Na,ia=n(),W=l("div"),p(xe.$$.fragment),da=n(),Qe=l("p"),Qe.textContent=Oa,ma=n(),Ce=l("ol"),Ce.innerHTML=Va,pa=n(),E=l("div"),p(ye.$$.fragment),ca=n(),Ye=l("p"),Ye.textContent=ja,ga=n(),Te=l("ol"),Te.innerHTML=Ka,Ct=n(),p(De.$$.fragment),yt=n(),q=l("div"),p(we.$$.fragment),fa=n(),Ze=l("p"),Ze.textContent=Ba,ua=n(),et=l("ul"),et.innerHTML=Ua,ha=n(),p(Z.$$.fragment),Tt=n(),p(Pe.$$.fragment),Dt=n(),B=l("div"),p(Fe.$$.fragment),_a=n(),tt=l("p"),tt.textContent=Ra,wt=n(),p(Me.$$.fragment),Pt=n(),rt=l("p"),this.h()},l(e){const o=Ya("svelte-u9bgzb",document.head);m=s(o,"META",{name:!0,content:!0}),o.forEach(t),P=r(e),_=s(e,"P",{}),$(_).forEach(t),x=r(e),c(I.$$.fragment,e),O=r(e),y=s(e,"P",{"data-svelte-h":!0}),d(y)!=="svelte-1kxfoc9"&&(y.innerHTML=nt),v=r(e),F=s(e,"P",{"data-svelte-h":!0}),d(F)!=="svelte-1s51gsw"&&(F.innerHTML=$a),lt=r(e),te=s(e,"P",{"data-svelte-h":!0}),d(te)!=="svelte-10kyi1a"&&(te.innerHTML=ka),st=r(e),c(ae.$$.fragment,e),it=r(e),M=s(e,"DIV",{class:!0});var S=$(M);c(oe.$$.fragment,S),Wt=r(S),Le=s(S,"P",{"data-svelte-h":!0}),d(Le)!=="svelte-1hmsgsg"&&(Le.textContent=xa),Et=r(S),ze=s(S,"UL",{"data-svelte-h":!0}),d(ze)!=="svelte-1pq8qks"&&(ze.innerHTML=Ca),Ht=r(S),Se=s(S,"P",{"data-svelte-h":!0}),d(Se)!=="svelte-1vi1gug"&&(Se.textContent=ya),S.forEach(t),dt=r(e),c(ne.$$.fragment,e),mt=r(e),T=s(e,"DIV",{class:!0});var L=$(T);c(re.$$.fragment,L),Nt=r(L),Ae=s(L,"P",{"data-svelte-h":!0}),d(Ae)!=="svelte-1hmsgsg"&&(Ae.textContent=Ta),Ot=r(L),Ie=s(L,"UL",{"data-svelte-h":!0}),d(Ie)!=="svelte-1pq8qks"&&(Ie.innerHTML=Da),Vt=r(L),We=s(L,"P",{"data-svelte-h":!0}),d(We)!=="svelte-1vi1gug"&&(We.textContent=wa),jt=r(L),Ee=s(L,"P",{"data-svelte-h":!0}),d(Ee)!=="svelte-vfvbwr"&&(Ee.textContent=Pa),L.forEach(t),pt=r(e),c(le.$$.fragment,e),ct=r(e),V=s(e,"DIV",{class:!0});var qe=$(V);c(se.$$.fragment,qe),Kt=r(qe),He=s(qe,"P",{"data-svelte-h":!0}),d(He)!=="svelte-1iebpai"&&(He.textContent=Fa),qe.forEach(t),gt=r(e),c(ie.$$.fragment,e),ft=r(e),j=s(e,"DIV",{class:!0});var Mt=$(j);c(de.$$.fragment,Mt),Bt=r(Mt),Ne=s(Mt,"P",{"data-svelte-h":!0}),d(Ne)!=="svelte-4uvw1w"&&(Ne.textContent=Ma),Mt.forEach(t),ut=r(e),c(me.$$.fragment,e),ht=r(e),K=s(e,"DIV",{class:!0});var qt=$(K);c(pe.$$.fragment,qt),Ut=r(qt),Oe=s(qt,"P",{"data-svelte-h":!0}),d(Oe)!=="svelte-4uvw1w"&&(Oe.textContent=qa),qt.forEach(t),_t=r(e),c(ce.$$.fragment,e),bt=r(e),D=s(e,"DIV",{class:!0});var H=$(D);c(ge.$$.fragment,H),Rt=r(H),Ve=s(H,"P",{"data-svelte-h":!0}),d(Ve)!=="svelte-10km8jk"&&(Ve.textContent=La),Gt=r(H),c(R.$$.fragment,H),Xt=r(H),G=s(H,"DIV",{class:!0});var Lt=$(G);c(fe.$$.fragment,Lt),Jt=r(Lt),je=s(Lt,"P",{"data-svelte-h":!0}),d(je)!=="svelte-iv4xqf"&&(je.textContent=za),Lt.forEach(t),Qt=r(H),X=s(H,"DIV",{class:!0});var zt=$(X);c(ue.$$.fragment,zt),Yt=r(zt),Ke=s(zt,"P",{"data-svelte-h":!0}),d(Ke)!=="svelte-iv4xqf"&&(Ke.textContent=Sa),zt.forEach(t),H.forEach(t),vt=r(e),c(he.$$.fragment,e),$t=r(e),k=s(e,"DIV",{class:!0});var z=$(k);c(_e.$$.fragment,z),Zt=r(z),Be=s(z,"P",{"data-svelte-h":!0}),d(Be)!=="svelte-sjqk55"&&(Be.textContent=Aa),ea=r(z),Ue=s(z,"UL",{"data-svelte-h":!0}),d(Ue)!=="svelte-d2ozxj"&&(Ue.innerHTML=Ia),ta=r(z),c(J.$$.fragment,z),aa=r(z),Q=s(z,"DIV",{class:!0});var St=$(Q);c(be.$$.fragment,St),oa=r(St),Re=s(St,"P",{"data-svelte-h":!0}),d(Re)!=="svelte-10msjh1"&&(Re.textContent=Wa),St.forEach(t),na=r(z),Y=s(z,"DIV",{class:!0});var At=$(Y);c(ve.$$.fragment,At),ra=r(At),Ge=s(At,"P",{"data-svelte-h":!0}),d(Ge)!=="svelte-10msjh1"&&(Ge.textContent=Ea),At.forEach(t),z.forEach(t),kt=r(e),c($e.$$.fragment,e),xt=r(e),w=s(e,"DIV",{class:!0});var N=$(w);c(ke.$$.fragment,N),la=r(N),Xe=s(N,"P",{"data-svelte-h":!0}),d(Xe)!=="svelte-12inif1"&&(Xe.textContent=Ha),sa=r(N),Je=s(N,"UL",{"data-svelte-h":!0}),d(Je)!=="svelte-1t4qh7k"&&(Je.innerHTML=Na),ia=r(N),W=s(N,"DIV",{class:!0});var at=$(W);c(xe.$$.fragment,at),da=r(at),Qe=s(at,"P",{"data-svelte-h":!0}),d(Qe)!=="svelte-1u70phx"&&(Qe.textContent=Oa),ma=r(at),Ce=s(at,"OL",{start:!0,"data-svelte-h":!0}),d(Ce)!=="svelte-1r6jrbi"&&(Ce.innerHTML=Va),at.forEach(t),pa=r(N),E=s(N,"DIV",{class:!0});var ot=$(E);c(ye.$$.fragment,ot),ca=r(ot),Ye=s(ot,"P",{"data-svelte-h":!0}),d(Ye)!=="svelte-1u70phx"&&(Ye.textContent=ja),ga=r(ot),Te=s(ot,"OL",{start:!0,"data-svelte-h":!0}),d(Te)!=="svelte-1r6jrbi"&&(Te.innerHTML=Ka),ot.forEach(t),N.forEach(t),Ct=r(e),c(De.$$.fragment,e),yt=r(e),q=s(e,"DIV",{class:!0});var ee=$(q);c(we.$$.fragment,ee),fa=r(ee),Ze=s(ee,"P",{"data-svelte-h":!0}),d(Ze)!=="svelte-4gb9ju"&&(Ze.textContent=Ba),ua=r(ee),et=s(ee,"UL",{"data-svelte-h":!0}),d(et)!=="svelte-16hhqyh"&&(et.innerHTML=Ua),ha=r(ee),c(Z.$$.fragment,ee),ee.forEach(t),Tt=r(e),c(Pe.$$.fragment,e),Dt=r(e),B=s(e,"DIV",{class:!0});var It=$(B);c(Fe.$$.fragment,It),_a=r(It),tt=s(It,"P",{"data-svelte-h":!0}),d(tt)!=="svelte-143gdbc"&&(tt.textContent=Ra),It.forEach(t),wt=r(e),c(Me.$$.fragment,e),Pt=r(e),rt=s(e,"P",{}),$(rt).forEach(t),this.h()},h(){b(m,"name","hf:doc:metadata"),b(m,"content",lo),b(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Ce,"start","0"),b(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Te,"start","0"),b(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){a(document.head,m),i(e,P,o),i(e,_,o),i(e,x,o),g(I,e,o),i(e,O,o),i(e,y,o),i(e,v,o),i(e,F,o),i(e,lt,o),i(e,te,o),i(e,st,o),g(ae,e,o),i(e,it,o),i(e,M,o),g(oe,M,null),a(M,Wt),a(M,Le),a(M,Et),a(M,ze),a(M,Ht),a(M,Se),i(e,dt,o),g(ne,e,o),i(e,mt,o),i(e,T,o),g(re,T,null),a(T,Nt),a(T,Ae),a(T,Ot),a(T,Ie),a(T,Vt),a(T,We),a(T,jt),a(T,Ee),i(e,pt,o),g(le,e,o),i(e,ct,o),i(e,V,o),g(se,V,null),a(V,Kt),a(V,He),i(e,gt,o),g(ie,e,o),i(e,ft,o),i(e,j,o),g(de,j,null),a(j,Bt),a(j,Ne),i(e,ut,o),g(me,e,o),i(e,ht,o),i(e,K,o),g(pe,K,null),a(K,Ut),a(K,Oe),i(e,_t,o),g(ce,e,o),i(e,bt,o),i(e,D,o),g(ge,D,null),a(D,Rt),a(D,Ve),a(D,Gt),g(R,D,null),a(D,Xt),a(D,G),g(fe,G,null),a(G,Jt),a(G,je),a(D,Qt),a(D,X),g(ue,X,null),a(X,Yt),a(X,Ke),i(e,vt,o),g(he,e,o),i(e,$t,o),i(e,k,o),g(_e,k,null),a(k,Zt),a(k,Be),a(k,ea),a(k,Ue),a(k,ta),g(J,k,null),a(k,aa),a(k,Q),g(be,Q,null),a(Q,oa),a(Q,Re),a(k,na),a(k,Y),g(ve,Y,null),a(Y,ra),a(Y,Ge),i(e,kt,o),g($e,e,o),i(e,xt,o),i(e,w,o),g(ke,w,null),a(w,la),a(w,Xe),a(w,sa),a(w,Je),a(w,ia),a(w,W),g(xe,W,null),a(W,da),a(W,Qe),a(W,ma),a(W,Ce),a(w,pa),a(w,E),g(ye,E,null),a(E,ca),a(E,Ye),a(E,ga),a(E,Te),i(e,Ct,o),g(De,e,o),i(e,yt,o),i(e,q,o),g(we,q,null),a(q,fa),a(q,Ze),a(q,ua),a(q,et),a(q,ha),g(Z,q,null),i(e,Tt,o),g(Pe,e,o),i(e,Dt,o),i(e,B,o),g(Fe,B,null),a(B,_a),a(B,tt),i(e,wt,o),g(Me,e,o),i(e,Pt,o),i(e,rt,o),Ft=!0},p(e,[o]){const S={};o&2&&(S.$$scope={dirty:o,ctx:e}),R.$set(S);const L={};o&2&&(L.$$scope={dirty:o,ctx:e}),J.$set(L);const qe={};o&2&&(qe.$$scope={dirty:o,ctx:e}),Z.$set(qe)},i(e){Ft||(f(I.$$.fragment,e),f(ae.$$.fragment,e),f(oe.$$.fragment,e),f(ne.$$.fragment,e),f(re.$$.fragment,e),f(le.$$.fragment,e),f(se.$$.fragment,e),f(ie.$$.fragment,e),f(de.$$.fragment,e),f(me.$$.fragment,e),f(pe.$$.fragment,e),f(ce.$$.fragment,e),f(ge.$$.fragment,e),f(R.$$.fragment,e),f(fe.$$.fragment,e),f(ue.$$.fragment,e),f(he.$$.fragment,e),f(_e.$$.fragment,e),f(J.$$.fragment,e),f(be.$$.fragment,e),f(ve.$$.fragment,e),f($e.$$.fragment,e),f(ke.$$.fragment,e),f(xe.$$.fragment,e),f(ye.$$.fragment,e),f(De.$$.fragment,e),f(we.$$.fragment,e),f(Z.$$.fragment,e),f(Pe.$$.fragment,e),f(Fe.$$.fragment,e),f(Me.$$.fragment,e),Ft=!0)},o(e){u(I.$$.fragment,e),u(ae.$$.fragment,e),u(oe.$$.fragment,e),u(ne.$$.fragment,e),u(re.$$.fragment,e),u(le.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(de.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(ce.$$.fragment,e),u(ge.$$.fragment,e),u(R.$$.fragment,e),u(fe.$$.fragment,e),u(ue.$$.fragment,e),u(he.$$.fragment,e),u(_e.$$.fragment,e),u(J.$$.fragment,e),u(be.$$.fragment,e),u(ve.$$.fragment,e),u($e.$$.fragment,e),u(ke.$$.fragment,e),u(xe.$$.fragment,e),u(ye.$$.fragment,e),u(De.$$.fragment,e),u(we.$$.fragment,e),u(Z.$$.fragment,e),u(Pe.$$.fragment,e),u(Fe.$$.fragment,e),u(Me.$$.fragment,e),Ft=!1},d(e){e&&(t(P),t(_),t(x),t(O),t(y),t(v),t(F),t(lt),t(te),t(st),t(it),t(M),t(dt),t(mt),t(T),t(pt),t(ct),t(V),t(gt),t(ft),t(j),t(ut),t(ht),t(K),t(_t),t(bt),t(D),t(vt),t($t),t(k),t(kt),t(xt),t(w),t(Ct),t(yt),t(q),t(Tt),t(Dt),t(B),t(wt),t(Pt),t(rt)),t(m),h(I,e),h(ae,e),h(oe),h(ne,e),h(re),h(le,e),h(se),h(ie,e),h(de),h(me,e),h(pe),h(ce,e),h(ge),h(R),h(fe),h(ue),h(he,e),h(_e),h(J),h(be),h(ve),h($e,e),h(ke),h(xe),h(ye),h(De,e),h(we),h(Z),h(Pe,e),h(Fe),h(Me,e)}}}const lo='{"title":"Data Collator","local":"data-collator","sections":[{"title":"Default data collator","local":"transformers.default_data_collator","sections":[],"depth":2},{"title":"DefaultDataCollator","local":"transformers.DefaultDataCollator","sections":[],"depth":2},{"title":"DataCollatorWithPadding","local":"transformers.DataCollatorWithPadding","sections":[],"depth":2},{"title":"DataCollatorForTokenClassification","local":"transformers.DataCollatorForTokenClassification","sections":[],"depth":2},{"title":"DataCollatorForSeq2Seq","local":"transformers.DataCollatorForSeq2Seq","sections":[],"depth":2},{"title":"DataCollatorForLanguageModeling","local":"transformers.DataCollatorForLanguageModeling","sections":[],"depth":2},{"title":"DataCollatorForWholeWordMask","local":"transformers.DataCollatorForWholeWordMask","sections":[],"depth":2},{"title":"DataCollatorForPermutationLanguageModeling","local":"transformers.DataCollatorForPermutationLanguageModeling","sections":[],"depth":2},{"title":"DataCollatorWithFlattening","local":"transformers.DataCollatorWithFlattening","sections":[],"depth":2}],"depth":1}';function so(U){return Xa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fo extends Ja{constructor(m){super(),Qa(this,m,so,ro,Ga,{})}}export{fo as component};
