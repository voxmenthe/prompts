import{s as Ns,z as Bs,o as Gs,n as vt}from"../chunks/scheduler.18a86fab.js";import{S as Vs,i as Xs,g as r,s as n,r as m,A as Ls,h as i,f as o,c as a,j as I,x as c,u as h,k as w,y as d,a as s,v as g,d as f,t as u,w as _}from"../chunks/index.98837b22.js";import{T as Hs}from"../chunks/Tip.77304350.js";import{D as W}from"../chunks/Docstring.a1ef7999.js";import{C as K}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as qo}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as F,E as Ss}from"../chunks/getInferenceSnippets.06c2775f.js";function qs(C){let l,T="Example:",y,b,v;return b=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoUHJvQ29uZmlnJTJDJTIwRGVwdGhQcm9Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBEZXB0aFBybyUyMGFwcGxlJTJGRGVwdGhQcm8lMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwRGVwdGhQcm9Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwYXBwbGUlMkZEZXB0aFBybyUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRGVwdGhQcm9Nb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProConfig, DepthProModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DepthPro apple/DepthPro style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DepthProConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the apple/DepthPro style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){l=r("p"),l.textContent=T,y=n(),m(b.$$.fragment)},l(p){l=i(p,"P",{"data-svelte-h":!0}),c(l)!=="svelte-11lpom8"&&(l.textContent=T),y=a(p),h(b.$$.fragment,p)},m(p,M){s(p,l,M),s(p,y,M),g(b,p,M),v=!0},p:vt,i(p){v||(f(b.$$.fragment,p),v=!0)},o(p){u(b.$$.fragment,p),v=!1},d(p){p&&(o(l),o(y)),_(b,p)}}}function Qs(C){let l,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){l=r("p"),l.innerHTML=T},l(y){l=i(y,"P",{"data-svelte-h":!0}),c(l)!=="svelte-fincs2"&&(l.innerHTML=T)},m(y,b){s(y,l,b)},p:vt,d(y){y&&o(l)}}}function As(C){let l,T="Examples:",y,b,v;return b=new K({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwRGVwdGhQcm9Nb2RlbCUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGd3d3LmlsYW5rZWxtYW4ub3JnJTJGc3RvcHNpZ25zJTJGYXVzdHJhbGlhLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJhcHBsZSUyRkRlcHRoUHJvLWhmJTIyJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMERlcHRoUHJvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQpJTBBJTBBJTIzJTIwcHJlcGFyZSUyMGltYWdlJTIwZm9yJTIwdGhlJTIwbW9kZWwlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXQlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFvdXRwdXQubGFzdF9oaWRkZW5fc3RhdGUuc2hhcGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, DepthProModel

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://www.ilankelman.org/stopsigns/australia.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProModel.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    output = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>output.last_hidden_state.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">35</span>, <span class="hljs-number">577</span>, <span class="hljs-number">1024</span>])`,wrap:!1}}),{c(){l=r("p"),l.textContent=T,y=n(),m(b.$$.fragment)},l(p){l=i(p,"P",{"data-svelte-h":!0}),c(l)!=="svelte-kvfsh7"&&(l.textContent=T),y=a(p),h(b.$$.fragment,p)},m(p,M){s(p,l,M),s(p,y,M),g(b,p,M),v=!0},p:vt,i(p){v||(f(b.$$.fragment,p),v=!0)},o(p){u(b.$$.fragment,p),v=!1},d(p){p&&(o(l),o(y)),_(b,p)}}}function Ys(C){let l,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){l=r("p"),l.innerHTML=T},l(y){l=i(y,"P",{"data-svelte-h":!0}),c(l)!=="svelte-fincs2"&&(l.innerHTML=T)},m(y,b){s(y,l,b)},p:vt,d(y){y&&o(l)}}}function Os(C){let l,T="Examples:",y,b,v;return b=new K({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DepthProForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProForDepthEstimation.from_pretrained(checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>post_processed_output = processor.post_process_depth_estimation(
<span class="hljs-meta">... </span>    outputs, target_sizes=[(image.height, image.width)],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># get the field of view (fov) predictions</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>field_of_view = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;field_of_view&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>focal_length = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;focal_length&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># visualize the prediction</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = predicted_depth * <span class="hljs-number">255</span> / predicted_depth.<span class="hljs-built_in">max</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth.detach().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint8&quot;</span>))`,wrap:!1}}),{c(){l=r("p"),l.textContent=T,y=n(),m(b.$$.fragment)},l(p){l=i(p,"P",{"data-svelte-h":!0}),c(l)!=="svelte-kvfsh7"&&(l.textContent=T),y=a(p),h(b.$$.fragment,p)},m(p,M){s(p,l,M),s(p,y,M),g(b,p,M),v=!0},p:vt,i(p){v||(f(b.$$.fragment,p),v=!0)},o(p){u(b.$$.fragment,p),v=!1},d(p){p&&(o(l),o(y)),_(b,p)}}}function Ks(C){let l,T,y,b,v,p="<em>This model was released on 2024-10-02 and added to Hugging Face Transformers on 2025-02-10.</em>",M,ee,wt,B,Qo='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Tt,te,Mt,oe,Ao='The DepthPro model was proposed in <a href="https://huggingface.co/papers/2410.02073" rel="nofollow">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a> by Aleksei Bochkovskii, AmaÃ«l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.',Pt,se,Yo="DepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.",Dt,ne,Oo="The abstract from the paper is the following:",$t,ae,Ko="<em>We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.</em>",jt,H,es,Ut,re,ts='DepthPro Outputs. Taken from the <a href="https://github.com/apple/ml-depth-pro" target="_blank">official code</a>.',It,ie,os='This model was contributed by <a href="https://github.com/geetu040" rel="nofollow">geetu040</a>. The original code can be found <a href="https://github.com/apple/ml-depth-pro" rel="nofollow">here</a>.',Ct,le,xt,de,ss="The DepthPro model processes an input image by first downsampling it at multiple scales and splitting each scaled version into patches. These patches are then encoded using a shared Vision Transformer (ViT)-based Dinov2 patch encoder, while the full image is processed by a separate image encoder. The extracted patch features are merged into feature maps, upsampled, and fused using a DPT-like decoder to generate the final depth estimation. If enabled, an additional Field of View (FOV) encoder processes the image for estimating the cameraâ€™s field of view, aiding in depth accuracy.",Jt,ce,Ft,pe,Rt,N,ns,zt,me,as='DepthPro architecture. Taken from the <a href="https://huggingface.co/papers/2410.02073" target="_blank">original paper</a>.',Zt,he,rs="The <code>DepthProForDepthEstimation</code> model uses a <code>DepthProEncoder</code>, for encoding the input image and a <code>FeatureFusionStage</code> for fusing the output features from encoder.",Wt,ge,is="The <code>DepthProEncoder</code> further uses two encoders:",kt,fe,ls="<li><code>patch_encoder</code><ul><li>Input image is scaled with multiple ratios, as specified in the <code>scaled_images_ratios</code> configuration.</li> <li>Each scaled image is split into smaller <strong>patches</strong> of size <code>patch_size</code> with overlapping areas determined by <code>scaled_images_overlap_ratios</code>.</li> <li>These patches are processed by the <strong><code>patch_encoder</code></strong></li></ul></li> <li><code>image_encoder</code><ul><li>Input image is also rescaled to <code>patch_size</code> and processed by the <strong><code>image_encoder</code></strong></li></ul></li>",Et,ue,ds="Both these encoders can be configured via <code>patch_model_config</code> and <code>image_model_config</code> respectively, both of which are separate <code>Dinov2Model</code> by default.",Bt,_e,cs="Outputs from both encoders (<code>last_hidden_state</code>) and selected intermediate states (<code>hidden_states</code>) from <strong><code>patch_encoder</code></strong> are fused by a <code>DPT</code>-based <code>FeatureFusionStage</code> for depth estimation.",Ht,ye,Nt,be,ps="The network is supplemented with a focal length estimation head. A small convolutional head ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field-of-view.",Gt,ve,ms="The <code>use_fov_model</code> parameter in <code>DepthProConfig</code> controls whether <strong>FOV prediction</strong> is enabled. By default, it is set to <code>False</code> to conserve memory and computation. When enabled, the <strong>FOV encoder</strong> is instantiated based on the <code>fov_model_config</code> parameter, which defaults to a <code>Dinov2Model</code>. The <code>use_fov_model</code> parameter can also be passed when initializing the <code>DepthProForDepthEstimation</code> model.",Vt,we,hs="The pretrained model at checkpoint <code>apple/DepthPro-hf</code> uses the FOV encoder. To use the pretrained-model without FOV encoder, set <code>use_fov_model=False</code> when loading the model, which saves computation.",Xt,Te,Lt,Me,gs="To instantiate a new model with FOV encoder, set <code>use_fov_model=True</code> in the config.",St,Pe,qt,De,fs="Or set <code>use_fov_model=True</code> when initializing the model, which overrides the value in config.",Qt,$e,At,je,Yt,Ue,us=`PyTorch includes a native scaled dot-product attention (SDPA) operator as part of <code>torch.nn.functional</code>. This function
encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow">official documentation</a>
or the <a href="https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention" rel="nofollow">GPU Inference</a>
page for more information.`,Ot,Ie,_s=`SDPA is used by default for <code>torch&gt;=2.1.1</code> when an implementation is available, but you may also set
<code>attn_implementation=&quot;sdpa&quot;</code> in <code>from_pretrained()</code> to explicitly request SDPA to be used.`,Kt,Ce,eo,xe,ys="For the best speedups, we recommend loading the model in half-precision (e.g. <code>torch.float16</code> or <code>torch.bfloat16</code>).",to,Je,bs="On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with <code>float32</code> and <code>google/vit-base-patch16-224</code> model, we saw the following speedups during inference.",oo,Fe,vs="<thead><tr><th>Batch size</th> <th>Average inference time (ms), eager mode</th> <th>Average inference time (ms), sdpa model</th> <th>Speed up, Sdpa / Eager (x)</th></tr></thead> <tbody><tr><td>1</td> <td>7</td> <td>6</td> <td>1.17</td></tr> <tr><td>2</td> <td>8</td> <td>6</td> <td>1.33</td></tr> <tr><td>4</td> <td>8</td> <td>6</td> <td>1.33</td></tr> <tr><td>8</td> <td>8</td> <td>6</td> <td>1.33</td></tr></tbody>",so,Re,no,ze,ws="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with DepthPro:",ao,Ze,Ts='<li>Research Paper: <a href="https://huggingface.co/papers/2410.02073" rel="nofollow">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a></li> <li>Official Implementation: <a href="https://github.com/apple/ml-depth-pro" rel="nofollow">apple/ml-depth-pro</a></li> <li>DepthPro Inference Notebook: <a href="https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DepthPro_inference.ipynb" rel="nofollow">DepthPro Inference</a></li> <li>DepthPro for Super Resolution and Image Segmentation<ul><li>Read blog on Medium: <a href="https://medium.com/@raoarmaghanshakir040/depth-pro-beyond-depth-9d822fc557ba" rel="nofollow">Depth Pro: Beyond Depth</a></li> <li>Code on Github: <a href="https://github.com/geetu040/depthpro-beyond-depth" rel="nofollow">geetu040/depthpro-beyond-depth</a></li></ul></li>',ro,We,Ms="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",io,ke,lo,$,Ee,To,st,Ps=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProModel">DepthProModel</a>. It is used to instantiate a
DepthPro model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the DepthPro
<a href="https://huggingface.co/apple/DepthPro" rel="nofollow">apple/DepthPro</a> architecture.`,Mo,nt,Ds=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Po,G,co,Be,po,j,He,Do,at,$s="Constructs a DepthPro image processor.",$o,V,Ne,jo,rt,js="Preprocess an image or batch of images.",Uo,X,Ge,Io,it,Us=`Post-processes the raw depth predictions from the model to generate
final depth predictions which is caliberated using the field of view if provided
and resized to specified target sizes if provided.`,mo,Ve,ho,U,Xe,Co,lt,Is="Constructs a fast Depth Pro image processor.",xo,dt,Le,Jo,L,Se,Fo,ct,Cs=`Post-processes the raw depth predictions from the model to generate
final depth predictions which is caliberated using the field of view if provided
and resized to specified target sizes if provided.`,go,qe,fo,P,Qe,Ro,pt,xs="The bare Depth Pro Model outputting raw hidden-states without any specific head on top.",zo,mt,Js=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Zo,ht,Fs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Wo,x,Ae,ko,gt,Rs='The <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProModel">DepthProModel</a> forward method, overrides the <code>__call__</code> special method.',Eo,S,Bo,q,uo,Ye,_o,D,Oe,Ho,ft,zs="DepthPro Model with a depth estimation head on top (consisting of 3 convolutional layers).",No,ut,Zs=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Go,_t,Ws=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Vo,J,Ke,Xo,yt,ks='The <a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation">DepthProForDepthEstimation</a> forward method, overrides the <code>__call__</code> special method.',Lo,Q,So,A,yo,et,bo,bt,vo;return ee=new F({props:{title:"DepthPro",local:"depthpro",headingTag:"h1"}}),te=new F({props:{title:"Overview",local:"overview",headingTag:"h2"}}),le=new F({props:{title:"Usage Tips",local:"usage-tips",headingTag:"h2"}}),ce=new K({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBEZXB0aFByb0ltYWdlUHJvY2Vzc29yRmFzdCUyQyUyMERlcHRoUHJvRm9yRGVwdGhFc3RpbWF0aW9uJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF1cmwlMjAlM0QlMjAnaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyclMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBEZXB0aFByb0ltYWdlUHJvY2Vzc29yRmFzdC5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZEZXB0aFByby1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMERlcHRoUHJvRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJhcHBsZSUyRkRlcHRoUHJvLWhmJTIyKS50byhkZXZpY2UpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBcG9zdF9wcm9jZXNzZWRfb3V0cHV0JTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19kZXB0aF9lc3RpbWF0aW9uKCUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMkMlMjB0YXJnZXRfc2l6ZXMlM0QlNUIoaW1hZ2UuaGVpZ2h0JTJDJTIwaW1hZ2Uud2lkdGgpJTVEJTJDJTBBKSUwQSUwQWZpZWxkX29mX3ZpZXclMjAlM0QlMjBwb3N0X3Byb2Nlc3NlZF9vdXRwdXQlNUIwJTVEJTVCJTIyZmllbGRfb2ZfdmlldyUyMiU1RCUwQWZvY2FsX2xlbmd0aCUyMCUzRCUyMHBvc3RfcHJvY2Vzc2VkX291dHB1dCU1QjAlNUQlNUIlMjJmb2NhbF9sZW5ndGglMjIlNUQlMEFkZXB0aCUyMCUzRCUyMHBvc3RfcHJvY2Vzc2VkX291dHB1dCU1QjAlNUQlNUIlMjJwcmVkaWN0ZWRfZGVwdGglMjIlNUQlMEFkZXB0aCUyMCUzRCUyMChkZXB0aCUyMC0lMjBkZXB0aC5taW4oKSklMjAlMkYlMjBkZXB0aC5tYXgoKSUwQWRlcHRoJTIwJTNEJTIwZGVwdGglMjAqJTIwMjU1LiUwQWRlcHRoJTIwJTNEJTIwZGVwdGguZGV0YWNoKCkuY3B1KCkubnVtcHkoKSUwQWRlcHRoJTIwJTNEJTIwSW1hZ2UuZnJvbWFycmF5KGRlcHRoLmFzdHlwZSglMjJ1aW50OCUyMikp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProImageProcessorFast, DepthProForDepthEstimation, infer_device

<span class="hljs-meta">&gt;&gt;&gt; </span>device = infer_device()

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = DepthProImageProcessorFast.from_pretrained(<span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>post_processed_output = image_processor.post_process_depth_estimation(
<span class="hljs-meta">... </span>    outputs, target_sizes=[(image.height, image.width)],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>field_of_view = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;field_of_view&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>focal_length = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;focal_length&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = post_processed_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;predicted_depth&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = (depth - depth.<span class="hljs-built_in">min</span>()) / depth.<span class="hljs-built_in">max</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth * <span class="hljs-number">255.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = depth.detach().cpu().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(depth.astype(<span class="hljs-string">&quot;uint8&quot;</span>))`,wrap:!1}}),pe=new F({props:{title:"Architecture and Configuration",local:"architecture-and-configuration",headingTag:"h3"}}),ye=new F({props:{title:"Field-of-View (FOV) Prediction",local:"field-of-view-fov-prediction",headingTag:"h3"}}),Te=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoUHJvRm9yRGVwdGhFc3RpbWF0aW9uJTBBbW9kZWwlMjAlM0QlMjBEZXB0aFByb0ZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZEZXB0aFByby1oZiUyMiUyQyUyMHVzZV9mb3ZfbW9kZWwlM0RGYWxzZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>, use_fov_model=<span class="hljs-literal">False</span>)`,wrap:!1}}),Pe=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoUHJvQ29uZmlnJTJDJTIwRGVwdGhQcm9Gb3JEZXB0aEVzdGltYXRpb24lMEFjb25maWclMjAlM0QlMjBEZXB0aFByb0NvbmZpZyh1c2VfZm92X21vZGVsJTNEVHJ1ZSklMEFtb2RlbCUyMCUzRCUyMERlcHRoUHJvRm9yRGVwdGhFc3RpbWF0aW9uKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProConfig, DepthProForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span>config = DepthProConfig(use_fov_model=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProForDepthEstimation(config)`,wrap:!1}}),$e=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoUHJvQ29uZmlnJTJDJTIwRGVwdGhQcm9Gb3JEZXB0aEVzdGltYXRpb24lMEFjb25maWclMjAlM0QlMjBEZXB0aFByb0NvbmZpZygpJTBBbW9kZWwlMjAlM0QlMjBEZXB0aFByb0ZvckRlcHRoRXN0aW1hdGlvbihjb25maWclMkMlMjB1c2VfZm92X21vZGVsJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProConfig, DepthProForDepthEstimation
<span class="hljs-meta">&gt;&gt;&gt; </span>config = DepthProConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DepthProForDepthEstimation(config, use_fov_model=<span class="hljs-literal">True</span>)`,wrap:!1}}),je=new F({props:{title:"Using Scaled Dot Product Attention (SDPA)",local:"using-scaled-dot-product-attention-sdpa",headingTag:"h3"}}),Ce=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlcHRoUHJvRm9yRGVwdGhFc3RpbWF0aW9uJTBBbW9kZWwlMjAlM0QlMjBEZXB0aFByb0ZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZEZXB0aFByby1oZiUyMiUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJzZHBhJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DepthProForDepthEstimation
model = DepthProForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;apple/DepthPro-hf&quot;</span>, attn_implementation=<span class="hljs-string">&quot;sdpa&quot;</span>, dtype=torch.float16)`,wrap:!1}}),Re=new F({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ke=new F({props:{title:"DepthProConfig",local:"transformers.DepthProConfig",headingTag:"h2"}}),Ee=new W({props:{name:"class transformers.DepthProConfig",anchor:"transformers.DepthProConfig",parameters:[{name:"fusion_hidden_size",val:" = 256"},{name:"patch_size",val:" = 384"},{name:"initializer_range",val:" = 0.02"},{name:"intermediate_hook_ids",val:" = [11, 5]"},{name:"intermediate_feature_dims",val:" = [256, 256]"},{name:"scaled_images_ratios",val:" = [0.25, 0.5, 1]"},{name:"scaled_images_overlap_ratios",val:" = [0.0, 0.5, 0.25]"},{name:"scaled_images_feature_dims",val:" = [1024, 1024, 512]"},{name:"merge_padding_value",val:" = 3"},{name:"use_batch_norm_in_fusion_residual",val:" = False"},{name:"use_bias_in_fusion_residual",val:" = True"},{name:"use_fov_model",val:" = False"},{name:"num_fov_head_layers",val:" = 2"},{name:"image_model_config",val:" = None"},{name:"patch_model_config",val:" = None"},{name:"fov_model_config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DepthProConfig.fusion_hidden_size",description:`<strong>fusion_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The number of channels before fusion.`,name:"fusion_hidden_size"},{anchor:"transformers.DepthProConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
The size (resolution) of each patch. This is also the image_size for backbone model.`,name:"patch_size"},{anchor:"transformers.DepthProConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DepthProConfig.intermediate_hook_ids",description:`<strong>intermediate_hook_ids</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[11, 5]</code>) &#x2014;
Indices of the intermediate hidden states from the patch encoder to use for fusion.`,name:"intermediate_hook_ids"},{anchor:"transformers.DepthProConfig.intermediate_feature_dims",description:`<strong>intermediate_feature_dims</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[256, 256]</code>) &#x2014;
Hidden state dimensions during upsampling for each intermediate hidden state in <code>intermediate_hook_ids</code>.`,name:"intermediate_feature_dims"},{anchor:"transformers.DepthProConfig.scaled_images_ratios",description:`<strong>scaled_images_ratios</strong> (<code>list[float]</code>, <em>optional</em>, defaults to <code>[0.25, 0.5, 1]</code>) &#x2014;
Ratios of scaled images to be used by the patch encoder.`,name:"scaled_images_ratios"},{anchor:"transformers.DepthProConfig.scaled_images_overlap_ratios",description:`<strong>scaled_images_overlap_ratios</strong> (<code>list[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.5, 0.25]</code>) &#x2014;
Overlap ratios between patches for each scaled image in <code>scaled_images_ratios</code>.`,name:"scaled_images_overlap_ratios"},{anchor:"transformers.DepthProConfig.scaled_images_feature_dims",description:`<strong>scaled_images_feature_dims</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1024, 1024, 512]</code>) &#x2014;
Hidden state dimensions during upsampling for each scaled image in <code>scaled_images_ratios</code>.`,name:"scaled_images_feature_dims"},{anchor:"transformers.DepthProConfig.merge_padding_value",description:`<strong>merge_padding_value</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
When merging smaller patches back to the image size, overlapping sections of this size are removed.`,name:"merge_padding_value"},{anchor:"transformers.DepthProConfig.use_batch_norm_in_fusion_residual",description:`<strong>use_batch_norm_in_fusion_residual</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use batch normalization in the pre-activate residual units of the fusion blocks.`,name:"use_batch_norm_in_fusion_residual"},{anchor:"transformers.DepthProConfig.use_bias_in_fusion_residual",description:`<strong>use_bias_in_fusion_residual</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use bias in the pre-activate residual units of the fusion blocks.`,name:"use_bias_in_fusion_residual"},{anchor:"transformers.DepthProConfig.use_fov_model",description:`<strong>use_fov_model</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use <code>DepthProFovModel</code> to generate the field of view.`,name:"use_fov_model"},{anchor:"transformers.DepthProConfig.num_fov_head_layers",description:`<strong>num_fov_head_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of convolution layers in the head of <code>DepthProFovModel</code>.`,name:"num_fov_head_layers"},{anchor:"transformers.DepthProConfig.image_model_config",description:`<strong>image_model_config</strong> (<code>Union[dict[str, Any], PretrainedConfig]</code>, <em>optional</em>) &#x2014;
The configuration of the image encoder model, which is loaded using the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> API.
By default, Dinov2 model is used as backbone.`,name:"image_model_config"},{anchor:"transformers.DepthProConfig.patch_model_config",description:`<strong>patch_model_config</strong> (<code>Union[dict[str, Any], PretrainedConfig]</code>, <em>optional</em>) &#x2014;
The configuration of the patch encoder model, which is loaded using the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> API.
By default, Dinov2 model is used as backbone.`,name:"patch_model_config"},{anchor:"transformers.DepthProConfig.fov_model_config",description:`<strong>fov_model_config</strong> (<code>Union[dict[str, Any], PretrainedConfig]</code>, <em>optional</em>) &#x2014;
The configuration of the fov encoder model, which is loaded using the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> API.
By default, Dinov2 model is used as backbone.`,name:"fov_model_config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/configuration_depth_pro.py#L27"}}),G=new qo({props:{anchor:"transformers.DepthProConfig.example",$$slots:{default:[qs]},$$scope:{ctx:C}}}),Be=new F({props:{title:"DepthProImageProcessor",local:"transformers.DepthProImageProcessor",headingTag:"h2"}}),He=new W({props:{name:"class transformers.DepthProImageProcessor",anchor:"transformers.DepthProImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DepthProImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. Can be overridden by the <code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.DepthProImageProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 1536, &quot;width&quot;: 1536}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.DepthProImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.DepthProImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.DepthProImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.DepthProImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.DepthProImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.DepthProImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/image_processing_depth_pro.py#L56"}}),Ne=new W({props:{name:"preprocess",anchor:"transformers.DepthProImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": typing.Optional[PIL.Image.Resampling] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension] = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.DepthProImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.DepthProImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.DepthProImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Dictionary in the format <code>{&quot;height&quot;: h, &quot;width&quot;: w}</code> specifying the size of the output image after
resizing.`,name:"size"},{anchor:"transformers.DepthProImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code> filter, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
<code>PILImageResampling</code> filter to use if resizing the image e.g. <code>PILImageResampling.BILINEAR</code>. Only has
an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.DepthProImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.DepthProImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.DepthProImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.DepthProImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.DepthProImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.DepthProImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DepthProImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.DepthProImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/image_processing_depth_pro.py#L191"}}),Ge=new W({props:{name:"post_process_depth_estimation",anchor:"transformers.DepthProImageProcessor.post_process_depth_estimation",parameters:[{name:"outputs",val:": DepthProDepthEstimatorOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple[int, int]], NoneType] = None"}],parametersDescription:[{anchor:"transformers.DepthProImageProcessor.post_process_depth_estimation.outputs",description:`<strong>outputs</strong> (<code>DepthProDepthEstimatorOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.DepthProImageProcessor.post_process_depth_estimation.target_sizes",description:`<strong>target_sizes</strong> (<code>Optional[Union[TensorType, list[tuple[int, int]], None]]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Target sizes to resize the depth predictions. Can be a tensor of shape <code>(batch_size, 2)</code>
or a list of tuples <code>(height, width)</code> for each image in the batch. If <code>None</code>, no resizing
is performed.`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/image_processing_depth_pro.py#L317",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries of tensors representing the processed depth
predictions, and field of view (degrees) and focal length (pixels) if <code>field_of_view</code> is given in <code>outputs</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[dict[str, TensorType]]</code></p>
`,raiseDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><code>ValueError</code> â€”
If the lengths of <code>predicted_depths</code>, <code>fovs</code>, or <code>target_sizes</code> are mismatched.</li>
</ul>
`,raiseType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>ValueError</code></p>
`}}),Ve=new F({props:{title:"DepthProImageProcessorFast",local:"transformers.DepthProImageProcessorFast",headingTag:"h2"}}),Xe=new W({props:{name:"class transformers.DepthProImageProcessorFast",anchor:"transformers.DepthProImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.image_processing_utils_fast.DefaultFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/image_processing_depth_pro_fast.py#L55"}}),Le=new W({props:{name:"preprocess",anchor:"transformers.DepthProImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"*args",val:""},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.image_processing_utils_fast.DefaultFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.DepthProImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.DepthProImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/image_processing_utils_fast.py#L639",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) â€” Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (â€˜pixel_valuesâ€™, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) â€” You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),Se=new W({props:{name:"post_process_depth_estimation",anchor:"transformers.DepthProImageProcessorFast.post_process_depth_estimation",parameters:[{name:"outputs",val:": DepthProDepthEstimatorOutput"},{name:"target_sizes",val:": typing.Union[transformers.utils.generic.TensorType, list[tuple[int, int]], NoneType] = None"}],parametersDescription:[{anchor:"transformers.DepthProImageProcessorFast.post_process_depth_estimation.outputs",description:`<strong>outputs</strong> (<code>DepthProDepthEstimatorOutput</code>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.DepthProImageProcessorFast.post_process_depth_estimation.target_sizes",description:`<strong>target_sizes</strong> (<code>Optional[Union[TensorType, list[tuple[int, int]], None]]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Target sizes to resize the depth predictions. Can be a tensor of shape <code>(batch_size, 2)</code>
or a list of tuples <code>(height, width)</code> for each image in the batch. If <code>None</code>, no resizing
is performed.`,name:"target_sizes"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/image_processing_depth_pro_fast.py#L105",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of dictionaries of tensors representing the processed depth
predictions, and field of view (degrees) and focal length (pixels) if <code>field_of_view</code> is given in <code>outputs</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[dict[str, TensorType]]</code></p>
`,raiseDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><code>ValueError</code> â€”
If the lengths of <code>predicted_depths</code>, <code>fovs</code>, or <code>target_sizes</code> are mismatched.</li>
</ul>
`,raiseType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>ValueError</code></p>
`}}),qe=new F({props:{title:"DepthProModel",local:"transformers.DepthProModel",headingTag:"h2"}}),Qe=new W({props:{name:"class transformers.DepthProModel",anchor:"transformers.DepthProModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.DepthProModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProModel">DepthProModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/modeling_depth_pro.py#L636"}}),Ae=new W({props:{name:"forward",anchor:"transformers.DepthProModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DepthProModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessor">DepthProImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">DepthProImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessor">DepthProImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.DepthProModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DepthProModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DepthProModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DepthProModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/modeling_depth_pro.py#L648",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.depth_pro.modeling_depth_pro.DepthProOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProConfig"
>DepthProConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, n_patches_per_batch, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>features</strong> (<code>Union[torch.FloatTensor, List[torch.FloatTensor]]</code>, <em>optional</em>) â€” Features from encoders. Can be a single feature or a list of features.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.depth_pro.modeling_depth_pro.DepthProOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new Hs({props:{$$slots:{default:[Qs]},$$scope:{ctx:C}}}),q=new qo({props:{anchor:"transformers.DepthProModel.forward.example",$$slots:{default:[As]},$$scope:{ctx:C}}}),Ye=new F({props:{title:"DepthProForDepthEstimation",local:"transformers.DepthProForDepthEstimation",headingTag:"h2"}}),Oe=new W({props:{name:"class transformers.DepthProForDepthEstimation",anchor:"transformers.DepthProForDepthEstimation",parameters:[{name:"config",val:""},{name:"use_fov_model",val:" = None"}],parametersDescription:[{anchor:"transformers.DepthProForDepthEstimation.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProForDepthEstimation">DepthProForDepthEstimation</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.DepthProForDepthEstimation.use_fov_model",description:`<strong>use_fov_model</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to use the field of view model.`,name:"use_fov_model"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/modeling_depth_pro.py#L1009"}}),Ke=new W({props:{name:"forward",anchor:"transformers.DepthProForDepthEstimation.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DepthProForDepthEstimation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessor">DepthProImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">DepthProImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProImageProcessor">DepthProImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.DepthProForDepthEstimation.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DepthProForDepthEstimation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth depth estimation maps for computing the loss.`,name:"labels"},{anchor:"transformers.DepthProForDepthEstimation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DepthProForDepthEstimation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DepthProForDepthEstimation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/depth_pro/modeling_depth_pro.py#L1034",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.depth_pro.modeling_depth_pro.DepthProDepthEstimatorOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/depth_pro#transformers.DepthProConfig"
>DepthProConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>, defaults to <code>None</code>) â€” Predicted depth for each pixel.</p>
</li>
<li>
<p><strong>field_of_view</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>, returned when <code>use_fov_model</code> is provided) â€” Field of View Scaler.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple[torch.FloatTensor, ...]</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.depth_pro.modeling_depth_pro.DepthProDepthEstimatorOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Q=new Hs({props:{$$slots:{default:[Ys]},$$scope:{ctx:C}}}),A=new qo({props:{anchor:"transformers.DepthProForDepthEstimation.forward.example",$$slots:{default:[Os]},$$scope:{ctx:C}}}),et=new Ss({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_pro.md"}}),{c(){l=r("meta"),T=n(),y=r("p"),b=n(),v=r("p"),v.innerHTML=p,M=n(),m(ee.$$.fragment),wt=n(),B=r("div"),B.innerHTML=Qo,Tt=n(),m(te.$$.fragment),Mt=n(),oe=r("p"),oe.innerHTML=Ao,Pt=n(),se=r("p"),se.textContent=Yo,Dt=n(),ne=r("p"),ne.textContent=Oo,$t=n(),ae=r("p"),ae.innerHTML=Ko,jt=n(),H=r("img"),Ut=n(),re=r("small"),re.innerHTML=ts,It=n(),ie=r("p"),ie.innerHTML=os,Ct=n(),m(le.$$.fragment),xt=n(),de=r("p"),de.textContent=ss,Jt=n(),m(ce.$$.fragment),Ft=n(),m(pe.$$.fragment),Rt=n(),N=r("img"),zt=n(),me=r("small"),me.innerHTML=as,Zt=n(),he=r("p"),he.innerHTML=rs,Wt=n(),ge=r("p"),ge.innerHTML=is,kt=n(),fe=r("ul"),fe.innerHTML=ls,Et=n(),ue=r("p"),ue.innerHTML=ds,Bt=n(),_e=r("p"),_e.innerHTML=cs,Ht=n(),m(ye.$$.fragment),Nt=n(),be=r("p"),be.textContent=ps,Gt=n(),ve=r("p"),ve.innerHTML=ms,Vt=n(),we=r("p"),we.innerHTML=hs,Xt=n(),m(Te.$$.fragment),Lt=n(),Me=r("p"),Me.innerHTML=gs,St=n(),m(Pe.$$.fragment),qt=n(),De=r("p"),De.innerHTML=fs,Qt=n(),m($e.$$.fragment),At=n(),m(je.$$.fragment),Yt=n(),Ue=r("p"),Ue.innerHTML=us,Ot=n(),Ie=r("p"),Ie.innerHTML=_s,Kt=n(),m(Ce.$$.fragment),eo=n(),xe=r("p"),xe.innerHTML=ys,to=n(),Je=r("p"),Je.innerHTML=bs,oo=n(),Fe=r("table"),Fe.innerHTML=vs,so=n(),m(Re.$$.fragment),no=n(),ze=r("p"),ze.textContent=ws,ao=n(),Ze=r("ul"),Ze.innerHTML=Ts,ro=n(),We=r("p"),We.textContent=Ms,io=n(),m(ke.$$.fragment),lo=n(),$=r("div"),m(Ee.$$.fragment),To=n(),st=r("p"),st.innerHTML=Ps,Mo=n(),nt=r("p"),nt.innerHTML=Ds,Po=n(),m(G.$$.fragment),co=n(),m(Be.$$.fragment),po=n(),j=r("div"),m(He.$$.fragment),Do=n(),at=r("p"),at.textContent=$s,$o=n(),V=r("div"),m(Ne.$$.fragment),jo=n(),rt=r("p"),rt.textContent=js,Uo=n(),X=r("div"),m(Ge.$$.fragment),Io=n(),it=r("p"),it.textContent=Us,mo=n(),m(Ve.$$.fragment),ho=n(),U=r("div"),m(Xe.$$.fragment),Co=n(),lt=r("p"),lt.textContent=Is,xo=n(),dt=r("div"),m(Le.$$.fragment),Jo=n(),L=r("div"),m(Se.$$.fragment),Fo=n(),ct=r("p"),ct.textContent=Cs,go=n(),m(qe.$$.fragment),fo=n(),P=r("div"),m(Qe.$$.fragment),Ro=n(),pt=r("p"),pt.textContent=xs,zo=n(),mt=r("p"),mt.innerHTML=Js,Zo=n(),ht=r("p"),ht.innerHTML=Fs,Wo=n(),x=r("div"),m(Ae.$$.fragment),ko=n(),gt=r("p"),gt.innerHTML=Rs,Eo=n(),m(S.$$.fragment),Bo=n(),m(q.$$.fragment),uo=n(),m(Ye.$$.fragment),_o=n(),D=r("div"),m(Oe.$$.fragment),Ho=n(),ft=r("p"),ft.textContent=zs,No=n(),ut=r("p"),ut.innerHTML=Zs,Go=n(),_t=r("p"),_t.innerHTML=Ws,Vo=n(),J=r("div"),m(Ke.$$.fragment),Xo=n(),yt=r("p"),yt.innerHTML=ks,Lo=n(),m(Q.$$.fragment),So=n(),m(A.$$.fragment),yo=n(),m(et.$$.fragment),bo=n(),bt=r("p"),this.h()},l(e){const t=Ls("svelte-u9bgzb",document.head);l=i(t,"META",{name:!0,content:!0}),t.forEach(o),T=a(e),y=i(e,"P",{}),I(y).forEach(o),b=a(e),v=i(e,"P",{"data-svelte-h":!0}),c(v)!=="svelte-ky7z8b"&&(v.innerHTML=p),M=a(e),h(ee.$$.fragment,e),wt=a(e),B=i(e,"DIV",{class:!0,"data-svelte-h":!0}),c(B)!=="svelte-13t8s2t"&&(B.innerHTML=Qo),Tt=a(e),h(te.$$.fragment,e),Mt=a(e),oe=i(e,"P",{"data-svelte-h":!0}),c(oe)!=="svelte-10zok3"&&(oe.innerHTML=Ao),Pt=a(e),se=i(e,"P",{"data-svelte-h":!0}),c(se)!=="svelte-iw0dw2"&&(se.textContent=Yo),Dt=a(e),ne=i(e,"P",{"data-svelte-h":!0}),c(ne)!=="svelte-vfdo9a"&&(ne.textContent=Oo),$t=a(e),ae=i(e,"P",{"data-svelte-h":!0}),c(ae)!=="svelte-1nlf61n"&&(ae.innerHTML=Ko),jt=a(e),H=i(e,"IMG",{src:!0,alt:!0,width:!0}),Ut=a(e),re=i(e,"SMALL",{"data-svelte-h":!0}),c(re)!=="svelte-v29z6c"&&(re.innerHTML=ts),It=a(e),ie=i(e,"P",{"data-svelte-h":!0}),c(ie)!=="svelte-l2shq"&&(ie.innerHTML=os),Ct=a(e),h(le.$$.fragment,e),xt=a(e),de=i(e,"P",{"data-svelte-h":!0}),c(de)!=="svelte-186igip"&&(de.textContent=ss),Jt=a(e),h(ce.$$.fragment,e),Ft=a(e),h(pe.$$.fragment,e),Rt=a(e),N=i(e,"IMG",{src:!0,alt:!0,width:!0}),zt=a(e),me=i(e,"SMALL",{"data-svelte-h":!0}),c(me)!=="svelte-1lb24c9"&&(me.innerHTML=as),Zt=a(e),he=i(e,"P",{"data-svelte-h":!0}),c(he)!=="svelte-tuav9r"&&(he.innerHTML=rs),Wt=a(e),ge=i(e,"P",{"data-svelte-h":!0}),c(ge)!=="svelte-1j3kmo3"&&(ge.innerHTML=is),kt=a(e),fe=i(e,"UL",{"data-svelte-h":!0}),c(fe)!=="svelte-z5afbx"&&(fe.innerHTML=ls),Et=a(e),ue=i(e,"P",{"data-svelte-h":!0}),c(ue)!=="svelte-2yjsdx"&&(ue.innerHTML=ds),Bt=a(e),_e=i(e,"P",{"data-svelte-h":!0}),c(_e)!=="svelte-52bz7i"&&(_e.innerHTML=cs),Ht=a(e),h(ye.$$.fragment,e),Nt=a(e),be=i(e,"P",{"data-svelte-h":!0}),c(be)!=="svelte-1ndzez1"&&(be.textContent=ps),Gt=a(e),ve=i(e,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-g9cm15"&&(ve.innerHTML=ms),Vt=a(e),we=i(e,"P",{"data-svelte-h":!0}),c(we)!=="svelte-a9hyvq"&&(we.innerHTML=hs),Xt=a(e),h(Te.$$.fragment,e),Lt=a(e),Me=i(e,"P",{"data-svelte-h":!0}),c(Me)!=="svelte-1npevgz"&&(Me.innerHTML=gs),St=a(e),h(Pe.$$.fragment,e),qt=a(e),De=i(e,"P",{"data-svelte-h":!0}),c(De)!=="svelte-1becr2q"&&(De.innerHTML=fs),Qt=a(e),h($e.$$.fragment,e),At=a(e),h(je.$$.fragment,e),Yt=a(e),Ue=i(e,"P",{"data-svelte-h":!0}),c(Ue)!=="svelte-1cid2pe"&&(Ue.innerHTML=us),Ot=a(e),Ie=i(e,"P",{"data-svelte-h":!0}),c(Ie)!=="svelte-1x11lxg"&&(Ie.innerHTML=_s),Kt=a(e),h(Ce.$$.fragment,e),eo=a(e),xe=i(e,"P",{"data-svelte-h":!0}),c(xe)!=="svelte-djb2w0"&&(xe.innerHTML=ys),to=a(e),Je=i(e,"P",{"data-svelte-h":!0}),c(Je)!=="svelte-18g4o0h"&&(Je.innerHTML=bs),oo=a(e),Fe=i(e,"TABLE",{"data-svelte-h":!0}),c(Fe)!=="svelte-vyu660"&&(Fe.innerHTML=vs),so=a(e),h(Re.$$.fragment,e),no=a(e),ze=i(e,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-1ub7hfb"&&(ze.textContent=ws),ao=a(e),Ze=i(e,"UL",{"data-svelte-h":!0}),c(Ze)!=="svelte-ol7wr5"&&(Ze.innerHTML=Ts),ro=a(e),We=i(e,"P",{"data-svelte-h":!0}),c(We)!=="svelte-1xesile"&&(We.textContent=Ms),io=a(e),h(ke.$$.fragment,e),lo=a(e),$=i(e,"DIV",{class:!0});var R=I($);h(Ee.$$.fragment,R),To=a(R),st=i(R,"P",{"data-svelte-h":!0}),c(st)!=="svelte-oyov92"&&(st.innerHTML=Ps),Mo=a(R),nt=i(R,"P",{"data-svelte-h":!0}),c(nt)!=="svelte-1ek1ss9"&&(nt.innerHTML=Ds),Po=a(R),h(G.$$.fragment,R),R.forEach(o),co=a(e),h(Be.$$.fragment,e),po=a(e),j=i(e,"DIV",{class:!0});var z=I(j);h(He.$$.fragment,z),Do=a(z),at=i(z,"P",{"data-svelte-h":!0}),c(at)!=="svelte-1s0ef1w"&&(at.textContent=$s),$o=a(z),V=i(z,"DIV",{class:!0});var tt=I(V);h(Ne.$$.fragment,tt),jo=a(tt),rt=i(tt,"P",{"data-svelte-h":!0}),c(rt)!=="svelte-1x3yxsa"&&(rt.textContent=js),tt.forEach(o),Uo=a(z),X=i(z,"DIV",{class:!0});var ot=I(X);h(Ge.$$.fragment,ot),Io=a(ot),it=i(ot,"P",{"data-svelte-h":!0}),c(it)!=="svelte-nc01ic"&&(it.textContent=Us),ot.forEach(o),z.forEach(o),mo=a(e),h(Ve.$$.fragment,e),ho=a(e),U=i(e,"DIV",{class:!0});var Z=I(U);h(Xe.$$.fragment,Z),Co=a(Z),lt=i(Z,"P",{"data-svelte-h":!0}),c(lt)!=="svelte-319q28"&&(lt.textContent=Is),xo=a(Z),dt=i(Z,"DIV",{class:!0});var Es=I(dt);h(Le.$$.fragment,Es),Es.forEach(o),Jo=a(Z),L=i(Z,"DIV",{class:!0});var wo=I(L);h(Se.$$.fragment,wo),Fo=a(wo),ct=i(wo,"P",{"data-svelte-h":!0}),c(ct)!=="svelte-nc01ic"&&(ct.textContent=Cs),wo.forEach(o),Z.forEach(o),go=a(e),h(qe.$$.fragment,e),fo=a(e),P=i(e,"DIV",{class:!0});var k=I(P);h(Qe.$$.fragment,k),Ro=a(k),pt=i(k,"P",{"data-svelte-h":!0}),c(pt)!=="svelte-65t3so"&&(pt.textContent=xs),zo=a(k),mt=i(k,"P",{"data-svelte-h":!0}),c(mt)!=="svelte-q52n56"&&(mt.innerHTML=Js),Zo=a(k),ht=i(k,"P",{"data-svelte-h":!0}),c(ht)!=="svelte-hswkmf"&&(ht.innerHTML=Fs),Wo=a(k),x=i(k,"DIV",{class:!0});var Y=I(x);h(Ae.$$.fragment,Y),ko=a(Y),gt=i(Y,"P",{"data-svelte-h":!0}),c(gt)!=="svelte-1gzbyf0"&&(gt.innerHTML=Rs),Eo=a(Y),h(S.$$.fragment,Y),Bo=a(Y),h(q.$$.fragment,Y),Y.forEach(o),k.forEach(o),uo=a(e),h(Ye.$$.fragment,e),_o=a(e),D=i(e,"DIV",{class:!0});var E=I(D);h(Oe.$$.fragment,E),Ho=a(E),ft=i(E,"P",{"data-svelte-h":!0}),c(ft)!=="svelte-1nk4p0z"&&(ft.textContent=zs),No=a(E),ut=i(E,"P",{"data-svelte-h":!0}),c(ut)!=="svelte-q52n56"&&(ut.innerHTML=Zs),Go=a(E),_t=i(E,"P",{"data-svelte-h":!0}),c(_t)!=="svelte-hswkmf"&&(_t.innerHTML=Ws),Vo=a(E),J=i(E,"DIV",{class:!0});var O=I(J);h(Ke.$$.fragment,O),Xo=a(O),yt=i(O,"P",{"data-svelte-h":!0}),c(yt)!=="svelte-rnxt46"&&(yt.innerHTML=ks),Lo=a(O),h(Q.$$.fragment,O),So=a(O),h(A.$$.fragment,O),O.forEach(o),E.forEach(o),yo=a(e),h(et.$$.fragment,e),bo=a(e),bt=i(e,"P",{}),I(bt).forEach(o),this.h()},h(){w(l,"name","hf:doc:metadata"),w(l,"content",en),w(B,"class","flex flex-wrap space-x-1"),Bs(H.src,es="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_teaser.png")||w(H,"src",es),w(H,"alt","drawing"),w(H,"width","600"),Bs(N.src,ns="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_architecture.png")||w(N,"src",ns),w(N,"alt","drawing"),w(N,"width","600"),w($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){d(document.head,l),s(e,T,t),s(e,y,t),s(e,b,t),s(e,v,t),s(e,M,t),g(ee,e,t),s(e,wt,t),s(e,B,t),s(e,Tt,t),g(te,e,t),s(e,Mt,t),s(e,oe,t),s(e,Pt,t),s(e,se,t),s(e,Dt,t),s(e,ne,t),s(e,$t,t),s(e,ae,t),s(e,jt,t),s(e,H,t),s(e,Ut,t),s(e,re,t),s(e,It,t),s(e,ie,t),s(e,Ct,t),g(le,e,t),s(e,xt,t),s(e,de,t),s(e,Jt,t),g(ce,e,t),s(e,Ft,t),g(pe,e,t),s(e,Rt,t),s(e,N,t),s(e,zt,t),s(e,me,t),s(e,Zt,t),s(e,he,t),s(e,Wt,t),s(e,ge,t),s(e,kt,t),s(e,fe,t),s(e,Et,t),s(e,ue,t),s(e,Bt,t),s(e,_e,t),s(e,Ht,t),g(ye,e,t),s(e,Nt,t),s(e,be,t),s(e,Gt,t),s(e,ve,t),s(e,Vt,t),s(e,we,t),s(e,Xt,t),g(Te,e,t),s(e,Lt,t),s(e,Me,t),s(e,St,t),g(Pe,e,t),s(e,qt,t),s(e,De,t),s(e,Qt,t),g($e,e,t),s(e,At,t),g(je,e,t),s(e,Yt,t),s(e,Ue,t),s(e,Ot,t),s(e,Ie,t),s(e,Kt,t),g(Ce,e,t),s(e,eo,t),s(e,xe,t),s(e,to,t),s(e,Je,t),s(e,oo,t),s(e,Fe,t),s(e,so,t),g(Re,e,t),s(e,no,t),s(e,ze,t),s(e,ao,t),s(e,Ze,t),s(e,ro,t),s(e,We,t),s(e,io,t),g(ke,e,t),s(e,lo,t),s(e,$,t),g(Ee,$,null),d($,To),d($,st),d($,Mo),d($,nt),d($,Po),g(G,$,null),s(e,co,t),g(Be,e,t),s(e,po,t),s(e,j,t),g(He,j,null),d(j,Do),d(j,at),d(j,$o),d(j,V),g(Ne,V,null),d(V,jo),d(V,rt),d(j,Uo),d(j,X),g(Ge,X,null),d(X,Io),d(X,it),s(e,mo,t),g(Ve,e,t),s(e,ho,t),s(e,U,t),g(Xe,U,null),d(U,Co),d(U,lt),d(U,xo),d(U,dt),g(Le,dt,null),d(U,Jo),d(U,L),g(Se,L,null),d(L,Fo),d(L,ct),s(e,go,t),g(qe,e,t),s(e,fo,t),s(e,P,t),g(Qe,P,null),d(P,Ro),d(P,pt),d(P,zo),d(P,mt),d(P,Zo),d(P,ht),d(P,Wo),d(P,x),g(Ae,x,null),d(x,ko),d(x,gt),d(x,Eo),g(S,x,null),d(x,Bo),g(q,x,null),s(e,uo,t),g(Ye,e,t),s(e,_o,t),s(e,D,t),g(Oe,D,null),d(D,Ho),d(D,ft),d(D,No),d(D,ut),d(D,Go),d(D,_t),d(D,Vo),d(D,J),g(Ke,J,null),d(J,Xo),d(J,yt),d(J,Lo),g(Q,J,null),d(J,So),g(A,J,null),s(e,yo,t),g(et,e,t),s(e,bo,t),s(e,bt,t),vo=!0},p(e,[t]){const R={};t&2&&(R.$$scope={dirty:t,ctx:e}),G.$set(R);const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),S.$set(z);const tt={};t&2&&(tt.$$scope={dirty:t,ctx:e}),q.$set(tt);const ot={};t&2&&(ot.$$scope={dirty:t,ctx:e}),Q.$set(ot);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),A.$set(Z)},i(e){vo||(f(ee.$$.fragment,e),f(te.$$.fragment,e),f(le.$$.fragment,e),f(ce.$$.fragment,e),f(pe.$$.fragment,e),f(ye.$$.fragment,e),f(Te.$$.fragment,e),f(Pe.$$.fragment,e),f($e.$$.fragment,e),f(je.$$.fragment,e),f(Ce.$$.fragment,e),f(Re.$$.fragment,e),f(ke.$$.fragment,e),f(Ee.$$.fragment,e),f(G.$$.fragment,e),f(Be.$$.fragment,e),f(He.$$.fragment,e),f(Ne.$$.fragment,e),f(Ge.$$.fragment,e),f(Ve.$$.fragment,e),f(Xe.$$.fragment,e),f(Le.$$.fragment,e),f(Se.$$.fragment,e),f(qe.$$.fragment,e),f(Qe.$$.fragment,e),f(Ae.$$.fragment,e),f(S.$$.fragment,e),f(q.$$.fragment,e),f(Ye.$$.fragment,e),f(Oe.$$.fragment,e),f(Ke.$$.fragment,e),f(Q.$$.fragment,e),f(A.$$.fragment,e),f(et.$$.fragment,e),vo=!0)},o(e){u(ee.$$.fragment,e),u(te.$$.fragment,e),u(le.$$.fragment,e),u(ce.$$.fragment,e),u(pe.$$.fragment,e),u(ye.$$.fragment,e),u(Te.$$.fragment,e),u(Pe.$$.fragment,e),u($e.$$.fragment,e),u(je.$$.fragment,e),u(Ce.$$.fragment,e),u(Re.$$.fragment,e),u(ke.$$.fragment,e),u(Ee.$$.fragment,e),u(G.$$.fragment,e),u(Be.$$.fragment,e),u(He.$$.fragment,e),u(Ne.$$.fragment,e),u(Ge.$$.fragment,e),u(Ve.$$.fragment,e),u(Xe.$$.fragment,e),u(Le.$$.fragment,e),u(Se.$$.fragment,e),u(qe.$$.fragment,e),u(Qe.$$.fragment,e),u(Ae.$$.fragment,e),u(S.$$.fragment,e),u(q.$$.fragment,e),u(Ye.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(Q.$$.fragment,e),u(A.$$.fragment,e),u(et.$$.fragment,e),vo=!1},d(e){e&&(o(T),o(y),o(b),o(v),o(M),o(wt),o(B),o(Tt),o(Mt),o(oe),o(Pt),o(se),o(Dt),o(ne),o($t),o(ae),o(jt),o(H),o(Ut),o(re),o(It),o(ie),o(Ct),o(xt),o(de),o(Jt),o(Ft),o(Rt),o(N),o(zt),o(me),o(Zt),o(he),o(Wt),o(ge),o(kt),o(fe),o(Et),o(ue),o(Bt),o(_e),o(Ht),o(Nt),o(be),o(Gt),o(ve),o(Vt),o(we),o(Xt),o(Lt),o(Me),o(St),o(qt),o(De),o(Qt),o(At),o(Yt),o(Ue),o(Ot),o(Ie),o(Kt),o(eo),o(xe),o(to),o(Je),o(oo),o(Fe),o(so),o(no),o(ze),o(ao),o(Ze),o(ro),o(We),o(io),o(lo),o($),o(co),o(po),o(j),o(mo),o(ho),o(U),o(go),o(fo),o(P),o(uo),o(_o),o(D),o(yo),o(bo),o(bt)),o(l),_(ee,e),_(te,e),_(le,e),_(ce,e),_(pe,e),_(ye,e),_(Te,e),_(Pe,e),_($e,e),_(je,e),_(Ce,e),_(Re,e),_(ke,e),_(Ee),_(G),_(Be,e),_(He),_(Ne),_(Ge),_(Ve,e),_(Xe),_(Le),_(Se),_(qe,e),_(Qe),_(Ae),_(S),_(q),_(Ye,e),_(Oe),_(Ke),_(Q),_(A),_(et,e)}}}const en='{"title":"DepthPro","local":"depthpro","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage Tips","local":"usage-tips","sections":[{"title":"Architecture and Configuration","local":"architecture-and-configuration","sections":[],"depth":3},{"title":"Field-of-View (FOV) Prediction","local":"field-of-view-fov-prediction","sections":[],"depth":3},{"title":"Using Scaled Dot Product Attention (SDPA)","local":"using-scaled-dot-product-attention-sdpa","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"DepthProConfig","local":"transformers.DepthProConfig","sections":[],"depth":2},{"title":"DepthProImageProcessor","local":"transformers.DepthProImageProcessor","sections":[],"depth":2},{"title":"DepthProImageProcessorFast","local":"transformers.DepthProImageProcessorFast","sections":[],"depth":2},{"title":"DepthProModel","local":"transformers.DepthProModel","sections":[],"depth":2},{"title":"DepthProForDepthEstimation","local":"transformers.DepthProForDepthEstimation","sections":[],"depth":2}],"depth":1}';function tn(C){return Gs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cn extends Vs{constructor(l){super(),Xs(this,l,tn,Ks,Ns,{})}}export{cn as component};
