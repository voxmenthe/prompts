import{s as ya,o as $a,n as ba}from"../chunks/scheduler.18a86fab.js";import{S as Oa,i as wa,g as r,s,r as i,A as qa,h as d,f as o,c as a,j as f,u as c,x as g,k as _,y as m,a as n,v as u,d as l,t as p,w as h}from"../chunks/index.98837b22.js";import{T as Ta}from"../chunks/Tip.77304350.js";import{D as T}from"../chunks/Docstring.a1ef7999.js";import{C as va}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as v,E as xa}from"../chunks/getInferenceSnippets.06c2775f.js";function Sa(wt){let b,w=`When passing <code>output_hidden_states=True</code> you may expect the <code>outputs.hidden_states[-1]</code> to match <code>outputs.last_hidden_state</code> exactly.
However, this is not always the case. Some models apply normalization or subsequent process to the last hidden state when it’s returned.`;return{c(){b=r("p"),b.innerHTML=w},l(y){b=d(y,"P",{"data-svelte-h":!0}),g(b)!=="svelte-9d9sv6"&&(b.innerHTML=w)},m(y,ee){n(y,b,ee)},p:ba,d(y){y&&o(b)}}}function Fa(wt){let b,w=`You can’t unpack a <code>ModelOutput</code> directly. Use the <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput.to_tuple">to_tuple()</a> method to convert it to a tuple
before.`;return{c(){b=r("p"),b.innerHTML=w},l(y){b=d(y,"P",{"data-svelte-h":!0}),g(b)!=="svelte-23hoh"&&(b.innerHTML=w)},m(y,ee){n(y,b,ee)},p:ba,d(y){y&&o(b)}}}function Ca(wt){let b,w,y,ee,se,no,ae,Ns=`All models have outputs that are instances of subclasses of <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a>. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`,so,re,As="Let’s see how this looks in an example:",ao,de,ro,ie,Ps=`The <code>outputs</code> object is a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput">SequenceClassifierOutput</a>, as we can see in the
documentation of that class below, it means it has an optional <code>loss</code>, a <code>logits</code>, an optional <code>hidden_states</code> and
an optional <code>attentions</code> attribute. Here we have the <code>loss</code> since we passed along <code>labels</code>, but we don’t have
<code>hidden_states</code> and <code>attentions</code> because we didn’t pass <code>output_hidden_states=True</code> or
<code>output_attentions=True</code>.`,io,te,co,ce,Ls=`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get <code>None</code>. Here for instance <code>outputs.loss</code> is the loss computed by the model, and <code>outputs.attentions</code> is
<code>None</code>.`,uo,ue,Ws=`When considering our <code>outputs</code> object as tuple, it only considers the attributes that don’t have <code>None</code> values.
Here for instance, it has two elements, <code>loss</code> then <code>logits</code>, so`,lo,le,po,pe,Bs="will return the tuple <code>(outputs.loss, outputs.logits)</code> for instance.",ho,he,Ds=`When considering our <code>outputs</code> object as dictionary, it only considers the attributes that don’t have <code>None</code>
values. Here for instance, it has two keys that are <code>loss</code> and <code>logits</code>.`,mo,me,Es=`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`,go,ge,fo,$,fe,es,qt,Is=`Base class for all model outputs as dataclass. Has a <code>__getitem__</code> that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the <code>None</code> attributes. Otherwise behaves like a regular
python dictionary.`,ts,oe,os,ne,_e,ns,xt,Hs="Convert self to a tuple containing all the attributes/keys that are not <code>None</code>.",_o,Te,To,q,ve,ss,St,Vs="Base class for model’s outputs, with potential hidden states and attentions.",vo,be,bo,x,ye,as,Ft,Qs="Base class for model’s outputs that also contains a pooling of the last hidden states.",yo,$e,$o,S,Oe,rs,Ct,js="Base class for model’s outputs, with potential hidden states and attentions.",Oo,we,wo,F,qe,ds,Mt,Js="Base class for model’s outputs that also contains a pooling of the last hidden states.",qo,xe,xo,C,Se,is,zt,Us="Base class for model’s outputs that may also contain a past key/values (to speed up sequential decoding).",So,Fe,Fo,M,Ce,cs,kt,Xs="Base class for model’s outputs that may also contain a past key/values (to speed up sequential decoding).",Co,Me,Mo,z,ze,us,Nt,Zs=`Base class for model encoder’s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`,zo,ke,ko,k,Ne,ls,At,Rs="Base class for causal language model (or autoregressive) outputs.",No,Ae,Ao,N,Pe,ps,Pt,Ys="Base class for causal language model (or autoregressive) outputs.",Po,Le,Lo,A,We,hs,Lt,Gs="Base class for causal language model (or autoregressive) outputs.",Wo,Be,Bo,P,De,ms,Wt,Ks="Base class for masked language models outputs.",Do,Ee,Eo,L,Ie,gs,Bt,ea="Base class for sequence-to-sequence language models outputs.",Io,He,Ho,W,Ve,fs,Dt,ta="Base class for outputs of models predicting if two sentences are consecutive or not.",Vo,Qe,Qo,B,je,_s,Et,oa="Base class for outputs of sentence classification models.",jo,Je,Jo,D,Ue,Ts,It,na="Base class for outputs of sequence-to-sequence sentence classification models.",Uo,Xe,Xo,E,Ze,vs,Ht,sa="Base class for outputs of multiple choice models.",Zo,Re,Ro,I,Ye,bs,Vt,aa="Base class for outputs of token classification models.",Yo,Ge,Go,H,Ke,ys,Qt,ra="Base class for outputs of question answering models.",Ko,et,en,V,tt,$s,jt,da="Base class for outputs of sequence-to-sequence question answering models.",tn,ot,on,Q,nt,Os,Jt,ia="Base class for sequence-to-sequence spectrogram outputs.",nn,st,sn,j,at,ws,Ut,ca="Base class for outputs of semantic segmentation models.",an,rt,rn,J,dt,qs,Xt,ua="Base class for outputs of image classification models.",dn,it,cn,U,ct,xs,Zt,la="Base class for outputs of image classification models.",un,ut,ln,X,lt,Ss,Rt,pa="Base class for outputs of depth estimation models.",pn,pt,hn,Z,ht,Fs,Yt,ha="Base class for models that have been trained with the Wav2Vec2 loss objective.",mn,mt,gn,R,gt,Cs,Gt,ma='Output type of <a href="/docs/transformers/v4.56.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a>.',fn,ft,_n,Y,_t,Ms,Kt,ga=`Base class for time series model’s encoder outputs that also contains pre-computed hidden states that can speed up
sequential decoding.`,Tn,Tt,vn,G,vt,zs,eo,fa=`Base class for time series model’s decoder outputs that also contain the loss as well as the parameters of the
chosen distribution.`,bn,bt,yn,K,yt,ks,to,_a=`Base class for time series model’s predictions outputs that contains the sampled values from the chosen
distribution.`,$n,$t,On,oo,wn;return se=new v({props:{title:"Model outputs",local:"model-outputs",headingTag:"h1"}}),de=new va({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRUb2tlbml6ZXIlMkMlMjBCZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQmVydFRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEJlcnRGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFsYWJlbHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCMSU1RCkudW5zcXVlZXplKDApJTIwJTIwJTIzJTIwQmF0Y2glMjBzaXplJTIwMSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMGxhYmVscyUzRGxhYmVscyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertForSequenceClassification
<span class="hljs-keyword">import</span> torch

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
outputs = model(**inputs, labels=labels)`,wrap:!1}}),te=new Ta({props:{$$slots:{default:[Sa]},$$scope:{ctx:wt}}}),le=new va({props:{code:"b3V0cHV0cyU1QiUzQTIlNUQ=",highlighted:'outputs[:<span class="hljs-number">2</span>]',wrap:!1}}),ge=new v({props:{title:"ModelOutput",local:"transformers.utils.ModelOutput",headingTag:"h2"}}),fe=new T({props:{name:"class transformers.utils.ModelOutput",anchor:"transformers.utils.ModelOutput",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/utils/generic.py#L331"}}),oe=new Ta({props:{warning:!0,$$slots:{default:[Fa]},$$scope:{ctx:wt}}}),_e=new T({props:{name:"to_tuple",anchor:"transformers.utils.ModelOutput.to_tuple",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/utils/generic.py#L466"}}),Te=new v({props:{title:"BaseModelOutput",local:"transformers.modeling_outputs.BaseModelOutput",headingTag:"h2"}}),ve=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutput",anchor:"transformers.modeling_outputs.BaseModelOutput",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L26"}}),be=new v({props:{title:"BaseModelOutputWithPooling",local:"transformers.modeling_outputs.BaseModelOutputWithPooling",headingTag:"h2"}}),ye=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPooling",anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pooler_output",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L71"}}),$e=new v({props:{title:"BaseModelOutputWithCrossAttentions",local:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",headingTag:"h2"}}),Oe=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L161"}}),we=new v({props:{title:"BaseModelOutputWithPoolingAndCrossAttentions",local:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",headingTag:"h2"}}),qe=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pooler_output",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L194"}}),xe=new v({props:{title:"BaseModelOutputWithPast",local:"transformers.modeling_outputs.BaseModelOutputWithPast",headingTag:"h2"}}),Se=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPast",anchor:"transformers.modeling_outputs.BaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L125"}}),Fe=new v({props:{title:"BaseModelOutputWithPastAndCrossAttentions",local:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",headingTag:"h2"}}),Ce=new T({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L240"}}),Me=new v({props:{title:"Seq2SeqModelOutput",local:"transformers.modeling_outputs.Seq2SeqModelOutput",headingTag:"h2"}}),ze=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L500"}}),ke=new v({props:{title:"CausalLMOutput",local:"transformers.modeling_outputs.CausalLMOutput",headingTag:"h2"}}),Ne=new T({props:{name:"class transformers.modeling_outputs.CausalLMOutput",anchor:"transformers.modeling_outputs.CausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L629"}}),Ae=new v({props:{title:"CausalLMOutputWithCrossAttentions",local:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",headingTag:"h2"}}),Pe=new T({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L693"}}),Le=new v({props:{title:"CausalLMOutputWithPast",local:"transformers.modeling_outputs.CausalLMOutputWithPast",headingTag:"h2"}}),We=new T({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithPast",anchor:"transformers.modeling_outputs.CausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.Cache] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L658"}}),Be=new v({props:{title:"MaskedLMOutput",local:"transformers.modeling_outputs.MaskedLMOutput",headingTag:"h2"}}),De=new T({props:{name:"class transformers.modeling_outputs.MaskedLMOutput",anchor:"transformers.modeling_outputs.MaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MaskedLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L770"}}),Ee=new v({props:{title:"Seq2SeqLMOutput",local:"transformers.modeling_outputs.Seq2SeqLMOutput",headingTag:"h2"}}),Ie=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqLMOutput",anchor:"transformers.modeling_outputs.Seq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L799"}}),He=new v({props:{title:"NextSentencePredictorOutput",local:"transformers.modeling_outputs.NextSentencePredictorOutput",headingTag:"h2"}}),Ve=new T({props:{name:"class transformers.modeling_outputs.NextSentencePredictorOutput",anchor:"transformers.modeling_outputs.NextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sequence prediction (classification) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L930"}}),Qe=new v({props:{title:"SequenceClassifierOutput",local:"transformers.modeling_outputs.SequenceClassifierOutput",headingTag:"h2"}}),je=new T({props:{name:"class transformers.modeling_outputs.SequenceClassifierOutput",anchor:"transformers.modeling_outputs.SequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L960"}}),Je=new v({props:{title:"Seq2SeqSequenceClassifierOutput",local:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",headingTag:"h2"}}),Ue=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L989"}}),Xe=new v({props:{title:"MultipleChoiceModelOutput",local:"transformers.modeling_outputs.MultipleChoiceModelOutput",headingTag:"h2"}}),Ze=new T({props:{name:"class transformers.modeling_outputs.MultipleChoiceModelOutput",anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1047"}}),Re=new v({props:{title:"TokenClassifierOutput",local:"transformers.modeling_outputs.TokenClassifierOutput",headingTag:"h2"}}),Ye=new T({props:{name:"class transformers.modeling_outputs.TokenClassifierOutput",anchor:"transformers.modeling_outputs.TokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.TokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1078"}}),Ge=new v({props:{title:"QuestionAnsweringModelOutput",local:"transformers.modeling_outputs.QuestionAnsweringModelOutput",headingTag:"h2"}}),Ke=new T({props:{name:"class transformers.modeling_outputs.QuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"end_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1107"}}),et=new v({props:{title:"Seq2SeqQuestionAnsweringModelOutput",local:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",headingTag:"h2"}}),tt=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"end_logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1139"}}),ot=new v({props:{title:"Seq2SeqSpectrogramOutput",local:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput",headingTag:"h2"}}),nt=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqSpectrogramOutput",anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"spectrogram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Spectrogram generation loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.spectrogram",description:`<strong>spectrogram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, num_bins)</code>) &#x2014;
The predicted spectrogram.`,name:"spectrogram"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSpectrogramOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1470"}}),st=new v({props:{title:"SemanticSegmenterOutput",local:"transformers.modeling_outputs.SemanticSegmenterOutput",headingTag:"h2"}}),at=new T({props:{name:"class transformers.modeling_outputs.SemanticSegmenterOutput",anchor:"transformers.modeling_outputs.SemanticSegmenterOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) &#x2014;
Classification scores for each pixel.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>

					</div>`,name:"logits"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SemanticSegmenterOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1200"}}),rt=new v({props:{title:"ImageClassifierOutput",local:"transformers.modeling_outputs.ImageClassifierOutput",headingTag:"h2"}}),dt=new T({props:{name:"class transformers.modeling_outputs.ImageClassifierOutput",anchor:"transformers.modeling_outputs.ImageClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.ImageClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.ImageClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1238"}}),it=new v({props:{title:"ImageClassifierOutputWithNoAttention",local:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",headingTag:"h2"}}),ct=new T({props:{name:"class transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.`,name:"hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1266"}}),ut=new v({props:{title:"DepthEstimatorOutput",local:"transformers.modeling_outputs.DepthEstimatorOutput",headingTag:"h2"}}),lt=new T({props:{name:"class transformers.modeling_outputs.DepthEstimatorOutput",anchor:"transformers.modeling_outputs.DepthEstimatorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"predicted_depth",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.predicted_depth",description:`<strong>predicted_depth</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, height, width)</code>) &#x2014;
Predicted depth for each pixel.`,name:"predicted_depth"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.DepthEstimatorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1287"}}),pt=new v({props:{title:"Wav2Vec2BaseModelOutput",local:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput",headingTag:"h2"}}),ht=new T({props:{name:"class transformers.modeling_outputs.Wav2Vec2BaseModelOutput",anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"extract_features",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.Wav2Vec2BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1345"}}),mt=new v({props:{title:"XVectorOutput",local:"transformers.modeling_outputs.XVectorOutput",headingTag:"h2"}}),gt=new T({props:{name:"class transformers.modeling_outputs.XVectorOutput",anchor:"transformers.modeling_outputs.XVectorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": typing.Optional[torch.FloatTensor] = None"},{name:"embeddings",val:": typing.Optional[torch.FloatTensor] = None"},{name:"hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.XVectorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.XVectorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) &#x2014;
Classification hidden states before AMSoftmax.`,name:"logits"},{anchor:"transformers.modeling_outputs.XVectorOutput.embeddings",description:`<strong>embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.xvector_output_dim)</code>) &#x2014;
Utterance embeddings used for vector similarity-based retrieval.`,name:"embeddings"},{anchor:"transformers.modeling_outputs.XVectorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.XVectorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1374"}}),ft=new v({props:{title:"Seq2SeqTSModelOutput",local:"transformers.modeling_outputs.Seq2SeqTSModelOutput",headingTag:"h2"}}),_t=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqTSModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput",parameters:[{name:"last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"loc",val:": typing.Optional[torch.FloatTensor] = None"},{name:"scale",val:": typing.Optional[torch.FloatTensor] = None"},{name:"static_features",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.loc",description:`<strong>loc</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code> or <code>(batch_size, input_size)</code>, <em>optional</em>) &#x2014;
Shift values of each time series&#x2019; context window which is used to give the model inputs of the same
magnitude and then used to shift back to the original magnitude.`,name:"loc"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.scale",description:`<strong>scale</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code> or <code>(batch_size, input_size)</code>, <em>optional</em>) &#x2014;
Scaling values of each time series&#x2019; context window which is used to give the model inputs of the same
magnitude and then used to rescale back to the original magnitude.`,name:"scale"},{anchor:"transformers.modeling_outputs.Seq2SeqTSModelOutput.static_features",description:`<strong>static_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, feature size)</code>, <em>optional</em>) &#x2014;
Static features of each time series&#x2019; in a batch which are copied to the covariates at inference time.`,name:"static_features"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1528"}}),Tt=new v({props:{title:"Seq2SeqTSPredictionOutput",local:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput",headingTag:"h2"}}),vt=new T({props:{name:"class transformers.modeling_outputs.Seq2SeqTSPredictionOutput",anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"params",val:": typing.Optional[tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[transformers.cache_utils.EncoderDecoderCache] = None"},{name:"decoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"decoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"cross_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"encoder_attentions",val:": typing.Optional[tuple[torch.FloatTensor, ...]] = None"},{name:"loc",val:": typing.Optional[torch.FloatTensor] = None"},{name:"scale",val:": typing.Optional[torch.FloatTensor] = None"},{name:"static_features",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when a <code>future_values</code> is provided) &#x2014;
Distributional loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.params",description:`<strong>params</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_samples, num_params)</code>) &#x2014;
Parameters of the chosen distribution.`,name:"params"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>EncoderDecoderCache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
It is a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> instance. For more details, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.loc",description:`<strong>loc</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code> or <code>(batch_size, input_size)</code>, <em>optional</em>) &#x2014;
Shift values of each time series&#x2019; context window which is used to give the model inputs of the same
magnitude and then used to shift back to the original magnitude.`,name:"loc"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.scale",description:`<strong>scale</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,)</code> or <code>(batch_size, input_size)</code>, <em>optional</em>) &#x2014;
Scaling values of each time series&#x2019; context window which is used to give the model inputs of the same
magnitude and then used to rescale back to the original magnitude.`,name:"scale"},{anchor:"transformers.modeling_outputs.Seq2SeqTSPredictionOutput.static_features",description:`<strong>static_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, feature size)</code>, <em>optional</em>) &#x2014;
Static features of each time series&#x2019; in a batch which are copied to the covariates at inference time.`,name:"static_features"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1598"}}),bt=new v({props:{title:"SampleTSPredictionOutput",local:"transformers.modeling_outputs.SampleTSPredictionOutput",headingTag:"h2"}}),yt=new T({props:{name:"class transformers.modeling_outputs.SampleTSPredictionOutput",anchor:"transformers.modeling_outputs.SampleTSPredictionOutput",parameters:[{name:"sequences",val:": typing.Optional[torch.FloatTensor] = None"}],parametersDescription:[{anchor:"transformers.modeling_outputs.SampleTSPredictionOutput.sequences",description:`<strong>sequences</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_samples, prediction_length)</code> or <code>(batch_size, num_samples, prediction_length, input_size)</code>) &#x2014;
Sampled values from the chosen distribution.`,name:"sequences"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/modeling_outputs.py#L1668"}}),$t=new xa({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/output.md"}}),{c(){b=r("meta"),w=s(),y=r("p"),ee=s(),i(se.$$.fragment),no=s(),ae=r("p"),ae.innerHTML=Ns,so=s(),re=r("p"),re.textContent=As,ao=s(),i(de.$$.fragment),ro=s(),ie=r("p"),ie.innerHTML=Ps,io=s(),i(te.$$.fragment),co=s(),ce=r("p"),ce.innerHTML=Ls,uo=s(),ue=r("p"),ue.innerHTML=Ws,lo=s(),i(le.$$.fragment),po=s(),pe=r("p"),pe.innerHTML=Bs,ho=s(),he=r("p"),he.innerHTML=Ds,mo=s(),me=r("p"),me.textContent=Es,go=s(),i(ge.$$.fragment),fo=s(),$=r("div"),i(fe.$$.fragment),es=s(),qt=r("p"),qt.innerHTML=Is,ts=s(),i(oe.$$.fragment),os=s(),ne=r("div"),i(_e.$$.fragment),ns=s(),xt=r("p"),xt.innerHTML=Hs,_o=s(),i(Te.$$.fragment),To=s(),q=r("div"),i(ve.$$.fragment),ss=s(),St=r("p"),St.textContent=Vs,vo=s(),i(be.$$.fragment),bo=s(),x=r("div"),i(ye.$$.fragment),as=s(),Ft=r("p"),Ft.textContent=Qs,yo=s(),i($e.$$.fragment),$o=s(),S=r("div"),i(Oe.$$.fragment),rs=s(),Ct=r("p"),Ct.textContent=js,Oo=s(),i(we.$$.fragment),wo=s(),F=r("div"),i(qe.$$.fragment),ds=s(),Mt=r("p"),Mt.textContent=Js,qo=s(),i(xe.$$.fragment),xo=s(),C=r("div"),i(Se.$$.fragment),is=s(),zt=r("p"),zt.textContent=Us,So=s(),i(Fe.$$.fragment),Fo=s(),M=r("div"),i(Ce.$$.fragment),cs=s(),kt=r("p"),kt.textContent=Xs,Co=s(),i(Me.$$.fragment),Mo=s(),z=r("div"),i(ze.$$.fragment),us=s(),Nt=r("p"),Nt.textContent=Zs,zo=s(),i(ke.$$.fragment),ko=s(),k=r("div"),i(Ne.$$.fragment),ls=s(),At=r("p"),At.textContent=Rs,No=s(),i(Ae.$$.fragment),Ao=s(),N=r("div"),i(Pe.$$.fragment),ps=s(),Pt=r("p"),Pt.textContent=Ys,Po=s(),i(Le.$$.fragment),Lo=s(),A=r("div"),i(We.$$.fragment),hs=s(),Lt=r("p"),Lt.textContent=Gs,Wo=s(),i(Be.$$.fragment),Bo=s(),P=r("div"),i(De.$$.fragment),ms=s(),Wt=r("p"),Wt.textContent=Ks,Do=s(),i(Ee.$$.fragment),Eo=s(),L=r("div"),i(Ie.$$.fragment),gs=s(),Bt=r("p"),Bt.textContent=ea,Io=s(),i(He.$$.fragment),Ho=s(),W=r("div"),i(Ve.$$.fragment),fs=s(),Dt=r("p"),Dt.textContent=ta,Vo=s(),i(Qe.$$.fragment),Qo=s(),B=r("div"),i(je.$$.fragment),_s=s(),Et=r("p"),Et.textContent=oa,jo=s(),i(Je.$$.fragment),Jo=s(),D=r("div"),i(Ue.$$.fragment),Ts=s(),It=r("p"),It.textContent=na,Uo=s(),i(Xe.$$.fragment),Xo=s(),E=r("div"),i(Ze.$$.fragment),vs=s(),Ht=r("p"),Ht.textContent=sa,Zo=s(),i(Re.$$.fragment),Ro=s(),I=r("div"),i(Ye.$$.fragment),bs=s(),Vt=r("p"),Vt.textContent=aa,Yo=s(),i(Ge.$$.fragment),Go=s(),H=r("div"),i(Ke.$$.fragment),ys=s(),Qt=r("p"),Qt.textContent=ra,Ko=s(),i(et.$$.fragment),en=s(),V=r("div"),i(tt.$$.fragment),$s=s(),jt=r("p"),jt.textContent=da,tn=s(),i(ot.$$.fragment),on=s(),Q=r("div"),i(nt.$$.fragment),Os=s(),Jt=r("p"),Jt.textContent=ia,nn=s(),i(st.$$.fragment),sn=s(),j=r("div"),i(at.$$.fragment),ws=s(),Ut=r("p"),Ut.textContent=ca,an=s(),i(rt.$$.fragment),rn=s(),J=r("div"),i(dt.$$.fragment),qs=s(),Xt=r("p"),Xt.textContent=ua,dn=s(),i(it.$$.fragment),cn=s(),U=r("div"),i(ct.$$.fragment),xs=s(),Zt=r("p"),Zt.textContent=la,un=s(),i(ut.$$.fragment),ln=s(),X=r("div"),i(lt.$$.fragment),Ss=s(),Rt=r("p"),Rt.textContent=pa,pn=s(),i(pt.$$.fragment),hn=s(),Z=r("div"),i(ht.$$.fragment),Fs=s(),Yt=r("p"),Yt.textContent=ha,mn=s(),i(mt.$$.fragment),gn=s(),R=r("div"),i(gt.$$.fragment),Cs=s(),Gt=r("p"),Gt.innerHTML=ma,fn=s(),i(ft.$$.fragment),_n=s(),Y=r("div"),i(_t.$$.fragment),Ms=s(),Kt=r("p"),Kt.textContent=ga,Tn=s(),i(Tt.$$.fragment),vn=s(),G=r("div"),i(vt.$$.fragment),zs=s(),eo=r("p"),eo.textContent=fa,bn=s(),i(bt.$$.fragment),yn=s(),K=r("div"),i(yt.$$.fragment),ks=s(),to=r("p"),to.textContent=_a,$n=s(),i($t.$$.fragment),On=s(),oo=r("p"),this.h()},l(e){const t=qa("svelte-u9bgzb",document.head);b=d(t,"META",{name:!0,content:!0}),t.forEach(o),w=a(e),y=d(e,"P",{}),f(y).forEach(o),ee=a(e),c(se.$$.fragment,e),no=a(e),ae=d(e,"P",{"data-svelte-h":!0}),g(ae)!=="svelte-1fj9wfz"&&(ae.innerHTML=Ns),so=a(e),re=d(e,"P",{"data-svelte-h":!0}),g(re)!=="svelte-zk7t5e"&&(re.textContent=As),ao=a(e),c(de.$$.fragment,e),ro=a(e),ie=d(e,"P",{"data-svelte-h":!0}),g(ie)!=="svelte-a3auze"&&(ie.innerHTML=Ps),io=a(e),c(te.$$.fragment,e),co=a(e),ce=d(e,"P",{"data-svelte-h":!0}),g(ce)!=="svelte-ww282k"&&(ce.innerHTML=Ls),uo=a(e),ue=d(e,"P",{"data-svelte-h":!0}),g(ue)!=="svelte-qrkykg"&&(ue.innerHTML=Ws),lo=a(e),c(le.$$.fragment,e),po=a(e),pe=d(e,"P",{"data-svelte-h":!0}),g(pe)!=="svelte-sv5ldg"&&(pe.innerHTML=Bs),ho=a(e),he=d(e,"P",{"data-svelte-h":!0}),g(he)!=="svelte-1tp97x6"&&(he.innerHTML=Ds),mo=a(e),me=d(e,"P",{"data-svelte-h":!0}),g(me)!=="svelte-1b6en9a"&&(me.textContent=Es),go=a(e),c(ge.$$.fragment,e),fo=a(e),$=d(e,"DIV",{class:!0});var O=f($);c(fe.$$.fragment,O),es=a(O),qt=d(O,"P",{"data-svelte-h":!0}),g(qt)!=="svelte-1q6gwat"&&(qt.innerHTML=Is),ts=a(O),c(oe.$$.fragment,O),os=a(O),ne=d(O,"DIV",{class:!0});var Ot=f(ne);c(_e.$$.fragment,Ot),ns=a(Ot),xt=d(Ot,"P",{"data-svelte-h":!0}),g(xt)!=="svelte-1gdpts"&&(xt.innerHTML=Hs),Ot.forEach(o),O.forEach(o),_o=a(e),c(Te.$$.fragment,e),To=a(e),q=d(e,"DIV",{class:!0});var qn=f(q);c(ve.$$.fragment,qn),ss=a(qn),St=d(qn,"P",{"data-svelte-h":!0}),g(St)!=="svelte-19jpj19"&&(St.textContent=Vs),qn.forEach(o),vo=a(e),c(be.$$.fragment,e),bo=a(e),x=d(e,"DIV",{class:!0});var xn=f(x);c(ye.$$.fragment,xn),as=a(xn),Ft=d(xn,"P",{"data-svelte-h":!0}),g(Ft)!=="svelte-rzpgnp"&&(Ft.textContent=Qs),xn.forEach(o),yo=a(e),c($e.$$.fragment,e),$o=a(e),S=d(e,"DIV",{class:!0});var Sn=f(S);c(Oe.$$.fragment,Sn),rs=a(Sn),Ct=d(Sn,"P",{"data-svelte-h":!0}),g(Ct)!=="svelte-19jpj19"&&(Ct.textContent=js),Sn.forEach(o),Oo=a(e),c(we.$$.fragment,e),wo=a(e),F=d(e,"DIV",{class:!0});var Fn=f(F);c(qe.$$.fragment,Fn),ds=a(Fn),Mt=d(Fn,"P",{"data-svelte-h":!0}),g(Mt)!=="svelte-rzpgnp"&&(Mt.textContent=Js),Fn.forEach(o),qo=a(e),c(xe.$$.fragment,e),xo=a(e),C=d(e,"DIV",{class:!0});var Cn=f(C);c(Se.$$.fragment,Cn),is=a(Cn),zt=d(Cn,"P",{"data-svelte-h":!0}),g(zt)!=="svelte-1kt4x95"&&(zt.textContent=Us),Cn.forEach(o),So=a(e),c(Fe.$$.fragment,e),Fo=a(e),M=d(e,"DIV",{class:!0});var Mn=f(M);c(Ce.$$.fragment,Mn),cs=a(Mn),kt=d(Mn,"P",{"data-svelte-h":!0}),g(kt)!=="svelte-1kt4x95"&&(kt.textContent=Xs),Mn.forEach(o),Co=a(e),c(Me.$$.fragment,e),Mo=a(e),z=d(e,"DIV",{class:!0});var zn=f(z);c(ze.$$.fragment,zn),us=a(zn),Nt=d(zn,"P",{"data-svelte-h":!0}),g(Nt)!=="svelte-k12cko"&&(Nt.textContent=Zs),zn.forEach(o),zo=a(e),c(ke.$$.fragment,e),ko=a(e),k=d(e,"DIV",{class:!0});var kn=f(k);c(Ne.$$.fragment,kn),ls=a(kn),At=d(kn,"P",{"data-svelte-h":!0}),g(At)!=="svelte-3npkmq"&&(At.textContent=Rs),kn.forEach(o),No=a(e),c(Ae.$$.fragment,e),Ao=a(e),N=d(e,"DIV",{class:!0});var Nn=f(N);c(Pe.$$.fragment,Nn),ps=a(Nn),Pt=d(Nn,"P",{"data-svelte-h":!0}),g(Pt)!=="svelte-3npkmq"&&(Pt.textContent=Ys),Nn.forEach(o),Po=a(e),c(Le.$$.fragment,e),Lo=a(e),A=d(e,"DIV",{class:!0});var An=f(A);c(We.$$.fragment,An),hs=a(An),Lt=d(An,"P",{"data-svelte-h":!0}),g(Lt)!=="svelte-3npkmq"&&(Lt.textContent=Gs),An.forEach(o),Wo=a(e),c(Be.$$.fragment,e),Bo=a(e),P=d(e,"DIV",{class:!0});var Pn=f(P);c(De.$$.fragment,Pn),ms=a(Pn),Wt=d(Pn,"P",{"data-svelte-h":!0}),g(Wt)!=="svelte-1xezu93"&&(Wt.textContent=Ks),Pn.forEach(o),Do=a(e),c(Ee.$$.fragment,e),Eo=a(e),L=d(e,"DIV",{class:!0});var Ln=f(L);c(Ie.$$.fragment,Ln),gs=a(Ln),Bt=d(Ln,"P",{"data-svelte-h":!0}),g(Bt)!=="svelte-1dobm33"&&(Bt.textContent=ea),Ln.forEach(o),Io=a(e),c(He.$$.fragment,e),Ho=a(e),W=d(e,"DIV",{class:!0});var Wn=f(W);c(Ve.$$.fragment,Wn),fs=a(Wn),Dt=d(Wn,"P",{"data-svelte-h":!0}),g(Dt)!=="svelte-1bm2i0r"&&(Dt.textContent=ta),Wn.forEach(o),Vo=a(e),c(Qe.$$.fragment,e),Qo=a(e),B=d(e,"DIV",{class:!0});var Bn=f(B);c(je.$$.fragment,Bn),_s=a(Bn),Et=d(Bn,"P",{"data-svelte-h":!0}),g(Et)!=="svelte-o4u80u"&&(Et.textContent=oa),Bn.forEach(o),jo=a(e),c(Je.$$.fragment,e),Jo=a(e),D=d(e,"DIV",{class:!0});var Dn=f(D);c(Ue.$$.fragment,Dn),Ts=a(Dn),It=d(Dn,"P",{"data-svelte-h":!0}),g(It)!=="svelte-1kfw2sn"&&(It.textContent=na),Dn.forEach(o),Uo=a(e),c(Xe.$$.fragment,e),Xo=a(e),E=d(e,"DIV",{class:!0});var En=f(E);c(Ze.$$.fragment,En),vs=a(En),Ht=d(En,"P",{"data-svelte-h":!0}),g(Ht)!=="svelte-yql1ve"&&(Ht.textContent=sa),En.forEach(o),Zo=a(e),c(Re.$$.fragment,e),Ro=a(e),I=d(e,"DIV",{class:!0});var In=f(I);c(Ye.$$.fragment,In),bs=a(In),Vt=d(In,"P",{"data-svelte-h":!0}),g(Vt)!=="svelte-1iv9pso"&&(Vt.textContent=aa),In.forEach(o),Yo=a(e),c(Ge.$$.fragment,e),Go=a(e),H=d(e,"DIV",{class:!0});var Hn=f(H);c(Ke.$$.fragment,Hn),ys=a(Hn),Qt=d(Hn,"P",{"data-svelte-h":!0}),g(Qt)!=="svelte-w7415f"&&(Qt.textContent=ra),Hn.forEach(o),Ko=a(e),c(et.$$.fragment,e),en=a(e),V=d(e,"DIV",{class:!0});var Vn=f(V);c(tt.$$.fragment,Vn),$s=a(Vn),jt=d(Vn,"P",{"data-svelte-h":!0}),g(jt)!=="svelte-dhzux4"&&(jt.textContent=da),Vn.forEach(o),tn=a(e),c(ot.$$.fragment,e),on=a(e),Q=d(e,"DIV",{class:!0});var Qn=f(Q);c(nt.$$.fragment,Qn),Os=a(Qn),Jt=d(Qn,"P",{"data-svelte-h":!0}),g(Jt)!=="svelte-8u8l5m"&&(Jt.textContent=ia),Qn.forEach(o),nn=a(e),c(st.$$.fragment,e),sn=a(e),j=d(e,"DIV",{class:!0});var jn=f(j);c(at.$$.fragment,jn),ws=a(jn),Ut=d(jn,"P",{"data-svelte-h":!0}),g(Ut)!=="svelte-kob7ff"&&(Ut.textContent=ca),jn.forEach(o),an=a(e),c(rt.$$.fragment,e),rn=a(e),J=d(e,"DIV",{class:!0});var Jn=f(J);c(dt.$$.fragment,Jn),qs=a(Jn),Xt=d(Jn,"P",{"data-svelte-h":!0}),g(Xt)!=="svelte-sv2ymo"&&(Xt.textContent=ua),Jn.forEach(o),dn=a(e),c(it.$$.fragment,e),cn=a(e),U=d(e,"DIV",{class:!0});var Un=f(U);c(ct.$$.fragment,Un),xs=a(Un),Zt=d(Un,"P",{"data-svelte-h":!0}),g(Zt)!=="svelte-sv2ymo"&&(Zt.textContent=la),Un.forEach(o),un=a(e),c(ut.$$.fragment,e),ln=a(e),X=d(e,"DIV",{class:!0});var Xn=f(X);c(lt.$$.fragment,Xn),Ss=a(Xn),Rt=d(Xn,"P",{"data-svelte-h":!0}),g(Rt)!=="svelte-1rjgll"&&(Rt.textContent=pa),Xn.forEach(o),pn=a(e),c(pt.$$.fragment,e),hn=a(e),Z=d(e,"DIV",{class:!0});var Zn=f(Z);c(ht.$$.fragment,Zn),Fs=a(Zn),Yt=d(Zn,"P",{"data-svelte-h":!0}),g(Yt)!=="svelte-1tmia4b"&&(Yt.textContent=ha),Zn.forEach(o),mn=a(e),c(mt.$$.fragment,e),gn=a(e),R=d(e,"DIV",{class:!0});var Rn=f(R);c(gt.$$.fragment,Rn),Cs=a(Rn),Gt=d(Rn,"P",{"data-svelte-h":!0}),g(Gt)!=="svelte-1n4s9rx"&&(Gt.innerHTML=ma),Rn.forEach(o),fn=a(e),c(ft.$$.fragment,e),_n=a(e),Y=d(e,"DIV",{class:!0});var Yn=f(Y);c(_t.$$.fragment,Yn),Ms=a(Yn),Kt=d(Yn,"P",{"data-svelte-h":!0}),g(Kt)!=="svelte-dz9ong"&&(Kt.textContent=ga),Yn.forEach(o),Tn=a(e),c(Tt.$$.fragment,e),vn=a(e),G=d(e,"DIV",{class:!0});var Gn=f(G);c(vt.$$.fragment,Gn),zs=a(Gn),eo=d(Gn,"P",{"data-svelte-h":!0}),g(eo)!=="svelte-os2t7w"&&(eo.textContent=fa),Gn.forEach(o),bn=a(e),c(bt.$$.fragment,e),yn=a(e),K=d(e,"DIV",{class:!0});var Kn=f(K);c(yt.$$.fragment,Kn),ks=a(Kn),to=d(Kn,"P",{"data-svelte-h":!0}),g(to)!=="svelte-1iolpnx"&&(to.textContent=_a),Kn.forEach(o),$n=a(e),c($t.$$.fragment,e),On=a(e),oo=d(e,"P",{}),f(oo).forEach(o),this.h()},h(){_(b,"name","hf:doc:metadata"),_(b,"content",Ma),_(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){m(document.head,b),n(e,w,t),n(e,y,t),n(e,ee,t),u(se,e,t),n(e,no,t),n(e,ae,t),n(e,so,t),n(e,re,t),n(e,ao,t),u(de,e,t),n(e,ro,t),n(e,ie,t),n(e,io,t),u(te,e,t),n(e,co,t),n(e,ce,t),n(e,uo,t),n(e,ue,t),n(e,lo,t),u(le,e,t),n(e,po,t),n(e,pe,t),n(e,ho,t),n(e,he,t),n(e,mo,t),n(e,me,t),n(e,go,t),u(ge,e,t),n(e,fo,t),n(e,$,t),u(fe,$,null),m($,es),m($,qt),m($,ts),u(oe,$,null),m($,os),m($,ne),u(_e,ne,null),m(ne,ns),m(ne,xt),n(e,_o,t),u(Te,e,t),n(e,To,t),n(e,q,t),u(ve,q,null),m(q,ss),m(q,St),n(e,vo,t),u(be,e,t),n(e,bo,t),n(e,x,t),u(ye,x,null),m(x,as),m(x,Ft),n(e,yo,t),u($e,e,t),n(e,$o,t),n(e,S,t),u(Oe,S,null),m(S,rs),m(S,Ct),n(e,Oo,t),u(we,e,t),n(e,wo,t),n(e,F,t),u(qe,F,null),m(F,ds),m(F,Mt),n(e,qo,t),u(xe,e,t),n(e,xo,t),n(e,C,t),u(Se,C,null),m(C,is),m(C,zt),n(e,So,t),u(Fe,e,t),n(e,Fo,t),n(e,M,t),u(Ce,M,null),m(M,cs),m(M,kt),n(e,Co,t),u(Me,e,t),n(e,Mo,t),n(e,z,t),u(ze,z,null),m(z,us),m(z,Nt),n(e,zo,t),u(ke,e,t),n(e,ko,t),n(e,k,t),u(Ne,k,null),m(k,ls),m(k,At),n(e,No,t),u(Ae,e,t),n(e,Ao,t),n(e,N,t),u(Pe,N,null),m(N,ps),m(N,Pt),n(e,Po,t),u(Le,e,t),n(e,Lo,t),n(e,A,t),u(We,A,null),m(A,hs),m(A,Lt),n(e,Wo,t),u(Be,e,t),n(e,Bo,t),n(e,P,t),u(De,P,null),m(P,ms),m(P,Wt),n(e,Do,t),u(Ee,e,t),n(e,Eo,t),n(e,L,t),u(Ie,L,null),m(L,gs),m(L,Bt),n(e,Io,t),u(He,e,t),n(e,Ho,t),n(e,W,t),u(Ve,W,null),m(W,fs),m(W,Dt),n(e,Vo,t),u(Qe,e,t),n(e,Qo,t),n(e,B,t),u(je,B,null),m(B,_s),m(B,Et),n(e,jo,t),u(Je,e,t),n(e,Jo,t),n(e,D,t),u(Ue,D,null),m(D,Ts),m(D,It),n(e,Uo,t),u(Xe,e,t),n(e,Xo,t),n(e,E,t),u(Ze,E,null),m(E,vs),m(E,Ht),n(e,Zo,t),u(Re,e,t),n(e,Ro,t),n(e,I,t),u(Ye,I,null),m(I,bs),m(I,Vt),n(e,Yo,t),u(Ge,e,t),n(e,Go,t),n(e,H,t),u(Ke,H,null),m(H,ys),m(H,Qt),n(e,Ko,t),u(et,e,t),n(e,en,t),n(e,V,t),u(tt,V,null),m(V,$s),m(V,jt),n(e,tn,t),u(ot,e,t),n(e,on,t),n(e,Q,t),u(nt,Q,null),m(Q,Os),m(Q,Jt),n(e,nn,t),u(st,e,t),n(e,sn,t),n(e,j,t),u(at,j,null),m(j,ws),m(j,Ut),n(e,an,t),u(rt,e,t),n(e,rn,t),n(e,J,t),u(dt,J,null),m(J,qs),m(J,Xt),n(e,dn,t),u(it,e,t),n(e,cn,t),n(e,U,t),u(ct,U,null),m(U,xs),m(U,Zt),n(e,un,t),u(ut,e,t),n(e,ln,t),n(e,X,t),u(lt,X,null),m(X,Ss),m(X,Rt),n(e,pn,t),u(pt,e,t),n(e,hn,t),n(e,Z,t),u(ht,Z,null),m(Z,Fs),m(Z,Yt),n(e,mn,t),u(mt,e,t),n(e,gn,t),n(e,R,t),u(gt,R,null),m(R,Cs),m(R,Gt),n(e,fn,t),u(ft,e,t),n(e,_n,t),n(e,Y,t),u(_t,Y,null),m(Y,Ms),m(Y,Kt),n(e,Tn,t),u(Tt,e,t),n(e,vn,t),n(e,G,t),u(vt,G,null),m(G,zs),m(G,eo),n(e,bn,t),u(bt,e,t),n(e,yn,t),n(e,K,t),u(yt,K,null),m(K,ks),m(K,to),n(e,$n,t),u($t,e,t),n(e,On,t),n(e,oo,t),wn=!0},p(e,[t]){const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),te.$set(O);const Ot={};t&2&&(Ot.$$scope={dirty:t,ctx:e}),oe.$set(Ot)},i(e){wn||(l(se.$$.fragment,e),l(de.$$.fragment,e),l(te.$$.fragment,e),l(le.$$.fragment,e),l(ge.$$.fragment,e),l(fe.$$.fragment,e),l(oe.$$.fragment,e),l(_e.$$.fragment,e),l(Te.$$.fragment,e),l(ve.$$.fragment,e),l(be.$$.fragment,e),l(ye.$$.fragment,e),l($e.$$.fragment,e),l(Oe.$$.fragment,e),l(we.$$.fragment,e),l(qe.$$.fragment,e),l(xe.$$.fragment,e),l(Se.$$.fragment,e),l(Fe.$$.fragment,e),l(Ce.$$.fragment,e),l(Me.$$.fragment,e),l(ze.$$.fragment,e),l(ke.$$.fragment,e),l(Ne.$$.fragment,e),l(Ae.$$.fragment,e),l(Pe.$$.fragment,e),l(Le.$$.fragment,e),l(We.$$.fragment,e),l(Be.$$.fragment,e),l(De.$$.fragment,e),l(Ee.$$.fragment,e),l(Ie.$$.fragment,e),l(He.$$.fragment,e),l(Ve.$$.fragment,e),l(Qe.$$.fragment,e),l(je.$$.fragment,e),l(Je.$$.fragment,e),l(Ue.$$.fragment,e),l(Xe.$$.fragment,e),l(Ze.$$.fragment,e),l(Re.$$.fragment,e),l(Ye.$$.fragment,e),l(Ge.$$.fragment,e),l(Ke.$$.fragment,e),l(et.$$.fragment,e),l(tt.$$.fragment,e),l(ot.$$.fragment,e),l(nt.$$.fragment,e),l(st.$$.fragment,e),l(at.$$.fragment,e),l(rt.$$.fragment,e),l(dt.$$.fragment,e),l(it.$$.fragment,e),l(ct.$$.fragment,e),l(ut.$$.fragment,e),l(lt.$$.fragment,e),l(pt.$$.fragment,e),l(ht.$$.fragment,e),l(mt.$$.fragment,e),l(gt.$$.fragment,e),l(ft.$$.fragment,e),l(_t.$$.fragment,e),l(Tt.$$.fragment,e),l(vt.$$.fragment,e),l(bt.$$.fragment,e),l(yt.$$.fragment,e),l($t.$$.fragment,e),wn=!0)},o(e){p(se.$$.fragment,e),p(de.$$.fragment,e),p(te.$$.fragment,e),p(le.$$.fragment,e),p(ge.$$.fragment,e),p(fe.$$.fragment,e),p(oe.$$.fragment,e),p(_e.$$.fragment,e),p(Te.$$.fragment,e),p(ve.$$.fragment,e),p(be.$$.fragment,e),p(ye.$$.fragment,e),p($e.$$.fragment,e),p(Oe.$$.fragment,e),p(we.$$.fragment,e),p(qe.$$.fragment,e),p(xe.$$.fragment,e),p(Se.$$.fragment,e),p(Fe.$$.fragment,e),p(Ce.$$.fragment,e),p(Me.$$.fragment,e),p(ze.$$.fragment,e),p(ke.$$.fragment,e),p(Ne.$$.fragment,e),p(Ae.$$.fragment,e),p(Pe.$$.fragment,e),p(Le.$$.fragment,e),p(We.$$.fragment,e),p(Be.$$.fragment,e),p(De.$$.fragment,e),p(Ee.$$.fragment,e),p(Ie.$$.fragment,e),p(He.$$.fragment,e),p(Ve.$$.fragment,e),p(Qe.$$.fragment,e),p(je.$$.fragment,e),p(Je.$$.fragment,e),p(Ue.$$.fragment,e),p(Xe.$$.fragment,e),p(Ze.$$.fragment,e),p(Re.$$.fragment,e),p(Ye.$$.fragment,e),p(Ge.$$.fragment,e),p(Ke.$$.fragment,e),p(et.$$.fragment,e),p(tt.$$.fragment,e),p(ot.$$.fragment,e),p(nt.$$.fragment,e),p(st.$$.fragment,e),p(at.$$.fragment,e),p(rt.$$.fragment,e),p(dt.$$.fragment,e),p(it.$$.fragment,e),p(ct.$$.fragment,e),p(ut.$$.fragment,e),p(lt.$$.fragment,e),p(pt.$$.fragment,e),p(ht.$$.fragment,e),p(mt.$$.fragment,e),p(gt.$$.fragment,e),p(ft.$$.fragment,e),p(_t.$$.fragment,e),p(Tt.$$.fragment,e),p(vt.$$.fragment,e),p(bt.$$.fragment,e),p(yt.$$.fragment,e),p($t.$$.fragment,e),wn=!1},d(e){e&&(o(w),o(y),o(ee),o(no),o(ae),o(so),o(re),o(ao),o(ro),o(ie),o(io),o(co),o(ce),o(uo),o(ue),o(lo),o(po),o(pe),o(ho),o(he),o(mo),o(me),o(go),o(fo),o($),o(_o),o(To),o(q),o(vo),o(bo),o(x),o(yo),o($o),o(S),o(Oo),o(wo),o(F),o(qo),o(xo),o(C),o(So),o(Fo),o(M),o(Co),o(Mo),o(z),o(zo),o(ko),o(k),o(No),o(Ao),o(N),o(Po),o(Lo),o(A),o(Wo),o(Bo),o(P),o(Do),o(Eo),o(L),o(Io),o(Ho),o(W),o(Vo),o(Qo),o(B),o(jo),o(Jo),o(D),o(Uo),o(Xo),o(E),o(Zo),o(Ro),o(I),o(Yo),o(Go),o(H),o(Ko),o(en),o(V),o(tn),o(on),o(Q),o(nn),o(sn),o(j),o(an),o(rn),o(J),o(dn),o(cn),o(U),o(un),o(ln),o(X),o(pn),o(hn),o(Z),o(mn),o(gn),o(R),o(fn),o(_n),o(Y),o(Tn),o(vn),o(G),o(bn),o(yn),o(K),o($n),o(On),o(oo)),o(b),h(se,e),h(de,e),h(te,e),h(le,e),h(ge,e),h(fe),h(oe),h(_e),h(Te,e),h(ve),h(be,e),h(ye),h($e,e),h(Oe),h(we,e),h(qe),h(xe,e),h(Se),h(Fe,e),h(Ce),h(Me,e),h(ze),h(ke,e),h(Ne),h(Ae,e),h(Pe),h(Le,e),h(We),h(Be,e),h(De),h(Ee,e),h(Ie),h(He,e),h(Ve),h(Qe,e),h(je),h(Je,e),h(Ue),h(Xe,e),h(Ze),h(Re,e),h(Ye),h(Ge,e),h(Ke),h(et,e),h(tt),h(ot,e),h(nt),h(st,e),h(at),h(rt,e),h(dt),h(it,e),h(ct),h(ut,e),h(lt),h(pt,e),h(ht),h(mt,e),h(gt),h(ft,e),h(_t),h(Tt,e),h(vt),h(bt,e),h(yt),h($t,e)}}}const Ma='{"title":"Model outputs","local":"model-outputs","sections":[{"title":"ModelOutput","local":"transformers.utils.ModelOutput","sections":[],"depth":2},{"title":"BaseModelOutput","local":"transformers.modeling_outputs.BaseModelOutput","sections":[],"depth":2},{"title":"BaseModelOutputWithPooling","local":"transformers.modeling_outputs.BaseModelOutputWithPooling","sections":[],"depth":2},{"title":"BaseModelOutputWithCrossAttentions","local":"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions","sections":[],"depth":2},{"title":"BaseModelOutputWithPoolingAndCrossAttentions","local":"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions","sections":[],"depth":2},{"title":"BaseModelOutputWithPast","local":"transformers.modeling_outputs.BaseModelOutputWithPast","sections":[],"depth":2},{"title":"BaseModelOutputWithPastAndCrossAttentions","local":"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions","sections":[],"depth":2},{"title":"Seq2SeqModelOutput","local":"transformers.modeling_outputs.Seq2SeqModelOutput","sections":[],"depth":2},{"title":"CausalLMOutput","local":"transformers.modeling_outputs.CausalLMOutput","sections":[],"depth":2},{"title":"CausalLMOutputWithCrossAttentions","local":"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions","sections":[],"depth":2},{"title":"CausalLMOutputWithPast","local":"transformers.modeling_outputs.CausalLMOutputWithPast","sections":[],"depth":2},{"title":"MaskedLMOutput","local":"transformers.modeling_outputs.MaskedLMOutput","sections":[],"depth":2},{"title":"Seq2SeqLMOutput","local":"transformers.modeling_outputs.Seq2SeqLMOutput","sections":[],"depth":2},{"title":"NextSentencePredictorOutput","local":"transformers.modeling_outputs.NextSentencePredictorOutput","sections":[],"depth":2},{"title":"SequenceClassifierOutput","local":"transformers.modeling_outputs.SequenceClassifierOutput","sections":[],"depth":2},{"title":"Seq2SeqSequenceClassifierOutput","local":"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput","sections":[],"depth":2},{"title":"MultipleChoiceModelOutput","local":"transformers.modeling_outputs.MultipleChoiceModelOutput","sections":[],"depth":2},{"title":"TokenClassifierOutput","local":"transformers.modeling_outputs.TokenClassifierOutput","sections":[],"depth":2},{"title":"QuestionAnsweringModelOutput","local":"transformers.modeling_outputs.QuestionAnsweringModelOutput","sections":[],"depth":2},{"title":"Seq2SeqQuestionAnsweringModelOutput","local":"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput","sections":[],"depth":2},{"title":"Seq2SeqSpectrogramOutput","local":"transformers.modeling_outputs.Seq2SeqSpectrogramOutput","sections":[],"depth":2},{"title":"SemanticSegmenterOutput","local":"transformers.modeling_outputs.SemanticSegmenterOutput","sections":[],"depth":2},{"title":"ImageClassifierOutput","local":"transformers.modeling_outputs.ImageClassifierOutput","sections":[],"depth":2},{"title":"ImageClassifierOutputWithNoAttention","local":"transformers.modeling_outputs.ImageClassifierOutputWithNoAttention","sections":[],"depth":2},{"title":"DepthEstimatorOutput","local":"transformers.modeling_outputs.DepthEstimatorOutput","sections":[],"depth":2},{"title":"Wav2Vec2BaseModelOutput","local":"transformers.modeling_outputs.Wav2Vec2BaseModelOutput","sections":[],"depth":2},{"title":"XVectorOutput","local":"transformers.modeling_outputs.XVectorOutput","sections":[],"depth":2},{"title":"Seq2SeqTSModelOutput","local":"transformers.modeling_outputs.Seq2SeqTSModelOutput","sections":[],"depth":2},{"title":"Seq2SeqTSPredictionOutput","local":"transformers.modeling_outputs.Seq2SeqTSPredictionOutput","sections":[],"depth":2},{"title":"SampleTSPredictionOutput","local":"transformers.modeling_outputs.SampleTSPredictionOutput","sections":[],"depth":2}],"depth":1}';function za(wt){return $a(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ba extends Oa{constructor(b){super(),wa(this,b,za,Ca,ya,{})}}export{Ba as component};
