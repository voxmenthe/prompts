import{s as $t,o as gt,n as bt}from"../chunks/scheduler.18a86fab.js";import{S as _t,i as At,g as c,s as l,r as M,A as Nt,h as i,f as s,c as n,j as It,u as J,x as u,k as Ut,y as vt,a,v as T,d as w,t as j,w as y}from"../chunks/index.98837b22.js";import{C as b}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as ue,E as Ct}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Zt,a as xt}from"../chunks/HfOption.6641485e.js";function kt($){let p,U='The <a href="./model_doc/auto">AutoClass</a> API automatically loads the correct feature extractor for a given model.',f,d,o='Use <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained">from_pretrained()</a> to load a feature extractor.',h,m,x;return m=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGd2hpc3Blci10aW55JTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)`,wrap:!1}}),{c(){p=c("p"),p.innerHTML=U,f=l(),d=c("p"),d.innerHTML=o,h=l(),M(m.$$.fragment)},l(r){p=i(r,"P",{"data-svelte-h":!0}),u(p)!=="svelte-1gxot7v"&&(p.innerHTML=U),f=n(r),d=i(r,"P",{"data-svelte-h":!0}),u(d)!=="svelte-1q5v375"&&(d.innerHTML=o),h=n(r),J(m.$$.fragment,r)},m(r,I){a(r,p,I),a(r,f,I),a(r,d,I),a(r,h,I),T(m,r,I),x=!0},p:bt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){j(m.$$.fragment,r),x=!1},d(r){r&&(s(p),s(f),s(d),s(h)),y(m,r)}}}function Vt($){let p,U='Every pretrained audio model has a specific associated feature extractor for correctly processing audio data. When you load a feature extractor, it retrieves the feature extractors configuration (feature size, chunk length, etc.) from <a href="https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json" rel="nofollow">preprocessor_config.json</a>.',f,d,o="A feature extractor can be loaded directly from its model-specific class.",h,m,x;return m=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFdoaXNwZXJGZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBXaGlzcGVyRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGd2hpc3Blci10aW55JTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-tiny&quot;</span>)`,wrap:!1}}),{c(){p=c("p"),p.innerHTML=U,f=l(),d=c("p"),d.textContent=o,h=l(),M(m.$$.fragment)},l(r){p=i(r,"P",{"data-svelte-h":!0}),u(p)!=="svelte-b0339v"&&(p.innerHTML=U),f=n(r),d=i(r,"P",{"data-svelte-h":!0}),u(d)!=="svelte-1knq7wb"&&(d.textContent=o),h=n(r),J(m.$$.fragment,r)},m(r,I){a(r,p,I),a(r,f,I),a(r,d,I),a(r,h,I),T(m,r,I),x=!0},p:bt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){j(m.$$.fragment,r),x=!1},d(r){r&&(s(p),s(f),s(d),s(h)),y(m,r)}}}function Xt($){let p,U,f,d;return p=new xt({props:{id:"feature-extractor-classes",option:"AutoFeatureExtractor",$$slots:{default:[kt]},$$scope:{ctx:$}}}),f=new xt({props:{id:"feature-extractor-classes",option:"model-specific feature extractor",$$slots:{default:[Vt]},$$scope:{ctx:$}}}),{c(){M(p.$$.fragment),U=l(),M(f.$$.fragment)},l(o){J(p.$$.fragment,o),U=n(o),J(f.$$.fragment,o)},m(o,h){T(p,o,h),a(o,U,h),T(f,o,h),d=!0},p(o,h){const m={};h&2&&(m.$$scope={dirty:h,ctx:o}),p.$set(m);const x={};h&2&&(x.$$scope={dirty:h,ctx:o}),f.$set(x)},i(o){d||(w(p.$$.fragment,o),w(f.$$.fragment,o),d=!0)},o(o){j(p.$$.fragment,o),j(f.$$.fragment,o),d=!1},d(o){o&&s(U),y(p,o),y(f,o)}}}function Ft($){let p,U,f,d,o,h,m,x="Feature extractors preprocess audio data into the correct format for a given model. It takes the raw audio signal and converts it into a tensor that can be fed to a model. The tensor shape depends on the model, but the feature extractor will correctly preprocess the audio data for you given the model you’re using. Feature extractors also include methods for padding, truncation, and resampling.",r,I,Ke='Call <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained">from_pretrained()</a> to load a feature extractor and its preprocessor configuration from the Hugging Face <a href="https://hf.co/models" rel="nofollow">Hub</a> or local directory. The feature extractor and preprocessor configuration is saved in a <a href="https://hf.co/openai/whisper-tiny/blob/main/preprocessor_config.json" rel="nofollow">preprocessor_config.json</a> file.',de,_,et="Pass the audio signal, typically stored in <code>array</code>, to the feature extractor and set the <code>sampling_rate</code> parameter to the pretrained audio models sampling rate. It is important the sampling rate of the audio data matches the sampling rate of the data a pretrained audio model was trained on.",fe,A,he,N,tt="The feature extractor returns an input, <code>input_values</code>, that is ready for the model to consume.",Me,v,st="This guide walks you through the feature extractor classes and how to preprocess audio data.",Je,C,Te,Z,at='Transformers feature extractors inherit from the base <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> class which subclasses <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin">FeatureExtractionMixin</a>.',we,k,lt='<li><a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> provides a method to <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> sequences to a certain length to avoid uneven sequence lengths.</li> <li><a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin">FeatureExtractionMixin</a> provides <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> to load and save a feature extractor.</li>',je,V,nt='There are two ways you can load a feature extractor, <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> and a model-specific feature extractor class.',ye,g,Ie,X,Ue,F,rt="A feature extractor expects the input as a PyTorch tensor of a certain shape. The exact input shape can vary depending on the specific audio model you’re using.",xe,Y,pt='For example, <a href="https://huggingface.co/docs/transformers/model_doc/whisper" rel="nofollow">Whisper</a> expects <code>input_features</code> to be a tensor of shape <code>(batch_size, feature_size, sequence_length)</code> but <a href="https://hf.co/docs/transformers/model_doc/wav2vec2" rel="nofollow">Wav2Vec2</a> expects <code>input_values</code> to be a tensor of shape <code>(batch_size, sequence_length)</code>.',be,R,ot="The feature extractor generates the correct input shape for whichever audio model you’re using.",$e,Q,ct="A feature extractor also sets the sampling rate (the number of audio signal values taken per second) of the audio files. The sampling rate of your audio data must match the sampling rate of the dataset a pretrained model was trained on. This value is typically given in the model card.",ge,B,it='Load a dataset and feature extractor with <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a>.',_e,G,Ae,E,ut="Check out the first example from the dataset and access the <code>audio</code> column which contains <code>array</code>, the raw audio signal.",Ne,H,ve,z,mt="The feature extractor preprocesses <code>array</code> into the expected input format for a given audio model. Use the <code>sampling_rate</code> parameter to set the appropriate sampling rate.",Ce,W,Ze,D,ke,q,dt="Audio sequence lengths that are different is an issue because Transformers expects all sequences to have the same lengths so they can be batched. Uneven sequence lengths can’t be batched.",Ve,L,Xe,S,ft="Padding adds a special <em>padding token</em> to ensure all sequences have the same length. The feature extractor adds a <code>0</code> - interpreted as silence - to <code>array</code> to pad it. Set <code>padding=True</code> to pad sequences to the longest sequence length in the batch.",Fe,P,Ye,O,Re,K,ht="Models can only process sequences up to a certain length before crashing.",Qe,ee,Mt="Truncation is a strategy for removing excess tokens from a sequence to ensure it doesn’t exceed the maximum length. Set <code>truncation=True</code> to truncate a sequence to the length in the <code>max_length</code> parameter.",Be,te,Ge,se,Ee,ae,Jt='The <a href="https://hf.co/docs/datasets/index" rel="nofollow">Datasets</a> library can also resample audio data to match an audio models expected sampling rate. This method resamples the audio data on the fly when they’re loaded which can be faster than resampling the entire dataset in-place.',He,le,Tt="The audio dataset you’ve been working on has a sampling rate of 8kHz and the pretrained model expects 16kHz.",ze,ne,We,re,wt='Call <a href="https://huggingface.co/docs/datasets/v4.1.0/en/package_reference/main_classes#datasets.Dataset.cast_column" rel="nofollow">cast_column</a> on the <code>audio</code> column to upsample the sampling rate to 16kHz.',De,pe,qe,oe,jt="When you load the dataset sample, it is now resampled to 16kHz.",Le,ce,Se,ie,Pe,me,Oe;return o=new ue({props:{title:"Feature extractors",local:"feature-extractors",headingTag:"h1"}}),A=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyUG9seUFJJTJGbWluZHMxNCUyMiUyQyUyMG5hbWUlM0QlMjJlbi1VUyUyMiUyQyUyMHNwbGl0JTNEJTIydHJhaW4lMjIpJTBBcHJvY2Vzc2VkX3NhbXBsZSUyMCUzRCUyMGZlYXR1cmVfZXh0cmFjdG9yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0QxNjAwMCklMEFwcm9jZXNzZWRfc2FtcGxlJTBBJTdCJ2lucHV0X3ZhbHVlcyclM0ElMjAlNUJhcnJheSglNUIlMjA5LjQ0NzI3NDRlLTA1JTJDJTIwJTIwMy4wNzc3ODgwZS0wMyUyQyUyMC0yLjg4ODg0MjdlLTAzJTJDJTIwLi4uJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwLTIuODg4ODQyN2UtMDMlMkMlMjAlMjA5LjQ0NzI3NDRlLTA1JTJDJTIwJTIwOS40NDcyNzQ0ZS0wNSU1RCUyQyUyMGR0eXBlJTNEZmxvYXQzMiklNUQlN0Q=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)
dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
processed_sample = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>)
processed_sample
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">9.4472744e-05</span>,  <span class="hljs-number">3.0777880e-03</span>, -<span class="hljs-number">2.8888427e-03</span>, ...,
       -<span class="hljs-number">2.8888427e-03</span>,  <span class="hljs-number">9.4472744e-05</span>,  <span class="hljs-number">9.4472744e-05</span>], dtype=float32)]}`,wrap:!1}}),C=new ue({props:{title:"Feature extractor classes",local:"feature-extractor-classes",headingTag:"h2"}}),g=new Zt({props:{id:"feature-extractor-classes",options:["AutoFeatureExtractor","model-specific feature extractor"],$$slots:{default:[Xt]},$$scope:{ctx:$}}}),X=new ue({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),G=new b({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZSUyMik=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`,wrap:!1}}),H=new b({props:{code:"ZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVEJTBBYXJyYXkoJTVCJTIwMC4lMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMkMlMjAlMjAwLjAwMDI0NDE0JTJDJTIwLTAuMDAwMjQ0MTQlMkMlMjAuLi4lMkMlMjAtMC4wMDAyNDQxNCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTJDJTIwJTIwMC4lMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUQp",highlighted:`dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
        <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ])`,wrap:!1}}),W=new b({props:{code:"cHJvY2Vzc2VkX2RhdGFzZXQlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEMTYwMDApJTBBcHJvY2Vzc2VkX2RhdGFzZXQlMEElN0InaW5wdXRfdmFsdWVzJyUzQSUyMCU1QmFycmF5KCU1QiUyMDkuNDQ3Mjc0NGUtMDUlMkMlMjAlMjAzLjA3Nzc4ODBlLTAzJTJDJTIwLTIuODg4ODQyN2UtMDMlMkMlMjAuLi4lMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAtMi44ODg4NDI3ZS0wMyUyQyUyMCUyMDkuNDQ3Mjc0NGUtMDUlMkMlMjAlMjA5LjQ0NzI3NDRlLTA1JTVEJTJDJTIwZHR5cGUlM0RmbG9hdDMyKSU1RCU3RA==",highlighted:`processed_dataset = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>)
processed_dataset
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">9.4472744e-05</span>,  <span class="hljs-number">3.0777880e-03</span>, -<span class="hljs-number">2.8888427e-03</span>, ...,
       -<span class="hljs-number">2.8888427e-03</span>,  <span class="hljs-number">9.4472744e-05</span>,  <span class="hljs-number">9.4472744e-05</span>], dtype=float32)]}`,wrap:!1}}),D=new ue({props:{title:"Padding",local:"padding",headingTag:"h3"}}),L=new b({props:{code:"ZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMmFycmF5JTIyJTVELnNoYXBlJTBBKDg2Njk5JTJDKSUwQSUwQWRhdGFzZXQlNUIxJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RC5zaGFwZSUwQSg1MzI0OCUyQyk=",highlighted:`dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">86699</span>,)

dataset[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">53248</span>,)`,wrap:!1}}),P=new b({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjBhdWRpb19hcnJheXMlMjAlM0QlMjAlNUJ4JTVCJTIyYXJyYXklMjIlNUQlMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmF1ZGlvJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwaW5wdXRzJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3IoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYXVkaW9fYXJyYXlzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2FtcGxpbmdfcmF0ZSUzRDE2MDAwJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcGFkZGluZyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjApJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5wdXRzJTBBJTBBcHJvY2Vzc2VkX2RhdGFzZXQlMjAlM0QlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uKGRhdGFzZXQlNUIlM0E1JTVEKSUwQXByb2Nlc3NlZF9kYXRhc2V0JTVCJTIyaW5wdXRfdmFsdWVzJTIyJTVEJTVCMCU1RC5zaGFwZSUwQSg4NjY5OSUyQyklMEElMEFwcm9jZXNzZWRfZGF0YXNldCU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCU1QjElNUQuc2hhcGUlMEEoODY2OTklMkMp",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=<span class="hljs-number">16000</span>,
        padding=<span class="hljs-literal">True</span>,
    )
    <span class="hljs-keyword">return</span> inputs

processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])
processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">86699</span>,)

processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">86699</span>,)`,wrap:!1}}),O=new ue({props:{title:"Truncation",local:"truncation",headingTag:"h3"}}),te=new b({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjBhdWRpb19hcnJheXMlMjAlM0QlMjAlNUJ4JTVCJTIyYXJyYXklMjIlNUQlMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmF1ZGlvJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwaW5wdXRzJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3IoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYXVkaW9fYXJyYXlzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2FtcGxpbmdfcmF0ZSUzRDE2MDAwJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbWF4X2xlbmd0aCUzRDUwMDAwJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjApJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5wdXRzJTBBJTBBcHJvY2Vzc2VkX2RhdGFzZXQlMjAlM0QlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uKGRhdGFzZXQlNUIlM0E1JTVEKSUwQXByb2Nlc3NlZF9kYXRhc2V0JTVCJTIyaW5wdXRfdmFsdWVzJTIyJTVEJTVCMCU1RC5zaGFwZSUwQSg1MDAwMCUyQyklMEElMEFwcm9jZXNzZWRfZGF0YXNldCU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCU1QjElNUQuc2hhcGUlMEEoNTAwMDAlMkMp",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=<span class="hljs-number">16000</span>,
        max_length=<span class="hljs-number">50000</span>,
        truncation=<span class="hljs-literal">True</span>,
    )
    <span class="hljs-keyword">return</span> inputs

processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])
processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">50000</span>,)

processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">50000</span>,)`,wrap:!1}}),se=new ue({props:{title:"Resampling",local:"resampling",headingTag:"h3"}}),ne=new b({props:{code:"ZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCUwQSU3QidwYXRoJyUzQSUyMCclMkZyb290JTJGLmNhY2hlJTJGaHVnZ2luZ2ZhY2UlMkZkYXRhc2V0cyUyRmRvd25sb2FkcyUyRmV4dHJhY3RlZCUyRmY1MDdmZGNhN2Y0NzVkOTYxZjViYjcwOTNiY2M5ZDU0NGYxNmY4Y2FiODYwOGU3NzJhMmVkNGZiZWI0ZDZmNTAlMkZlbi1VU35KT0lOVF9BQ0NPVU5UJTJGNjAyYmE1NWFiYjFlNmQwZmJjZTkyMDY1LndhdiclMkMlMEElMjAnYXJyYXknJTNBJTIwYXJyYXkoJTVCJTIwMC4lMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMkMlMjAlMjAwLjAwMDI0NDE0JTJDJTIwLTAuMDAwMjQ0MTQlMkMlMjAuLi4lMkMlMjAtMC4wMDAyNDQxNCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTJDJTIwJTIwMC4lMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUQpJTJDJTBBJTIwJ3NhbXBsaW5nX3JhdGUnJTNBJTIwODAwMCU3RA==",highlighted:`dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ]),
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`,wrap:!1}}),pe=new b({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEMTYwMDApKQ==",highlighted:'dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))',wrap:!1}}),ce=new b({props:{code:"ZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCUwQSU3QidwYXRoJyUzQSUyMCclMkZyb290JTJGLmNhY2hlJTJGaHVnZ2luZ2ZhY2UlMkZkYXRhc2V0cyUyRmRvd25sb2FkcyUyRmV4dHJhY3RlZCUyRmY1MDdmZGNhN2Y0NzVkOTYxZjViYjcwOTNiY2M5ZDU0NGYxNmY4Y2FiODYwOGU3NzJhMmVkNGZiZWI0ZDZmNTAlMkZlbi1VU35KT0lOVF9BQ0NPVU5UJTJGNjAyYmE1NWFiYjFlNmQwZmJjZTkyMDY1LndhdiclMkMlMEElMjAnYXJyYXknJTNBJTIwYXJyYXkoJTVCJTIwMS43MDU2MjQxNmUtMDUlMkMlMjAlMjAyLjE4NzI3NDUxZS0wNCUyQyUyMCUyMDIuMjgwOTk4NzRlLTA0JTJDJTIwLi4uJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwMy40Mzg0MjQwM2UtMDUlMkMlMjAtNS45NjM2NDc3MWUtMDYlMkMlMjAtMS43Njg0NjY2MWUtMDUlNUQpJTJDJTBBJTIwJ3NhbXBsaW5nX3JhdGUnJTNBJTIwMTYwMDAlN0Q=",highlighted:`dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f507fdca7f475d961f5bb7093bcc9d544f16f8cab8608e772a2ed4fbeb4d6f50/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">1.70562416e-05</span>,  <span class="hljs-number">2.18727451e-04</span>,  <span class="hljs-number">2.28099874e-04</span>, ...,
         <span class="hljs-number">3.43842403e-05</span>, -<span class="hljs-number">5.96364771e-06</span>, -<span class="hljs-number">1.76846661e-05</span>]),
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`,wrap:!1}}),ie=new Ct({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md"}}),{c(){p=c("meta"),U=l(),f=c("p"),d=l(),M(o.$$.fragment),h=l(),m=c("p"),m.textContent=x,r=l(),I=c("p"),I.innerHTML=Ke,de=l(),_=c("p"),_.innerHTML=et,fe=l(),M(A.$$.fragment),he=l(),N=c("p"),N.innerHTML=tt,Me=l(),v=c("p"),v.textContent=st,Je=l(),M(C.$$.fragment),Te=l(),Z=c("p"),Z.innerHTML=at,we=l(),k=c("ul"),k.innerHTML=lt,je=l(),V=c("p"),V.innerHTML=nt,ye=l(),M(g.$$.fragment),Ie=l(),M(X.$$.fragment),Ue=l(),F=c("p"),F.textContent=rt,xe=l(),Y=c("p"),Y.innerHTML=pt,be=l(),R=c("p"),R.textContent=ot,$e=l(),Q=c("p"),Q.textContent=ct,ge=l(),B=c("p"),B.innerHTML=it,_e=l(),M(G.$$.fragment),Ae=l(),E=c("p"),E.innerHTML=ut,Ne=l(),M(H.$$.fragment),ve=l(),z=c("p"),z.innerHTML=mt,Ce=l(),M(W.$$.fragment),Ze=l(),M(D.$$.fragment),ke=l(),q=c("p"),q.textContent=dt,Ve=l(),M(L.$$.fragment),Xe=l(),S=c("p"),S.innerHTML=ft,Fe=l(),M(P.$$.fragment),Ye=l(),M(O.$$.fragment),Re=l(),K=c("p"),K.textContent=ht,Qe=l(),ee=c("p"),ee.innerHTML=Mt,Be=l(),M(te.$$.fragment),Ge=l(),M(se.$$.fragment),Ee=l(),ae=c("p"),ae.innerHTML=Jt,He=l(),le=c("p"),le.textContent=Tt,ze=l(),M(ne.$$.fragment),We=l(),re=c("p"),re.innerHTML=wt,De=l(),M(pe.$$.fragment),qe=l(),oe=c("p"),oe.textContent=jt,Le=l(),M(ce.$$.fragment),Se=l(),M(ie.$$.fragment),Pe=l(),me=c("p"),this.h()},l(e){const t=Nt("svelte-u9bgzb",document.head);p=i(t,"META",{name:!0,content:!0}),t.forEach(s),U=n(e),f=i(e,"P",{}),It(f).forEach(s),d=n(e),J(o.$$.fragment,e),h=n(e),m=i(e,"P",{"data-svelte-h":!0}),u(m)!=="svelte-11npljq"&&(m.textContent=x),r=n(e),I=i(e,"P",{"data-svelte-h":!0}),u(I)!=="svelte-1djhl8e"&&(I.innerHTML=Ke),de=n(e),_=i(e,"P",{"data-svelte-h":!0}),u(_)!=="svelte-t8kulv"&&(_.innerHTML=et),fe=n(e),J(A.$$.fragment,e),he=n(e),N=i(e,"P",{"data-svelte-h":!0}),u(N)!=="svelte-1cuyph"&&(N.innerHTML=tt),Me=n(e),v=i(e,"P",{"data-svelte-h":!0}),u(v)!=="svelte-xmmv9"&&(v.textContent=st),Je=n(e),J(C.$$.fragment,e),Te=n(e),Z=i(e,"P",{"data-svelte-h":!0}),u(Z)!=="svelte-13cu2d1"&&(Z.innerHTML=at),we=n(e),k=i(e,"UL",{"data-svelte-h":!0}),u(k)!=="svelte-11zcnej"&&(k.innerHTML=lt),je=n(e),V=i(e,"P",{"data-svelte-h":!0}),u(V)!=="svelte-sx3e8q"&&(V.innerHTML=nt),ye=n(e),J(g.$$.fragment,e),Ie=n(e),J(X.$$.fragment,e),Ue=n(e),F=i(e,"P",{"data-svelte-h":!0}),u(F)!=="svelte-f4il9r"&&(F.textContent=rt),xe=n(e),Y=i(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-1xo1c8p"&&(Y.innerHTML=pt),be=n(e),R=i(e,"P",{"data-svelte-h":!0}),u(R)!=="svelte-sizsez"&&(R.textContent=ot),$e=n(e),Q=i(e,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-81ch6r"&&(Q.textContent=ct),ge=n(e),B=i(e,"P",{"data-svelte-h":!0}),u(B)!=="svelte-xykknr"&&(B.innerHTML=it),_e=n(e),J(G.$$.fragment,e),Ae=n(e),E=i(e,"P",{"data-svelte-h":!0}),u(E)!=="svelte-10j4ocw"&&(E.innerHTML=ut),Ne=n(e),J(H.$$.fragment,e),ve=n(e),z=i(e,"P",{"data-svelte-h":!0}),u(z)!=="svelte-1tguczj"&&(z.innerHTML=mt),Ce=n(e),J(W.$$.fragment,e),Ze=n(e),J(D.$$.fragment,e),ke=n(e),q=i(e,"P",{"data-svelte-h":!0}),u(q)!=="svelte-1m4vi3k"&&(q.textContent=dt),Ve=n(e),J(L.$$.fragment,e),Xe=n(e),S=i(e,"P",{"data-svelte-h":!0}),u(S)!=="svelte-1iybb0r"&&(S.innerHTML=ft),Fe=n(e),J(P.$$.fragment,e),Ye=n(e),J(O.$$.fragment,e),Re=n(e),K=i(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-fkz01s"&&(K.textContent=ht),Qe=n(e),ee=i(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-sxi7vo"&&(ee.innerHTML=Mt),Be=n(e),J(te.$$.fragment,e),Ge=n(e),J(se.$$.fragment,e),Ee=n(e),ae=i(e,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-1tbpz6p"&&(ae.innerHTML=Jt),He=n(e),le=i(e,"P",{"data-svelte-h":!0}),u(le)!=="svelte-8axfxt"&&(le.textContent=Tt),ze=n(e),J(ne.$$.fragment,e),We=n(e),re=i(e,"P",{"data-svelte-h":!0}),u(re)!=="svelte-h6ab2w"&&(re.innerHTML=wt),De=n(e),J(pe.$$.fragment,e),qe=n(e),oe=i(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-57e76x"&&(oe.textContent=jt),Le=n(e),J(ce.$$.fragment,e),Se=n(e),J(ie.$$.fragment,e),Pe=n(e),me=i(e,"P",{}),It(me).forEach(s),this.h()},h(){Ut(p,"name","hf:doc:metadata"),Ut(p,"content",Yt)},m(e,t){vt(document.head,p),a(e,U,t),a(e,f,t),a(e,d,t),T(o,e,t),a(e,h,t),a(e,m,t),a(e,r,t),a(e,I,t),a(e,de,t),a(e,_,t),a(e,fe,t),T(A,e,t),a(e,he,t),a(e,N,t),a(e,Me,t),a(e,v,t),a(e,Je,t),T(C,e,t),a(e,Te,t),a(e,Z,t),a(e,we,t),a(e,k,t),a(e,je,t),a(e,V,t),a(e,ye,t),T(g,e,t),a(e,Ie,t),T(X,e,t),a(e,Ue,t),a(e,F,t),a(e,xe,t),a(e,Y,t),a(e,be,t),a(e,R,t),a(e,$e,t),a(e,Q,t),a(e,ge,t),a(e,B,t),a(e,_e,t),T(G,e,t),a(e,Ae,t),a(e,E,t),a(e,Ne,t),T(H,e,t),a(e,ve,t),a(e,z,t),a(e,Ce,t),T(W,e,t),a(e,Ze,t),T(D,e,t),a(e,ke,t),a(e,q,t),a(e,Ve,t),T(L,e,t),a(e,Xe,t),a(e,S,t),a(e,Fe,t),T(P,e,t),a(e,Ye,t),T(O,e,t),a(e,Re,t),a(e,K,t),a(e,Qe,t),a(e,ee,t),a(e,Be,t),T(te,e,t),a(e,Ge,t),T(se,e,t),a(e,Ee,t),a(e,ae,t),a(e,He,t),a(e,le,t),a(e,ze,t),T(ne,e,t),a(e,We,t),a(e,re,t),a(e,De,t),T(pe,e,t),a(e,qe,t),a(e,oe,t),a(e,Le,t),T(ce,e,t),a(e,Se,t),T(ie,e,t),a(e,Pe,t),a(e,me,t),Oe=!0},p(e,[t]){const yt={};t&2&&(yt.$$scope={dirty:t,ctx:e}),g.$set(yt)},i(e){Oe||(w(o.$$.fragment,e),w(A.$$.fragment,e),w(C.$$.fragment,e),w(g.$$.fragment,e),w(X.$$.fragment,e),w(G.$$.fragment,e),w(H.$$.fragment,e),w(W.$$.fragment,e),w(D.$$.fragment,e),w(L.$$.fragment,e),w(P.$$.fragment,e),w(O.$$.fragment,e),w(te.$$.fragment,e),w(se.$$.fragment,e),w(ne.$$.fragment,e),w(pe.$$.fragment,e),w(ce.$$.fragment,e),w(ie.$$.fragment,e),Oe=!0)},o(e){j(o.$$.fragment,e),j(A.$$.fragment,e),j(C.$$.fragment,e),j(g.$$.fragment,e),j(X.$$.fragment,e),j(G.$$.fragment,e),j(H.$$.fragment,e),j(W.$$.fragment,e),j(D.$$.fragment,e),j(L.$$.fragment,e),j(P.$$.fragment,e),j(O.$$.fragment,e),j(te.$$.fragment,e),j(se.$$.fragment,e),j(ne.$$.fragment,e),j(pe.$$.fragment,e),j(ce.$$.fragment,e),j(ie.$$.fragment,e),Oe=!1},d(e){e&&(s(U),s(f),s(d),s(h),s(m),s(r),s(I),s(de),s(_),s(fe),s(he),s(N),s(Me),s(v),s(Je),s(Te),s(Z),s(we),s(k),s(je),s(V),s(ye),s(Ie),s(Ue),s(F),s(xe),s(Y),s(be),s(R),s($e),s(Q),s(ge),s(B),s(_e),s(Ae),s(E),s(Ne),s(ve),s(z),s(Ce),s(Ze),s(ke),s(q),s(Ve),s(Xe),s(S),s(Fe),s(Ye),s(Re),s(K),s(Qe),s(ee),s(Be),s(Ge),s(Ee),s(ae),s(He),s(le),s(ze),s(We),s(re),s(De),s(qe),s(oe),s(Le),s(Se),s(Pe),s(me)),s(p),y(o,e),y(A,e),y(C,e),y(g,e),y(X,e),y(G,e),y(H,e),y(W,e),y(D,e),y(L,e),y(P,e),y(O,e),y(te,e),y(se,e),y(ne,e),y(pe,e),y(ce,e),y(ie,e)}}}const Yt='{"title":"Feature extractors","local":"feature-extractors","sections":[{"title":"Feature extractor classes","local":"feature-extractor-classes","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[{"title":"Padding","local":"padding","sections":[],"depth":3},{"title":"Truncation","local":"truncation","sections":[],"depth":3},{"title":"Resampling","local":"resampling","sections":[],"depth":3}],"depth":2}],"depth":1}';function Rt($){return gt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class zt extends _t{constructor(p){super(),At(this,p,Rt,Ft,$t,{})}}export{zt as component};
