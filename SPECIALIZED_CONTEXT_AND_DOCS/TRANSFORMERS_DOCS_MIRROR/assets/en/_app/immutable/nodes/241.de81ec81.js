import{s as Un,z as Zn,o as Pn,n as He}from"../chunks/scheduler.18a86fab.js";import{S as Nn,i as qn,g as d,s as n,r as p,A as Gn,h as l,f as o,c as r,j as V,x as v,u,k as T,y as s,a,v as f,d as h,t as g,w as _}from"../chunks/index.98837b22.js";import{T as en}from"../chunks/Tip.77304350.js";import{D as w}from"../chunks/Docstring.a1ef7999.js";import{C as ut}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as pt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as W,E as Rn}from"../chunks/getInferenceSnippets.06c2775f.js";function Ln(C){let i,I="Example:",m,b,y;return b=new ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBJbnN0cnVjdEJsaXBWaWRlb1Zpc2lvbkNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEluc3RydWN0QmxpcFZpZGVvUUZvcm1lckNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyME9QVENvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEluc3RydWN0QmxpcFZpZGVvQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwSW5zdHJ1Y3RCbGlwVmlkZW9Gb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEluc3RydWN0QmxpcFZpZGVvQ29uZmlnJTIwd2l0aCUyMFNhbGVzZm9yY2UlMkZpbnN0cnVjdC1ibGlwLWZsYW4tdDUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwSW5zdHJ1Y3RCbGlwVmlkZW9Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBJbnN0cnVjdEJsaXBWaWRlb0ZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwU2FsZXNmb3JjZSUyRmluc3RydWN0LWJsaXAtZmxhbi10NSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwSW5zdHJ1Y3RCbGlwVmlkZW9Gb3JDb25kaXRpb25hbEdlbmVyYXRpb24oY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBJbnN0cnVjdEJsaXBWaWRlb0NvbmZpZyUyMGZyb20lMjBhJTIwSW5zdHJ1Y3RCbGlwVmlkZW9WaXNpb25Db25maWclMkMlMjBJbnN0cnVjdEJsaXBWaWRlb1FGb3JtZXJDb25maWclMjBhbmQlMjBhbnklMjBQcmV0cmFpbmVkQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwSW5zdHJ1Y3RibGlwdmlkZW8lMjB2aXNpb24lMkMlMjBJbnN0cnVjdGJsaXB2aWRlbyUyMFEtRm9ybWVyJTIwYW5kJTIwbGFuZ3VhZ2UlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb25zJTBBdmlzaW9uX2NvbmZpZyUyMCUzRCUyMEluc3RydWN0QmxpcFZpZGVvVmlzaW9uQ29uZmlnKCklMEFxZm9ybWVyX2NvbmZpZyUyMCUzRCUyMEluc3RydWN0QmxpcFZpZGVvUUZvcm1lckNvbmZpZygpJTBBdGV4dF9jb25maWclMjAlM0QlMjBPUFRDb25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMEluc3RydWN0QmxpcFZpZGVvQ29uZmlnLmZyb21fdGV4dF92aXNpb25fY29uZmlncyh2aXNpb25fY29uZmlnJTJDJTIwcWZvcm1lcl9jb25maWclMkMlMjB0ZXh0X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    InstructBlipVideoVisionConfig,
<span class="hljs-meta">... </span>    InstructBlipVideoQFormerConfig,
<span class="hljs-meta">... </span>    OPTConfig,
<span class="hljs-meta">... </span>    InstructBlipVideoConfig,
<span class="hljs-meta">... </span>    InstructBlipVideoForConditionalGeneration,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a InstructBlipVideoConfig with Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = InstructBlipVideoConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a InstructBlipVideoForConditionalGeneration (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = InstructBlipVideoForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PretrainedConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing Instructblipvideo vision, Instructblipvideo Q-Former and language model configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = InstructBlipVideoVisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>qformer_config = InstructBlipVideoQFormerConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = OPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = InstructBlipVideoConfig.from_text_vision_configs(vision_config, qformer_config, text_config)`,wrap:!1}}),{c(){i=d("p"),i.textContent=I,m=n(),p(b.$$.fragment)},l(c){i=l(c,"P",{"data-svelte-h":!0}),v(i)!=="svelte-11lpom8"&&(i.textContent=I),m=r(c),u(b.$$.fragment,c)},m(c,M){a(c,i,M),a(c,m,M),f(b,c,M),y=!0},p:He,i(c){y||(h(b.$$.fragment,c),y=!0)},o(c){g(b.$$.fragment,c),y=!1},d(c){c&&(o(i),o(m)),_(b,c)}}}function Qn(C){let i,I="Example:",m,b,y;return b=new ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEluc3RydWN0QmxpcFZpZGVvVmlzaW9uQ29uZmlnJTJDJTIwSW5zdHJ1Y3RCbGlwVmlkZW9WaXNpb25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBJbnN0cnVjdEJsaXBWaWRlb1Zpc2lvbkNvbmZpZyUyMHdpdGglMjBTYWxlc2ZvcmNlJTJGaW5zdHJ1Y3QtYmxpcC1mbGFuLXQ1JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEluc3RydWN0QmxpcFZpZGVvVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwSW5zdHJ1Y3RCbGlwVmlkZW9WaXNpb25Nb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwU2FsZXNmb3JjZSUyRmluc3RydWN0LWJsaXAtZmxhbi10NSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwSW5zdHJ1Y3RCbGlwVmlkZW9WaXNpb25Nb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> InstructBlipVideoVisionConfig, InstructBlipVideoVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a InstructBlipVideoVisionConfig with Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = InstructBlipVideoVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a InstructBlipVideoVisionModel (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = InstructBlipVideoVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=d("p"),i.textContent=I,m=n(),p(b.$$.fragment)},l(c){i=l(c,"P",{"data-svelte-h":!0}),v(i)!=="svelte-11lpom8"&&(i.textContent=I),m=r(c),u(b.$$.fragment,c)},m(c,M){a(c,i,M),a(c,m,M),f(b,c,M),y=!0},p:He,i(c){y||(h(b.$$.fragment,c),y=!0)},o(c){g(b.$$.fragment,c),y=!1},d(c){c&&(o(i),o(m)),_(b,c)}}}function En(C){let i,I="Examples:",m,b,y;return b=new ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEluc3RydWN0QmxpcFZpZGVvUUZvcm1lckNvbmZpZyUyQyUyMEluc3RydWN0QmxpcFZpZGVvUUZvcm1lck1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEluc3RydWN0QmxpcFZpZGVvJTIwU2FsZXNmb3JjZSUyRmluc3RydWN0LWJsaXAtZmxhbi10NSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBJbnN0cnVjdEJsaXBWaWRlb1FGb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwU2FsZXNmb3JjZSUyRmluc3RydWN0LWJsaXAtZmxhbi10NSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwSW5zdHJ1Y3RCbGlwVmlkZW9RRm9ybWVyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> InstructBlipVideoQFormerConfig, InstructBlipVideoQFormerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a InstructBlipVideo Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = InstructBlipVideoQFormerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = InstructBlipVideoQFormerModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=d("p"),i.textContent=I,m=n(),p(b.$$.fragment)},l(c){i=l(c,"P",{"data-svelte-h":!0}),v(i)!=="svelte-kvfsh7"&&(i.textContent=I),m=r(c),u(b.$$.fragment,c)},m(c,M){a(c,i,M),a(c,m,M),f(b,c,M),y=!0},p:He,i(c){y||(h(b.$$.fragment,c),y=!0)},o(c){g(b.$$.fragment,c),y=!1},d(c){c&&(o(i),o(m)),_(b,c)}}}function Sn(C){let i,I=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=I},l(m){i=l(m,"P",{"data-svelte-h":!0}),v(i)!=="svelte-fincs2"&&(i.innerHTML=I)},m(m,b){a(m,i,b)},p:He,d(m){m&&o(i)}}}function An(C){let i,I=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=I},l(m){i=l(m,"P",{"data-svelte-h":!0}),v(i)!=="svelte-fincs2"&&(i.innerHTML=I)},m(m,b){a(m,i,b)},p:He,d(m){m&&o(i)}}}function Hn(C){let i,I=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=d("p"),i.innerHTML=I},l(m){i=l(m,"P",{"data-svelte-h":!0}),v(i)!=="svelte-fincs2"&&(i.innerHTML=I)},m(m,b){a(m,i,b)},p:He,d(m){m&&o(i)}}}function Xn(C){let i,I="Examples:",m,b,y;return b=new ut({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> av
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_video_pyav</span>(<span class="hljs-params">container, indices</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;&#x27;&#x27;
<span class="hljs-meta">... </span>    Decode the video with PyAV decoder.
<span class="hljs-meta">... </span>    Args:
<span class="hljs-meta">... </span>        container (\`av.container.input.InputContainer\`): PyAV container.
<span class="hljs-meta">... </span>        indices (\`list[int]\`): List of frame indices to decode.
<span class="hljs-meta">... </span>    Returns:
<span class="hljs-meta">... </span>        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
<span class="hljs-meta">... </span>    &#x27;&#x27;&#x27;</span>
<span class="hljs-meta">... </span>    frames = []
<span class="hljs-meta">... </span>    container.seek(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>    start_index = indices[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    end_index = indices[-<span class="hljs-number">1</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, frame <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(container.decode(video=<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt; end_index:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt;= start_index <span class="hljs-keyword">and</span> i <span class="hljs-keyword">in</span> indices:
<span class="hljs-meta">... </span>            frames.append(frame)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> np.stack([x.to_ndarray(<span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;rgb24&quot;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> frames])

<span class="hljs-meta">&gt;&gt;&gt; </span>model = InstructBlipVideoForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;Salesforce/instructblip-vicuna-7b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = InstructBlipVideoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/instructblip-vicuna-7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(
<span class="hljs-meta">... </span>      repo_id=<span class="hljs-string">&quot;nielsr/video-demo&quot;</span>, filename=<span class="hljs-string">&quot;eating_spaghetti.mp4&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>container = av.<span class="hljs-built_in">open</span>(file_path)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample uniformly 4 frames from the videWhy is this video funny?o</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>total_frames = container.streams.video[<span class="hljs-number">0</span>].frames
<span class="hljs-meta">&gt;&gt;&gt; </span>indices = np.arange(<span class="hljs-number">0</span>, total_frames, total_frames / <span class="hljs-number">4</span>).astype(<span class="hljs-built_in">int</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>clip = read_video_pyav(container, indices)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;What is happening in the video?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=prompt, images=clip, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    num_beams=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">256</span>,
<span class="hljs-meta">... </span>    repetition_penalty=<span class="hljs-number">1.5</span>,
<span class="hljs-meta">... </span>    length_penalty=<span class="hljs-number">1.0</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
<span class="hljs-string">&quot;A person is eating a bowl of pasta, and they are using a fork to eat it. The person is sitting at a table, and the plate of pasta is on the table in front&quot;</span>`,wrap:!1}}),{c(){i=d("p"),i.textContent=I,m=n(),p(b.$$.fragment)},l(c){i=l(c,"P",{"data-svelte-h":!0}),v(i)!=="svelte-kvfsh7"&&(i.textContent=I),m=r(c),u(b.$$.fragment,c)},m(c,M){a(c,i,M),a(c,m,M),f(b,c,M),y=!0},p:He,i(c){y||(h(b.$$.fragment,c),y=!0)},o(c){g(b.$$.fragment,c),y=!1},d(c){c&&(o(i),o(m)),_(b,c)}}}function Dn(C){let i,I,m,b,y,c="<em>This model was released on 2023-05-11 and added to Hugging Face Transformers on 2024-06-25.</em>",M,ie,xo,H,on='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ko,de,jo,le,tn=`The InstructBLIPVideo is an extension of the models proposed in <a href="https://huggingface.co/papers/2305.06500" rel="nofollow">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a> by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.
InstructBLIPVideo uses the same architecture as <a href="instructblip">InstructBLIP</a> and works with the same checkpoints as <a href="instructblip">InstructBLIP</a>. The only difference is the ability to process videos.`,zo,ce,nn="The abstract from the paper is the following:",Jo,me,rn="<em>General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.</em>",Wo,X,sn,Fo,pe,an='InstructBLIPVideo architecture. Taken from the <a href="https://huggingface.co/papers/2305.06500">original paper.</a>',Uo,ue,dn=`This model was contributed by <a href="https://huggingface.co/RaushanTurganbay" rel="nofollow">RaushanTurganbay</a>.
The original code can be found <a href="https://github.com/salesforce/LAVIS/tree/main/projects/instructblip" rel="nofollow">here</a>.`,Zo,fe,Po,he,ln="<li>The model was trained by sampling 4 frames per video, so it’s recommended to sample 4 frames</li>",No,ge,cn=`<p>[!NOTE]
BLIP models after release v4.46 will raise warnings about adding <code>processor.num_query_tokens = {{num_query_tokens}}</code> and expand model embeddings layer to add special <code>&lt;image&gt;</code> token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many <code>&lt;image&gt;</code> placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.
The attributes can be obtained from model config, as <code>model.config.num_query_tokens</code> and model embeddings expansion can be done by following <a href="https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042" rel="nofollow">this link</a>.</p>`,qo,_e,Go,$,be,ft,Xe,mn=`<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a> is the configuration class to store the configuration of a
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoForConditionalGeneration">InstructBlipVideoForConditionalGeneration</a>. It is used to instantiate a Instructblipvideo model according to the specified
arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with
the defaults will yield a similar configuration to that of the Instructblipvideo
<a href="https://huggingface.co/Salesforce/instruct-blip-flan-t5" rel="nofollow">Salesforce/instruct-blip-flan-t5</a> architecture.`,ht,De,pn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,gt,D,_t,Y,ve,bt,Ye,un=`Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a> (or a derived class) from a InstructBlipVideo vision model, Q-Former and
language model configurations.`,Ro,ye,Lo,j,Ie,vt,Oe,fn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVisionModel">InstructBlipVideoVisionModel</a>. It is used to
instantiate a InstructBlipVideo vision encoder according to the specified arguments, defining the model architecture.
Instantiating a configuration defaults will yield a similar configuration to that of the InstructBlipVideo
<a href="https://huggingface.co/Salesforce/instruct-blip-flan-t5" rel="nofollow">Salesforce/instruct-blip-flan-t5</a> architecture.`,yt,Ke,hn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,It,O,Qo,Te,Eo,z,Me,Tt,eo,gn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoQFormerModel">InstructBlipVideoQFormerModel</a>. It is used to
instantiate a InstructBlipVideo Querying Transformer (Q-Former) model according to the specified arguments, defining the
model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the InstructBlipVideo <a href="https://huggingface.co/Salesforce/instruct-blip-flan-t5" rel="nofollow">Salesforce/instruct-blip-flan-t5</a>
architecture. Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs.
Read the documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Mt,oo,_n='Note that <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoQFormerModel">InstructBlipVideoQFormerModel</a> is very similar to <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> with interleaved cross-attention.',Vt,K,So,Ve,Ao,U,Be,Bt,to,bn=`Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single
processor.`,wt,no,vn=`<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See the
docstring of <code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,Ho,we,Xo,E,Ce,Ct,ro,$e,Do,xe,Yo,Z,ke,$t,so,yn="Constructs a InstructBLIPVideo image processor.",xt,ee,je,kt,ao,In="Preprocess a video or batch of images/videos.",Oo,ze,Ko,S,Je,jt,G,We,zt,io,Tn='The <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVisionModel">InstructBlipVideoVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Jt,oe,et,Fe,ot,P,Ue,Wt,lo,Mn=`Querying Transformer (Q-Former), used in InstructBlipVideo. Slightly modified from BLIP-2 as it also takes the
instruction as input.`,Ft,R,Ze,Ut,co,Vn=`encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>):
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.
encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,Zt,mo,Bn=`<li>1 for tokens that are <strong>not masked</strong>,</li> <li>0 for tokens that are <strong>masked</strong>.
past_key_values (<code>Cache</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of:
shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>): Contains precomputed key and
value hidden states of the attention blocks. Can be used to speed up decoding. If <code>past_key_values</code> are
used, the user can optionally input only the last <code>decoder_input_ids</code> (those that don’t have their past key
value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>decoder_input_ids</code> of shape
<code>(batch_size, sequence_length)</code>.
use_cache (<code>bool</code>, <em>optional</em>):
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</li>`,tt,Pe,nt,x,Ne,Pt,po,wn="InstructBlipVideo base Model consisting of language model, qformer and vision encoder.",Nt,uo,Cn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qt,fo,$n=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Gt,L,qe,Rt,ho,xn='The <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoModel">InstructBlipVideoModel</a> forward method, overrides the <code>__call__</code> special method.',Lt,te,rt,Ge,st,B,Re,Qt,go,kn=`InstructBlipVideo Model for generating text given an image and an optional text prompt. The model consists of a vision
encoder, Querying Transformer (Q-Former) and a language model.`,Et,_o,jn=`One can optionally pass <code>input_ids</code> to the model, which serve as a text prompt, to make the language model continue
the prompt. Otherwise, the language model starts generating text from the [BOS] (beginning-of-sequence) token.`,St,bo,zn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,At,vo,Jn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ht,F,Le,Xt,yo,Wn='The <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoForConditionalGeneration">InstructBlipVideoForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Dt,ne,Yt,re,Ot,se,Qe,Kt,Io,Fn="Overrides <code>generate</code> function to be able to use the model as a conditional generator.",at,Ee,it,Co,dt;return ie=new W({props:{title:"InstructBlipVideo",local:"instructblipvideo",headingTag:"h1"}}),de=new W({props:{title:"Overview",local:"overview",headingTag:"h2"}}),fe=new W({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),_e=new W({props:{title:"InstructBlipVideoConfig",local:"transformers.InstructBlipVideoConfig",headingTag:"h2"}}),be=new w({props:{name:"class transformers.InstructBlipVideoConfig",anchor:"transformers.InstructBlipVideoConfig",parameters:[{name:"vision_config",val:" = None"},{name:"qformer_config",val:" = None"},{name:"text_config",val:" = None"},{name:"num_query_tokens",val:" = 32"},{name:"video_token_index",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVisionConfig">InstructBlipVideoVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.InstructBlipVideoConfig.qformer_config",description:`<strong>qformer_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoQFormerConfig">InstructBlipVideoQFormerConfig</a>.`,name:"qformer_config"},{anchor:"transformers.InstructBlipVideoConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize any <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>.`,name:"text_config"},{anchor:"transformers.InstructBlipVideoConfig.num_query_tokens",description:`<strong>num_query_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of query tokens passed through the Transformer.`,name:"num_query_tokens"},{anchor:"transformers.InstructBlipVideoConfig.video_token_index",description:`<strong>video_token_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Token index of special video token.`,name:"video_token_index"},{anchor:"transformers.InstructBlipVideoConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/configuration_instructblipvideo.py#L220"}}),D=new pt({props:{anchor:"transformers.InstructBlipVideoConfig.example",$$slots:{default:[Ln]},$$scope:{ctx:C}}}),ve=new w({props:{name:"from_vision_qformer_text_configs",anchor:"transformers.InstructBlipVideoConfig.from_vision_qformer_text_configs",parameters:[{name:"vision_config",val:": InstructBlipVideoVisionConfig"},{name:"qformer_config",val:": InstructBlipVideoQFormerConfig"},{name:"text_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/configuration_instructblipvideo.py#L321",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig"
>InstructBlipVideoConfig</a></p>
`}}),ye=new W({props:{title:"InstructBlipVideoVisionConfig",local:"transformers.InstructBlipVideoVisionConfig",headingTag:"h2"}}),Ie=new w({props:{name:"class transformers.InstructBlipVideoVisionConfig",anchor:"transformers.InstructBlipVideoVisionConfig",parameters:[{name:"hidden_size",val:" = 1408"},{name:"intermediate_size",val:" = 6144"},{name:"num_hidden_layers",val:" = 39"},{name:"num_attention_heads",val:" = 16"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 14"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 1e-10"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.InstructBlipVideoVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 6144) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.InstructBlipVideoVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 39) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.InstructBlipVideoVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.InstructBlipVideoVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.InstructBlipVideoVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.InstructBlipVideoVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;gelu&quot;</code> are supported. to 1e-5): The epsilon used by the layer
normalization layers.`,name:"hidden_act"},{anchor:"transformers.InstructBlipVideoVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.InstructBlipVideoVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.InstructBlipVideoVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-10) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.InstructBlipVideoVisionConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries and values in the self-attention layers.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/configuration_instructblipvideo.py#L32"}}),O=new pt({props:{anchor:"transformers.InstructBlipVideoVisionConfig.example",$$slots:{default:[Qn]},$$scope:{ctx:C}}}),Te=new W({props:{title:"InstructBlipVideoQFormerConfig",local:"transformers.InstructBlipVideoQFormerConfig",headingTag:"h2"}}),Me=new w({props:{name:"class transformers.InstructBlipVideoQFormerConfig",anchor:"transformers.InstructBlipVideoQFormerConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"cross_attention_frequency",val:" = 2"},{name:"encoder_hidden_size",val:" = 1408"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoQFormerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Q-Former model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling the model.`,name:"vocab_size"},{anchor:"transformers.InstructBlipVideoQFormerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.InstructBlipVideoQFormerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.InstructBlipVideoQFormerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.InstructBlipVideoQFormerConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.InstructBlipVideoQFormerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.InstructBlipVideoQFormerConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.InstructBlipVideoQFormerConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.InstructBlipVideoQFormerConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.InstructBlipVideoQFormerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.InstructBlipVideoQFormerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.InstructBlipVideoQFormerConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Token id used for padding sequences.`,name:"pad_token_id"},{anchor:"transformers.InstructBlipVideoQFormerConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.InstructBlipVideoQFormerConfig.cross_attention_frequency",description:`<strong>cross_attention_frequency</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The frequency of adding cross-attention to the Transformer layers.`,name:"cross_attention_frequency"},{anchor:"transformers.InstructBlipVideoQFormerConfig.encoder_hidden_size",description:`<strong>encoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
The hidden size of the hidden states for cross-attention.`,name:"encoder_hidden_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/configuration_instructblipvideo.py#L116"}}),K=new pt({props:{anchor:"transformers.InstructBlipVideoQFormerConfig.example",$$slots:{default:[En]},$$scope:{ctx:C}}}),Ve=new W({props:{title:"InstructBlipVideoProcessor",local:"transformers.InstructBlipVideoProcessor",headingTag:"h2"}}),Be=new w({props:{name:"class transformers.InstructBlipVideoProcessor",anchor:"transformers.InstructBlipVideoProcessor",parameters:[{name:"video_processor",val:""},{name:"tokenizer",val:""},{name:"qformer_tokenizer",val:""},{name:"num_query_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoProcessor.video_processor",description:`<strong>video_processor</strong> (<code>InstructBlipVideoVideoProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoVideoProcessor">InstructBlipVideoVideoProcessor</a>. The video processor is a required input.`,name:"video_processor"},{anchor:"transformers.InstructBlipVideoProcessor.tokenizer",description:"<strong>tokenizer</strong> (<code>AutoTokenizer</code>) &#x2014;\nAn instance of [&#x2018;PreTrainedTokenizer`]. The tokenizer is a required input.",name:"tokenizer"},{anchor:"transformers.InstructBlipVideoProcessor.qformer_tokenizer",description:"<strong>qformer_tokenizer</strong> (<code>AutoTokenizer</code>) &#x2014;\nAn instance of [&#x2018;PreTrainedTokenizer`]. The Q-Former tokenizer is a required input.",name:"qformer_tokenizer"},{anchor:"transformers.InstructBlipVideoProcessor.num_query_tokens",description:`<strong>num_query_tokens</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of tokens used by the Qformer as queries, should be same as in model&#x2019;s config.`,name:"num_query_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/processing_instructblipvideo.py#L39"}}),we=new W({props:{title:"InstructBlipVideoVideoProcessor",local:"transformers.InstructBlipVideoVideoProcessor",headingTag:"h2"}}),Ce=new w({props:{name:"class transformers.InstructBlipVideoVideoProcessor",anchor:"transformers.InstructBlipVideoVideoProcessor",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.instructblipvideo.video_processing_instructblipvideo.InstructBlipVideoVideoProcessorInitKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py#L59"}}),$e=new w({props:{name:"preprocess",anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess",parameters:[{name:"videos",val:": typing.Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]]]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.processing_utils.VideosKwargs]"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the video&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output video after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.size_divisor</code>) &#x2014;
The size by which to make sure both the height and width can be divided.`,name:"size_divisor"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.default_to_square</code>) &#x2014;
Whether to default to a square video when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the video. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the video to the specified <code>crop_size</code>. Can be overridden by <code>do_center_crop</code> in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to pad the video to the <code>(max_height, max_width)</code> of the videos in the batch.`,name:"do_pad"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the output video after applying <code>center_crop</code>. Can be overridden by <code>crop_size</code> in the <code>preprocess</code>
method.`,name:"crop_size"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the video by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Scale factor to use if rescaling the video. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the video. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Mean to use if normalizing the video. This is a float or list of floats the length of the number of
channels in the video. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Standard deviation to use if normalizing the video. This is a float or list of floats the length of the
number of channels in the video. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Whether to convert the video to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.video_metadata",description:`<strong>video_metadata</strong> (<code>VideoMetadata</code>, <em>optional</em>) &#x2014;
Metadata of the video containing information about total duration, fps and total number of frames.`,name:"video_metadata"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.do_sample_frames",description:`<strong>do_sample_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.do_sample_frames</code>) &#x2014;
Whether to sample frames from the video before processing or to process the whole video.`,name:"do_sample_frames"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.num_frames</code>) &#x2014;
Maximum number of frames to sample when <code>do_sample_frames=True</code>.`,name:"num_frames"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.fps",description:`<strong>fps</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.fps</code>) &#x2014;
Target frames to sample per second when <code>do_sample_frames=True</code>.`,name:"fps"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input video.</li>
</ul>`,name:"data_format"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input video. If unset, the channel dimension format is inferred
from the input video. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: video in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: video in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: video in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the videos on. If unset, the device is inferred from the input videos.`,name:"device"},{anchor:"transformers.InstructBlipVideoVideoProcessor.preprocess.return_metadata",description:`<strong>return_metadata</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return video metadata or not.`,name:"return_metadata"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/video_processing_utils.py#L355"}}),xe=new W({props:{title:"InstructBlipVideoImageProcessor",local:"transformers.InstructBlipVideoImageProcessor",headingTag:"h2"}}),ke=new w({props:{name:"class transformers.InstructBlipVideoImageProcessor",anchor:"transformers.InstructBlipVideoImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"do_convert_rgb",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.InstructBlipVideoImageProcessor.size",description:`<strong>size</strong> (<code>dict</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 384, &quot;width&quot;: 384}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.InstructBlipVideoImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.InstructBlipVideoImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the
<code>do_rescale</code> parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.InstructBlipVideoImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.InstructBlipVideoImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.InstructBlipVideoImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.InstructBlipVideoImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.InstructBlipVideoImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py#L47"}}),je=new w({props:{name:"preprocess",anchor:"transformers.InstructBlipVideoImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[list['PIL.Image.Image'], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), list['np.ndarray'], list['torch.Tensor'], list[list['PIL.Image.Image']], list[list['np.ndarrray']], list[list['torch.Tensor']], transformers.video_utils.URL, list[transformers.video_utils.URL], list[list[transformers.video_utils.URL]], transformers.video_utils.Path, list[transformers.video_utils.Path], list[list[transformers.video_utils.Path]]] = None"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"do_convert_rgb",val:": typing.Optional[bool] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.videos",description:`<strong>videos</strong> (<code>VideoInput</code>) &#x2014;
Video frames to preprocess. Expects a single or batch of videos as a list of frames with pixel values
ranging from 0 to 255. If passing in video with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"videos"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the video.`,name:"do_resize"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Controls the size of the video after <code>resize</code>. The shortest edge of the image is resized to
<code>size[&quot;shortest_edge&quot;]</code> whilst preserving the aspect ratio. If the longest edge of this resized image
is &gt; <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>, then the image is resized again to make the longest
edge equal to <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>.`,name:"size"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the video. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the video values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the video by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the video.`,name:"do_normalize"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to normalize the video by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to normalize the video by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.InstructBlipVideoImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py#L162"}}),ze=new W({props:{title:"InstructBlipVideoVisionModel",local:"transformers.InstructBlipVideoVisionModel",headingTag:"h2"}}),Je=new w({props:{name:"class transformers.InstructBlipVideoVisionModel",anchor:"transformers.InstructBlipVideoVisionModel",parameters:[{name:"config",val:": InstructBlipVideoVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L417"}}),We=new w({props:{name:"forward",anchor:"transformers.InstructBlipVideoVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">InstructBlipVideoImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.InstructBlipVideoVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.InstructBlipVideoVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.InstructBlipVideoVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.InstructBlipVideoVisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L432",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig"
>InstructBlipVideoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new en({props:{$$slots:{default:[Sn]},$$scope:{ctx:C}}}),Fe=new W({props:{title:"InstructBlipVideoQFormerModel",local:"transformers.InstructBlipVideoQFormerModel",headingTag:"h2"}}),Ue=new w({props:{name:"class transformers.InstructBlipVideoQFormerModel",anchor:"transformers.InstructBlipVideoQFormerModel",parameters:[{name:"config",val:": InstructBlipVideoQFormerConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L909"}}),Ze=new w({props:{name:"forward",anchor:"transformers.InstructBlipVideoQFormerModel.forward",parameters:[{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"query_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L987"}}),Pe=new W({props:{title:"InstructBlipVideoModel",local:"transformers.InstructBlipVideoModel",headingTag:"h2"}}),Ne=new w({props:{name:"class transformers.InstructBlipVideoModel",anchor:"transformers.InstructBlipVideoModel",parameters:[{name:"config",val:": InstructBlipVideoConfig"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L1140"}}),qe=new w({props:{name:"forward",anchor:"transformers.InstructBlipVideoModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"qformer_input_ids",val:": FloatTensor"},{name:"qformer_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs]"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">InstructBlipVideoImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.InstructBlipVideoModel.forward.qformer_input_ids",description:`<strong>qformer_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided
to serve as text prompt, which the Q-Former model will encode.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a>. See <code>InstructBlipVideoProcessor.__call__()</code> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"qformer_input_ids"},{anchor:"transformers.InstructBlipVideoModel.forward.qformer_attention_mask",description:`<strong>qformer_attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"qformer_attention_mask"},{anchor:"transformers.InstructBlipVideoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a>. See <code>InstructBlipVideoProcessor.__call__()</code> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.InstructBlipVideoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.InstructBlipVideoModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.InstructBlipVideoModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.InstructBlipVideoModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.InstructBlipVideoModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.InstructBlipVideoModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.InstructBlipVideoModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.InstructBlipVideoModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.InstructBlipVideoModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L1209",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.instructblipvideo.modeling_instructblipvideo.InstructBlipVideoForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig"
>InstructBlipVideoConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) — Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, defaults to <code>None</code>) — Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, defaults to <code>None</code>) — Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) — Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.instructblipvideo.modeling_instructblipvideo.InstructBlipVideoForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),te=new en({props:{$$slots:{default:[An]},$$scope:{ctx:C}}}),Ge=new W({props:{title:"InstructBlipVideoForConditionalGeneration",local:"transformers.InstructBlipVideoForConditionalGeneration",headingTag:"h2"}}),Re=new w({props:{name:"class transformers.InstructBlipVideoForConditionalGeneration",anchor:"transformers.InstructBlipVideoForConditionalGeneration",parameters:[{name:"config",val:": InstructBlipVideoConfig"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig">InstructBlipVideoConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L1358"}}),Le=new w({props:{name:"forward",anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"qformer_input_ids",val:": FloatTensor"},{name:"qformer_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">InstructBlipVideoImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoProcessor">InstructBlipVideoProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoImageProcessor">InstructBlipVideoImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.qformer_input_ids",description:`<strong>qformer_input_ids</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length)) &#x2014;
The sequence used as a prompt to be fed to the Q-Former module.`,name:"qformer_input_ids"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.qformer_attention_mask",description:`<strong>qformer_attention_mask</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices.`,name:"qformer_attention_mask"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to
make sure the model can only look at previous inputs in order to predict the future.`,name:"decoder_attention_mask"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L1466",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.instructblipvideo.modeling_instructblipvideo.InstructBlipVideoForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/instructblipvideo#transformers.InstructBlipVideoConfig"
>InstructBlipVideoConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) — Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, defaults to <code>None</code>) — Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>tuple[torch.FloatTensor]</code>, <em>optional</em>, defaults to <code>None</code>) — Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) — Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.instructblipvideo.modeling_instructblipvideo.InstructBlipVideoForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ne=new en({props:{$$slots:{default:[Hn]},$$scope:{ctx:C}}}),re=new pt({props:{anchor:"transformers.InstructBlipVideoForConditionalGeneration.forward.example",$$slots:{default:[Xn]},$$scope:{ctx:C}}}),Qe=new w({props:{name:"generate",anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"qformer_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"qformer_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"**generate_kwargs",val:""}],parametersDescription:[{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape (batch_size, num_channels, height, width) or &#x2014;
(batch_size, num_frames, num_channels, height, width)): Input images or videos to be processed.`,name:"pixel_values"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.qformer_input_ids",description:`<strong>qformer_input_ids</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
The sequence used as a prompt to be fed to the Q-Former module.`,name:"qformer_input_ids"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.qformer_attention_mask",description:`<strong>qformer_attention_mask</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices.`,name:"qformer_attention_mask"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices.`,name:"attention_mask"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Embedded representation of the inputs. Should be float, not int tokens.`,name:"inputs_embeds"},{anchor:"transformers.InstructBlipVideoForConditionalGeneration.generate.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the positional encoding of the image embeddings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py#L1612",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of strings of length batch_size * num_captions.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>captions (list)</p>
`}}),Ee=new Rn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/instructblipvideo.md"}}),{c(){i=d("meta"),I=n(),m=d("p"),b=n(),y=d("p"),y.innerHTML=c,M=n(),p(ie.$$.fragment),xo=n(),H=d("div"),H.innerHTML=on,ko=n(),p(de.$$.fragment),jo=n(),le=d("p"),le.innerHTML=tn,zo=n(),ce=d("p"),ce.textContent=nn,Jo=n(),me=d("p"),me.innerHTML=rn,Wo=n(),X=d("img"),Fo=n(),pe=d("small"),pe.innerHTML=an,Uo=n(),ue=d("p"),ue.innerHTML=dn,Zo=n(),p(fe.$$.fragment),Po=n(),he=d("ul"),he.innerHTML=ln,No=n(),ge=d("blockquote"),ge.innerHTML=cn,qo=n(),p(_e.$$.fragment),Go=n(),$=d("div"),p(be.$$.fragment),ft=n(),Xe=d("p"),Xe.innerHTML=mn,ht=n(),De=d("p"),De.innerHTML=pn,gt=n(),p(D.$$.fragment),_t=n(),Y=d("div"),p(ve.$$.fragment),bt=n(),Ye=d("p"),Ye.innerHTML=un,Ro=n(),p(ye.$$.fragment),Lo=n(),j=d("div"),p(Ie.$$.fragment),vt=n(),Oe=d("p"),Oe.innerHTML=fn,yt=n(),Ke=d("p"),Ke.innerHTML=hn,It=n(),p(O.$$.fragment),Qo=n(),p(Te.$$.fragment),Eo=n(),z=d("div"),p(Me.$$.fragment),Tt=n(),eo=d("p"),eo.innerHTML=gn,Mt=n(),oo=d("p"),oo.innerHTML=_n,Vt=n(),p(K.$$.fragment),So=n(),p(Ve.$$.fragment),Ao=n(),U=d("div"),p(Be.$$.fragment),Bt=n(),to=d("p"),to.textContent=bn,wt=n(),no=d("p"),no.innerHTML=vn,Ho=n(),p(we.$$.fragment),Xo=n(),E=d("div"),p(Ce.$$.fragment),Ct=n(),ro=d("div"),p($e.$$.fragment),Do=n(),p(xe.$$.fragment),Yo=n(),Z=d("div"),p(ke.$$.fragment),$t=n(),so=d("p"),so.textContent=yn,xt=n(),ee=d("div"),p(je.$$.fragment),kt=n(),ao=d("p"),ao.textContent=In,Oo=n(),p(ze.$$.fragment),Ko=n(),S=d("div"),p(Je.$$.fragment),jt=n(),G=d("div"),p(We.$$.fragment),zt=n(),io=d("p"),io.innerHTML=Tn,Jt=n(),p(oe.$$.fragment),et=n(),p(Fe.$$.fragment),ot=n(),P=d("div"),p(Ue.$$.fragment),Wt=n(),lo=d("p"),lo.textContent=Mn,Ft=n(),R=d("div"),p(Ze.$$.fragment),Ut=n(),co=d("p"),co.innerHTML=Vn,Zt=n(),mo=d("ul"),mo.innerHTML=Bn,tt=n(),p(Pe.$$.fragment),nt=n(),x=d("div"),p(Ne.$$.fragment),Pt=n(),po=d("p"),po.textContent=wn,Nt=n(),uo=d("p"),uo.innerHTML=Cn,qt=n(),fo=d("p"),fo.innerHTML=$n,Gt=n(),L=d("div"),p(qe.$$.fragment),Rt=n(),ho=d("p"),ho.innerHTML=xn,Lt=n(),p(te.$$.fragment),rt=n(),p(Ge.$$.fragment),st=n(),B=d("div"),p(Re.$$.fragment),Qt=n(),go=d("p"),go.textContent=kn,Et=n(),_o=d("p"),_o.innerHTML=jn,St=n(),bo=d("p"),bo.innerHTML=zn,At=n(),vo=d("p"),vo.innerHTML=Jn,Ht=n(),F=d("div"),p(Le.$$.fragment),Xt=n(),yo=d("p"),yo.innerHTML=Wn,Dt=n(),p(ne.$$.fragment),Yt=n(),p(re.$$.fragment),Ot=n(),se=d("div"),p(Qe.$$.fragment),Kt=n(),Io=d("p"),Io.innerHTML=Fn,at=n(),p(Ee.$$.fragment),it=n(),Co=d("p"),this.h()},l(e){const t=Gn("svelte-u9bgzb",document.head);i=l(t,"META",{name:!0,content:!0}),t.forEach(o),I=r(e),m=l(e,"P",{}),V(m).forEach(o),b=r(e),y=l(e,"P",{"data-svelte-h":!0}),v(y)!=="svelte-n7suft"&&(y.innerHTML=c),M=r(e),u(ie.$$.fragment,e),xo=r(e),H=l(e,"DIV",{class:!0,"data-svelte-h":!0}),v(H)!=="svelte-13t8s2t"&&(H.innerHTML=on),ko=r(e),u(de.$$.fragment,e),jo=r(e),le=l(e,"P",{"data-svelte-h":!0}),v(le)!=="svelte-1kd91i3"&&(le.innerHTML=tn),zo=r(e),ce=l(e,"P",{"data-svelte-h":!0}),v(ce)!=="svelte-vfdo9a"&&(ce.textContent=nn),Jo=r(e),me=l(e,"P",{"data-svelte-h":!0}),v(me)!=="svelte-g3w4hv"&&(me.innerHTML=rn),Wo=r(e),X=l(e,"IMG",{src:!0,alt:!0,width:!0}),Fo=r(e),pe=l(e,"SMALL",{"data-svelte-h":!0}),v(pe)!=="svelte-vu7sya"&&(pe.innerHTML=an),Uo=r(e),ue=l(e,"P",{"data-svelte-h":!0}),v(ue)!=="svelte-1plrd7p"&&(ue.innerHTML=dn),Zo=r(e),u(fe.$$.fragment,e),Po=r(e),he=l(e,"UL",{"data-svelte-h":!0}),v(he)!=="svelte-l3xryg"&&(he.innerHTML=ln),No=r(e),ge=l(e,"BLOCKQUOTE",{"data-svelte-h":!0}),v(ge)!=="svelte-8y5dv0"&&(ge.innerHTML=cn),qo=r(e),u(_e.$$.fragment,e),Go=r(e),$=l(e,"DIV",{class:!0});var J=V($);u(be.$$.fragment,J),ft=r(J),Xe=l(J,"P",{"data-svelte-h":!0}),v(Xe)!=="svelte-fkhg68"&&(Xe.innerHTML=mn),ht=r(J),De=l(J,"P",{"data-svelte-h":!0}),v(De)!=="svelte-1ek1ss9"&&(De.innerHTML=pn),gt=r(J),u(D.$$.fragment,J),_t=r(J),Y=l(J,"DIV",{class:!0});var Se=V(Y);u(ve.$$.fragment,Se),bt=r(Se),Ye=l(Se,"P",{"data-svelte-h":!0}),v(Ye)!=="svelte-fi4pgu"&&(Ye.innerHTML=un),Se.forEach(o),J.forEach(o),Ro=r(e),u(ye.$$.fragment,e),Lo=r(e),j=l(e,"DIV",{class:!0});var N=V(j);u(Ie.$$.fragment,N),vt=r(N),Oe=l(N,"P",{"data-svelte-h":!0}),v(Oe)!=="svelte-49233"&&(Oe.innerHTML=fn),yt=r(N),Ke=l(N,"P",{"data-svelte-h":!0}),v(Ke)!=="svelte-1ek1ss9"&&(Ke.innerHTML=hn),It=r(N),u(O.$$.fragment,N),N.forEach(o),Qo=r(e),u(Te.$$.fragment,e),Eo=r(e),z=l(e,"DIV",{class:!0});var q=V(z);u(Me.$$.fragment,q),Tt=r(q),eo=l(q,"P",{"data-svelte-h":!0}),v(eo)!=="svelte-kcn3mt"&&(eo.innerHTML=gn),Mt=r(q),oo=l(q,"P",{"data-svelte-h":!0}),v(oo)!=="svelte-1i6neo0"&&(oo.innerHTML=_n),Vt=r(q),u(K.$$.fragment,q),q.forEach(o),So=r(e),u(Ve.$$.fragment,e),Ao=r(e),U=l(e,"DIV",{class:!0});var A=V(U);u(Be.$$.fragment,A),Bt=r(A),to=l(A,"P",{"data-svelte-h":!0}),v(to)!=="svelte-vkjqyr"&&(to.textContent=bn),wt=r(A),no=l(A,"P",{"data-svelte-h":!0}),v(no)!=="svelte-1szdokh"&&(no.innerHTML=vn),A.forEach(o),Ho=r(e),u(we.$$.fragment,e),Xo=r(e),E=l(e,"DIV",{class:!0});var Ae=V(E);u(Ce.$$.fragment,Ae),Ct=r(Ae),ro=l(Ae,"DIV",{class:!0});var $o=V(ro);u($e.$$.fragment,$o),$o.forEach(o),Ae.forEach(o),Do=r(e),u(xe.$$.fragment,e),Yo=r(e),Z=l(e,"DIV",{class:!0});var To=V(Z);u(ke.$$.fragment,To),$t=r(To),so=l(To,"P",{"data-svelte-h":!0}),v(so)!=="svelte-7qjslq"&&(so.textContent=yn),xt=r(To),ee=l(To,"DIV",{class:!0});var lt=V(ee);u(je.$$.fragment,lt),kt=r(lt),ao=l(lt,"P",{"data-svelte-h":!0}),v(ao)!=="svelte-llto5r"&&(ao.textContent=In),lt.forEach(o),To.forEach(o),Oo=r(e),u(ze.$$.fragment,e),Ko=r(e),S=l(e,"DIV",{class:!0});var ct=V(S);u(Je.$$.fragment,ct),jt=r(ct),G=l(ct,"DIV",{class:!0});var Mo=V(G);u(We.$$.fragment,Mo),zt=r(Mo),io=l(Mo,"P",{"data-svelte-h":!0}),v(io)!=="svelte-f89f2l"&&(io.innerHTML=Tn),Jt=r(Mo),u(oe.$$.fragment,Mo),Mo.forEach(o),ct.forEach(o),et=r(e),u(Fe.$$.fragment,e),ot=r(e),P=l(e,"DIV",{class:!0});var Vo=V(P);u(Ue.$$.fragment,Vo),Wt=r(Vo),lo=l(Vo,"P",{"data-svelte-h":!0}),v(lo)!=="svelte-cox9zr"&&(lo.textContent=Mn),Ft=r(Vo),R=l(Vo,"DIV",{class:!0});var Bo=V(R);u(Ze.$$.fragment,Bo),Ut=r(Bo),co=l(Bo,"P",{"data-svelte-h":!0}),v(co)!=="svelte-1h74cdd"&&(co.innerHTML=Vn),Zt=r(Bo),mo=l(Bo,"UL",{"data-svelte-h":!0}),v(mo)!=="svelte-1q82yv1"&&(mo.innerHTML=Bn),Bo.forEach(o),Vo.forEach(o),tt=r(e),u(Pe.$$.fragment,e),nt=r(e),x=l(e,"DIV",{class:!0});var Q=V(x);u(Ne.$$.fragment,Q),Pt=r(Q),po=l(Q,"P",{"data-svelte-h":!0}),v(po)!=="svelte-1cmlk1y"&&(po.textContent=wn),Nt=r(Q),uo=l(Q,"P",{"data-svelte-h":!0}),v(uo)!=="svelte-q52n56"&&(uo.innerHTML=Cn),qt=r(Q),fo=l(Q,"P",{"data-svelte-h":!0}),v(fo)!=="svelte-hswkmf"&&(fo.innerHTML=$n),Gt=r(Q),L=l(Q,"DIV",{class:!0});var wo=V(L);u(qe.$$.fragment,wo),Rt=r(wo),ho=l(wo,"P",{"data-svelte-h":!0}),v(ho)!=="svelte-6cd3mp"&&(ho.innerHTML=xn),Lt=r(wo),u(te.$$.fragment,wo),wo.forEach(o),Q.forEach(o),rt=r(e),u(Ge.$$.fragment,e),st=r(e),B=l(e,"DIV",{class:!0});var k=V(B);u(Re.$$.fragment,k),Qt=r(k),go=l(k,"P",{"data-svelte-h":!0}),v(go)!=="svelte-74lplt"&&(go.textContent=kn),Et=r(k),_o=l(k,"P",{"data-svelte-h":!0}),v(_o)!=="svelte-1ks26sg"&&(_o.innerHTML=jn),St=r(k),bo=l(k,"P",{"data-svelte-h":!0}),v(bo)!=="svelte-q52n56"&&(bo.innerHTML=zn),At=r(k),vo=l(k,"P",{"data-svelte-h":!0}),v(vo)!=="svelte-hswkmf"&&(vo.innerHTML=Jn),Ht=r(k),F=l(k,"DIV",{class:!0});var ae=V(F);u(Le.$$.fragment,ae),Xt=r(ae),yo=l(ae,"P",{"data-svelte-h":!0}),v(yo)!=="svelte-az893"&&(yo.innerHTML=Wn),Dt=r(ae),u(ne.$$.fragment,ae),Yt=r(ae),u(re.$$.fragment,ae),ae.forEach(o),Ot=r(k),se=l(k,"DIV",{class:!0});var mt=V(se);u(Qe.$$.fragment,mt),Kt=r(mt),Io=l(mt,"P",{"data-svelte-h":!0}),v(Io)!=="svelte-eq620n"&&(Io.innerHTML=Fn),mt.forEach(o),k.forEach(o),at=r(e),u(Ee.$$.fragment,e),it=r(e),Co=l(e,"P",{}),V(Co).forEach(o),this.h()},h(){T(i,"name","hf:doc:metadata"),T(i,"content",Yn),T(H,"class","flex flex-wrap space-x-1"),Zn(X.src,sn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg")||T(X,"src",sn),T(X,"alt","drawing"),T(X,"width","600"),T(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ro,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,i),a(e,I,t),a(e,m,t),a(e,b,t),a(e,y,t),a(e,M,t),f(ie,e,t),a(e,xo,t),a(e,H,t),a(e,ko,t),f(de,e,t),a(e,jo,t),a(e,le,t),a(e,zo,t),a(e,ce,t),a(e,Jo,t),a(e,me,t),a(e,Wo,t),a(e,X,t),a(e,Fo,t),a(e,pe,t),a(e,Uo,t),a(e,ue,t),a(e,Zo,t),f(fe,e,t),a(e,Po,t),a(e,he,t),a(e,No,t),a(e,ge,t),a(e,qo,t),f(_e,e,t),a(e,Go,t),a(e,$,t),f(be,$,null),s($,ft),s($,Xe),s($,ht),s($,De),s($,gt),f(D,$,null),s($,_t),s($,Y),f(ve,Y,null),s(Y,bt),s(Y,Ye),a(e,Ro,t),f(ye,e,t),a(e,Lo,t),a(e,j,t),f(Ie,j,null),s(j,vt),s(j,Oe),s(j,yt),s(j,Ke),s(j,It),f(O,j,null),a(e,Qo,t),f(Te,e,t),a(e,Eo,t),a(e,z,t),f(Me,z,null),s(z,Tt),s(z,eo),s(z,Mt),s(z,oo),s(z,Vt),f(K,z,null),a(e,So,t),f(Ve,e,t),a(e,Ao,t),a(e,U,t),f(Be,U,null),s(U,Bt),s(U,to),s(U,wt),s(U,no),a(e,Ho,t),f(we,e,t),a(e,Xo,t),a(e,E,t),f(Ce,E,null),s(E,Ct),s(E,ro),f($e,ro,null),a(e,Do,t),f(xe,e,t),a(e,Yo,t),a(e,Z,t),f(ke,Z,null),s(Z,$t),s(Z,so),s(Z,xt),s(Z,ee),f(je,ee,null),s(ee,kt),s(ee,ao),a(e,Oo,t),f(ze,e,t),a(e,Ko,t),a(e,S,t),f(Je,S,null),s(S,jt),s(S,G),f(We,G,null),s(G,zt),s(G,io),s(G,Jt),f(oe,G,null),a(e,et,t),f(Fe,e,t),a(e,ot,t),a(e,P,t),f(Ue,P,null),s(P,Wt),s(P,lo),s(P,Ft),s(P,R),f(Ze,R,null),s(R,Ut),s(R,co),s(R,Zt),s(R,mo),a(e,tt,t),f(Pe,e,t),a(e,nt,t),a(e,x,t),f(Ne,x,null),s(x,Pt),s(x,po),s(x,Nt),s(x,uo),s(x,qt),s(x,fo),s(x,Gt),s(x,L),f(qe,L,null),s(L,Rt),s(L,ho),s(L,Lt),f(te,L,null),a(e,rt,t),f(Ge,e,t),a(e,st,t),a(e,B,t),f(Re,B,null),s(B,Qt),s(B,go),s(B,Et),s(B,_o),s(B,St),s(B,bo),s(B,At),s(B,vo),s(B,Ht),s(B,F),f(Le,F,null),s(F,Xt),s(F,yo),s(F,Dt),f(ne,F,null),s(F,Yt),f(re,F,null),s(B,Ot),s(B,se),f(Qe,se,null),s(se,Kt),s(se,Io),a(e,at,t),f(Ee,e,t),a(e,it,t),a(e,Co,t),dt=!0},p(e,[t]){const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),D.$set(J);const Se={};t&2&&(Se.$$scope={dirty:t,ctx:e}),O.$set(Se);const N={};t&2&&(N.$$scope={dirty:t,ctx:e}),K.$set(N);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),oe.$set(q);const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),te.$set(A);const Ae={};t&2&&(Ae.$$scope={dirty:t,ctx:e}),ne.$set(Ae);const $o={};t&2&&($o.$$scope={dirty:t,ctx:e}),re.$set($o)},i(e){dt||(h(ie.$$.fragment,e),h(de.$$.fragment,e),h(fe.$$.fragment,e),h(_e.$$.fragment,e),h(be.$$.fragment,e),h(D.$$.fragment,e),h(ve.$$.fragment,e),h(ye.$$.fragment,e),h(Ie.$$.fragment,e),h(O.$$.fragment,e),h(Te.$$.fragment,e),h(Me.$$.fragment,e),h(K.$$.fragment,e),h(Ve.$$.fragment,e),h(Be.$$.fragment,e),h(we.$$.fragment,e),h(Ce.$$.fragment,e),h($e.$$.fragment,e),h(xe.$$.fragment,e),h(ke.$$.fragment,e),h(je.$$.fragment,e),h(ze.$$.fragment,e),h(Je.$$.fragment,e),h(We.$$.fragment,e),h(oe.$$.fragment,e),h(Fe.$$.fragment,e),h(Ue.$$.fragment,e),h(Ze.$$.fragment,e),h(Pe.$$.fragment,e),h(Ne.$$.fragment,e),h(qe.$$.fragment,e),h(te.$$.fragment,e),h(Ge.$$.fragment,e),h(Re.$$.fragment,e),h(Le.$$.fragment,e),h(ne.$$.fragment,e),h(re.$$.fragment,e),h(Qe.$$.fragment,e),h(Ee.$$.fragment,e),dt=!0)},o(e){g(ie.$$.fragment,e),g(de.$$.fragment,e),g(fe.$$.fragment,e),g(_e.$$.fragment,e),g(be.$$.fragment,e),g(D.$$.fragment,e),g(ve.$$.fragment,e),g(ye.$$.fragment,e),g(Ie.$$.fragment,e),g(O.$$.fragment,e),g(Te.$$.fragment,e),g(Me.$$.fragment,e),g(K.$$.fragment,e),g(Ve.$$.fragment,e),g(Be.$$.fragment,e),g(we.$$.fragment,e),g(Ce.$$.fragment,e),g($e.$$.fragment,e),g(xe.$$.fragment,e),g(ke.$$.fragment,e),g(je.$$.fragment,e),g(ze.$$.fragment,e),g(Je.$$.fragment,e),g(We.$$.fragment,e),g(oe.$$.fragment,e),g(Fe.$$.fragment,e),g(Ue.$$.fragment,e),g(Ze.$$.fragment,e),g(Pe.$$.fragment,e),g(Ne.$$.fragment,e),g(qe.$$.fragment,e),g(te.$$.fragment,e),g(Ge.$$.fragment,e),g(Re.$$.fragment,e),g(Le.$$.fragment,e),g(ne.$$.fragment,e),g(re.$$.fragment,e),g(Qe.$$.fragment,e),g(Ee.$$.fragment,e),dt=!1},d(e){e&&(o(I),o(m),o(b),o(y),o(M),o(xo),o(H),o(ko),o(jo),o(le),o(zo),o(ce),o(Jo),o(me),o(Wo),o(X),o(Fo),o(pe),o(Uo),o(ue),o(Zo),o(Po),o(he),o(No),o(ge),o(qo),o(Go),o($),o(Ro),o(Lo),o(j),o(Qo),o(Eo),o(z),o(So),o(Ao),o(U),o(Ho),o(Xo),o(E),o(Do),o(Yo),o(Z),o(Oo),o(Ko),o(S),o(et),o(ot),o(P),o(tt),o(nt),o(x),o(rt),o(st),o(B),o(at),o(it),o(Co)),o(i),_(ie,e),_(de,e),_(fe,e),_(_e,e),_(be),_(D),_(ve),_(ye,e),_(Ie),_(O),_(Te,e),_(Me),_(K),_(Ve,e),_(Be),_(we,e),_(Ce),_($e),_(xe,e),_(ke),_(je),_(ze,e),_(Je),_(We),_(oe),_(Fe,e),_(Ue),_(Ze),_(Pe,e),_(Ne),_(qe),_(te),_(Ge,e),_(Re),_(Le),_(ne),_(re),_(Qe),_(Ee,e)}}}const Yn='{"title":"InstructBlipVideo","local":"instructblipvideo","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"InstructBlipVideoConfig","local":"transformers.InstructBlipVideoConfig","sections":[],"depth":2},{"title":"InstructBlipVideoVisionConfig","local":"transformers.InstructBlipVideoVisionConfig","sections":[],"depth":2},{"title":"InstructBlipVideoQFormerConfig","local":"transformers.InstructBlipVideoQFormerConfig","sections":[],"depth":2},{"title":"InstructBlipVideoProcessor","local":"transformers.InstructBlipVideoProcessor","sections":[],"depth":2},{"title":"InstructBlipVideoVideoProcessor","local":"transformers.InstructBlipVideoVideoProcessor","sections":[],"depth":2},{"title":"InstructBlipVideoImageProcessor","local":"transformers.InstructBlipVideoImageProcessor","sections":[],"depth":2},{"title":"InstructBlipVideoVisionModel","local":"transformers.InstructBlipVideoVisionModel","sections":[],"depth":2},{"title":"InstructBlipVideoQFormerModel","local":"transformers.InstructBlipVideoQFormerModel","sections":[],"depth":2},{"title":"InstructBlipVideoModel","local":"transformers.InstructBlipVideoModel","sections":[],"depth":2},{"title":"InstructBlipVideoForConditionalGeneration","local":"transformers.InstructBlipVideoForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function On(C){return Pn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ar extends Nn{constructor(i){super(),qn(this,i,On,Dn,Un,{})}}export{ar as component};
