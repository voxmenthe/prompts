import{s as Jo,o as wo,n as A}from"../chunks/scheduler.18a86fab.js";import{S as vo,i as jo,g as M,s as i,r as m,A as $o,h as b,f as r,c as d,j as J,x as C,u as f,k as w,l as xo,y,a as l,v as g,d as u,t as h,w as _}from"../chunks/index.98837b22.js";import{T as At}from"../chunks/Tip.77304350.js";import{D as k}from"../chunks/Docstring.a1ef7999.js";import{C as E}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as ce}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as q,E as Ao}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Po,a as Lo}from"../chunks/HfOption.6641485e.js";function Uo(I){let t,c="Click on the AltCLIP models in the right sidebar for more examples of how to apply AltCLIP to different tasks.";return{c(){t=M("p"),t.textContent=c},l(n){t=b(n,"P",{"data-svelte-h":!0}),C(t)!=="svelte-1mudr4f"&&(t.textContent=c)},m(n,a){l(n,t,a)},p:A,d(n){n&&r(t)}}}function ko(I){let t,c;return t=new E({props:{code:"aW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbHRDTElQTW9kZWwlMkMlMjBBbHRDTElQUHJvY2Vzc29yJTBBJTBBbW9kZWwlMjAlM0QlMjBBbHRDTElQTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkJBQUklMkZBbHRDTElQJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBbHRDTElQUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZwaXBlbGluZS1jYXQtY2hvbmsuanBlZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBsb2dpdHNfcGVyX2ltYWdlLnNvZnRtYXgoZGltJTNEMSklMjAlMjAlMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXMlMEElMEFsYWJlbHMlMjAlM0QlMjAlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMEFmb3IlMjBsYWJlbCUyQyUyMHByb2IlMjBpbiUyMHppcChsYWJlbHMlMkMlMjBwcm9icyU1QjAlNUQpJTNBJTBBJTIwJTIwJTIwJTIwcHJpbnQoZiUyMiU3QmxhYmVsJTdEJTNBJTIwJTdCcHJvYi5pdGVtKCklM0EuNGYlN0QlMjIp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPModel, AltCLIPProcessor

model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>, dtype=torch.bfloat16)
processor = AltCLIPProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>

labels = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]
<span class="hljs-keyword">for</span> label, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(labels, probs[<span class="hljs-number">0</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{label}</span>: <span class="hljs-subst">{prob.item():<span class="hljs-number">.4</span>f}</span>&quot;</span>)`,wrap:!1}}),{c(){m(t.$$.fragment)},l(n){f(t.$$.fragment,n)},m(n,a){g(t,n,a),c=!0},p:A,i(n){c||(u(t.$$.fragment,n),c=!0)},o(n){h(t.$$.fragment,n),c=!1},d(n){_(t,n)}}}function Bo(I){let t,c;return t=new Lo({props:{id:"usage",option:"AutoModel",$$slots:{default:[ko]},$$scope:{ctx:I}}}),{c(){m(t.$$.fragment)},l(n){f(t.$$.fragment,n)},m(n,a){g(t,n,a),c=!0},p(n,a){const p={};a&2&&(p.$$scope={dirty:a,ctx:n}),t.$set(p)},i(n){c||(u(t.$$.fragment,n),c=!0)},o(n){h(t.$$.fragment,n),c=!1},d(n){_(t,n)}}}function Wo(I){let t,c="Example:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBDb25maWclMkMlMjBBbHRDTElQTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUENvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUE1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQWx0Q0xJUENvbmZpZyUyMGZyb20lMjBhJTIwQWx0Q0xJUFRleHRDb25maWclMjBhbmQlMjBhJTIwQWx0Q0xJUFZpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbHRDTElQVGV4dCUyMGFuZCUyMEFsdENMSVBWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBBbHRDTElQVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMEFsdENMSVBWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMEFsdENMSVBDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPConfig, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a AltCLIPConfig from a AltCLIPTextConfig and a AltCLIPVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPText and AltCLIPVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = AltCLIPTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = AltCLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-11lpom8"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function Zo(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBUZXh0TW9kZWwlMkMlMjBBbHRDTElQVGV4dENvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbHRDTElQVGV4dENvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsdENMSVBUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMEJBQUklMkZBbHRDTElQJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbHRDTElQVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPTextModel, AltCLIPTextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPTextConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPTextModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function zo(I){let t,c="Example:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBWaXNpb25Db25maWclMkMlMjBBbHRDTElQVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUFZpc2lvbkNvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUFZpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUFZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPVisionConfig, AltCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPVisionConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPVisionModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-11lpom8"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function Qo(I){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=M("p"),t.innerHTML=c},l(n){t=b(n,"P",{"data-svelte-h":!0}),C(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,a){l(n,t,a)},p:A,d(n){n&&r(t)}}}function Vo(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUwQSUyMCUyMCUyMCUyMHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUwQSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzX3Blcl9pbWFnZSUyMCUzRCUyMG91dHB1dHMubG9naXRzX3Blcl9pbWFnZSUyMCUyMCUyMyUyMHRoaXMlMjBpcyUyMHRoZSUyMGltYWdlLXRleHQlMjBzaW1pbGFyaXR5JTIwc2NvcmUlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUyMCUyMCUyMyUyMHdlJTIwY2FuJTIwdGFrZSUyMHRoZSUyMHNvZnRtYXglMjB0byUyMGdldCUyMHRoZSUyMGxhYmVsJTIwcHJvYmFiaWxpdGllcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function Go(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFpbWFnZV9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF9pbWFnZV9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function Ho(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBbHRDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsdENMSVBNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyQkFBSSUyRkFsdENMSVAlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyQkFBSSUyRkFsdENMSVAlMjIpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function Eo(I){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=M("p"),t.innerHTML=c},l(n){t=b(n,"P",{"data-svelte-h":!0}),C(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,a){l(n,t,a)},p:A,d(n){n&&r(t)}}}function Ro(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBbHRDTElQVGV4dE1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBBbHRDTElQVGV4dE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF0ZXh0cyUyMCUzRCUyMCU1QiUyMml0J3MlMjBhJTIwY2F0JTIyJTJDJTIwJTIyaXQncyUyMGElMjBkb2clMjIlNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRHRleHRzJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwQ0xTJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPTextModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;it&#x27;s a cat&quot;</span>, <span class="hljs-string">&quot;it&#x27;s a dog&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function No(I){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=M("p"),t.innerHTML=c},l(n){t=b(n,"P",{"data-svelte-h":!0}),C(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,a){l(n,t,a)},p:A,d(n){n&&r(t)}}}function Fo(I){let t,c="Examples:",n,a,p;return a=new E({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBWaXNpb25Nb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUFZpc2lvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMENMUyUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPVisionModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){t=M("p"),t.textContent=c,n=i(),m(a.$$.fragment)},l(o){t=b(o,"P",{"data-svelte-h":!0}),C(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(o),f(a.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),g(a,o,T),p=!0},p:A,i(o){p||(u(a.$$.fragment,o),p=!0)},o(o){h(a.$$.fragment,o),p=!1},d(o){o&&(r(t),r(n)),_(a,o)}}}function qo(I){let t,c,n,a,p,o="<em>This model was released on 2022-11-12 and added to Hugging Face Transformers on 2023-01-04.</em>",T,X,so='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ot,pe,nt,me,ao='<a href="https://huggingface.co/papers/2211.06679" rel="nofollow">AltCLIP</a> replaces the <a href="./clip">CLIP</a> text encoder with a multilingual XLM-R encoder and aligns image and text representations with teacher learning and contrastive learning.',st,fe,ro='You can find all the original AltCLIP checkpoints under the <a href="https://huggingface.co/collections/BAAI/alt-clip-diffusion-66987a97de8525205f1221bf" rel="nofollow">AltClip</a> collection.',at,S,rt,ge,lo='The examples below demonstrates how to calculate similarity scores between an image and one or more captions with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',lt,Y,it,ue,io='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',dt,he,co='The example below uses <a href="../quantization/torchao">torchao</a> to only quantize the weights to int4.',ct,_e,pt,Me,mt,be,po='<li>AltCLIP uses bidirectional attention instead of causal attention and it uses the <code>[CLS]</code> token in XLM-R to represent a text embedding.</li> <li>Use <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> to resize (or rescale) and normalize images for the model.</li> <li><a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> combines <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> into a single instance to encode text and prepare images.</li>',ft,ye,gt,v,Te,Pt,Re,mo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a>. It is used to instantiate an
AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,Lt,Ne,fo=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ut,D,ut,Ce,ht,j,Ie,kt,Fe,go=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>. It is used to instantiate a
AltCLIP text model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,Bt,qe,uo=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Wt,O,_t,Je,Mt,$,we,Zt,Xe,ho=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a>. It is used to instantiate an
AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,zt,Se,_o=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Qt,K,bt,ve,yt,x,je,Vt,P,$e,Gt,Ye,Mo='The <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',Ht,ee,Et,te,Rt,oe,xe,Nt,ne,Ft,se,Ae,qt,ae,Tt,Pe,Ct,R,Le,Xt,L,Ue,St,De,bo='The <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a> forward method, overrides the <code>__call__</code> special method.',Yt,re,Dt,le,It,ke,Jt,N,Be,Ot,U,We,Kt,Oe,yo='The <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPVisionModel">AltCLIPVisionModel</a> forward method, overrides the <code>__call__</code> special method.',eo,ie,to,de,wt,Ze,vt,B,ze,oo,Ke,To=`Constructs a AltCLIP processor which wraps a CLIP image processor and a XLM-Roberta tokenizer into a single
processor.`,no,et,Co=`<a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a>. See
the <code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,jt,Qe,$t,tt,xt;return pe=new q({props:{title:"AltCLIP",local:"altclip",headingTag:"h1"}}),S=new At({props:{warning:!1,$$slots:{default:[Uo]},$$scope:{ctx:I}}}),Y=new Po({props:{id:"usage",options:["AutoModel"],$$slots:{default:[Bo]},$$scope:{ctx:I}}}),_e=new E({props:{code:"JTIzJTIwIXBpcCUyMGluc3RhbGwlMjB0b3JjaGFvJTBBaW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbHRDTElQTW9kZWwlMkMlMjBBbHRDTElQUHJvY2Vzc29yJTJDJTIwVG9yY2hBb0NvbmZpZyUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJCQUFJJTJGQWx0Q0xJUCUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RUb3JjaEFvQ29uZmlnKCUyMmludDRfd2VpZ2h0X29ubHklMjIlMkMlMjBncm91cF9zaXplJTNEMTI4KSUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEEpJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQWx0Q0xJUFByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyQkFBSSUyRkFsdENMSVAlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaHVnZ2luZ2ZhY2UlMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGcGlwZWxpbmUtY2F0LWNob25rLmpwZWclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMHBhZGRpbmclM0RUcnVlKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHNfcGVyX2ltYWdlJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHNfcGVyX2ltYWdlJTIwJTIwJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQXByb2JzJTIwJTNEJTIwbG9naXRzX3Blcl9pbWFnZS5zb2Z0bWF4KGRpbSUzRDEpJTIwJTIwJTIzJTIwd2UlMjBjYW4lMjB0YWtlJTIwdGhlJTIwc29mdG1heCUyMHRvJTIwZ2V0JTIwdGhlJTIwbGFiZWwlMjBwcm9iYWJpbGl0aWVzJTBBJTBBbGFiZWxzJTIwJTNEJTIwJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTBBZm9yJTIwbGFiZWwlMkMlMjBwcm9iJTIwaW4lMjB6aXAobGFiZWxzJTJDJTIwcHJvYnMlNUIwJTVEKSUzQSUwQSUyMCUyMCUyMCUyMHByaW50KGYlMjIlN0JsYWJlbCU3RCUzQSUyMCU3QnByb2IuaXRlbSgpJTNBLjRmJTdEJTIyKQ==",highlighted:`<span class="hljs-comment"># !pip install torchao</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPModel, AltCLIPProcessor, TorchAoConfig

model = AltCLIPModel.from_pretrained(
    <span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>,
    quantization_config=TorchAoConfig(<span class="hljs-string">&quot;int4_weight_only&quot;</span>, group_size=<span class="hljs-number">128</span>),
    dtype=torch.bfloat16,
)

processor = AltCLIPProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>

labels = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]
<span class="hljs-keyword">for</span> label, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(labels, probs[<span class="hljs-number">0</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{label}</span>: <span class="hljs-subst">{prob.item():<span class="hljs-number">.4</span>f}</span>&quot;</span>)`,wrap:!1}}),Me=new q({props:{title:"Notes",local:"notes",headingTag:"h2"}}),ye=new q({props:{title:"AltCLIPConfig",local:"transformers.AltCLIPConfig",headingTag:"h2"}}),Te=new k({props:{name:"class transformers.AltCLIPConfig",anchor:"transformers.AltCLIPConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextConfig">AltCLIPTextConfig</a>.`,name:"text_config"},{anchor:"transformers.AltCLIPConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPVisionConfig">AltCLIPVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.AltCLIPConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AltCLIPConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The initial value of the <em>logit_scale</em> parameter. Default is used as per the original CLIP implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.AltCLIPConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/configuration_altclip.py#L235"}}),D=new ce({props:{anchor:"transformers.AltCLIPConfig.example",$$slots:{default:[Wo]},$$scope:{ctx:I}}}),Ce=new q({props:{title:"AltCLIPTextConfig",local:"transformers.AltCLIPTextConfig",headingTag:"h2"}}),Ie=new k({props:{name:"class transformers.AltCLIPTextConfig",anchor:"transformers.AltCLIPTextConfig",parameters:[{name:"vocab_size",val:" = 250002"},{name:"hidden_size",val:" = 1024"},{name:"num_hidden_layers",val:" = 24"},{name:"num_attention_heads",val:" = 16"},{name:"intermediate_size",val:" = 4096"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 514"},{name:"type_vocab_size",val:" = 1"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"project_dim",val:" = 768"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250002) &#x2014;
Vocabulary size of the AltCLIP model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.AltCLIPTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AltCLIPTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AltCLIPTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AltCLIPTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AltCLIPTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AltCLIPTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.AltCLIPTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.AltCLIPTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 514) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.AltCLIPTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>`,name:"type_vocab_size"},{anchor:"transformers.AltCLIPTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AltCLIPTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.AltCLIPTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AltCLIPTextConfig.pad_token_id",description:"<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The id of the <em>padding</em> token.",name:"pad_token_id"},{anchor:"transformers.AltCLIPTextConfig.bos_token_id",description:"<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The id of the <em>beginning-of-sequence</em> token.",name:"bos_token_id"},{anchor:"transformers.AltCLIPTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>Union[int, list[int]]</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the <em>end-of-sequence</em> token. Optionally, use a list to set multiple <em>end-of-sequence</em> tokens.`,name:"eos_token_id"},{anchor:"transformers.AltCLIPTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://huggingface.co/papers/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://huggingface.co/papers/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.AltCLIPTextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.AltCLIPTextConfig.project_dim",description:`<strong>project_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The dimensions of the teacher model before the mapping layer.`,name:"project_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/configuration_altclip.py#L24"}}),O=new ce({props:{anchor:"transformers.AltCLIPTextConfig.example",$$slots:{default:[Zo]},$$scope:{ctx:I}}}),Je=new q({props:{title:"AltCLIPVisionConfig",local:"transformers.AltCLIPVisionConfig",headingTag:"h2"}}),we=new k({props:{name:"class transformers.AltCLIPVisionConfig",anchor:"transformers.AltCLIPVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AltCLIPVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AltCLIPVisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AltCLIPVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AltCLIPVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AltCLIPVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.AltCLIPVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.AltCLIPVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.AltCLIPVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AltCLIPVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AltCLIPVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.AltCLIPVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AltCLIPVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/configuration_altclip.py#L142"}}),K=new ce({props:{anchor:"transformers.AltCLIPVisionConfig.example",$$slots:{default:[zo]},$$scope:{ctx:I}}}),ve=new q({props:{title:"AltCLIPModel",local:"transformers.AltCLIPModel",headingTag:"h2"}}),je=new k({props:{name:"class transformers.AltCLIPModel",anchor:"transformers.AltCLIPModel",parameters:[{name:"config",val:": AltCLIPConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1171"}}),$e=new k({props:{name:"forward",anchor:"transformers.AltCLIPModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.AltCLIPModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AltCLIPModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.AltCLIPModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.AltCLIPModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1302",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.altclip.modeling_altclip.AltCLIPOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig"
>AltCLIPConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>)  Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>)  The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>)  The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)  The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)  The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</li>
<li><strong>text_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.text_model_output</code>, defaults to <code>None</code>)  The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>&lt;class '~modeling_outputs.BaseModelOutputWithPooling'&gt;.vision_model_output</code>, defaults to <code>None</code>)  The output of the <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.altclip.modeling_altclip.AltCLIPOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ee=new At({props:{$$slots:{default:[Qo]},$$scope:{ctx:I}}}),te=new ce({props:{anchor:"transformers.AltCLIPModel.forward.example",$$slots:{default:[Vo]},$$scope:{ctx:I}}}),xe=new k({props:{name:"get_image_features",anchor:"transformers.AltCLIPModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.AltCLIPModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.AltCLIPModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1254",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ne=new ce({props:{anchor:"transformers.AltCLIPModel.get_image_features.example",$$slots:{default:[Go]},$$scope:{ctx:I}}}),Ae=new k({props:{name:"get_text_features",anchor:"transformers.AltCLIPModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:" = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (\`<code>of shape</code>(batch_size, sequence_length)<code>) -- Segment token indices to indicate first and second portions of the inputs. Indices are selected in </code>[0, 1]\`:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AltCLIPModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1207",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ae=new ce({props:{anchor:"transformers.AltCLIPModel.get_text_features.example",$$slots:{default:[Ho]},$$scope:{ctx:I}}}),Pe=new q({props:{title:"AltCLIPTextModel",local:"transformers.AltCLIPTextModel",headingTag:"h2"}}),Le=new k({props:{name:"class transformers.AltCLIPTextModel",anchor:"transformers.AltCLIPTextModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1088"}}),Ue=new k({props:{name:"forward",anchor:"transformers.AltCLIPTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.AltCLIPTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AltCLIPTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.AltCLIPTextModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.AltCLIPTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.AltCLIPTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L1107",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig"
>AltCLIPConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)  Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>)  Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>projection_state</strong> (<code>tuple(torch.FloatTensor)</code>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> of shape <code>(batch_size,config.project_dim)</code>.</p>
<p>Text embeddings before the projection layer, used to mimic the last hidden state of the teacher encoder.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new At({props:{$$slots:{default:[Eo]},$$scope:{ctx:I}}}),le=new ce({props:{anchor:"transformers.AltCLIPTextModel.forward.example",$$slots:{default:[Ro]},$$scope:{ctx:I}}}),ke=new q({props:{title:"AltCLIPVisionModel",local:"transformers.AltCLIPVisionModel",headingTag:"h2"}}),Be=new k({props:{name:"class transformers.AltCLIPVisionModel",anchor:"transformers.AltCLIPVisionModel",parameters:[{name:"config",val:": AltCLIPVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L916"}}),We=new k({props:{name:"forward",anchor:"transformers.AltCLIPVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.AltCLIPVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.AltCLIPVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPVisionModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.AltCLIPVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/modeling_altclip.py#L929",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/altclip#transformers.AltCLIPConfig"
>AltCLIPConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)  Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>)  Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>)  Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new At({props:{$$slots:{default:[No]},$$scope:{ctx:I}}}),de=new ce({props:{anchor:"transformers.AltCLIPVisionModel.forward.example",$$slots:{default:[Fo]},$$scope:{ctx:I}}}),Ze=new q({props:{title:"AltCLIPProcessor",local:"transformers.AltCLIPProcessor",headingTag:"h2"}}),ze=new k({props:{name:"class transformers.AltCLIPProcessor",anchor:"transformers.AltCLIPProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"}],parametersDescription:[{anchor:"transformers.AltCLIPProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.AltCLIPProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/altclip/processing_altclip.py#L31"}}),Qe=new Ao({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/altclip.md"}}),{c(){t=M("meta"),c=i(),n=M("p"),a=i(),p=M("p"),p.innerHTML=o,T=i(),X=M("div"),X.innerHTML=so,ot=i(),m(pe.$$.fragment),nt=i(),me=M("p"),me.innerHTML=ao,st=i(),fe=M("p"),fe.innerHTML=ro,at=i(),m(S.$$.fragment),rt=i(),ge=M("p"),ge.innerHTML=lo,lt=i(),m(Y.$$.fragment),it=i(),ue=M("p"),ue.innerHTML=io,dt=i(),he=M("p"),he.innerHTML=co,ct=i(),m(_e.$$.fragment),pt=i(),m(Me.$$.fragment),mt=i(),be=M("ul"),be.innerHTML=po,ft=i(),m(ye.$$.fragment),gt=i(),v=M("div"),m(Te.$$.fragment),Pt=i(),Re=M("p"),Re.innerHTML=mo,Lt=i(),Ne=M("p"),Ne.innerHTML=fo,Ut=i(),m(D.$$.fragment),ut=i(),m(Ce.$$.fragment),ht=i(),j=M("div"),m(Ie.$$.fragment),kt=i(),Fe=M("p"),Fe.innerHTML=go,Bt=i(),qe=M("p"),qe.innerHTML=uo,Wt=i(),m(O.$$.fragment),_t=i(),m(Je.$$.fragment),Mt=i(),$=M("div"),m(we.$$.fragment),Zt=i(),Xe=M("p"),Xe.innerHTML=ho,zt=i(),Se=M("p"),Se.innerHTML=_o,Qt=i(),m(K.$$.fragment),bt=i(),m(ve.$$.fragment),yt=i(),x=M("div"),m(je.$$.fragment),Vt=i(),P=M("div"),m($e.$$.fragment),Gt=i(),Ye=M("p"),Ye.innerHTML=Mo,Ht=i(),m(ee.$$.fragment),Et=i(),m(te.$$.fragment),Rt=i(),oe=M("div"),m(xe.$$.fragment),Nt=i(),m(ne.$$.fragment),Ft=i(),se=M("div"),m(Ae.$$.fragment),qt=i(),m(ae.$$.fragment),Tt=i(),m(Pe.$$.fragment),Ct=i(),R=M("div"),m(Le.$$.fragment),Xt=i(),L=M("div"),m(Ue.$$.fragment),St=i(),De=M("p"),De.innerHTML=bo,Yt=i(),m(re.$$.fragment),Dt=i(),m(le.$$.fragment),It=i(),m(ke.$$.fragment),Jt=i(),N=M("div"),m(Be.$$.fragment),Ot=i(),U=M("div"),m(We.$$.fragment),Kt=i(),Oe=M("p"),Oe.innerHTML=yo,eo=i(),m(ie.$$.fragment),to=i(),m(de.$$.fragment),wt=i(),m(Ze.$$.fragment),vt=i(),B=M("div"),m(ze.$$.fragment),oo=i(),Ke=M("p"),Ke.textContent=To,no=i(),et=M("p"),et.innerHTML=Co,jt=i(),m(Qe.$$.fragment),$t=i(),tt=M("p"),this.h()},l(e){const s=$o("svelte-u9bgzb",document.head);t=b(s,"META",{name:!0,content:!0}),s.forEach(r),c=d(e),n=b(e,"P",{}),J(n).forEach(r),a=d(e),p=b(e,"P",{"data-svelte-h":!0}),C(p)!=="svelte-p9hohh"&&(p.innerHTML=o),T=d(e),X=b(e,"DIV",{style:!0,"data-svelte-h":!0}),C(X)!=="svelte-qicb85"&&(X.innerHTML=so),ot=d(e),f(pe.$$.fragment,e),nt=d(e),me=b(e,"P",{"data-svelte-h":!0}),C(me)!=="svelte-eghmme"&&(me.innerHTML=ao),st=d(e),fe=b(e,"P",{"data-svelte-h":!0}),C(fe)!=="svelte-1ehamva"&&(fe.innerHTML=ro),at=d(e),f(S.$$.fragment,e),rt=d(e),ge=b(e,"P",{"data-svelte-h":!0}),C(ge)!=="svelte-1hhfqk6"&&(ge.innerHTML=lo),lt=d(e),f(Y.$$.fragment,e),it=d(e),ue=b(e,"P",{"data-svelte-h":!0}),C(ue)!=="svelte-nf5ooi"&&(ue.innerHTML=io),dt=d(e),he=b(e,"P",{"data-svelte-h":!0}),C(he)!=="svelte-w36i1c"&&(he.innerHTML=co),ct=d(e),f(_e.$$.fragment,e),pt=d(e),f(Me.$$.fragment,e),mt=d(e),be=b(e,"UL",{"data-svelte-h":!0}),C(be)!=="svelte-x65f9u"&&(be.innerHTML=po),ft=d(e),f(ye.$$.fragment,e),gt=d(e),v=b(e,"DIV",{class:!0});var W=J(v);f(Te.$$.fragment,W),Pt=d(W),Re=b(W,"P",{"data-svelte-h":!0}),C(Re)!=="svelte-1gz6zai"&&(Re.innerHTML=mo),Lt=d(W),Ne=b(W,"P",{"data-svelte-h":!0}),C(Ne)!=="svelte-1ek1ss9"&&(Ne.innerHTML=fo),Ut=d(W),f(D.$$.fragment,W),W.forEach(r),ut=d(e),f(Ce.$$.fragment,e),ht=d(e),j=b(e,"DIV",{class:!0});var Z=J(j);f(Ie.$$.fragment,Z),kt=d(Z),Fe=b(Z,"P",{"data-svelte-h":!0}),C(Fe)!=="svelte-12e0ulf"&&(Fe.innerHTML=go),Bt=d(Z),qe=b(Z,"P",{"data-svelte-h":!0}),C(qe)!=="svelte-1ek1ss9"&&(qe.innerHTML=uo),Wt=d(Z),f(O.$$.fragment,Z),Z.forEach(r),_t=d(e),f(Je.$$.fragment,e),Mt=d(e),$=b(e,"DIV",{class:!0});var z=J($);f(we.$$.fragment,z),Zt=d(z),Xe=b(z,"P",{"data-svelte-h":!0}),C(Xe)!=="svelte-1gz6zai"&&(Xe.innerHTML=ho),zt=d(z),Se=b(z,"P",{"data-svelte-h":!0}),C(Se)!=="svelte-1ek1ss9"&&(Se.innerHTML=_o),Qt=d(z),f(K.$$.fragment,z),z.forEach(r),bt=d(e),f(ve.$$.fragment,e),yt=d(e),x=b(e,"DIV",{class:!0});var Q=J(x);f(je.$$.fragment,Q),Vt=d(Q),P=b(Q,"DIV",{class:!0});var V=J(P);f($e.$$.fragment,V),Gt=d(V),Ye=b(V,"P",{"data-svelte-h":!0}),C(Ye)!=="svelte-6e87ho"&&(Ye.innerHTML=Mo),Ht=d(V),f(ee.$$.fragment,V),Et=d(V),f(te.$$.fragment,V),V.forEach(r),Rt=d(Q),oe=b(Q,"DIV",{class:!0});var Ve=J(oe);f(xe.$$.fragment,Ve),Nt=d(Ve),f(ne.$$.fragment,Ve),Ve.forEach(r),Ft=d(Q),se=b(Q,"DIV",{class:!0});var Ge=J(se);f(Ae.$$.fragment,Ge),qt=d(Ge),f(ae.$$.fragment,Ge),Ge.forEach(r),Q.forEach(r),Tt=d(e),f(Pe.$$.fragment,e),Ct=d(e),R=b(e,"DIV",{class:!0});var He=J(R);f(Le.$$.fragment,He),Xt=d(He),L=b(He,"DIV",{class:!0});var G=J(L);f(Ue.$$.fragment,G),St=d(G),De=b(G,"P",{"data-svelte-h":!0}),C(De)!=="svelte-nlu11y"&&(De.innerHTML=bo),Yt=d(G),f(re.$$.fragment,G),Dt=d(G),f(le.$$.fragment,G),G.forEach(r),He.forEach(r),It=d(e),f(ke.$$.fragment,e),Jt=d(e),N=b(e,"DIV",{class:!0});var Ee=J(N);f(Be.$$.fragment,Ee),Ot=d(Ee),U=b(Ee,"DIV",{class:!0});var H=J(U);f(We.$$.fragment,H),Kt=d(H),Oe=b(H,"P",{"data-svelte-h":!0}),C(Oe)!=="svelte-1j3lie4"&&(Oe.innerHTML=yo),eo=d(H),f(ie.$$.fragment,H),to=d(H),f(de.$$.fragment,H),H.forEach(r),Ee.forEach(r),wt=d(e),f(Ze.$$.fragment,e),vt=d(e),B=b(e,"DIV",{class:!0});var F=J(B);f(ze.$$.fragment,F),oo=d(F),Ke=b(F,"P",{"data-svelte-h":!0}),C(Ke)!=="svelte-1rdismf"&&(Ke.textContent=To),no=d(F),et=b(F,"P",{"data-svelte-h":!0}),C(et)!=="svelte-15nh8hc"&&(et.innerHTML=Co),F.forEach(r),jt=d(e),f(Qe.$$.fragment,e),$t=d(e),tt=b(e,"P",{}),J(tt).forEach(r),this.h()},h(){w(t,"name","hf:doc:metadata"),w(t,"content",Xo),xo(X,"float","right"),w(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){y(document.head,t),l(e,c,s),l(e,n,s),l(e,a,s),l(e,p,s),l(e,T,s),l(e,X,s),l(e,ot,s),g(pe,e,s),l(e,nt,s),l(e,me,s),l(e,st,s),l(e,fe,s),l(e,at,s),g(S,e,s),l(e,rt,s),l(e,ge,s),l(e,lt,s),g(Y,e,s),l(e,it,s),l(e,ue,s),l(e,dt,s),l(e,he,s),l(e,ct,s),g(_e,e,s),l(e,pt,s),g(Me,e,s),l(e,mt,s),l(e,be,s),l(e,ft,s),g(ye,e,s),l(e,gt,s),l(e,v,s),g(Te,v,null),y(v,Pt),y(v,Re),y(v,Lt),y(v,Ne),y(v,Ut),g(D,v,null),l(e,ut,s),g(Ce,e,s),l(e,ht,s),l(e,j,s),g(Ie,j,null),y(j,kt),y(j,Fe),y(j,Bt),y(j,qe),y(j,Wt),g(O,j,null),l(e,_t,s),g(Je,e,s),l(e,Mt,s),l(e,$,s),g(we,$,null),y($,Zt),y($,Xe),y($,zt),y($,Se),y($,Qt),g(K,$,null),l(e,bt,s),g(ve,e,s),l(e,yt,s),l(e,x,s),g(je,x,null),y(x,Vt),y(x,P),g($e,P,null),y(P,Gt),y(P,Ye),y(P,Ht),g(ee,P,null),y(P,Et),g(te,P,null),y(x,Rt),y(x,oe),g(xe,oe,null),y(oe,Nt),g(ne,oe,null),y(x,Ft),y(x,se),g(Ae,se,null),y(se,qt),g(ae,se,null),l(e,Tt,s),g(Pe,e,s),l(e,Ct,s),l(e,R,s),g(Le,R,null),y(R,Xt),y(R,L),g(Ue,L,null),y(L,St),y(L,De),y(L,Yt),g(re,L,null),y(L,Dt),g(le,L,null),l(e,It,s),g(ke,e,s),l(e,Jt,s),l(e,N,s),g(Be,N,null),y(N,Ot),y(N,U),g(We,U,null),y(U,Kt),y(U,Oe),y(U,eo),g(ie,U,null),y(U,to),g(de,U,null),l(e,wt,s),g(Ze,e,s),l(e,vt,s),l(e,B,s),g(ze,B,null),y(B,oo),y(B,Ke),y(B,no),y(B,et),l(e,jt,s),g(Qe,e,s),l(e,$t,s),l(e,tt,s),xt=!0},p(e,[s]){const W={};s&2&&(W.$$scope={dirty:s,ctx:e}),S.$set(W);const Z={};s&2&&(Z.$$scope={dirty:s,ctx:e}),Y.$set(Z);const z={};s&2&&(z.$$scope={dirty:s,ctx:e}),D.$set(z);const Q={};s&2&&(Q.$$scope={dirty:s,ctx:e}),O.$set(Q);const V={};s&2&&(V.$$scope={dirty:s,ctx:e}),K.$set(V);const Ve={};s&2&&(Ve.$$scope={dirty:s,ctx:e}),ee.$set(Ve);const Ge={};s&2&&(Ge.$$scope={dirty:s,ctx:e}),te.$set(Ge);const He={};s&2&&(He.$$scope={dirty:s,ctx:e}),ne.$set(He);const G={};s&2&&(G.$$scope={dirty:s,ctx:e}),ae.$set(G);const Ee={};s&2&&(Ee.$$scope={dirty:s,ctx:e}),re.$set(Ee);const H={};s&2&&(H.$$scope={dirty:s,ctx:e}),le.$set(H);const F={};s&2&&(F.$$scope={dirty:s,ctx:e}),ie.$set(F);const Io={};s&2&&(Io.$$scope={dirty:s,ctx:e}),de.$set(Io)},i(e){xt||(u(pe.$$.fragment,e),u(S.$$.fragment,e),u(Y.$$.fragment,e),u(_e.$$.fragment,e),u(Me.$$.fragment,e),u(ye.$$.fragment,e),u(Te.$$.fragment,e),u(D.$$.fragment,e),u(Ce.$$.fragment,e),u(Ie.$$.fragment,e),u(O.$$.fragment,e),u(Je.$$.fragment,e),u(we.$$.fragment,e),u(K.$$.fragment,e),u(ve.$$.fragment,e),u(je.$$.fragment,e),u($e.$$.fragment,e),u(ee.$$.fragment,e),u(te.$$.fragment,e),u(xe.$$.fragment,e),u(ne.$$.fragment,e),u(Ae.$$.fragment,e),u(ae.$$.fragment,e),u(Pe.$$.fragment,e),u(Le.$$.fragment,e),u(Ue.$$.fragment,e),u(re.$$.fragment,e),u(le.$$.fragment,e),u(ke.$$.fragment,e),u(Be.$$.fragment,e),u(We.$$.fragment,e),u(ie.$$.fragment,e),u(de.$$.fragment,e),u(Ze.$$.fragment,e),u(ze.$$.fragment,e),u(Qe.$$.fragment,e),xt=!0)},o(e){h(pe.$$.fragment,e),h(S.$$.fragment,e),h(Y.$$.fragment,e),h(_e.$$.fragment,e),h(Me.$$.fragment,e),h(ye.$$.fragment,e),h(Te.$$.fragment,e),h(D.$$.fragment,e),h(Ce.$$.fragment,e),h(Ie.$$.fragment,e),h(O.$$.fragment,e),h(Je.$$.fragment,e),h(we.$$.fragment,e),h(K.$$.fragment,e),h(ve.$$.fragment,e),h(je.$$.fragment,e),h($e.$$.fragment,e),h(ee.$$.fragment,e),h(te.$$.fragment,e),h(xe.$$.fragment,e),h(ne.$$.fragment,e),h(Ae.$$.fragment,e),h(ae.$$.fragment,e),h(Pe.$$.fragment,e),h(Le.$$.fragment,e),h(Ue.$$.fragment,e),h(re.$$.fragment,e),h(le.$$.fragment,e),h(ke.$$.fragment,e),h(Be.$$.fragment,e),h(We.$$.fragment,e),h(ie.$$.fragment,e),h(de.$$.fragment,e),h(Ze.$$.fragment,e),h(ze.$$.fragment,e),h(Qe.$$.fragment,e),xt=!1},d(e){e&&(r(c),r(n),r(a),r(p),r(T),r(X),r(ot),r(nt),r(me),r(st),r(fe),r(at),r(rt),r(ge),r(lt),r(it),r(ue),r(dt),r(he),r(ct),r(pt),r(mt),r(be),r(ft),r(gt),r(v),r(ut),r(ht),r(j),r(_t),r(Mt),r($),r(bt),r(yt),r(x),r(Tt),r(Ct),r(R),r(It),r(Jt),r(N),r(wt),r(vt),r(B),r(jt),r($t),r(tt)),r(t),_(pe,e),_(S,e),_(Y,e),_(_e,e),_(Me,e),_(ye,e),_(Te),_(D),_(Ce,e),_(Ie),_(O),_(Je,e),_(we),_(K),_(ve,e),_(je),_($e),_(ee),_(te),_(xe),_(ne),_(Ae),_(ae),_(Pe,e),_(Le),_(Ue),_(re),_(le),_(ke,e),_(Be),_(We),_(ie),_(de),_(Ze,e),_(ze),_(Qe,e)}}}const Xo='{"title":"AltCLIP","local":"altclip","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"AltCLIPConfig","local":"transformers.AltCLIPConfig","sections":[],"depth":2},{"title":"AltCLIPTextConfig","local":"transformers.AltCLIPTextConfig","sections":[],"depth":2},{"title":"AltCLIPVisionConfig","local":"transformers.AltCLIPVisionConfig","sections":[],"depth":2},{"title":"AltCLIPModel","local":"transformers.AltCLIPModel","sections":[],"depth":2},{"title":"AltCLIPTextModel","local":"transformers.AltCLIPTextModel","sections":[],"depth":2},{"title":"AltCLIPVisionModel","local":"transformers.AltCLIPVisionModel","sections":[],"depth":2},{"title":"AltCLIPProcessor","local":"transformers.AltCLIPProcessor","sections":[],"depth":2}],"depth":1}';function So(I){return wo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sn extends vo{constructor(t){super(),jo(this,t,So,qo,Jo,{})}}export{sn as component};
