import{s as mt,o as ft,n as ct}from"../chunks/scheduler.18a86fab.js";import{S as Mt,i as ht,g as i,s,r as v,A as yt,h as r,f as n,c as o,j as rt,u as P,x as u,k as pt,y as dt,a as l,v as _,d as q,t as S,w as k}from"../chunks/index.98837b22.js";import{T as wt}from"../chunks/Tip.77304350.js";import{C as ut}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as K,E as bt}from"../chunks/getInferenceSnippets.06c2775f.js";function Tt(Z){let a,f='Find models pre-quantized with FP-Quant in the official ISTA-DASLab <a href="https://huggingface.co/collections/ISTA-DASLab/fp-quant-6877c186103a21d3a02568ee" rel="nofollow">collection</a>.';return{c(){a=i("p"),a.innerHTML=f},l(p){a=r(p,"P",{"data-svelte-h":!0}),u(a)!=="svelte-y9hp5d"&&(a.innerHTML=f)},m(p,J){l(p,a,J)},p:ct,d(p){p&&n(a)}}}function gt(Z){let a,f,p,J,c,E,M,O='<a href="https://github.com/IST-DASLab/FP-Quant" rel="nofollow">FP-Quant</a> is a family of quantization algorithms tailored for the Blackwell generation of Nvidia GPUs. The goal is to allow for efficient post-training quantization (PTQ) and quantization-aware training (QAT) of LLMs in the <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" rel="nofollow">MXFP4 and NVFP4 data-types</a>.',j,h,tt="Currently, only PTQ with MXFP4 is supported. Models can either be quantized on the fly with <code>quantization_config=FPQuantConfig()</code>:",A,y,V,d,et='or pre-processed with GPTQ for better quality (see <a href="https://github.com/IST-DASLab/FP-Quant" rel="nofollow">FP Format Quantization Harness</a>).',G,w,nt='A <strong>Blackwell-generation GPU is required</strong> to run the kernels. Runtime support for FP-Quant is implemented through the <a href="https://github.com/IST-DASLab/qutlass" rel="nofollow">QuTLASS</a> library and a lightweight PyTorch interface lib <a href="https://github.com/IST-DASLab/FP-Quant/tree/master/inference_lib" rel="nofollow"><code>fp_quant</code></a>. We recommend installing the former <strong>from source</strong> and the latter with  <code>pip install fp_quant</code>.',W,b,lt='Users <strong>without a Blackwell-generation GPU</strong> , can use the method with <code>quantization_config=FPQuantConfig(pseudoquant=True)</code> without having to install <a href="https://github.com/IST-DASLab/qutlass" rel="nofollow">QuTLASS</a>. This would provide no speedups but would fully emulate the effect of quantization.',H,m,R,T,X,g,at='FP-Quant is fully compatible with <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow">torch.compile</a>.',z,U,B,$,I,Q,st="FP-Quant currently performs best for very large batch size processing.",x,C,ot='See <a href="https://github.com/IST-DASLab/qutlass/blob/main/README.md" rel="nofollow">QuTLASS README</a> for speedups.',Y,F,N,L,D;return c=new K({props:{title:"FP-Quant",local:"fp-quant",headingTag:"h1"}}),y=new ut({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEZQUXVhbnRDb25maWclMEFpbXBvcnQlMjB0b3JjaCUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnF3ZW4lMkZRd2VuMy04QiUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RGUFF1YW50Q29uZmlnKCklMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEEp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, FPQuantConfig
<span class="hljs-keyword">import</span> torch

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;qwen/Qwen3-8B&quot;</span>,
    quantization_config=FPQuantConfig(),
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    dtype=torch.bfloat16,
)`,wrap:!1}}),m=new wt({props:{warning:!1,$$slots:{default:[Tt]},$$scope:{ctx:Z}}}),T=new K({props:{title:"torch.compile",local:"torchcompile",headingTag:"h2"}}),U=new ut({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwRlBRdWFudENvbmZpZyUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnF3ZW4lMkZRd2VuMy04QiUyMiUyQyUwQSUyMCUyMCUyMCUyMHF1YW50aXphdGlvbl9jb25maWclM0RGUFF1YW50Q29uZmlnKCklMkMlMEElMjAlMjAlMjAlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMEEpJTBBJTBBbW9kZWwuZm9yd2FyZCUyMCUzRCUyMHRvcmNoLmNvbXBpbGUobW9kZWwuZm9yd2FyZCUyQyUyMG1vZGUlM0QlMjJtYXgtYXV0b3R1bmUlMjIlMkMlMjBmdWxsZ3JhcGglM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, FPQuantConfig

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;qwen/Qwen3-8B&quot;</span>,
    quantization_config=FPQuantConfig(),
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    dtype=torch.bfloat16,
)

model.forward = torch.<span class="hljs-built_in">compile</span>(model.forward, mode=<span class="hljs-string">&quot;max-autotune&quot;</span>, fullgraph=<span class="hljs-literal">True</span>)`,wrap:!1}}),$=new K({props:{title:"Speedups",local:"speedups",headingTag:"h2"}}),F=new bt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/fp_quant.md"}}),{c(){a=i("meta"),f=s(),p=i("p"),J=s(),v(c.$$.fragment),E=s(),M=i("p"),M.innerHTML=O,j=s(),h=i("p"),h.innerHTML=tt,A=s(),v(y.$$.fragment),V=s(),d=i("p"),d.innerHTML=et,G=s(),w=i("p"),w.innerHTML=nt,W=s(),b=i("p"),b.innerHTML=lt,H=s(),v(m.$$.fragment),R=s(),v(T.$$.fragment),X=s(),g=i("p"),g.innerHTML=at,z=s(),v(U.$$.fragment),B=s(),v($.$$.fragment),I=s(),Q=i("p"),Q.textContent=st,x=s(),C=i("p"),C.innerHTML=ot,Y=s(),v(F.$$.fragment),N=s(),L=i("p"),this.h()},l(t){const e=yt("svelte-u9bgzb",document.head);a=r(e,"META",{name:!0,content:!0}),e.forEach(n),f=o(t),p=r(t,"P",{}),rt(p).forEach(n),J=o(t),P(c.$$.fragment,t),E=o(t),M=r(t,"P",{"data-svelte-h":!0}),u(M)!=="svelte-aubki2"&&(M.innerHTML=O),j=o(t),h=r(t,"P",{"data-svelte-h":!0}),u(h)!=="svelte-1n6zck4"&&(h.innerHTML=tt),A=o(t),P(y.$$.fragment,t),V=o(t),d=r(t,"P",{"data-svelte-h":!0}),u(d)!=="svelte-ma9b5t"&&(d.innerHTML=et),G=o(t),w=r(t,"P",{"data-svelte-h":!0}),u(w)!=="svelte-cvh8pv"&&(w.innerHTML=nt),W=o(t),b=r(t,"P",{"data-svelte-h":!0}),u(b)!=="svelte-zd4af"&&(b.innerHTML=lt),H=o(t),P(m.$$.fragment,t),R=o(t),P(T.$$.fragment,t),X=o(t),g=r(t,"P",{"data-svelte-h":!0}),u(g)!=="svelte-69rgxv"&&(g.innerHTML=at),z=o(t),P(U.$$.fragment,t),B=o(t),P($.$$.fragment,t),I=o(t),Q=r(t,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-qcezu2"&&(Q.textContent=st),x=o(t),C=r(t,"P",{"data-svelte-h":!0}),u(C)!=="svelte-1jkywbh"&&(C.innerHTML=ot),Y=o(t),P(F.$$.fragment,t),N=o(t),L=r(t,"P",{}),rt(L).forEach(n),this.h()},h(){pt(a,"name","hf:doc:metadata"),pt(a,"content",Ut)},m(t,e){dt(document.head,a),l(t,f,e),l(t,p,e),l(t,J,e),_(c,t,e),l(t,E,e),l(t,M,e),l(t,j,e),l(t,h,e),l(t,A,e),_(y,t,e),l(t,V,e),l(t,d,e),l(t,G,e),l(t,w,e),l(t,W,e),l(t,b,e),l(t,H,e),_(m,t,e),l(t,R,e),_(T,t,e),l(t,X,e),l(t,g,e),l(t,z,e),_(U,t,e),l(t,B,e),_($,t,e),l(t,I,e),l(t,Q,e),l(t,x,e),l(t,C,e),l(t,Y,e),_(F,t,e),l(t,N,e),l(t,L,e),D=!0},p(t,[e]){const it={};e&2&&(it.$$scope={dirty:e,ctx:t}),m.$set(it)},i(t){D||(q(c.$$.fragment,t),q(y.$$.fragment,t),q(m.$$.fragment,t),q(T.$$.fragment,t),q(U.$$.fragment,t),q($.$$.fragment,t),q(F.$$.fragment,t),D=!0)},o(t){S(c.$$.fragment,t),S(y.$$.fragment,t),S(m.$$.fragment,t),S(T.$$.fragment,t),S(U.$$.fragment,t),S($.$$.fragment,t),S(F.$$.fragment,t),D=!1},d(t){t&&(n(f),n(p),n(J),n(E),n(M),n(j),n(h),n(A),n(V),n(d),n(G),n(w),n(W),n(b),n(H),n(R),n(X),n(g),n(z),n(B),n(I),n(Q),n(x),n(C),n(Y),n(N),n(L)),n(a),k(c,t),k(y,t),k(m,t),k(T,t),k(U,t),k($,t),k(F,t)}}}const Ut='{"title":"FP-Quant","local":"fp-quant","sections":[{"title":"torch.compile","local":"torchcompile","sections":[],"depth":2},{"title":"Speedups","local":"speedups","sections":[],"depth":2}],"depth":1}';function $t(Z){return ft(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _t extends Mt{constructor(a){super(),ht(this,a,$t,gt,mt,{})}}export{_t as component};
