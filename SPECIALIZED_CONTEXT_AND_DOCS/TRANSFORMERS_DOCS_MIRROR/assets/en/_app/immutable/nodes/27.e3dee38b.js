import{s as mn,o as dn,n as Mt}from"../chunks/scheduler.18a86fab.js";import{S as cn,i as Mn,g as i,s as n,r as d,A as un,h as r,f as l,c as a,j as on,u as c,x as p,k as rn,y as yn,a as s,v as M,d as u,t as y,w as h}from"../chunks/index.98837b22.js";import{T as ls}from"../chunks/Tip.77304350.js";import{C as U}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as g,E as hn}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as bn,a as pn}from"../chunks/HfOption.6641485e.js";function Jn(Z){let o,T='Check out the <a href="https://huggingface.co/spaces/m-ric/beam_search_visualizer" rel="nofollow">beam search visualizer</a> to see how beam search works.';return{c(){o=i("p"),o.innerHTML=T},l(m){o=r(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-17rce64"&&(o.innerHTML=T)},m(m,w){s(m,o,w)},p:Mt,d(m){m&&l(o)}}}function Tn(Z){let o,T,m,w='Speculative decoding is also supported in <a href="/docs/transformers/v4.56.2/en/main_classes/pipelines#transformers.Pipeline">Pipeline</a> with the <code>assistant_model</code> parameter.',b,J,j;return o=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0tMS43QiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJIdWdnaW5nRmFjZVRCJTJGU21vbExNLTEuN0IlMjIpJTBBYXNzaXN0YW50X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0tMTM1TSUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBhc3Npc3RhbnRfbW9kZWwlM0Rhc3Npc3RhbnRfbW9kZWwpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJ0h1Z2dpbmclMjBGYWNlJTIwaXMlMjBhbiUyMG9wZW4tc291cmNlJTIwY29tcGFueSUyMHRoYXQlMjBwcm92aWRlcyUyMGElMjBwbGF0Zm9ybSUyMGZvciUyMGRldmVsb3BlcnMlMjB0byUyMGJ1aWxkJTIwYW5kJTIwZGVwbG95JTIwbWFjaGluZSc=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>)
assistant_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-135M&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company that provides a platform for developers to build and deploy machine&#x27;</span>`,wrap:!1}}),J=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBaW1wb3J0JTIwdG9yY2glMEElMEFwaXBlJTIwJTNEJTIwcGlwZWxpbmUoJTBBJTIwJTIwJTIwJTIwJTIydGV4dC1nZW5lcmF0aW9uJTIyJTJDJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0QlMjJtZXRhLWxsYW1hJTJGTGxhbWEtMy4xLThCJTIyJTJDJTBBJTIwJTIwJTIwJTIwYXNzaXN0YW50X21vZGVsJTNEJTIybWV0YS1sbGFtYSUyRkxsYW1hLTMuMi0xQiUyMiUyQyUwQSUyMCUyMCUyMCUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMEEpJTBBcGlwZV9vdXRwdXQlMjAlM0QlMjBwaXBlKCUyMk9uY2UlMjB1cG9uJTIwYSUyMHRpbWUlMkMlMjAlMjIlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUwJTJDJTIwZG9fc2FtcGxlJTNERmFsc2UpJTBBcGlwZV9vdXRwdXQlNUIwJTVEJTVCJTIyZ2VuZXJhdGVkX3RleHQlMjIlNUQ=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">import</span> torch

pipe = pipeline(
    <span class="hljs-string">&quot;text-generation&quot;</span>,
    model=<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B&quot;</span>,
    assistant_model=<span class="hljs-string">&quot;meta-llama/Llama-3.2-1B&quot;</span>,
    dtype=torch.bfloat16
)
pipe_output = pipe(<span class="hljs-string">&quot;Once upon a time, &quot;</span>, max_new_tokens=<span class="hljs-number">50</span>, do_sample=<span class="hljs-literal">False</span>)
pipe_output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;generated_text&quot;</span>]`,wrap:!1}}),{c(){d(o.$$.fragment),T=n(),m=i("p"),m.innerHTML=w,b=n(),d(J.$$.fragment)},l(f){c(o.$$.fragment,f),T=a(f),m=r(f,"P",{"data-svelte-h":!0}),p(m)!=="svelte-1lctnzh"&&(m.innerHTML=w),b=a(f),c(J.$$.fragment,f)},m(f,G){M(o,f,G),s(f,T,G),s(f,m,G),s(f,b,G),M(J,f,G),j=!0},p:Mt,i(f){j||(u(o.$$.fragment,f),u(J.$$.fragment,f),j=!0)},o(f){y(o.$$.fragment,f),y(J.$$.fragment,f),j=!1},d(f){f&&(l(T),l(m),l(b)),h(o,f),h(J,f)}}}function wn(Z){let o,T="Add the <code>temperature</code> parameter to control sampling randomness. For speculative decoding, a lower temperature may improve latency.",m,w,b;return w=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0tMS43QiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJIdWdnaW5nRmFjZVRCJTJGU21vbExNLTEuN0IlMjIpJTBBYXNzaXN0YW50X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0tMTM1TSUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBhc3Npc3RhbnRfbW9kZWwlM0Rhc3Npc3RhbnRfbW9kZWwlMkMlMjBkb19zYW1wbGUlM0RUcnVlJTJDJTIwdGVtcGVyYXR1cmUlM0QwLjUpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJ0h1Z2dpbmclMjBGYWNlJTIwaXMlMjBhbiUyMG9wZW4tc291cmNlJTIwY29tcGFueSUyMHRoYXQlMjBpcyUyMGRlZGljYXRlZCUyMHRvJTIwY3JlYXRpbmclMjBhJTIwYmV0dGVyJTIwd29ybGQlMjB0aHJvdWdoJTIwdGVjaG5vbG9neS4n",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>)
assistant_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-135M&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=<span class="hljs-literal">True</span>, temperature=<span class="hljs-number">0.5</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company that is dedicated to creating a better world through technology.&#x27;</span>`,wrap:!1}}),{c(){o=i("p"),o.innerHTML=T,m=n(),d(w.$$.fragment)},l(J){o=r(J,"P",{"data-svelte-h":!0}),p(o)!=="svelte-u3of1l"&&(o.innerHTML=T),m=a(J),c(w.$$.fragment,J)},m(J,j){s(J,o,j),s(J,m,j),M(w,J,j),b=!0},p:Mt,i(J){b||(u(w.$$.fragment,J),b=!0)},o(J){y(w.$$.fragment,J),b=!1},d(J){J&&(l(o),l(m)),h(w,J)}}}function fn(Z){let o,T,m,w;return o=new pn({props:{id:"spec-decoding",option:"greedy search",$$slots:{default:[Tn]},$$scope:{ctx:Z}}}),m=new pn({props:{id:"spec-decoding",option:"multinomial sampling",$$slots:{default:[wn]},$$scope:{ctx:Z}}}),{c(){d(o.$$.fragment),T=n(),d(m.$$.fragment)},l(b){c(o.$$.fragment,b),T=a(b),c(m.$$.fragment,b)},m(b,J){M(o,b,J),s(b,T,J),M(m,b,J),w=!0},p(b,J){const j={};J&2&&(j.$$scope={dirty:J,ctx:b}),o.$set(j);const f={};J&2&&(f.$$scope={dirty:J,ctx:b}),m.$set(f)},i(b){w||(u(o.$$.fragment,b),u(m.$$.fragment,b),w=!0)},o(b){y(o.$$.fragment,b),y(m.$$.fragment,b),w=!1},d(b){b&&l(T),h(o,b),h(m,b)}}}function gn(Z){let o,T='You can find all custom generation methods by <a href="https://huggingface.co/models?other=custom_generate" rel="nofollow">searching for their custom tag.</a>, <code>custom_generate</code>.';return{c(){o=i("p"),o.innerHTML=T},l(m){o=r(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-1tm23cx"&&(o.innerHTML=T)},m(m,w){s(m,o,w)},p:Mt,d(m){m&&l(o)}}}function Un(Z){let o,T="<code>generate.py</code> must be placed in a folder named <code>custom_generate</code>, and not at the root level of the repository. The file paths for this feature are hardcoded.";return{c(){o=i("p"),o.innerHTML=T},l(m){o=r(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fb1jan"&&(o.innerHTML=T)},m(m,w){s(m,o,w)},p:Mt,d(m){m&&l(o)}}}function jn(Z){let o,T="If you publish a <code>custom_generate</code> repository, your <code>generate</code> implementation can itself define a callable and pass it to <code>model.generate()</code>. This lets you customize the decoding loop while still benefiting from Transformers‚Äô built-in input preparation logic.";return{c(){o=i("p"),o.innerHTML=T},l(m){o=r(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-om9c4o"&&(o.innerHTML=T)},m(m,w){s(m,o,w)},p:Mt,d(m){m&&l(o)}}}function Zn(Z){let o,T,m,w,b,J,j,f="A decoding strategy informs how a model should select the next generated token. There are many types of decoding strategies, and choosing the appropriate one has a significant impact on the quality of the generated text.",G,B,ss="This guide will help you understand the different decoding strategies available in Transformers and how and when to use them.",yt,X,ht,C,ns="These are well established decoding methods, and should be your starting point for text generation tasks.",bt,_,Jt,R,as='Greedy search is the default decoding strategy. It selects the next most likely token at each step. Unless specified in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>, this strategy generates a maximum of 20 new tokens.',Tt,$,os="Greedy search works well for tasks with relatively short outputs where creativity is not a priority. However, it breaks down when generating longer sequences because it begins to repeat itself.",wt,F,ft,N,gt,x,is="Sampling, or multinomial sampling, randomly selects a token based on the probability distribution over the entire model‚Äôs vocabulary (as opposed to the most likely token, as in greedy search). This means every token with a non-zero probability has a chance to be selected. Sampling strategies reduce repetition and can generate more creative and diverse outputs.",Ut,Y,rs="Enable multinomial sampling with <code>do_sample=True</code> and <code>num_beams=1</code>.",jt,E,Zt,H,Gt,z,ps="Beam search keeps track of several generated sequences (beams) at each time step. After a certain number of steps, it selects the sequence with the highest <em>overall</em> probability. Unlike greedy search, this strategy can ‚Äúlook ahead‚Äù and pick a sequence with a higher probability overall even if the initial tokens have a lower probability. It is best suited for input-grounded tasks, like describing an image or speech recognition. You can also use <code>do_sample=True</code> with beam search to sample at each step, but beam search will still greedily prune out low probability sequences between steps.",kt,k,vt,Q,ms="Enable beam search with the <code>num_beams</code> parameter (should be greater than 1 otherwise it‚Äôs equivalent to greedy search).",Wt,S,It,A,Vt,L,ds="Advanced decoding methods aim at either tackling specific generation quality issues (e.g. repetition) or at improving the generation throughput in certain situations. These techniques are more complex, and may not work correctly with all models.",Bt,q,Xt,D,cs='<a href="https://hf.co/papers/2211.17192" rel="nofollow">Speculative</a> or assistive decoding isn‚Äôt a search or sampling strategy. Instead, speculative decoding adds a second smaller model to generate candidate tokens. The main model verifies the candidate tokens in a single <code>forward</code> pass, which speeds up the decoding process overall. This method is especially useful for LLMs where it can be more costly and slower to generate tokens. Refer to the <a href="./llm_optims#speculative-decoding">speculative decoding</a> guide to learn more.',Ct,K,Ms="Currently, only greedy search and multinomial sampling are supported with speculative decoding. Batched inputs aren‚Äôt supported either.",_t,P,us="Enable speculative decoding with the <code>assistant_model</code> parameter. You‚Äôll notice the fastest speed up with an assistant model that is much smaller than the main model. Add <code>do_sample=True</code> to enable token validation with resampling.",Rt,v,$t,O,Ft,ee,ys='<a href="./llm_optims#prompt-lookup-decoding">Prompt lookup decoding</a> is a variant of speculative decoding that uses overlapping n-grams as the candidate tokens. It works well for input-grounded tasks such as summarization. Refer to the <a href="./llm_optims#prompt-lookup-decoding">prompt lookup decoding</a> guide to learn more.',Nt,te,hs="Enable prompt lookup decoding with the <code>prompt_lookup_num_tokens</code> parameter.",xt,le,Yt,se,Et,ne,bs="Early exiting uses the earlier hidden states from the language modeling head as inputs, effectively skipping layers to yield a lower quality output. The lower quality output is used as the assistant output and self-speculation is applied to fix the output using the remaining layers. The final generated result from this self-speculative method is the same (or has the same distribution) as the original models generation.",Ht,ae,Js="The assistant model is also part of the target model, so the caches and weights can be shared, resulting in lower memory requirements.",zt,oe,Ts='For a model trained with early exit, pass <code>assistant_early_exit</code> to <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>.',Qt,ie,St,re,At,pe,ws='Universal assisted decoding (UAD) enables the main and assistant models to use different tokenizers. The main models input tokens are re-encoded into assistant model tokens. Candidate tokens are generated in the assistant encoding which are re-encoded into the main model candidate tokens. The candidate tokens are verified as explained in <a href="#speculative-decoding">speculative decoding</a>.',Lt,me,fs="Re-encoding involves decoding token ids into text and encoding the text with a different tokenizer. To prevent tokenization discrepancies during re-encoding, UAD finds the longest common sub-sequence between the source and target encodings to ensure the new tokens include the correct prompt suffix.",qt,de,gs='Add the <code>tokenizer</code> and <code>assistant_tokenizer</code> parameters to <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> to enable UAD.',Dt,ce,Kt,Me,Pt,ue,Us='<a href="https://hf.co/papers/1610.02424" rel="nofollow">Diverse beam search</a> is a variant of beam search that produces more diverse output candidates to choose from. This strategy measures the dissimilarity of sequences and a penalty is applied if sequences are too similar. To avoid high computation costs, the number of beams is divided into groups.',Ot,ye,js="Enable diverse beam search with the <code>num_beams</code>, <code>num_beam_groups</code> and <code>diversity_penalty</code> parameters (the <code>num_beams</code> parameter should be divisible by <code>num_beam_groups</code>).",el,he,tl,be,ll,Je,Zs="Custom generation methods enable specialized behavior such as:",sl,Te,Gs="<li>have the model continue thinking if it is uncertain;</li> <li>roll back generation if the model gets stuck;</li> <li>handle special tokens with custom logic;</li> <li>use specialized KV caches;</li>",nl,we,ks='We enable custom generation methods through model repositories, assuming a specific model tag and file structure (see subsection below). This feature is an extension of <a href="./models.md#custom-models">custom modeling code</a> and, like such, requires setting <code>trust_remote_code=True</code>.',al,fe,vs="If a model repository holds a custom generation method, the easiest way to try it out is to load the model and generate with it:",ol,ge,il,Ue,Ws='Model repositories with custom generation methods have a special property: their generation method can be loaded from <strong>any</strong> model through <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>‚Äôs <code>custom_generate</code> argument. This means anyone can create and share their custom generation method to potentially work with any Transformers model, without requiring users to install additional Python packages.',rl,je,pl,Ze,Is='You should read the <code>README.md</code> file of the repository containing the custom generation strategy to see what the new arguments and output type differences are, if they exist. Otherwise, you can assume it works like the base <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> method.',ml,W,dl,Ge,Vs='Consider the Hub repository <a href="https://huggingface.co/transformers-community/custom_generate_example" rel="nofollow">transformers-community/custom_generate_example</a> as an example. The <code>README.md</code> states that it has an additional input argument, <code>left_padding</code>, which adds a number of padding tokens before the prompt.',cl,ke,Ml,ve,Bs='If the custom method has pinned Python requirements that your environment doesn‚Äôt meet, you‚Äôll get an exception about missing requirements. For instance, <a href="https://huggingface.co/transformers-community/custom_generate_bad_requirements" rel="nofollow">transformers-community/custom_generate_bad_requirements</a> has an impossible set of requirements defined in its <code>custom_generate/requirements.txt</code> file, and you‚Äôll see the error message below if you try to run it.',ul,We,yl,Ie,Xs="Updating your Python requirements accordingly will remove this error message.",hl,Ve,bl,Be,Cs='To create a new generation method, you need to create a new <a href="https://huggingface.co/new" rel="nofollow"><strong>Model</strong></a> repository and push a few files into it.',Jl,Xe,_s="<li>The model you‚Äôve designed your generation method with.</li> <li><code>custom_generate/generate.py</code>, which contains all the logic for your custom generation method.</li> <li><code>custom_generate/requirements.txt</code>, used to optionally add new Python requirements and/or lock specific versions to correctly use your method.</li> <li><code>README.md</code>, where you should add the <code>custom_generate</code> tag and document any new arguments or output type differences of your custom method here.</li>",Tl,Ce,Rs="After you‚Äôve added all required files, your repository should look like this",wl,_e,fl,Re,gl,$e,$s="The starting point for your custom generation method is a model repository just like any other. The model to add to this repository should be the model you‚Äôve designed your method with, and it is meant to be part of a working self-contained model-generate pair. When the model in this repository is loaded, your custom generation method will override <code>generate</code>. Don‚Äôt worry ‚Äî your generation method can still be loaded with any other Transformers model, as explained in the section above.",Ul,Fe,Fs="If you simply want to copy an existing model, you can do",jl,Ne,Zl,xe,Gl,Ye,Ns='This is the core of your generation method. It <em>must</em> contain a method named <code>generate</code>, and this method <em>must</em> contain a <code>model</code> argument as its first argument. <code>model</code> is the model instance, which means you have access to all attributes and methods in the model, including the ones defined in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin">GenerationMixin</a> (like the base <code>generate</code> method).',kl,I,vl,Ee,xs='Under the hood, when the base <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> method is called with a <code>custom_generate</code> argument, it first checks its Python requirements (if any), then locates the custom <code>generate</code> method in <code>generate.py</code>, and finally calls the custom <code>generate</code>. All received arguments and <code>model</code> are forwarded to your custom <code>generate</code> method, with the exception of the arguments used to trigger the custom generation (<code>trust_remote_code</code> and <code>custom_generate</code>).',Wl,He,Ys="This means your <code>generate</code> can have a mix of original and custom arguments (as well as a different output type) as shown below.",Il,ze,Vl,Qe,Es="Follow the recommended practices below to ensure your custom generation method works as expected.",Bl,Se,Hs='<li>Feel free to reuse the logic for validation and input preparation in the original <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>.</li> <li>Pin the <code>transformers</code> version in the requirements if you use any private method/attribute in <code>model</code>.</li> <li>Consider adding model validation, input validation, or even a separate test file to help users sanity-check your code in their environment.</li>',Xl,Ae,zs="Your custom <code>generate</code> method can relative import code from the <code>custom_generate</code> folder. For example, if you have a <code>utils.py</code> file, you can import it like this:",Cl,Le,_l,qe,Qs="Only relative imports from the same-level <code>custom_generate</code> folder are supported. Parent/sibling folder imports are not valid. The <code>custom_generate</code> argument also works locally with any directory that contains a <code>custom_generate</code> structure. This is the recommended workflow for developing your custom generation method.",Rl,De,$l,Ke,Ss="You can optionally specify additional Python requirements in a <code>requirements.txt</code> file inside the <code>custom_generate</code> folder. These are checked at runtime and an exception will be thrown if they‚Äôre missing, nudging users to update their environment accordingly.",Fl,Pe,Nl,Oe,As='The root level <code>README.md</code> in the model repository usually describes the model therein. However, since the focus of the repository is the custom generation method, we highly recommend to shift its focus towards describing the custom generation method. In addition to a description of the method, we recommend documenting any input and/or output differences to the original <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>. This way, users can focus on what‚Äôs new, and rely on Transformers docs for generic implementation details.',xl,et,Ls="For discoverability, we highly recommend you to add the <code>custom_generate</code> tag to your repository. To do so, the top of your <code>README.md</code> file should look like the example below. After you push the file, you should see the tag in your repository!",Yl,tt,El,lt,qs="Recommended practices:",Hl,st,Ds='<li>Document input and output differences in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>.</li> <li>Add self-contained examples to enable quick experimentation.</li> <li>Describe soft-requirements such as if the method only works well with a certain family of models.</li>',zl,nt,Ql,at,Ks='If you‚Äôre adding a new decoding loop, you might want to preserve the input preparation present in <code>generate</code> (batch expansion, attention masks, logits processors, stopping criteria, etc.). You can also pass a <strong>callable</strong> to <code>custom_generate</code> to reuse <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>‚Äôs full preparation pipeline while overriding only the decoding loop.',Sl,ot,Al,V,Ll,it,ql,rt,Ps='You can find all custom generation methods by <a href="https://huggingface.co/models?other=custom_generate" rel="nofollow">searching for their custom tag.</a>, <code>custom_generate</code>. In addition to the tag, we curate two collections of <code>custom_generate</code> methods:',Dl,pt,Os='<li><a href="https://huggingface.co/collections/transformers-community/custom-generation-methods-community-6888fb1da0efbc592d3a8ab6" rel="nofollow">Custom generation methods - Community</a> ‚Äî a collection of powerful methods contributed by the community;</li> <li><a href="https://huggingface.co/collections/transformers-community/custom-generation-methods-tutorials-6823589657a94940ea02cfec" rel="nofollow">Custom generation methods - Tutorials</a> ‚Äî a collection of reference implementations for methods that previously were part of <code>transformers</code>, as well as tutorials for <code>custom_generate</code>.</li>',Kl,mt,Pl,dt,en='Read the <a href="https://huggingface.co/blog/how-to-generate" rel="nofollow">How to generate text: using different decoding methods for language generation with Transformers</a> blog post for an explanation of how common decoding strategies work.',Ol,ct,es,ut,ts;return b=new g({props:{title:"Generation strategies",local:"generation-strategies",headingTag:"h1"}}),X=new g({props:{title:"Basic decoding methods",local:"basic-decoding-methods",headingTag:"h2"}}),_=new g({props:{title:"Greedy search",local:"greedy-search",headingTag:"h3"}}),F=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTIzJTIwZXhwbGljaXRseSUyMHNldCUyMHRvJTIwZGVmYXVsdCUyMGxlbmd0aCUyMGJlY2F1c2UlMjBMbGFtYTIlMjBnZW5lcmF0aW9uJTIwbGVuZ3RoJTIwaXMlMjA0MDk2JTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dHB1dHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEEnSHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIwdGhhdCUyMHByb3ZpZGVzJTIwYSUyMHN1aXRlJTIwb2YlMjB0b29scyUyMGFuZCUyMHNlcnZpY2VzJTIwZm9yJTIwYnVpbGRpbmclMkMlMjBkZXBsb3lpbmclMkMlMjBhbmQlMjBtYWludGFpbmluZyUyMG5hdHVyYWwlMjBsYW5ndWFnZSUyMHByb2Nlc3Npbmcn",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device

device = infer_device()

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, dtype=torch.float16).to(device)
<span class="hljs-comment"># explicitly set to default length because Llama2 generation length is 4096</span>
outputs = model.generate(**inputs, max_new_tokens=<span class="hljs-number">20</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing&#x27;</span>`,wrap:!1}}),N=new g({props:{title:"Sampling",local:"sampling",headingTag:"h3"}}),E=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTIzJTIwZXhwbGljaXRseSUyMHNldCUyMHRvJTIwMTAwJTIwYmVjYXVzZSUyMExsYW1hMiUyMGdlbmVyYXRpb24lMjBsZW5ndGglMjBpcyUyMDQwOTYlMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUwJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSUyQyUyMG51bV9iZWFtcyUzRDEpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJ0h1Z2dpbmclMjBGYWNlJTIwaXMlMjBhbiUyMG9wZW4tc291cmNlJTIwY29tcGFueSUyMCVGMCU5RiVBNCU5NyU1Q25XZSUyMGFyZSUyMG9wZW4tc291cmNlJTIwYW5kJTIwYmVsaWV2ZSUyMHRoYXQlMjBvcGVuLXNvdXJjZSUyMGlzJTIwdGhlJTIwYmVzdCUyMHdheSUyMHRvJTIwYnVpbGQlMjB0ZWNobm9sb2d5LiUyME91ciUyMG1pc3Npb24lMjBpcyUyMHRvJTIwbWFrZSUyMEFJJTIwYWNjZXNzaWJsZSUyMHRvJTIwZXZlcnlvbmUlMkMlMjBhbmQlMjB3ZSUyMGJlbGlldmUlMjB0aGF0JTIwb3Blbi1zb3VyY2UlMjBpcyUyMHRoZSUyMGJlc3QlMjB3YXklMjB0byUyMGFjaGlldmUlMjB0aGF0Lic=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device

device = infer_device()

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, dtype=torch.float16).to(device)
<span class="hljs-comment"># explicitly set to 100 because Llama2 generation length is 4096</span>
outputs = model.generate(**inputs, max_new_tokens=<span class="hljs-number">50</span>, do_sample=<span class="hljs-literal">True</span>, num_beams=<span class="hljs-number">1</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company ü§ó\\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.&#x27;</span>`,wrap:!1}}),H=new g({props:{title:"Beam search",local:"beam-search",headingTag:"h3"}}),k=new ls({props:{warning:!1,$$slots:{default:[Jn]},$$scope:{ctx:Z}}}),S=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTIzJTIwZXhwbGljaXRseSUyMHNldCUyMHRvJTIwMTAwJTIwYmVjYXVzZSUyMExsYW1hMiUyMGdlbmVyYXRpb24lMjBsZW5ndGglMjBpcyUyMDQwOTYlMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUwJTJDJTIwbnVtX2JlYW1zJTNEMiklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dHB1dHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjIlNUInSHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIwdGhhdCUyMGRldmVsb3BzJTIwYW5kJTIwbWFpbnRhaW5zJTIwdGhlJTIwSHVnZ2luZyUyMEZhY2UlMjBwbGF0Zm9ybSUyQyUyMHdoaWNoJTIwaXMlMjBhJTIwY29sbGVjdGlvbiUyMG9mJTIwdG9vbHMlMjBhbmQlMjBsaWJyYXJpZXMlMjBmb3IlMjBidWlsZGluZyUyMGFuZCUyMGRlcGxveWluZyUyMG5hdHVyYWwlMjBsYW5ndWFnZSUyMHByb2Nlc3NpbmclMjAoTkxQKSUyMG1vZGVscy4lMjBIdWdnaW5nJTIwRmFjZSUyMHdhcyUyMGZvdW5kZWQlMjBpbiUyMDIwMTglMjBieSUyMFRob21hcyUyMFdvbGYnJTVEJTIy",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device

device = infer_device()

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, dtype=torch.float16).to(device)
<span class="hljs-comment"># explicitly set to 100 because Llama2 generation length is 4096</span>
outputs = model.generate(**inputs, max_new_tokens=<span class="hljs-number">50</span>, num_beams=<span class="hljs-number">2</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;[&#x27;Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf&#x27;]&quot;</span>`,wrap:!1}}),A=new g({props:{title:"Advanced decoding methods",local:"advanced-decoding-methods",headingTag:"h2"}}),q=new g({props:{title:"Speculative decoding",local:"speculative-decoding",headingTag:"h3"}}),v=new bn({props:{id:"spec-decoding",options:["greedy search","multinomial sampling"],$$slots:{default:[fn]},$$scope:{ctx:Z}}}),O=new g({props:{title:"Prompt lookup decoding",local:"prompt-lookup-decoding",headingTag:"h4"}}),le=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJIdWdnaW5nRmFjZVRCJTJGU21vbExNLTEuN0IlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIySHVnZ2luZ0ZhY2VUQiUyRlNtb2xMTS0xLjdCJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBYXNzaXN0YW50X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMkh1Z2dpbmdGYWNlVEIlMkZTbW9sTE0tMTM1TSUyMiUyQyUyMGR0eXBlJTNEdG9yY2guZmxvYXQxNikudG8oZGV2aWNlKSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJIdWdnaW5nJTIwRmFjZSUyMGlzJTIwYW4lMjBvcGVuLXNvdXJjZSUyMGNvbXBhbnklMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwYXNzaXN0YW50X21vZGVsJTNEYXNzaXN0YW50X21vZGVsJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCUyQyUyMHByb21wdF9sb29rdXBfbnVtX3Rva2VucyUzRDUpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJ0h1Z2dpbmclMjBGYWNlJTIwaXMlMjBhbiUyMG9wZW4tc291cmNlJTIwY29tcGFueSUyMHRoYXQlMjBwcm92aWRlcyUyMGElMjBwbGF0Zm9ybSUyMGZvciUyMGRldmVsb3BlcnMlMjB0byUyMGJ1aWxkJTIwYW5kJTIwZGVwbG95JTIwbWFjaGluZSUyMGxlYXJuaW5nJTIwbW9kZWxzLiUyMEl0JTIwb2ZmZXJzJTIwYSUyMHZhcmlldHklMjBvZiUyMHRvb2xzJw==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device

device = infer_device()

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-1.7B&quot;</span>, dtype=torch.float16).to(device)
assistant_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;HuggingFaceTB/SmolLM-135M&quot;</span>, dtype=torch.float16).to(device)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=<span class="hljs-number">20</span>, prompt_lookup_num_tokens=<span class="hljs-number">5</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools&#x27;</span>`,wrap:!1}}),se=new g({props:{title:"Self-speculative decoding",local:"self-speculative-decoding",headingTag:"h3"}}),ie=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFsaWNlJTIwYW5kJTIwQm9iJTIyJTBBY2hlY2twb2ludCUyMCUzRCUyMCUyMmZhY2Vib29rJTJGbGF5ZXJza2lwLWxsYW1hMy4yLTFCJTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIocHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGFzc2lzdGFudF9lYXJseV9leGl0JTNENCUyQyUyMGRvX3NhbXBsZSUzREZhbHNlJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dHB1dHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

prompt = <span class="hljs-string">&quot;Alice and Bob&quot;</span>
checkpoint = <span class="hljs-string">&quot;facebook/layerskip-llama3.2-1B&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs, assistant_early_exit=<span class="hljs-number">4</span>, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),re=new g({props:{title:"Universal assisted decoding",local:"universal-assisted-decoding",headingTag:"h4"}}),ce=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXByb21wdCUyMCUzRCUyMCUyMkFsaWNlJTIwYW5kJTIwQm9iJTIyJTBBJTBBYXNzaXN0YW50X3Rva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmRvdWJsZTclMkZ2aWN1bmEtNjhtJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmdlbW1hLTItOWIlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGZ2VtbWEtMi05YiUyMiklMEFhc3Npc3RhbnRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZG91YmxlNyUyRnZpY3VuYS02OG0lMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwYXNzaXN0YW50X21vZGVsJTNEYXNzaXN0YW50X21vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyJTJDJTIwYXNzaXN0YW50X3Rva2VuaXplciUzRGFzc2lzdGFudF90b2tlbml6ZXIpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXRwdXRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTBBJTVCJ0FsaWNlJTIwYW5kJTIwQm9iJTIwYXJlJTIwc2l0dGluZyUyMGluJTIwYSUyMGJhci4lMjBBbGljZSUyMGlzJTIwZHJpbmtpbmclMjBhJTIwYmVlciUyMGFuZCUyMEJvYiUyMGlzJTIwZHJpbmtpbmclMjBhJyU1RA==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

prompt = <span class="hljs-string">&quot;Alice and Bob&quot;</span>

assistant_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;double7/vicuna-68m&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-9b&quot;</span>)
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google/gemma-2-9b&quot;</span>)
assistant_model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;double7/vicuna-68m&quot;</span>)
outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a&#x27;</span>]`,wrap:!1}}),Me=new g({props:{title:"Diverse beam search",local:"diverse-beam-search",headingTag:"h3"}}),he=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySHVnZ2luZyUyMEZhY2UlMjBpcyUyMGFuJTIwb3Blbi1zb3VyY2UlMjBjb21wYW55JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTIzJTIwZXhwbGljaXRseSUyMHNldCUyMHRvJTIwMTAwJTIwYmVjYXVzZSUyMExsYW1hMiUyMGdlbmVyYXRpb24lMjBsZW5ndGglMjBpcyUyMDQwOTYlMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUwJTJDJTIwbnVtX2JlYW1zJTNENiUyQyUyMG51bV9iZWFtX2dyb3VwcyUzRDMlMkMlMjBkaXZlcnNpdHlfcGVuYWx0eSUzRDEuMCUyQyUyMGRvX3NhbXBsZSUzREZhbHNlKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUob3V0cHV0cyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQSdIdWdnaW5nJTIwRmFjZSUyMGlzJTIwYW4lMjBvcGVuLXNvdXJjZSUyMGNvbXBhbnklMjAlRjAlOUYlQTQlOTclNUNuV2UlMjBhcmUlMjBhbiUyMG9wZW4tc291cmNlJTIwY29tcGFueS4lMjBPdXIlMjBtaXNzaW9uJTIwaXMlMjB0byUyMGRlbW9jcmF0aXplJTIwQUklMjBhbmQlMjBtYWtlJTIwaXQlMjBhY2Nlc3NpYmxlJTIwdG8lMjBldmVyeW9uZS4lMjBXZSUyMGJlbGlldmUlMjB0aGF0JTIwQUklMjBzaG91bGQlMjBiZSUyMHVzZWQlMjBmb3IlMjB0aGUlMjBiZW5lZml0JTIwb2YlMjBodW1hbml0eSUyQyUyMG5vdCUyMGZvciUyMHRoZSUyMGJlbmVmaXQlMjBvZiUyMGEn",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, infer_device

device = infer_device()

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hugging Face is an open-source company&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>, dtype=torch.float16).to(device)
<span class="hljs-comment"># explicitly set to 100 because Llama2 generation length is 4096</span>
outputs = model.generate(**inputs, max_new_tokens=<span class="hljs-number">50</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>, diversity_penalty=<span class="hljs-number">1.0</span>, do_sample=<span class="hljs-literal">False</span>)
tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Hugging Face is an open-source company ü§ó\\nWe are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a&#x27;</span>`,wrap:!1}}),be=new g({props:{title:"Custom generation methods",local:"custom-generation-methods",headingTag:"h2"}}),ge=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQSUyMyUyMCU2MHRyYW5zZm9ybWVycy1jb21tdW5pdHklMkZjdXN0b21fZ2VuZXJhdGVfZXhhbXBsZSU2MCUyMGhvbGRzJTIwYSUyMGNvcHklMjBvZiUyMCU2MFF3ZW4lMkZRd2VuMi41LTAuNUItSW5zdHJ1Y3QlNjAlMkMlMjBidXQlMEElMjMlMjB3aXRoJTIwY3VzdG9tJTIwZ2VuZXJhdGlvbiUyMGNvZGUlMjAtJTNFJTIwY2FsbGluZyUyMCU2MGdlbmVyYXRlJTYwJTIwdXNlcyUyMHRoZSUyMGN1c3RvbSUyMGdlbmVyYXRpb24lMjBtZXRob2QhJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIydHJhbnNmb3JtZXJzLWNvbW11bml0eSUyRmN1c3RvbV9nZW5lcmF0ZV9leGFtcGxlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVycy1jb21tdW5pdHklMkZjdXN0b21fZ2VuZXJhdGVfZXhhbXBsZSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwdHJ1c3RfcmVtb3RlX2NvZGUlM0RUcnVlJTBBKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJUaGUlMjBxdWljayUyMGJyb3duJTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUyMyUyMFRoZSUyMGN1c3RvbSUyMGdlbmVyYXRpb24lMjBtZXRob2QlMjBpcyUyMGElMjBtaW5pbWFsJTIwZ3JlZWR5JTIwZGVjb2RpbmclMjBpbXBsZW1lbnRhdGlvbi4lMjBJdCUyMGFsc28lMjBwcmludHMlMjBhJTIwY3VzdG9tJTIwbWVzc2FnZSUyMGF0JTIwcnVuJTIwdGltZS4lMEFnZW5fb3V0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBJTIzJTIweW91JTIwc2hvdWxkJTIwbm93JTIwc2VlJTIwaXRzJTIwY3VzdG9tJTIwbWVzc2FnZSUyQyUyMCUyMiVFMiU5QyVBOCUyMHVzaW5nJTIwYSUyMGN1c3RvbSUyMGdlbmVyYXRpb24lMjBtZXRob2QlMjAlRTIlOUMlQTglMjIlMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbl9vdXQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkpJTBBJ1RoZSUyMHF1aWNrJTIwYnJvd24lMjBmb3glMjBqdW1wcyUyMG92ZXIlMjBhJTIwbGF6eSUyMGRvZyUyQyUyMGFuZCUyMHRoZSUyMGRvZyUyMGlzJTIwYSUyMHR5cGUlMjBvZiUyMGFuaW1hbC4lMjBJcyc=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-comment"># \`transformers-community/custom_generate_example\` holds a copy of \`Qwen/Qwen2.5-0.5B-Instruct\`, but</span>
<span class="hljs-comment"># with custom generation code -&gt; calling \`generate\` uses the custom generation method!</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;transformers-community/custom_generate_example&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;transformers-community/custom_generate_example&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>
)

inputs = tokenizer([<span class="hljs-string">&quot;The quick brown&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-comment"># The custom generation method is a minimal greedy decoding implementation. It also prints a custom message at run time.</span>
gen_out = model.generate(**inputs)
<span class="hljs-comment"># you should now see its custom message, &quot;‚ú® using a custom generation method ‚ú®&quot;</span>
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(gen_out, skip_special_tokens=<span class="hljs-literal">True</span>))
<span class="hljs-string">&#x27;The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is&#x27;</span>`,wrap:!1}}),je=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlF3ZW4lMkZRd2VuMi41LTAuNUItSW5zdHJ1Y3QlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyUXdlbiUyRlF3ZW4yLjUtMC41Qi1JbnN0cnVjdCUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJUaGUlMjBxdWljayUyMGJyb3duJTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUyMyUyMCU2MGN1c3RvbV9nZW5lcmF0ZSU2MCUyMHJlcGxhY2VzJTIwdGhlJTIwb3JpZ2luYWwlMjAlNjBnZW5lcmF0ZSU2MCUyMGJ5JTIwdGhlJTIwY3VzdG9tJTIwZ2VuZXJhdGlvbiUyMG1ldGhvZCUyMGRlZmluZWQlMjBpbiUwQSUyMyUyMCU2MHRyYW5zZm9ybWVycy1jb21tdW5pdHklMkZjdXN0b21fZ2VuZXJhdGVfZXhhbXBsZSU2MCUwQWdlbl9vdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGN1c3RvbV9nZW5lcmF0ZSUzRCUyMnRyYW5zZm9ybWVycy1jb21tdW5pdHklMkZjdXN0b21fZ2VuZXJhdGVfZXhhbXBsZSUyMiUyQyUyMHRydXN0X3JlbW90ZV9jb2RlJTNEVHJ1ZSklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbl9vdXQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEKSUwQSdUaGUlMjBxdWljayUyMGJyb3duJTIwZm94JTIwanVtcHMlMjBvdmVyJTIwYSUyMGxhenklMjBkb2clMkMlMjBhbmQlMjB0aGUlMjBkb2clMjBpcyUyMGElMjB0eXBlJTIwb2YlMjBhbmltYWwuJTIwSXMn",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)

inputs = tokenizer([<span class="hljs-string">&quot;The quick brown&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
<span class="hljs-comment"># \`custom_generate\` replaces the original \`generate\` by the custom generation method defined in</span>
<span class="hljs-comment"># \`transformers-community/custom_generate_example\`</span>
gen_out = model.generate(**inputs, custom_generate=<span class="hljs-string">&quot;transformers-community/custom_generate_example&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(gen_out, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
<span class="hljs-string">&#x27;The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is&#x27;</span>`,wrap:!1}}),W=new ls({props:{warning:!1,$$slots:{default:[gn]},$$scope:{ctx:Z}}}),ke=new U({props:{code:"Z2VuX291dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCUwQSUyMCUyMCUyMCUyMCoqaW5wdXRzJTJDJTIwY3VzdG9tX2dlbmVyYXRlJTNEJTIydHJhbnNmb3JtZXJzLWNvbW11bml0eSUyRmN1c3RvbV9nZW5lcmF0ZV9leGFtcGxlJTIyJTJDJTIwdHJ1c3RfcmVtb3RlX2NvZGUlM0RUcnVlJTJDJTIwbGVmdF9wYWRkaW5nJTNENSUwQSklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbl9vdXQpJTVCMCU1RCklMEEnJTNDJTdDZW5kb2Z0ZXh0JTdDJTNFJTNDJTdDZW5kb2Z0ZXh0JTdDJTNFJTNDJTdDZW5kb2Z0ZXh0JTdDJTNFJTNDJTdDZW5kb2Z0ZXh0JTdDJTNFJTNDJTdDZW5kb2Z0ZXh0JTdDJTNFVGhlJTIwcXVpY2slMjBicm93biUyMGZveCUyMGp1bXBzJTIwb3ZlciUyMHRoZSUyMGxhenklMjBkb2cuJTVDbiU1Q25UaGUlMjBzZW50ZW5jZSUyMCUyMlRoZSUyMHF1aWNrJw==",highlighted:`gen_out = model.generate(
    **inputs, custom_generate=<span class="hljs-string">&quot;transformers-community/custom_generate_example&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>, left_padding=<span class="hljs-number">5</span>
)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(gen_out)[<span class="hljs-number">0</span>])
<span class="hljs-string">&#x27;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;The quick brown fox jumps over the lazy dog.\\n\\nThe sentence &quot;The quick&#x27;</span>`,wrap:!1}}),We=new U({props:{code:"SW1wb3J0RXJyb3IlM0ElMjBNaXNzaW5nJTIwcmVxdWlyZW1lbnRzJTIwaW4lMjB5b3VyJTIwbG9jYWwlMjBlbnZpcm9ubWVudCUyMGZvciUyMCU2MHRyYW5zZm9ybWVycy1jb21tdW5pdHklMkZjdXN0b21fZ2VuZXJhdGVfYmFkX3JlcXVpcmVtZW50cyU2MCUzQSUwQWZvbyUyMChpbnN0YWxsZWQlM0ElMjBOb25lKSUwQWJhciUzRCUzRDAuMC4wJTIwKGluc3RhbGxlZCUzQSUyME5vbmUpJTBBdG9yY2glM0UlM0Q5OS4wJTIwKGluc3RhbGxlZCUzQSUyMDIuNi4wKQ==",highlighted:`<span class="hljs-attribute">ImportError</span>: Missing requirements in your local environment for \`transformers-community/custom_generate_bad_requirements\`:
<span class="hljs-attribute">foo</span> (installed: None)
<span class="hljs-attribute">bar</span>==<span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span> (installed: None)
<span class="hljs-attribute">torch</span>&gt;=<span class="hljs-number">99</span>.<span class="hljs-number">0</span> (installed: <span class="hljs-number">2</span>.<span class="hljs-number">6</span>.<span class="hljs-number">0</span>)`,wrap:!1}}),Ve=new g({props:{title:"Creating a custom generation method",local:"creating-a-custom-generation-method",headingTag:"h3"}}),_e=new U({props:{code:"eW91cl9yZXBvJTJGJTBBJUUyJTk0JTlDJUUyJTk0JTgwJUUyJTk0JTgwJTIwUkVBRE1FLm1kJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwaW5jbHVkZSUyMHRoZSUyMCdjdXN0b21fZ2VuZXJhdGUnJTIwdGFnJTBBJUUyJTk0JTlDJUUyJTk0JTgwJUUyJTk0JTgwJTIwY29uZmlnLmpzb24lMEElRTIlOTQlOUMlRTIlOTQlODAlRTIlOTQlODAlMjAuLi4lMEElRTIlOTQlOTQlRTIlOTQlODAlRTIlOTQlODAlMjBjdXN0b21fZ2VuZXJhdGUlMkYlMEElMjAlMjAlMjAlMjAlRTIlOTQlOUMlRTIlOTQlODAlRTIlOTQlODAlMjBnZW5lcmF0ZS5weSUwQSUyMCUyMCUyMCUyMCVFMiU5NCU5NCVFMiU5NCU4MCVFMiU5NCU4MCUyMHJlcXVpcmVtZW50cy50eHQ=",highlighted:`your_repo/
‚îú‚îÄ‚îÄ README<span class="hljs-variable">.md</span>          # <span class="hljs-keyword">include</span> the &#x27;custom_generate&#x27; tag
‚îú‚îÄ‚îÄ <span class="hljs-keyword">config</span><span class="hljs-variable">.json</span>
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ custom_generate/
    ‚îú‚îÄ‚îÄ <span class="hljs-keyword">generate</span><span class="hljs-variable">.py</span>
    ‚îî‚îÄ‚îÄ requirements<span class="hljs-variable">.txt</span>`,wrap:!1}}),Re=new g({props:{title:"Adding the base model",local:"adding-the-base-model",headingTag:"h4"}}),Ne=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnNvdXJjZSUyRm1vZGVsX3JlcG8lMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyc291cmNlJTJGbW9kZWxfcmVwbyUyMiklMEF0b2tlbml6ZXIuc2F2ZV9wcmV0cmFpbmVkKCUyMnlvdXIlMkZnZW5lcmF0aW9uX21ldGhvZCUyMiUyQyUyMHB1c2hfdG9faHViJTNEVHJ1ZSklMEFtb2RlbC5zYXZlX3ByZXRyYWluZWQoJTIyeW91ciUyRmdlbmVyYXRpb25fbWV0aG9kJTIyJTJDJTIwcHVzaF90b19odWIlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;source/model_repo&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;source/model_repo&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;your/generation_method&quot;</span>, push_to_hub=<span class="hljs-literal">True</span>)
model.save_pretrained(<span class="hljs-string">&quot;your/generation_method&quot;</span>, push_to_hub=<span class="hljs-literal">True</span>)`,wrap:!1}}),xe=new g({props:{title:"generate.py",local:"generatepy",headingTag:"h4"}}),I=new ls({props:{warning:!0,$$slots:{default:[Un]},$$scope:{ctx:Z}}}),ze=new U({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">model, input_ids, generation_config=<span class="hljs-literal">None</span>, left_padding=<span class="hljs-literal">None</span>, **kwargs</span>):
    generation_config = generation_config <span class="hljs-keyword">or</span> model.generation_config  <span class="hljs-comment"># default to the model generation config</span>
    cur_length = input_ids.shape[<span class="hljs-number">1</span>]
    max_length = generation_config.max_length <span class="hljs-keyword">or</span> cur_length + generation_config.max_new_tokens

    <span class="hljs-comment"># Example of custom argument: add \`left_padding\` (integer) pad tokens before the prompt</span>
    <span class="hljs-keyword">if</span> left_padding <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(left_padding, <span class="hljs-built_in">int</span>) <span class="hljs-keyword">or</span> left_padding &lt; <span class="hljs-number">0</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;left_padding must be an integer larger than 0, but is <span class="hljs-subst">{left_padding}</span>&quot;</span>)

        pad_token = kwargs.pop(<span class="hljs-string">&quot;pad_token&quot;</span>, <span class="hljs-literal">None</span>) <span class="hljs-keyword">or</span> generation_config.pad_token_id <span class="hljs-keyword">or</span> model.config.pad_token_id
        <span class="hljs-keyword">if</span> pad_token <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;pad_token is not defined&quot;</span>)
        batch_size = input_ids.shape[<span class="hljs-number">0</span>]
        pad_tensor = torch.full(size=(batch_size, left_padding), fill_value=pad_token).to(input_ids.device)
        input_ids = torch.cat((pad_tensor, input_ids), dim=<span class="hljs-number">1</span>)
        cur_length = input_ids.shape[<span class="hljs-number">1</span>]

    <span class="hljs-comment"># Simple greedy decoding loop</span>
    <span class="hljs-keyword">while</span> cur_length &lt; max_length:
        logits = model(input_ids).logits
        next_token_logits = logits[:, -<span class="hljs-number">1</span>, :]
        next_tokens = torch.argmax(next_token_logits, dim=-<span class="hljs-number">1</span>)
        input_ids = torch.cat((input_ids, next_tokens[:, <span class="hljs-literal">None</span>]), dim=-<span class="hljs-number">1</span>)
        cur_length += <span class="hljs-number">1</span>

    <span class="hljs-keyword">return</span> input_ids`,wrap:!1}}),Le=new U({props:{code:"ZnJvbSUyMC51dGlscyUyMGltcG9ydCUyMHNvbWVfZnVuY3Rpb24=",highlighted:'<span class="hljs-keyword">from</span> .utils <span class="hljs-keyword">import</span> some_function',wrap:!1}}),De=new g({props:{title:"requirements.txt",local:"requirementstxt",headingTag:"h4"}}),Pe=new g({props:{title:"README.md",local:"readmemd",headingTag:"h4"}}),tt=new U({props:{code:"LS0tJTBBbGlicmFyeV9uYW1lJTNBJTIwdHJhbnNmb3JtZXJzJTBBdGFncyUzQSUwQSUyMCUyMC0lMjBjdXN0b21fZ2VuZXJhdGUlMEEtLS0lMEElMEEoeW91ciUyMG1hcmtkb3duJTIwY29udGVudCUyMGhlcmUp",highlighted:`<span class="hljs-meta">---</span>
<span class="hljs-attr">library_name:</span> <span class="hljs-string">transformers</span>
<span class="hljs-attr">tags:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">custom_generate</span>
<span class="hljs-meta">---
</span>
<span class="hljs-string">(your</span> <span class="hljs-string">markdown</span> <span class="hljs-string">content</span> <span class="hljs-string">here)</span>`,wrap:!1}}),nt=new g({props:{title:"Reusing generate ‚Äôs input preparation",local:"reusing-generate-s-input-preparation",headingTag:"h3"}}),ot=new U({props:{code:"ZGVmJTIwY3VzdG9tX2xvb3AobW9kZWwlMkMlMjBpbnB1dF9pZHMlMkMlMjBhdHRlbnRpb25fbWFzayUyQyUyMGxvZ2l0c19wcm9jZXNzb3IlMkMlMjBzdG9wcGluZ19jcml0ZXJpYSUyQyUyMGdlbmVyYXRpb25fY29uZmlnJTJDJTIwKiptb2RlbF9rd2FyZ3MpJTNBJTBBJTIwJTIwJTIwJTIwbmV4dF90b2tlbnMlMjAlM0QlMjBpbnB1dF9pZHMlMEElMjAlMjAlMjAlMjB3aGlsZSUyMGlucHV0X2lkcy5zaGFwZSU1QjElNUQlMjAlM0MlMjBzdG9wcGluZ19jcml0ZXJpYSU1QjAlNUQubWF4X2xlbmd0aCUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKG5leHRfdG9rZW5zJTJDJTIwYXR0ZW50aW9uX21hc2slM0RhdHRlbnRpb25fbWFzayUyQyUyMCoqbW9kZWxfa3dhcmdzKS5sb2dpdHMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBuZXh0X3Rva2VuX2xvZ2l0cyUyMCUzRCUyMGxvZ2l0c19wcm9jZXNzb3IoaW5wdXRfaWRzJTJDJTIwbG9naXRzJTVCJTNBJTJDJTIwLTElMkMlMjAlM0ElNUQpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbmV4dF90b2tlbnMlMjAlM0QlMjB0b3JjaC5hcmdtYXgobmV4dF90b2tlbl9sb2dpdHMlMkMlMjBkaW0lM0QtMSklNUIlM0ElMkMlMjBOb25lJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaW5wdXRfaWRzJTIwJTNEJTIwdG9yY2guY2F0KChpbnB1dF9pZHMlMkMlMjBuZXh0X3Rva2VucyklMkMlMjBkaW0lM0QtMSklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBhdHRlbnRpb25fbWFzayUyMCUzRCUyMHRvcmNoLmNhdCgoYXR0ZW50aW9uX21hc2slMkMlMjB0b3JjaC5vbmVzX2xpa2UobmV4dF90b2tlbnMpKSUyQyUyMGRpbSUzRC0xKSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMGlucHV0X2lkcyUwQSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCUwQSUyMCUyMCUyMCUyMCoqaW5wdXRzJTJDJTBBJTIwJTIwJTIwJTIwY3VzdG9tX2dlbmVyYXRlJTNEY3VzdG9tX2xvb3AlMkMlMEElMjAlMjAlMjAlMjBtYXhfbmV3X3Rva2VucyUzRDEwJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_loop</span>(<span class="hljs-params">model, input_ids, attention_mask, logits_processor, stopping_criteria, generation_config, **model_kwargs</span>):
    next_tokens = input_ids
    <span class="hljs-keyword">while</span> input_ids.shape[<span class="hljs-number">1</span>] &lt; stopping_criteria[<span class="hljs-number">0</span>].max_length:
        logits = model(next_tokens, attention_mask=attention_mask, **model_kwargs).logits
        next_token_logits = logits_processor(input_ids, logits[:, -<span class="hljs-number">1</span>, :])
        next_tokens = torch.argmax(next_token_logits, dim=-<span class="hljs-number">1</span>)[:, <span class="hljs-literal">None</span>]
        input_ids = torch.cat((input_ids, next_tokens), dim=-<span class="hljs-number">1</span>)
        attention_mask = torch.cat((attention_mask, torch.ones_like(next_tokens)), dim=-<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> input_ids

output = model.generate(
    **inputs,
    custom_generate=custom_loop,
    max_new_tokens=<span class="hljs-number">10</span>,
)`,wrap:!1}}),V=new ls({props:{warning:!1,$$slots:{default:[jn]},$$scope:{ctx:Z}}}),it=new g({props:{title:"Finding custom generation methods",local:"finding-custom-generation-methods",headingTag:"h3"}}),mt=new g({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ct=new hn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md"}}),{c(){o=i("meta"),T=n(),m=i("p"),w=n(),d(b.$$.fragment),J=n(),j=i("p"),j.textContent=f,G=n(),B=i("p"),B.textContent=ss,yt=n(),d(X.$$.fragment),ht=n(),C=i("p"),C.textContent=ns,bt=n(),d(_.$$.fragment),Jt=n(),R=i("p"),R.innerHTML=as,Tt=n(),$=i("p"),$.textContent=os,wt=n(),d(F.$$.fragment),ft=n(),d(N.$$.fragment),gt=n(),x=i("p"),x.textContent=is,Ut=n(),Y=i("p"),Y.innerHTML=rs,jt=n(),d(E.$$.fragment),Zt=n(),d(H.$$.fragment),Gt=n(),z=i("p"),z.innerHTML=ps,kt=n(),d(k.$$.fragment),vt=n(),Q=i("p"),Q.innerHTML=ms,Wt=n(),d(S.$$.fragment),It=n(),d(A.$$.fragment),Vt=n(),L=i("p"),L.textContent=ds,Bt=n(),d(q.$$.fragment),Xt=n(),D=i("p"),D.innerHTML=cs,Ct=n(),K=i("p"),K.textContent=Ms,_t=n(),P=i("p"),P.innerHTML=us,Rt=n(),d(v.$$.fragment),$t=n(),d(O.$$.fragment),Ft=n(),ee=i("p"),ee.innerHTML=ys,Nt=n(),te=i("p"),te.innerHTML=hs,xt=n(),d(le.$$.fragment),Yt=n(),d(se.$$.fragment),Et=n(),ne=i("p"),ne.textContent=bs,Ht=n(),ae=i("p"),ae.textContent=Js,zt=n(),oe=i("p"),oe.innerHTML=Ts,Qt=n(),d(ie.$$.fragment),St=n(),d(re.$$.fragment),At=n(),pe=i("p"),pe.innerHTML=ws,Lt=n(),me=i("p"),me.textContent=fs,qt=n(),de=i("p"),de.innerHTML=gs,Dt=n(),d(ce.$$.fragment),Kt=n(),d(Me.$$.fragment),Pt=n(),ue=i("p"),ue.innerHTML=Us,Ot=n(),ye=i("p"),ye.innerHTML=js,el=n(),d(he.$$.fragment),tl=n(),d(be.$$.fragment),ll=n(),Je=i("p"),Je.textContent=Zs,sl=n(),Te=i("ul"),Te.innerHTML=Gs,nl=n(),we=i("p"),we.innerHTML=ks,al=n(),fe=i("p"),fe.textContent=vs,ol=n(),d(ge.$$.fragment),il=n(),Ue=i("p"),Ue.innerHTML=Ws,rl=n(),d(je.$$.fragment),pl=n(),Ze=i("p"),Ze.innerHTML=Is,ml=n(),d(W.$$.fragment),dl=n(),Ge=i("p"),Ge.innerHTML=Vs,cl=n(),d(ke.$$.fragment),Ml=n(),ve=i("p"),ve.innerHTML=Bs,ul=n(),d(We.$$.fragment),yl=n(),Ie=i("p"),Ie.textContent=Xs,hl=n(),d(Ve.$$.fragment),bl=n(),Be=i("p"),Be.innerHTML=Cs,Jl=n(),Xe=i("ol"),Xe.innerHTML=_s,Tl=n(),Ce=i("p"),Ce.textContent=Rs,wl=n(),d(_e.$$.fragment),fl=n(),d(Re.$$.fragment),gl=n(),$e=i("p"),$e.innerHTML=$s,Ul=n(),Fe=i("p"),Fe.textContent=Fs,jl=n(),d(Ne.$$.fragment),Zl=n(),d(xe.$$.fragment),Gl=n(),Ye=i("p"),Ye.innerHTML=Ns,kl=n(),d(I.$$.fragment),vl=n(),Ee=i("p"),Ee.innerHTML=xs,Wl=n(),He=i("p"),He.innerHTML=Ys,Il=n(),d(ze.$$.fragment),Vl=n(),Qe=i("p"),Qe.textContent=Es,Bl=n(),Se=i("ul"),Se.innerHTML=Hs,Xl=n(),Ae=i("p"),Ae.innerHTML=zs,Cl=n(),d(Le.$$.fragment),_l=n(),qe=i("p"),qe.innerHTML=Qs,Rl=n(),d(De.$$.fragment),$l=n(),Ke=i("p"),Ke.innerHTML=Ss,Fl=n(),d(Pe.$$.fragment),Nl=n(),Oe=i("p"),Oe.innerHTML=As,xl=n(),et=i("p"),et.innerHTML=Ls,Yl=n(),d(tt.$$.fragment),El=n(),lt=i("p"),lt.textContent=qs,Hl=n(),st=i("ul"),st.innerHTML=Ds,zl=n(),d(nt.$$.fragment),Ql=n(),at=i("p"),at.innerHTML=Ks,Sl=n(),d(ot.$$.fragment),Al=n(),d(V.$$.fragment),Ll=n(),d(it.$$.fragment),ql=n(),rt=i("p"),rt.innerHTML=Ps,Dl=n(),pt=i("ul"),pt.innerHTML=Os,Kl=n(),d(mt.$$.fragment),Pl=n(),dt=i("p"),dt.innerHTML=en,Ol=n(),d(ct.$$.fragment),es=n(),ut=i("p"),this.h()},l(e){const t=un("svelte-u9bgzb",document.head);o=r(t,"META",{name:!0,content:!0}),t.forEach(l),T=a(e),m=r(e,"P",{}),on(m).forEach(l),w=a(e),c(b.$$.fragment,e),J=a(e),j=r(e,"P",{"data-svelte-h":!0}),p(j)!=="svelte-mu4vht"&&(j.textContent=f),G=a(e),B=r(e,"P",{"data-svelte-h":!0}),p(B)!=="svelte-14bli3k"&&(B.textContent=ss),yt=a(e),c(X.$$.fragment,e),ht=a(e),C=r(e,"P",{"data-svelte-h":!0}),p(C)!=="svelte-i7eisu"&&(C.textContent=ns),bt=a(e),c(_.$$.fragment,e),Jt=a(e),R=r(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-a6hznb"&&(R.innerHTML=as),Tt=a(e),$=r(e,"P",{"data-svelte-h":!0}),p($)!=="svelte-1mt54k3"&&($.textContent=os),wt=a(e),c(F.$$.fragment,e),ft=a(e),c(N.$$.fragment,e),gt=a(e),x=r(e,"P",{"data-svelte-h":!0}),p(x)!=="svelte-e4o6mk"&&(x.textContent=is),Ut=a(e),Y=r(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-1bwisky"&&(Y.innerHTML=rs),jt=a(e),c(E.$$.fragment,e),Zt=a(e),c(H.$$.fragment,e),Gt=a(e),z=r(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-licetj"&&(z.innerHTML=ps),kt=a(e),c(k.$$.fragment,e),vt=a(e),Q=r(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-odkbop"&&(Q.innerHTML=ms),Wt=a(e),c(S.$$.fragment,e),It=a(e),c(A.$$.fragment,e),Vt=a(e),L=r(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-fs2igz"&&(L.textContent=ds),Bt=a(e),c(q.$$.fragment,e),Xt=a(e),D=r(e,"P",{"data-svelte-h":!0}),p(D)!=="svelte-1s1wab2"&&(D.innerHTML=cs),Ct=a(e),K=r(e,"P",{"data-svelte-h":!0}),p(K)!=="svelte-1r2bbth"&&(K.textContent=Ms),_t=a(e),P=r(e,"P",{"data-svelte-h":!0}),p(P)!=="svelte-fl11x4"&&(P.innerHTML=us),Rt=a(e),c(v.$$.fragment,e),$t=a(e),c(O.$$.fragment,e),Ft=a(e),ee=r(e,"P",{"data-svelte-h":!0}),p(ee)!=="svelte-1wlycdx"&&(ee.innerHTML=ys),Nt=a(e),te=r(e,"P",{"data-svelte-h":!0}),p(te)!=="svelte-cluu0c"&&(te.innerHTML=hs),xt=a(e),c(le.$$.fragment,e),Yt=a(e),c(se.$$.fragment,e),Et=a(e),ne=r(e,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-9ukubm"&&(ne.textContent=bs),Ht=a(e),ae=r(e,"P",{"data-svelte-h":!0}),p(ae)!=="svelte-18mz1b0"&&(ae.textContent=Js),zt=a(e),oe=r(e,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-wls1vk"&&(oe.innerHTML=Ts),Qt=a(e),c(ie.$$.fragment,e),St=a(e),c(re.$$.fragment,e),At=a(e),pe=r(e,"P",{"data-svelte-h":!0}),p(pe)!=="svelte-g35kae"&&(pe.innerHTML=ws),Lt=a(e),me=r(e,"P",{"data-svelte-h":!0}),p(me)!=="svelte-1jwkvjw"&&(me.textContent=fs),qt=a(e),de=r(e,"P",{"data-svelte-h":!0}),p(de)!=="svelte-e3zpro"&&(de.innerHTML=gs),Dt=a(e),c(ce.$$.fragment,e),Kt=a(e),c(Me.$$.fragment,e),Pt=a(e),ue=r(e,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-v1t7jy"&&(ue.innerHTML=Us),Ot=a(e),ye=r(e,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-22i19"&&(ye.innerHTML=js),el=a(e),c(he.$$.fragment,e),tl=a(e),c(be.$$.fragment,e),ll=a(e),Je=r(e,"P",{"data-svelte-h":!0}),p(Je)!=="svelte-8489ne"&&(Je.textContent=Zs),sl=a(e),Te=r(e,"UL",{"data-svelte-h":!0}),p(Te)!=="svelte-1kcz2qm"&&(Te.innerHTML=Gs),nl=a(e),we=r(e,"P",{"data-svelte-h":!0}),p(we)!=="svelte-yr3ixl"&&(we.innerHTML=ks),al=a(e),fe=r(e,"P",{"data-svelte-h":!0}),p(fe)!=="svelte-8y0a21"&&(fe.textContent=vs),ol=a(e),c(ge.$$.fragment,e),il=a(e),Ue=r(e,"P",{"data-svelte-h":!0}),p(Ue)!=="svelte-mnyz5p"&&(Ue.innerHTML=Ws),rl=a(e),c(je.$$.fragment,e),pl=a(e),Ze=r(e,"P",{"data-svelte-h":!0}),p(Ze)!=="svelte-wiaeby"&&(Ze.innerHTML=Is),ml=a(e),c(W.$$.fragment,e),dl=a(e),Ge=r(e,"P",{"data-svelte-h":!0}),p(Ge)!=="svelte-6fygoq"&&(Ge.innerHTML=Vs),cl=a(e),c(ke.$$.fragment,e),Ml=a(e),ve=r(e,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1twkewh"&&(ve.innerHTML=Bs),ul=a(e),c(We.$$.fragment,e),yl=a(e),Ie=r(e,"P",{"data-svelte-h":!0}),p(Ie)!=="svelte-7d74dd"&&(Ie.textContent=Xs),hl=a(e),c(Ve.$$.fragment,e),bl=a(e),Be=r(e,"P",{"data-svelte-h":!0}),p(Be)!=="svelte-1t2t779"&&(Be.innerHTML=Cs),Jl=a(e),Xe=r(e,"OL",{"data-svelte-h":!0}),p(Xe)!=="svelte-miimk7"&&(Xe.innerHTML=_s),Tl=a(e),Ce=r(e,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-uqkppw"&&(Ce.textContent=Rs),wl=a(e),c(_e.$$.fragment,e),fl=a(e),c(Re.$$.fragment,e),gl=a(e),$e=r(e,"P",{"data-svelte-h":!0}),p($e)!=="svelte-1k64pn0"&&($e.innerHTML=$s),Ul=a(e),Fe=r(e,"P",{"data-svelte-h":!0}),p(Fe)!=="svelte-rqtax3"&&(Fe.textContent=Fs),jl=a(e),c(Ne.$$.fragment,e),Zl=a(e),c(xe.$$.fragment,e),Gl=a(e),Ye=r(e,"P",{"data-svelte-h":!0}),p(Ye)!=="svelte-1a5siiz"&&(Ye.innerHTML=Ns),kl=a(e),c(I.$$.fragment,e),vl=a(e),Ee=r(e,"P",{"data-svelte-h":!0}),p(Ee)!=="svelte-1jkuynj"&&(Ee.innerHTML=xs),Wl=a(e),He=r(e,"P",{"data-svelte-h":!0}),p(He)!=="svelte-1m56d80"&&(He.innerHTML=Ys),Il=a(e),c(ze.$$.fragment,e),Vl=a(e),Qe=r(e,"P",{"data-svelte-h":!0}),p(Qe)!=="svelte-vo2bry"&&(Qe.textContent=Es),Bl=a(e),Se=r(e,"UL",{"data-svelte-h":!0}),p(Se)!=="svelte-1pfs26a"&&(Se.innerHTML=Hs),Xl=a(e),Ae=r(e,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-1gfwext"&&(Ae.innerHTML=zs),Cl=a(e),c(Le.$$.fragment,e),_l=a(e),qe=r(e,"P",{"data-svelte-h":!0}),p(qe)!=="svelte-3fuf8e"&&(qe.innerHTML=Qs),Rl=a(e),c(De.$$.fragment,e),$l=a(e),Ke=r(e,"P",{"data-svelte-h":!0}),p(Ke)!=="svelte-i80d4s"&&(Ke.innerHTML=Ss),Fl=a(e),c(Pe.$$.fragment,e),Nl=a(e),Oe=r(e,"P",{"data-svelte-h":!0}),p(Oe)!=="svelte-1f6dncd"&&(Oe.innerHTML=As),xl=a(e),et=r(e,"P",{"data-svelte-h":!0}),p(et)!=="svelte-1r2pptl"&&(et.innerHTML=Ls),Yl=a(e),c(tt.$$.fragment,e),El=a(e),lt=r(e,"P",{"data-svelte-h":!0}),p(lt)!=="svelte-gw1adn"&&(lt.textContent=qs),Hl=a(e),st=r(e,"UL",{"data-svelte-h":!0}),p(st)!=="svelte-1b8vxen"&&(st.innerHTML=Ds),zl=a(e),c(nt.$$.fragment,e),Ql=a(e),at=r(e,"P",{"data-svelte-h":!0}),p(at)!=="svelte-1wjep2b"&&(at.innerHTML=Ks),Sl=a(e),c(ot.$$.fragment,e),Al=a(e),c(V.$$.fragment,e),Ll=a(e),c(it.$$.fragment,e),ql=a(e),rt=r(e,"P",{"data-svelte-h":!0}),p(rt)!=="svelte-1oqs5iw"&&(rt.innerHTML=Ps),Dl=a(e),pt=r(e,"UL",{"data-svelte-h":!0}),p(pt)!=="svelte-186mhwr"&&(pt.innerHTML=Os),Kl=a(e),c(mt.$$.fragment,e),Pl=a(e),dt=r(e,"P",{"data-svelte-h":!0}),p(dt)!=="svelte-7psx0p"&&(dt.innerHTML=en),Ol=a(e),c(ct.$$.fragment,e),es=a(e),ut=r(e,"P",{}),on(ut).forEach(l),this.h()},h(){rn(o,"name","hf:doc:metadata"),rn(o,"content",Gn)},m(e,t){yn(document.head,o),s(e,T,t),s(e,m,t),s(e,w,t),M(b,e,t),s(e,J,t),s(e,j,t),s(e,G,t),s(e,B,t),s(e,yt,t),M(X,e,t),s(e,ht,t),s(e,C,t),s(e,bt,t),M(_,e,t),s(e,Jt,t),s(e,R,t),s(e,Tt,t),s(e,$,t),s(e,wt,t),M(F,e,t),s(e,ft,t),M(N,e,t),s(e,gt,t),s(e,x,t),s(e,Ut,t),s(e,Y,t),s(e,jt,t),M(E,e,t),s(e,Zt,t),M(H,e,t),s(e,Gt,t),s(e,z,t),s(e,kt,t),M(k,e,t),s(e,vt,t),s(e,Q,t),s(e,Wt,t),M(S,e,t),s(e,It,t),M(A,e,t),s(e,Vt,t),s(e,L,t),s(e,Bt,t),M(q,e,t),s(e,Xt,t),s(e,D,t),s(e,Ct,t),s(e,K,t),s(e,_t,t),s(e,P,t),s(e,Rt,t),M(v,e,t),s(e,$t,t),M(O,e,t),s(e,Ft,t),s(e,ee,t),s(e,Nt,t),s(e,te,t),s(e,xt,t),M(le,e,t),s(e,Yt,t),M(se,e,t),s(e,Et,t),s(e,ne,t),s(e,Ht,t),s(e,ae,t),s(e,zt,t),s(e,oe,t),s(e,Qt,t),M(ie,e,t),s(e,St,t),M(re,e,t),s(e,At,t),s(e,pe,t),s(e,Lt,t),s(e,me,t),s(e,qt,t),s(e,de,t),s(e,Dt,t),M(ce,e,t),s(e,Kt,t),M(Me,e,t),s(e,Pt,t),s(e,ue,t),s(e,Ot,t),s(e,ye,t),s(e,el,t),M(he,e,t),s(e,tl,t),M(be,e,t),s(e,ll,t),s(e,Je,t),s(e,sl,t),s(e,Te,t),s(e,nl,t),s(e,we,t),s(e,al,t),s(e,fe,t),s(e,ol,t),M(ge,e,t),s(e,il,t),s(e,Ue,t),s(e,rl,t),M(je,e,t),s(e,pl,t),s(e,Ze,t),s(e,ml,t),M(W,e,t),s(e,dl,t),s(e,Ge,t),s(e,cl,t),M(ke,e,t),s(e,Ml,t),s(e,ve,t),s(e,ul,t),M(We,e,t),s(e,yl,t),s(e,Ie,t),s(e,hl,t),M(Ve,e,t),s(e,bl,t),s(e,Be,t),s(e,Jl,t),s(e,Xe,t),s(e,Tl,t),s(e,Ce,t),s(e,wl,t),M(_e,e,t),s(e,fl,t),M(Re,e,t),s(e,gl,t),s(e,$e,t),s(e,Ul,t),s(e,Fe,t),s(e,jl,t),M(Ne,e,t),s(e,Zl,t),M(xe,e,t),s(e,Gl,t),s(e,Ye,t),s(e,kl,t),M(I,e,t),s(e,vl,t),s(e,Ee,t),s(e,Wl,t),s(e,He,t),s(e,Il,t),M(ze,e,t),s(e,Vl,t),s(e,Qe,t),s(e,Bl,t),s(e,Se,t),s(e,Xl,t),s(e,Ae,t),s(e,Cl,t),M(Le,e,t),s(e,_l,t),s(e,qe,t),s(e,Rl,t),M(De,e,t),s(e,$l,t),s(e,Ke,t),s(e,Fl,t),M(Pe,e,t),s(e,Nl,t),s(e,Oe,t),s(e,xl,t),s(e,et,t),s(e,Yl,t),M(tt,e,t),s(e,El,t),s(e,lt,t),s(e,Hl,t),s(e,st,t),s(e,zl,t),M(nt,e,t),s(e,Ql,t),s(e,at,t),s(e,Sl,t),M(ot,e,t),s(e,Al,t),M(V,e,t),s(e,Ll,t),M(it,e,t),s(e,ql,t),s(e,rt,t),s(e,Dl,t),s(e,pt,t),s(e,Kl,t),M(mt,e,t),s(e,Pl,t),s(e,dt,t),s(e,Ol,t),M(ct,e,t),s(e,es,t),s(e,ut,t),ts=!0},p(e,[t]){const tn={};t&2&&(tn.$$scope={dirty:t,ctx:e}),k.$set(tn);const ln={};t&2&&(ln.$$scope={dirty:t,ctx:e}),v.$set(ln);const sn={};t&2&&(sn.$$scope={dirty:t,ctx:e}),W.$set(sn);const nn={};t&2&&(nn.$$scope={dirty:t,ctx:e}),I.$set(nn);const an={};t&2&&(an.$$scope={dirty:t,ctx:e}),V.$set(an)},i(e){ts||(u(b.$$.fragment,e),u(X.$$.fragment,e),u(_.$$.fragment,e),u(F.$$.fragment,e),u(N.$$.fragment,e),u(E.$$.fragment,e),u(H.$$.fragment,e),u(k.$$.fragment,e),u(S.$$.fragment,e),u(A.$$.fragment,e),u(q.$$.fragment,e),u(v.$$.fragment,e),u(O.$$.fragment,e),u(le.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(re.$$.fragment,e),u(ce.$$.fragment,e),u(Me.$$.fragment,e),u(he.$$.fragment,e),u(be.$$.fragment,e),u(ge.$$.fragment,e),u(je.$$.fragment,e),u(W.$$.fragment,e),u(ke.$$.fragment,e),u(We.$$.fragment,e),u(Ve.$$.fragment,e),u(_e.$$.fragment,e),u(Re.$$.fragment,e),u(Ne.$$.fragment,e),u(xe.$$.fragment,e),u(I.$$.fragment,e),u(ze.$$.fragment,e),u(Le.$$.fragment,e),u(De.$$.fragment,e),u(Pe.$$.fragment,e),u(tt.$$.fragment,e),u(nt.$$.fragment,e),u(ot.$$.fragment,e),u(V.$$.fragment,e),u(it.$$.fragment,e),u(mt.$$.fragment,e),u(ct.$$.fragment,e),ts=!0)},o(e){y(b.$$.fragment,e),y(X.$$.fragment,e),y(_.$$.fragment,e),y(F.$$.fragment,e),y(N.$$.fragment,e),y(E.$$.fragment,e),y(H.$$.fragment,e),y(k.$$.fragment,e),y(S.$$.fragment,e),y(A.$$.fragment,e),y(q.$$.fragment,e),y(v.$$.fragment,e),y(O.$$.fragment,e),y(le.$$.fragment,e),y(se.$$.fragment,e),y(ie.$$.fragment,e),y(re.$$.fragment,e),y(ce.$$.fragment,e),y(Me.$$.fragment,e),y(he.$$.fragment,e),y(be.$$.fragment,e),y(ge.$$.fragment,e),y(je.$$.fragment,e),y(W.$$.fragment,e),y(ke.$$.fragment,e),y(We.$$.fragment,e),y(Ve.$$.fragment,e),y(_e.$$.fragment,e),y(Re.$$.fragment,e),y(Ne.$$.fragment,e),y(xe.$$.fragment,e),y(I.$$.fragment,e),y(ze.$$.fragment,e),y(Le.$$.fragment,e),y(De.$$.fragment,e),y(Pe.$$.fragment,e),y(tt.$$.fragment,e),y(nt.$$.fragment,e),y(ot.$$.fragment,e),y(V.$$.fragment,e),y(it.$$.fragment,e),y(mt.$$.fragment,e),y(ct.$$.fragment,e),ts=!1},d(e){e&&(l(T),l(m),l(w),l(J),l(j),l(G),l(B),l(yt),l(ht),l(C),l(bt),l(Jt),l(R),l(Tt),l($),l(wt),l(ft),l(gt),l(x),l(Ut),l(Y),l(jt),l(Zt),l(Gt),l(z),l(kt),l(vt),l(Q),l(Wt),l(It),l(Vt),l(L),l(Bt),l(Xt),l(D),l(Ct),l(K),l(_t),l(P),l(Rt),l($t),l(Ft),l(ee),l(Nt),l(te),l(xt),l(Yt),l(Et),l(ne),l(Ht),l(ae),l(zt),l(oe),l(Qt),l(St),l(At),l(pe),l(Lt),l(me),l(qt),l(de),l(Dt),l(Kt),l(Pt),l(ue),l(Ot),l(ye),l(el),l(tl),l(ll),l(Je),l(sl),l(Te),l(nl),l(we),l(al),l(fe),l(ol),l(il),l(Ue),l(rl),l(pl),l(Ze),l(ml),l(dl),l(Ge),l(cl),l(Ml),l(ve),l(ul),l(yl),l(Ie),l(hl),l(bl),l(Be),l(Jl),l(Xe),l(Tl),l(Ce),l(wl),l(fl),l(gl),l($e),l(Ul),l(Fe),l(jl),l(Zl),l(Gl),l(Ye),l(kl),l(vl),l(Ee),l(Wl),l(He),l(Il),l(Vl),l(Qe),l(Bl),l(Se),l(Xl),l(Ae),l(Cl),l(_l),l(qe),l(Rl),l($l),l(Ke),l(Fl),l(Nl),l(Oe),l(xl),l(et),l(Yl),l(El),l(lt),l(Hl),l(st),l(zl),l(Ql),l(at),l(Sl),l(Al),l(Ll),l(ql),l(rt),l(Dl),l(pt),l(Kl),l(Pl),l(dt),l(Ol),l(es),l(ut)),l(o),h(b,e),h(X,e),h(_,e),h(F,e),h(N,e),h(E,e),h(H,e),h(k,e),h(S,e),h(A,e),h(q,e),h(v,e),h(O,e),h(le,e),h(se,e),h(ie,e),h(re,e),h(ce,e),h(Me,e),h(he,e),h(be,e),h(ge,e),h(je,e),h(W,e),h(ke,e),h(We,e),h(Ve,e),h(_e,e),h(Re,e),h(Ne,e),h(xe,e),h(I,e),h(ze,e),h(Le,e),h(De,e),h(Pe,e),h(tt,e),h(nt,e),h(ot,e),h(V,e),h(it,e),h(mt,e),h(ct,e)}}}const Gn='{"title":"Generation strategies","local":"generation-strategies","sections":[{"title":"Basic decoding methods","local":"basic-decoding-methods","sections":[{"title":"Greedy search","local":"greedy-search","sections":[],"depth":3},{"title":"Sampling","local":"sampling","sections":[],"depth":3},{"title":"Beam search","local":"beam-search","sections":[],"depth":3}],"depth":2},{"title":"Advanced decoding methods","local":"advanced-decoding-methods","sections":[{"title":"Speculative decoding","local":"speculative-decoding","sections":[{"title":"Prompt lookup decoding","local":"prompt-lookup-decoding","sections":[],"depth":4}],"depth":3},{"title":"Self-speculative decoding","local":"self-speculative-decoding","sections":[{"title":"Universal assisted decoding","local":"universal-assisted-decoding","sections":[],"depth":4}],"depth":3},{"title":"Diverse beam search","local":"diverse-beam-search","sections":[],"depth":3}],"depth":2},{"title":"Custom generation methods","local":"custom-generation-methods","sections":[{"title":"Creating a custom generation method","local":"creating-a-custom-generation-method","sections":[{"title":"Adding the base model","local":"adding-the-base-model","sections":[],"depth":4},{"title":"generate.py","local":"generatepy","sections":[],"depth":4},{"title":"requirements.txt","local":"requirementstxt","sections":[],"depth":4},{"title":"README.md","local":"readmemd","sections":[],"depth":4}],"depth":3},{"title":"Reusing generate ‚Äôs input preparation","local":"reusing-generate-s-input-preparation","sections":[],"depth":3},{"title":"Finding custom generation methods","local":"finding-custom-generation-methods","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2}],"depth":1}';function kn(Z){return dn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cn extends cn{constructor(o){super(),Mn(this,o,kn,Zn,mn,{})}}export{Cn as component};
