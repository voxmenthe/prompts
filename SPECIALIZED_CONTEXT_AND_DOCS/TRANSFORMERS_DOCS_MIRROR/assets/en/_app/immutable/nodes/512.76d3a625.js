import{s as se,o as ie,n as oe}from"../chunks/scheduler.18a86fab.js";import{S as re,i as pe,g as r,s,r as T,A as de,h as p,f as a,c as i,j as le,u as C,x as u,k as A,y as me,a as n,v as U,d as k,t as J,w as j}from"../chunks/index.98837b22.js";import{T as ue}from"../chunks/Tip.77304350.js";import{C as N}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as fe,E as ce}from"../chunks/getInferenceSnippets.06c2775f.js";function he(R){let l,f="You need a GPU with Compute Capability>=9 (H100), and install a PyTorch version compatible with the CUDA version of your GPU.";return{c(){l=r("p"),l.textContent=f},l(o){l=p(o,"P",{"data-svelte-h":!0}),u(l)!=="svelte-aidjqb"&&(l.textContent=f)},m(o,F){n(o,l,F)},p:oe,d(o){o&&a(l)}}}function Me(R){let l,f,o,F,c,q,h,S="Fine-grained FP8 quantization quantizes the weights and activations to fp8.",X,M,D="<li>The weights are quantized to 8-bits for each 2D block (<code>weight_block_size=(128, 128)</code>).</li> <li>The activations are quantized to 8-bits for each group per token. The group value matches the weights in the input channel (128 by default).</li>",G,b,K='FP8 quantization enables support for <a href="https://hf.co/papers/2412.19437" rel="nofollow">DeepSeek-V3</a> and DeepSeek-R1.',z,d,O='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/b7b3b34bf826a6423ea82ffc57ecac80c46c3c76/transformers/quantization/quantization_deepseek.png"/>',V,m,H,_,ee="Install Accelerate and upgrade to the latest version of PyTorch.",x,y,E,g,te='Create a <a href="/docs/transformers/v4.56.2/en/main_classes/quantization#transformers.FineGrainedFP8Config">FineGrainedFP8Config</a> class and pass it to <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> to quantize it. The weights are loaded in full precision (<code>torch.float32</code>) by default regardless of the actual data type the weights are stored in. Set <code>dtype=&quot;auto&quot;</code> to load the weights in the data type defined in a models <code>config.json</code> file to automatically load the most memory-optiomal data type.',B,v,P,$,ae='Use <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> to save the quantized model and reload it with <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a>.',I,Z,Y,w,L,W,Q;return c=new fe({props:{title:"Fine-grained FP8",local:"fine-grained-fp8",headingTag:"h1"}}),m=new ue({props:{warning:!1,$$slots:{default:[he]},$$scope:{ctx:R}}}),y=new N({props:{code:"cGlwJTIwaW5zdGFsbCUyMC0tdXBncmFkZSUyMGFjY2VsZXJhdGUlMjB0b3JjaA==",highlighted:"pip install --upgrade accelerate torch",wrap:!1}}),v=new N({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZpbmVHcmFpbmVkRlA4Q29uZmlnJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMm1ldGEtbGxhbWElMkZNZXRhLUxsYW1hLTMtOEIlMjIlMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwRmluZUdyYWluZWRGUDhDb25maWcoKSUwQXF1YW50aXplZF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lJTJDJTIwZHR5cGUlM0QlMjJhdXRvJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKSUwQWlucHV0X3RleHQlMjAlM0QlMjAlMjJXaGF0JTIwYXJlJTIwd2UlMjBoYXZpbmclMjBmb3IlMjBkaW5uZXIlM0YlMjIlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKHF1YW50aXplZF9tb2RlbC5kZXZpY2UudHlwZSklMEElMEFvdXRwdXQlMjAlM0QlMjBxdWFudGl6ZWRfbW9kZWwuZ2VuZXJhdGUoKippbnB1dF9pZHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwKSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0JTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer

model_name = <span class="hljs-string">&quot;meta-llama/Meta-Llama-3-8B&quot;</span>
quantization_config = FineGrainedFP8Config()
quantized_model = AutoModelForCausalLM.from_pretrained(model_name, dtype=<span class="hljs-string">&quot;auto&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=quantization_config)

tokenizer = AutoTokenizer.from_pretrained(model_name)
input_text = <span class="hljs-string">&quot;What are we having for dinner?&quot;</span>
input_ids = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(quantized_model.device.<span class="hljs-built_in">type</span>)

output = quantized_model.generate(**input_ids, max_new_tokens=<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),Z=new N({props:{code:"cXVhbnRfcGF0aCUyMCUzRCUyMCUyMiUyRnBhdGglMkZ0byUyRnNhdmUlMkZxdWFudGl6ZWQlMkZtb2RlbCUyMiUwQW1vZGVsLnNhdmVfcHJldHJhaW5lZChxdWFudF9wYXRoKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKHF1YW50X3BhdGglMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`quant_path = <span class="hljs-string">&quot;/path/to/save/quantized/model&quot;</span>
model.save_pretrained(quant_path)
model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),w=new ce({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/finegrained_fp8.md"}}),{c(){l=r("meta"),f=s(),o=r("p"),F=s(),T(c.$$.fragment),q=s(),h=r("p"),h.textContent=S,X=s(),M=r("ul"),M.innerHTML=D,G=s(),b=r("p"),b.innerHTML=K,z=s(),d=r("div"),d.innerHTML=O,V=s(),T(m.$$.fragment),H=s(),_=r("p"),_.textContent=ee,x=s(),T(y.$$.fragment),E=s(),g=r("p"),g.innerHTML=te,B=s(),T(v.$$.fragment),P=s(),$=r("p"),$.innerHTML=ae,I=s(),T(Z.$$.fragment),Y=s(),T(w.$$.fragment),L=s(),W=r("p"),this.h()},l(e){const t=de("svelte-u9bgzb",document.head);l=p(t,"META",{name:!0,content:!0}),t.forEach(a),f=i(e),o=p(e,"P",{}),le(o).forEach(a),F=i(e),C(c.$$.fragment,e),q=i(e),h=p(e,"P",{"data-svelte-h":!0}),u(h)!=="svelte-hrrhz7"&&(h.textContent=S),X=i(e),M=p(e,"UL",{"data-svelte-h":!0}),u(M)!=="svelte-1rx58b3"&&(M.innerHTML=D),G=i(e),b=p(e,"P",{"data-svelte-h":!0}),u(b)!=="svelte-10oa2n0"&&(b.innerHTML=K),z=i(e),d=p(e,"DIV",{class:!0,"data-svelte-h":!0}),u(d)!=="svelte-cd82ay"&&(d.innerHTML=O),V=i(e),C(m.$$.fragment,e),H=i(e),_=p(e,"P",{"data-svelte-h":!0}),u(_)!=="svelte-1rty8u8"&&(_.textContent=ee),x=i(e),C(y.$$.fragment,e),E=i(e),g=p(e,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1tzm8il"&&(g.innerHTML=te),B=i(e),C(v.$$.fragment,e),P=i(e),$=p(e,"P",{"data-svelte-h":!0}),u($)!=="svelte-hi5u23"&&($.innerHTML=ae),I=i(e),C(Z.$$.fragment,e),Y=i(e),C(w.$$.fragment,e),L=i(e),W=p(e,"P",{}),le(W).forEach(a),this.h()},h(){A(l,"name","hf:doc:metadata"),A(l,"content",be),A(d,"class","flex justify-center")},m(e,t){me(document.head,l),n(e,f,t),n(e,o,t),n(e,F,t),U(c,e,t),n(e,q,t),n(e,h,t),n(e,X,t),n(e,M,t),n(e,G,t),n(e,b,t),n(e,z,t),n(e,d,t),n(e,V,t),U(m,e,t),n(e,H,t),n(e,_,t),n(e,x,t),U(y,e,t),n(e,E,t),n(e,g,t),n(e,B,t),U(v,e,t),n(e,P,t),n(e,$,t),n(e,I,t),U(Z,e,t),n(e,Y,t),U(w,e,t),n(e,L,t),n(e,W,t),Q=!0},p(e,[t]){const ne={};t&2&&(ne.$$scope={dirty:t,ctx:e}),m.$set(ne)},i(e){Q||(k(c.$$.fragment,e),k(m.$$.fragment,e),k(y.$$.fragment,e),k(v.$$.fragment,e),k(Z.$$.fragment,e),k(w.$$.fragment,e),Q=!0)},o(e){J(c.$$.fragment,e),J(m.$$.fragment,e),J(y.$$.fragment,e),J(v.$$.fragment,e),J(Z.$$.fragment,e),J(w.$$.fragment,e),Q=!1},d(e){e&&(a(f),a(o),a(F),a(q),a(h),a(X),a(M),a(G),a(b),a(z),a(d),a(V),a(H),a(_),a(x),a(E),a(g),a(B),a(P),a($),a(I),a(Y),a(L),a(W)),a(l),j(c,e),j(m,e),j(y,e),j(v,e),j(Z,e),j(w,e)}}}const be='{"title":"Fine-grained FP8","local":"fine-grained-fp8","sections":[],"depth":1}';function _e(R){return ie(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class we extends re{constructor(l){super(),pe(this,l,_e,Me,se,{})}}export{we as component};
