import{s as co,z as mo,o as po,n as Je}from"../chunks/scheduler.18a86fab.js";import{S as ho,i as fo,g as c,s as r,r as u,A as uo,h as d,f as s,c as a,j as U,x as f,u as g,k as J,l as go,y as i,a as l,v as _,d as T,t as b,w as y}from"../chunks/index.98837b22.js";import{T as ht}from"../chunks/Tip.77304350.js";import{D as L}from"../chunks/Docstring.a1ef7999.js";import{C as ft}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as io}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Re,E as _o}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as To,a as bo}from"../chunks/HfOption.6641485e.js";function yo(C){let t,h='This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.',n,p,M="Click on the TrOCR models in the right sidebar for more examples of how to apply TrOCR to different image and text tasks.";return{c(){t=c("p"),t.innerHTML=h,n=r(),p=c("p"),p.textContent=M},l(m){t=d(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kk4gup"&&(t.innerHTML=h),n=a(m),p=d(m,"P",{"data-svelte-h":!0}),f(p)!=="svelte-gzyv7u"&&(p.textContent=M)},m(m,k){l(m,t,k),l(m,n,k),l(m,p,k)},p:Je,d(m){m&&(s(t),s(n),s(p))}}}function Mo(C){let t,h;return t=new ft({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyT0NSUHJvY2Vzc29yJTJDJTIwVmlzaW9uRW5jb2RlckRlY29kZXJNb2RlbCUwQWltcG9ydCUyMHJlcXVlc3RzJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwVHJPQ1JQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnRyb2NyLWJhc2UtaGFuZHdyaXR0ZW4lMjIpJTBBbW9kZWwlMjAlM0QlMjBWaXNpb25FbmNvZGVyRGVjb2Rlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZ0cm9jci1iYXNlLWhhbmR3cml0dGVuJTIyKSUwQSUwQSUyMyUyMGxvYWQlMjBpbWFnZSUyMGZyb20lMjB0aGUlMjBJQU0lMjBkYXRhc2V0JTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZma2kudGljLmhlaWEtZnIuY2glMkZzdGF0aWMlMkZpbWclMkZhMDEtMTIyLTAyLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMlJHQiUyMiklMEElMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUocGl4ZWxfdmFsdWVzKSUwQSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQXByaW50KGdlbmVyYXRlZF90ZXh0KQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRProcessor, VisionEncoderDecoderModel
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)
model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)

<span class="hljs-comment"># load image from the IAM dataset</span>
url = <span class="hljs-string">&quot;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
generated_ids = model.generate(pixel_values)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-built_in">print</span>(generated_text)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,p){_(t,n,p),h=!0},p:Je,i(n){h||(T(t.$$.fragment,n),h=!0)},o(n){b(t.$$.fragment,n),h=!1},d(n){y(t,n)}}}function vo(C){let t,h;return t=new bo({props:{id:"usage",option:"AutoModel",$$slots:{default:[Mo]},$$scope:{ctx:C}}}),{c(){u(t.$$.fragment)},l(n){g(t.$$.fragment,n)},m(n,p){_(t,n,p),h=!0},p(n,p){const M={};p&2&&(M.$$scope={dirty:p,ctx:n}),t.$set(M)},i(n){h||(T(t.$$.fragment,n),h=!0)},o(n){b(t.$$.fragment,n),h=!1},d(n){y(t,n)}}}function wo(C){let t,h="Example:",n,p,M;return p=new ft({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyT0NSQ29uZmlnJTJDJTIwVHJPQ1JGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBUck9DUi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFRyT0NSQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMFRyT0NSLWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFRyT0NSRm9yQ2F1c2FsTE0oY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRConfig, TrOCRForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a TrOCR-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = TrOCRConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the TrOCR-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TrOCRForCausalLM(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,n=r(),u(p.$$.fragment)},l(m){t=d(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=h),n=a(m),g(p.$$.fragment,m)},m(m,k){l(m,t,k),l(m,n,k),_(p,m,k),M=!0},p:Je,i(m){M||(T(p.$$.fragment,m),M=!0)},o(m){b(p.$$.fragment,m),M=!1},d(m){m&&(s(t),s(n)),y(p,m)}}}function Co(C){let t,h=`This class method is simply calling the feature extractor
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a>, image processor
<a href="/docs/transformers/v4.56.2/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> and the tokenizer
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code> methods. Please refer to the docstrings of the
methods above for more information.`;return{c(){t=c("p"),t.innerHTML=h},l(n){t=d(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-vj9ud3"&&(t.innerHTML=h)},m(n,p){l(n,t,p)},p:Je,d(n){n&&s(t)}}}function ko(C){let t,h=`This class method is simply calling <a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a>. Please refer to the docstrings of the
methods above for more information.`;return{c(){t=c("p"),t.innerHTML=h},l(n){t=d(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1euzcqa"&&(t.innerHTML=h)},m(n,p){l(n,t,p)},p:Je,d(n){n&&s(t)}}}function Ro(C){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(n){t=d(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,p){l(n,t,p)},p:Je,d(n){n&&s(t)}}}function Jo(C){let t,h="Example:",n,p,M;return p=new ft({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    TrOCRConfig,
<span class="hljs-meta">... </span>    TrOCRProcessor,
<span class="hljs-meta">... </span>    TrOCRForCausalLM,
<span class="hljs-meta">... </span>    ViTConfig,
<span class="hljs-meta">... </span>    ViTModel,
<span class="hljs-meta">... </span>    VisionEncoderDecoderModel,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># init vision2text model with random weights</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder = ViTModel(ViTConfig())
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder = TrOCRForCausalLM(TrOCRConfig())
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If you want to start from the pretrained model, load the checkpoint with \`VisionEncoderDecoderModel\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load image from the IAM dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;industry, &#x27; Mr. Brown commented icily. &#x27; Let us have a&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = processor.tokenizer.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = processor.tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = processor.tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">5.30</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inference</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text
<span class="hljs-string">&#x27;industry, &quot; Mr. Brown commented icily. &quot; Let us have a&#x27;</span>`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,n=r(),u(p.$$.fragment)},l(m){t=d(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=h),n=a(m),g(p.$$.fragment,m)},m(m,k){l(m,t,k),l(m,n,k),_(p,m,k),M=!0},p:Je,i(m){M||(T(p.$$.fragment,m),M=!0)},o(m){b(p.$$.fragment,m),M=!1},d(m){m&&(s(t),s(n)),y(p,m)}}}function $o(C){let t,h,n,p,M,m="<em>This model was released on 2021-09-21 and added to Hugging Face Transformers on 2021-10-13.</em>",k,O,Vt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',Ge,D,Ne,A,Bt='<a href="https://huggingface.co/papers/2109.10282" rel="nofollow">TrOCR</a> is a text recognition model for both image understanding and text generation. It doesn’t require separate models for image processing or character generation. TrOCR is a simple single end-to-end system that uses a transformer to handle visual understanding and text generation.',Xe,K,Ht='You can find all the original TrOCR checkpoints under the <a href="https://huggingface.co/microsoft/models?search=trocr" rel="nofollow">Microsoft</a> organization.',Pe,V,Gt,Ee,ee,Nt='TrOCR architecture. Taken from the <a href="https://huggingface.co/papers/2109.10282">original paper</a>.',Ye,B,Qe,te,Xt='The example below demonstrates how to perform optical character recognition (OCR) with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.',Se,H,qe,oe,De,se,Pt='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the <a href="../quantization/overview">Quantization</a> overview for more available quantization backends.',Ae,ne,Et='The example below uses <a href="../quantization/bitsandbytes">bitsandbytes</a> to quantize the weights to 8-bits.',Ke,re,et,ae,tt,le,Yt='<li>TrOCR wraps <a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>/<a href="/docs/transformers/v4.56.2/en/model_doc/deit#transformers.DeiTImageProcessor">DeiTImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>/<a href="/docs/transformers/v4.56.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> into a single instance of <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRProcessor">TrOCRProcessor</a> to handle images and text.</li> <li>TrOCR is always used within the <a href="vision-encoder-decoder">VisionEncoderDecoder</a> framework.</li>',ot,ie,st,ce,Qt='<li>A blog post on <a href="https://huggingface.co/blog/document-ai" rel="nofollow">Accelerating Document AI</a> with TrOCR.</li> <li>A blog post on how to <a href="https://github.com/philschmid/document-ai-transformers" rel="nofollow">Document AI</a> with TrOCR.</li> <li>A notebook on how to <a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb" rel="nofollow">finetune TrOCR on IAM Handwriting Database using Seq2SeqTrainer</a>.</li> <li>An interactive-demo on <a href="https://huggingface.co/spaces/nielsr/TrOCR-handwritten" rel="nofollow">TrOCR handwritten character recognition</a>.</li> <li>A notebook on <a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb" rel="nofollow">inference with TrOCR</a> and Gradio demo.</li> <li>A notebook on <a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb" rel="nofollow">evaluating TrOCR on the IAM test set</a>.</li>',nt,de,rt,$,me,ut,$e,St=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a>. It is used to instantiate an
TrOCR model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the TrOCR
<a href="https://huggingface.co/microsoft/trocr-base-handwritten" rel="nofollow">microsoft/trocr-base-handwritten</a> architecture.`,gt,je,qt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,_t,G,at,pe,lt,v,he,Tt,Ze,Dt="Constructs a TrOCR processor which wraps a vision image processor and a TrOCR tokenizer into a single processor.",bt,Ue,At=`<a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRProcessor">TrOCRProcessor</a> offers all the functionalities of [<code>ViTImageProcessor</code>/<code>DeiTImageProcessor</code>] and
[<code>RobertaTokenizer</code>/<code>XLMRobertaTokenizer</code>]. See the <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRProcessor.__call__"><strong>call</strong>()</a> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for
more information.`,yt,N,fe,Mt,xe,Kt=`When used in normal mode, this method forwards all its arguments to AutoImageProcessor’s
<code>__call__()</code> and returns its output. If used in the context
<code>as_target_processor()</code> this method forwards all its arguments to TrOCRTokenizer’s
<code>~TrOCRTokenizer.__call__</code>. Please refer to the docstring of the above two methods for more information.`,vt,x,ue,wt,Ie,eo="Instantiate a processor associated with a pretrained model.",Ct,X,kt,I,ge,Rt,We,to=`Saves the attributes of this processor (feature extractor, tokenizer…) in the specified directory so that it
can be reloaded using the <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained">from_pretrained()</a> method.`,Jt,P,$t,E,_e,jt,ze,oo=`This method forwards all its arguments to PreTrainedTokenizer’s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,Zt,Y,Te,Ut,Fe,so=`This method forwards all its arguments to PreTrainedTokenizer’s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,it,be,ct,R,ye,xt,Le,no='The TrOCR Decoder with a language modeling head. Can be used as the decoder part of <a href="/docs/transformers/v4.56.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> and',It,Oe,ro=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Wt,Ve,ao=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,zt,j,Me,Ft,Be,lo='The <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',Lt,Q,Ot,S,dt,ve,mt,He,pt;return D=new Re({props:{title:"TrOCR",local:"trocr",headingTag:"h1"}}),B=new ht({props:{warning:!1,$$slots:{default:[yo]},$$scope:{ctx:C}}}),H=new To({props:{id:"usage",options:["AutoModel"],$$slots:{default:[vo]},$$scope:{ctx:C}}}),oe=new Re({props:{title:"Quantization",local:"quantization",headingTag:"h2"}}),re=new ft({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMGJpdHNhbmRieXRlcyUyMGFjY2VsZXJhdGUlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwVHJPQ1JQcm9jZXNzb3IlMkMlMjBWaXNpb25FbmNvZGVyRGVjb2Rlck1vZGVsJTJDJTIwQml0c2FuZEJ5dGVzQ29uZmlnJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEElMjMlMjBTZXQlMjB1cCUyMHRoZSUyMHF1YW50aXphdGlvbiUyMGNvbmZpZ3VyYXRpb24lMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwQml0c2FuZEJ5dGVzQ29uZmlnKGxvYWRfaW5fOGJpdCUzRFRydWUpJTBBJTBBJTIzJTIwVXNlJTIwYSUyMGxhcmdlJTIwY2hlY2twb2ludCUyMGZvciUyMGElMjBtb3JlJTIwbm90aWNlYWJsZSUyMGltcGFjdCUwQXByb2Nlc3NvciUyMCUzRCUyMFRyT0NSUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZ0cm9jci1sYXJnZS1oYW5kd3JpdHRlbiUyMiklMEFtb2RlbCUyMCUzRCUyMFZpc2lvbkVuY29kZXJEZWNvZGVyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm1pY3Jvc29mdCUyRnRyb2NyLWxhcmdlLWhhbmR3cml0dGVuJTIyJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMEEpJTBBJTBBJTIzJTIwbG9hZCUyMGltYWdlJTIwZnJvbSUyMHRoZSUyMElBTSUyMGRhdGFzZXQlMEF1cmwlMjAlM0QlMjAlMjIlNUJodHRwcyUzQSUyRiUyRmZraS50aWMuaGVpYS1mci5jaCUyRnN0YXRpYyUyRmltZyUyRmEwMS0xMjItMDIuanBnJTVEKGh0dHBzJTNBJTJGJTJGZmtpLnRpYy5oZWlhLWZyLmNoJTJGc3RhdGljJTJGaW1nJTJGYTAxLTEyMi0wMi5qcGcpJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpLmNvbnZlcnQoJTIyUkdCJTIyKSUwQSUwQXBpeGVsX3ZhbHVlcyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnBpeGVsX3ZhbHVlcyUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShwaXhlbF92YWx1ZXMpJTBBJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEJTBBcHJpbnQoZ2VuZXJhdGVkX3RleHQp",highlighted:`<span class="hljs-comment"># pip install bitsandbytes accelerate</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRProcessor, VisionEncoderDecoderModel, BitsandBytesConfig
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-comment"># Set up the quantization configuration</span>
quantization_config = BitsandBytesConfig(load_in_8bit=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Use a large checkpoint for a more noticeable impact</span>
processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-large-handwritten&quot;</span>)
model = VisionEncoderDecoderModel.from_pretrained(
    <span class="hljs-string">&quot;microsoft/trocr-large-handwritten&quot;</span>,
    quantization_config=quantization_config
)

<span class="hljs-comment"># load image from the IAM dataset</span>
url = <span class="hljs-string">&quot;[https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg](https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg)&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
generated_ids = model.generate(pixel_values)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-built_in">print</span>(generated_text)`,wrap:!1}}),ae=new Re({props:{title:"Notes",local:"notes",headingTag:"h2"}}),ie=new Re({props:{title:"Resources",local:"resources",headingTag:"h2"}}),de=new Re({props:{title:"TrOCRConfig",local:"transformers.TrOCRConfig",headingTag:"h2"}}),me=new L({props:{name:"class transformers.TrOCRConfig",anchor:"transformers.TrOCRConfig",parameters:[{name:"vocab_size",val:" = 50265"},{name:"d_model",val:" = 1024"},{name:"decoder_layers",val:" = 12"},{name:"decoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"activation_function",val:" = 'gelu'"},{name:"max_position_embeddings",val:" = 512"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"decoder_start_token_id",val:" = 2"},{name:"init_std",val:" = 0.02"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"scale_embedding",val:" = False"},{name:"use_learned_position_embeddings",val:" = True"},{name:"layernorm_embedding",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TrOCRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the TrOCR model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a>.`,name:"vocab_size"},{anchor:"transformers.TrOCRConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.TrOCRConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.TrOCRConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.TrOCRConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.TrOCRConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the pooler. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.TrOCRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.TrOCRConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, and pooler.`,name:"dropout"},{anchor:"transformers.TrOCRConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.TrOCRConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.TrOCRConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.TrOCRConfig.decoder_layerdrop",description:`<strong>decoder_layerdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The LayerDrop probability for the decoder. See the [LayerDrop paper](see <a href="https://huggingface.co/papers/1909.11556" rel="nofollow">https://huggingface.co/papers/1909.11556</a>)
for more details.`,name:"decoder_layerdrop"},{anchor:"transformers.TrOCRConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.TrOCRConfig.scale_embedding",description:`<strong>scale_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to scale the word embeddings by sqrt(d_model).`,name:"scale_embedding"},{anchor:"transformers.TrOCRConfig.use_learned_position_embeddings",description:`<strong>use_learned_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use learned position embeddings. If not, sinusoidal position embeddings will be used.`,name:"use_learned_position_embeddings"},{anchor:"transformers.TrOCRConfig.layernorm_embedding",description:`<strong>layernorm_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use a layernorm after the word + position embeddings.`,name:"layernorm_embedding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/trocr/configuration_trocr.py#L24"}}),G=new io({props:{anchor:"transformers.TrOCRConfig.example",$$slots:{default:[wo]},$$scope:{ctx:C}}}),pe=new Re({props:{title:"TrOCRProcessor",local:"transformers.TrOCRProcessor",headingTag:"h2"}}),he=new L({props:{name:"class transformers.TrOCRProcessor",anchor:"transformers.TrOCRProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TrOCRProcessor.image_processor",description:`<strong>image_processor</strong> ([<code>ViTImageProcessor</code>/<code>DeiTImageProcessor</code>], <em>optional</em>) &#x2014;
An instance of [<code>ViTImageProcessor</code>/<code>DeiTImageProcessor</code>]. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.TrOCRProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>RobertaTokenizer</code>/<code>XLMRobertaTokenizer</code>], <em>optional</em>) &#x2014;
An instance of [<code>RobertaTokenizer</code>/<code>XLMRobertaTokenizer</code>]. The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/trocr/processing_trocr.py#L33"}}),fe=new L({props:{name:"__call__",anchor:"transformers.TrOCRProcessor.__call__",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None"},{name:"text",val:": typing.Union[str, list[str], list[list[str]]] = None"},{name:"audio",val:" = None"},{name:"videos",val:" = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.trocr.processing_trocr.TrOCRProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/trocr/processing_trocr.py#L72"}}),ue=new L({props:{name:"from_pretrained",anchor:"transformers.TrOCRProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"cache_dir",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": typing.Union[bool, str, NoneType] = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TrOCRProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TrOCRProcessor.from_pretrained.*kwargs",description:`*<strong>*kwargs</strong> &#x2014;
Additional keyword arguments passed along to both
<a href="/docs/transformers/v4.56.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/processing_utils.py#L1272"}}),X=new ht({props:{$$slots:{default:[Co]},$$scope:{ctx:C}}}),ge=new L({props:{name:"save_pretrained",anchor:"transformers.TrOCRProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"push_to_hub",val:": bool = False"},{name:"legacy_serialization",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TrOCRProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.TrOCRProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.TrOCRProcessor.save_pretrained.legacy_serialization",description:`<strong>legacy_serialization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to save processor attributes in separate config files (legacy) or in processor&#x2019;s config
file as a nested dict. Saving all attributes in a single dict will become the default in future versions.
Set to <code>legacy_serialization=True</code> until then.`,name:"legacy_serialization"},{anchor:"transformers.TrOCRProcessor.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/processing_utils.py#L653"}}),P=new ht({props:{$$slots:{default:[ko]},$$scope:{ctx:C}}}),_e=new L({props:{name:"batch_decode",anchor:"transformers.TrOCRProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/processing_utils.py#L1419"}}),Te=new L({props:{name:"decode",anchor:"transformers.TrOCRProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/processing_utils.py#L1428"}}),be=new Re({props:{title:"TrOCRForCausalLM",local:"transformers.TrOCRForCausalLM",headingTag:"h2"}}),ye=new L({props:{name:"class transformers.TrOCRForCausalLM",anchor:"transformers.TrOCRForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.TrOCRForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/trocr/modeling_trocr.py#L711"}}),Me=new L({props:{name:"forward",anchor:"transformers.TrOCRForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[tuple[tuple[torch.FloatTensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"cache_position",val:": typing.Optional[torch.Tensor] = None"}],parametersDescription:[{anchor:"transformers.TrOCRForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TrOCRForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TrOCRForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.TrOCRForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.TrOCRForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TrOCRForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.TrOCRForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple[tuple[torch.FloatTensor]]</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Only <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> instance is allowed as input, see our <a href="https://huggingface.co/docs/transformers/en/kv_cache" rel="nofollow">kv cache guide</a>.
If no <code>past_key_values</code> are passed, <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> will be initialized by default.</p>
<p>The model will output the same cache format that is fed as input.</p>
<p>If <code>past_key_values</code> are used, the user is expected to input only unprocessed <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, unprocessed_length)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TrOCRForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TrOCRForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.TrOCRForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.TrOCRForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TrOCRForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TrOCRForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TrOCRForCausalLM.forward.cache_position",description:`<strong>cache_position</strong> (<code>torch.Tensor</code> of shape <code>(sequence_length)</code>, <em>optional</em>) &#x2014;
Indices depicting the position of the input sequence tokens in the sequence. Contrarily to <code>position_ids</code>,
this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
the complete sequence length.`,name:"cache_position"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/trocr/modeling_trocr.py#L743",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/trocr#transformers.TrOCRConfig"
>TrOCRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Q=new ht({props:{$$slots:{default:[Ro]},$$scope:{ctx:C}}}),S=new io({props:{anchor:"transformers.TrOCRForCausalLM.forward.example",$$slots:{default:[Jo]},$$scope:{ctx:C}}}),ve=new _o({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/trocr.md"}}),{c(){t=c("meta"),h=r(),n=c("p"),p=r(),M=c("p"),M.innerHTML=m,k=r(),O=c("div"),O.innerHTML=Vt,Ge=r(),u(D.$$.fragment),Ne=r(),A=c("p"),A.innerHTML=Bt,Xe=r(),K=c("p"),K.innerHTML=Ht,Pe=r(),V=c("img"),Ee=r(),ee=c("small"),ee.innerHTML=Nt,Ye=r(),u(B.$$.fragment),Qe=r(),te=c("p"),te.innerHTML=Xt,Se=r(),u(H.$$.fragment),qe=r(),u(oe.$$.fragment),De=r(),se=c("p"),se.innerHTML=Pt,Ae=r(),ne=c("p"),ne.innerHTML=Et,Ke=r(),u(re.$$.fragment),et=r(),u(ae.$$.fragment),tt=r(),le=c("ul"),le.innerHTML=Yt,ot=r(),u(ie.$$.fragment),st=r(),ce=c("ul"),ce.innerHTML=Qt,nt=r(),u(de.$$.fragment),rt=r(),$=c("div"),u(me.$$.fragment),ut=r(),$e=c("p"),$e.innerHTML=St,gt=r(),je=c("p"),je.innerHTML=qt,_t=r(),u(G.$$.fragment),at=r(),u(pe.$$.fragment),lt=r(),v=c("div"),u(he.$$.fragment),Tt=r(),Ze=c("p"),Ze.textContent=Dt,bt=r(),Ue=c("p"),Ue.innerHTML=At,yt=r(),N=c("div"),u(fe.$$.fragment),Mt=r(),xe=c("p"),xe.innerHTML=Kt,vt=r(),x=c("div"),u(ue.$$.fragment),wt=r(),Ie=c("p"),Ie.textContent=eo,Ct=r(),u(X.$$.fragment),kt=r(),I=c("div"),u(ge.$$.fragment),Rt=r(),We=c("p"),We.innerHTML=to,Jt=r(),u(P.$$.fragment),$t=r(),E=c("div"),u(_e.$$.fragment),jt=r(),ze=c("p"),ze.innerHTML=oo,Zt=r(),Y=c("div"),u(Te.$$.fragment),Ut=r(),Fe=c("p"),Fe.innerHTML=so,it=r(),u(be.$$.fragment),ct=r(),R=c("div"),u(ye.$$.fragment),xt=r(),Le=c("p"),Le.innerHTML=no,It=r(),Oe=c("p"),Oe.innerHTML=ro,Wt=r(),Ve=c("p"),Ve.innerHTML=ao,zt=r(),j=c("div"),u(Me.$$.fragment),Ft=r(),Be=c("p"),Be.innerHTML=lo,Lt=r(),u(Q.$$.fragment),Ot=r(),u(S.$$.fragment),dt=r(),u(ve.$$.fragment),mt=r(),He=c("p"),this.h()},l(e){const o=uo("svelte-u9bgzb",document.head);t=d(o,"META",{name:!0,content:!0}),o.forEach(s),h=a(e),n=d(e,"P",{}),U(n).forEach(s),p=a(e),M=d(e,"P",{"data-svelte-h":!0}),f(M)!=="svelte-4106zl"&&(M.innerHTML=m),k=a(e),O=d(e,"DIV",{style:!0,"data-svelte-h":!0}),f(O)!=="svelte-383xsf"&&(O.innerHTML=Vt),Ge=a(e),g(D.$$.fragment,e),Ne=a(e),A=d(e,"P",{"data-svelte-h":!0}),f(A)!=="svelte-1k15zx6"&&(A.innerHTML=Bt),Xe=a(e),K=d(e,"P",{"data-svelte-h":!0}),f(K)!=="svelte-17savu8"&&(K.innerHTML=Ht),Pe=a(e),V=d(e,"IMG",{src:!0,alt:!0,width:!0}),Ee=a(e),ee=d(e,"SMALL",{"data-svelte-h":!0}),f(ee)!=="svelte-1c5ir54"&&(ee.innerHTML=Nt),Ye=a(e),g(B.$$.fragment,e),Qe=a(e),te=d(e,"P",{"data-svelte-h":!0}),f(te)!=="svelte-i5mmgh"&&(te.innerHTML=Xt),Se=a(e),g(H.$$.fragment,e),qe=a(e),g(oe.$$.fragment,e),De=a(e),se=d(e,"P",{"data-svelte-h":!0}),f(se)!=="svelte-nf5ooi"&&(se.innerHTML=Pt),Ae=a(e),ne=d(e,"P",{"data-svelte-h":!0}),f(ne)!=="svelte-u672qo"&&(ne.innerHTML=Et),Ke=a(e),g(re.$$.fragment,e),et=a(e),g(ae.$$.fragment,e),tt=a(e),le=d(e,"UL",{"data-svelte-h":!0}),f(le)!=="svelte-b8otk5"&&(le.innerHTML=Yt),ot=a(e),g(ie.$$.fragment,e),st=a(e),ce=d(e,"UL",{"data-svelte-h":!0}),f(ce)!=="svelte-s96s3v"&&(ce.innerHTML=Qt),nt=a(e),g(de.$$.fragment,e),rt=a(e),$=d(e,"DIV",{class:!0});var Z=U($);g(me.$$.fragment,Z),ut=a(Z),$e=d(Z,"P",{"data-svelte-h":!0}),f($e)!=="svelte-15bcbjf"&&($e.innerHTML=St),gt=a(Z),je=d(Z,"P",{"data-svelte-h":!0}),f(je)!=="svelte-1ek1ss9"&&(je.innerHTML=qt),_t=a(Z),g(G.$$.fragment,Z),Z.forEach(s),at=a(e),g(pe.$$.fragment,e),lt=a(e),v=d(e,"DIV",{class:!0});var w=U(v);g(he.$$.fragment,w),Tt=a(w),Ze=d(w,"P",{"data-svelte-h":!0}),f(Ze)!=="svelte-ndaubh"&&(Ze.textContent=Dt),bt=a(w),Ue=d(w,"P",{"data-svelte-h":!0}),f(Ue)!=="svelte-6ha3mp"&&(Ue.innerHTML=At),yt=a(w),N=d(w,"DIV",{class:!0});var we=U(N);g(fe.$$.fragment,we),Mt=a(we),xe=d(we,"P",{"data-svelte-h":!0}),f(xe)!=="svelte-xwje5k"&&(xe.innerHTML=Kt),we.forEach(s),vt=a(w),x=d(w,"DIV",{class:!0});var z=U(x);g(ue.$$.fragment,z),wt=a(z),Ie=d(z,"P",{"data-svelte-h":!0}),f(Ie)!=="svelte-1cj8dcb"&&(Ie.textContent=eo),Ct=a(z),g(X.$$.fragment,z),z.forEach(s),kt=a(w),I=d(w,"DIV",{class:!0});var F=U(I);g(ge.$$.fragment,F),Rt=a(F),We=d(F,"P",{"data-svelte-h":!0}),f(We)!=="svelte-1fjnvpp"&&(We.innerHTML=to),Jt=a(F),g(P.$$.fragment,F),F.forEach(s),$t=a(w),E=d(w,"DIV",{class:!0});var Ce=U(E);g(_e.$$.fragment,Ce),jt=a(Ce),ze=d(Ce,"P",{"data-svelte-h":!0}),f(ze)!=="svelte-njenc7"&&(ze.innerHTML=oo),Ce.forEach(s),Zt=a(w),Y=d(w,"DIV",{class:!0});var ke=U(Y);g(Te.$$.fragment,ke),Ut=a(ke),Fe=d(ke,"P",{"data-svelte-h":!0}),f(Fe)!=="svelte-f8t9ud"&&(Fe.innerHTML=so),ke.forEach(s),w.forEach(s),it=a(e),g(be.$$.fragment,e),ct=a(e),R=d(e,"DIV",{class:!0});var W=U(R);g(ye.$$.fragment,W),xt=a(W),Le=d(W,"P",{"data-svelte-h":!0}),f(Le)!=="svelte-frt75v"&&(Le.innerHTML=no),It=a(W),Oe=d(W,"P",{"data-svelte-h":!0}),f(Oe)!=="svelte-q52n56"&&(Oe.innerHTML=ro),Wt=a(W),Ve=d(W,"P",{"data-svelte-h":!0}),f(Ve)!=="svelte-hswkmf"&&(Ve.innerHTML=ao),zt=a(W),j=d(W,"DIV",{class:!0});var q=U(j);g(Me.$$.fragment,q),Ft=a(q),Be=d(q,"P",{"data-svelte-h":!0}),f(Be)!=="svelte-key9b3"&&(Be.innerHTML=lo),Lt=a(q),g(Q.$$.fragment,q),Ot=a(q),g(S.$$.fragment,q),q.forEach(s),W.forEach(s),dt=a(e),g(ve.$$.fragment,e),mt=a(e),He=d(e,"P",{}),U(He).forEach(s),this.h()},h(){J(t,"name","hf:doc:metadata"),J(t,"content",jo),go(O,"float","right"),mo(V.src,Gt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/trocr_architecture.jpg")||J(V,"src",Gt),J(V,"alt","drawing"),J(V,"width","600"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){i(document.head,t),l(e,h,o),l(e,n,o),l(e,p,o),l(e,M,o),l(e,k,o),l(e,O,o),l(e,Ge,o),_(D,e,o),l(e,Ne,o),l(e,A,o),l(e,Xe,o),l(e,K,o),l(e,Pe,o),l(e,V,o),l(e,Ee,o),l(e,ee,o),l(e,Ye,o),_(B,e,o),l(e,Qe,o),l(e,te,o),l(e,Se,o),_(H,e,o),l(e,qe,o),_(oe,e,o),l(e,De,o),l(e,se,o),l(e,Ae,o),l(e,ne,o),l(e,Ke,o),_(re,e,o),l(e,et,o),_(ae,e,o),l(e,tt,o),l(e,le,o),l(e,ot,o),_(ie,e,o),l(e,st,o),l(e,ce,o),l(e,nt,o),_(de,e,o),l(e,rt,o),l(e,$,o),_(me,$,null),i($,ut),i($,$e),i($,gt),i($,je),i($,_t),_(G,$,null),l(e,at,o),_(pe,e,o),l(e,lt,o),l(e,v,o),_(he,v,null),i(v,Tt),i(v,Ze),i(v,bt),i(v,Ue),i(v,yt),i(v,N),_(fe,N,null),i(N,Mt),i(N,xe),i(v,vt),i(v,x),_(ue,x,null),i(x,wt),i(x,Ie),i(x,Ct),_(X,x,null),i(v,kt),i(v,I),_(ge,I,null),i(I,Rt),i(I,We),i(I,Jt),_(P,I,null),i(v,$t),i(v,E),_(_e,E,null),i(E,jt),i(E,ze),i(v,Zt),i(v,Y),_(Te,Y,null),i(Y,Ut),i(Y,Fe),l(e,it,o),_(be,e,o),l(e,ct,o),l(e,R,o),_(ye,R,null),i(R,xt),i(R,Le),i(R,It),i(R,Oe),i(R,Wt),i(R,Ve),i(R,zt),i(R,j),_(Me,j,null),i(j,Ft),i(j,Be),i(j,Lt),_(Q,j,null),i(j,Ot),_(S,j,null),l(e,dt,o),_(ve,e,o),l(e,mt,o),l(e,He,o),pt=!0},p(e,[o]){const Z={};o&2&&(Z.$$scope={dirty:o,ctx:e}),B.$set(Z);const w={};o&2&&(w.$$scope={dirty:o,ctx:e}),H.$set(w);const we={};o&2&&(we.$$scope={dirty:o,ctx:e}),G.$set(we);const z={};o&2&&(z.$$scope={dirty:o,ctx:e}),X.$set(z);const F={};o&2&&(F.$$scope={dirty:o,ctx:e}),P.$set(F);const Ce={};o&2&&(Ce.$$scope={dirty:o,ctx:e}),Q.$set(Ce);const ke={};o&2&&(ke.$$scope={dirty:o,ctx:e}),S.$set(ke)},i(e){pt||(T(D.$$.fragment,e),T(B.$$.fragment,e),T(H.$$.fragment,e),T(oe.$$.fragment,e),T(re.$$.fragment,e),T(ae.$$.fragment,e),T(ie.$$.fragment,e),T(de.$$.fragment,e),T(me.$$.fragment,e),T(G.$$.fragment,e),T(pe.$$.fragment,e),T(he.$$.fragment,e),T(fe.$$.fragment,e),T(ue.$$.fragment,e),T(X.$$.fragment,e),T(ge.$$.fragment,e),T(P.$$.fragment,e),T(_e.$$.fragment,e),T(Te.$$.fragment,e),T(be.$$.fragment,e),T(ye.$$.fragment,e),T(Me.$$.fragment,e),T(Q.$$.fragment,e),T(S.$$.fragment,e),T(ve.$$.fragment,e),pt=!0)},o(e){b(D.$$.fragment,e),b(B.$$.fragment,e),b(H.$$.fragment,e),b(oe.$$.fragment,e),b(re.$$.fragment,e),b(ae.$$.fragment,e),b(ie.$$.fragment,e),b(de.$$.fragment,e),b(me.$$.fragment,e),b(G.$$.fragment,e),b(pe.$$.fragment,e),b(he.$$.fragment,e),b(fe.$$.fragment,e),b(ue.$$.fragment,e),b(X.$$.fragment,e),b(ge.$$.fragment,e),b(P.$$.fragment,e),b(_e.$$.fragment,e),b(Te.$$.fragment,e),b(be.$$.fragment,e),b(ye.$$.fragment,e),b(Me.$$.fragment,e),b(Q.$$.fragment,e),b(S.$$.fragment,e),b(ve.$$.fragment,e),pt=!1},d(e){e&&(s(h),s(n),s(p),s(M),s(k),s(O),s(Ge),s(Ne),s(A),s(Xe),s(K),s(Pe),s(V),s(Ee),s(ee),s(Ye),s(Qe),s(te),s(Se),s(qe),s(De),s(se),s(Ae),s(ne),s(Ke),s(et),s(tt),s(le),s(ot),s(st),s(ce),s(nt),s(rt),s($),s(at),s(lt),s(v),s(it),s(ct),s(R),s(dt),s(mt),s(He)),s(t),y(D,e),y(B,e),y(H,e),y(oe,e),y(re,e),y(ae,e),y(ie,e),y(de,e),y(me),y(G),y(pe,e),y(he),y(fe),y(ue),y(X),y(ge),y(P),y(_e),y(Te),y(be,e),y(ye),y(Me),y(Q),y(S),y(ve,e)}}}const jo='{"title":"TrOCR","local":"trocr","sections":[{"title":"Quantization","local":"quantization","sections":[],"depth":2},{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"TrOCRConfig","local":"transformers.TrOCRConfig","sections":[],"depth":2},{"title":"TrOCRProcessor","local":"transformers.TrOCRProcessor","sections":[],"depth":2},{"title":"TrOCRForCausalLM","local":"transformers.TrOCRForCausalLM","sections":[],"depth":2}],"depth":1}';function Zo(C){return po(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vo extends ho{constructor(t){super(),fo(this,t,Zo,$o,co,{})}}export{Vo as component};
