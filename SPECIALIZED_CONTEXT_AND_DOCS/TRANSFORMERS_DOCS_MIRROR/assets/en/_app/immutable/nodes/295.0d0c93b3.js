import{s as Gt,o as Ut,n as ie}from"../chunks/scheduler.18a86fab.js";import{S as kt,i as Jt,g as m,s as l,r as f,A as xt,h as g,f as s,c as d,j as ae,x as h,u as _,k as re,l as Ct,y as p,a as r,v as M,d as b,t as y,w as v}from"../chunks/index.98837b22.js";import{T as st}from"../chunks/Tip.77304350.js";import{D as be}from"../chunks/Docstring.a1ef7999.js";import{C as Ve}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as at}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ye,E as $t}from"../chunks/getInferenceSnippets.06c2775f.js";import{H as Dt,a as Zt}from"../chunks/HfOption.6641485e.js";function It(w){let t,c="Click on the MM Grounding DINO models in the right sidebar for more examples of how to apply MM Grounding DINO to different MM Grounding DINO tasks.";return{c(){t=m("p"),t.textContent=c},l(n){t=g(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-12jboxc"&&(t.textContent=c)},m(n,i){r(n,t,i)},p:ie,d(n){n&&s(t)}}}function Wt(w){let t,c;return t=new Ve({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForZeroShotObjectDetection, AutoProcessor, infer_device
<span class="hljs-keyword">from</span> transformers.image_utils <span class="hljs-keyword">import</span> load_image


<span class="hljs-comment"># Prepare processor and model</span>
model_id = <span class="hljs-string">&quot;openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det&quot;</span>
device = infer_device()
processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)

<span class="hljs-comment"># Prepare inputs</span>
image_url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = load_image(image_url)
text_labels = [[<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote control&quot;</span>]]
inputs = processor(images=image, text=text_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-comment"># Run inference</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="hljs-comment"># Postprocess outputs</span>
results = processor.post_process_grounded_object_detection(
    outputs,
    threshold=<span class="hljs-number">0.4</span>,
    target_sizes=[(image.height, image.width)]
)

<span class="hljs-comment"># Retrieve the first image result</span>
result = results[<span class="hljs-number">0</span>]
<span class="hljs-keyword">for</span> box, score, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(result[<span class="hljs-string">&quot;boxes&quot;</span>], result[<span class="hljs-string">&quot;scores&quot;</span>], result[<span class="hljs-string">&quot;labels&quot;</span>]):
    box = [<span class="hljs-built_in">round</span>(x, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> box.tolist()]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{labels}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)`,wrap:!1}}),{c(){f(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,i){M(t,n,i),c=!0},p:ie,i(n){c||(b(t.$$.fragment,n),c=!0)},o(n){y(t.$$.fragment,n),c=!1},d(n){v(t,n)}}}function Rt(w){let t,c;return t=new Zt({props:{id:"usage",option:"AutoModel",$$slots:{default:[Wt]},$$scope:{ctx:w}}}),{c(){f(t.$$.fragment)},l(n){_(t.$$.fragment,n)},m(n,i){M(t,n,i),c=!0},p(n,i){const u={};i&2&&(u.$$scope={dirty:i,ctx:n}),t.$set(u)},i(n){c||(b(t.$$.fragment,n),c=!0)},o(n){y(t.$$.fragment,n),c=!1},d(n){v(t,n)}}}function Nt(w){let t,c="Examples:",n,i,u;return i=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1NR3JvdW5kaW5nRGlub0NvbmZpZyUyQyUyME1NR3JvdW5kaW5nRGlub01vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME1NJTIwR3JvdW5kaW5nJTIwRElOTyUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwTU1Hcm91bmRpbmdEaW5vQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME1NR3JvdW5kaW5nRGlub01vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MMGroundingDinoConfig, MMGroundingDinoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a MM Grounding DINO configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MMGroundingDinoConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MMGroundingDinoModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),f(i.$$.fragment)},l(a){t=g(a,"P",{"data-svelte-h":!0}),h(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),_(i.$$.fragment,a)},m(a,j){r(a,t,j),r(a,n,j),M(i,a,j),u=!0},p:ie,i(a){u||(b(i.$$.fragment,a),u=!0)},o(a){y(i.$$.fragment,a),u=!1},d(a){a&&(s(t),s(n)),v(i,a)}}}function Et(w){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=c},l(n){t=g(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,i){r(n,t,i)},p:ie,d(n){n&&s(t)}}}function Bt(w){let t,c="Examples:",n,i,u;return i=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWwlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEF0ZXh0JTIwJTNEJTIwJTIyYSUyMGNhdC4lMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJJREVBLVJlc2VhcmNoJTJGZ3JvdW5kaW5nLWRpbm8tdGlueSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIySURFQS1SZXNlYXJjaCUyRmdyb3VuZGluZy1kaW5vLXRpbnklMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwdGV4dCUzRHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;a cat.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/grounding-dino-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;IDEA-Research/grounding-dino-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">900</span>, <span class="hljs-number">256</span>]`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),f(i.$$.fragment)},l(a){t=g(a,"P",{"data-svelte-h":!0}),h(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),_(i.$$.fragment,a)},m(a,j){r(a,t,j),r(a,n,j),M(i,a,j),u=!0},p:ie,i(a){u||(b(i.$$.fragment,a),u=!0)},o(a){y(i.$$.fragment,a),u=!1},d(a){a&&(s(t),s(n)),v(i,a)}}}function Vt(w){let t,c=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=c},l(n){t=g(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=c)},m(n,i){r(n,t,i)},p:ie,d(n){n&&s(t)}}}function Ht(w){let t,c="Examples:",n,i,u;return i=new Ve({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;IDEA-Research/grounding-dino-tiny&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(image_url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Check for cats and remote controls</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_labels = [[<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote control&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=text_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process_grounded_object_detection(
<span class="hljs-meta">... </span>    outputs,
<span class="hljs-meta">... </span>    threshold=<span class="hljs-number">0.4</span>,
<span class="hljs-meta">... </span>    text_threshold=<span class="hljs-number">0.3</span>,
<span class="hljs-meta">... </span>    target_sizes=[(image.height, image.width)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Retrieve the first image result</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>result = results[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, text_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(result[<span class="hljs-string">&quot;boxes&quot;</span>], result[<span class="hljs-string">&quot;scores&quot;</span>], result[<span class="hljs-string">&quot;text_labels&quot;</span>]):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(x, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text_label}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.479</span> at location [<span class="hljs-number">344.7</span>, <span class="hljs-number">23.11</span>, <span class="hljs-number">637.18</span>, <span class="hljs-number">374.28</span>]
Detected a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.438</span> at location [<span class="hljs-number">12.27</span>, <span class="hljs-number">51.91</span>, <span class="hljs-number">316.86</span>, <span class="hljs-number">472.44</span>]
Detected a remote control <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.478</span> at location [<span class="hljs-number">38.57</span>, <span class="hljs-number">70.0</span>, <span class="hljs-number">176.78</span>, <span class="hljs-number">118.18</span>]`,wrap:!1}}),{c(){t=m("p"),t.textContent=c,n=l(),f(i.$$.fragment)},l(a){t=g(a,"P",{"data-svelte-h":!0}),h(t)!=="svelte-kvfsh7"&&(t.textContent=c),n=d(a),_(i.$$.fragment,a)},m(a,j){r(a,t,j),r(a,n,j),M(i,a,j),u=!0},p:ie,i(a){u||(b(i.$$.fragment,a),u=!0)},o(a){y(i.$$.fragment,a),u=!1},d(a){a&&(s(t),s(n)),v(i,a)}}}function zt(w){let t,c,n,i,u,a="<em>This model was released on 2024-01-04 and added to Hugging Face Transformers on 2025-08-01.</em>",j,I,rt='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/></div>',ve,z,we,F,it='<a href="https://huggingface.co/papers/2401.02361" rel="nofollow">MM Grounding DINO</a> model was proposed in <a href="https://huggingface.co/papers/2401.02361" rel="nofollow">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</a> by Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang&gt;.',je,Q,lt='MM Grounding DINO improves upon the <a href="https://huggingface.co/docs/transformers/model_doc/grounding-dino" rel="nofollow">Grounding DINO</a> by improving the contrastive class head and removing the parameter sharing in the decoder, improving zero-shot detection performance on both COCO (50.6(+2.2) AP) and LVIS (31.9(+11.8) val AP and 41.4(+12.6) minival AP).',Te,L,dt='You can find all the original MM Grounding DINO checkpoints under the <a href="https://huggingface.co/collections/openmmlab-community/mm-grounding-dino-688cbde05b814c4e2832f9df" rel="nofollow">MM Grounding DINO</a> collection. This model also supports LLMDet inference. You can find LLMDet checkpoints under the <a href="https://huggingface.co/collections/iSEE-Laboratory/llmdet-688475906dc235d5f1dc678e" rel="nofollow">LLMDet</a> collection.',Ge,W,Ue,P,ct='The example below demonstrates how to generate text based on an image with the <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoModelForZeroShotObjectDetection">AutoModelForZeroShotObjectDetection</a> class.',ke,R,Je,X,xe,S,mt='<li><p>Here’s a table of models and their object detection performance results on COCO (results from <a href="https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md" rel="nofollow">official repo</a>):</p> <table><thead><tr><th>Model</th> <th>Backbone</th> <th>Pre-Train Data</th> <th>Style</th> <th>COCO mAP</th></tr></thead> <tbody><tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg</a></td> <td>Swin-T</td> <td>O365,GoldG</td> <td>Zero-shot</td> <td>50.4(+2.3)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_grit</a></td> <td>Swin-T</td> <td>O365,GoldG,GRIT</td> <td>Zero-shot</td> <td>50.5(+2.1)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_v3det</a></td> <td>Swin-T</td> <td>O365,GoldG,V3Det</td> <td>Zero-shot</td> <td>50.6(+2.2)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit_v3det" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_grit_v3det</a></td> <td>Swin-T</td> <td>O365,GoldG,GRIT,V3Det</td> <td>Zero-shot</td> <td>50.4(+2.0)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_base_o365v1_goldg_v3det" rel="nofollow">mm_grounding_dino_base_o365v1_goldg_v3det</a></td> <td>Swin-B</td> <td>O365,GoldG,V3Det</td> <td>Zero-shot</td> <td>52.5</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_base_all" rel="nofollow">mm_grounding_dino_base_all</a></td> <td>Swin-B</td> <td>O365,ALL</td> <td>-</td> <td>59.5</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_large_o365v2_oiv6_goldg" rel="nofollow">mm_grounding_dino_large_o365v2_oiv6_goldg</a></td> <td>Swin-L</td> <td>O365V2,OpenImageV6,GoldG</td> <td>Zero-shot</td> <td>53.0</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_large_all" rel="nofollow">mm_grounding_dino_large_all</a></td> <td>Swin-L</td> <td>O365V2,OpenImageV6,ALL</td> <td>-</td> <td>60.3</td></tr></tbody></table></li> <li><p>Here’s a table of MM Grounding DINO tiny models and their object detection performance on LVIS (results from <a href="https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md" rel="nofollow">official repo</a>):</p> <table><thead><tr><th>Model</th> <th>Pre-Train Data</th> <th>MiniVal APr</th> <th>MiniVal APc</th> <th>MiniVal APf</th> <th>MiniVal AP</th> <th>Val1.0 APr</th> <th>Val1.0 APc</th> <th>Val1.0 APf</th> <th>Val1.0 AP</th></tr></thead> <tbody><tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg</a></td> <td>O365,GoldG</td> <td>28.1</td> <td>30.2</td> <td>42.0</td> <td>35.7(+6.9)</td> <td>17.1</td> <td>22.4</td> <td>36.5</td> <td>27.0(+6.9)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_grit</a></td> <td>O365,GoldG,GRIT</td> <td>26.6</td> <td>32.4</td> <td>41.8</td> <td>36.5(+7.7)</td> <td>17.3</td> <td>22.6</td> <td>36.4</td> <td>27.1(+7.0)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_v3det</a></td> <td>O365,GoldG,V3Det</td> <td>33.0</td> <td>36.0</td> <td>45.9</td> <td>40.5(+11.7)</td> <td>21.5</td> <td>25.5</td> <td>40.2</td> <td>30.6(+10.5)</td></tr> <tr><td><a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit_v3det" rel="nofollow">mm_grounding_dino_tiny_o365v1_goldg_grit_v3det</a></td> <td>O365,GoldG,GRIT,V3Det</td> <td>34.2</td> <td>37.4</td> <td>46.2</td> <td>41.4(+12.6)</td> <td>23.6</td> <td>27.6</td> <td>40.5</td> <td>31.9(+11.8)</td></tr></tbody></table></li>',Ce,A,gt='<li><p>This implementation also supports inference for <a href="https://github.com/iSEE-Laboratory/LLMDet" rel="nofollow">LLMDet</a>. Here’s a table of LLMDet models and their performance on LVIS (results from <a href="https://github.com/iSEE-Laboratory/LLMDet" rel="nofollow">official repo</a>):</p> <table><thead><tr><th>Model</th> <th>Pre-Train Data</th> <th>MiniVal APr</th> <th>MiniVal APc</th> <th>MiniVal APf</th> <th>MiniVal AP</th> <th>Val1.0 APr</th> <th>Val1.0 APc</th> <th>Val1.0 APf</th> <th>Val1.0 AP</th></tr></thead> <tbody><tr><td><a href="https://huggingface.co/iSEE-Laboratory/llmdet_tiny" rel="nofollow">llmdet_tiny</a></td> <td>(O365,GoldG,GRIT,V3Det) + GroundingCap-1M</td> <td>44.7</td> <td>37.3</td> <td>39.5</td> <td>50.7</td> <td>34.9</td> <td>26.0</td> <td>30.1</td> <td>44.3</td></tr> <tr><td><a href="https://huggingface.co/iSEE-Laboratory/llmdet_base" rel="nofollow">llmdet_base</a></td> <td>(O365,GoldG,V3Det) + GroundingCap-1M</td> <td>48.3</td> <td>40.8</td> <td>43.1</td> <td>54.3</td> <td>38.5</td> <td>28.2</td> <td>34.3</td> <td>47.8</td></tr> <tr><td><a href="https://huggingface.co/iSEE-Laboratory/llmdet_large" rel="nofollow">llmdet_large</a></td> <td>(O365V2,OpenImageV6,GoldG) + GroundingCap-1M</td> <td>51.1</td> <td>45.1</td> <td>46.1</td> <td>56.6</td> <td>42.0</td> <td>31.6</td> <td>38.8</td> <td>50.2</td></tr></tbody></table></li>',$e,q,De,U,O,He,le,pt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel">MMGroundingDinoModel</a>. It is used to instantiate a
MM Grounding DINO model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the MM Grounding DINO tiny architecture
<a href="https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det" rel="nofollow">openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det</a>.`,ze,de,ut=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Fe,N,Ze,Y,Ie,T,K,Qe,ce,ht=`The bare Grounding DINO Model (consisting of a backbone and encoder-decoder Transformer) outputting raw
hidden-states without any specific head on top.`,Le,me,ft=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Pe,ge,_t=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Xe,x,ee,Se,pe,Mt='The <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel">MMGroundingDinoModel</a> forward method, overrides the <code>__call__</code> special method.',Ae,E,qe,B,We,te,Re,G,ne,Oe,ue,bt=`Grounding DINO Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top,
for tasks such as COCO detection.`,Ye,he,yt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ke,fe,vt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,et,C,oe,tt,_e,wt='The <a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoForObjectDetection">MMGroundingDinoForObjectDetection</a> forward method, overrides the <code>__call__</code> special method.',nt,V,ot,H,Ne,se,Ee,Me,Be;return z=new ye({props:{title:"MM Grounding DINO",local:"mm-grounding-dino",headingTag:"h1"}}),W=new st({props:{warning:!1,$$slots:{default:[It]},$$scope:{ctx:w}}}),R=new Dt({props:{id:"usage",options:["AutoModel"],$$slots:{default:[Rt]},$$scope:{ctx:w}}}),X=new ye({props:{title:"Notes",local:"notes",headingTag:"h2"}}),q=new ye({props:{title:"MMGroundingDinoConfig",local:"transformers.MMGroundingDinoConfig",headingTag:"h2"}}),O=new be({props:{name:"class transformers.MMGroundingDinoConfig",anchor:"transformers.MMGroundingDinoConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"text_config",val:" = None"},{name:"num_queries",val:" = 900"},{name:"encoder_layers",val:" = 6"},{name:"encoder_ffn_dim",val:" = 2048"},{name:"encoder_attention_heads",val:" = 8"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 8"},{name:"is_encoder_decoder",val:" = True"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"auxiliary_loss",val:" = False"},{name:"position_embedding_type",val:" = 'sine'"},{name:"num_feature_levels",val:" = 4"},{name:"encoder_n_points",val:" = 4"},{name:"decoder_n_points",val:" = 4"},{name:"two_stage",val:" = True"},{name:"class_cost",val:" = 1.0"},{name:"bbox_cost",val:" = 5.0"},{name:"giou_cost",val:" = 2.0"},{name:"bbox_loss_coefficient",val:" = 5.0"},{name:"giou_loss_coefficient",val:" = 2.0"},{name:"focal_alpha",val:" = 0.25"},{name:"disable_custom_kernels",val:" = False"},{name:"max_text_len",val:" = 256"},{name:"text_enhancer_dropout",val:" = 0.0"},{name:"fusion_droppath",val:" = 0.1"},{name:"fusion_dropout",val:" = 0.0"},{name:"embedding_init_target",val:" = True"},{name:"query_dim",val:" = 4"},{name:"positional_embedding_temperature",val:" = 20"},{name:"init_std",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MMGroundingDinoConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>, defaults to <code>ResNetConfig()</code>) &#x2014;
The configuration of the backbone model.`,name:"backbone_config"},{anchor:"transformers.MMGroundingDinoConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.MMGroundingDinoConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.MMGroundingDinoConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to load <code>backbone</code> from the timm library. If <code>False</code>, the backbone is loaded from the transformers
library.`,name:"use_timm_backbone"},{anchor:"transformers.MMGroundingDinoConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.MMGroundingDinoConfig.text_config",description:`<strong>text_config</strong> (<code>Union[AutoConfig, dict]</code>, <em>optional</em>, defaults to <code>BertConfig</code>) &#x2014;
The config object or dictionary of the text backbone.`,name:"text_config"},{anchor:"transformers.MMGroundingDinoConfig.num_queries",description:`<strong>num_queries</strong> (<code>int</code>, <em>optional</em>, defaults to 900) &#x2014;
Number of object queries, i.e. detection slots. This is the maximal number of objects
<a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoModel">MMGroundingDinoModel</a> can detect in a single image.`,name:"num_queries"},{anchor:"transformers.MMGroundingDinoConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.MMGroundingDinoConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.MMGroundingDinoConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.MMGroundingDinoConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.MMGroundingDinoConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.MMGroundingDinoConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.MMGroundingDinoConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the model is used as an encoder/decoder or not.`,name:"is_encoder_decoder"},{anchor:"transformers.MMGroundingDinoConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.MMGroundingDinoConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the layers.`,name:"d_model"},{anchor:"transformers.MMGroundingDinoConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.MMGroundingDinoConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.MMGroundingDinoConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.MMGroundingDinoConfig.auxiliary_loss",description:`<strong>auxiliary_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether auxiliary decoding losses (loss at each decoder layer) are to be used.`,name:"auxiliary_loss"},{anchor:"transformers.MMGroundingDinoConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;sine&quot;</code>) &#x2014;
Type of position embeddings to be used on top of the image features. One of <code>&quot;sine&quot;</code> or <code>&quot;learned&quot;</code>.`,name:"position_embedding_type"},{anchor:"transformers.MMGroundingDinoConfig.num_feature_levels",description:`<strong>num_feature_levels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of input feature levels.`,name:"num_feature_levels"},{anchor:"transformers.MMGroundingDinoConfig.encoder_n_points",description:`<strong>encoder_n_points</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of sampled keys in each feature level for each attention head in the encoder.`,name:"encoder_n_points"},{anchor:"transformers.MMGroundingDinoConfig.decoder_n_points",description:`<strong>decoder_n_points</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of sampled keys in each feature level for each attention head in the decoder.`,name:"decoder_n_points"},{anchor:"transformers.MMGroundingDinoConfig.two_stage",description:`<strong>two_stage</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply a two-stage deformable DETR, where the region proposals are also generated by a variant of
Grounding DINO, which are further fed into the decoder for iterative bounding box refinement.`,name:"two_stage"},{anchor:"transformers.MMGroundingDinoConfig.class_cost",description:`<strong>class_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Relative weight of the classification error in the Hungarian matching cost.`,name:"class_cost"},{anchor:"transformers.MMGroundingDinoConfig.bbox_cost",description:`<strong>bbox_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 5.0) &#x2014;
Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.`,name:"bbox_cost"},{anchor:"transformers.MMGroundingDinoConfig.giou_cost",description:`<strong>giou_cost</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.`,name:"giou_cost"},{anchor:"transformers.MMGroundingDinoConfig.bbox_loss_coefficient",description:`<strong>bbox_loss_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 5.0) &#x2014;
Relative weight of the L1 bounding box loss in the object detection loss.`,name:"bbox_loss_coefficient"},{anchor:"transformers.MMGroundingDinoConfig.giou_loss_coefficient",description:`<strong>giou_loss_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Relative weight of the generalized IoU loss in the object detection loss.`,name:"giou_loss_coefficient"},{anchor:"transformers.MMGroundingDinoConfig.focal_alpha",description:`<strong>focal_alpha</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Alpha parameter in the focal loss.`,name:"focal_alpha"},{anchor:"transformers.MMGroundingDinoConfig.disable_custom_kernels",description:`<strong>disable_custom_kernels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Disable the use of custom CUDA and CPU kernels. This option is necessary for the ONNX export, as custom
kernels are not supported by PyTorch ONNX export.`,name:"disable_custom_kernels"},{anchor:"transformers.MMGroundingDinoConfig.max_text_len",description:`<strong>max_text_len</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The maximum length of the text input.`,name:"max_text_len"},{anchor:"transformers.MMGroundingDinoConfig.text_enhancer_dropout",description:`<strong>text_enhancer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the text enhancer.`,name:"text_enhancer_dropout"},{anchor:"transformers.MMGroundingDinoConfig.fusion_droppath",description:`<strong>fusion_droppath</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The droppath ratio for the fusion module.`,name:"fusion_droppath"},{anchor:"transformers.MMGroundingDinoConfig.fusion_dropout",description:`<strong>fusion_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the fusion module.`,name:"fusion_dropout"},{anchor:"transformers.MMGroundingDinoConfig.embedding_init_target",description:`<strong>embedding_init_target</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to initialize the target with Embedding weights.`,name:"embedding_init_target"},{anchor:"transformers.MMGroundingDinoConfig.query_dim",description:`<strong>query_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The dimension of the query vector.`,name:"query_dim"},{anchor:"transformers.MMGroundingDinoConfig.positional_embedding_temperature",description:`<strong>positional_embedding_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 20) &#x2014;
The temperature for Sine Positional Embedding that is used together with vision backbone.`,name:"positional_embedding_temperature"},{anchor:"transformers.MMGroundingDinoConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.MMGroundingDinoConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py#L31"}}),N=new at({props:{anchor:"transformers.MMGroundingDinoConfig.example",$$slots:{default:[Nt]},$$scope:{ctx:w}}}),Y=new ye({props:{title:"MMGroundingDinoModel",local:"transformers.MMGroundingDinoModel",headingTag:"h2"}}),K=new be({props:{name:"class transformers.MMGroundingDinoModel",anchor:"transformers.MMGroundingDinoModel",parameters:[{name:"config",val:": MMGroundingDinoConfig"}],parametersDescription:[{anchor:"transformers.MMGroundingDinoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig">MMGroundingDinoConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py#L1824"}}),ee=new be({props:{name:"forward",anchor:"transformers.MMGroundingDinoModel.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"input_ids",val:": Tensor"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"pixel_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.MMGroundingDinoModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">GroundingDinoImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor">GroundingDinoProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MMGroundingDinoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">BertTokenizer.<strong>call</strong>()</a> for details.`,name:"input_ids"},{anchor:"transformers.MMGroundingDinoModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>: 0 corresponds to a <code>sentence A</code> token, 1 corresponds to a <code>sentence B</code> token</p>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.MMGroundingDinoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MMGroundingDinoModel.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"pixel_mask"},{anchor:"transformers.MMGroundingDinoModel.forward.encoder_outputs",description:"<strong>encoder_outputs</strong> (`<code>) -- Tuple consists of (</code>last_hidden_state<code>, *optional*: </code>hidden_states<code>, *optional*: </code>attentions<code>) </code>last_hidden_state<code>of shape</code>(batch_size, sequence_length, hidden_size)`, <em>optional</em>) is a sequence of\nhidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.",name:"encoder_outputs"},{anchor:"transformers.MMGroundingDinoModel.forward.output_attentions",description:"<strong>output_attentions</strong> (`<code>) -- Whether or not to return the attentions tensors of all attention layers. See </code>attentions` under returned\ntensors for more detail.",name:"output_attentions"},{anchor:"transformers.MMGroundingDinoModel.forward.output_hidden_states",description:"<strong>output_hidden_states</strong> (`<code>) -- Whether or not to return the hidden states of all layers. See </code>hidden_states` under returned tensors for\nmore detail.",name:"output_hidden_states"},{anchor:"transformers.MMGroundingDinoModel.forward.return_dict",description:`<strong>return_dict</strong> (&#x201C;) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py#L1945"}}),E=new st({props:{$$slots:{default:[Et]},$$scope:{ctx:w}}}),B=new at({props:{anchor:"transformers.MMGroundingDinoModel.forward.example",$$slots:{default:[Bt]},$$scope:{ctx:w}}}),te=new ye({props:{title:"MMGroundingDinoForObjectDetection",local:"transformers.MMGroundingDinoForObjectDetection",headingTag:"h2"}}),ne=new be({props:{name:"class transformers.MMGroundingDinoForObjectDetection",anchor:"transformers.MMGroundingDinoForObjectDetection",parameters:[{name:"config",val:": MMGroundingDinoConfig"}],parametersDescription:[{anchor:"transformers.MMGroundingDinoForObjectDetection.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/mm-grounding-dino#transformers.MMGroundingDinoConfig">MMGroundingDinoConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py#L2389"}}),oe=new be({props:{name:"forward",anchor:"transformers.MMGroundingDinoForObjectDetection.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": LongTensor"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"encoder_outputs",val:": typing.Union[transformers.models.mm_grounding_dino.modeling_mm_grounding_dino.MMGroundingDinoEncoderOutput, tuple, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Optional[list[dict[str, typing.Union[torch.LongTensor, torch.FloatTensor]]]] = None"}],parametersDescription:[{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">GroundingDinoImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoProcessor">GroundingDinoProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/grounding-dino#transformers.GroundingDinoImageProcessor">GroundingDinoImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">BertTokenizer.<strong>call</strong>()</a> for details.`,name:"input_ids"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>: 0 corresponds to a <code>sentence A</code> token, 1 corresponds to a <code>sentence B</code> token</p>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"pixel_mask"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Union[~models.mm_grounding_dino.modeling_mm_grounding_dino.MMGroundingDinoEncoderOutput, tuple, NoneType]</code>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.labels",description:`<strong>labels</strong> (<code>list[Dict]</code> of len <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the
following 2 keys: &#x2018;class_labels&#x2019; and &#x2018;boxes&#x2019; (the class labels and bounding boxes of an image in the batch
respectively). The class labels themselves should be a <code>torch.LongTensor</code> of len <code>(number of bounding boxes in the image,)</code> and the boxes a <code>torch.FloatTensor</code> of shape <code>(number of bounding boxes in the image, 4)</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py#L2423"}}),V=new st({props:{$$slots:{default:[Vt]},$$scope:{ctx:w}}}),H=new at({props:{anchor:"transformers.MMGroundingDinoForObjectDetection.forward.example",$$slots:{default:[Ht]},$$scope:{ctx:w}}}),se=new $t({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mm-grounding-dino.md"}}),{c(){t=m("meta"),c=l(),n=m("p"),i=l(),u=m("p"),u.innerHTML=a,j=l(),I=m("div"),I.innerHTML=rt,ve=l(),f(z.$$.fragment),we=l(),F=m("p"),F.innerHTML=it,je=l(),Q=m("p"),Q.innerHTML=lt,Te=l(),L=m("p"),L.innerHTML=dt,Ge=l(),f(W.$$.fragment),Ue=l(),P=m("p"),P.innerHTML=ct,ke=l(),f(R.$$.fragment),Je=l(),f(X.$$.fragment),xe=l(),S=m("ul"),S.innerHTML=mt,Ce=l(),A=m("ul"),A.innerHTML=gt,$e=l(),f(q.$$.fragment),De=l(),U=m("div"),f(O.$$.fragment),He=l(),le=m("p"),le.innerHTML=pt,ze=l(),de=m("p"),de.innerHTML=ut,Fe=l(),f(N.$$.fragment),Ze=l(),f(Y.$$.fragment),Ie=l(),T=m("div"),f(K.$$.fragment),Qe=l(),ce=m("p"),ce.textContent=ht,Le=l(),me=m("p"),me.innerHTML=ft,Pe=l(),ge=m("p"),ge.innerHTML=_t,Xe=l(),x=m("div"),f(ee.$$.fragment),Se=l(),pe=m("p"),pe.innerHTML=Mt,Ae=l(),f(E.$$.fragment),qe=l(),f(B.$$.fragment),We=l(),f(te.$$.fragment),Re=l(),G=m("div"),f(ne.$$.fragment),Oe=l(),ue=m("p"),ue.textContent=bt,Ye=l(),he=m("p"),he.innerHTML=yt,Ke=l(),fe=m("p"),fe.innerHTML=vt,et=l(),C=m("div"),f(oe.$$.fragment),tt=l(),_e=m("p"),_e.innerHTML=wt,nt=l(),f(V.$$.fragment),ot=l(),f(H.$$.fragment),Ne=l(),f(se.$$.fragment),Ee=l(),Me=m("p"),this.h()},l(e){const o=xt("svelte-u9bgzb",document.head);t=g(o,"META",{name:!0,content:!0}),o.forEach(s),c=d(e),n=g(e,"P",{}),ae(n).forEach(s),i=d(e),u=g(e,"P",{"data-svelte-h":!0}),h(u)!=="svelte-1p6c7d7"&&(u.innerHTML=a),j=d(e),I=g(e,"DIV",{style:!0,"data-svelte-h":!0}),h(I)!=="svelte-383xsf"&&(I.innerHTML=rt),ve=d(e),_(z.$$.fragment,e),we=d(e),F=g(e,"P",{"data-svelte-h":!0}),h(F)!=="svelte-lx4tio"&&(F.innerHTML=it),je=d(e),Q=g(e,"P",{"data-svelte-h":!0}),h(Q)!=="svelte-1svu5d9"&&(Q.innerHTML=lt),Te=d(e),L=g(e,"P",{"data-svelte-h":!0}),h(L)!=="svelte-1aibbnp"&&(L.innerHTML=dt),Ge=d(e),_(W.$$.fragment,e),Ue=d(e),P=g(e,"P",{"data-svelte-h":!0}),h(P)!=="svelte-x2c3k7"&&(P.innerHTML=ct),ke=d(e),_(R.$$.fragment,e),Je=d(e),_(X.$$.fragment,e),xe=d(e),S=g(e,"UL",{"data-svelte-h":!0}),h(S)!=="svelte-k7satk"&&(S.innerHTML=mt),Ce=d(e),A=g(e,"UL",{"data-svelte-h":!0}),h(A)!=="svelte-1hlnsl1"&&(A.innerHTML=gt),$e=d(e),_(q.$$.fragment,e),De=d(e),U=g(e,"DIV",{class:!0});var $=ae(U);_(O.$$.fragment,$),He=d($),le=g($,"P",{"data-svelte-h":!0}),h(le)!=="svelte-jood5i"&&(le.innerHTML=pt),ze=d($),de=g($,"P",{"data-svelte-h":!0}),h(de)!=="svelte-1ek1ss9"&&(de.innerHTML=ut),Fe=d($),_(N.$$.fragment,$),$.forEach(s),Ze=d(e),_(Y.$$.fragment,e),Ie=d(e),T=g(e,"DIV",{class:!0});var k=ae(T);_(K.$$.fragment,k),Qe=d(k),ce=g(k,"P",{"data-svelte-h":!0}),h(ce)!=="svelte-7hyzcp"&&(ce.textContent=ht),Le=d(k),me=g(k,"P",{"data-svelte-h":!0}),h(me)!=="svelte-q52n56"&&(me.innerHTML=ft),Pe=d(k),ge=g(k,"P",{"data-svelte-h":!0}),h(ge)!=="svelte-hswkmf"&&(ge.innerHTML=_t),Xe=d(k),x=g(k,"DIV",{class:!0});var D=ae(x);_(ee.$$.fragment,D),Se=d(D),pe=g(D,"P",{"data-svelte-h":!0}),h(pe)!=="svelte-69y78s"&&(pe.innerHTML=Mt),Ae=d(D),_(E.$$.fragment,D),qe=d(D),_(B.$$.fragment,D),D.forEach(s),k.forEach(s),We=d(e),_(te.$$.fragment,e),Re=d(e),G=g(e,"DIV",{class:!0});var J=ae(G);_(ne.$$.fragment,J),Oe=d(J),ue=g(J,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-hgz8xq"&&(ue.textContent=bt),Ye=d(J),he=g(J,"P",{"data-svelte-h":!0}),h(he)!=="svelte-q52n56"&&(he.innerHTML=yt),Ke=d(J),fe=g(J,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-hswkmf"&&(fe.innerHTML=vt),et=d(J),C=g(J,"DIV",{class:!0});var Z=ae(C);_(oe.$$.fragment,Z),tt=d(Z),_e=g(Z,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-1bqwkl8"&&(_e.innerHTML=wt),nt=d(Z),_(V.$$.fragment,Z),ot=d(Z),_(H.$$.fragment,Z),Z.forEach(s),J.forEach(s),Ne=d(e),_(se.$$.fragment,e),Ee=d(e),Me=g(e,"P",{}),ae(Me).forEach(s),this.h()},h(){re(t,"name","hf:doc:metadata"),re(t,"content",Ft),Ct(I,"float","right"),re(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),re(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),re(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),re(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),re(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){p(document.head,t),r(e,c,o),r(e,n,o),r(e,i,o),r(e,u,o),r(e,j,o),r(e,I,o),r(e,ve,o),M(z,e,o),r(e,we,o),r(e,F,o),r(e,je,o),r(e,Q,o),r(e,Te,o),r(e,L,o),r(e,Ge,o),M(W,e,o),r(e,Ue,o),r(e,P,o),r(e,ke,o),M(R,e,o),r(e,Je,o),M(X,e,o),r(e,xe,o),r(e,S,o),r(e,Ce,o),r(e,A,o),r(e,$e,o),M(q,e,o),r(e,De,o),r(e,U,o),M(O,U,null),p(U,He),p(U,le),p(U,ze),p(U,de),p(U,Fe),M(N,U,null),r(e,Ze,o),M(Y,e,o),r(e,Ie,o),r(e,T,o),M(K,T,null),p(T,Qe),p(T,ce),p(T,Le),p(T,me),p(T,Pe),p(T,ge),p(T,Xe),p(T,x),M(ee,x,null),p(x,Se),p(x,pe),p(x,Ae),M(E,x,null),p(x,qe),M(B,x,null),r(e,We,o),M(te,e,o),r(e,Re,o),r(e,G,o),M(ne,G,null),p(G,Oe),p(G,ue),p(G,Ye),p(G,he),p(G,Ke),p(G,fe),p(G,et),p(G,C),M(oe,C,null),p(C,tt),p(C,_e),p(C,nt),M(V,C,null),p(C,ot),M(H,C,null),r(e,Ne,o),M(se,e,o),r(e,Ee,o),r(e,Me,o),Be=!0},p(e,[o]){const $={};o&2&&($.$$scope={dirty:o,ctx:e}),W.$set($);const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),R.$set(k);const D={};o&2&&(D.$$scope={dirty:o,ctx:e}),N.$set(D);const J={};o&2&&(J.$$scope={dirty:o,ctx:e}),E.$set(J);const Z={};o&2&&(Z.$$scope={dirty:o,ctx:e}),B.$set(Z);const jt={};o&2&&(jt.$$scope={dirty:o,ctx:e}),V.$set(jt);const Tt={};o&2&&(Tt.$$scope={dirty:o,ctx:e}),H.$set(Tt)},i(e){Be||(b(z.$$.fragment,e),b(W.$$.fragment,e),b(R.$$.fragment,e),b(X.$$.fragment,e),b(q.$$.fragment,e),b(O.$$.fragment,e),b(N.$$.fragment,e),b(Y.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(E.$$.fragment,e),b(B.$$.fragment,e),b(te.$$.fragment,e),b(ne.$$.fragment,e),b(oe.$$.fragment,e),b(V.$$.fragment,e),b(H.$$.fragment,e),b(se.$$.fragment,e),Be=!0)},o(e){y(z.$$.fragment,e),y(W.$$.fragment,e),y(R.$$.fragment,e),y(X.$$.fragment,e),y(q.$$.fragment,e),y(O.$$.fragment,e),y(N.$$.fragment,e),y(Y.$$.fragment,e),y(K.$$.fragment,e),y(ee.$$.fragment,e),y(E.$$.fragment,e),y(B.$$.fragment,e),y(te.$$.fragment,e),y(ne.$$.fragment,e),y(oe.$$.fragment,e),y(V.$$.fragment,e),y(H.$$.fragment,e),y(se.$$.fragment,e),Be=!1},d(e){e&&(s(c),s(n),s(i),s(u),s(j),s(I),s(ve),s(we),s(F),s(je),s(Q),s(Te),s(L),s(Ge),s(Ue),s(P),s(ke),s(Je),s(xe),s(S),s(Ce),s(A),s($e),s(De),s(U),s(Ze),s(Ie),s(T),s(We),s(Re),s(G),s(Ne),s(Ee),s(Me)),s(t),v(z,e),v(W,e),v(R,e),v(X,e),v(q,e),v(O),v(N),v(Y,e),v(K),v(ee),v(E),v(B),v(te,e),v(ne),v(oe),v(V),v(H),v(se,e)}}}const Ft='{"title":"MM Grounding DINO","local":"mm-grounding-dino","sections":[{"title":"Notes","local":"notes","sections":[],"depth":2},{"title":"MMGroundingDinoConfig","local":"transformers.MMGroundingDinoConfig","sections":[],"depth":2},{"title":"MMGroundingDinoModel","local":"transformers.MMGroundingDinoModel","sections":[],"depth":2},{"title":"MMGroundingDinoForObjectDetection","local":"transformers.MMGroundingDinoForObjectDetection","sections":[],"depth":2}],"depth":1}';function Qt(w){return Ut(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kt extends kt{constructor(t){super(),Jt(this,t,Qt,zt,Gt,{})}}export{Kt as component};
