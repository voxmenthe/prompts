import{s as ya,o as Ma,n as ua}from"../chunks/scheduler.18a86fab.js";import{S as Ja,i as fa,g as o,s as l,r as p,m as Ta,A as ba,h as i,f as a,c as n,j as ma,u as m,x as r,n as wa,k as ha,y as Ua,a as s,v as h,d,t as u,w as y}from"../chunks/index.98837b22.js";import{T as da}from"../chunks/Tip.77304350.js";import{C as f}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as T,E as ja}from"../chunks/getInferenceSnippets.06c2775f.js";function Ca(je){let c,J="You may notice a small degradation in generation throughput compared to a full on-device cache, depending on your model and generation choices (context size, number of generated tokens, number of beams, etc.). This is because moving the key/value states back and forth requires some work.";return{c(){c=o("p"),c.textContent=J},l(M){c=i(M,"P",{"data-svelte-h":!0}),r(c)!=="svelte-1hzz9zp"&&(c.textContent=J)},m(M,b){s(M,c,b)},p:ua,d(M){M&&a(c)}}}function ga(je){let c,J="Quantizing the cache can harm latency if the context length is short and there is enough GPU memory available for generation without enabling cache quantization. Try to find a balance between memory efficiency and latency.";return{c(){c=o("p"),c.textContent=J},l(M){c=i(M,"P",{"data-svelte-h":!0}),r(c)!=="svelte-1n6vxi"&&(c.textContent=J)},m(M,b){s(M,c,b)},p:ua,d(M){M&&a(c)}}}function ka(je){let c,J,M,b,j,ge,C,Zt="The key-value (KV) vectors are used to calculate attention scores. For autoregressive models, KV scores are calculated <em>every</em> time because the model predicts one token at a time. Each prediction depends on the previous tokens, which means the model performs the same computations each time.",ke,g,vt='A KV <em>cache</em> stores these calculations so they can be reused without recomputing them. Efficient caching is crucial for optimizing model performance because it reduces computation time and improves response rates. Refer to the <a href="./cache_explanation">Caching</a> doc for a more detailed explanation about how a cache works.',Ie,k,Wt='Transformers offers several <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> classes that implement different caching mechanisms. Some of these <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> classes are optimized to save memory while others are designed to maximize generation speed. Refer to the table below to compare cache types and use it to help you select the best cache for your use case.',Ze,I,Gt="<thead><tr><th>Cache Type</th> <th>Supports sliding layers</th> <th>Supports offloading</th> <th>Supports torch.compile()</th> <th>Expected memory usage</th></tr></thead> <tbody><tr><td>Dynamic Cache</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>Medium</td></tr> <tr><td>Static Cache</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>High</td></tr> <tr><td>Quantized Cache</td> <td>No</td> <td>No      </td> <td>No</td> <td>Low</td></tr></tbody>",ve,Z,_t='This guide introduces you to the different <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> classes and shows you how to use them for generation.',We,v,Ge,W,Xt='The <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> is the default cache class for all models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.',_e,G,Vt="Note that for models using sliding window attention (Mistral, Gemma2,…) or chunked attention (Llama4), the cache will stop growing when the layers using these types of attention have reached their maximum size (the sliding window or chunk size).",Xe,_,Bt='Disable the cache by configuring <code>use_cache=False</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>.',Ve,X,Be,V,Rt='Cache classes can also be initialized first before calling and passing it to the models <a href="https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values" rel="nofollow">past_key_values</a> parameter. This can be useful for more fine-grained control, or more advanced usage such as context caching.',Re,B,zt='In most cases, it’s easier to define the cache strategy in the <a href="https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation" rel="nofollow">cache_implementation</a> parameter.',ze,R,Ne,z,Ye,N,Nt='The default <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> prevents you from taking advantage of most just-in-time (JIT) optimizations because the cache size isn’t fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like <a href="./llm_optims#static-kv-cache-and-torchcompile">torch.compile</a> to accelerate generation.',Fe,Y,Yt='A fixed-size cache (<a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a>) pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it. However, having a fixed (usually large) size for the key/value states means that while generating, a lot of tokens will actually be masked as they should not take part in the attention. So this trick allows to easily <code>compile</code> the decoding stage, but it incurs a waste of tokens in the attention computation. As all things, it’s then a trade-off which should be very good if you generate with several sequence of more or less the same lengths, but may be sub-optimal if you have for example 1 very large sequence, and then only short sequences (as the fix cache size would be large, a lot would be wasted for the short sequences). Make sure you understand the impact if you use it!',He,F,Ft='As for <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a>, note that for models using sliding window attention (Mistral, Gemma2,…) or chunked attention (Llama4), the cache will never be larger than the sliding window/chunk size on layers using these types of attention, even if the maximum length specified is larger.',xe,H,Ht='You can enable <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a> by configuring <code>cache_implementation=&quot;static&quot;</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>. This will also turn on automatic <code>compilation</code> of the decoding stage for greedy and sample decoding strategies.',Qe,x,qe,Q,Ee,q,xt='The KV cache can occupy a significant portion of memory and become a <a href="https://hf.co/blog/llama31#inference-memory-requirements" rel="nofollow">bottleneck</a> for long-context generation. Memory efficient caches focus on trading off speed for reduced memory usage. This is especially important for large language models (LLMs) and if your hardware is memory constrained.',$e,E,Qt="Offloading the cache saves GPU memory by moving the KV cache for model layers except one to the CPU. Only the current layer cache is maintained on the GPU during a models <code>forward</code> iteration over the layers. It will asynchronously prefetch the next layer’s cache, and send back the current layer’s cache back to the CPU after attention computation.",Le,$,qt="You may want to consider offloading if you have a small GPU and you’re getting out-of-memory (OOM) errors.",Se,w,Ae,L,Et=`Offloading is available for both <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> and <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a>. You can enable it by configuring <code>cache_implementation=&quot;offloaded&quot;</code> for the dynamic version, or <code>cache_implementation=&quot;offloaded_static&quot;</code> for the static version, in either <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> or <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>.
Additionally, you can also instantiate your own <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> or <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a> with the <code>offloading=True</code> option, and pass this cache in <code>generate</code> or your model’s <code>forward</code> (for example, <code>past_key_values=DynamicCache(config=model.config, offloading=True)</code> for a dynamic cache).`,De,S,$t=`Note that the 2 <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> classes mentionned above have an additional option when instantiating them directly, <code>offload_only_non_sliding</code>.
This additional argument decides if the layers using sliding window/chunk attention (if any), will be offloaded as well. Since
these layers are usually short anyway, it may be better to avoid offloading them, as offloading may incur a speed penalty. By default, this option is <code>False</code> for <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a>, and <code>True</code> for <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a>.`,Ke,A,Pe,D,Lt="The example below shows how you can fallback to an offloaded cache if you run out of memory:",Oe,K,et,P,tt,O,St='The <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.QuantizedCache">QuantizedCache</a> reduces memory requirements by quantizing the KV values to a lower precision. <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.QuantizedCache">QuantizedCache</a> currently supports two quantization backends:',at,ee,At="<li><code>hqq</code> supports int2, int4, and int8 datatypes.</li> <li><code>quanto</code> supports int2 and int4 datatypes. This is the default quantization backend.</li>",st,U,lt,te,Dt='Enable <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.QuantizedCache">QuantizedCache</a> by configuring <code>cache_implementation=&quot;quantized&quot;</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>, and the quantization backend, as well as any additional quantization related parameters should also be passed either as a dict. You should use the default values for these additional parameters unless you’re running out-of-memory. In that case, consider decreasing the residual length.',nt,ae,Kt="For the <code>hqq</code> backend, we recommend setting the <code>axis-key</code> and <code>axis-value</code> parameters to <code>1</code>.",ot,se,it,le,Pt="For <code>quanto</code> backend, we recommend setting the <code>axis-key</code> and <code>axis-value</code> parameters to <code>0</code>.",rt,ne,ct,oe,pt,ie,Ot='<a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.EncoderDecoderCache">EncoderDecoderCache</a> is designed for encoder-decoder models. It manages both the self-attention and cross-attention caches to ensure storage and retrieval of previous kv pairs. It is possible to individually set a different cache type for the encoder and decoder.',mt,re,ea='This cache type doesn’t require any setup. It is a simple wrapper around 2 <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a>s as described above, that will be used independently directly by the model.',ht,ce,dt,pe,ta="Some models have a unique way of storing past kv pairs or states that is not compatible with any other cache classes.",ut,me,aa='Mamba models, such as <a href="./model_doc/mamba">Mamba</a>, require a specific cache because the model doesn’t have an attention mechanism or kv states. Thus, they are not compatible with the above <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> classes.',yt,he,Mt,de,sa="A cache can also work in iterative generation settings where there is back-and-forth interaction with a model (chatbots). Like regular generation, iterative generation with a cache allows a model to efficiently handle ongoing conversations without recomputing the entire context at each step.",Jt,ue,la='For iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a <a href="./chat_templating">chat template</a>.',ft,ye,na='The following example demonstrates <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" rel="nofollow">Llama-2-7b-chat-hf</a>. If you’re using a different chat-style model, <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template">apply_chat_template()</a> may process messages differently. It might cut out important tokens depending on how the Jinja template is written.',Tt,Me,oa="For example, some models use special <code>&lt;think&gt; ... &lt;/think&gt;</code> tokens during reasoning. These could get lost during re-encoding, causing indexing issues. You might need to manually remove or adjust extra tokens from the completions to keep things stable.",bt,Je,wt,fe,Ut,Te,ia='In some situations, you may want to fill a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> with kv pairs for a certain prefix prompt and reuse it to generate different sequences.',jt,be,ra='The example below initializes a <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.StaticCache">StaticCache</a>, and then caches an initial prompt. Now you can generate several sequences from the prefilled prompt.',Ct,we,gt,Ue,kt,Ce,It;return j=new T({props:{title:"KV cache strategies",local:"kv-cache-strategies",headingTag:"h1"}}),v=new T({props:{title:"Default cache",local:"default-cache",headingTag:"h2"}}),X=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkklMjBsaWtlJTIwcm9jayUyMG11c2ljJTIwYmVjYXVzZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGRvX3NhbXBsZSUzREZhbHNlJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCUyQyUyMHVzZV9jYWNoZSUzREZhbHNlKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;I like rock music because&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>, use_cache=<span class="hljs-literal">False</span>)`,wrap:!1}}),R=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwRHluYW1pY0NhY2hlJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkklMjBsaWtlJTIwcm9jayUyMG11c2ljJTIwYmVjYXVzZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFwYXN0X2tleV92YWx1ZXMlMjAlM0QlMjBEeW5hbWljQ2FjaGUoY29uZmlnJTNEbW9kZWwuY29uZmlnKSUwQW91dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwZG9fc2FtcGxlJTNERmFsc2UlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDIwJTJDJTIwcGFzdF9rZXlfdmFsdWVzJTNEcGFzdF9rZXlfdmFsdWVzKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DynamicCache

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;I like rock music because&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

past_key_values = DynamicCache(config=model.config)
out = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>, past_key_values=past_key_values)`,wrap:!1}}),z=new T({props:{title:"Fixed-size cache",local:"fixed-size-cache",headingTag:"h2"}}),x=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBuYW1lJTIwaXMlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBb3V0JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBkb19zYW1wbGUlM0RGYWxzZSUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMjAlMkMlMjBjYWNoZV9pbXBsZW1lbnRhdGlvbiUzRCUyMnN0YXRpYyUyMiklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEElMjJIZWxsbyUyQyUyMG15JTIwbmFtZSUyMGlzJTIwJTVCWW91ciUyME5hbWUlNUQlMkMlMjBhbmQlMjBJJTIwYW0lMjBhJTIwJTVCWW91ciUyMFByb2Zlc3Npb24lNUQlMjB3aXRoJTIwJTVCTnVtYmVyJTIwb2YlMjBZZWFycyU1RCUyMG9mJTIy",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hello, my name is&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

out = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>, cache_implementation=<span class="hljs-string">&quot;static&quot;</span>)
tokenizer.batch_decode(out, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of&quot;</span>`,wrap:!1}}),Q=new T({props:{title:"Cache offloading",local:"cache-offloading",headingTag:"h2"}}),w=new da({props:{warning:!0,$$slots:{default:[Ca]},$$scope:{ctx:je}}}),A=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBY2twdCUyMCUzRCUyMCUyMm1pY3Jvc29mdCUyRlBoaS0zLW1pbmktNGstaW5zdHJ1Y3QlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChja3B0KSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKGNrcHQlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyRnVuJTIwZmFjdCUzQSUyMFRoZSUyMHNob3J0ZXN0JTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQW91dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwZG9fc2FtcGxlJTNERmFsc2UlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDIzJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJvZmZsb2FkZWQlMjIpJTBBcHJpbnQodG9rZW5pemVyLmJhdGNoX2RlY29kZShvdXQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEKSUwQUZ1biUyMGZhY3QlM0ElMjBUaGUlMjBzaG9ydGVzdCUyMHdhciUyMGluJTIwaGlzdG9yeSUyMHdhcyUyMGJldHdlZW4lMjBCcml0YWluJTIwYW5kJTIwWmFuemliYXIlMjBvbiUyMEF1Z3VzdCUyMDI3JTJDJTIwMTg5Ni4=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

ckpt = <span class="hljs-string">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForCausalLM.from_pretrained(ckpt, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Fun fact: The shortest&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

out = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">23</span>, cache_implementation=<span class="hljs-string">&quot;offloaded&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(out, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
Fun fact: The shortest war <span class="hljs-keyword">in</span> history was between Britain <span class="hljs-keyword">and</span> Zanzibar on August <span class="hljs-number">27</span>, <span class="hljs-number">1896.</span>`,wrap:!1}}),K=new f({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, infer_device

<span class="hljs-keyword">def</span> <span class="hljs-title function_">resilient_generate</span>(<span class="hljs-params">model, *args, **kwargs</span>):
    oom = <span class="hljs-literal">False</span>
    device = infer_device()
    torch_device_module = <span class="hljs-built_in">getattr</span>(torch, device, torch.cuda)
    <span class="hljs-keyword">try</span>:
        <span class="hljs-keyword">return</span> model.generate(*args, **kwargs)
    <span class="hljs-keyword">except</span> torch.OutOfMemoryError <span class="hljs-keyword">as</span> e:
        <span class="hljs-built_in">print</span>(e)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;retrying with cache_implementation=&#x27;offloaded&#x27;&quot;</span>)
        oom = <span class="hljs-literal">True</span>
    <span class="hljs-keyword">if</span> oom:
        torch_device_module.empty_cache()
        kwargs[<span class="hljs-string">&quot;cache_implementation&quot;</span>] = <span class="hljs-string">&quot;offloaded&quot;</span>
        <span class="hljs-keyword">return</span> model.generate(*args, **kwargs)

ckpt = <span class="hljs-string">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForCausalLM.from_pretrained(ckpt, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
prompt = [<span class="hljs-string">&quot;okay &quot;</span>*<span class="hljs-number">1000</span> + <span class="hljs-string">&quot;Fun fact: The most&quot;</span>]
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)
beams = { <span class="hljs-string">&quot;num_beams&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;num_beam_groups&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;num_return_sequences&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;diversity_penalty&quot;</span>: <span class="hljs-number">1.0</span>, <span class="hljs-string">&quot;max_new_tokens&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;early_stopping&quot;</span>: <span class="hljs-literal">True</span>, }
out = resilient_generate(model, **inputs, **beams)
responses = tokenizer.batch_decode(out[:,-<span class="hljs-number">28</span>:], skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),P=new T({props:{title:"Quantized cache",local:"quantized-cache",headingTag:"h2"}}),U=new da({props:{warning:!0,$$slots:{default:[ga]},$$scope:{ctx:je}}}),se=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwUXVhbnRpemVkQ2FjaGUlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWNoYXQtaGYlMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySSUyMGxpa2UlMjByb2NrJTIwbXVzaWMlMjBiZWNhdXNlJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8obW9kZWwuZGV2aWNlKSUwQSUwQW91dCUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwZG9fc2FtcGxlJTNERmFsc2UlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDIwJTJDJTIwY2FjaGVfaW1wbGVtZW50YXRpb24lM0QlMjJxdWFudGl6ZWQlMjIlMkMlMjBjYWNoZV9jb25maWclM0QlN0IlMjJiYWNrZW5kJTIyJTNBJTIwJTIyaHFxJTIyJTdEKSUwQXByaW50KHRva2VuaXplci5iYXRjaF9kZWNvZGUob3V0JTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCklMEFJJTIwbGlrZSUyMHJvY2slMjBtdXNpYyUyMGJlY2F1c2UlMjBpdCdzJTIwbG91ZCUyMGFuZCUyMGVuZXJnZXRpYy4lMjBJdCdzJTIwYSUyMGdyZWF0JTIwd2F5JTIwdG8lMjBleHByZXNzJTIwbXlzZWxmJTIwYW5kJTIwcmVs",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, QuantizedCache

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;I like rock music because&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

out = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>, cache_implementation=<span class="hljs-string">&quot;quantized&quot;</span>, cache_config={<span class="hljs-string">&quot;backend&quot;</span>: <span class="hljs-string">&quot;hqq&quot;</span>})
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(out, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
I like rock music because it<span class="hljs-string">&#x27;s loud and energetic. It&#x27;</span>s a great way to express myself <span class="hljs-keyword">and</span> rel`,wrap:!1}}),ne=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkklMjBsaWtlJTIwcm9jayUyMG11c2ljJTIwYmVjYXVzZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEFvdXQlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGRvX3NhbXBsZSUzREZhbHNlJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCUyQyUyMGNhY2hlX2ltcGxlbWVudGF0aW9uJTNEJTIycXVhbnRpemVkJTIyJTJDJTIwY2FjaGVfY29uZmlnJTNEJTdCJTIybmJpdHMlMjIlM0ElMjA0JTJDJTIwJTIyYmFja2VuZCUyMiUzQSUyMCUyMnF1YW50byUyMiU3RCklMEFwcmludCh0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKG91dCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQpJTBBSSUyMGxpa2UlMjByb2NrJTIwbXVzaWMlMjBiZWNhdXNlJTIwaXQncyUyMGxvdWQlMjBhbmQlMjBlbmVyZ2V0aWMuJTIwSXQncyUyMGElMjBncmVhdCUyMHdheSUyMHRvJTIwZXhwcmVzcyUyMG15c2VsZiUyMGFuZCUyMHJlbA==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;I like rock music because&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

out = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">20</span>, cache_implementation=<span class="hljs-string">&quot;quantized&quot;</span>, cache_config={<span class="hljs-string">&quot;nbits&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;backend&quot;</span>: <span class="hljs-string">&quot;quanto&quot;</span>})
<span class="hljs-built_in">print</span>(tokenizer.batch_decode(out, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
I like rock music because it<span class="hljs-string">&#x27;s loud and energetic. It&#x27;</span>s a great way to express myself <span class="hljs-keyword">and</span> rel`,wrap:!1}}),oe=new T({props:{title:"Encoder-decoder cache",local:"encoder-decoder-cache",headingTag:"h2"}}),ce=new T({props:{title:"Model-specific caches",local:"model-specific-caches",headingTag:"h2"}}),he=new T({props:{title:"Iterative generation",local:"iterative-generation",headingTag:"h1"}}),Je=new f({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQ0F1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwRHluYW1pY0NhY2hlJTJDJTIwU3RhdGljQ2FjaGUlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMm1ldGEtbGxhbWElMkZMbGFtYS0yLTdiLWNoYXQtaGYlMjIlMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMGR0eXBlJTNEdG9yY2guYmZsb2F0MTYlMkMlMjBkZXZpY2VfbWFwJTNEJ2F1dG8nKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQSUwQXVzZXJfcHJvbXB0cyUyMCUzRCUyMCU1QiUyMkhlbGxvJTJDJTIwd2hhdCdzJTIweW91ciUyMG5hbWUlM0YlMjIlMkMlMjAlMjJCdHclMkMlMjB5ZXN0ZXJkYXklMjBJJTIwd2FzJTIwb24lMjBhJTIwcm9jayUyMGNvbmNlcnQuJTIyJTVEJTBBJTBBcGFzdF9rZXlfdmFsdWVzJTIwJTNEJTIwRHluYW1pY0NhY2hlKGNvbmZpZyUzRG1vZGVsLmNvbmZpZyklMEElMEFtZXNzYWdlcyUyMCUzRCUyMCU1QiU1RCUwQWZvciUyMHByb21wdCUyMGluJTIwdXNlcl9wcm9tcHRzJTNBJTBBJTIwJTIwJTIwJTIwbWVzc2FnZXMuYXBwZW5kKCU3QiUyMnJvbGUlMjIlM0ElMjAlMjJ1c2VyJTIyJTJDJTIwJTIyY29udGVudCUyMiUzQSUyMHByb21wdCU3RCklMEElMjAlMjAlMjAlMjBpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYXBwbHlfY2hhdF90ZW1wbGF0ZShtZXNzYWdlcyUyQyUyMGFkZF9nZW5lcmF0aW9uX3Byb21wdCUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcmV0dXJuX2RpY3QlM0RUcnVlKS50byhtb2RlbC5kZXZpY2UpJTBBJTIwJTIwJTIwJTIwaW5wdXRfbGVuZ3RoJTIwJTNEJTIwaW5wdXRzJTVCJTIyaW5wdXRfaWRzJTIyJTVELnNoYXBlJTVCMSU1RCUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGRvX3NhbXBsZSUzREZhbHNlJTJDJTIwbWF4X25ld190b2tlbnMlM0QyNTYlMkMlMjBwYXN0X2tleV92YWx1ZXMlM0RwYXN0X2tleV92YWx1ZXMpJTBBJTIwJTIwJTIwJTIwY29tcGxldGlvbiUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlMkMlMjBpbnB1dF9sZW5ndGglM0ElMjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjBtZXNzYWdlcy5hcHBlbmQoJTdCJTIycm9sZSUyMiUzQSUyMCUyMmFzc2lzdGFudCUyMiUyQyUyMCUyMmNvbnRlbnQlMjIlM0ElMjBjb21wbGV0aW9uJTdEKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer,AutoModelForCausalLM, DynamicCache, StaticCache

model_id = <span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=<span class="hljs-string">&#x27;auto&#x27;</span>)
tokenizer = AutoTokenizer.from_pretrained(model_id)

user_prompts = [<span class="hljs-string">&quot;Hello, what&#x27;s your name?&quot;</span>, <span class="hljs-string">&quot;Btw, yesterday I was on a rock concert.&quot;</span>]

past_key_values = DynamicCache(config=model.config)

messages = []
<span class="hljs-keyword">for</span> prompt <span class="hljs-keyword">in</span> user_prompts:
    messages.append({<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: prompt})
    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, return_dict=<span class="hljs-literal">True</span>).to(model.device)
    input_length = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape[<span class="hljs-number">1</span>]
    outputs = model.generate(**inputs, do_sample=<span class="hljs-literal">False</span>, max_new_tokens=<span class="hljs-number">256</span>, past_key_values=past_key_values)
    completion = tokenizer.decode(outputs[<span class="hljs-number">0</span>, input_length: ], skip_special_tokens=<span class="hljs-literal">True</span>)
    messages.append({<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: completion})`,wrap:!1}}),fe=new T({props:{title:"Prefill a cache (prefix caching)",local:"prefill-a-cache-prefix-caching",headingTag:"h2"}}),we=new f({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> copy
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache

model_id = <span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-number">0</span>})
tokenizer = AutoTokenizer.from_pretrained(model_id)

<span class="hljs-comment"># Init StaticCache with big enough max-length (1024 tokens for the below example)</span>
<span class="hljs-comment"># You can also init a DynamicCache, if that suits you better</span>
prompt_cache = StaticCache(config=model.config, max_cache_len=<span class="hljs-number">1024</span>)

INITIAL_PROMPT = <span class="hljs-string">&quot;You are a helpful assistant. &quot;</span>
inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device.<span class="hljs-built_in">type</span>)
<span class="hljs-comment"># This is the common prompt cached, we need to run forward without grad to be able to copy</span>
<span class="hljs-keyword">with</span> torch.no_grad():
     prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values

prompts = [<span class="hljs-string">&quot;Help me to write a blogpost about travelling.&quot;</span>, <span class="hljs-string">&quot;What is the capital of France?&quot;</span>]
responses = []
<span class="hljs-keyword">for</span> prompt <span class="hljs-keyword">in</span> prompts:
    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device.<span class="hljs-built_in">type</span>)
    past_key_values = copy.deepcopy(prompt_cache)
    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=<span class="hljs-number">20</span>)
    response = tokenizer.batch_decode(outputs)[<span class="hljs-number">0</span>]
    responses.append(response)

<span class="hljs-built_in">print</span>(responses)`,wrap:!1}}),Ue=new ja({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md"}}),{c(){c=o("meta"),J=l(),M=o("p"),b=l(),p(j.$$.fragment),ge=l(),C=o("p"),C.innerHTML=Zt,ke=l(),g=o("p"),g.innerHTML=vt,Ie=l(),k=o("p"),k.innerHTML=Wt,Ze=l(),I=o("table"),I.innerHTML=Gt,ve=l(),Z=o("p"),Z.innerHTML=_t,We=l(),p(v.$$.fragment),Ge=l(),W=o("p"),W.innerHTML=Xt,_e=l(),G=o("p"),G.textContent=Vt,Xe=l(),_=o("p"),_.innerHTML=Bt,Ve=l(),p(X.$$.fragment),Be=l(),V=o("p"),V.innerHTML=Rt,Re=l(),B=o("p"),B.innerHTML=zt,ze=l(),p(R.$$.fragment),Ne=l(),p(z.$$.fragment),Ye=l(),N=o("p"),N.innerHTML=Nt,Fe=l(),Y=o("p"),Y.innerHTML=Yt,He=l(),F=o("p"),F.innerHTML=Ft,xe=l(),H=o("p"),H.innerHTML=Ht,Qe=l(),p(x.$$.fragment),qe=l(),p(Q.$$.fragment),Ee=l(),q=o("p"),q.innerHTML=xt,$e=l(),E=o("p"),E.innerHTML=Qt,Le=l(),$=o("p"),$.textContent=qt,Se=l(),p(w.$$.fragment),Ae=l(),L=o("p"),L.innerHTML=Et,De=l(),S=o("p"),S.innerHTML=$t,Ke=l(),p(A.$$.fragment),Pe=l(),D=o("p"),D.textContent=Lt,Oe=l(),p(K.$$.fragment),et=l(),p(P.$$.fragment),tt=l(),O=o("p"),O.innerHTML=St,at=l(),ee=o("ul"),ee.innerHTML=At,st=l(),p(U.$$.fragment),lt=l(),te=o("p"),te.innerHTML=Dt,nt=Ta(`
<hfoptions id="quantized-cache">
`),ae=o("p"),ae.innerHTML=Kt,ot=l(),p(se.$$.fragment),it=l(),le=o("p"),le.innerHTML=Pt,rt=l(),p(ne.$$.fragment),ct=l(),p(oe.$$.fragment),pt=l(),ie=o("p"),ie.innerHTML=Ot,mt=l(),re=o("p"),re.innerHTML=ea,ht=l(),p(ce.$$.fragment),dt=l(),pe=o("p"),pe.textContent=ta,ut=l(),me=o("p"),me.innerHTML=aa,yt=l(),p(he.$$.fragment),Mt=l(),de=o("p"),de.textContent=sa,Jt=l(),ue=o("p"),ue.innerHTML=la,ft=l(),ye=o("p"),ye.innerHTML=na,Tt=l(),Me=o("p"),Me.innerHTML=oa,bt=l(),p(Je.$$.fragment),wt=l(),p(fe.$$.fragment),Ut=l(),Te=o("p"),Te.innerHTML=ia,jt=l(),be=o("p"),be.innerHTML=ra,Ct=l(),p(we.$$.fragment),gt=l(),p(Ue.$$.fragment),kt=l(),Ce=o("p"),this.h()},l(e){const t=ba("svelte-u9bgzb",document.head);c=i(t,"META",{name:!0,content:!0}),t.forEach(a),J=n(e),M=i(e,"P",{}),ma(M).forEach(a),b=n(e),m(j.$$.fragment,e),ge=n(e),C=i(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-1hxt7ht"&&(C.innerHTML=Zt),ke=n(e),g=i(e,"P",{"data-svelte-h":!0}),r(g)!=="svelte-og33ji"&&(g.innerHTML=vt),Ie=n(e),k=i(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-1eqdzbq"&&(k.innerHTML=Wt),Ze=n(e),I=i(e,"TABLE",{"data-svelte-h":!0}),r(I)!=="svelte-dxv7tp"&&(I.innerHTML=Gt),ve=n(e),Z=i(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-137qz9f"&&(Z.innerHTML=_t),We=n(e),m(v.$$.fragment,e),Ge=n(e),W=i(e,"P",{"data-svelte-h":!0}),r(W)!=="svelte-18r5jma"&&(W.innerHTML=Xt),_e=n(e),G=i(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-1wo1duj"&&(G.textContent=Vt),Xe=n(e),_=i(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-1e6emmm"&&(_.innerHTML=Bt),Ve=n(e),m(X.$$.fragment,e),Be=n(e),V=i(e,"P",{"data-svelte-h":!0}),r(V)!=="svelte-1ryljih"&&(V.innerHTML=Rt),Re=n(e),B=i(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-1potdr8"&&(B.innerHTML=zt),ze=n(e),m(R.$$.fragment,e),Ne=n(e),m(z.$$.fragment,e),Ye=n(e),N=i(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-1hp9ikp"&&(N.innerHTML=Nt),Fe=n(e),Y=i(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-6mfppr"&&(Y.innerHTML=Yt),He=n(e),F=i(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-zbe81j"&&(F.innerHTML=Ft),xe=n(e),H=i(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-n0k9fq"&&(H.innerHTML=Ht),Qe=n(e),m(x.$$.fragment,e),qe=n(e),m(Q.$$.fragment,e),Ee=n(e),q=i(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-15jbxrx"&&(q.innerHTML=xt),$e=n(e),E=i(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-7m6bkf"&&(E.innerHTML=Qt),Le=n(e),$=i(e,"P",{"data-svelte-h":!0}),r($)!=="svelte-m6bxw0"&&($.textContent=qt),Se=n(e),m(w.$$.fragment,e),Ae=n(e),L=i(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-txd8x1"&&(L.innerHTML=Et),De=n(e),S=i(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-21do2a"&&(S.innerHTML=$t),Ke=n(e),m(A.$$.fragment,e),Pe=n(e),D=i(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-pkg9iq"&&(D.textContent=Lt),Oe=n(e),m(K.$$.fragment,e),et=n(e),m(P.$$.fragment,e),tt=n(e),O=i(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-s2p6tv"&&(O.innerHTML=St),at=n(e),ee=i(e,"UL",{"data-svelte-h":!0}),r(ee)!=="svelte-14p9huo"&&(ee.innerHTML=At),st=n(e),m(U.$$.fragment,e),lt=n(e),te=i(e,"P",{"data-svelte-h":!0}),r(te)!=="svelte-10ejgkc"&&(te.innerHTML=Dt),nt=wa(e,`
<hfoptions id="quantized-cache">
`),ae=i(e,"P",{"data-svelte-h":!0}),r(ae)!=="svelte-1ur1l6q"&&(ae.innerHTML=Kt),ot=n(e),m(se.$$.fragment,e),it=n(e),le=i(e,"P",{"data-svelte-h":!0}),r(le)!=="svelte-14czee2"&&(le.innerHTML=Pt),rt=n(e),m(ne.$$.fragment,e),ct=n(e),m(oe.$$.fragment,e),pt=n(e),ie=i(e,"P",{"data-svelte-h":!0}),r(ie)!=="svelte-adnizp"&&(ie.innerHTML=Ot),mt=n(e),re=i(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-12aql8"&&(re.innerHTML=ea),ht=n(e),m(ce.$$.fragment,e),dt=n(e),pe=i(e,"P",{"data-svelte-h":!0}),r(pe)!=="svelte-u1f4zv"&&(pe.textContent=ta),ut=n(e),me=i(e,"P",{"data-svelte-h":!0}),r(me)!=="svelte-1babolt"&&(me.innerHTML=aa),yt=n(e),m(he.$$.fragment,e),Mt=n(e),de=i(e,"P",{"data-svelte-h":!0}),r(de)!=="svelte-67pkdu"&&(de.textContent=sa),Jt=n(e),ue=i(e,"P",{"data-svelte-h":!0}),r(ue)!=="svelte-1ejbghf"&&(ue.innerHTML=la),ft=n(e),ye=i(e,"P",{"data-svelte-h":!0}),r(ye)!=="svelte-1h6uxrq"&&(ye.innerHTML=na),Tt=n(e),Me=i(e,"P",{"data-svelte-h":!0}),r(Me)!=="svelte-fhggpa"&&(Me.innerHTML=oa),bt=n(e),m(Je.$$.fragment,e),wt=n(e),m(fe.$$.fragment,e),Ut=n(e),Te=i(e,"P",{"data-svelte-h":!0}),r(Te)!=="svelte-42m0qq"&&(Te.innerHTML=ia),jt=n(e),be=i(e,"P",{"data-svelte-h":!0}),r(be)!=="svelte-1e5cxuq"&&(be.innerHTML=ra),Ct=n(e),m(we.$$.fragment,e),gt=n(e),m(Ue.$$.fragment,e),kt=n(e),Ce=i(e,"P",{}),ma(Ce).forEach(a),this.h()},h(){ha(c,"name","hf:doc:metadata"),ha(c,"content",Ia)},m(e,t){Ua(document.head,c),s(e,J,t),s(e,M,t),s(e,b,t),h(j,e,t),s(e,ge,t),s(e,C,t),s(e,ke,t),s(e,g,t),s(e,Ie,t),s(e,k,t),s(e,Ze,t),s(e,I,t),s(e,ve,t),s(e,Z,t),s(e,We,t),h(v,e,t),s(e,Ge,t),s(e,W,t),s(e,_e,t),s(e,G,t),s(e,Xe,t),s(e,_,t),s(e,Ve,t),h(X,e,t),s(e,Be,t),s(e,V,t),s(e,Re,t),s(e,B,t),s(e,ze,t),h(R,e,t),s(e,Ne,t),h(z,e,t),s(e,Ye,t),s(e,N,t),s(e,Fe,t),s(e,Y,t),s(e,He,t),s(e,F,t),s(e,xe,t),s(e,H,t),s(e,Qe,t),h(x,e,t),s(e,qe,t),h(Q,e,t),s(e,Ee,t),s(e,q,t),s(e,$e,t),s(e,E,t),s(e,Le,t),s(e,$,t),s(e,Se,t),h(w,e,t),s(e,Ae,t),s(e,L,t),s(e,De,t),s(e,S,t),s(e,Ke,t),h(A,e,t),s(e,Pe,t),s(e,D,t),s(e,Oe,t),h(K,e,t),s(e,et,t),h(P,e,t),s(e,tt,t),s(e,O,t),s(e,at,t),s(e,ee,t),s(e,st,t),h(U,e,t),s(e,lt,t),s(e,te,t),s(e,nt,t),s(e,ae,t),s(e,ot,t),h(se,e,t),s(e,it,t),s(e,le,t),s(e,rt,t),h(ne,e,t),s(e,ct,t),h(oe,e,t),s(e,pt,t),s(e,ie,t),s(e,mt,t),s(e,re,t),s(e,ht,t),h(ce,e,t),s(e,dt,t),s(e,pe,t),s(e,ut,t),s(e,me,t),s(e,yt,t),h(he,e,t),s(e,Mt,t),s(e,de,t),s(e,Jt,t),s(e,ue,t),s(e,ft,t),s(e,ye,t),s(e,Tt,t),s(e,Me,t),s(e,bt,t),h(Je,e,t),s(e,wt,t),h(fe,e,t),s(e,Ut,t),s(e,Te,t),s(e,jt,t),s(e,be,t),s(e,Ct,t),h(we,e,t),s(e,gt,t),h(Ue,e,t),s(e,kt,t),s(e,Ce,t),It=!0},p(e,[t]){const ca={};t&2&&(ca.$$scope={dirty:t,ctx:e}),w.$set(ca);const pa={};t&2&&(pa.$$scope={dirty:t,ctx:e}),U.$set(pa)},i(e){It||(d(j.$$.fragment,e),d(v.$$.fragment,e),d(X.$$.fragment,e),d(R.$$.fragment,e),d(z.$$.fragment,e),d(x.$$.fragment,e),d(Q.$$.fragment,e),d(w.$$.fragment,e),d(A.$$.fragment,e),d(K.$$.fragment,e),d(P.$$.fragment,e),d(U.$$.fragment,e),d(se.$$.fragment,e),d(ne.$$.fragment,e),d(oe.$$.fragment,e),d(ce.$$.fragment,e),d(he.$$.fragment,e),d(Je.$$.fragment,e),d(fe.$$.fragment,e),d(we.$$.fragment,e),d(Ue.$$.fragment,e),It=!0)},o(e){u(j.$$.fragment,e),u(v.$$.fragment,e),u(X.$$.fragment,e),u(R.$$.fragment,e),u(z.$$.fragment,e),u(x.$$.fragment,e),u(Q.$$.fragment,e),u(w.$$.fragment,e),u(A.$$.fragment,e),u(K.$$.fragment,e),u(P.$$.fragment,e),u(U.$$.fragment,e),u(se.$$.fragment,e),u(ne.$$.fragment,e),u(oe.$$.fragment,e),u(ce.$$.fragment,e),u(he.$$.fragment,e),u(Je.$$.fragment,e),u(fe.$$.fragment,e),u(we.$$.fragment,e),u(Ue.$$.fragment,e),It=!1},d(e){e&&(a(J),a(M),a(b),a(ge),a(C),a(ke),a(g),a(Ie),a(k),a(Ze),a(I),a(ve),a(Z),a(We),a(Ge),a(W),a(_e),a(G),a(Xe),a(_),a(Ve),a(Be),a(V),a(Re),a(B),a(ze),a(Ne),a(Ye),a(N),a(Fe),a(Y),a(He),a(F),a(xe),a(H),a(Qe),a(qe),a(Ee),a(q),a($e),a(E),a(Le),a($),a(Se),a(Ae),a(L),a(De),a(S),a(Ke),a(Pe),a(D),a(Oe),a(et),a(tt),a(O),a(at),a(ee),a(st),a(lt),a(te),a(nt),a(ae),a(ot),a(it),a(le),a(rt),a(ct),a(pt),a(ie),a(mt),a(re),a(ht),a(dt),a(pe),a(ut),a(me),a(yt),a(Mt),a(de),a(Jt),a(ue),a(ft),a(ye),a(Tt),a(Me),a(bt),a(wt),a(Ut),a(Te),a(jt),a(be),a(Ct),a(gt),a(kt),a(Ce)),a(c),y(j,e),y(v,e),y(X,e),y(R,e),y(z,e),y(x,e),y(Q,e),y(w,e),y(A,e),y(K,e),y(P,e),y(U,e),y(se,e),y(ne,e),y(oe,e),y(ce,e),y(he,e),y(Je,e),y(fe,e),y(we,e),y(Ue,e)}}}const Ia='{"title":"KV cache strategies","local":"kv-cache-strategies","sections":[{"title":"Default cache","local":"default-cache","sections":[],"depth":2},{"title":"Fixed-size cache","local":"fixed-size-cache","sections":[],"depth":2},{"title":"Cache offloading","local":"cache-offloading","sections":[],"depth":2},{"title":"Quantized cache","local":"quantized-cache","sections":[],"depth":2},{"title":"Encoder-decoder cache","local":"encoder-decoder-cache","sections":[],"depth":2},{"title":"Model-specific caches","local":"model-specific-caches","sections":[],"depth":2}],"depth":1}';function Za(je){return Ma(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Va extends Ja{constructor(c){super(),fa(this,c,Za,ka,ya,{})}}export{Va as component};
