import{s as qn,o as Wn,n as me}from"../chunks/scheduler.18a86fab.js";import{S as Sn,i as jn,g as d,s,r as f,A as Jn,h as c,f as o,c as r,j as J,x as h,u as g,k as q,y as l,a as i,v as _,d as b,t as v,w as y}from"../chunks/index.98837b22.js";import{T as Ce}from"../chunks/Tip.77304350.js";import{D as pe}from"../chunks/Docstring.a1ef7999.js";import{C as In}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Pn}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as Me,E as Hn}from"../chunks/getInferenceSnippets.06c2775f.js";function Un(L){let n,m="This model is in maintenance mode only, we don’t accept any new PRs changing its code.",a,p,w=`If you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0.
You can do so by running the following command: <code>pip install -U transformers==4.31.0</code>.`;return{c(){n=d("p"),n.textContent=m,a=s(),p=d("p"),p.innerHTML=w},l(u){n=c(u,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1dwyvn5"&&(n.textContent=m),a=r(u),p=c(u,"P",{"data-svelte-h":!0}),h(p)!=="svelte-1o63f7c"&&(p.innerHTML=w)},m(u,C){i(u,n,C),i(u,a,C),i(u,p,C)},p:me,d(u){u&&(o(n),o(a),o(p))}}}function Bn(L){let n,m='This model differs from the <a href="https://huggingface.co/models?search=openllama" rel="nofollow">OpenLLaMA models</a> on the Hugging Face Hub, which primarily use the <a href="llama">LLaMA</a> architecture.';return{c(){n=d("p"),n.innerHTML=m},l(a){n=c(a,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1kc01gl"&&(n.innerHTML=m)},m(a,p){i(a,n,p)},p:me,d(a){a&&o(n)}}}function Zn(L){let n,m;return n=new In({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME9wZW5MbGFtYU1vZGVsJTJDJTIwT3BlbkxsYW1hQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME9wZW4tTGxhbWElMjBvcGVuX2xsYW1hLTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyME9wZW5MbGFtYUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMG9wZW5fbGxhbWEtN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME9wZW5MbGFtYU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OpenLlamaModel, OpenLlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Open-Llama open_llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OpenLlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the open_llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenLlamaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){f(n.$$.fragment)},l(a){g(n.$$.fragment,a)},m(a,p){_(n,a,p),m=!0},p:me,i(a){m||(b(n.$$.fragment,a),m=!0)},o(a){v(n.$$.fragment,a),m=!1},d(a){y(n,a)}}}function Nn(L){let n,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=m},l(a){n=c(a,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=m)},m(a,p){i(a,n,p)},p:me,d(a){a&&o(n)}}}function Gn(L){let n,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=m},l(a){n=c(a,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=m)},m(a,p){i(a,n,p)},p:me,d(a){a&&o(n)}}}function En(L){let n,m="Example:",a,p,w;return p=new In({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBPcGVuTGxhbWFGb3JDYXVzYWxMTSUwQSUwQW1vZGVsJTIwJTNEJTIwT3BlbkxsYW1hRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5sbS1yZXNlYXJjaCUyRm9wZW5fbGxhbWFfN2IlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmxtLXJlc2VhcmNoJTJGb3Blbl9sbGFtYV83YiUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJIZXklMkMlMjBhcmUlMjB5b3UlMjBjb25zY2lvdXMlM0YlMjBDYW4lMjB5b3UlMjB0YWxrJTIwdG8lMjBtZSUzRiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRzLmlucHV0X2lkcyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenLlamaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenLlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){n=d("p"),n.textContent=m,a=s(),f(p.$$.fragment)},l(u){n=c(u,"P",{"data-svelte-h":!0}),h(n)!=="svelte-11lpom8"&&(n.textContent=m),a=r(u),g(p.$$.fragment,u)},m(u,C){i(u,n,C),i(u,a,C),_(p,u,C),w=!0},p:me,i(u){w||(b(p.$$.fragment,u),w=!0)},o(u){v(p.$$.fragment,u),w=!1},d(u){u&&(o(n),o(a)),y(p,u)}}}function Vn(L){let n,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=m},l(a){n=c(a,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=m)},m(a,p){i(a,n,p)},p:me,d(a){a&&o(n)}}}function Dn(L){let n,m,a,p,w,u="<em>This model was released on 2023-04-16 and added to Hugging Face Transformers on 2023-06-20.</em>",C,D,ze,H,fn='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',xe,U,Fe,B,Pe,Y,Ie,A,gn="The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL.",qe,R,_n=`The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.
And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.`,We,X,bn=`This model was contributed by <a href="https://huggingface.co/s-JoL" rel="nofollow">s-JoL</a>.
The original code was released on GitHub by <a href="https://github.com/s-JoL" rel="nofollow">s-JoL</a>, but is now removed.`,Se,Q,je,M,K,De,ue,vn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a>. It is used to instantiate an
Open-Llama model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/s-JoL/Open-Llama-V1" rel="nofollow">s-JoL/Open-Llama-V1</a>.`,Ye,he,yn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ae,Z,Je,ee,He,k,ne,Re,fe,Tn=`The bare Open-Llama Model outputting raw hidden-states without any specific head on top.
This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Xe,ge,$n=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Qe,_e,Ln="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>OpenLlamaDecoderLayer</code>",Ke,P,te,en,be,wn='The <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> forward method, overrides the <code>__call__</code> special method.',nn,N,Ue,oe,Be,W,ae,tn,z,se,on,ve,kn='The <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',an,G,sn,E,Ze,re,Ne,T,ie,rn,ye,Mn="The LLaMa Model transformer with a sequence classification head on top (linear layer).",ln,Te,On=`<a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> uses the last token in order to do the classification, as other causal
models (e.g. GPT-2) do.`,dn,$e,Cn=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,cn,Le,zn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,pn,we,xn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,mn,I,le,un,ke,Fn='The <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',hn,V,Ge,de,Ee,Oe,Ve;return D=new Me({props:{title:"Open-Llama",local:"open-llama",headingTag:"h1"}}),U=new Ce({props:{warning:!0,$$slots:{default:[Un]},$$scope:{ctx:L}}}),B=new Ce({props:{warning:!0,$$slots:{default:[Bn]},$$scope:{ctx:L}}}),Y=new Me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Q=new Me({props:{title:"OpenLlamaConfig",local:"transformers.OpenLlamaConfig",headingTag:"h2"}}),K=new pe({props:{name:"class transformers.OpenLlamaConfig",anchor:"transformers.OpenLlamaConfig",parameters:[{name:"vocab_size",val:" = 100000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 11008"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"tie_word_embeddings",val:" = False"},{name:"use_memory_efficient_attention",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_dropout_prob",val:" = 0.1"},{name:"use_stable_embedding",val:" = True"},{name:"shared_input_output_embedding",val:" = True"},{name:"rope_theta",val:" = 10000.0"},{name:"rope_scaling",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenLlamaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the Open-Llama model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a>`,name:"vocab_size"},{anchor:"transformers.OpenLlamaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.OpenLlamaConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 11008) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.OpenLlamaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OpenLlamaConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OpenLlamaConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.OpenLlamaConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OpenLlamaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OpenLlamaConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.OpenLlamaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.OpenLlamaConfig.tie_word_embeddings(bool,",description:`<strong>tie_word_embeddings(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie weight embeddings`,name:"tie_word_embeddings(bool,"},{anchor:"transformers.OpenLlamaConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.OpenLlamaConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling
strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is
<code>{&quot;type&quot;: strategy name, &quot;factor&quot;: scaling factor}</code>. When using this flag, don&#x2019;t update
<code>max_position_embeddings</code> to the expected new maximum. See the following thread for more information on how
these scaling strategies behave:
<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/" rel="nofollow">https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/</a>. This is an
experimental feature, subject to breaking API changes in future versions.`,name:"rope_scaling"},{anchor:"transformers.OpenLlamaConfig.Example",description:"<strong>Example</strong> &#x2014;",name:"Example"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/configuration_open_llama.py#L29"}}),Z=new Pn({props:{anchor:"transformers.OpenLlamaConfig.example",$$slots:{default:[Zn]},$$scope:{ctx:L}}}),ee=new Me({props:{title:"OpenLlamaModel",local:"transformers.OpenLlamaModel",headingTag:"h2"}}),ne=new pe({props:{name:"class transformers.OpenLlamaModel",anchor:"transformers.OpenLlamaModel",parameters:[{name:"config",val:": OpenLlamaConfig"}],parametersDescription:[{anchor:"transformers.OpenLlamaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.OpenLlamaModel.config",description:"<strong>config</strong> &#x2014; OpenLlamaConfig",name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L529"}}),te=new pe({props:{name:"forward",anchor:"transformers.OpenLlamaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[list[torch.FloatTensor]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://huggingface.co/papers/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L554"}}),N=new Ce({props:{$$slots:{default:[Nn]},$$scope:{ctx:L}}}),oe=new Me({props:{title:"OpenLlamaForCausalLM",local:"transformers.OpenLlamaForCausalLM",headingTag:"h2"}}),ae=new pe({props:{name:"class transformers.OpenLlamaForCausalLM",anchor:"transformers.OpenLlamaForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L668"}}),se=new pe({props:{name:"forward",anchor:"transformers.OpenLlamaForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[list[torch.FloatTensor]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://huggingface.co/papers/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.OpenLlamaForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L680",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig"
>OpenLlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Cache</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — It is a <a
  href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache"
>Cache</a> instance. For more details, see our <a
  href="https://huggingface.co/docs/transformers/en/kv_cache"
  rel="nofollow"
>kv cache guide</a>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new Ce({props:{$$slots:{default:[Gn]},$$scope:{ctx:L}}}),E=new Pn({props:{anchor:"transformers.OpenLlamaForCausalLM.forward.example",$$slots:{default:[En]},$$scope:{ctx:L}}}),re=new Me({props:{title:"OpenLlamaForSequenceClassification",local:"transformers.OpenLlamaForSequenceClassification",headingTag:"h2"}}),ie=new pe({props:{name:"class transformers.OpenLlamaForSequenceClassification",anchor:"transformers.OpenLlamaForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenLlamaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L838"}}),le=new pe({props:{name:"forward",anchor:"transformers.OpenLlamaForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[list[torch.FloatTensor]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://huggingface.co/papers/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L848"}}),V=new Ce({props:{$$slots:{default:[Vn]},$$scope:{ctx:L}}}),de=new Hn({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/open-llama.md"}}),{c(){n=d("meta"),m=s(),a=d("p"),p=s(),w=d("p"),w.innerHTML=u,C=s(),f(D.$$.fragment),ze=s(),H=d("div"),H.innerHTML=fn,xe=s(),f(U.$$.fragment),Fe=s(),f(B.$$.fragment),Pe=s(),f(Y.$$.fragment),Ie=s(),A=d("p"),A.textContent=gn,qe=s(),R=d("p"),R.textContent=_n,We=s(),X=d("p"),X.innerHTML=bn,Se=s(),f(Q.$$.fragment),je=s(),M=d("div"),f(K.$$.fragment),De=s(),ue=d("p"),ue.innerHTML=vn,Ye=s(),he=d("p"),he.innerHTML=yn,Ae=s(),f(Z.$$.fragment),Je=s(),f(ee.$$.fragment),He=s(),k=d("div"),f(ne.$$.fragment),Re=s(),fe=d("p"),fe.innerHTML=Tn,Xe=s(),ge=d("p"),ge.innerHTML=$n,Qe=s(),_e=d("p"),_e.innerHTML=Ln,Ke=s(),P=d("div"),f(te.$$.fragment),en=s(),be=d("p"),be.innerHTML=wn,nn=s(),f(N.$$.fragment),Ue=s(),f(oe.$$.fragment),Be=s(),W=d("div"),f(ae.$$.fragment),tn=s(),z=d("div"),f(se.$$.fragment),on=s(),ve=d("p"),ve.innerHTML=kn,an=s(),f(G.$$.fragment),sn=s(),f(E.$$.fragment),Ze=s(),f(re.$$.fragment),Ne=s(),T=d("div"),f(ie.$$.fragment),rn=s(),ye=d("p"),ye.textContent=Mn,ln=s(),Te=d("p"),Te.innerHTML=On,dn=s(),$e=d("p"),$e.innerHTML=Cn,cn=s(),Le=d("p"),Le.innerHTML=zn,pn=s(),we=d("p"),we.innerHTML=xn,mn=s(),I=d("div"),f(le.$$.fragment),un=s(),ke=d("p"),ke.innerHTML=Fn,hn=s(),f(V.$$.fragment),Ge=s(),f(de.$$.fragment),Ee=s(),Oe=d("p"),this.h()},l(e){const t=Jn("svelte-u9bgzb",document.head);n=c(t,"META",{name:!0,content:!0}),t.forEach(o),m=r(e),a=c(e,"P",{}),J(a).forEach(o),p=r(e),w=c(e,"P",{"data-svelte-h":!0}),h(w)!=="svelte-1c2enaf"&&(w.innerHTML=u),C=r(e),g(D.$$.fragment,e),ze=r(e),H=c(e,"DIV",{class:!0,"data-svelte-h":!0}),h(H)!=="svelte-13t8s2t"&&(H.innerHTML=fn),xe=r(e),g(U.$$.fragment,e),Fe=r(e),g(B.$$.fragment,e),Pe=r(e),g(Y.$$.fragment,e),Ie=r(e),A=c(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-ycxpq0"&&(A.textContent=gn),qe=r(e),R=c(e,"P",{"data-svelte-h":!0}),h(R)!=="svelte-1snb5sc"&&(R.textContent=_n),We=r(e),X=c(e,"P",{"data-svelte-h":!0}),h(X)!=="svelte-glizh3"&&(X.innerHTML=bn),Se=r(e),g(Q.$$.fragment,e),je=r(e),M=c(e,"DIV",{class:!0});var x=J(M);g(K.$$.fragment,x),De=r(x),ue=c(x,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-1ermu4"&&(ue.innerHTML=vn),Ye=r(x),he=c(x,"P",{"data-svelte-h":!0}),h(he)!=="svelte-1ek1ss9"&&(he.innerHTML=yn),Ae=r(x),g(Z.$$.fragment,x),x.forEach(o),Je=r(e),g(ee.$$.fragment,e),He=r(e),k=c(e,"DIV",{class:!0});var O=J(k);g(ne.$$.fragment,O),Re=r(O),fe=c(O,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-1c84xf4"&&(fe.innerHTML=Tn),Xe=r(O),ge=c(O,"P",{"data-svelte-h":!0}),h(ge)!=="svelte-hswkmf"&&(ge.innerHTML=$n),Qe=r(O),_e=c(O,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-aj729g"&&(_e.innerHTML=Ln),Ke=r(O),P=c(O,"DIV",{class:!0});var S=J(P);g(te.$$.fragment,S),en=r(S),be=c(S,"P",{"data-svelte-h":!0}),h(be)!=="svelte-1hemhtn"&&(be.innerHTML=wn),nn=r(S),g(N.$$.fragment,S),S.forEach(o),O.forEach(o),Ue=r(e),g(oe.$$.fragment,e),Be=r(e),W=c(e,"DIV",{class:!0});var ce=J(W);g(ae.$$.fragment,ce),tn=r(ce),z=c(ce,"DIV",{class:!0});var F=J(z);g(se.$$.fragment,F),on=r(F),ve=c(F,"P",{"data-svelte-h":!0}),h(ve)!=="svelte-1yhngrn"&&(ve.innerHTML=kn),an=r(F),g(G.$$.fragment,F),sn=r(F),g(E.$$.fragment,F),F.forEach(o),ce.forEach(o),Ze=r(e),g(re.$$.fragment,e),Ne=r(e),T=c(e,"DIV",{class:!0});var $=J(T);g(ie.$$.fragment,$),rn=r($),ye=c($,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-62must"&&(ye.textContent=Mn),ln=r($),Te=c($,"P",{"data-svelte-h":!0}),h(Te)!=="svelte-ue4lvg"&&(Te.innerHTML=On),dn=r($),$e=c($,"P",{"data-svelte-h":!0}),h($e)!=="svelte-10ugs3m"&&($e.innerHTML=Cn),cn=r($),Le=c($,"P",{"data-svelte-h":!0}),h(Le)!=="svelte-q52n56"&&(Le.innerHTML=zn),pn=r($),we=c($,"P",{"data-svelte-h":!0}),h(we)!=="svelte-hswkmf"&&(we.innerHTML=xn),mn=r($),I=c($,"DIV",{class:!0});var j=J(I);g(le.$$.fragment,j),un=r(j),ke=c(j,"P",{"data-svelte-h":!0}),h(ke)!=="svelte-1cd8oj1"&&(ke.innerHTML=Fn),hn=r(j),g(V.$$.fragment,j),j.forEach(o),$.forEach(o),Ge=r(e),g(de.$$.fragment,e),Ee=r(e),Oe=c(e,"P",{}),J(Oe).forEach(o),this.h()},h(){q(n,"name","hf:doc:metadata"),q(n,"content",Yn),q(H,"class","flex flex-wrap space-x-1"),q(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){l(document.head,n),i(e,m,t),i(e,a,t),i(e,p,t),i(e,w,t),i(e,C,t),_(D,e,t),i(e,ze,t),i(e,H,t),i(e,xe,t),_(U,e,t),i(e,Fe,t),_(B,e,t),i(e,Pe,t),_(Y,e,t),i(e,Ie,t),i(e,A,t),i(e,qe,t),i(e,R,t),i(e,We,t),i(e,X,t),i(e,Se,t),_(Q,e,t),i(e,je,t),i(e,M,t),_(K,M,null),l(M,De),l(M,ue),l(M,Ye),l(M,he),l(M,Ae),_(Z,M,null),i(e,Je,t),_(ee,e,t),i(e,He,t),i(e,k,t),_(ne,k,null),l(k,Re),l(k,fe),l(k,Xe),l(k,ge),l(k,Qe),l(k,_e),l(k,Ke),l(k,P),_(te,P,null),l(P,en),l(P,be),l(P,nn),_(N,P,null),i(e,Ue,t),_(oe,e,t),i(e,Be,t),i(e,W,t),_(ae,W,null),l(W,tn),l(W,z),_(se,z,null),l(z,on),l(z,ve),l(z,an),_(G,z,null),l(z,sn),_(E,z,null),i(e,Ze,t),_(re,e,t),i(e,Ne,t),i(e,T,t),_(ie,T,null),l(T,rn),l(T,ye),l(T,ln),l(T,Te),l(T,dn),l(T,$e),l(T,cn),l(T,Le),l(T,pn),l(T,we),l(T,mn),l(T,I),_(le,I,null),l(I,un),l(I,ke),l(I,hn),_(V,I,null),i(e,Ge,t),_(de,e,t),i(e,Ee,t),i(e,Oe,t),Ve=!0},p(e,[t]){const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),U.$set(x);const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),B.$set(O);const S={};t&2&&(S.$$scope={dirty:t,ctx:e}),Z.$set(S);const ce={};t&2&&(ce.$$scope={dirty:t,ctx:e}),N.$set(ce);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),G.$set(F);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),E.$set($);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),V.$set(j)},i(e){Ve||(b(D.$$.fragment,e),b(U.$$.fragment,e),b(B.$$.fragment,e),b(Y.$$.fragment,e),b(Q.$$.fragment,e),b(K.$$.fragment,e),b(Z.$$.fragment,e),b(ee.$$.fragment,e),b(ne.$$.fragment,e),b(te.$$.fragment,e),b(N.$$.fragment,e),b(oe.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(G.$$.fragment,e),b(E.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(le.$$.fragment,e),b(V.$$.fragment,e),b(de.$$.fragment,e),Ve=!0)},o(e){v(D.$$.fragment,e),v(U.$$.fragment,e),v(B.$$.fragment,e),v(Y.$$.fragment,e),v(Q.$$.fragment,e),v(K.$$.fragment,e),v(Z.$$.fragment,e),v(ee.$$.fragment,e),v(ne.$$.fragment,e),v(te.$$.fragment,e),v(N.$$.fragment,e),v(oe.$$.fragment,e),v(ae.$$.fragment,e),v(se.$$.fragment,e),v(G.$$.fragment,e),v(E.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(le.$$.fragment,e),v(V.$$.fragment,e),v(de.$$.fragment,e),Ve=!1},d(e){e&&(o(m),o(a),o(p),o(w),o(C),o(ze),o(H),o(xe),o(Fe),o(Pe),o(Ie),o(A),o(qe),o(R),o(We),o(X),o(Se),o(je),o(M),o(Je),o(He),o(k),o(Ue),o(Be),o(W),o(Ze),o(Ne),o(T),o(Ge),o(Ee),o(Oe)),o(n),y(D,e),y(U,e),y(B,e),y(Y,e),y(Q,e),y(K),y(Z),y(ee,e),y(ne),y(te),y(N),y(oe,e),y(ae),y(se),y(G),y(E),y(re,e),y(ie),y(le),y(V),y(de,e)}}}const Yn='{"title":"Open-Llama","local":"open-llama","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"OpenLlamaConfig","local":"transformers.OpenLlamaConfig","sections":[],"depth":2},{"title":"OpenLlamaModel","local":"transformers.OpenLlamaModel","sections":[],"depth":2},{"title":"OpenLlamaForCausalLM","local":"transformers.OpenLlamaForCausalLM","sections":[],"depth":2},{"title":"OpenLlamaForSequenceClassification","local":"transformers.OpenLlamaForSequenceClassification","sections":[],"depth":2}],"depth":1}';function An(L){return Wn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ot extends Sn{constructor(n){super(),jn(this,n,An,Dn,qn,{})}}export{ot as component};
