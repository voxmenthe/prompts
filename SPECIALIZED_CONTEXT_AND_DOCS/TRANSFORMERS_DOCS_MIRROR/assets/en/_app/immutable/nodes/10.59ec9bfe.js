import{s as xt,o as kt,n as _t}from"../chunks/scheduler.18a86fab.js";import{S as jt,i as Ct,g as p,s as n,r as y,m as u,H as hs,A as Zt,h as i,f as t,c as l,j as xs,u as v,x as m,n as g,B as ds,k as yt,y as c,a as e,v as w,d as f,t as M,w as T}from"../chunks/index.98837b22.js";import{T as Ut}from"../chunks/Tip.77304350.js";import{C as ks}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as bs,E as zt}from"../chunks/getInferenceSnippets.06c2775f.js";function Vt(_s){let r,_="Caching should only be used for <strong>inference</strong>. It may cause unexpected errors if it’s enabled during training.";return{c(){r=p("p"),r.innerHTML=_},l(b){r=i(b,"P",{"data-svelte-h":!0}),m(r)!=="svelte-16dnkl8"&&(r.innerHTML=_)},m(b,us){e(b,r,us)},p:_t,d(b){b&&t(r)}}}function It(_s){let r,_,b,us,j,js,C,Na="Imagine you’re having a conversation with someone, and instead of remembering what they previously said, they have to start from scratch every time you respond. This would be slow and inefficient, right?",Cs,Z,Ra="You can extend this analogy to transformer models. Autoregressive model generation can be slow because it makes a prediction one token at a time. Each new prediction is dependent on all the previous context.",Zs,U,Ba="To predict the 1000th token, the model requires information from the previous 999 tokens. The information is represented as matrix multiplications across the token representations.",Us,z,Ya="To predict the 1001th token, you need the same information from the previous 999 tokens in addition to any information from the 1000th token. This is a lot of matrix multiplications a model has to compute over and over for each token!",zs,V,$a="A key-value (KV) cache eliminates this inefficiency by storing kv pairs derived from the attention layers of previously processed tokens. The stored kv pairs are retrieved from the cache and reused for subsequent tokens, avoiding the need to recompute.",Vs,k,Is,I,Ha="To better understand how and why caching works, let’s take a closer look at the structure of the attention matrices.",Ws,W,Xs,o,ja,gs,Ea="scaled dot-product attention",Ca,ys,Fa="b",Za,vs,Qa="h",Ua,ws,La="T",za,fs,Aa="d_head",Va,Gs,vt=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mtext>head</mtext></msub></msqrt></mfrac><mo>×</mo><mtext>mask</mtext><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">
\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_{\\text{head}}}} \\times \\text{mask} \\right) V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4761em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">head</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">mask</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span>`,Ns,X,qa="The query (<code>Q</code>), key (<code>K</code>), and value (<code>V</code>) matrices are projections from the input embeddings of shape <code>(b, h, T, d_head)</code>.",Rs,J,Ia,Bs,wt='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mtext>past</mtext></msub></mrow><annotation encoding="application/x-tex"> K_{\\text{past}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">past</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',Ys,$s,ft='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mtext>past</mtext></msub></mrow><annotation encoding="application/x-tex"> V_{\\text{past}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">past</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>',Hs,Es,Mt=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo separator="true">,</mo><mo stretchy="false">[</mo><munder><munder><mrow><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">⏟</mo></munder><mtext>cached</mtext></munder><mo separator="true">,</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">]</mo><mo separator="true">,</mo><mo stretchy="false">[</mo><munder><munder><mrow><msub><mi>v</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">⏟</mo></munder><mtext>cached</mtext></munder><mo separator="true">,</mo><msub><mi>v</mi><mi>t</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\\text{Attention}(q_t, [\\underbrace{k_1, k_2, \\dots, k_{t-1}}_{\\text{cached}}, k_{t}], [\\underbrace{v_1, v_2, \\dots, v_{t-1}}_{\\text{cached}}, v_{t}])
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.2924em;vertical-align:-1.5424em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">[</span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-1.4576em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cached</span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span class="svg-align" style="top:-2.1437em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMinYMin slice'><path d='M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z'/></svg></span><span class="brace-center" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMidYMin slice'><path d='M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z'/></svg></span><span class="brace-right" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMaxYMin slice'><path d='M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z'/></svg></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8563em;"><span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5424em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">[</span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-1.4576em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cached</span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span class="svg-align" style="top:-2.1437em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.548em;min-width:1.6em;"><span class="brace-left" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMinYMin slice'><path d='M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z'/></svg></span><span class="brace-center" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMidYMin slice'><path d='M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z'/></svg></span><span class="brace-right" style="height:0.548em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='0.548em' viewBox='0 0 400000 548' preserveAspectRatio='xMaxYMin slice'><path d='M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z'/></svg></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8563em;"><span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5424em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">])</span></span></span></span></span>`,Fs,d,Wa,Qs,Tt='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex"> x_t </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Ls,As,bt='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex"> t+1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>',qs,Ms,Sa="stored",Xa,Ts,Da="appended",Ga,Ss,Jt=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>cache</mtext></msub><mo>←</mo><mtext>concat</mtext><mo stretchy="false">(</mo><msub><mi>K</mi><mtext>past</mtext></msub><mo separator="true">,</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"/><msub><mi>V</mi><mtext>cache</mtext></msub><mo>←</mo><mtext>concat</mtext><mo stretchy="false">(</mo><msub><mi>V</mi><mtext>past</mtext></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
K_{\\text{cache}} \\leftarrow \\text{concat}(K_{\\text{past}}, k_t), \\quad V_{\\text{cache}} \\leftarrow \\text{concat}(V_{\\text{past}}, v_t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cache</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">past</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cache</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">past</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>`,Ds,G,Ka="Attention is calculated independently in each layer of the model, and caching is done on a per-layer basis.",Ks,N,Pa="Refer to the table below to compare how caching improves efficiency.",Ps,R,Oa="<thead><tr><th>without caching</th> <th>with caching</th></tr></thead> <tbody><tr><td>for each step, recompute all previous <code>K</code> and <code>V</code></td> <td>for each step, only compute current <code>K</code> and <code>V</code></td></tr> <tr><td>attention cost per step is <strong>quadratic</strong> with sequence length</td> <td>attention cost per step is <strong>linear</strong> with sequence length (memory grows linearly, but compute/token remains low)</td></tr></tbody>",Os,B,sa,Y,st="A basic KV cache interface takes a key and value tensor for the current token and returns the updated <code>K</code> and <code>V</code> tensors. This is internally managed by a model’s <code>forward</code> method.",aa,$,ta,H,at='When you use Transformers’ <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> class, the self-attention module performs several critical steps to integrate past and present information.',ea,E,tt='<li><p>The attention module concatenates current kv pairs with past kv pairs stored in the cache. This creates attentions weights with the shape <code>(new_tokens_length, past_kv_length + new_tokens_length)</code>. The current and past kv pairs are essentially combined to compute the attention scores, ensuring a model is aware of previous context and the current input.</p></li> <li><p>When the <code>forward</code> method is called iteratively, it’s crucial that the attention mask shape matches the combined length of the past and current kv pairs. The attention mask should have the shape <code>(batch_size, past_kv_length + new_tokens_length)</code>. This is typically handled internally in <a href="/docs/transformers/v4.56.2/en/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>, but if you want to implement your own generation loop with <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a>, keep this in mind! The attention mask should hold the past and current token values.</p></li> <li><p>It is also important to be aware of the <code>cache_position</code>. This is important if you want to reuse a prefilled <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> with the <code>forward</code> method because you have to pass a valid <code>cache_position</code> value. This indicates the input positions in a sequence. <code>cache_position</code> is unaffected by padding, and it always adds one more position for each token. For example, if a kv cache contains 10 tokens - regardless of pad tokens - the cache position for the next token should be <code>torch.tensor([10])</code>.</p></li>',na,F,la,Q,et="Caches are structured as a list of layers, where each layer contains a key and value cache. The key and value caches are tensors with the shape <code>[batch_size, num_heads, seq_len, head_dim]</code>.",pa,L,nt="Layers can be of different types (e.g. <code>DynamicLayer</code>, <code>StaticLayer</code>, <code>SlidingWindowLayer</code>), which mostly changes how sequence length is handled and how the cache is updated.",ia,A,lt="The simplest is a <code>DynamicLayer</code> that grows as more tokens are processed. The sequence length dimension (<code>seq_len</code>) increases with each new token:",ma,q,ca,S,pt="Other layer types like <code>StaticLayer</code> and <code>SlidingWindowLayer</code> have a fixed sequence length that is set when the cache is created. This makes them compatible with <code>torch.compile</code>. In the case of <code>SlidingWindowLayer</code>, existing tokens are shifted out of the cache when a new token is added.",ra,D,it='The example below demonstrates how to create a generation loop with <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a>. As discussed, the attention mask is a concatenation of past and current token values and <code>1</code> is added to the cache position for the next token.',oa,K,ha,P,da,O,mt="The cache position tracks where to insert new tokens in the attention cache. It represents the <em>absolute</em> position of each token in the context, independent of padding or batch structure. Suppose you already cached <code>N</code> tokens and are now processing <code>K</code> new tokens. The cache position for the new tokens will range from <code>N</code> to <code>N + K - 1</code>. In other words, you’re processing tokens at positions - <code>[N, N + 1, N + 2, ..., N + K - 1]</code>.",ua,ss,ct="Cache position is used internally for two purposes:",ga,as,rt="<li>Selecting new tokens to process in the input sequence and ensuring only tokens that haven’t been cached yet are passed to the model’s <code>forward</code>.</li> <li>Storing key/value pairs at the correct positions in the cache. This is especially important for fixed-size caches, that pre-allocates a specific cache length.</li>",ya,ts,ot="The generation loop usually takes care of the cache position, but if you’re writing a custom generation method, it is important that cache positions are accurate since they are used to write and read key/value states into fixed slots.",va,es,wa,ns,fa,ls,ht='Before the <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.Cache">Cache</a> class, the cache used to be stored as a tuple of tuples of tensors. This format is dynamic because it grows as text is generated, similar to <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a>.',Ma,ps,dt="The legacy format is essentially the same data structure but organized differently.",Ta,is,ut="<li>It’s a tuple of tuples, where each inner tuple contains the key and value tensors for a layer.</li> <li>The tensors have the same shape <code>[batch_size, num_heads, seq_len, head_dim]</code>.</li> <li>The format is less flexible and doesn’t support features like quantization or offloading.</li>",ba,ms,gt='If your project depends on this legacy format, we recommend to convert to <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache">DynamicCache</a> with <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache.from_legacy_cache">from_legacy_cache()</a>. Note that legacy cache format is deprecated and not used anymore in <code>Transformers</code>. You can convert back to tuple format with <a href="/docs/transformers/v4.56.2/en/internal/generation_utils#transformers.DynamicCache.to_legacy_cache">DynamicCache.to_legacy_cache()</a> functions, which is helpful if you have custom logic for manipulating a cache in a specific format.',Ja,cs,xa,rs,ka,Js,_a;return j=new bs({props:{title:"Caching",local:"caching",headingTag:"h1"}}),k=new Ut({props:{warning:!0,$$slots:{default:[Vt]},$$scope:{ctx:_s}}}),W=new bs({props:{title:"Attention matrices",local:"attention-matrices",headingTag:"h2"}}),B=new bs({props:{title:"Cache class",local:"cache-class",headingTag:"h2"}}),$=new ks({props:{code:"bmV3X0slMkMlMjBuZXdfViUyMCUzRCUyMGNhY2hlLnVwZGF0ZShrX3QlMkMlMjB2X3QlMkMlMjBsYXllcl9pZHgpJTBBYXR0bl9vdXRwdXQlMjAlM0QlMjBhdHRuX2xheWVyX2lkeF9mbihxX3QlMkMlMjBuZXdfSyUyQyUyMG5ld19WKQ==",highlighted:`new_K, new_V = cache.update(k_t, v_t, layer_idx)
attn_output = attn_layer_idx_fn(q_t, new_K, new_V)`,wrap:!1}}),F=new bs({props:{title:"Cache storage implementation",local:"cache-storage-implementation",headingTag:"h2"}}),q=new ks({props:{code:"Y2FjaGUubGF5ZXJzJTVCaWR4JTVELmtleXMlMjAlM0QlMjB0b3JjaC5jYXQoJTVCY2FjaGUubGF5ZXJzJTVCaWR4JTVELmtleXMlMkMlMjBrZXlfc3RhdGVzJTVEJTJDJTIwZGltJTNELTIpJTBBY2FjaGUubGF5ZXJzJTVCaWR4JTVELnZhbHVlcyUyMCUzRCUyMHRvcmNoLmNhdCglNUJjYWNoZS5sYXllcnMlNUJpZHglNUQudmFsdWVzJTJDJTIwdmFsdWVfc3RhdGVzJTVEJTJDJTIwZGltJTNELTIp",highlighted:`cache.layers[idx].keys = torch.cat([cache.layers[idx].keys, key_states], dim=-<span class="hljs-number">2</span>)
cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], dim=-<span class="hljs-number">2</span>)`,wrap:!1}}),K=new ks({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwRHluYW1pY0NhY2hlJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwZiUyMiU3QmluZmVyX2RldmljZSgpJTdEJTNBMCUyMiUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUyMGRldmljZV9tYXAlM0RkZXZpY2UpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTBBcGFzdF9rZXlfdmFsdWVzJTIwJTNEJTIwRHluYW1pY0NhY2hlKGNvbmZpZyUzRG1vZGVsLmNvbmZpZyklMEFtZXNzYWdlcyUyMCUzRCUyMCU1QiU3QiUyMnJvbGUlMjIlM0ElMjAlMjJ1c2VyJTIyJTJDJTIwJTIyY29udGVudCUyMiUzQSUyMCUyMkhlbGxvJTJDJTIwd2hhdCdzJTIweW91ciUyMG5hbWUuJTIyJTdEJTVEJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyLmFwcGx5X2NoYXRfdGVtcGxhdGUobWVzc2FnZXMlMkMlMjBhZGRfZ2VuZXJhdGlvbl9wcm9tcHQlM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMHJldHVybl9kaWN0JTNEVHJ1ZSkudG8obW9kZWwuZGV2aWNlKSUwQSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBpbnB1dHMuaW5wdXRfaWRzJTBBY2FjaGVfcG9zaXRpb24lMjAlM0QlMjB0b3JjaC5hcmFuZ2UoaW5wdXRzLmlucHV0X2lkcy5zaGFwZSU1QjElNUQlMkMlMjBkdHlwZSUzRHRvcmNoLmludDY0JTJDJTIwZGV2aWNlJTNEbW9kZWwuZGV2aWNlKSUwQW1heF9uZXdfdG9rZW5zJTIwJTNEJTIwMTAlMEElMEFmb3IlMjBfJTIwaW4lMjByYW5nZShtYXhfbmV3X3Rva2VucyklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjBjYWNoZV9wb3NpdGlvbiUzRGNhY2hlX3Bvc2l0aW9uJTJDJTIwcGFzdF9rZXlfdmFsdWVzJTNEcGFzdF9rZXlfdmFsdWVzJTJDJTIwdXNlX2NhY2hlJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjAlMjMlMjBHcmVlZGlseSUyMHNhbXBsZSUyMG9uZSUyMG5leHQlMjB0b2tlbiUwQSUyMCUyMCUyMCUyMG5leHRfdG9rZW5faWRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlNUIlM0ElMkMlMjAtMSUzQSU1RC5hcmdtYXgoLTEpJTBBJTIwJTIwJTIwJTIwZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMHRvcmNoLmNhdCglNUJnZW5lcmF0ZWRfaWRzJTJDJTIwbmV4dF90b2tlbl9pZHMlNUQlMkMlMjBkaW0lM0QtMSklMEElMjAlMjAlMjAlMjAlMjMlMjBQcmVwYXJlJTIwaW5wdXRzJTIwZm9yJTIwdGhlJTIwbmV4dCUyMGdlbmVyYXRpb24lMjBzdGVwJTIwYnklMjBsZWF2aW5nJTIwdW5wcm9jZXNzZWQlMjB0b2tlbnMlMkMlMjBpbiUyMG91ciUyMGNhc2UlMjB3ZSUyMGhhdmUlMjBvbmx5JTIwb25lJTIwbmV3JTIwdG9rZW4lMEElMjAlMjAlMjAlMjAlMjMlMjBhbmQlMjBleHBhbmRpbmclMjBhdHRuJTIwbWFzayUyMGZvciUyMHRoZSUyMG5ldyUyMHRva2VuJTJDJTIwYXMlMjBleHBsYWluZWQlMjBhYm92ZSUwQSUyMCUyMCUyMCUyMGF0dGVudGlvbl9tYXNrJTIwJTNEJTIwaW5wdXRzJTVCJTIyYXR0ZW50aW9uX21hc2slMjIlNUQlMEElMjAlMjAlMjAlMjBhdHRlbnRpb25fbWFzayUyMCUzRCUyMHRvcmNoLmNhdCglNUJhdHRlbnRpb25fbWFzayUyQyUyMGF0dGVudGlvbl9tYXNrLm5ld19vbmVzKChhdHRlbnRpb25fbWFzay5zaGFwZSU1QjAlNUQlMkMlMjAxKSklNUQlMkMlMjBkaW0lM0QtMSklMEElMjAlMjAlMjAlMjBpbnB1dHMlMjAlM0QlMjAlN0IlMjJpbnB1dF9pZHMlMjIlM0ElMjBuZXh0X3Rva2VuX2lkcyUyQyUyMCUyMmF0dGVudGlvbl9tYXNrJTIyJTNBJTIwYXR0ZW50aW9uX21hc2slN0QlMEElMjAlMjAlMjAlMjBjYWNoZV9wb3NpdGlvbiUyMCUzRCUyMGNhY2hlX3Bvc2l0aW9uJTVCLTElM0ElNUQlMjAlMkIlMjAxJTIwJTIzJTIwYWRkJTIwb25lJTIwbW9yZSUyMHBvc2l0aW9uJTIwZm9yJTIwdGhlJTIwbmV4dCUyMHRva2VuJTBBJTBBcHJpbnQodG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCklMEElMjIlNUJJTlNUJTVEJTIwSGVsbG8lMkMlMjB3aGF0J3MlMjB5b3VyJTIwbmFtZS4lMjAlNUIlMkZJTlNUJTVEJTIwJTIwSGVsbG8hJTIwTXklMjBuYW1lJTIwaXMlMjBMTGFNQSUyQyUyMg==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device

device = <span class="hljs-string">f&quot;<span class="hljs-subst">{infer_device()}</span>:0&quot;</span>

model_id = <span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)
tokenizer = AutoTokenizer.from_pretrained(model_id)

past_key_values = DynamicCache(config=model.config)
messages = [{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello, what&#x27;s your name.&quot;</span>}]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, return_dict=<span class="hljs-literal">True</span>).to(model.device)

generated_ids = inputs.input_ids
cache_position = torch.arange(inputs.input_ids.shape[<span class="hljs-number">1</span>], dtype=torch.int64, device=model.device)
max_new_tokens = <span class="hljs-number">10</span>

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):
    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Greedily sample one next token</span>
    next_token_ids = outputs.logits[:, -<span class="hljs-number">1</span>:].argmax(-<span class="hljs-number">1</span>)
    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-<span class="hljs-number">1</span>)
    <span class="hljs-comment"># Prepare inputs for the next generation step by leaving unprocessed tokens, in our case we have only one new token</span>
    <span class="hljs-comment"># and expanding attn mask for the new token, as explained above</span>
    attention_mask = inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>]
    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))], dim=-<span class="hljs-number">1</span>)
    inputs = {<span class="hljs-string">&quot;input_ids&quot;</span>: next_token_ids, <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask}
    cache_position = cache_position[-<span class="hljs-number">1</span>:] + <span class="hljs-number">1</span> <span class="hljs-comment"># add one more position for the next token</span>

<span class="hljs-built_in">print</span>(tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;[INST] Hello, what&#x27;s your name. [/INST]  Hello! My name is LLaMA,&quot;</span>`,wrap:!1}}),P=new bs({props:{title:"Cache position",local:"cache-position",headingTag:"h2"}}),es=new ks({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwRHluYW1pY0NhY2hlJTJDJTIwaW5mZXJfZGV2aWNlJTBBJTBBZGV2aWNlJTIwJTNEJTIwZiUyMiU3QmluZmVyX2RldmljZSgpJTdEJTNBMCUyMiUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUyQyUyMGRldmljZV9tYXAlM0RkZXZpY2UpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTBBbWVzc2FnZXMlMjAlM0QlMjAlNUIlN0IlMjJyb2xlJTIyJTNBJTIwJTIydXNlciUyMiUyQyUyMCUyMmNvbnRlbnQlMjIlM0ElMjAlMjJZb3UlMjBhcmUlMjBhJTIwaGVscGZ1bCUyMGFzc2lzdGFudC4lMjIlN0QlNUQlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYXBwbHlfY2hhdF90ZW1wbGF0ZShtZXNzYWdlcyUyQyUyMGFkZF9nZW5lcmF0aW9uX3Byb21wdCUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcmV0dXJuX2RpY3QlM0RUcnVlKS50byhtb2RlbC5kZXZpY2UpJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwdXNlX2NhY2hlJTNEVHJ1ZSUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTApJTBB",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device

device = <span class="hljs-string">f&quot;<span class="hljs-subst">{infer_device()}</span>:0&quot;</span>

model_id = <span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)
tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>}]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, return_dict=<span class="hljs-literal">True</span>).to(model.device)
generated_ids = model.generate(**inputs, use_cache=<span class="hljs-literal">True</span>, max_new_tokens=<span class="hljs-number">10</span>)
`,wrap:!1}}),ns=new bs({props:{title:"Legacy cache format",local:"legacy-cache-format",headingTag:"h2"}}),cs=new ks({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwRHluYW1pY0NhY2hlJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWV0YS1sbGFtYSUyRkxsYW1hLTItN2ItY2hhdC1oZiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1jaGF0LWhmJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwbXklMjBuYW1lJTIwaXMlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBJTIzJTIwJTYwcmV0dXJuX2RpY3RfaW5fZ2VuZXJhdGUlM0RUcnVlJTYwJTIwaXMlMjByZXF1aXJlZCUyMHRvJTIwcmV0dXJuJTIwdGhlJTIwY2FjaGUlMjBhbmQlMjAlNjByZXR1cm5fbGVnYWN5X2NhY2hlJTYwJTIwZm9yY2VzJTIwdGhlJTIwcmV0dXJuZWQlMjBjYWNoZSUwQSUyMyUyMGluJTIwdGhlJTIwbGVnYWN5JTIwZm9ybWF0JTBBZ2VuZXJhdGlvbl9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjByZXR1cm5fZGljdF9pbl9nZW5lcmF0ZSUzRFRydWUlMkMlMjByZXR1cm5fbGVnYWN5X2NhY2hlJTNEVHJ1ZSUyQyUyMG1heF9uZXdfdG9rZW5zJTNENSklMEElMEFjYWNoZSUyMCUzRCUyMER5bmFtaWNDYWNoZS5mcm9tX2xlZ2FjeV9jYWNoZShnZW5lcmF0aW9uX291dHB1dHMucGFzdF9rZXlfdmFsdWVzKSUwQWxlZ2FjeV9mb3JtYXRfY2FjaGUlMjAlM0QlMjBjYWNoZS50b19sZWdhY3lfY2FjaGUoKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM, DynamicCache

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>, dtype=torch.float16, device_map=<span class="hljs-string">&quot;auto&quot;</span>)
inputs = tokenizer(<span class="hljs-string">&quot;Hello, my name is&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-comment"># \`return_dict_in_generate=True\` is required to return the cache and \`return_legacy_cache\` forces the returned cache</span>
<span class="hljs-comment"># in the legacy format</span>
generation_outputs = model.generate(**inputs, return_dict_in_generate=<span class="hljs-literal">True</span>, return_legacy_cache=<span class="hljs-literal">True</span>, max_new_tokens=<span class="hljs-number">5</span>)

cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)
legacy_format_cache = cache.to_legacy_cache()`,wrap:!1}}),rs=new zt({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/cache_explanation.md"}}),{c(){r=p("meta"),_=n(),b=p("p"),us=n(),y(j.$$.fragment),js=n(),C=p("p"),C.textContent=Na,Cs=n(),Z=p("p"),Z.textContent=Ra,Zs=n(),U=p("p"),U.textContent=Ba,Us=n(),z=p("p"),z.textContent=Ya,zs=n(),V=p("p"),V.textContent=$a,Vs=n(),y(k.$$.fragment),Is=n(),I=p("p"),I.textContent=Ha,Ws=n(),y(W.$$.fragment),Xs=n(),o=p("p"),ja=u("The "),gs=p("strong"),gs.textContent=Ea,Ca=u(" is calculated as shown below for a batch of size "),ys=p("code"),ys.textContent=Fa,Za=u(", number of attention heads "),vs=p("code"),vs.textContent=Qa,Ua=u(", sequence length so far "),ws=p("code"),ws.textContent=La,za=u(", and dimension per attention head "),fs=p("code"),fs.textContent=Aa,Va=u(`.
`),Gs=new hs(!1),Ns=n(),X=p("p"),X.innerHTML=qa,Rs=n(),J=p("p"),Ia=u("For causal attention, the mask prevents the model from attending to future tokens. Once a token is processed, its representation never changes with respect to future tokens, which means"),Bs=new hs(!1),Ys=u(" and"),$s=new hs(!1),Hs=u(` can be cached and reused to compute the last token’s representation.
`),Es=new hs(!1),Fs=n(),d=p("p"),Wa=u("At inference time, you only need the last token’s query to compute the representation"),Qs=new hs(!1),Ls=u(" that predicts the next token"),As=new hs(!1),qs=u(". At each step, the new key and value vectors are "),Ms=p("strong"),Ms.textContent=Sa,Xa=u(" in the cache and "),Ts=p("strong"),Ts.textContent=Da,Ga=u(` to the past keys and values.
`),Ss=new hs(!1),Ds=n(),G=p("p"),G.textContent=Ka,Ks=n(),N=p("p"),N.textContent=Pa,Ps=n(),R=p("table"),R.innerHTML=Oa,Os=n(),y(B.$$.fragment),sa=n(),Y=p("p"),Y.innerHTML=st,aa=n(),y($.$$.fragment),ta=n(),H=p("p"),H.innerHTML=at,ea=n(),E=p("ol"),E.innerHTML=tt,na=n(),y(F.$$.fragment),la=n(),Q=p("p"),Q.innerHTML=et,pa=n(),L=p("p"),L.innerHTML=nt,ia=n(),A=p("p"),A.innerHTML=lt,ma=n(),y(q.$$.fragment),ca=n(),S=p("p"),S.innerHTML=pt,ra=n(),D=p("p"),D.innerHTML=it,oa=n(),y(K.$$.fragment),ha=n(),y(P.$$.fragment),da=n(),O=p("p"),O.innerHTML=mt,ua=n(),ss=p("p"),ss.textContent=ct,ga=n(),as=p("ol"),as.innerHTML=rt,ya=n(),ts=p("p"),ts.textContent=ot,va=n(),y(es.$$.fragment),wa=n(),y(ns.$$.fragment),fa=n(),ls=p("p"),ls.innerHTML=ht,Ma=n(),ps=p("p"),ps.textContent=dt,Ta=n(),is=p("ul"),is.innerHTML=ut,ba=n(),ms=p("p"),ms.innerHTML=gt,Ja=n(),y(cs.$$.fragment),xa=n(),y(rs.$$.fragment),ka=n(),Js=p("p"),this.h()},l(s){const a=Zt("svelte-u9bgzb",document.head);r=i(a,"META",{name:!0,content:!0}),a.forEach(t),_=l(s),b=i(s,"P",{}),xs(b).forEach(t),us=l(s),v(j.$$.fragment,s),js=l(s),C=i(s,"P",{"data-svelte-h":!0}),m(C)!=="svelte-1vgfrh0"&&(C.textContent=Na),Cs=l(s),Z=i(s,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-110unin"&&(Z.textContent=Ra),Zs=l(s),U=i(s,"P",{"data-svelte-h":!0}),m(U)!=="svelte-1sc0ng3"&&(U.textContent=Ba),Us=l(s),z=i(s,"P",{"data-svelte-h":!0}),m(z)!=="svelte-stwnf4"&&(z.textContent=Ya),zs=l(s),V=i(s,"P",{"data-svelte-h":!0}),m(V)!=="svelte-k1lwaa"&&(V.textContent=$a),Vs=l(s),v(k.$$.fragment,s),Is=l(s),I=i(s,"P",{"data-svelte-h":!0}),m(I)!=="svelte-aj1f1y"&&(I.textContent=Ha),Ws=l(s),v(W.$$.fragment,s),Xs=l(s),o=i(s,"P",{});var h=xs(o);ja=g(h,"The "),gs=i(h,"STRONG",{"data-svelte-h":!0}),m(gs)!=="svelte-1et3y0d"&&(gs.textContent=Ea),Ca=g(h," is calculated as shown below for a batch of size "),ys=i(h,"CODE",{"data-svelte-h":!0}),m(ys)!=="svelte-1y90nls"&&(ys.textContent=Fa),Za=g(h,", number of attention heads "),vs=i(h,"CODE",{"data-svelte-h":!0}),m(vs)!=="svelte-1blmdqe"&&(vs.textContent=Qa),Ua=g(h,", sequence length so far "),ws=i(h,"CODE",{"data-svelte-h":!0}),m(ws)!=="svelte-18tc35m"&&(ws.textContent=La),za=g(h,", and dimension per attention head "),fs=i(h,"CODE",{"data-svelte-h":!0}),m(fs)!=="svelte-krx0ij"&&(fs.textContent=Aa),Va=g(h,`.
`),Gs=ds(h,!1),h.forEach(t),Ns=l(s),X=i(s,"P",{"data-svelte-h":!0}),m(X)!=="svelte-11ldl4s"&&(X.innerHTML=qa),Rs=l(s),J=i(s,"P",{});var os=xs(J);Ia=g(os,"For causal attention, the mask prevents the model from attending to future tokens. Once a token is processed, its representation never changes with respect to future tokens, which means"),Bs=ds(os,!1),Ys=g(os," and"),$s=ds(os,!1),Hs=g(os,` can be cached and reused to compute the last token’s representation.
`),Es=ds(os,!1),os.forEach(t),Fs=l(s),d=i(s,"P",{});var x=xs(d);Wa=g(x,"At inference time, you only need the last token’s query to compute the representation"),Qs=ds(x,!1),Ls=g(x," that predicts the next token"),As=ds(x,!1),qs=g(x,". At each step, the new key and value vectors are "),Ms=i(x,"STRONG",{"data-svelte-h":!0}),m(Ms)!=="svelte-ho7h3"&&(Ms.textContent=Sa),Xa=g(x," in the cache and "),Ts=i(x,"STRONG",{"data-svelte-h":!0}),m(Ts)!=="svelte-ygjkh1"&&(Ts.textContent=Da),Ga=g(x,` to the past keys and values.
`),Ss=ds(x,!1),x.forEach(t),Ds=l(s),G=i(s,"P",{"data-svelte-h":!0}),m(G)!=="svelte-fo5dj4"&&(G.textContent=Ka),Ks=l(s),N=i(s,"P",{"data-svelte-h":!0}),m(N)!=="svelte-ebwicu"&&(N.textContent=Pa),Ps=l(s),R=i(s,"TABLE",{"data-svelte-h":!0}),m(R)!=="svelte-1s32lyj"&&(R.innerHTML=Oa),Os=l(s),v(B.$$.fragment,s),sa=l(s),Y=i(s,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-ranj33"&&(Y.innerHTML=st),aa=l(s),v($.$$.fragment,s),ta=l(s),H=i(s,"P",{"data-svelte-h":!0}),m(H)!=="svelte-15wke35"&&(H.innerHTML=at),ea=l(s),E=i(s,"OL",{"data-svelte-h":!0}),m(E)!=="svelte-1achmxq"&&(E.innerHTML=tt),na=l(s),v(F.$$.fragment,s),la=l(s),Q=i(s,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-1su092y"&&(Q.innerHTML=et),pa=l(s),L=i(s,"P",{"data-svelte-h":!0}),m(L)!=="svelte-1ibghkg"&&(L.innerHTML=nt),ia=l(s),A=i(s,"P",{"data-svelte-h":!0}),m(A)!=="svelte-1nihuku"&&(A.innerHTML=lt),ma=l(s),v(q.$$.fragment,s),ca=l(s),S=i(s,"P",{"data-svelte-h":!0}),m(S)!=="svelte-ge5x3w"&&(S.innerHTML=pt),ra=l(s),D=i(s,"P",{"data-svelte-h":!0}),m(D)!=="svelte-caqz0a"&&(D.innerHTML=it),oa=l(s),v(K.$$.fragment,s),ha=l(s),v(P.$$.fragment,s),da=l(s),O=i(s,"P",{"data-svelte-h":!0}),m(O)!=="svelte-anb3re"&&(O.innerHTML=mt),ua=l(s),ss=i(s,"P",{"data-svelte-h":!0}),m(ss)!=="svelte-ecafas"&&(ss.textContent=ct),ga=l(s),as=i(s,"OL",{"data-svelte-h":!0}),m(as)!=="svelte-afdkur"&&(as.innerHTML=rt),ya=l(s),ts=i(s,"P",{"data-svelte-h":!0}),m(ts)!=="svelte-j9c3os"&&(ts.textContent=ot),va=l(s),v(es.$$.fragment,s),wa=l(s),v(ns.$$.fragment,s),fa=l(s),ls=i(s,"P",{"data-svelte-h":!0}),m(ls)!=="svelte-174s0p4"&&(ls.innerHTML=ht),Ma=l(s),ps=i(s,"P",{"data-svelte-h":!0}),m(ps)!=="svelte-t0zkvk"&&(ps.textContent=dt),Ta=l(s),is=i(s,"UL",{"data-svelte-h":!0}),m(is)!=="svelte-tticje"&&(is.innerHTML=ut),ba=l(s),ms=i(s,"P",{"data-svelte-h":!0}),m(ms)!=="svelte-yvobv"&&(ms.innerHTML=gt),Ja=l(s),v(cs.$$.fragment,s),xa=l(s),v(rs.$$.fragment,s),ka=l(s),Js=i(s,"P",{}),xs(Js).forEach(t),this.h()},h(){yt(r,"name","hf:doc:metadata"),yt(r,"content",Wt),Gs.a=null,Bs.a=Ys,$s.a=Hs,Es.a=null,Qs.a=Ls,As.a=qs,Ss.a=null},m(s,a){c(document.head,r),e(s,_,a),e(s,b,a),e(s,us,a),w(j,s,a),e(s,js,a),e(s,C,a),e(s,Cs,a),e(s,Z,a),e(s,Zs,a),e(s,U,a),e(s,Us,a),e(s,z,a),e(s,zs,a),e(s,V,a),e(s,Vs,a),w(k,s,a),e(s,Is,a),e(s,I,a),e(s,Ws,a),w(W,s,a),e(s,Xs,a),e(s,o,a),c(o,ja),c(o,gs),c(o,Ca),c(o,ys),c(o,Za),c(o,vs),c(o,Ua),c(o,ws),c(o,za),c(o,fs),c(o,Va),Gs.m(vt,o),e(s,Ns,a),e(s,X,a),e(s,Rs,a),e(s,J,a),c(J,Ia),Bs.m(wt,J),c(J,Ys),$s.m(ft,J),c(J,Hs),Es.m(Mt,J),e(s,Fs,a),e(s,d,a),c(d,Wa),Qs.m(Tt,d),c(d,Ls),As.m(bt,d),c(d,qs),c(d,Ms),c(d,Xa),c(d,Ts),c(d,Ga),Ss.m(Jt,d),e(s,Ds,a),e(s,G,a),e(s,Ks,a),e(s,N,a),e(s,Ps,a),e(s,R,a),e(s,Os,a),w(B,s,a),e(s,sa,a),e(s,Y,a),e(s,aa,a),w($,s,a),e(s,ta,a),e(s,H,a),e(s,ea,a),e(s,E,a),e(s,na,a),w(F,s,a),e(s,la,a),e(s,Q,a),e(s,pa,a),e(s,L,a),e(s,ia,a),e(s,A,a),e(s,ma,a),w(q,s,a),e(s,ca,a),e(s,S,a),e(s,ra,a),e(s,D,a),e(s,oa,a),w(K,s,a),e(s,ha,a),w(P,s,a),e(s,da,a),e(s,O,a),e(s,ua,a),e(s,ss,a),e(s,ga,a),e(s,as,a),e(s,ya,a),e(s,ts,a),e(s,va,a),w(es,s,a),e(s,wa,a),w(ns,s,a),e(s,fa,a),e(s,ls,a),e(s,Ma,a),e(s,ps,a),e(s,Ta,a),e(s,is,a),e(s,ba,a),e(s,ms,a),e(s,Ja,a),w(cs,s,a),e(s,xa,a),w(rs,s,a),e(s,ka,a),e(s,Js,a),_a=!0},p(s,[a]){const h={};a&2&&(h.$$scope={dirty:a,ctx:s}),k.$set(h)},i(s){_a||(f(j.$$.fragment,s),f(k.$$.fragment,s),f(W.$$.fragment,s),f(B.$$.fragment,s),f($.$$.fragment,s),f(F.$$.fragment,s),f(q.$$.fragment,s),f(K.$$.fragment,s),f(P.$$.fragment,s),f(es.$$.fragment,s),f(ns.$$.fragment,s),f(cs.$$.fragment,s),f(rs.$$.fragment,s),_a=!0)},o(s){M(j.$$.fragment,s),M(k.$$.fragment,s),M(W.$$.fragment,s),M(B.$$.fragment,s),M($.$$.fragment,s),M(F.$$.fragment,s),M(q.$$.fragment,s),M(K.$$.fragment,s),M(P.$$.fragment,s),M(es.$$.fragment,s),M(ns.$$.fragment,s),M(cs.$$.fragment,s),M(rs.$$.fragment,s),_a=!1},d(s){s&&(t(_),t(b),t(us),t(js),t(C),t(Cs),t(Z),t(Zs),t(U),t(Us),t(z),t(zs),t(V),t(Vs),t(Is),t(I),t(Ws),t(Xs),t(o),t(Ns),t(X),t(Rs),t(J),t(Fs),t(d),t(Ds),t(G),t(Ks),t(N),t(Ps),t(R),t(Os),t(sa),t(Y),t(aa),t(ta),t(H),t(ea),t(E),t(na),t(la),t(Q),t(pa),t(L),t(ia),t(A),t(ma),t(ca),t(S),t(ra),t(D),t(oa),t(ha),t(da),t(O),t(ua),t(ss),t(ga),t(as),t(ya),t(ts),t(va),t(wa),t(fa),t(ls),t(Ma),t(ps),t(Ta),t(is),t(ba),t(ms),t(Ja),t(xa),t(ka),t(Js)),t(r),T(j,s),T(k,s),T(W,s),T(B,s),T($,s),T(F,s),T(q,s),T(K,s),T(P,s),T(es,s),T(ns,s),T(cs,s),T(rs,s)}}}const Wt='{"title":"Caching","local":"caching","sections":[{"title":"Attention matrices","local":"attention-matrices","sections":[],"depth":2},{"title":"Cache class","local":"cache-class","sections":[],"depth":2},{"title":"Cache storage implementation","local":"cache-storage-implementation","sections":[],"depth":2},{"title":"Cache position","local":"cache-position","sections":[],"depth":2},{"title":"Legacy cache format","local":"legacy-cache-format","sections":[],"depth":2}],"depth":1}';function Xt(_s){return kt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $t extends jt{constructor(r){super(),Ct(this,r,Xt,It,xt,{})}}export{$t as component};
