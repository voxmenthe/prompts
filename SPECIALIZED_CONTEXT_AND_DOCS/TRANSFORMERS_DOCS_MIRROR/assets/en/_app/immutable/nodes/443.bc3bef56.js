import{s as ss,z as ns,o as os,n as Je}from"../chunks/scheduler.18a86fab.js";import{S as as,i as rs,g as l,s as o,r as h,A as is,h as d,f as s,c as a,j as Me,x as p,u as g,k as U,y as f,a as n,v as u,d as _,t as T,w as M}from"../chunks/index.98837b22.js";import{T as ts}from"../chunks/Tip.77304350.js";import{D as Se}from"../chunks/Docstring.a1ef7999.js";import{C as ct}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Nt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{P as ls}from"../chunks/PipelineTag.7749150e.js";import{H as L,E as ds}from"../chunks/getInferenceSnippets.06c2775f.js";function cs(N){let r,v="Example:",c,m,b;return m=new ct({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpVE1TTk1vZGVsJTJDJTIwVmlUTVNOQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZpVCUyME1TTiUyMHZpdC1tc24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBWaVRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjB2aXQtbXNuLWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFZpVE1TTk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ViTMSNModel, ViTMSNConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT MSN vit-msn-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ViTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the vit-msn-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){r=l("p"),r.textContent=v,c=o(),h(m.$$.fragment)},l(i){r=d(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-11lpom8"&&(r.textContent=v),c=a(i),g(m.$$.fragment,i)},m(i,w){n(i,r,w),n(i,c,w),u(m,i,w),b=!0},p:Je,i(i){b||(_(m.$$.fragment,i),b=!0)},o(i){T(m.$$.fragment,i),b=!1},d(i){i&&(s(r),s(c)),M(m,i)}}}function ms(N){let r,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=l("p"),r.innerHTML=v},l(c){r=d(c,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=v)},m(c,m){n(c,r,m)},p:Je,d(c){c&&s(r)}}}function ps(N){let r,v="Examples:",c,m,b;return m=new ct({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1TTk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGdml0LW1zbi1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMFZpVE1TTk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMSNModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNModel.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){r=l("p"),r.textContent=v,c=o(),h(m.$$.fragment)},l(i){r=d(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-kvfsh7"&&(r.textContent=v),c=a(i),g(m.$$.fragment,i)},m(i,w){n(i,r,w),n(i,c,w),u(m,i,w),b=!0},p:Je,i(i){b||(_(m.$$.fragment,i),b=!0)},o(i){T(m.$$.fragment,i),b=!1},d(i){i&&(s(r),s(c)),M(m,i)}}}function fs(N){let r,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=l("p"),r.innerHTML=v},l(c){r=d(c,"P",{"data-svelte-h":!0}),p(r)!=="svelte-fincs2"&&(r.innerHTML=v)},m(c,m){n(c,r,m)},p:Je,d(c){c&&s(r)}}}function hs(N){let r,v="Examples:",c,m,b;return m=new ct({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1TTkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdG9yY2gubWFudWFsX3NlZWQoMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBbW9kZWwlMjAlM0QlMjBWaVRNU05Gb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMSNForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tusker`,wrap:!1}}),{c(){r=l("p"),r.textContent=v,c=o(),h(m.$$.fragment)},l(i){r=d(i,"P",{"data-svelte-h":!0}),p(r)!=="svelte-kvfsh7"&&(r.textContent=v),c=a(i),g(m.$$.fragment,i)},m(i,w){n(i,r,w),n(i,c,w),u(m,i,w),b=!0},p:Je,i(i){b||(_(m.$$.fragment,i),b=!0)},o(i){T(m.$$.fragment,i),b=!1},d(i){i&&(s(r),s(c)),M(m,i)}}}function gs(N){let r,v,c,m,b,i="<em>This model was released on 2022-04-14 and added to Hugging Face Transformers on 2022-09-22.</em>",w,G,Ie,Z,jt='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',Ue,R,Ze,E,xt=`The ViTMSN model was proposed in <a href="https://huggingface.co/papers/2204.07141" rel="nofollow">Masked Siamese Networks for Label-Efficient Learning</a> by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,
Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypes
of masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low-shot
regimes.`,Fe,q,St="The abstract from the paper is the following:",ze,Y,Jt=`<em>We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our
approach matches the representation of an image view containing randomly masked patches to the representation of the original
unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the
unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures,
while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance,
on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy,
and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark.</em>`,We,F,It,Pe,A,Ut='MSN architecture. Taken from the <a href="https://huggingface.co/papers/2204.07141">original paper.</a>',He,X,Zt='This model was contributed by <a href="https://huggingface.co/sayakpaul" rel="nofollow">sayakpaul</a>. The original code can be found <a href="https://github.com/facebookresearch/msn" rel="nofollow">here</a>.',Be,D,Le,Q,Ft=`<li>MSN (masked siamese networks) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training
objective is to match the prototypes assigned to the unmasked views of the images to that of the masked views of the same images.</li> <li>The authors have only released pre-trained weights of the backbone (ImageNet-1k pre-training). So, to use that on your own image classification dataset,
use the <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> class which is initialized from <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a>. Follow
<a href="https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">this notebook</a> for a detailed tutorial on fine-tuning.</li> <li>MSN is particularly useful in the low-shot and extreme low-shot regimes. Notably, it achieves 75.7% top-1 accuracy with only 1% of ImageNet-1K
labels when fine-tuned.</li>`,Ge,O,Re,K,zt=`PyTorch includes a native scaled dot-product attention (SDPA) operator as part of <code>torch.nn.functional</code>. This function
encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow">official documentation</a>
or the <a href="https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention" rel="nofollow">GPU Inference</a>
page for more information.`,Ee,ee,Wt=`SDPA is used by default for <code>torch&gt;=2.1.1</code> when an implementation is available, but you may also set
<code>attn_implementation=&quot;sdpa&quot;</code> in <code>from_pretrained()</code> to explicitly request SDPA to be used.`,qe,te,Ye,se,Pt="For the best speedups, we recommend loading the model in half-precision (e.g. <code>torch.float16</code> or <code>torch.bfloat16</code>).",Ae,ne,Ht="On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with <code>float32</code> and <code>facebook/vit-msn-base</code> model, we saw the following speedups during inference.",Xe,oe,Bt="<thead><tr><th>Batch size</th> <th>Average inference time (ms), eager mode</th> <th>Average inference time (ms), sdpa model</th> <th>Speed up, Sdpa / Eager (x)</th></tr></thead> <tbody><tr><td>1</td> <td>7</td> <td>6</td> <td>1.17</td></tr> <tr><td>2</td> <td>8</td> <td>6</td> <td>1.33</td></tr> <tr><td>4</td> <td>8</td> <td>6</td> <td>1.33</td></tr> <tr><td>8</td> <td>8</td> <td>6</td> <td>1.33</td></tr></tbody>",De,ae,Qe,re,Lt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViT MSN.",Oe,ie,Ke,le,Gt='<li><a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',et,de,Rt="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",tt,ce,st,C,me,mt,be,Et=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a>. It is used to instantiate an ViT
MSN model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the ViT
<a href="https://huggingface.co/facebook/vit_msn_base" rel="nofollow">facebook/vit_msn_base</a> architecture.`,pt,ve,qt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ft,z,nt,pe,ot,y,fe,ht,we,Yt="The bare Vit Msn Model outputting raw hidden-states without any specific head on top.",gt,ye,At=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ut,$e,Xt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,_t,j,he,Tt,Ce,Dt='The <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> forward method, overrides the <code>__call__</code> special method.',Mt,W,bt,P,at,ge,rt,$,ue,vt,Ve,Qt="The Vit Msn Model with an image classification head on top e.g. for ImageNet.",wt,ke,Ot=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,yt,Ne,Kt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,$t,x,_e,Ct,je,es='The <a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Vt,H,kt,B,it,Te,lt,xe,dt;return G=new L({props:{title:"ViTMSN",local:"vitmsn",headingTag:"h1"}}),R=new L({props:{title:"Overview",local:"overview",headingTag:"h2"}}),D=new L({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),O=new L({props:{title:"Using Scaled Dot Product Attention (SDPA)",local:"using-scaled-dot-product-attention-sdpa",headingTag:"h3"}}),te=new ct({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpVE1TTkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFtb2RlbCUyMCUzRCUyMFZpVE1TTkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGdml0LW1zbi1iYXNlJTIyJTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMnNkcGElMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBLi4u",highlighted:`from transformers import ViTMSNForImageClassification
model = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">ViTMSNForImageClassification</span>.</span></span>from<span class="hljs-constructor">_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-base&quot;</span>, <span class="hljs-params">attn_implementation</span>=<span class="hljs-string">&quot;sdpa&quot;</span>, <span class="hljs-params">dtype</span>=<span class="hljs-params">torch</span>.<span class="hljs-params">float16</span>)</span>
...`,wrap:!1}}),ae=new L({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ie=new ls({props:{pipeline:"image-classification"}}),ce=new L({props:{title:"ViTMSNConfig",local:"transformers.ViTMSNConfig",headingTag:"h2"}}),me=new Se({props:{name:"class transformers.ViTMSNConfig",anchor:"transformers.ViTMSNConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ViTMSNConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ViTMSNConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.ViTMSNConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ViTMSNConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.ViTMSNConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ViTMSNConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ViTMSNConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.ViTMSNConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ViTMSNConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ViTMSNConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.ViTMSNConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.ViTMSNConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ViTMSNConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_msn/configuration_vit_msn.py#L24"}}),z=new Nt({props:{anchor:"transformers.ViTMSNConfig.example",$$slots:{default:[cs]},$$scope:{ctx:N}}}),pe=new L({props:{title:"ViTMSNModel",local:"transformers.ViTMSNModel",headingTag:"h2"}}),fe=new Se({props:{name:"class transformers.ViTMSNModel",anchor:"transformers.ViTMSNModel",parameters:[{name:"config",val:": ViTMSNConfig"},{name:"use_mask_token",val:": bool = False"}],parametersDescription:[{anchor:"transformers.ViTMSNModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.ViTMSNModel.use_mask_token",description:`<strong>use_mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a mask token for masked image modeling.`,name:"use_mask_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_msn/modeling_vit_msn.py#L415"}}),he=new Se({props:{name:"forward",anchor:"transformers.ViTMSNModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.ViTMSNModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.ViTMSNModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.ViTMSNModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMSNModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_msn/modeling_vit_msn.py#L443",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig"
>ViTMSNConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),W=new ts({props:{$$slots:{default:[ms]},$$scope:{ctx:N}}}),P=new Nt({props:{anchor:"transformers.ViTMSNModel.forward.example",$$slots:{default:[ps]},$$scope:{ctx:N}}}),ge=new L({props:{title:"ViTMSNForImageClassification",local:"transformers.ViTMSNForImageClassification",headingTag:"h2"}}),ue=new Se({props:{name:"class transformers.ViTMSNForImageClassification",anchor:"transformers.ViTMSNForImageClassification",parameters:[{name:"config",val:": ViTMSNConfig"}],parametersDescription:[{anchor:"transformers.ViTMSNForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_msn/modeling_vit_msn.py#L499"}}),_e=new Se({props:{name:"forward",anchor:"transformers.ViTMSNForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.ViTMSNForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">ViTImageProcessor.<strong>call</strong>()</a> for details (<code>processor_class</code> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.ViTMSNForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMSNForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.ViTMSNForImageClassification.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/vit_msn/modeling_vit_msn.py#L512",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/vit_msn#transformers.ViTMSNConfig"
>ViTMSNConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new ts({props:{$$slots:{default:[fs]},$$scope:{ctx:N}}}),B=new Nt({props:{anchor:"transformers.ViTMSNForImageClassification.forward.example",$$slots:{default:[hs]},$$scope:{ctx:N}}}),Te=new ds({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit_msn.md"}}),{c(){r=l("meta"),v=o(),c=l("p"),m=o(),b=l("p"),b.innerHTML=i,w=o(),h(G.$$.fragment),Ie=o(),Z=l("div"),Z.innerHTML=jt,Ue=o(),h(R.$$.fragment),Ze=o(),E=l("p"),E.innerHTML=xt,Fe=o(),q=l("p"),q.textContent=St,ze=o(),Y=l("p"),Y.innerHTML=Jt,We=o(),F=l("img"),Pe=o(),A=l("small"),A.innerHTML=Ut,He=o(),X=l("p"),X.innerHTML=Zt,Be=o(),h(D.$$.fragment),Le=o(),Q=l("ul"),Q.innerHTML=Ft,Ge=o(),h(O.$$.fragment),Re=o(),K=l("p"),K.innerHTML=zt,Ee=o(),ee=l("p"),ee.innerHTML=Wt,qe=o(),h(te.$$.fragment),Ye=o(),se=l("p"),se.innerHTML=Pt,Ae=o(),ne=l("p"),ne.innerHTML=Ht,Xe=o(),oe=l("table"),oe.innerHTML=Bt,De=o(),h(ae.$$.fragment),Qe=o(),re=l("p"),re.textContent=Lt,Oe=o(),h(ie.$$.fragment),Ke=o(),le=l("ul"),le.innerHTML=Gt,et=o(),de=l("p"),de.textContent=Rt,tt=o(),h(ce.$$.fragment),st=o(),C=l("div"),h(me.$$.fragment),mt=o(),be=l("p"),be.innerHTML=Et,pt=o(),ve=l("p"),ve.innerHTML=qt,ft=o(),h(z.$$.fragment),nt=o(),h(pe.$$.fragment),ot=o(),y=l("div"),h(fe.$$.fragment),ht=o(),we=l("p"),we.textContent=Yt,gt=o(),ye=l("p"),ye.innerHTML=At,ut=o(),$e=l("p"),$e.innerHTML=Xt,_t=o(),j=l("div"),h(he.$$.fragment),Tt=o(),Ce=l("p"),Ce.innerHTML=Dt,Mt=o(),h(W.$$.fragment),bt=o(),h(P.$$.fragment),at=o(),h(ge.$$.fragment),rt=o(),$=l("div"),h(ue.$$.fragment),vt=o(),Ve=l("p"),Ve.textContent=Qt,wt=o(),ke=l("p"),ke.innerHTML=Ot,yt=o(),Ne=l("p"),Ne.innerHTML=Kt,$t=o(),x=l("div"),h(_e.$$.fragment),Ct=o(),je=l("p"),je.innerHTML=es,Vt=o(),h(H.$$.fragment),kt=o(),h(B.$$.fragment),it=o(),h(Te.$$.fragment),lt=o(),xe=l("p"),this.h()},l(e){const t=is("svelte-u9bgzb",document.head);r=d(t,"META",{name:!0,content:!0}),t.forEach(s),v=a(e),c=d(e,"P",{}),Me(c).forEach(s),m=a(e),b=d(e,"P",{"data-svelte-h":!0}),p(b)!=="svelte-agz8zg"&&(b.innerHTML=i),w=a(e),g(G.$$.fragment,e),Ie=a(e),Z=d(e,"DIV",{class:!0,"data-svelte-h":!0}),p(Z)!=="svelte-b95w5j"&&(Z.innerHTML=jt),Ue=a(e),g(R.$$.fragment,e),Ze=a(e),E=d(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-13cbqc7"&&(E.innerHTML=xt),Fe=a(e),q=d(e,"P",{"data-svelte-h":!0}),p(q)!=="svelte-vfdo9a"&&(q.textContent=St),ze=a(e),Y=d(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-abxoxf"&&(Y.innerHTML=Jt),We=a(e),F=d(e,"IMG",{src:!0,alt:!0,width:!0}),Pe=a(e),A=d(e,"SMALL",{"data-svelte-h":!0}),p(A)!=="svelte-i9jdue"&&(A.innerHTML=Ut),He=a(e),X=d(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-ig50j8"&&(X.innerHTML=Zt),Be=a(e),g(D.$$.fragment,e),Le=a(e),Q=d(e,"UL",{"data-svelte-h":!0}),p(Q)!=="svelte-acnjkv"&&(Q.innerHTML=Ft),Ge=a(e),g(O.$$.fragment,e),Re=a(e),K=d(e,"P",{"data-svelte-h":!0}),p(K)!=="svelte-1cid2pe"&&(K.innerHTML=zt),Ee=a(e),ee=d(e,"P",{"data-svelte-h":!0}),p(ee)!=="svelte-1x11lxg"&&(ee.innerHTML=Wt),qe=a(e),g(te.$$.fragment,e),Ye=a(e),se=d(e,"P",{"data-svelte-h":!0}),p(se)!=="svelte-djb2w0"&&(se.innerHTML=Pt),Ae=a(e),ne=d(e,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-r19fu6"&&(ne.innerHTML=Ht),Xe=a(e),oe=d(e,"TABLE",{"data-svelte-h":!0}),p(oe)!=="svelte-vyu660"&&(oe.innerHTML=Bt),De=a(e),g(ae.$$.fragment,e),Qe=a(e),re=d(e,"P",{"data-svelte-h":!0}),p(re)!=="svelte-s3w6c0"&&(re.textContent=Lt),Oe=a(e),g(ie.$$.fragment,e),Ke=a(e),le=d(e,"UL",{"data-svelte-h":!0}),p(le)!=="svelte-1vk2x0h"&&(le.innerHTML=Gt),et=a(e),de=d(e,"P",{"data-svelte-h":!0}),p(de)!=="svelte-1xesile"&&(de.textContent=Rt),tt=a(e),g(ce.$$.fragment,e),st=a(e),C=d(e,"DIV",{class:!0});var S=Me(C);g(me.$$.fragment,S),mt=a(S),be=d(S,"P",{"data-svelte-h":!0}),p(be)!=="svelte-11p0qhf"&&(be.innerHTML=Et),pt=a(S),ve=d(S,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1ek1ss9"&&(ve.innerHTML=qt),ft=a(S),g(z.$$.fragment,S),S.forEach(s),nt=a(e),g(pe.$$.fragment,e),ot=a(e),y=d(e,"DIV",{class:!0});var V=Me(y);g(fe.$$.fragment,V),ht=a(V),we=d(V,"P",{"data-svelte-h":!0}),p(we)!=="svelte-1be5hlp"&&(we.textContent=Yt),gt=a(V),ye=d(V,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-q52n56"&&(ye.innerHTML=At),ut=a(V),$e=d(V,"P",{"data-svelte-h":!0}),p($e)!=="svelte-hswkmf"&&($e.innerHTML=Xt),_t=a(V),j=d(V,"DIV",{class:!0});var J=Me(j);g(he.$$.fragment,J),Tt=a(J),Ce=d(J,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-mf0i6l"&&(Ce.innerHTML=Dt),Mt=a(J),g(W.$$.fragment,J),bt=a(J),g(P.$$.fragment,J),J.forEach(s),V.forEach(s),at=a(e),g(ge.$$.fragment,e),rt=a(e),$=d(e,"DIV",{class:!0});var k=Me($);g(ue.$$.fragment,k),vt=a(k),Ve=d(k,"P",{"data-svelte-h":!0}),p(Ve)!=="svelte-1495rr6"&&(Ve.textContent=Qt),wt=a(k),ke=d(k,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-q52n56"&&(ke.innerHTML=Ot),yt=a(k),Ne=d(k,"P",{"data-svelte-h":!0}),p(Ne)!=="svelte-hswkmf"&&(Ne.innerHTML=Kt),$t=a(k),x=d(k,"DIV",{class:!0});var I=Me(x);g(_e.$$.fragment,I),Ct=a(I),je=d(I,"P",{"data-svelte-h":!0}),p(je)!=="svelte-1pj4at7"&&(je.innerHTML=es),Vt=a(I),g(H.$$.fragment,I),kt=a(I),g(B.$$.fragment,I),I.forEach(s),k.forEach(s),it=a(e),g(Te.$$.fragment,e),lt=a(e),xe=d(e,"P",{}),Me(xe).forEach(s),this.h()},h(){U(r,"name","hf:doc:metadata"),U(r,"content",us),U(Z,"class","flex flex-wrap space-x-1"),ns(F.src,It="https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png")||U(F,"src",It),U(F,"alt","drawing"),U(F,"width","600"),U(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),U($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){f(document.head,r),n(e,v,t),n(e,c,t),n(e,m,t),n(e,b,t),n(e,w,t),u(G,e,t),n(e,Ie,t),n(e,Z,t),n(e,Ue,t),u(R,e,t),n(e,Ze,t),n(e,E,t),n(e,Fe,t),n(e,q,t),n(e,ze,t),n(e,Y,t),n(e,We,t),n(e,F,t),n(e,Pe,t),n(e,A,t),n(e,He,t),n(e,X,t),n(e,Be,t),u(D,e,t),n(e,Le,t),n(e,Q,t),n(e,Ge,t),u(O,e,t),n(e,Re,t),n(e,K,t),n(e,Ee,t),n(e,ee,t),n(e,qe,t),u(te,e,t),n(e,Ye,t),n(e,se,t),n(e,Ae,t),n(e,ne,t),n(e,Xe,t),n(e,oe,t),n(e,De,t),u(ae,e,t),n(e,Qe,t),n(e,re,t),n(e,Oe,t),u(ie,e,t),n(e,Ke,t),n(e,le,t),n(e,et,t),n(e,de,t),n(e,tt,t),u(ce,e,t),n(e,st,t),n(e,C,t),u(me,C,null),f(C,mt),f(C,be),f(C,pt),f(C,ve),f(C,ft),u(z,C,null),n(e,nt,t),u(pe,e,t),n(e,ot,t),n(e,y,t),u(fe,y,null),f(y,ht),f(y,we),f(y,gt),f(y,ye),f(y,ut),f(y,$e),f(y,_t),f(y,j),u(he,j,null),f(j,Tt),f(j,Ce),f(j,Mt),u(W,j,null),f(j,bt),u(P,j,null),n(e,at,t),u(ge,e,t),n(e,rt,t),n(e,$,t),u(ue,$,null),f($,vt),f($,Ve),f($,wt),f($,ke),f($,yt),f($,Ne),f($,$t),f($,x),u(_e,x,null),f(x,Ct),f(x,je),f(x,Vt),u(H,x,null),f(x,kt),u(B,x,null),n(e,it,t),u(Te,e,t),n(e,lt,t),n(e,xe,t),dt=!0},p(e,[t]){const S={};t&2&&(S.$$scope={dirty:t,ctx:e}),z.$set(S);const V={};t&2&&(V.$$scope={dirty:t,ctx:e}),W.$set(V);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),P.$set(J);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),H.$set(k);const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),B.$set(I)},i(e){dt||(_(G.$$.fragment,e),_(R.$$.fragment,e),_(D.$$.fragment,e),_(O.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(z.$$.fragment,e),_(pe.$$.fragment,e),_(fe.$$.fragment,e),_(he.$$.fragment,e),_(W.$$.fragment,e),_(P.$$.fragment,e),_(ge.$$.fragment,e),_(ue.$$.fragment,e),_(_e.$$.fragment,e),_(H.$$.fragment,e),_(B.$$.fragment,e),_(Te.$$.fragment,e),dt=!0)},o(e){T(G.$$.fragment,e),T(R.$$.fragment,e),T(D.$$.fragment,e),T(O.$$.fragment,e),T(te.$$.fragment,e),T(ae.$$.fragment,e),T(ie.$$.fragment,e),T(ce.$$.fragment,e),T(me.$$.fragment,e),T(z.$$.fragment,e),T(pe.$$.fragment,e),T(fe.$$.fragment,e),T(he.$$.fragment,e),T(W.$$.fragment,e),T(P.$$.fragment,e),T(ge.$$.fragment,e),T(ue.$$.fragment,e),T(_e.$$.fragment,e),T(H.$$.fragment,e),T(B.$$.fragment,e),T(Te.$$.fragment,e),dt=!1},d(e){e&&(s(v),s(c),s(m),s(b),s(w),s(Ie),s(Z),s(Ue),s(Ze),s(E),s(Fe),s(q),s(ze),s(Y),s(We),s(F),s(Pe),s(A),s(He),s(X),s(Be),s(Le),s(Q),s(Ge),s(Re),s(K),s(Ee),s(ee),s(qe),s(Ye),s(se),s(Ae),s(ne),s(Xe),s(oe),s(De),s(Qe),s(re),s(Oe),s(Ke),s(le),s(et),s(de),s(tt),s(st),s(C),s(nt),s(ot),s(y),s(at),s(rt),s($),s(it),s(lt),s(xe)),s(r),M(G,e),M(R,e),M(D,e),M(O,e),M(te,e),M(ae,e),M(ie,e),M(ce,e),M(me),M(z),M(pe,e),M(fe),M(he),M(W),M(P),M(ge,e),M(ue),M(_e),M(H),M(B),M(Te,e)}}}const us='{"title":"ViTMSN","local":"vitmsn","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[{"title":"Using Scaled Dot Product Attention (SDPA)","local":"using-scaled-dot-product-attention-sdpa","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ViTMSNConfig","local":"transformers.ViTMSNConfig","sections":[],"depth":2},{"title":"ViTMSNModel","local":"transformers.ViTMSNModel","sections":[],"depth":2},{"title":"ViTMSNForImageClassification","local":"transformers.ViTMSNForImageClassification","sections":[],"depth":2}],"depth":1}';function _s(N){return os(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vs extends as{constructor(r){super(),rs(this,r,_s,gs,ss,{})}}export{Vs as component};
