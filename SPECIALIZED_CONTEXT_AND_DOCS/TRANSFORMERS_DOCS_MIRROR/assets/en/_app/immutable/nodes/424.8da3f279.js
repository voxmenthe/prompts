import{s as en,z as on,o as tn,n as Ao}from"../chunks/scheduler.18a86fab.js";import{S as nn,i as sn,g as i,s as n,r as m,A as rn,h as l,f as t,c as s,j as U,x as c,u as f,k as M,y as a,a as r,v as g,d as h,t as u,w as T}from"../chunks/index.98837b22.js";import{T as Kt}from"../chunks/Tip.77304350.js";import{D as x}from"../chunks/Docstring.a1ef7999.js";import{C as Tt}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as Ot}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as F,E as an}from"../chunks/getInferenceSnippets.06c2775f.js";function ln($){let d,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=b},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=b)},m(_,y){r(_,d,y)},p:Ao,d(_){_&&t(d)}}}function dn($){let d,b="Examples:",_,y,v;return y=new Tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0NvbmZpZyUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBUdnBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwVHZwTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkppcWluZyUyRnRpbnktcmFuZG9tLXR2cCUyMiklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJKaXFpbmclMkZ0aW55LXJhbmRvbS10dnAlMjIpJTBBJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwdG9yY2gucmFuZCgxJTJDJTIwMSUyQyUyMDMlMkMlMjA0NDglMkMlMjA0NDgpJTBBdGV4dF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhpcyUyMGlzJTIwYW4lMjBleGFtcGxlJTIwaW5wdXQlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsKHRleHRfaW5wdXRzLmlucHV0X2lkcyUyQyUyMHBpeGVsX3ZhbHVlcyUyQyUyMHRleHRfaW5wdXRzLmF0dGVudGlvbl9tYXNrKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoTokenizer, TvpModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TvpModel.from_pretrained(<span class="hljs-string">&quot;Jiqing/tiny-random-tvp&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Jiqing/tiny-random-tvp&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_inputs = tokenizer(<span class="hljs-string">&quot;This is an example input&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)`,wrap:!1}}),{c(){d=i("p"),d.textContent=b,_=n(),m(y.$$.fragment)},l(p){d=l(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-kvfsh7"&&(d.textContent=b),_=s(p),f(y.$$.fragment,p)},m(p,k){r(p,d,k),r(p,_,k),g(y,p,k),v=!0},p:Ao,i(p){v||(h(y.$$.fragment,p),v=!0)},o(p){u(y.$$.fragment,p),v=!1},d(p){p&&(t(d),t(_)),T(y,p)}}}function cn($){let d,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=b},l(_){d=l(_,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=b)},m(_,y){r(_,d,y)},p:Ao,d(_){_&&t(d)}}}function pn($){let d,b="Examples:",_,y,v;return y=new Tt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0NvbmZpZyUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBUdnBGb3JWaWRlb0dyb3VuZGluZyUwQSUwQW1vZGVsJTIwJTNEJTIwVHZwRm9yVmlkZW9Hcm91bmRpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMkppcWluZyUyRnRpbnktcmFuZG9tLXR2cCUyMiklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJKaXFpbmclMkZ0aW55LXJhbmRvbS10dnAlMjIpJTBBJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwdG9yY2gucmFuZCgxJTJDJTIwMSUyQyUyMDMlMkMlMjA0NDglMkMlMjA0NDgpJTBBdGV4dF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhpcyUyMGlzJTIwYW4lMjBleGFtcGxlJTIwaW5wdXQlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsKHRleHRfaW5wdXRzLmlucHV0X2lkcyUyQyUyMHBpeGVsX3ZhbHVlcyUyQyUyMHRleHRfaW5wdXRzLmF0dGVudGlvbl9tYXNrKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoTokenizer, TvpForVideoGrounding

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TvpForVideoGrounding.from_pretrained(<span class="hljs-string">&quot;Jiqing/tiny-random-tvp&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Jiqing/tiny-random-tvp&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">448</span>, <span class="hljs-number">448</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_inputs = tokenizer(<span class="hljs-string">&quot;This is an example input&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)`,wrap:!1}}),{c(){d=i("p"),d.textContent=b,_=n(),m(y.$$.fragment)},l(p){d=l(p,"P",{"data-svelte-h":!0}),c(d)!=="svelte-kvfsh7"&&(d.textContent=b),_=s(p),f(y.$$.fragment,p)},m(p,k){r(p,d,k),r(p,_,k),g(y,p,k),v=!0},p:Ao,i(p){v||(h(y.$$.fragment,p),v=!0)},o(p){u(y.$$.fragment,p),v=!1},d(p){p&&(t(d),t(_)),T(y,p)}}}function mn($){let d,b,_,y,v,p="<em>This model was released on 2023-03-09 and added to Hugging Face Transformers on 2023-11-22.</em>",k,O,ro,R,_t='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',ao,ee,io,oe,yt='The text-visual prompting (TVP) framework was proposed in the paper <a href="https://huggingface.co/papers/2303.04995" rel="nofollow">Text-Visual Prompting for Efficient 2D Temporal Video Grounding</a> by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.',lo,te,Mt="The abstract from the paper is the following:",co,ne,vt="<em>In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call ‘prompts’) into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of cross-modal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA and ActivityNet Captions datasets, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves 5× inference acceleration over TVG using 3D visual features.</em>",po,se,bt="This research addresses temporal video grounding (TVG), which is the process of pinpointing the start and end times of specific events in a long video, as described by a text sentence. Text-visual prompting (TVP), is proposed to enhance TVG. TVP involves integrating specially designed patterns, known as ‘prompts’, into both the visual (image-based) and textual (word-based) input components of a TVG model. These prompts provide additional spatial-temporal context, improving the model’s ability to accurately determine event timings in the video. The approach employs 2D visual inputs in place of 3D ones. Although 3D inputs offer more spatial-temporal detail, they are also more time-consuming to process. The use of 2D inputs with the prompting method aims to provide similar levels of context and accuracy more efficiently.",mo,A,wt,fo,re,It='TVP architecture. Taken from the <a href="https://huggingface.co/papers/2303.04995">original paper.</a>',go,ae,Jt='This model was contributed by <a href="https://huggingface.co/Jiqing" rel="nofollow">Jiqing Feng</a>. The original code can be found <a href="https://github.com/intel/TVP" rel="nofollow">here</a>.',ho,ie,uo,le,Ut="Prompts are optimized perturbation patterns, which would be added to input video frames or text features. Universal set refers to using the same exact set of prompts for any input, this means that these prompts are added consistently to all video frames and text features, regardless of the input’s content.",To,de,jt="TVP consists of a visual encoder and cross-modal encoder. A universal set of visual prompts and text prompts to be integrated into sampled video frames and textual features, respectively. Specially, a set of different visual prompts are applied to uniformly-sampled frames of one untrimmed video in order.",_o,ce,Ct=`The goal of this model is to incorporate trainable prompts into both visual inputs and textual features to temporal video grounding(TVG) problems.
In principle, one can apply any visual, cross-modal encoder in the proposed architecture.`,yo,pe,xt=`The <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> wraps <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> into a single instance to both
encode the text and prepare the images respectively.`,Mo,me,kt='The following example shows how to run temporal video grounding using <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpForVideoGrounding">TvpForVideoGrounding</a>.',vo,fe,bo,ge,Bt="Tips:",wo,he,Zt='<li>This implementation of TVP uses <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> to generate text embeddings and Resnet-50 model to compute visual embeddings.</li> <li>Checkpoints for pre-trained <a href="https://huggingface.co/Intel/tvp-base" rel="nofollow">tvp-base</a> is released.</li> <li>Please refer to <a href="https://huggingface.co/papers/2303.04995" rel="nofollow">Table 2</a> for TVP’s performance on Temporal Video Grounding task.</li>',Io,ue,Jo,w,Te,No,We,zt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a>. It is used to instantiate an Tvp
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Tvp
<a href="https://huggingface.co/Intel/tvp-base" rel="nofollow">Intel/tvp-base</a> architecture.`,Xo,Fe,Gt=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ho,N,_e,Eo,Re,$t='Instantiate a <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpConfig">TvpConfig</a> (or a derived class) from a pre-trained backbone model configuration.',Qo,X,ye,So,Ae,Pt='Serializes this instance to a Python dictionary. Override the default <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict">to_dict()</a>.',Uo,Me,jo,z,ve,Lo,Ne,Vt="Constructs a Tvp image processor.",qo,H,be,Yo,Xe,Wt="Preprocess an image or batch of images.",Co,we,xo,G,Ie,Do,He,Ft="Constructs a fast Tvp image processor.",Ko,Ee,Je,ko,Ue,Bo,j,je,Oo,Qe,Rt="Constructs an TVP processor which wraps a TVP image processor and a Bert tokenizer into a single processor.",et,Se,At=`<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>. See the
<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor.__call__"><strong>call</strong>()</a> and <a href="/docs/transformers/v4.56.2/en/main_classes/processors#transformers.ProcessorMixin.decode">decode()</a> for more information.`,ot,E,Ce,tt,Le,Nt=`Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the <code>text</code>
and <code>kwargs</code> arguments to BertTokenizerFast’s <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a> if <code>text</code> is not <code>None</code> to encode
the text. To prepare the image(s), this method forwards the <code>videos</code> and <code>kwargs</code> arguments to
TvpImageProcessor’s <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__"><strong>call</strong>()</a> if <code>videos</code> is not <code>None</code>. Please refer to the docstring of
the above two methods for more information.`,Zo,xe,zo,I,ke,nt,qe,Xt="The bare Tvp Model transformer outputting BaseModelOutputWithPooling object without any specific head on top.",st,Ye,Ht=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,rt,De,Et=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,at,B,Be,it,Ke,Qt='The <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a> forward method, overrides the <code>__call__</code> special method.',lt,Q,dt,S,Go,Ze,$o,J,ze,ct,Oe,St="Tvp Model with a video grounding head on top computing IoU, distance, and duration loss.",pt,eo,Lt=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mt,oo,qt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ft,Z,Ge,gt,to,Yt='The <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpForVideoGrounding">TvpForVideoGrounding</a> forward method, overrides the <code>__call__</code> special method.',ht,L,ut,q,Po,$e,Vo,so,Wo;return O=new F({props:{title:"TVP",local:"tvp",headingTag:"h1"}}),ee=new F({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ie=new F({props:{title:"Usage tips and examples",local:"usage-tips-and-examples",headingTag:"h2"}}),fe=new Tt({props:{code:"import%20av%0Aimport%20cv2%0Aimport%20numpy%20as%20np%0Aimport%20torch%0Afrom%20huggingface_hub%20import%20hf_hub_download%0Afrom%20transformers%20import%20AutoProcessor%2C%20TvpForVideoGrounding%0A%0A%0Adef%20pyav_decode(container%2C%20sampling_rate%2C%20num_frames%2C%20clip_idx%2C%20num_clips%2C%20target_fps)%3A%0A%20%20%20%20'''%0A%20%20%20%20Convert%20the%20video%20from%20its%20original%20fps%20to%20the%20target_fps%20and%20decode%20the%20video%20with%20PyAV%20decoder.%0A%20%20%20%20Args%3A%0A%20%20%20%20%20%20%20%20container%20(container)%3A%20pyav%20container.%0A%20%20%20%20%20%20%20%20sampling_rate%20(int)%3A%20frame%20sampling%20rate%20(interval%20between%20two%20sampled%20frames).%0A%20%20%20%20%20%20%20%20num_frames%20(int)%3A%20number%20of%20frames%20to%20sample.%0A%20%20%20%20%20%20%20%20clip_idx%20(int)%3A%20if%20clip_idx%20is%20-1%2C%20perform%20random%20temporal%20sampling.%0A%20%20%20%20%20%20%20%20%20%20%20%20If%20clip_idx%20is%20larger%20than%20-1%2C%20uniformly%20split%20the%20video%20to%20num_clips%0A%20%20%20%20%20%20%20%20%20%20%20%20clips%2C%20and%20select%20the%20clip_idx-th%20video%20clip.%0A%20%20%20%20%20%20%20%20num_clips%20(int)%3A%20overall%20number%20of%20clips%20to%20uniformly%20sample%20from%20the%20given%20video.%0A%20%20%20%20%20%20%20%20target_fps%20(int)%3A%20the%20input%20video%20may%20have%20different%20fps%2C%20convert%20it%20to%0A%20%20%20%20%20%20%20%20%20%20%20%20the%20target%20video%20fps%20before%20frame%20sampling.%0A%20%20%20%20Returns%3A%0A%20%20%20%20%20%20%20%20frames%20(tensor)%3A%20decoded%20frames%20from%20the%20video.%20Return%20None%20if%20the%20no%0A%20%20%20%20%20%20%20%20%20%20%20%20video%20stream%20was%20found.%0A%20%20%20%20%20%20%20%20fps%20(float)%3A%20the%20number%20of%20frames%20per%20second%20of%20the%20video.%0A%20%20%20%20'''%0A%20%20%20%20video%20%3D%20container.streams.video%5B0%5D%0A%20%20%20%20fps%20%3D%20float(video.average_rate)%0A%20%20%20%20clip_size%20%3D%20sampling_rate%20*%20num_frames%20%2F%20target_fps%20*%20fps%0A%20%20%20%20delta%20%3D%20max(num_frames%20-%20clip_size%2C%200)%0A%20%20%20%20start_idx%20%3D%20delta%20*%20clip_idx%20%2F%20num_clips%0A%20%20%20%20end_idx%20%3D%20start_idx%20%2B%20clip_size%20-%201%0A%20%20%20%20timebase%20%3D%20video.duration%20%2F%20num_frames%0A%20%20%20%20video_start_pts%20%3D%20int(start_idx%20*%20timebase)%0A%20%20%20%20video_end_pts%20%3D%20int(end_idx%20*%20timebase)%0A%20%20%20%20seek_offset%20%3D%20max(video_start_pts%20-%201024%2C%200)%0A%20%20%20%20container.seek(seek_offset%2C%20any_frame%3DFalse%2C%20backward%3DTrue%2C%20stream%3Dvideo)%0A%20%20%20%20frames%20%3D%20%7B%7D%0A%20%20%20%20for%20frame%20in%20container.decode(video%3D0)%3A%0A%20%20%20%20%20%20%20%20if%20frame.pts%20%3C%20video_start_pts%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20continue%0A%20%20%20%20%20%20%20%20frames%5Bframe.pts%5D%20%3D%20frame%0A%20%20%20%20%20%20%20%20if%20frame.pts%20%3E%20video_end_pts%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20break%0A%20%20%20%20frames%20%3D%20%5Bframes%5Bpts%5D%20for%20pts%20in%20sorted(frames)%5D%0A%20%20%20%20return%20frames%2C%20fps%0A%0A%0Adef%20decode(container%2C%20sampling_rate%2C%20num_frames%2C%20clip_idx%2C%20num_clips%2C%20target_fps)%3A%0A%20%20%20%20'''%0A%20%20%20%20Decode%20the%20video%20and%20perform%20temporal%20sampling.%0A%20%20%20%20Args%3A%0A%20%20%20%20%20%20%20%20container%20(container)%3A%20pyav%20container.%0A%20%20%20%20%20%20%20%20sampling_rate%20(int)%3A%20frame%20sampling%20rate%20(interval%20between%20two%20sampled%20frames).%0A%20%20%20%20%20%20%20%20num_frames%20(int)%3A%20number%20of%20frames%20to%20sample.%0A%20%20%20%20%20%20%20%20clip_idx%20(int)%3A%20if%20clip_idx%20is%20-1%2C%20perform%20random%20temporal%20sampling.%0A%20%20%20%20%20%20%20%20%20%20%20%20If%20clip_idx%20is%20larger%20than%20-1%2C%20uniformly%20split%20the%20video%20to%20num_clips%0A%20%20%20%20%20%20%20%20%20%20%20%20clips%2C%20and%20select%20the%20clip_idx-th%20video%20clip.%0A%20%20%20%20%20%20%20%20num_clips%20(int)%3A%20overall%20number%20of%20clips%20to%20uniformly%20sample%20from%20the%20given%20video.%0A%20%20%20%20%20%20%20%20target_fps%20(int)%3A%20the%20input%20video%20may%20have%20different%20fps%2C%20convert%20it%20to%0A%20%20%20%20%20%20%20%20%20%20%20%20the%20target%20video%20fps%20before%20frame%20sampling.%0A%20%20%20%20Returns%3A%0A%20%20%20%20%20%20%20%20frames%20(tensor)%3A%20decoded%20frames%20from%20the%20video.%0A%20%20%20%20'''%0A%20%20%20%20assert%20clip_idx%20%3E%3D%20-2%2C%20%22Not%20a%20valid%20clip_idx%20%7B%7D%22.format(clip_idx)%0A%20%20%20%20frames%2C%20fps%20%3D%20pyav_decode(container%2C%20sampling_rate%2C%20num_frames%2C%20clip_idx%2C%20num_clips%2C%20target_fps)%0A%20%20%20%20clip_size%20%3D%20sampling_rate%20*%20num_frames%20%2F%20target_fps%20*%20fps%0A%20%20%20%20index%20%3D%20np.linspace(0%2C%20clip_size%20-%201%2C%20num_frames)%0A%20%20%20%20index%20%3D%20np.clip(index%2C%200%2C%20len(frames)%20-%201).astype(np.int64)%0A%20%20%20%20frames%20%3D%20np.array(%5Bframes%5Bidx%5D.to_rgb().to_ndarray()%20for%20idx%20in%20index%5D)%0A%20%20%20%20frames%20%3D%20frames.transpose(0%2C%203%2C%201%2C%202)%0A%20%20%20%20return%20frames%0A%0A%0Afile%20%3D%20hf_hub_download(repo_id%3D%22Intel%2Ftvp_demo%22%2C%20filename%3D%22AK2KG.mp4%22%2C%20repo_type%3D%22dataset%22)%0Amodel%20%3D%20TvpForVideoGrounding.from_pretrained(%22Intel%2Ftvp-base%22)%0A%0Adecoder_kwargs%20%3D%20dict(%0A%20%20%20%20container%3Dav.open(file%2C%20metadata_errors%3D%22ignore%22)%2C%0A%20%20%20%20sampling_rate%3D1%2C%0A%20%20%20%20num_frames%3Dmodel.config.num_frames%2C%0A%20%20%20%20clip_idx%3D0%2C%0A%20%20%20%20num_clips%3D1%2C%0A%20%20%20%20target_fps%3D3%2C%0A)%0Araw_sampled_frms%20%3D%20decode(**decoder_kwargs)%0A%0Atext%20%3D%20%22a%20person%20is%20sitting%20on%20a%20bed.%22%0Aprocessor%20%3D%20AutoProcessor.from_pretrained(%22Intel%2Ftvp-base%22)%0Amodel_inputs%20%3D%20processor(%0A%20%20%20%20text%3D%5Btext%5D%2C%20videos%3Dlist(raw_sampled_frms)%2C%20return_tensors%3D%22pt%22%2C%20max_text_length%3D100%23%2C%20size%3Dsize%0A)%0A%0Amodel_inputs%5B%22pixel_values%22%5D%20%3D%20model_inputs%5B%22pixel_values%22%5D.to(model.dtype)%0Aoutput%20%3D%20model(**model_inputs)%0A%0Adef%20get_video_duration(filename)%3A%0A%20%20%20%20cap%20%3D%20cv2.VideoCapture(filename)%0A%20%20%20%20if%20cap.isOpened()%3A%0A%20%20%20%20%20%20%20%20rate%20%3D%20cap.get(5)%0A%20%20%20%20%20%20%20%20frame_num%20%3D%20cap.get(7)%0A%20%20%20%20%20%20%20%20duration%20%3D%20frame_num%2Frate%0A%20%20%20%20%20%20%20%20return%20duration%0A%20%20%20%20return%20-1%0A%0Aduration%20%3D%20get_video_duration(file)%0Astart%2C%20end%20%3D%20processor.post_process_video_grounding(output.logits%2C%20duration)%0A%0Aprint(f%22The%20time%20slot%20of%20the%20video%20corresponding%20to%20the%20text%20%5C%22%7Btext%7D%5C%22%20is%20from%20%7Bstart%7Ds%20to%20%7Bend%7Ds%22)",highlighted:`<span class="hljs-keyword">import</span> av
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, TvpForVideoGrounding


<span class="hljs-keyword">def</span> <span class="hljs-title function_">pyav_decode</span>(<span class="hljs-params">container, sampling_rate, num_frames, clip_idx, num_clips, target_fps</span>):
    <span class="hljs-string">&#x27;&#x27;&#x27;
    Convert the video from its original fps to the target_fps and decode the video with PyAV decoder.
    Args:
        container (container): pyav container.
        sampling_rate (int): frame sampling rate (interval between two sampled frames).
        num_frames (int): number of frames to sample.
        clip_idx (int): if clip_idx is -1, perform random temporal sampling.
            If clip_idx is larger than -1, uniformly split the video to num_clips
            clips, and select the clip_idx-th video clip.
        num_clips (int): overall number of clips to uniformly sample from the given video.
        target_fps (int): the input video may have different fps, convert it to
            the target video fps before frame sampling.
    Returns:
        frames (tensor): decoded frames from the video. Return None if the no
            video stream was found.
        fps (float): the number of frames per second of the video.
    &#x27;&#x27;&#x27;</span>
    video = container.streams.video[<span class="hljs-number">0</span>]
    fps = <span class="hljs-built_in">float</span>(video.average_rate)
    clip_size = sampling_rate * num_frames / target_fps * fps
    delta = <span class="hljs-built_in">max</span>(num_frames - clip_size, <span class="hljs-number">0</span>)
    start_idx = delta * clip_idx / num_clips
    end_idx = start_idx + clip_size - <span class="hljs-number">1</span>
    timebase = video.duration / num_frames
    video_start_pts = <span class="hljs-built_in">int</span>(start_idx * timebase)
    video_end_pts = <span class="hljs-built_in">int</span>(end_idx * timebase)
    seek_offset = <span class="hljs-built_in">max</span>(video_start_pts - <span class="hljs-number">1024</span>, <span class="hljs-number">0</span>)
    container.seek(seek_offset, any_frame=<span class="hljs-literal">False</span>, backward=<span class="hljs-literal">True</span>, stream=video)
    frames = {}
    <span class="hljs-keyword">for</span> frame <span class="hljs-keyword">in</span> container.decode(video=<span class="hljs-number">0</span>):
        <span class="hljs-keyword">if</span> frame.pts &lt; video_start_pts:
            <span class="hljs-keyword">continue</span>
        frames[frame.pts] = frame
        <span class="hljs-keyword">if</span> frame.pts &gt; video_end_pts:
            <span class="hljs-keyword">break</span>
    frames = [frames[pts] <span class="hljs-keyword">for</span> pts <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(frames)]
    <span class="hljs-keyword">return</span> frames, fps


<span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">container, sampling_rate, num_frames, clip_idx, num_clips, target_fps</span>):
    <span class="hljs-string">&#x27;&#x27;&#x27;
    Decode the video and perform temporal sampling.
    Args:
        container (container): pyav container.
        sampling_rate (int): frame sampling rate (interval between two sampled frames).
        num_frames (int): number of frames to sample.
        clip_idx (int): if clip_idx is -1, perform random temporal sampling.
            If clip_idx is larger than -1, uniformly split the video to num_clips
            clips, and select the clip_idx-th video clip.
        num_clips (int): overall number of clips to uniformly sample from the given video.
        target_fps (int): the input video may have different fps, convert it to
            the target video fps before frame sampling.
    Returns:
        frames (tensor): decoded frames from the video.
    &#x27;&#x27;&#x27;</span>
    <span class="hljs-keyword">assert</span> clip_idx &gt;= -<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Not a valid clip_idx {}&quot;</span>.<span class="hljs-built_in">format</span>(clip_idx)
    frames, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)
    clip_size = sampling_rate * num_frames / target_fps * fps
    index = np.linspace(<span class="hljs-number">0</span>, clip_size - <span class="hljs-number">1</span>, num_frames)
    index = np.clip(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(frames) - <span class="hljs-number">1</span>).astype(np.int64)
    frames = np.array([frames[idx].to_rgb().to_ndarray() <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> index])
    frames = frames.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
    <span class="hljs-keyword">return</span> frames


file = hf_hub_download(repo_id=<span class="hljs-string">&quot;Intel/tvp_demo&quot;</span>, filename=<span class="hljs-string">&quot;AK2KG.mp4&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)
model = TvpForVideoGrounding.from_pretrained(<span class="hljs-string">&quot;Intel/tvp-base&quot;</span>)

decoder_kwargs = <span class="hljs-built_in">dict</span>(
    container=av.<span class="hljs-built_in">open</span>(file, metadata_errors=<span class="hljs-string">&quot;ignore&quot;</span>),
    sampling_rate=<span class="hljs-number">1</span>,
    num_frames=model.config.num_frames,
    clip_idx=<span class="hljs-number">0</span>,
    num_clips=<span class="hljs-number">1</span>,
    target_fps=<span class="hljs-number">3</span>,
)
raw_sampled_frms = decode(**decoder_kwargs)

text = <span class="hljs-string">&quot;a person is sitting on a bed.&quot;</span>
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Intel/tvp-base&quot;</span>)
model_inputs = processor(
    text=[text], videos=<span class="hljs-built_in">list</span>(raw_sampled_frms), return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, max_text_length=<span class="hljs-number">100</span><span class="hljs-comment">#, size=size</span>
)

model_inputs[<span class="hljs-string">&quot;pixel_values&quot;</span>] = model_inputs[<span class="hljs-string">&quot;pixel_values&quot;</span>].to(model.dtype)
output = model(**model_inputs)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_video_duration</span>(<span class="hljs-params">filename</span>):
    cap = cv2.VideoCapture(filename)
    <span class="hljs-keyword">if</span> cap.isOpened():
        rate = cap.get(<span class="hljs-number">5</span>)
        frame_num = cap.get(<span class="hljs-number">7</span>)
        duration = frame_num/rate
        <span class="hljs-keyword">return</span> duration
    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>

duration = get_video_duration(file)
start, end = processor.post_process_video_grounding(output.logits, duration)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;The time slot of the video corresponding to the text \\&quot;<span class="hljs-subst">{text}</span>\\&quot; is from <span class="hljs-subst">{start}</span>s to <span class="hljs-subst">{end}</span>s&quot;</span>)`,wrap:!1}}),ue=new F({props:{title:"TvpConfig",local:"transformers.TvpConfig",headingTag:"h2"}}),Te=new x({props:{name:"class transformers.TvpConfig",anchor:"transformers.TvpConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"distance_loss_weight",val:" = 1.0"},{name:"duration_loss_weight",val:" = 0.1"},{name:"visual_prompter_type",val:" = 'framepad'"},{name:"visual_prompter_apply",val:" = 'replace'"},{name:"visual_prompt_size",val:" = 96"},{name:"max_img_size",val:" = 448"},{name:"num_frames",val:" = 48"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"max_position_embeddings",val:" = 512"},{name:"max_grid_col_position_embeddings",val:" = 100"},{name:"max_grid_row_position_embeddings",val:" = 100"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"initializer_range",val:" = 0.02"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TvpConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>) &#x2014;
The configuration of the backbone model.`,name:"backbone_config"},{anchor:"transformers.TvpConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.TvpConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.TvpConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to load <code>backbone</code> from the timm library. If <code>False</code>, the backbone is loaded from the transformers
library.`,name:"use_timm_backbone"},{anchor:"transformers.TvpConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.TvpConfig.distance_loss_weight",description:`<strong>distance_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The weight of distance loss.`,name:"distance_loss_weight"},{anchor:"transformers.TvpConfig.duration_loss_weight",description:`<strong>duration_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The weight of duration loss.`,name:"duration_loss_weight"},{anchor:"transformers.TvpConfig.visual_prompter_type",description:`<strong>visual_prompter_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;framepad&quot;</code>) &#x2014;
Visual prompt type. The type of padding. Framepad means padding on each frame. Should be one of &#x201C;framepad&#x201D;
or &#x201C;framedownpad&#x201D;`,name:"visual_prompter_type"},{anchor:"transformers.TvpConfig.visual_prompter_apply",description:`<strong>visual_prompter_apply</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;replace&quot;</code>) &#x2014;
The way of applying visual prompt. Replace means use the value of prompt to change the original value in
visual inputs. Should be one of &#x201C;replace&#x201D;, or &#x201C;add&#x201D;, or &#x201C;remove&#x201D;.`,name:"visual_prompter_apply"},{anchor:"transformers.TvpConfig.visual_prompt_size",description:`<strong>visual_prompt_size</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
The size of visual prompt.`,name:"visual_prompt_size"},{anchor:"transformers.TvpConfig.max_img_size",description:`<strong>max_img_size</strong> (<code>int</code>, <em>optional</em>, defaults to 448) &#x2014;
The maximum size of frame.`,name:"max_img_size"},{anchor:"transformers.TvpConfig.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>, defaults to 48) &#x2014;
The number of frames extracted from a video.`,name:"num_frames"},{anchor:"transformers.TvpConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Tvp text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a>.`,name:"vocab_size"},{anchor:"transformers.TvpConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers.`,name:"hidden_size"},{anchor:"transformers.TvpConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.TvpConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.TvpConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.TvpConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.TvpConfig.max_grid_col_position_embeddings",description:`<strong>max_grid_col_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The largest number of horizontal patches from a video frame.`,name:"max_grid_col_position_embeddings"},{anchor:"transformers.TvpConfig.max_grid_row_position_embeddings",description:`<strong>max_grid_row_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The largest number of vertical patches from a video frame.`,name:"max_grid_row_position_embeddings"},{anchor:"transformers.TvpConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability of hidden layers.`,name:"hidden_dropout_prob"},{anchor:"transformers.TvpConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.TvpConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.TvpConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.TvpConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability of attention layers.`,name:"attention_probs_dropout_prob"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/configuration_tvp.py#L28"}}),_e=new x({props:{name:"from_backbone_config",anchor:"transformers.TvpConfig.from_backbone_config",parameters:[{name:"backbone_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TvpConfig.from_backbone_config.backbone_config",description:`<strong>backbone_config</strong> (<a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The backbone configuration.`,name:"backbone_config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/configuration_tvp.py#L183",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpConfig"
>TvpConfig</a></p>
`}}),ye=new x({props:{name:"to_dict",anchor:"transformers.TvpConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/configuration_tvp.py#L195",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>dict[str, any]</code></p>
`}}),Me=new F({props:{title:"TvpImageProcessor",local:"transformers.TvpImageProcessor",headingTag:"h2"}}),ve=new x({props:{name:"class transformers.TvpImageProcessor",anchor:"transformers.TvpImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_pad",val:": bool = True"},{name:"pad_size",val:": typing.Optional[dict[str, int]] = None"},{name:"constant_values",val:": typing.Union[float, collections.abc.Iterable[float]] = 0"},{name:"pad_mode",val:": PaddingMode = <PaddingMode.CONSTANT: 'constant'>"},{name:"do_normalize",val:": bool = True"},{name:"do_flip_channel_order",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TvpImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.TvpImageProcessor.size",description:`<strong>size</strong> (<code>dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;longest_edge&quot; -- 448}</code>):
Size of the output image after resizing. The longest edge of the image will be resized to
<code>size[&quot;longest_edge&quot;]</code> while maintaining the aspect ratio of the original image. Can be overridden by
<code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.TvpImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.TvpImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image to the specified <code>crop_size</code>. Can be overridden by the <code>do_center_crop</code>
parameter in the <code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.TvpImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 448, &quot;width&quot;: 448}</code>):
Size of the image after applying the center crop. Can be overridden by the <code>crop_size</code> parameter in the
<code>preprocess</code> method.`,name:"crop_size"},{anchor:"transformers.TvpImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.TvpImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter
in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.TvpImageProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to pad the image. Can be overridden by the <code>do_pad</code> parameter in the <code>preprocess</code> method.`,name:"do_pad"},{anchor:"transformers.TvpImageProcessor.pad_size",description:`<strong>pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 448, &quot;width&quot;: 448}</code>):
Size of the image after applying the padding. Can be overridden by the <code>pad_size</code> parameter in the
<code>preprocess</code> method.`,name:"pad_size"},{anchor:"transformers.TvpImageProcessor.constant_values",description:`<strong>constant_values</strong> (<code>Union[float, Iterable[float]]</code>, <em>optional</em>, defaults to 0) &#x2014;
The fill value to use when padding the image.`,name:"constant_values"},{anchor:"transformers.TvpImageProcessor.pad_mode",description:`<strong>pad_mode</strong> (<code>PaddingMode</code>, <em>optional</em>, defaults to <code>PaddingMode.CONSTANT</code>) &#x2014;
Use what kind of mode in padding.`,name:"pad_mode"},{anchor:"transformers.TvpImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.TvpImageProcessor.do_flip_channel_order",description:`<strong>do_flip_channel_order</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to flip the color channels from RGB to BGR. Can be overridden by the <code>do_flip_channel_order</code>
parameter in the <code>preprocess</code> method.`,name:"do_flip_channel_order"},{anchor:"transformers.TvpImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.TvpImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/image_processing_tvp.py#L85"}}),be=new x({props:{name:"preprocess",anchor:"transformers.TvpImageProcessor.preprocess",parameters:[{name:"videos",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], list[typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]], list[list[typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]]]]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Optional[dict[str, int]] = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Optional[dict[str, int]] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_pad",val:": typing.Optional[bool] = None"},{name:"pad_size",val:": typing.Optional[dict[str, int]] = None"},{name:"constant_values",val:": typing.Union[float, collections.abc.Iterable[float], NoneType] = None"},{name:"pad_mode",val:": PaddingMode = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"do_flip_channel_order",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, list[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, list[float], NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"}],parametersDescription:[{anchor:"transformers.TvpImageProcessor.preprocess.videos",description:`<strong>videos</strong> (<code>ImageInput</code> or <code>list[ImageInput]</code> or <code>list[list[ImageInput]]</code>) &#x2014;
Frames to preprocess.`,name:"videos"},{anchor:"transformers.TvpImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.TvpImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after applying resize.`,name:"size"},{anchor:"transformers.TvpImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.TvpImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_centre_crop</code>) &#x2014;
Whether to centre crop the image.`,name:"do_center_crop"},{anchor:"transformers.TvpImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the image after applying the centre crop.`,name:"crop_size"},{anchor:"transformers.TvpImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.TvpImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.TvpImageProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to pad the image. Can be overridden by the <code>do_pad</code> parameter in the <code>preprocess</code> method.`,name:"do_pad"},{anchor:"transformers.TvpImageProcessor.preprocess.pad_size",description:`<strong>pad_size</strong> (<code>dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 448, &quot;width&quot;: 448}</code>):
Size of the image after applying the padding. Can be overridden by the <code>pad_size</code> parameter in the
<code>preprocess</code> method.`,name:"pad_size"},{anchor:"transformers.TvpImageProcessor.preprocess.constant_values",description:`<strong>constant_values</strong> (<code>Union[float, Iterable[float]]</code>, <em>optional</em>, defaults to 0) &#x2014;
The fill value to use when padding the image.`,name:"constant_values"},{anchor:"transformers.TvpImageProcessor.preprocess.pad_mode",description:`<strong>pad_mode</strong> (<code>PaddingMode</code>, <em>optional</em>, defaults to &#x201C;PaddingMode.CONSTANT&#x201D;) &#x2014;
Use what kind of mode in padding.`,name:"pad_mode"},{anchor:"transformers.TvpImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.TvpImageProcessor.preprocess.do_flip_channel_order",description:`<strong>do_flip_channel_order</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_flip_channel_order</code>) &#x2014;
Whether to flip the channel order of the image.`,name:"do_flip_channel_order"},{anchor:"transformers.TvpImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.TvpImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>list[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.TvpImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.TvpImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the inferred channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.TvpImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/image_processing_tvp.py#L340"}}),we=new F({props:{title:"TvpImageProcessorFast",local:"transformers.TvpImageProcessorFast",headingTag:"h2"}}),Ie=new x({props:{name:"class transformers.TvpImageProcessorFast",anchor:"transformers.TvpImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.tvp.image_processing_tvp_fast.TvpFastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/image_processing_tvp_fast.py#L76"}}),Je=new x({props:{name:"preprocess",anchor:"transformers.TvpImageProcessorFast.preprocess",parameters:[{name:"videos",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], list[typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]], list[list[typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]]]]"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.tvp.image_processing_tvp_fast.TvpFastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.TvpImageProcessorFast.preprocess.videos",description:`<strong>videos</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;], list[Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]], list[list[Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]]]]</code>) &#x2014;
Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If
passing in videos with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"videos"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.TvpImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.TvpImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.TvpImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.TvpImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.TvpImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.TvpImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.TvpImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.TvpImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.TvpImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.TvpImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.TvpImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.TvpImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_flip_channel_order",description:`<strong>do_flip_channel_order</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to flip the channel order of the image from RGB to BGR.`,name:"do_flip_channel_order"},{anchor:"transformers.TvpImageProcessorFast.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to pad the image.`,name:"do_pad"},{anchor:"transformers.TvpImageProcessorFast.preprocess.pad_size",description:`<strong>pad_size</strong> (<code>Dict[str, int]</code> or <code>SizeDict</code>, <em>optional</em>) &#x2014;
Size dictionary specifying the desired height and width for padding.`,name:"pad_size"},{anchor:"transformers.TvpImageProcessorFast.preprocess.constant_values",description:`<strong>constant_values</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>) &#x2014;
Value used to fill the padding area when <code>pad_mode</code> is <code>&apos;constant&apos;</code>.`,name:"constant_values"},{anchor:"transformers.TvpImageProcessorFast.preprocess.pad_mode",description:`<strong>pad_mode</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Padding mode to use &#x2014; <code>&apos;constant&apos;</code>, <code>&apos;edge&apos;</code>, <code>&apos;reflect&apos;</code>, or <code>&apos;symmetric&apos;</code>.`,name:"pad_mode"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/image_processing_tvp_fast.py#L98",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),Ue=new F({props:{title:"TvpProcessor",local:"transformers.TvpProcessor",headingTag:"h2"}}),je=new x({props:{name:"class transformers.TvpProcessor",anchor:"transformers.TvpProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TvpProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.TvpProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/processing_tvp.py#L23"}}),Ce=new x({props:{name:"__call__",anchor:"transformers.TvpProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"videos",val:" = None"},{name:"return_tensors",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TvpProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>list[str]</code>, <code>list[list[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.TvpProcessor.__call__.videos",description:`<strong>videos</strong> (<code>list[PIL.Image.Image]</code>, <code>list[np.ndarray]</code>, <code>list[torch.Tensor]</code>, <code>list[list[PIL.Image.Image]]</code>, <code>list[list[np.ndarray]]</code>, &#x2014;
<code>list[list[torch.Tensor]]</code>): The video or batch of videos to be prepared. Each video should be a list
of frames, which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch tensors,
each frame should be of shape (H, W, C), where H and W are frame height and width, and C is a number of
channels.`,name:"videos"},{anchor:"transformers.TvpProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.56.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/processing_tvp.py#L49",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields:</p>
<ul>
<li><strong>input_ids</strong> — List of token ids to be fed to a model. Returned when <code>text</code> is not <code>None</code>.</li>
<li><strong>attention_mask</strong> — List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>“attention_mask”</em> is in <code>self.model_input_names</code> and if <code>text</code> is not
<code>None</code>).</li>
<li><strong>pixel_values</strong> — Pixel values to be fed to a model. Returned when <code>videos</code> is not <code>None</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),xe=new F({props:{title:"TvpModel",local:"transformers.TvpModel",headingTag:"h2"}}),ke=new x({props:{name:"class transformers.TvpModel",anchor:"transformers.TvpModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.TvpModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpModel">TvpModel</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/modeling_tvp.py#L724"}}),Be=new x({props:{name:"forward",anchor:"transformers.TvpModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TvpModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TvpModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">TvpImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.TvpModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TvpModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TvpModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TvpModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TvpModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TvpModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/modeling_tvp.py#L754"}}),Q=new Kt({props:{$$slots:{default:[ln]},$$scope:{ctx:$}}}),S=new Ot({props:{anchor:"transformers.TvpModel.forward.example",$$slots:{default:[dn]},$$scope:{ctx:$}}}),Ze=new F({props:{title:"TvpForVideoGrounding",local:"transformers.TvpForVideoGrounding",headingTag:"h2"}}),ze=new x({props:{name:"class transformers.TvpForVideoGrounding",anchor:"transformers.TvpForVideoGrounding",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.TvpForVideoGrounding.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpForVideoGrounding">TvpForVideoGrounding</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/modeling_tvp.py#L847"}}),Ge=new x({props:{name:"forward",anchor:"transformers.TvpForVideoGrounding.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"labels",val:": typing.Optional[tuple[torch.Tensor]] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"interpolate_pos_encoding",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TvpForVideoGrounding.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.56.2/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.56.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TvpForVideoGrounding.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a>. See <a href="/docs/transformers/v4.56.2/en/model_doc/fuyu#transformers.FuyuImageProcessor.__call__">TvpImageProcessor.<strong>call</strong>()</a> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> uses
<a href="/docs/transformers/v4.56.2/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> for processing images).`,name:"pixel_values"},{anchor:"transformers.TvpForVideoGrounding.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TvpForVideoGrounding.forward.labels",description:`<strong>labels</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 3)</code>, <em>optional</em>) &#x2014;
The labels contains duration, start time, and end time of the video corresponding to the text.`,name:"labels"},{anchor:"transformers.TvpForVideoGrounding.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TvpForVideoGrounding.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TvpForVideoGrounding.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TvpForVideoGrounding.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.56.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TvpForVideoGrounding.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/tvp/modeling_tvp.py#L856"}}),L=new Kt({props:{$$slots:{default:[cn]},$$scope:{ctx:$}}}),q=new Ot({props:{anchor:"transformers.TvpForVideoGrounding.forward.example",$$slots:{default:[pn]},$$scope:{ctx:$}}}),$e=new an({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tvp.md"}}),{c(){d=i("meta"),b=n(),_=i("p"),y=n(),v=i("p"),v.innerHTML=p,k=n(),m(O.$$.fragment),ro=n(),R=i("div"),R.innerHTML=_t,ao=n(),m(ee.$$.fragment),io=n(),oe=i("p"),oe.innerHTML=yt,lo=n(),te=i("p"),te.textContent=Mt,co=n(),ne=i("p"),ne.innerHTML=vt,po=n(),se=i("p"),se.textContent=bt,mo=n(),A=i("img"),fo=n(),re=i("small"),re.innerHTML=It,go=n(),ae=i("p"),ae.innerHTML=Jt,ho=n(),m(ie.$$.fragment),uo=n(),le=i("p"),le.textContent=Ut,To=n(),de=i("p"),de.textContent=jt,_o=n(),ce=i("p"),ce.textContent=Ct,yo=n(),pe=i("p"),pe.innerHTML=xt,Mo=n(),me=i("p"),me.innerHTML=kt,vo=n(),m(fe.$$.fragment),bo=n(),ge=i("p"),ge.textContent=Bt,wo=n(),he=i("ul"),he.innerHTML=Zt,Io=n(),m(ue.$$.fragment),Jo=n(),w=i("div"),m(Te.$$.fragment),No=n(),We=i("p"),We.innerHTML=zt,Xo=n(),Fe=i("p"),Fe.innerHTML=Gt,Ho=n(),N=i("div"),m(_e.$$.fragment),Eo=n(),Re=i("p"),Re.innerHTML=$t,Qo=n(),X=i("div"),m(ye.$$.fragment),So=n(),Ae=i("p"),Ae.innerHTML=Pt,Uo=n(),m(Me.$$.fragment),jo=n(),z=i("div"),m(ve.$$.fragment),Lo=n(),Ne=i("p"),Ne.textContent=Vt,qo=n(),H=i("div"),m(be.$$.fragment),Yo=n(),Xe=i("p"),Xe.textContent=Wt,Co=n(),m(we.$$.fragment),xo=n(),G=i("div"),m(Ie.$$.fragment),Do=n(),He=i("p"),He.textContent=Ft,Ko=n(),Ee=i("div"),m(Je.$$.fragment),ko=n(),m(Ue.$$.fragment),Bo=n(),j=i("div"),m(je.$$.fragment),Oo=n(),Qe=i("p"),Qe.textContent=Rt,et=n(),Se=i("p"),Se.innerHTML=At,ot=n(),E=i("div"),m(Ce.$$.fragment),tt=n(),Le=i("p"),Le.innerHTML=Nt,Zo=n(),m(xe.$$.fragment),zo=n(),I=i("div"),m(ke.$$.fragment),nt=n(),qe=i("p"),qe.textContent=Xt,st=n(),Ye=i("p"),Ye.innerHTML=Ht,rt=n(),De=i("p"),De.innerHTML=Et,at=n(),B=i("div"),m(Be.$$.fragment),it=n(),Ke=i("p"),Ke.innerHTML=Qt,lt=n(),m(Q.$$.fragment),dt=n(),m(S.$$.fragment),Go=n(),m(Ze.$$.fragment),$o=n(),J=i("div"),m(ze.$$.fragment),ct=n(),Oe=i("p"),Oe.textContent=St,pt=n(),eo=i("p"),eo.innerHTML=Lt,mt=n(),oo=i("p"),oo.innerHTML=qt,ft=n(),Z=i("div"),m(Ge.$$.fragment),gt=n(),to=i("p"),to.innerHTML=Yt,ht=n(),m(L.$$.fragment),ut=n(),m(q.$$.fragment),Po=n(),m($e.$$.fragment),Vo=n(),so=i("p"),this.h()},l(e){const o=rn("svelte-u9bgzb",document.head);d=l(o,"META",{name:!0,content:!0}),o.forEach(t),b=s(e),_=l(e,"P",{}),U(_).forEach(t),y=s(e),v=l(e,"P",{"data-svelte-h":!0}),c(v)!=="svelte-6ggs72"&&(v.innerHTML=p),k=s(e),f(O.$$.fragment,e),ro=s(e),R=l(e,"DIV",{class:!0,"data-svelte-h":!0}),c(R)!=="svelte-13t8s2t"&&(R.innerHTML=_t),ao=s(e),f(ee.$$.fragment,e),io=s(e),oe=l(e,"P",{"data-svelte-h":!0}),c(oe)!=="svelte-tg2rvh"&&(oe.innerHTML=yt),lo=s(e),te=l(e,"P",{"data-svelte-h":!0}),c(te)!=="svelte-vfdo9a"&&(te.textContent=Mt),co=s(e),ne=l(e,"P",{"data-svelte-h":!0}),c(ne)!=="svelte-ez3ksy"&&(ne.innerHTML=vt),po=s(e),se=l(e,"P",{"data-svelte-h":!0}),c(se)!=="svelte-e6jl89"&&(se.textContent=bt),mo=s(e),A=l(e,"IMG",{src:!0,alt:!0,width:!0}),fo=s(e),re=l(e,"SMALL",{"data-svelte-h":!0}),c(re)!=="svelte-r45li6"&&(re.innerHTML=It),go=s(e),ae=l(e,"P",{"data-svelte-h":!0}),c(ae)!=="svelte-3zatzf"&&(ae.innerHTML=Jt),ho=s(e),f(ie.$$.fragment,e),uo=s(e),le=l(e,"P",{"data-svelte-h":!0}),c(le)!=="svelte-1cyrqov"&&(le.textContent=Ut),To=s(e),de=l(e,"P",{"data-svelte-h":!0}),c(de)!=="svelte-1bgttui"&&(de.textContent=jt),_o=s(e),ce=l(e,"P",{"data-svelte-h":!0}),c(ce)!=="svelte-1w5jusu"&&(ce.textContent=Ct),yo=s(e),pe=l(e,"P",{"data-svelte-h":!0}),c(pe)!=="svelte-1u4ou07"&&(pe.innerHTML=xt),Mo=s(e),me=l(e,"P",{"data-svelte-h":!0}),c(me)!=="svelte-1sf76kj"&&(me.innerHTML=kt),vo=s(e),f(fe.$$.fragment,e),bo=s(e),ge=l(e,"P",{"data-svelte-h":!0}),c(ge)!=="svelte-axv494"&&(ge.textContent=Bt),wo=s(e),he=l(e,"UL",{"data-svelte-h":!0}),c(he)!=="svelte-1owzexq"&&(he.innerHTML=Zt),Io=s(e),f(ue.$$.fragment,e),Jo=s(e),w=l(e,"DIV",{class:!0});var C=U(w);f(Te.$$.fragment,C),No=s(C),We=l(C,"P",{"data-svelte-h":!0}),c(We)!=="svelte-1a95lb3"&&(We.innerHTML=zt),Xo=s(C),Fe=l(C,"P",{"data-svelte-h":!0}),c(Fe)!=="svelte-1ek1ss9"&&(Fe.innerHTML=Gt),Ho=s(C),N=l(C,"DIV",{class:!0});var Pe=U(N);f(_e.$$.fragment,Pe),Eo=s(Pe),Re=l(Pe,"P",{"data-svelte-h":!0}),c(Re)!=="svelte-4jmq68"&&(Re.innerHTML=$t),Pe.forEach(t),Qo=s(C),X=l(C,"DIV",{class:!0});var Ve=U(X);f(ye.$$.fragment,Ve),So=s(Ve),Ae=l(Ve,"P",{"data-svelte-h":!0}),c(Ae)!=="svelte-14z5e6y"&&(Ae.innerHTML=Pt),Ve.forEach(t),C.forEach(t),Uo=s(e),f(Me.$$.fragment,e),jo=s(e),z=l(e,"DIV",{class:!0});var W=U(z);f(ve.$$.fragment,W),Lo=s(W),Ne=l(W,"P",{"data-svelte-h":!0}),c(Ne)!=="svelte-18mjwp8"&&(Ne.textContent=Vt),qo=s(W),H=l(W,"DIV",{class:!0});var Fo=U(H);f(be.$$.fragment,Fo),Yo=s(Fo),Xe=l(Fo,"P",{"data-svelte-h":!0}),c(Xe)!=="svelte-1x3yxsa"&&(Xe.textContent=Wt),Fo.forEach(t),W.forEach(t),Co=s(e),f(we.$$.fragment,e),xo=s(e),G=l(e,"DIV",{class:!0});var no=U(G);f(Ie.$$.fragment,no),Do=s(no),He=l(no,"P",{"data-svelte-h":!0}),c(He)!=="svelte-1mqxby4"&&(He.textContent=Ft),Ko=s(no),Ee=l(no,"DIV",{class:!0});var Dt=U(Ee);f(Je.$$.fragment,Dt),Dt.forEach(t),no.forEach(t),ko=s(e),f(Ue.$$.fragment,e),Bo=s(e),j=l(e,"DIV",{class:!0});var Y=U(j);f(je.$$.fragment,Y),Oo=s(Y),Qe=l(Y,"P",{"data-svelte-h":!0}),c(Qe)!=="svelte-zs1zlw"&&(Qe.textContent=Rt),et=s(Y),Se=l(Y,"P",{"data-svelte-h":!0}),c(Se)!=="svelte-1ty2mkc"&&(Se.innerHTML=At),ot=s(Y),E=l(Y,"DIV",{class:!0});var Ro=U(E);f(Ce.$$.fragment,Ro),tt=s(Ro),Le=l(Ro,"P",{"data-svelte-h":!0}),c(Le)!=="svelte-1ndpzph"&&(Le.innerHTML=Nt),Ro.forEach(t),Y.forEach(t),Zo=s(e),f(xe.$$.fragment,e),zo=s(e),I=l(e,"DIV",{class:!0});var P=U(I);f(ke.$$.fragment,P),nt=s(P),qe=l(P,"P",{"data-svelte-h":!0}),c(qe)!=="svelte-1q1y37s"&&(qe.textContent=Xt),st=s(P),Ye=l(P,"P",{"data-svelte-h":!0}),c(Ye)!=="svelte-q52n56"&&(Ye.innerHTML=Ht),rt=s(P),De=l(P,"P",{"data-svelte-h":!0}),c(De)!=="svelte-hswkmf"&&(De.innerHTML=Et),at=s(P),B=l(P,"DIV",{class:!0});var D=U(B);f(Be.$$.fragment,D),it=s(D),Ke=l(D,"P",{"data-svelte-h":!0}),c(Ke)!=="svelte-2enwmj"&&(Ke.innerHTML=Qt),lt=s(D),f(Q.$$.fragment,D),dt=s(D),f(S.$$.fragment,D),D.forEach(t),P.forEach(t),Go=s(e),f(Ze.$$.fragment,e),$o=s(e),J=l(e,"DIV",{class:!0});var V=U(J);f(ze.$$.fragment,V),ct=s(V),Oe=l(V,"P",{"data-svelte-h":!0}),c(Oe)!=="svelte-sl1gwa"&&(Oe.textContent=St),pt=s(V),eo=l(V,"P",{"data-svelte-h":!0}),c(eo)!=="svelte-q52n56"&&(eo.innerHTML=Lt),mt=s(V),oo=l(V,"P",{"data-svelte-h":!0}),c(oo)!=="svelte-hswkmf"&&(oo.innerHTML=qt),ft=s(V),Z=l(V,"DIV",{class:!0});var K=U(Z);f(Ge.$$.fragment,K),gt=s(K),to=l(K,"P",{"data-svelte-h":!0}),c(to)!=="svelte-3ds6dv"&&(to.innerHTML=Yt),ht=s(K),f(L.$$.fragment,K),ut=s(K),f(q.$$.fragment,K),K.forEach(t),V.forEach(t),Po=s(e),f($e.$$.fragment,e),Vo=s(e),so=l(e,"P",{}),U(so).forEach(t),this.h()},h(){M(d,"name","hf:doc:metadata"),M(d,"content",fn),M(R,"class","flex flex-wrap space-x-1"),on(A.src,wt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvp_architecture.png")||M(A,"src",wt),M(A,"alt","drawing"),M(A,"width","600"),M(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){a(document.head,d),r(e,b,o),r(e,_,o),r(e,y,o),r(e,v,o),r(e,k,o),g(O,e,o),r(e,ro,o),r(e,R,o),r(e,ao,o),g(ee,e,o),r(e,io,o),r(e,oe,o),r(e,lo,o),r(e,te,o),r(e,co,o),r(e,ne,o),r(e,po,o),r(e,se,o),r(e,mo,o),r(e,A,o),r(e,fo,o),r(e,re,o),r(e,go,o),r(e,ae,o),r(e,ho,o),g(ie,e,o),r(e,uo,o),r(e,le,o),r(e,To,o),r(e,de,o),r(e,_o,o),r(e,ce,o),r(e,yo,o),r(e,pe,o),r(e,Mo,o),r(e,me,o),r(e,vo,o),g(fe,e,o),r(e,bo,o),r(e,ge,o),r(e,wo,o),r(e,he,o),r(e,Io,o),g(ue,e,o),r(e,Jo,o),r(e,w,o),g(Te,w,null),a(w,No),a(w,We),a(w,Xo),a(w,Fe),a(w,Ho),a(w,N),g(_e,N,null),a(N,Eo),a(N,Re),a(w,Qo),a(w,X),g(ye,X,null),a(X,So),a(X,Ae),r(e,Uo,o),g(Me,e,o),r(e,jo,o),r(e,z,o),g(ve,z,null),a(z,Lo),a(z,Ne),a(z,qo),a(z,H),g(be,H,null),a(H,Yo),a(H,Xe),r(e,Co,o),g(we,e,o),r(e,xo,o),r(e,G,o),g(Ie,G,null),a(G,Do),a(G,He),a(G,Ko),a(G,Ee),g(Je,Ee,null),r(e,ko,o),g(Ue,e,o),r(e,Bo,o),r(e,j,o),g(je,j,null),a(j,Oo),a(j,Qe),a(j,et),a(j,Se),a(j,ot),a(j,E),g(Ce,E,null),a(E,tt),a(E,Le),r(e,Zo,o),g(xe,e,o),r(e,zo,o),r(e,I,o),g(ke,I,null),a(I,nt),a(I,qe),a(I,st),a(I,Ye),a(I,rt),a(I,De),a(I,at),a(I,B),g(Be,B,null),a(B,it),a(B,Ke),a(B,lt),g(Q,B,null),a(B,dt),g(S,B,null),r(e,Go,o),g(Ze,e,o),r(e,$o,o),r(e,J,o),g(ze,J,null),a(J,ct),a(J,Oe),a(J,pt),a(J,eo),a(J,mt),a(J,oo),a(J,ft),a(J,Z),g(Ge,Z,null),a(Z,gt),a(Z,to),a(Z,ht),g(L,Z,null),a(Z,ut),g(q,Z,null),r(e,Po,o),g($e,e,o),r(e,Vo,o),r(e,so,o),Wo=!0},p(e,[o]){const C={};o&2&&(C.$$scope={dirty:o,ctx:e}),Q.$set(C);const Pe={};o&2&&(Pe.$$scope={dirty:o,ctx:e}),S.$set(Pe);const Ve={};o&2&&(Ve.$$scope={dirty:o,ctx:e}),L.$set(Ve);const W={};o&2&&(W.$$scope={dirty:o,ctx:e}),q.$set(W)},i(e){Wo||(h(O.$$.fragment,e),h(ee.$$.fragment,e),h(ie.$$.fragment,e),h(fe.$$.fragment,e),h(ue.$$.fragment,e),h(Te.$$.fragment,e),h(_e.$$.fragment,e),h(ye.$$.fragment,e),h(Me.$$.fragment,e),h(ve.$$.fragment,e),h(be.$$.fragment,e),h(we.$$.fragment,e),h(Ie.$$.fragment,e),h(Je.$$.fragment,e),h(Ue.$$.fragment,e),h(je.$$.fragment,e),h(Ce.$$.fragment,e),h(xe.$$.fragment,e),h(ke.$$.fragment,e),h(Be.$$.fragment,e),h(Q.$$.fragment,e),h(S.$$.fragment,e),h(Ze.$$.fragment,e),h(ze.$$.fragment,e),h(Ge.$$.fragment,e),h(L.$$.fragment,e),h(q.$$.fragment,e),h($e.$$.fragment,e),Wo=!0)},o(e){u(O.$$.fragment,e),u(ee.$$.fragment,e),u(ie.$$.fragment,e),u(fe.$$.fragment,e),u(ue.$$.fragment,e),u(Te.$$.fragment,e),u(_e.$$.fragment,e),u(ye.$$.fragment,e),u(Me.$$.fragment,e),u(ve.$$.fragment,e),u(be.$$.fragment,e),u(we.$$.fragment,e),u(Ie.$$.fragment,e),u(Je.$$.fragment,e),u(Ue.$$.fragment,e),u(je.$$.fragment,e),u(Ce.$$.fragment,e),u(xe.$$.fragment,e),u(ke.$$.fragment,e),u(Be.$$.fragment,e),u(Q.$$.fragment,e),u(S.$$.fragment,e),u(Ze.$$.fragment,e),u(ze.$$.fragment,e),u(Ge.$$.fragment,e),u(L.$$.fragment,e),u(q.$$.fragment,e),u($e.$$.fragment,e),Wo=!1},d(e){e&&(t(b),t(_),t(y),t(v),t(k),t(ro),t(R),t(ao),t(io),t(oe),t(lo),t(te),t(co),t(ne),t(po),t(se),t(mo),t(A),t(fo),t(re),t(go),t(ae),t(ho),t(uo),t(le),t(To),t(de),t(_o),t(ce),t(yo),t(pe),t(Mo),t(me),t(vo),t(bo),t(ge),t(wo),t(he),t(Io),t(Jo),t(w),t(Uo),t(jo),t(z),t(Co),t(xo),t(G),t(ko),t(Bo),t(j),t(Zo),t(zo),t(I),t(Go),t($o),t(J),t(Po),t(Vo),t(so)),t(d),T(O,e),T(ee,e),T(ie,e),T(fe,e),T(ue,e),T(Te),T(_e),T(ye),T(Me,e),T(ve),T(be),T(we,e),T(Ie),T(Je),T(Ue,e),T(je),T(Ce),T(xe,e),T(ke),T(Be),T(Q),T(S),T(Ze,e),T(ze),T(Ge),T(L),T(q),T($e,e)}}}const fn='{"title":"TVP","local":"tvp","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips and examples","local":"usage-tips-and-examples","sections":[],"depth":2},{"title":"TvpConfig","local":"transformers.TvpConfig","sections":[],"depth":2},{"title":"TvpImageProcessor","local":"transformers.TvpImageProcessor","sections":[],"depth":2},{"title":"TvpImageProcessorFast","local":"transformers.TvpImageProcessorFast","sections":[],"depth":2},{"title":"TvpProcessor","local":"transformers.TvpProcessor","sections":[],"depth":2},{"title":"TvpModel","local":"transformers.TvpModel","sections":[],"depth":2},{"title":"TvpForVideoGrounding","local":"transformers.TvpForVideoGrounding","sections":[],"depth":2}],"depth":1}';function gn($){return tn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bn extends nn{constructor(d){super(),sn(this,d,gn,mn,en,{})}}export{bn as component};
