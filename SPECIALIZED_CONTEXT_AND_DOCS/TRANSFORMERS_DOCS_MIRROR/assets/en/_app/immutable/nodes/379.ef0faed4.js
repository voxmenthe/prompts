import{s as eo,o as so,n as nn}from"../chunks/scheduler.18a86fab.js";import{S as to,i as ao,g as l,s as a,r as m,A as no,h as i,f as t,c as n,j as y,x as f,u as c,k as T,l as oo,y as r,a as o,v as p,d,t as g,w as h}from"../chunks/index.98837b22.js";import{T as ro}from"../chunks/Tip.77304350.js";import{D as w}from"../chunks/Docstring.a1ef7999.js";import{C as B}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as On}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as b,E as lo}from"../chunks/getInferenceSnippets.06c2775f.js";function io(P){let u,z="Example:",j,M,U;return M=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBTYW0yVmlzaW9uQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwU2FtMlByb21wdEVuY29kZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBTYW0yTWFza0RlY29kZXJDb25maWclMkMlMEElMjAlMjAlMjAlMjBTYW0yTW9kZWwlMkMlMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFNhbTJDb25maWclMjB3aXRoJTIwJTYwJTIyZmFjZWJvb2slMkZzYW0yLjFfaGllcmFfdGlueSUyMiU2MCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBTYW0yY29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwU2FtMk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjAlNjAlMjJmYWNlYm9vayUyRnNhbTIuMV9oaWVyYV90aW55JTIyJTYwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBTYW0yTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBTYW0yQ29uZmlnJTIwZnJvbSUyMGElMjBTYW0yVmlzaW9uQ29uZmlnJTJDJTIwU2FtMlByb21wdEVuY29kZXJDb25maWclMkMlMjBhbmQlMjBTYW0yTWFza0RlY29kZXJDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBTQU0yJTIwdmlzaW9uJTIwZW5jb2RlciUyQyUyMG1lbW9yeSUyMGF0dGVudGlvbiUyQyUyMGFuZCUyMG1lbW9yeSUyMGVuY29kZXIlMjBjb25maWd1cmF0aW9ucyUwQXZpc2lvbl9jb25maWclMjAlM0QlMjBTYW0yVmlzaW9uQ29uZmlnKCklMEFwcm9tcHRfZW5jb2Rlcl9jb25maWclMjAlM0QlMjBTYW0yUHJvbXB0RW5jb2RlckNvbmZpZygpJTBBbWFza19kZWNvZGVyX2NvbmZpZyUyMCUzRCUyMFNhbTJNYXNrRGVjb2RlckNvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwU2FtMkNvbmZpZyh2aXNpb25fY29uZmlnJTJDJTIwcHJvbXB0X2VuY29kZXJfY29uZmlnJTJDJTIwbWFza19kZWNvZGVyX2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Sam2VisionConfig,
<span class="hljs-meta">... </span>    Sam2PromptEncoderConfig,
<span class="hljs-meta">... </span>    Sam2MaskDecoderConfig,
<span class="hljs-meta">... </span>    Sam2Model,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Sam2Config with \`&quot;facebook/sam2.1_hiera_tiny&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Sam2config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Sam2Model (with random weights) from the \`&quot;facebook/sam2.1_hiera_tiny&quot;\` style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Sam2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a Sam2Config from a Sam2VisionConfig, Sam2PromptEncoderConfig, and Sam2MaskDecoderConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing SAM2 vision encoder, memory attention, and memory encoder configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = Sam2VisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>prompt_encoder_config = Sam2PromptEncoderConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_decoder_config = Sam2MaskDecoderConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = Sam2Config(vision_config, prompt_encoder_config, mask_decoder_config)`,wrap:!1}}),{c(){u=l("p"),u.textContent=z,j=a(),m(M.$$.fragment)},l(_){u=i(_,"P",{"data-svelte-h":!0}),f(u)!=="svelte-11lpom8"&&(u.textContent=z),j=n(_),c(M.$$.fragment,_)},m(_,x){o(_,u,x),o(_,j,x),p(M,_,x),U=!0},p:nn,i(_){U||(d(M.$$.fragment,_),U=!0)},o(_){g(M.$$.fragment,_),U=!1},d(_){_&&(t(u),t(j)),h(M,_)}}}function mo(P){let u,z=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){u=l("p"),u.innerHTML=z},l(j){u=i(j,"P",{"data-svelte-h":!0}),f(u)!=="svelte-fincs2"&&(u.innerHTML=z)},m(j,M){o(j,u,M)},p:nn,d(j){j&&t(u)}}}function co(P){let u,z="Example:",j,M,U;return M=new B({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Byb2Nlc3NvciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJkYW5lbGNzYiUyRnNhbTIuMV9oaWVyYV90aW55JTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmRhbmVsY3NiJTJGc2FtMi4xX2hpZXJhX3RpbnklMjIpJTBBJTBBaW1nX3VybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnRyYW5zZm9ybWVycyUyRm1vZGVsX2RvYyUyRnNhbS1jYXIucG5nJTIyJTBBcmF3X2ltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1nX3VybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJSR0IlMjIpJTBBaW5wdXRfcG9pbnRzJTIwJTNEJTIwJTVCJTVCJTVCNDAwJTJDJTIwNjUwJTVEJTVEJTVEJTIwJTIwJTIzJTIwMkQlMjBsb2NhdGlvbiUyMG9mJTIwYSUyMHdpbmRvdyUyMG9uJTIwdGhlJTIwY2FyJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRHJhd19pbWFnZSUyQyUyMGlucHV0X3BvaW50cyUzRGlucHV0X3BvaW50cyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwR2V0JTIwc2VnbWVudGF0aW9uJTIwbWFzayUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEElMjMlMjBQb3N0cHJvY2VzcyUyMG1hc2tzJTBBbWFza3MlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX21hc2tzKCUwQSUyMCUyMCUyMCUyMG91dHB1dHMucHJlZF9tYXNrcyUyQyUyMGlucHV0cyU1QiUyMm9yaWdpbmFsX3NpemVzJTIyJTVEJTJDJTIwaW5wdXRzJTVCJTIycmVzaGFwZWRfaW5wdXRfc2l6ZXMlMjIlNUQlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;danelcsb/sam2.1_hiera_tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;danelcsb/sam2.1_hiera_tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>img_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_image = Image.<span class="hljs-built_in">open</span>(requests.get(img_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[<span class="hljs-number">400</span>, <span class="hljs-number">650</span>]]]  <span class="hljs-comment"># 2D location of a window on the car</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get segmentation mask</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Postprocess masks</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(
<span class="hljs-meta">... </span>    outputs.pred_masks, inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>], inputs[<span class="hljs-string">&quot;reshaped_input_sizes&quot;</span>]
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){u=l("p"),u.textContent=z,j=a(),m(M.$$.fragment)},l(_){u=i(_,"P",{"data-svelte-h":!0}),f(u)!=="svelte-11lpom8"&&(u.textContent=z),j=n(_),c(M.$$.fragment,_)},m(_,x){o(_,u,x),o(_,j,x),p(M,_,x),U=!0},p:nn,i(_){U||(d(M.$$.fragment,_),U=!0)},o(_){g(M.$$.fragment,_),U=!1},d(_){_&&(t(u),t(j)),h(M,_)}}}function po(P){let u,z,j,M,U,_='<div class="flex flex-wrap space-x-1"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/> <img alt="FlashAttention" src="https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat"/></div>',x,ae,st,ne,tt,oe,on='SAM2 (Segment Anything Model 2) was proposed in <a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/" rel="nofollow">Segment Anything in Images and Videos</a> by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer.',at,re,rn="The model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.",nt,le,ln='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif" alt="example image"/>',ot,ie,mn="The abstract from the paper is the following:",rt,me,cn="<em>We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.</em>",lt,ce,pn="Tips:",it,pe,dn="<li>Batch &amp; Video Support: SAM2 natively supports batch processing and seamless video segmentation, while original SAM is designed for static images and simpler one-image-at-a-time workflows.</li> <li>Accuracy &amp; Generalization: SAM2 shows improved segmentation quality, robustness, and zero-shot generalization to new domains compared to the original SAM, especially with mixed prompts.</li>",mt,de,gn=`This model was contributed by <a href="https://github.com/SangbumChoi" rel="nofollow">sangbumchoi</a> and <a href="https://huggingface.co/yonigozlan" rel="nofollow">yonigozlan</a>.
The original code can be found <a href="https://github.com/facebookresearch/sam2/tree/main" rel="nofollow">here</a>.`,ct,ge,pt,he,dt,fe,hn="SAM2 can be used for automatic mask generation to segment all objects in an image using the <code>mask-generation</code> pipeline:",gt,ue,ht,_e,ft,be,ut,Me,fn="You can segment objects by providing a single point click on the object you want to segment:",_t,ye,bt,Te,Mt,je,un="You can provide multiple points to refine the segmentation:",yt,we,Tt,Je,jt,Ue,_n="SAM2 also supports bounding box inputs for segmentation:",wt,ve,Jt,Ie,Ut,ke,bn="You can segment multiple objects simultaneously:",vt,Ce,It,ze,kt,$e,Ct,xe,Mn="Process multiple images simultaneously for improved efficiency:",zt,Se,$t,Be,xt,We,yn="Segment multiple objects within each image using batch inference:",St,Ne,Bt,Ve,Wt,Ze,Tn="Handle complex batch scenarios with multiple points per object:",Nt,Re,Vt,Fe,Zt,Ge,jn="Process multiple images with bounding box inputs:",Rt,Xe,Ft,Ee,Gt,He,wn="SAM2 can use masks from previous predictions as input to refine segmentation:",Xt,Qe,Et,Pe,Ht,$,Ye,Ma,ws,Jn=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config">Sam2Config</a> is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Model">Sam2Model</a>. It is used to instantiate a
SAM2 model according to the specified arguments, defining the memory attention, memory encoder, and image encoder
configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny
<a href="https://huggingface.co/facebook/sam2.1-hiera-tiny" rel="nofollow">facebook/sam2.1-hiera-tiny</a> architecture.`,ya,Js,Un=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ta,Y,Qt,De,Pt,W,Ae,ja,Us,vn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2HieraDetModel">Sam2HieraDetModel</a>. It is used to instantiate
a HieraDet model as defined in the original sam2 repo according to the specified arguments, defining the model architecture.
Instantiating a configuration defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny
<a href="https://huggingface.co/facebook/sam2.1-hiera-tiny" rel="nofollow">facebook/sam2.1-hiera-tiny</a> architecture.`,wa,vs,In=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Yt,qe,Dt,N,Le,Ja,Is,kn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionModel">Sam2VisionModel</a>. It is used to instantiate a SAM
vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny
<a href="https://huggingface.co/facebook/sam2.1-hiera-tiny" rel="nofollow">facebook/sam2.1-hiera-tiny</a> architecture.`,Ua,ks,Cn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,At,Ke,qt,V,Oe,va,Cs,zn=`This is the configuration class to store the configuration of a <code>Sam2MaskDecoder</code>. It is used to instantiate a SAM2
memory encoder according to the specified arguments, defining the model architecture.`,Ia,zs,$n=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Lt,es,Kt,Z,ss,ka,$s,xn=`This is the configuration class to store the configuration of a <code>Sam2PromptEncoder</code>. The <code>Sam2PromptEncoder</code>
module is used to encode the input 2D points and bounding boxes.`,Ca,xs,Sn=`Configuration objects inherit from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/v4.56.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ot,ts,ea,v,as,za,Ss,Bn=`Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a
single processor.`,$a,Bs,Wn=`<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Processor">Sam2Processor</a> offers all the functionalities of <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2ImageProcessorFast">Sam2ImageProcessorFast</a> and <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoProcessor">Sam2VideoProcessor</a>. See the docstring of
<code>__call__()</code> and <a href="/docs/transformers/v4.56.2/en/model_doc/sam2_video#transformers.Sam2VideoProcessor.__call__"><strong>call</strong>()</a> for more information.`,xa,D,ns,Sa,Ws,Nn=`This method uses <code>Sam2ImageProcessorFast.__call__()</code> method to prepare image(s) for the model. It also prepares 2D
points and bounding boxes for the model if they are provided.`,Ba,A,os,Wa,Ns,Vn="Remove padding and upscale masks to the original image size.",sa,rs,ta,J,ls,Na,Vs,Zn="Constructs a fast Sam2 image processor.",Va,q,is,Za,Zs,Rn=`Filters the predicted masks by selecting only the ones that meets several criteria. The first criterion being
that the iou scores needs to be greater than <code>pred_iou_thresh</code>. The second criterion is that the stability
score needs to be greater than <code>stability_score_thresh</code>. The method also converts the predicted masks to
bounding boxes and pad the predicted masks if necessary.`,Ra,L,ms,Fa,Rs,Fn="Generates a list of crop boxes of different sizes. Each layer has (2<strong>i)</strong>2 boxes for the ith layer.",Ga,K,cs,Xa,Fs,Gn="Post processes mask that are generated by calling the Non Maximum Suppression algorithm on the predicted masks.",Ea,O,ps,Ha,Gs,Xn="Remove padding and upscale masks to the original image size.",Qa,Xs,ds,aa,gs,na,E,hs,Pa,Es,fs,oa,us,ra,I,_s,Ya,Hs,En="The vision model from Sam without any head or projection on top.",Da,Qs,Hn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Aa,Ps,Qn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,qa,Ys,bs,la,Ms,ia,k,ys,La,Ds,Pn=`Segment Anything Model 2 (SAM 2) for generating segmentation masks, given an input image and
input points and labels, boxes, or masks.`,Ka,As,Yn=`This model inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Oa,qs,Dn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,en,S,Ts,sn,Ls,An='The <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Model">Sam2Model</a> forward method, overrides the <code>__call__</code> special method.',tn,ee,an,se,ma,js,ca,et,pa;return ae=new b({props:{title:"SAM2",local:"sam2",headingTag:"h1"}}),ne=new b({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ge=new b({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),he=new b({props:{title:"Automatic Mask Generation with Pipeline",local:"automatic-mask-generation-with-pipeline",headingTag:"h3"}}),ue=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBZ2VuZXJhdG9yJTIwJTNEJTIwcGlwZWxpbmUoJTIybWFzay1nZW5lcmF0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJmYWNlYm9vayUyRnNhbTIuMS1oaWVyYS1sYXJnZSUyMiUyQyUyMGRldmljZSUzRDApJTBBaW1hZ2VfdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaGYtaW50ZXJuYWwtdGVzdGluZyUyRnNhbTItZml4dHVyZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnRydWNrLmpwZyUyMiUwQW91dHB1dHMlMjAlM0QlMjBnZW5lcmF0b3IoaW1hZ2VfdXJsJTJDJTIwcG9pbnRzX3Blcl9iYXRjaCUzRDY0KSUwQSUwQWxlbihvdXRwdXRzJTVCJTIybWFza3MlMjIlNUQpJTIwJTIwJTIzJTIwTnVtYmVyJTIwb2YlMjBtYXNrcyUyMGdlbmVyYXRlZA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>generator = pipeline(<span class="hljs-string">&quot;mask-generation&quot;</span>, model=<span class="hljs-string">&quot;facebook/sam2.1-hiera-large&quot;</span>, device=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = generator(image_url, points_per_batch=<span class="hljs-number">64</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(outputs[<span class="hljs-string">&quot;masks&quot;</span>])  <span class="hljs-comment"># Number of masks generated</span>
<span class="hljs-number">39</span>`,wrap:!1}}),_e=new b({props:{title:"Basic Image Segmentation",local:"basic-image-segmentation",headingTag:"h3"}}),be=new b({props:{title:"Single Point Click",local:"single-point-click",headingTag:"h4"}}),ye=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNhbTJQcm9jZXNzb3IlMkMlMjBTYW0yTW9kZWwlMkMlMjBpbmZlcl9kZXZpY2UlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEFtb2RlbCUyMCUzRCUyMFNhbTJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZzYW0yLjEtaGllcmEtbGFyZ2UlMjIpLnRvKGRldmljZSklMEFwcm9jZXNzb3IlMjAlM0QlMjBTYW0yUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnNhbTIuMS1oaWVyYS1sYXJnZSUyMiklMEElMEFpbWFnZV91cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZoZi1pbnRlcm5hbC10ZXN0aW5nJTJGc2FtMi1maXh0dXJlcyUyRnJlc29sdmUlMkZtYWluJTJGdHJ1Y2suanBnJTIyJTBBcmF3X2ltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQoaW1hZ2VfdXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMlJHQiUyMiklMEElMEFpbnB1dF9wb2ludHMlMjAlM0QlMjAlNUIlNUIlNUIlNUI1MDAlMkMlMjAzNzUlNUQlNUQlNUQlNUQlMjAlMjAlMjMlMjBTaW5nbGUlMjBwb2ludCUyMGNsaWNrJTJDJTIwNCUyMGRpbWVuc2lvbnMlMjAoaW1hZ2VfZGltJTJDJTIwb2JqZWN0X2RpbSUyQyUyMHBvaW50X3Blcl9vYmplY3RfZGltJTJDJTIwY29vcmRpbmF0ZXMpJTBBaW5wdXRfbGFiZWxzJTIwJTNEJTIwJTVCJTVCJTVCMSU1RCU1RCU1RCUyMCUyMCUyMyUyMDElMjBmb3IlMjBwb3NpdGl2ZSUyMGNsaWNrJTJDJTIwMCUyMGZvciUyMG5lZ2F0aXZlJTIwY2xpY2slMkMlMjAzJTIwZGltZW5zaW9ucyUyMChpbWFnZV9kaW0lMkMlMjBvYmplY3RfZGltJTJDJTIwcG9pbnRfbGFiZWwpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRHJhd19pbWFnZSUyQyUyMGlucHV0X3BvaW50cyUzRGlucHV0X3BvaW50cyUyQyUyMGlucHV0X2xhYmVscyUzRGlucHV0X2xhYmVscyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQW1hc2tzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyhvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RCklNUIwJTVEJTBBJTBBJTIzJTIwVGhlJTIwbW9kZWwlMjBvdXRwdXRzJTIwbXVsdGlwbGUlMjBtYXNrJTIwcHJlZGljdGlvbnMlMjByYW5rZWQlMjBieSUyMHF1YWxpdHklMjBzY29yZSUwQXByaW50KGYlMjJHZW5lcmF0ZWQlMjAlN0JtYXNrcy5zaGFwZSU1QjElNUQlN0QlMjBtYXNrcyUyMHdpdGglMjBzaGFwZSUyMCU3Qm1hc2tzLnNoYXBlJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Sam2Processor, Sam2Model, infer_device
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>device = infer_device()

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Sam2Model.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-large&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Sam2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_image = Image.<span class="hljs-built_in">open</span>(requests.get(image_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]]]]  <span class="hljs-comment"># Single point click, 4 dimensions (image_dim, object_dim, point_per_object_dim, coordinates)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>]]]  <span class="hljs-comment"># 1 for positive click, 0 for negative click, 3 dimensions (image_dim, object_dim, point_label)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The model outputs multiple mask predictions ranked by quality score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{masks.shape[<span class="hljs-number">1</span>]}</span> masks with shape <span class="hljs-subst">{masks.shape}</span>&quot;</span>)
Generated <span class="hljs-number">3</span> masks <span class="hljs-keyword">with</span> shape torch.Size(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1500</span>, <span class="hljs-number">2250</span>)`,wrap:!1}}),Te=new b({props:{title:"Multiple Points for Refinement",local:"multiple-points-for-refinement",headingTag:"h4"}}),we=new B({props:{code:"JTIzJTIwQWRkJTIwYm90aCUyMHBvc2l0aXZlJTIwYW5kJTIwbmVnYXRpdmUlMjBwb2ludHMlMjB0byUyMHJlZmluZSUyMHRoZSUyMG1hc2slMEFpbnB1dF9wb2ludHMlMjAlM0QlMjAlNUIlNUIlNUIlNUI1MDAlMkMlMjAzNzUlNUQlMkMlMjAlNUIxMTI1JTJDJTIwNjI1JTVEJTVEJTVEJTVEJTIwJTIwJTIzJTIwTXVsdGlwbGUlMjBwb2ludHMlMjBmb3IlMjByZWZpbmVtZW50JTBBaW5wdXRfbGFiZWxzJTIwJTNEJTIwJTVCJTVCJTVCMSUyQyUyMDElNUQlNUQlNUQlMjAlMjAlMjMlMjBCb3RoJTIwcG9zaXRpdmUlMjBjbGlja3MlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlJTJDJTIwaW5wdXRfcG9pbnRzJTNEaW5wdXRfcG9pbnRzJTJDJTIwaW5wdXRfbGFiZWxzJTNEaW5wdXRfbGFiZWxzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBbWFza3MlMjAlM0QlMjBwcm9jZXNzb3IucG9zdF9wcm9jZXNzX21hc2tzKG91dHB1dHMucHJlZF9tYXNrcy5jcHUoKSUyQyUyMGlucHV0cyU1QiUyMm9yaWdpbmFsX3NpemVzJTIyJTVEKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add both positive and negative points to refine the mask</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>], [<span class="hljs-number">1125</span>, <span class="hljs-number">625</span>]]]]  <span class="hljs-comment"># Multiple points for refinement</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]]  <span class="hljs-comment"># Both positive clicks</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])[<span class="hljs-number">0</span>]`,wrap:!1}}),Je=new b({props:{title:"Bounding Box Input",local:"bounding-box-input",headingTag:"h4"}}),ve=new B({props:{code:"JTIzJTIwRGVmaW5lJTIwYm91bmRpbmclMjBib3glMjBhcyUyMCU1QnhfbWluJTJDJTIweV9taW4lMkMlMjB4X21heCUyQyUyMHlfbWF4JTVEJTBBaW5wdXRfYm94ZXMlMjAlM0QlMjAlNUIlNUIlNUI3NSUyQyUyMDI3NSUyQyUyMDE3MjUlMkMlMjA4NTAlNUQlNUQlNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlJTJDJTIwaW5wdXRfYm94ZXMlM0RpbnB1dF9ib3hlcyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQW1hc2tzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyhvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RCklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Define bounding box as [x_min, y_min, x_max, y_max]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_boxes = [[[<span class="hljs-number">75</span>, <span class="hljs-number">275</span>, <span class="hljs-number">1725</span>, <span class="hljs-number">850</span>]]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_boxes=input_boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])[<span class="hljs-number">0</span>]`,wrap:!1}}),Ie=new b({props:{title:"Multiple Objects Segmentation",local:"multiple-objects-segmentation",headingTag:"h4"}}),Ce=new B({props:{code:"JTIzJTIwRGVmaW5lJTIwcG9pbnRzJTIwZm9yJTIwdHdvJTIwZGlmZmVyZW50JTIwb2JqZWN0cyUwQWlucHV0X3BvaW50cyUyMCUzRCUyMCU1QiU1QiU1QiU1QjUwMCUyQyUyMDM3NSU1RCU1RCUyQyUyMCU1QiU1QjY1MCUyQyUyMDc1MCU1RCU1RCU1RCU1RCUyMCUyMCUyMyUyMFBvaW50cyUyMGZvciUyMHR3byUyMG9iamVjdHMlMjBpbiUyMHNhbWUlMjBpbWFnZSUwQWlucHV0X2xhYmVscyUyMCUzRCUyMCU1QiU1QiU1QjElNUQlMkMlMjAlNUIxJTVEJTVEJTVEJTIwJTIwJTIzJTIwUG9zaXRpdmUlMjBjbGlja3MlMjBmb3IlMjBib3RoJTIwb2JqZWN0cyUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RyYXdfaW1hZ2UlMkMlMjBpbnB1dF9wb2ludHMlM0RpbnB1dF9wb2ludHMlMkMlMjBpbnB1dF9sYWJlbHMlM0RpbnB1dF9sYWJlbHMlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMG11bHRpbWFza19vdXRwdXQlM0RGYWxzZSklMEElMEElMjMlMjBFYWNoJTIwb2JqZWN0JTIwZ2V0cyUyMGl0cyUyMG93biUyMG1hc2slMEFtYXNrcyUyMCUzRCUyMHByb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfbWFza3Mob3V0cHV0cy5wcmVkX21hc2tzLmNwdSgpJTJDJTIwaW5wdXRzJTVCJTIyb3JpZ2luYWxfc2l6ZXMlMjIlNUQpJTVCMCU1RCUwQXByaW50KGYlMjJHZW5lcmF0ZWQlMjBtYXNrcyUyMGZvciUyMCU3Qm1hc2tzLnNoYXBlJTVCMCU1RCU3RCUyMG9iamVjdHMlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Define points for two different objects</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]], [[<span class="hljs-number">650</span>, <span class="hljs-number">750</span>]]]]  <span class="hljs-comment"># Points for two objects in same image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>]]]  <span class="hljs-comment"># Positive clicks for both objects</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, multimask_output=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Each object gets its own mask</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated masks for <span class="hljs-subst">{masks.shape[<span class="hljs-number">0</span>]}</span> objects&quot;</span>)
Generated masks <span class="hljs-keyword">for</span> <span class="hljs-number">2</span> objects`,wrap:!1}}),ze=new b({props:{title:"Batch Inference",local:"batch-inference",headingTag:"h3"}}),$e=new b({props:{title:"Batched Images",local:"batched-images",headingTag:"h4"}}),Se=new B({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNhbTJQcm9jZXNzb3IlMkMlMjBTYW0yTW9kZWwlMkMlMjBpbmZlcl9kZXZpY2UlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBZGV2aWNlJTIwJTNEJTIwaW5mZXJfZGV2aWNlKCklMEElMEFtb2RlbCUyMCUzRCUyMFNhbTJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZzYW0yLjEtaGllcmEtbGFyZ2UlMjIpLnRvKGRldmljZSklMEFwcm9jZXNzb3IlMjAlM0QlMjBTYW0yUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnNhbTIuMS1oaWVyYS1sYXJnZSUyMiklMEElMEElMjMlMjBMb2FkJTIwbXVsdGlwbGUlMjBpbWFnZXMlMEFpbWFnZV91cmxzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGaGYtaW50ZXJuYWwtdGVzdGluZyUyRnNhbTItZml4dHVyZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnRydWNrLmpwZyUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRmh1Z2dpbmdmYWNlJTJGZG9jdW1lbnRhdGlvbi1pbWFnZXMlMkZyZXNvbHZlJTJGbWFpbiUyRnRyYW5zZm9ybWVycyUyRm1vZGVsX2RvYyUyRmRvZy1zYW0ucG5nJTIyJTBBJTVEJTBBcmF3X2ltYWdlcyUyMCUzRCUyMCU1QkltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdykuY29udmVydCglMjJSR0IlMjIpJTIwZm9yJTIwdXJsJTIwaW4lMjBpbWFnZV91cmxzJTVEJTBBJTBBJTIzJTIwU2luZ2xlJTIwcG9pbnQlMjBwZXIlMjBpbWFnZSUwQWlucHV0X3BvaW50cyUyMCUzRCUyMCU1QiU1QiU1QiU1QjUwMCUyQyUyMDM3NSU1RCU1RCU1RCUyQyUyMCU1QiU1QiU1Qjc3MCUyQyUyMDIwMCU1RCU1RCU1RCU1RCUyMCUyMCUyMyUyME9uZSUyMHBvaW50JTIwZm9yJTIwZWFjaCUyMGltYWdlJTBBaW5wdXRfbGFiZWxzJTIwJTNEJTIwJTVCJTVCJTVCMSU1RCU1RCUyQyUyMCU1QiU1QjElNUQlNUQlNUQlMjAlMjAlMjMlMjBQb3NpdGl2ZSUyMGNsaWNrcyUyMGZvciUyMGJvdGglMjBpbWFnZXMlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlcyUyQyUyMGlucHV0X3BvaW50cyUzRGlucHV0X3BvaW50cyUyQyUyMGlucHV0X2xhYmVscyUzRGlucHV0X2xhYmVscyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKG1vZGVsLmRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzJTJDJTIwbXVsdGltYXNrX291dHB1dCUzREZhbHNlKSUwQSUwQSUyMyUyMFBvc3QtcHJvY2VzcyUyMG1hc2tzJTIwZm9yJTIwZWFjaCUyMGltYWdlJTBBYWxsX21hc2tzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyhvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RCklMEFwcmludChmJTIyUHJvY2Vzc2VkJTIwJTdCbGVuKGFsbF9tYXNrcyklN0QlMjBpbWFnZXMlMkMlMjBlYWNoJTIwd2l0aCUyMCU3QmFsbF9tYXNrcyU1QjAlNUQuc2hhcGUlNUIwJTVEJTdEJTIwb2JqZWN0cyUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Sam2Processor, Sam2Model, infer_device
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>device = infer_device()

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Sam2Model.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-large&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Sam2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/sam2.1-hiera-large&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load multiple images</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_urls = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png&quot;</span>
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_images = [Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> image_urls]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Single point per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]]], [[[<span class="hljs-number">770</span>, <span class="hljs-number">200</span>]]]]  <span class="hljs-comment"># One point for each image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>]]]  <span class="hljs-comment"># Positive clicks for both images</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, multimask_output=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Post-process masks for each image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Processed <span class="hljs-subst">{<span class="hljs-built_in">len</span>(all_masks)}</span> images, each with <span class="hljs-subst">{all_masks[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]}</span> objects&quot;</span>)
Processed <span class="hljs-number">2</span> images, each <span class="hljs-keyword">with</span> <span class="hljs-number">1</span> objects`,wrap:!1}}),Be=new b({props:{title:"Batched Objects per Image",local:"batched-objects-per-image",headingTag:"h4"}}),Ne=new B({props:{code:"JTIzJTIwTXVsdGlwbGUlMjBvYmplY3RzJTIwcGVyJTIwaW1hZ2UlMjAtJTIwZGlmZmVyZW50JTIwbnVtYmVycyUyMG9mJTIwb2JqZWN0cyUyMHBlciUyMGltYWdlJTBBaW5wdXRfcG9pbnRzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTVCJTVCJTVCNTAwJTJDJTIwMzc1JTVEJTVEJTJDJTIwJTVCJTVCNjUwJTJDJTIwNzUwJTVEJTVEJTVEJTJDJTIwJTIwJTIzJTIwVHJ1Y2slMjBpbWFnZSUzQSUyMDIlMjBvYmplY3RzJTBBJTIwJTIwJTIwJTIwJTVCJTVCJTVCNzcwJTJDJTIwMjAwJTVEJTVEJTVEJTIwJTIwJTIzJTIwRG9nJTIwaW1hZ2UlM0ElMjAxJTIwb2JqZWN0JTBBJTVEJTBBaW5wdXRfbGFiZWxzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTVCJTVCMSU1RCUyQyUyMCU1QjElNUQlNUQlMkMlMjAlMjAlMjMlMjBUcnVjayUyMGltYWdlJTNBJTIwcG9zaXRpdmUlMjBjbGlja3MlMjBmb3IlMjBib3RoJTIwb2JqZWN0cyUwQSUyMCUyMCUyMCUyMCU1QiU1QjElNUQlNUQlMjAlMjAlMjMlMjBEb2clMjBpbWFnZSUzQSUyMHBvc2l0aXZlJTIwY2xpY2slMjBmb3IlMjB0aGUlMjBvYmplY3QlMEElNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlcyUyQyUyMGlucHV0X3BvaW50cyUzRGlucHV0X3BvaW50cyUyQyUyMGlucHV0X2xhYmVscyUzRGlucHV0X2xhYmVscyUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzJTJDJTIwbXVsdGltYXNrX291dHB1dCUzREZhbHNlKSUwQSUwQWFsbF9tYXNrcyUyMCUzRCUyMHByb2Nlc3Nvci5wb3N0X3Byb2Nlc3NfbWFza3Mob3V0cHV0cy5wcmVkX21hc2tzLmNwdSgpJTJDJTIwaW5wdXRzJTVCJTIyb3JpZ2luYWxfc2l6ZXMlMjIlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple objects per image - different numbers of objects per image</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [
<span class="hljs-meta">... </span>    [[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]], [[<span class="hljs-number">650</span>, <span class="hljs-number">750</span>]]],  <span class="hljs-comment"># Truck image: 2 objects</span>
<span class="hljs-meta">... </span>    [[[<span class="hljs-number">770</span>, <span class="hljs-number">200</span>]]]  <span class="hljs-comment"># Dog image: 1 object</span>
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [
<span class="hljs-meta">... </span>    [[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>]],  <span class="hljs-comment"># Truck image: positive clicks for both objects</span>
<span class="hljs-meta">... </span>    [[<span class="hljs-number">1</span>]]  <span class="hljs-comment"># Dog image: positive click for the object</span>
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, multimask_output=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])`,wrap:!1}}),Ve=new b({props:{title:"Batched Images with Batched Objects and Multiple Points",local:"batched-images-with-batched-objects-and-multiple-points",headingTag:"h4"}}),Re=new B({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add groceries image for more complex example</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>groceries_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/groceries.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>groceries_image = Image.<span class="hljs-built_in">open</span>(requests.get(groceries_url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_images = [raw_images[<span class="hljs-number">0</span>], groceries_image]  <span class="hljs-comment"># Use truck and groceries images</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Complex batching: multiple images, multiple objects, multiple points per object</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [
<span class="hljs-meta">... </span>    [[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]], [[<span class="hljs-number">650</span>, <span class="hljs-number">750</span>]]],  <span class="hljs-comment"># Truck image: 2 objects with 1 point each</span>
<span class="hljs-meta">... </span>    [[[<span class="hljs-number">400</span>, <span class="hljs-number">300</span>]], [[<span class="hljs-number">630</span>, <span class="hljs-number">300</span>], [<span class="hljs-number">550</span>, <span class="hljs-number">300</span>]]]  <span class="hljs-comment"># Groceries image: obj1 has 1 point, obj2 has 2 points</span>
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [
<span class="hljs-meta">... </span>    [[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>]],  <span class="hljs-comment"># Truck image: positive clicks</span>
<span class="hljs-meta">... </span>    [[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]  <span class="hljs-comment"># Groceries image: positive clicks for refinement</span>
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_images, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, multimask_output=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])`,wrap:!1}}),Fe=new b({props:{title:"Batched Bounding Boxes",local:"batched-bounding-boxes",headingTag:"h4"}}),Xe=new B({props:{code:"JTIzJTIwTXVsdGlwbGUlMjBib3VuZGluZyUyMGJveGVzJTIwcGVyJTIwaW1hZ2UlMjAodXNpbmclMjB0cnVjayUyMGFuZCUyMGdyb2NlcmllcyUyMGltYWdlcyklMEFpbnB1dF9ib3hlcyUyMCUzRCUyMCU1QiUwQSUyMCUyMCUyMCUyMCU1QiU1Qjc1JTJDJTIwMjc1JTJDJTIwMTcyNSUyQyUyMDg1MCU1RCUyQyUyMCU1QjQyNSUyQyUyMDYwMCUyQyUyMDcwMCUyQyUyMDg3NSU1RCUyQyUyMCU1QjEzNzUlMkMlMjA1NTAlMkMlMjAxNjUwJTJDJTIwODAwJTVEJTJDJTIwJTVCMTI0MCUyQyUyMDY3NSUyQyUyMDE0MDAlMkMlMjA3NTAlNUQlNUQlMkMlMjAlMjAlMjMlMjBUcnVjayUyMGltYWdlJTNBJTIwNCUyMGJveGVzJTBBJTIwJTIwJTIwJTIwJTVCJTVCNDUwJTJDJTIwMTcwJTJDJTIwNTIwJTJDJTIwMzUwJTVEJTJDJTIwJTVCMzUwJTJDJTIwMTkwJTJDJTIwNDUwJTJDJTIwMzUwJTVEJTJDJTIwJTVCNTAwJTJDJTIwMTcwJTJDJTIwNTgwJTJDJTIwMzUwJTVEJTJDJTIwJTVCNTgwJTJDJTIwMTcwJTJDJTIwNjQwJTJDJTIwMzUwJTVEJTVEJTIwJTIwJTIzJTIwR3JvY2VyaWVzJTIwaW1hZ2UlM0ElMjA0JTIwYm94ZXMlMEElNUQlMEElMEElMjMlMjBVcGRhdGUlMjBpbWFnZXMlMjBmb3IlMjB0aGlzJTIwZXhhbXBsZSUwQXJhd19pbWFnZXMlMjAlM0QlMjAlNUJyYXdfaW1hZ2VzJTVCMCU1RCUyQyUyMGdyb2Nlcmllc19pbWFnZSU1RCUyMCUyMCUyMyUyMHRydWNrJTIwYW5kJTIwZ3JvY2VyaWVzJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRHJhd19pbWFnZXMlMkMlMjBpbnB1dF9ib3hlcyUzRGlucHV0X2JveGVzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjBtdWx0aW1hc2tfb3V0cHV0JTNERmFsc2UpJTBBJTBBYWxsX21hc2tzJTIwJTNEJTIwcHJvY2Vzc29yLnBvc3RfcHJvY2Vzc19tYXNrcyhvdXRwdXRzLnByZWRfbWFza3MuY3B1KCklMkMlMjBpbnB1dHMlNUIlMjJvcmlnaW5hbF9zaXplcyUyMiU1RCklMEFwcmludChmJTIyUHJvY2Vzc2VkJTIwJTdCbGVuKGlucHV0X2JveGVzKSU3RCUyMGltYWdlcyUyMHdpdGglMjAlN0JsZW4oaW5wdXRfYm94ZXMlNUIwJTVEKSU3RCUyMGFuZCUyMCU3QmxlbihpbnB1dF9ib3hlcyU1QjElNUQpJTdEJTIwYm94ZXMlMjByZXNwZWN0aXZlbHklMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple bounding boxes per image (using truck and groceries images)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_boxes = [
<span class="hljs-meta">... </span>    [[<span class="hljs-number">75</span>, <span class="hljs-number">275</span>, <span class="hljs-number">1725</span>, <span class="hljs-number">850</span>], [<span class="hljs-number">425</span>, <span class="hljs-number">600</span>, <span class="hljs-number">700</span>, <span class="hljs-number">875</span>], [<span class="hljs-number">1375</span>, <span class="hljs-number">550</span>, <span class="hljs-number">1650</span>, <span class="hljs-number">800</span>], [<span class="hljs-number">1240</span>, <span class="hljs-number">675</span>, <span class="hljs-number">1400</span>, <span class="hljs-number">750</span>]],  <span class="hljs-comment"># Truck image: 4 boxes</span>
<span class="hljs-meta">... </span>    [[<span class="hljs-number">450</span>, <span class="hljs-number">170</span>, <span class="hljs-number">520</span>, <span class="hljs-number">350</span>], [<span class="hljs-number">350</span>, <span class="hljs-number">190</span>, <span class="hljs-number">450</span>, <span class="hljs-number">350</span>], [<span class="hljs-number">500</span>, <span class="hljs-number">170</span>, <span class="hljs-number">580</span>, <span class="hljs-number">350</span>], [<span class="hljs-number">580</span>, <span class="hljs-number">170</span>, <span class="hljs-number">640</span>, <span class="hljs-number">350</span>]]  <span class="hljs-comment"># Groceries image: 4 boxes</span>
<span class="hljs-meta">... </span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update images for this example</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>raw_images = [raw_images[<span class="hljs-number">0</span>], groceries_image]  <span class="hljs-comment"># truck and groceries</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_images, input_boxes=input_boxes, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs, multimask_output=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>all_masks = processor.post_process_masks(outputs.pred_masks.cpu(), inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Processed <span class="hljs-subst">{<span class="hljs-built_in">len</span>(input_boxes)}</span> images with <span class="hljs-subst">{<span class="hljs-built_in">len</span>(input_boxes[<span class="hljs-number">0</span>])}</span> and <span class="hljs-subst">{<span class="hljs-built_in">len</span>(input_boxes[<span class="hljs-number">1</span>])}</span> boxes respectively&quot;</span>)
Processed <span class="hljs-number">2</span> images <span class="hljs-keyword">with</span> <span class="hljs-number">4</span> <span class="hljs-keyword">and</span> <span class="hljs-number">4</span> boxes respectively`,wrap:!1}}),Ee=new b({props:{title:"Using Previous Masks as Input",local:"using-previous-masks-as-input",headingTag:"h3"}}),Qe=new B({props:{code:"JTIzJTIwR2V0JTIwaW5pdGlhbCUyMHNlZ21lbnRhdGlvbiUwQWlucHV0X3BvaW50cyUyMCUzRCUyMCU1QiU1QiU1QiU1QjUwMCUyQyUyMDM3NSU1RCU1RCU1RCU1RCUwQWlucHV0X2xhYmVscyUyMCUzRCUyMCU1QiU1QiU1QjElNUQlNUQlNUQlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEcmF3X2ltYWdlJTJDJTIwaW5wdXRfcG9pbnRzJTNEaW5wdXRfcG9pbnRzJTJDJTIwaW5wdXRfbGFiZWxzJTNEaW5wdXRfbGFiZWxzJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwVXNlJTIwdGhlJTIwYmVzdCUyMG1hc2slMjBhcyUyMGlucHV0JTIwZm9yJTIwcmVmaW5lbWVudCUwQW1hc2tfaW5wdXQlMjAlM0QlMjBvdXRwdXRzLnByZWRfbWFza3MlNUIlM0ElMkMlMjAlM0ElMkMlMjB0b3JjaC5hcmdtYXgob3V0cHV0cy5pb3Vfc2NvcmVzLnNxdWVlemUoKSklNUQlMEElMEElMjMlMjBBZGQlMjBhZGRpdGlvbmFsJTIwcG9pbnRzJTIwd2l0aCUyMHRoZSUyMG1hc2slMjBpbnB1dCUwQW5ld19pbnB1dF9wb2ludHMlMjAlM0QlMjAlNUIlNUIlNUIlNUI1MDAlMkMlMjAzNzUlNUQlMkMlMjAlNUI0NTAlMkMlMjAzMDAlNUQlNUQlNUQlNUQlMEFuZXdfaW5wdXRfbGFiZWxzJTIwJTNEJTIwJTVCJTVCJTVCMSUyQyUyMDElNUQlNUQlNUQlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTBBJTIwJTIwJTIwJTIwaW5wdXRfcG9pbnRzJTNEbmV3X2lucHV0X3BvaW50cyUyQyUwQSUyMCUyMCUyMCUyMGlucHV0X2xhYmVscyUzRG5ld19pbnB1dF9sYWJlbHMlMkMlMEElMjAlMjAlMjAlMjBvcmlnaW5hbF9zaXplcyUzRGlucHV0cyU1QiUyMm9yaWdpbmFsX3NpemVzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSkudG8oZGV2aWNlKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjByZWZpbmVkX291dHB1dHMlMjAlM0QlMjBtb2RlbCglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAqKmlucHV0cyUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGlucHV0X21hc2tzJTNEbWFza19pbnB1dCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGltYWdlX2VtYmVkZGluZ3MlM0RvdXRwdXRzLmltYWdlX2VtYmVkZGluZ3MlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBtdWx0aW1hc2tfb3V0cHV0JTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjAp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Get initial segmentation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>]]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>input_labels = [[[<span class="hljs-number">1</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Use the best mask as input for refinement</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_input = outputs.pred_masks[:, :, torch.argmax(outputs.iou_scores.squeeze())]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Add additional points with the mask input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>new_input_points = [[[[<span class="hljs-number">500</span>, <span class="hljs-number">375</span>], [<span class="hljs-number">450</span>, <span class="hljs-number">300</span>]]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>new_input_labels = [[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    input_points=new_input_points,
<span class="hljs-meta">... </span>    input_labels=new_input_labels,
<span class="hljs-meta">... </span>    original_sizes=inputs[<span class="hljs-string">&quot;original_sizes&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>).to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    refined_outputs = model(
<span class="hljs-meta">... </span>        **inputs,
<span class="hljs-meta">... </span>        input_masks=mask_input,
<span class="hljs-meta">... </span>        image_embeddings=outputs.image_embeddings,
<span class="hljs-meta">... </span>        multimask_output=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    )`,wrap:!1}}),Pe=new b({props:{title:"Sam2Config",local:"transformers.Sam2Config",headingTag:"h2"}}),Ye=new w({props:{name:"class transformers.Sam2Config",anchor:"transformers.Sam2Config",parameters:[{name:"vision_config",val:" = None"},{name:"prompt_encoder_config",val:" = None"},{name:"mask_decoder_config",val:" = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2Config.vision_config",description:`<strong>vision_config</strong> (Union[<code>dict</code>, <code>Sam2VisionConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionConfig">Sam2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.Sam2Config.prompt_encoder_config",description:`<strong>prompt_encoder_config</strong> (Union[<code>dict</code>, <code>Sam2PromptEncoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2PromptEncoderConfig">Sam2PromptEncoderConfig</a>.`,name:"prompt_encoder_config"},{anchor:"transformers.Sam2Config.mask_decoder_config",description:`<strong>mask_decoder_config</strong> (Union[<code>dict</code>, <code>Sam2MaskDecoderConfig</code>], <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2MaskDecoderConfig">Sam2MaskDecoderConfig</a>.`,name:"mask_decoder_config"},{anchor:"transformers.Sam2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
Standard deviation for parameter initialization.`,name:"initializer_range"},{anchor:"transformers.Sam2Config.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/configuration_sam2.py#L363"}}),Y=new On({props:{anchor:"transformers.Sam2Config.example",$$slots:{default:[io]},$$scope:{ctx:P}}}),De=new b({props:{title:"Sam2HieraDetConfig",local:"transformers.Sam2HieraDetConfig",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.Sam2HieraDetConfig",anchor:"transformers.Sam2HieraDetConfig",parameters:[{name:"hidden_size",val:" = 96"},{name:"num_attention_heads",val:" = 1"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = None"},{name:"patch_kernel_size",val:" = None"},{name:"patch_stride",val:" = None"},{name:"patch_padding",val:" = None"},{name:"query_stride",val:" = None"},{name:"window_positional_embedding_background_size",val:" = None"},{name:"num_query_pool_stages",val:" = 3"},{name:"blocks_per_stage",val:" = None"},{name:"embed_dim_per_stage",val:" = None"},{name:"num_attention_heads_per_stage",val:" = None"},{name:"window_size_per_stage",val:" = None"},{name:"global_attention_blocks",val:" = None"},{name:"mlp_ratio",val:" = 4.0"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2HieraDetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
The hidden dimension of the image encoder.`,name:"hidden_size"},{anchor:"transformers.Sam2HieraDetConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Sam2HieraDetConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of channels in the image.`,name:"num_channels"},{anchor:"transformers.Sam2HieraDetConfig.image_size",description:`<strong>image_size</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1024, 1024]</code>) &#x2014;
The size of the image.`,name:"image_size"},{anchor:"transformers.Sam2HieraDetConfig.patch_kernel_size",description:`<strong>patch_kernel_size</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[7, 7]</code>) &#x2014;
The kernel size of the patch.`,name:"patch_kernel_size"},{anchor:"transformers.Sam2HieraDetConfig.patch_stride",description:`<strong>patch_stride</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[4, 4]</code>) &#x2014;
The stride of the patch.`,name:"patch_stride"},{anchor:"transformers.Sam2HieraDetConfig.patch_padding",description:`<strong>patch_padding</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[3, 3]</code>) &#x2014;
The padding of the patch.`,name:"patch_padding"},{anchor:"transformers.Sam2HieraDetConfig.query_stride",description:`<strong>query_stride</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[2, 2]</code>) &#x2014;
The downsample stride between stages.`,name:"query_stride"},{anchor:"transformers.Sam2HieraDetConfig.window_positional_embedding_background_size",description:`<strong>window_positional_embedding_background_size</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[7, 7]</code>) &#x2014;
The window size per stage when not using global attention.`,name:"window_positional_embedding_background_size"},{anchor:"transformers.Sam2HieraDetConfig.num_query_pool_stages",description:`<strong>num_query_pool_stages</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of query pool stages.`,name:"num_query_pool_stages"},{anchor:"transformers.Sam2HieraDetConfig.blocks_per_stage",description:`<strong>blocks_per_stage</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 7, 2]</code>) &#x2014;
The number of blocks per stage.`,name:"blocks_per_stage"},{anchor:"transformers.Sam2HieraDetConfig.embed_dim_per_stage",description:`<strong>embed_dim_per_stage</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[96, 192, 384, 768]</code>) &#x2014;
The embedding dimension per stage.`,name:"embed_dim_per_stage"},{anchor:"transformers.Sam2HieraDetConfig.num_attention_heads_per_stage",description:`<strong>num_attention_heads_per_stage</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 4, 8]</code>) &#x2014;
The number of attention heads per stage.`,name:"num_attention_heads_per_stage"},{anchor:"transformers.Sam2HieraDetConfig.window_size_per_stage",description:`<strong>window_size_per_stage</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[8, 4, 14, 7]</code>) &#x2014;
The window size per stage.`,name:"window_size_per_stage"},{anchor:"transformers.Sam2HieraDetConfig.global_attention_blocks",description:`<strong>global_attention_blocks</strong> (<code>list[int]</code>, <em>optional</em>, defaults to <code>[5, 7, 9]</code>) &#x2014;
The blocks where global attention is used.`,name:"global_attention_blocks"},{anchor:"transformers.Sam2HieraDetConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
The ratio of the MLP hidden dimension to the embedding dimension.`,name:"mlp_ratio"},{anchor:"transformers.Sam2HieraDetConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the neck.`,name:"hidden_act"},{anchor:"transformers.Sam2HieraDetConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon for the layer normalization.`,name:"layer_norm_eps"},{anchor:"transformers.Sam2HieraDetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/configuration_sam2.py#L25"}}),qe=new b({props:{title:"Sam2VisionConfig",local:"transformers.Sam2VisionConfig",headingTag:"h2"}}),Le=new w({props:{name:"class transformers.Sam2VisionConfig",anchor:"transformers.Sam2VisionConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone_channel_list",val:" = None"},{name:"backbone_feature_sizes",val:" = None"},{name:"fpn_hidden_size",val:" = 256"},{name:"fpn_kernel_size",val:" = 1"},{name:"fpn_stride",val:" = 1"},{name:"fpn_padding",val:" = 0"},{name:"fpn_top_down_levels",val:" = None"},{name:"num_feature_levels",val:" = 3"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2VisionConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>Union[dict, &quot;PretrainedConfig&quot;]</code>, <em>optional</em>) &#x2014;
Configuration for the vision backbone. This is used to instantiate the backbone using
<code>AutoModel.from_config</code>.`,name:"backbone_config"},{anchor:"transformers.Sam2VisionConfig.backbone_channel_list",description:`<strong>backbone_channel_list</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[768, 384, 192, 96]</code>) &#x2014;
The list of channel dimensions for the backbone.`,name:"backbone_channel_list"},{anchor:"transformers.Sam2VisionConfig.backbone_feature_sizes",description:`<strong>backbone_feature_sizes</strong> (<code>List[List[int]]</code>, <em>optional</em>, defaults to <code>[[256, 256], [128, 128], [64, 64]]</code>) &#x2014;
The spatial sizes of the feature maps from the backbone.`,name:"backbone_feature_sizes"},{anchor:"transformers.Sam2VisionConfig.fpn_hidden_size",description:`<strong>fpn_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The hidden dimension of the FPN.`,name:"fpn_hidden_size"},{anchor:"transformers.Sam2VisionConfig.fpn_kernel_size",description:`<strong>fpn_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The kernel size for the convolutions in the neck.`,name:"fpn_kernel_size"},{anchor:"transformers.Sam2VisionConfig.fpn_stride",description:`<strong>fpn_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The stride for the convolutions in the neck.`,name:"fpn_stride"},{anchor:"transformers.Sam2VisionConfig.fpn_padding",description:`<strong>fpn_padding</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The padding for the convolutions in the neck.`,name:"fpn_padding"},{anchor:"transformers.Sam2VisionConfig.fpn_top_down_levels",description:`<strong>fpn_top_down_levels</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 3]</code>) &#x2014;
The levels for the top-down FPN connections.`,name:"fpn_top_down_levels"},{anchor:"transformers.Sam2VisionConfig.num_feature_levels",description:`<strong>num_feature_levels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of feature levels from the FPN to use.`,name:"num_feature_levels"},{anchor:"transformers.Sam2VisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the neck.`,name:"hidden_act"},{anchor:"transformers.Sam2VisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon for the layer normalization.`,name:"layer_norm_eps"},{anchor:"transformers.Sam2VisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/configuration_sam2.py#L144"}}),Ke=new b({props:{title:"Sam2MaskDecoderConfig",local:"transformers.Sam2MaskDecoderConfig",headingTag:"h2"}}),Oe=new w({props:{name:"class transformers.Sam2MaskDecoderConfig",anchor:"transformers.Sam2MaskDecoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"hidden_act",val:" = 'gelu'"},{name:"mlp_dim",val:" = 2048"},{name:"num_hidden_layers",val:" = 2"},{name:"num_attention_heads",val:" = 8"},{name:"attention_downsample_rate",val:" = 2"},{name:"num_multimask_outputs",val:" = 3"},{name:"iou_head_depth",val:" = 3"},{name:"iou_head_hidden_dim",val:" = 256"},{name:"dynamic_multimask_via_stability",val:" = True"},{name:"dynamic_multimask_stability_delta",val:" = 0.05"},{name:"dynamic_multimask_stability_thresh",val:" = 0.98"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2MaskDecoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.Sam2MaskDecoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the SAM2 mask decoder.`,name:"hidden_act"},{anchor:"transformers.Sam2MaskDecoderConfig.mlp_dim",description:`<strong>mlp_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The dimension of the MLP in the two-way transformer.`,name:"mlp_dim"},{anchor:"transformers.Sam2MaskDecoderConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of hidden layers in the two-way transformer.`,name:"num_hidden_layers"},{anchor:"transformers.Sam2MaskDecoderConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The number of attention heads in the two-way transformer.`,name:"num_attention_heads"},{anchor:"transformers.Sam2MaskDecoderConfig.attention_downsample_rate",description:`<strong>attention_downsample_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The downsample rate for the attention layers.`,name:"attention_downsample_rate"},{anchor:"transformers.Sam2MaskDecoderConfig.num_multimask_outputs",description:`<strong>num_multimask_outputs</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of multimask outputs.`,name:"num_multimask_outputs"},{anchor:"transformers.Sam2MaskDecoderConfig.iou_head_depth",description:`<strong>iou_head_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The depth of the IoU head.`,name:"iou_head_depth"},{anchor:"transformers.Sam2MaskDecoderConfig.iou_head_hidden_dim",description:`<strong>iou_head_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The hidden dimension of the IoU head.`,name:"iou_head_hidden_dim"},{anchor:"transformers.Sam2MaskDecoderConfig.dynamic_multimask_via_stability",description:`<strong>dynamic_multimask_via_stability</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use dynamic multimask via stability.`,name:"dynamic_multimask_via_stability"},{anchor:"transformers.Sam2MaskDecoderConfig.dynamic_multimask_stability_delta",description:`<strong>dynamic_multimask_stability_delta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
The stability delta for the dynamic multimask.`,name:"dynamic_multimask_stability_delta"},{anchor:"transformers.Sam2MaskDecoderConfig.dynamic_multimask_stability_thresh",description:`<strong>dynamic_multimask_stability_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.98) &#x2014;
The stability threshold for the dynamic multimask.`,name:"dynamic_multimask_stability_thresh"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/configuration_sam2.py#L290"}}),es=new b({props:{title:"Sam2PromptEncoderConfig",local:"transformers.Sam2PromptEncoderConfig",headingTag:"h2"}}),ss=new w({props:{name:"class transformers.Sam2PromptEncoderConfig",anchor:"transformers.Sam2PromptEncoderConfig",parameters:[{name:"hidden_size",val:" = 256"},{name:"image_size",val:" = 1024"},{name:"patch_size",val:" = 16"},{name:"mask_input_channels",val:" = 16"},{name:"num_point_embeddings",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"scale",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2PromptEncoderConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden states.`,name:"hidden_size"},{anchor:"transformers.Sam2PromptEncoderConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The expected output resolution of the image.`,name:"image_size"},{anchor:"transformers.Sam2PromptEncoderConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Sam2PromptEncoderConfig.mask_input_channels",description:`<strong>mask_input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The number of channels to be fed to the <code>MaskDecoder</code> module.`,name:"mask_input_channels"},{anchor:"transformers.Sam2PromptEncoderConfig.num_point_embeddings",description:`<strong>num_point_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of point embeddings to be used.`,name:"num_point_embeddings"},{anchor:"transformers.Sam2PromptEncoderConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.Sam2PromptEncoderConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Sam2PromptEncoderConfig.scale",description:`<strong>scale</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The scale factor for the prompt encoder.`,name:"scale"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/configuration_sam2.py#L238"}}),ts=new b({props:{title:"Sam2Processor",local:"transformers.Sam2Processor",headingTag:"h2"}}),as=new w({props:{name:"class transformers.Sam2Processor",anchor:"transformers.Sam2Processor",parameters:[{name:"image_processor",val:""},{name:"target_size",val:": typing.Optional[int] = None"},{name:"point_pad_value",val:": int = -10"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2Processor.image_processor",description:`<strong>image_processor</strong> (<code>Sam2ImageProcessorFast</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2ImageProcessorFast">Sam2ImageProcessorFast</a>.`,name:"image_processor"},{anchor:"transformers.Sam2Processor.target_size",description:`<strong>target_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The target size (target_size, target_size) to which the image will be resized.`,name:"target_size"},{anchor:"transformers.Sam2Processor.point_pad_value",description:`<strong>point_pad_value</strong> (<code>int</code>, <em>optional</em>, defaults to -10) &#x2014;
The value used for padding input points.`,name:"point_pad_value"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/processing_sam2.py#L38"}}),ns=new w({props:{name:"__call__",anchor:"transformers.Sam2Processor.__call__",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None"},{name:"segmentation_maps",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']] = None"},{name:"input_points",val:": typing.Union[list[list[list[list[float]]]], torch.Tensor, NoneType] = None"},{name:"input_labels",val:": typing.Union[list[list[list[int]]], torch.Tensor, NoneType] = None"},{name:"input_boxes",val:": typing.Union[list[list[list[float]]], torch.Tensor, NoneType] = None"},{name:"original_sizes",val:": typing.Union[list[list[float]], torch.Tensor, NoneType] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2Processor.__call__.images",description:`<strong>images</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The image(s) to process.`,name:"images"},{anchor:"transformers.Sam2Processor.__call__.segmentation_maps",description:`<strong>segmentation_maps</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The segmentation maps to process.`,name:"segmentation_maps"},{anchor:"transformers.Sam2Processor.__call__.input_points",description:`<strong>input_points</strong> (<code>list[list[list[list[float]]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The points to add to the frame.`,name:"input_points"},{anchor:"transformers.Sam2Processor.__call__.input_labels",description:`<strong>input_labels</strong> (<code>list[list[list[int]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The labels for the points.`,name:"input_labels"},{anchor:"transformers.Sam2Processor.__call__.input_boxes",description:`<strong>input_boxes</strong> (<code>list[list[list[float]]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The bounding boxes to add to the frame.`,name:"input_boxes"},{anchor:"transformers.Sam2Processor.__call__.original_sizes",description:`<strong>original_sizes</strong> (<code>list[list[float]]</code>, <code>torch.Tensor</code>, <em>optional</em>) &#x2014;
The original sizes of the images.`,name:"original_sizes"},{anchor:"transformers.Sam2Processor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return.`,name:"return_tensors"},{anchor:"transformers.Sam2Processor.__call__.*kwargs",description:`*<strong>*kwargs</strong> &#x2014;
Additional keyword arguments to pass to the image processor.`,name:"*kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/processing_sam2.py#L63",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><code>pixel_values</code> (<code>torch.Tensor</code>): The processed image(s).</li>
<li><code>original_sizes</code> (<code>list[list[float]]</code>): The original sizes of the images.</li>
<li><code>reshaped_input_sizes</code> (<code>torch.Tensor</code>): The reshaped input sizes of the images.</li>
<li><code>labels</code> (<code>torch.Tensor</code>): The processed segmentation maps (if provided).</li>
<li><code>input_points</code> (<code>torch.Tensor</code>): The processed points.</li>
<li><code>input_labels</code> (<code>torch.Tensor</code>): The processed labels.</li>
<li><code>input_boxes</code> (<code>torch.Tensor</code>): The processed bounding boxes.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields</p>
`}}),os=new w({props:{name:"post_process_masks",anchor:"transformers.Sam2Processor.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"max_hole_area",val:" = 0.0"},{name:"max_sprinkle_area",val:" = 0.0"},{name:"apply_non_overlapping_constraints",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2Processor.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[List[torch.Tensor], List[np.ndarray]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.Sam2Processor.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.Sam2Processor.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold for binarization and post-processing operations.`,name:"mask_threshold"},{anchor:"transformers.Sam2Processor.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.Sam2Processor.post_process_masks.max_hole_area",description:`<strong>max_hole_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a hole to fill.`,name:"max_hole_area"},{anchor:"transformers.Sam2Processor.post_process_masks.max_sprinkle_area",description:`<strong>max_sprinkle_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a sprinkle to fill.`,name:"max_sprinkle_area"},{anchor:"transformers.Sam2Processor.post_process_masks.apply_non_overlapping_constraints",description:`<strong>apply_non_overlapping_constraints</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply non-overlapping constraints to the masks.`,name:"apply_non_overlapping_constraints"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/processing_sam2.py#L479",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where (height, width)
is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>torch.Tensor</code>)</p>
`}}),rs=new b({props:{title:"Sam2ImageProcessorFast",local:"transformers.Sam2ImageProcessorFast",headingTag:"h2"}}),ls=new w({props:{name:"class transformers.Sam2ImageProcessorFast",anchor:"transformers.Sam2ImageProcessorFast",parameters:[{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.sam2.image_processing_sam2_fast.Sam2FastImageProcessorKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L380"}}),is=new w({props:{name:"filter_masks",anchor:"transformers.Sam2ImageProcessorFast.filter_masks",parameters:[{name:"masks",val:""},{name:"iou_scores",val:""},{name:"original_size",val:""},{name:"cropped_box_image",val:""},{name:"pred_iou_thresh",val:" = 0.88"},{name:"stability_score_thresh",val:" = 0.95"},{name:"mask_threshold",val:" = 0"},{name:"stability_score_offset",val:" = 1"}],parametersDescription:[{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.masks",description:`<strong>masks</strong> (<code>torch.Tensor</code>) &#x2014;
Input masks.`,name:"masks"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.iou_scores",description:`<strong>iou_scores</strong> (<code>torch.Tensor</code>) &#x2014;
List of IoU scores.`,name:"iou_scores"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.original_size",description:`<strong>original_size</strong> (<code>tuple[int,int]</code>) &#x2014;
Size of the original image.`,name:"original_size"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.cropped_box_image",description:`<strong>cropped_box_image</strong> (<code>torch.Tensor</code>) &#x2014;
The cropped image.`,name:"cropped_box_image"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.pred_iou_thresh",description:`<strong>pred_iou_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.88) &#x2014;
The threshold for the iou scores.`,name:"pred_iou_thresh"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.stability_score_thresh",description:`<strong>stability_score_thresh</strong> (<code>float</code>, <em>optional</em>, defaults to 0.95) &#x2014;
The threshold for the stability score.`,name:"stability_score_thresh"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The threshold for the predicted masks.`,name:"mask_threshold"},{anchor:"transformers.Sam2ImageProcessorFast.filter_masks.stability_score_offset",description:`<strong>stability_score_offset</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The offset for the stability score used in the <code>_compute_stability_score</code> method.`,name:"stability_score_offset"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L568"}}),ms=new w({props:{name:"generate_crop_boxes",anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes",parameters:[{name:"image",val:": torch.Tensor"},{name:"target_size",val:""},{name:"crop_n_layers",val:": int = 0"},{name:"overlap_ratio",val:": float = 0.3413333333333333"},{name:"points_per_crop",val:": typing.Optional[int] = 32"},{name:"crop_n_points_downscale_factor",val:": typing.Optional[list[int]] = 1"},{name:"device",val:": typing.Optional[ForwardRef('torch.device')] = None"}],parametersDescription:[{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.image",description:`<strong>image</strong> (<code>torch.Tensor</code>) &#x2014;
Input original image`,name:"image"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.target_size",description:`<strong>target_size</strong> (<code>int</code>) &#x2014;
Target size of the resized image`,name:"target_size"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.crop_n_layers",description:`<strong>crop_n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where
each layer has 2**i_layer number of image crops.`,name:"crop_n_layers"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.overlap_ratio",description:`<strong>overlap_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 512/1500) &#x2014;
Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of
the image length. Later layers with more crops scale down this overlap.`,name:"overlap_ratio"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.points_per_crop",description:`<strong>points_per_crop</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of points to sam2ple from each crop.`,name:"points_per_crop"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.crop_n_points_downscale_factor",description:`<strong>crop_n_points_downscale_factor</strong> (<code>list[int]</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of points-per-side sam2pled in layer n is scaled down by crop_n_points_downscale_factor**n.`,name:"crop_n_points_downscale_factor"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>, defaults to None) &#x2014;
Device to use for the computation. If None, cpu will be used.`,name:"device"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred.`,name:"input_data_format"},{anchor:"transformers.Sam2ImageProcessorFast.generate_crop_boxes.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>pt</code>) &#x2014;
If <code>pt</code>, returns <code>torch.Tensor</code>. If <code>tf</code>, returns <code>tf.Tensor</code>.`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L515"}}),cs=new w({props:{name:"post_process_for_mask_generation",anchor:"transformers.Sam2ImageProcessorFast.post_process_for_mask_generation",parameters:[{name:"all_masks",val:""},{name:"all_scores",val:""},{name:"all_boxes",val:""},{name:"crops_nms_thresh",val:""}],parametersDescription:[{anchor:"transformers.Sam2ImageProcessorFast.post_process_for_mask_generation.all_masks",description:`<strong>all_masks</strong> (<code>torch.Tensor</code>) &#x2014;
List of all predicted segmentation masks`,name:"all_masks"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_for_mask_generation.all_scores",description:`<strong>all_scores</strong> (<code>torch.Tensor</code>) &#x2014;
List of all predicted iou scores`,name:"all_scores"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_for_mask_generation.all_boxes",description:`<strong>all_boxes</strong> (<code>torch.Tensor</code>) &#x2014;
List of all bounding boxes of the predicted masks`,name:"all_boxes"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_for_mask_generation.crops_nms_thresh",description:`<strong>crops_nms_thresh</strong> (<code>float</code>) &#x2014;
Threshold for NMS (Non Maximum Suppression) algorithm.`,name:"crops_nms_thresh"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L700"}}),ps=new w({props:{name:"post_process_masks",anchor:"transformers.Sam2ImageProcessorFast.post_process_masks",parameters:[{name:"masks",val:""},{name:"original_sizes",val:""},{name:"mask_threshold",val:" = 0.0"},{name:"binarize",val:" = True"},{name:"max_hole_area",val:" = 0.0"},{name:"max_sprinkle_area",val:" = 0.0"},{name:"apply_non_overlapping_constraints",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.masks",description:`<strong>masks</strong> (<code>Union[torch.Tensor, List[torch.Tensor], np.ndarray, List[np.ndarray]]</code>) &#x2014;
Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.`,name:"masks"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.original_sizes",description:`<strong>original_sizes</strong> (<code>Union[torch.Tensor, List[Tuple[int,int]]]</code>) &#x2014;
The original sizes of each image before it was resized to the model&#x2019;s expected input shape, in (height,
width) format.`,name:"original_sizes"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.mask_threshold",description:`<strong>mask_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Threshold for binarization and post-processing operations.`,name:"mask_threshold"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.binarize",description:`<strong>binarize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to binarize the masks.`,name:"binarize"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.max_hole_area",description:`<strong>max_hole_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a hole to fill.`,name:"max_hole_area"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.max_sprinkle_area",description:`<strong>max_sprinkle_area</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The maximum area of a sprinkle to fill.`,name:"max_sprinkle_area"},{anchor:"transformers.Sam2ImageProcessorFast.post_process_masks.apply_non_overlapping_constraints",description:`<strong>apply_non_overlapping_constraints</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply non-overlapping constraints to the masks.`,name:"apply_non_overlapping_constraints"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L647",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Batched masks in batch_size, num_channels, height, width) format, where (height, width)
is given by original_size.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(<code>torch.Tensor</code>)</p>
`}}),ds=new w({props:{name:"preprocess",anchor:"transformers.Sam2ImageProcessorFast.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor']]"},{name:"segmentation_maps",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), list['PIL.Image.Image'], list[numpy.ndarray], list['torch.Tensor'], NoneType] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.models.sam2.image_processing_sam2_fast.Sam2FastImageProcessorKwargs]"}],parametersDescription:[{anchor:"transformers.Sam2ImageProcessorFast.preprocess.images",description:`<strong>images</strong> (<code>Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, list[&apos;PIL.Image.Image&apos;], list[numpy.ndarray], list[&apos;torch.Tensor&apos;]]</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.segmentation_maps",description:`<strong>segmentation_maps</strong> (<code>ImageInput</code>, <em>optional</em>) &#x2014;
The segmentation maps to preprocess.`,name:"segmentation_maps"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.size",description:`<strong>size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Describes the maximum input dimensions to the model.`,name:"size"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to default to a square image when resizing, if size is an int.`,name:"default_to_square"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.resample",description:`<strong>resample</strong> (<code>Union[PILImageResampling, F.InterpolationMode, NoneType]</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
Size of the output image after applying <code>center_crop</code>.`,name:"crop_size"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>Union[int, float, NoneType]</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.image_std",description:`<strong>image_std</strong> (<code>Union[float, list[float], NoneType]</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.return_tensors",description:"<strong>return_tensors</strong> (<code>Union[str, ~utils.generic.TensorType, NoneType]</code>) &#x2014;\nReturns stacked tensors if set to `pt, otherwise returns a list of tensors.",name:"return_tensors"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.data_format",description:`<strong>data_format</strong> (<code>~image_utils.ChannelDimension</code>, <em>optional</em>) &#x2014;
Only <code>ChannelDimension.FIRST</code> is supported. Added for compatibility with slow processors.`,name:"data_format"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>Union[str, ~image_utils.ChannelDimension, NoneType]</code>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
The device to process the images on. If unset, the device is inferred from the input images.`,name:"device"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.disable_grouping",description:`<strong>disable_grouping</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to disable grouping of images by size to process them individually and not in batches.
If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on
empirical observations, as detailed here: <a href="https://github.com/huggingface/transformers/pull/38157" rel="nofollow">https://github.com/huggingface/transformers/pull/38157</a>`,name:"disable_grouping"},{anchor:"transformers.Sam2ImageProcessorFast.preprocess.mask_size",description:`<strong>mask_size</strong> (<code>dict[str, int]</code>, <em>optional</em>) &#x2014;
The size <code>{&quot;height&quot;: int, &quot;width&quot;: int}</code> to resize the segmentation maps to.`,name:"mask_size"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/image_processing_sam2_fast.py#L445",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>data</strong> (<code>dict</code>) — Dictionary of lists/arrays/tensors returned by the <strong>call</strong> method (‘pixel_values’, etc.).</li>
<li><strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) — You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>&lt;class 'transformers.image_processing_base.BatchFeature'&gt;</code></p>
`}}),gs=new b({props:{title:"Sam2HieraDetModel",local:"transformers.Sam2HieraDetModel",headingTag:"h2"}}),hs=new w({props:{name:"class transformers.Sam2HieraDetModel",anchor:"transformers.Sam2HieraDetModel",parameters:[{name:"config",val:": Sam2HieraDetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L583"}}),fs=new w({props:{name:"forward",anchor:"transformers.Sam2HieraDetModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L624"}}),us=new b({props:{title:"Sam2VisionModel",local:"transformers.Sam2VisionModel",headingTag:"h2"}}),_s=new w({props:{name:"class transformers.Sam2VisionModel",anchor:"transformers.Sam2VisionModel",parameters:[{name:"config",val:": Sam2VisionConfig"}],parametersDescription:[{anchor:"transformers.Sam2VisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2VisionConfig">Sam2VisionConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L654"}}),bs=new w({props:{name:"forward",anchor:"transformers.Sam2VisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L676"}}),Ms=new b({props:{title:"Sam2Model",local:"transformers.Sam2Model",headingTag:"h2"}}),ys=new w({props:{name:"class transformers.Sam2Model",anchor:"transformers.Sam2Model",parameters:[{name:"config",val:": Sam2Config"}],parametersDescription:[{anchor:"transformers.Sam2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config">Sam2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/v4.56.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L1279"}}),Ts=new w({props:{name:"forward",anchor:"transformers.Sam2Model.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_points",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_boxes",val:": typing.Optional[torch.FloatTensor] = None"},{name:"input_masks",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_embeddings",val:": typing.Optional[torch.FloatTensor] = None"},{name:"multimask_output",val:": bool = True"},{name:"attention_similarity",val:": typing.Optional[torch.FloatTensor] = None"},{name:"target_embedding",val:": typing.Optional[torch.FloatTensor] = None"},{name:"**kwargs",val:": typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]"}],parametersDescription:[{anchor:"transformers.Sam2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)</code>, <em>optional</em>) &#x2014;
The tensors corresponding to the input images. Pixel values can be obtained using
<code>image_processor_class</code>. See <code>image_processor_class.__call__</code> for details (<a href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Processor">Sam2Processor</a> uses
<code>image_processor_class</code> for processing images).`,name:"pixel_values"},{anchor:"transformers.Sam2Model.forward.input_points",description:`<strong>input_points</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_points, 2)</code>) &#x2014;
Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much
better results. The points can be obtained by passing a list of list of list to the processor that will
create corresponding <code>torch</code> tensors of dimension 4. The first dimension is the image batch size, the
second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict
per input point), the third dimension is the number of points per segmentation mask (it is possible to pass
multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)
coordinates of the point. If a different number of points is passed either for each image, or for each
mask, the processor will create &#x201C;PAD&#x201D; points that will correspond to the (0, 0) coordinate, and the
computation of the embedding will be skipped for these points using the labels.`,name:"input_points"},{anchor:"transformers.Sam2Model.forward.input_labels",description:`<strong>input_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, point_batch_size, num_points)</code>) &#x2014;
Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the
official implementation, there are 3 types of labels</p>
<ul>
<li><code>1</code>: the point is a point that contains the object of interest</li>
<li><code>0</code>: the point is a point that does not contain the object of interest</li>
<li><code>-1</code>: the point corresponds to the background</li>
</ul>
<p>We added the label:</p>
<ul>
<li><code>-10</code>: the point is a padding point, thus should be ignored by the prompt encoder</li>
</ul>
<p>The padding labels should be automatically done by the processor.`,name:"input_labels"},{anchor:"transformers.Sam2Model.forward.input_boxes",description:`<strong>input_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_boxes, 4)</code>) &#x2014;
Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to
much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,
that will generate a <code>torch</code> tensor, with each dimension corresponding respectively to the image batch
size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.
In the order (<code>x1</code>, <code>y1</code>, <code>x2</code>, <code>y2</code>):</p>
<ul>
<li><code>x1</code>: the x coordinate of the top left point of the input box</li>
<li><code>y1</code>: the y coordinate of the top left point of the input box</li>
<li><code>x2</code>: the x coordinate of the bottom right point of the input box</li>
<li><code>y2</code>: the y coordinate of the bottom right point of the input box</li>
</ul>`,name:"input_boxes"},{anchor:"transformers.Sam2Model.forward.input_masks",description:`<strong>input_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_size, image_size)</code>) &#x2014;
SAM model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to
generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be
manually fed by the user, and they need to be of shape (<code>batch_size</code>, <code>image_size</code>, <code>image_size</code>).`,name:"input_masks"},{anchor:"transformers.Sam2Model.forward.image_embeddings",description:`<strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_channels, window_size, window_size)</code>) &#x2014;
Image embeddings, this is used by the mask decoder to generate masks and iou scores. For more memory
efficient computation, users can first retrieve the image embeddings using the <code>get_image_embeddings</code>
method, and then feed them to the <code>forward</code> method instead of feeding the <code>pixel_values</code>.`,name:"image_embeddings"},{anchor:"transformers.Sam2Model.forward.multimask_output",description:`<strong>multimask_output</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
In the original implementation and paper, the model always outputs 3 masks per image (or per point / per
bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the
&#x201C;best&#x201D; mask, by specifying <code>multimask_output=False</code>.`,name:"multimask_output"},{anchor:"transformers.Sam2Model.forward.attention_similarity",description:`<strong>attention_similarity</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the
model is used for personalization as introduced in <a href="https://huggingface.co/papers/2305.03048" rel="nofollow">PerSAM</a>.`,name:"attention_similarity"},{anchor:"transformers.Sam2Model.forward.target_embedding",description:`<strong>target_embedding</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case
the model is used for personalization as introduced in <a href="https://huggingface.co/papers/2305.03048" rel="nofollow">PerSAM</a>.`,name:"target_embedding"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/sam2/modeling_sam2.py#L1392",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.sam2.modeling_sam2.Sam2ImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.56.2/en/model_doc/sam2#transformers.Sam2Config"
>Sam2Config</a>) and inputs.</p>
<ul>
<li><strong>iou_scores</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, point_batch_size, num_masks)</code>) — The Intersection over Union (IoU) scores of the predicted masks.</li>
<li><strong>pred_masks</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, point_batch_size, num_masks, height, width)</code>) — The predicted low-resolution masks. This is an alias for <code>low_res_masks</code>. These masks need to be post-processed
by the processor to be brought to the original image size.</li>
<li><strong>object_score_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, point_batch_size, 1)</code>) — Logits for the object score, indicating if an object is present.</li>
<li><strong>image_embeddings</strong> (<code>tuple(torch.FloatTensor)</code>) — The features from the FPN, which are used by the mask decoder. This is a tuple of <code>torch.FloatTensor</code> where each
tensor has shape <code>(batch_size, channels, height, width)</code>.</li>
<li><strong>vision_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, height, width, hidden_size)</code>.
Hidden-states of the vision model at the output of each stage.</li>
<li><strong>vision_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.
Attentions weights of the vision model.</li>
<li><strong>mask_decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.
Attentions weights of the mask decoder.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.sam2.modeling_sam2.Sam2ImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ee=new ro({props:{$$slots:{default:[mo]},$$scope:{ctx:P}}}),se=new On({props:{anchor:"transformers.Sam2Model.forward.example",$$slots:{default:[co]},$$scope:{ctx:P}}}),js=new lo({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam2.md"}}),{c(){u=l("meta"),z=a(),j=l("p"),M=a(),U=l("div"),U.innerHTML=_,x=a(),m(ae.$$.fragment),st=a(),m(ne.$$.fragment),tt=a(),oe=l("p"),oe.innerHTML=on,at=a(),re=l("p"),re.textContent=rn,nt=a(),le=l("p"),le.innerHTML=ln,ot=a(),ie=l("p"),ie.textContent=mn,rt=a(),me=l("p"),me.innerHTML=cn,lt=a(),ce=l("p"),ce.textContent=pn,it=a(),pe=l("ul"),pe.innerHTML=dn,mt=a(),de=l("p"),de.innerHTML=gn,ct=a(),m(ge.$$.fragment),pt=a(),m(he.$$.fragment),dt=a(),fe=l("p"),fe.innerHTML=hn,gt=a(),m(ue.$$.fragment),ht=a(),m(_e.$$.fragment),ft=a(),m(be.$$.fragment),ut=a(),Me=l("p"),Me.textContent=fn,_t=a(),m(ye.$$.fragment),bt=a(),m(Te.$$.fragment),Mt=a(),je=l("p"),je.textContent=un,yt=a(),m(we.$$.fragment),Tt=a(),m(Je.$$.fragment),jt=a(),Ue=l("p"),Ue.textContent=_n,wt=a(),m(ve.$$.fragment),Jt=a(),m(Ie.$$.fragment),Ut=a(),ke=l("p"),ke.textContent=bn,vt=a(),m(Ce.$$.fragment),It=a(),m(ze.$$.fragment),kt=a(),m($e.$$.fragment),Ct=a(),xe=l("p"),xe.textContent=Mn,zt=a(),m(Se.$$.fragment),$t=a(),m(Be.$$.fragment),xt=a(),We=l("p"),We.textContent=yn,St=a(),m(Ne.$$.fragment),Bt=a(),m(Ve.$$.fragment),Wt=a(),Ze=l("p"),Ze.textContent=Tn,Nt=a(),m(Re.$$.fragment),Vt=a(),m(Fe.$$.fragment),Zt=a(),Ge=l("p"),Ge.textContent=jn,Rt=a(),m(Xe.$$.fragment),Ft=a(),m(Ee.$$.fragment),Gt=a(),He=l("p"),He.textContent=wn,Xt=a(),m(Qe.$$.fragment),Et=a(),m(Pe.$$.fragment),Ht=a(),$=l("div"),m(Ye.$$.fragment),Ma=a(),ws=l("p"),ws.innerHTML=Jn,ya=a(),Js=l("p"),Js.innerHTML=Un,Ta=a(),m(Y.$$.fragment),Qt=a(),m(De.$$.fragment),Pt=a(),W=l("div"),m(Ae.$$.fragment),ja=a(),Us=l("p"),Us.innerHTML=vn,wa=a(),vs=l("p"),vs.innerHTML=In,Yt=a(),m(qe.$$.fragment),Dt=a(),N=l("div"),m(Le.$$.fragment),Ja=a(),Is=l("p"),Is.innerHTML=kn,Ua=a(),ks=l("p"),ks.innerHTML=Cn,At=a(),m(Ke.$$.fragment),qt=a(),V=l("div"),m(Oe.$$.fragment),va=a(),Cs=l("p"),Cs.innerHTML=zn,Ia=a(),zs=l("p"),zs.innerHTML=$n,Lt=a(),m(es.$$.fragment),Kt=a(),Z=l("div"),m(ss.$$.fragment),ka=a(),$s=l("p"),$s.innerHTML=xn,Ca=a(),xs=l("p"),xs.innerHTML=Sn,Ot=a(),m(ts.$$.fragment),ea=a(),v=l("div"),m(as.$$.fragment),za=a(),Ss=l("p"),Ss.textContent=Bn,$a=a(),Bs=l("p"),Bs.innerHTML=Wn,xa=a(),D=l("div"),m(ns.$$.fragment),Sa=a(),Ws=l("p"),Ws.innerHTML=Nn,Ba=a(),A=l("div"),m(os.$$.fragment),Wa=a(),Ns=l("p"),Ns.textContent=Vn,sa=a(),m(rs.$$.fragment),ta=a(),J=l("div"),m(ls.$$.fragment),Na=a(),Vs=l("p"),Vs.textContent=Zn,Va=a(),q=l("div"),m(is.$$.fragment),Za=a(),Zs=l("p"),Zs.innerHTML=Rn,Ra=a(),L=l("div"),m(ms.$$.fragment),Fa=a(),Rs=l("p"),Rs.innerHTML=Fn,Ga=a(),K=l("div"),m(cs.$$.fragment),Xa=a(),Fs=l("p"),Fs.textContent=Gn,Ea=a(),O=l("div"),m(ps.$$.fragment),Ha=a(),Gs=l("p"),Gs.textContent=Xn,Qa=a(),Xs=l("div"),m(ds.$$.fragment),aa=a(),m(gs.$$.fragment),na=a(),E=l("div"),m(hs.$$.fragment),Pa=a(),Es=l("div"),m(fs.$$.fragment),oa=a(),m(us.$$.fragment),ra=a(),I=l("div"),m(_s.$$.fragment),Ya=a(),Hs=l("p"),Hs.textContent=En,Da=a(),Qs=l("p"),Qs.innerHTML=Hn,Aa=a(),Ps=l("p"),Ps.innerHTML=Qn,qa=a(),Ys=l("div"),m(bs.$$.fragment),la=a(),m(Ms.$$.fragment),ia=a(),k=l("div"),m(ys.$$.fragment),La=a(),Ds=l("p"),Ds.textContent=Pn,Ka=a(),As=l("p"),As.innerHTML=Yn,Oa=a(),qs=l("p"),qs.innerHTML=Dn,en=a(),S=l("div"),m(Ts.$$.fragment),sn=a(),Ls=l("p"),Ls.innerHTML=An,tn=a(),m(ee.$$.fragment),an=a(),m(se.$$.fragment),ma=a(),m(js.$$.fragment),ca=a(),et=l("p"),this.h()},l(e){const s=no("svelte-u9bgzb",document.head);u=i(s,"META",{name:!0,content:!0}),s.forEach(t),z=n(e),j=i(e,"P",{}),y(j).forEach(t),M=n(e),U=i(e,"DIV",{style:!0,"data-svelte-h":!0}),f(U)!=="svelte-1dwwnh7"&&(U.innerHTML=_),x=n(e),c(ae.$$.fragment,e),st=n(e),c(ne.$$.fragment,e),tt=n(e),oe=i(e,"P",{"data-svelte-h":!0}),f(oe)!=="svelte-1yalyum"&&(oe.innerHTML=on),at=n(e),re=i(e,"P",{"data-svelte-h":!0}),f(re)!=="svelte-sl1nob"&&(re.textContent=rn),nt=n(e),le=i(e,"P",{"data-svelte-h":!0}),f(le)!=="svelte-vowmqa"&&(le.innerHTML=ln),ot=n(e),ie=i(e,"P",{"data-svelte-h":!0}),f(ie)!=="svelte-vfdo9a"&&(ie.textContent=mn),rt=n(e),me=i(e,"P",{"data-svelte-h":!0}),f(me)!=="svelte-1grlhd0"&&(me.innerHTML=cn),lt=n(e),ce=i(e,"P",{"data-svelte-h":!0}),f(ce)!=="svelte-axv494"&&(ce.textContent=pn),it=n(e),pe=i(e,"UL",{"data-svelte-h":!0}),f(pe)!=="svelte-htibf2"&&(pe.innerHTML=dn),mt=n(e),de=i(e,"P",{"data-svelte-h":!0}),f(de)!=="svelte-13u8sgy"&&(de.innerHTML=gn),ct=n(e),c(ge.$$.fragment,e),pt=n(e),c(he.$$.fragment,e),dt=n(e),fe=i(e,"P",{"data-svelte-h":!0}),f(fe)!=="svelte-1wtxo2k"&&(fe.innerHTML=hn),gt=n(e),c(ue.$$.fragment,e),ht=n(e),c(_e.$$.fragment,e),ft=n(e),c(be.$$.fragment,e),ut=n(e),Me=i(e,"P",{"data-svelte-h":!0}),f(Me)!=="svelte-13mmbkc"&&(Me.textContent=fn),_t=n(e),c(ye.$$.fragment,e),bt=n(e),c(Te.$$.fragment,e),Mt=n(e),je=i(e,"P",{"data-svelte-h":!0}),f(je)!=="svelte-1vg2caq"&&(je.textContent=un),yt=n(e),c(we.$$.fragment,e),Tt=n(e),c(Je.$$.fragment,e),jt=n(e),Ue=i(e,"P",{"data-svelte-h":!0}),f(Ue)!=="svelte-1uzxvjd"&&(Ue.textContent=_n),wt=n(e),c(ve.$$.fragment,e),Jt=n(e),c(Ie.$$.fragment,e),Ut=n(e),ke=i(e,"P",{"data-svelte-h":!0}),f(ke)!=="svelte-jrx9p0"&&(ke.textContent=bn),vt=n(e),c(Ce.$$.fragment,e),It=n(e),c(ze.$$.fragment,e),kt=n(e),c($e.$$.fragment,e),Ct=n(e),xe=i(e,"P",{"data-svelte-h":!0}),f(xe)!=="svelte-8cjtrx"&&(xe.textContent=Mn),zt=n(e),c(Se.$$.fragment,e),$t=n(e),c(Be.$$.fragment,e),xt=n(e),We=i(e,"P",{"data-svelte-h":!0}),f(We)!=="svelte-h4po15"&&(We.textContent=yn),St=n(e),c(Ne.$$.fragment,e),Bt=n(e),c(Ve.$$.fragment,e),Wt=n(e),Ze=i(e,"P",{"data-svelte-h":!0}),f(Ze)!=="svelte-16tbdbc"&&(Ze.textContent=Tn),Nt=n(e),c(Re.$$.fragment,e),Vt=n(e),c(Fe.$$.fragment,e),Zt=n(e),Ge=i(e,"P",{"data-svelte-h":!0}),f(Ge)!=="svelte-1898udf"&&(Ge.textContent=jn),Rt=n(e),c(Xe.$$.fragment,e),Ft=n(e),c(Ee.$$.fragment,e),Gt=n(e),He=i(e,"P",{"data-svelte-h":!0}),f(He)!=="svelte-1fy8k46"&&(He.textContent=wn),Xt=n(e),c(Qe.$$.fragment,e),Et=n(e),c(Pe.$$.fragment,e),Ht=n(e),$=i(e,"DIV",{class:!0});var R=y($);c(Ye.$$.fragment,R),Ma=n(R),ws=i(R,"P",{"data-svelte-h":!0}),f(ws)!=="svelte-18z3dap"&&(ws.innerHTML=Jn),ya=n(R),Js=i(R,"P",{"data-svelte-h":!0}),f(Js)!=="svelte-1ek1ss9"&&(Js.innerHTML=Un),Ta=n(R),c(Y.$$.fragment,R),R.forEach(t),Qt=n(e),c(De.$$.fragment,e),Pt=n(e),W=i(e,"DIV",{class:!0});var H=y(W);c(Ae.$$.fragment,H),ja=n(H),Us=i(H,"P",{"data-svelte-h":!0}),f(Us)!=="svelte-1bpa72v"&&(Us.innerHTML=vn),wa=n(H),vs=i(H,"P",{"data-svelte-h":!0}),f(vs)!=="svelte-1ek1ss9"&&(vs.innerHTML=In),H.forEach(t),Yt=n(e),c(qe.$$.fragment,e),Dt=n(e),N=i(e,"DIV",{class:!0});var Q=y(N);c(Le.$$.fragment,Q),Ja=n(Q),Is=i(Q,"P",{"data-svelte-h":!0}),f(Is)!=="svelte-19326va"&&(Is.innerHTML=kn),Ua=n(Q),ks=i(Q,"P",{"data-svelte-h":!0}),f(ks)!=="svelte-1ek1ss9"&&(ks.innerHTML=Cn),Q.forEach(t),At=n(e),c(Ke.$$.fragment,e),qt=n(e),V=i(e,"DIV",{class:!0});var Ks=y(V);c(Oe.$$.fragment,Ks),va=n(Ks),Cs=i(Ks,"P",{"data-svelte-h":!0}),f(Cs)!=="svelte-1abq9yj"&&(Cs.innerHTML=zn),Ia=n(Ks),zs=i(Ks,"P",{"data-svelte-h":!0}),f(zs)!=="svelte-1ek1ss9"&&(zs.innerHTML=$n),Ks.forEach(t),Lt=n(e),c(es.$$.fragment,e),Kt=n(e),Z=i(e,"DIV",{class:!0});var Os=y(Z);c(ss.$$.fragment,Os),ka=n(Os),$s=i(Os,"P",{"data-svelte-h":!0}),f($s)!=="svelte-szixzp"&&($s.innerHTML=xn),Ca=n(Os),xs=i(Os,"P",{"data-svelte-h":!0}),f(xs)!=="svelte-1ek1ss9"&&(xs.innerHTML=Sn),Os.forEach(t),Ot=n(e),c(ts.$$.fragment,e),ea=n(e),v=i(e,"DIV",{class:!0});var F=y(v);c(as.$$.fragment,F),za=n(F),Ss=i(F,"P",{"data-svelte-h":!0}),f(Ss)!=="svelte-1er9ay4"&&(Ss.textContent=Bn),$a=n(F),Bs=i(F,"P",{"data-svelte-h":!0}),f(Bs)!=="svelte-2x431r"&&(Bs.innerHTML=Wn),xa=n(F),D=i(F,"DIV",{class:!0});var da=y(D);c(ns.$$.fragment,da),Sa=n(da),Ws=i(da,"P",{"data-svelte-h":!0}),f(Ws)!=="svelte-16w063a"&&(Ws.innerHTML=Nn),da.forEach(t),Ba=n(F),A=i(F,"DIV",{class:!0});var ga=y(A);c(os.$$.fragment,ga),Wa=n(ga),Ns=i(ga,"P",{"data-svelte-h":!0}),f(Ns)!=="svelte-juomob"&&(Ns.textContent=Vn),ga.forEach(t),F.forEach(t),sa=n(e),c(rs.$$.fragment,e),ta=n(e),J=i(e,"DIV",{class:!0});var C=y(J);c(ls.$$.fragment,C),Na=n(C),Vs=i(C,"P",{"data-svelte-h":!0}),f(Vs)!=="svelte-6z1ndd"&&(Vs.textContent=Zn),Va=n(C),q=i(C,"DIV",{class:!0});var ha=y(q);c(is.$$.fragment,ha),Za=n(ha),Zs=i(ha,"P",{"data-svelte-h":!0}),f(Zs)!=="svelte-jpa0fq"&&(Zs.innerHTML=Rn),ha.forEach(t),Ra=n(C),L=i(C,"DIV",{class:!0});var fa=y(L);c(ms.$$.fragment,fa),Fa=n(fa),Rs=i(fa,"P",{"data-svelte-h":!0}),f(Rs)!=="svelte-1j7xts3"&&(Rs.innerHTML=Fn),fa.forEach(t),Ga=n(C),K=i(C,"DIV",{class:!0});var ua=y(K);c(cs.$$.fragment,ua),Xa=n(ua),Fs=i(ua,"P",{"data-svelte-h":!0}),f(Fs)!=="svelte-wwrho9"&&(Fs.textContent=Gn),ua.forEach(t),Ea=n(C),O=i(C,"DIV",{class:!0});var _a=y(O);c(ps.$$.fragment,_a),Ha=n(_a),Gs=i(_a,"P",{"data-svelte-h":!0}),f(Gs)!=="svelte-juomob"&&(Gs.textContent=Xn),_a.forEach(t),Qa=n(C),Xs=i(C,"DIV",{class:!0});var qn=y(Xs);c(ds.$$.fragment,qn),qn.forEach(t),C.forEach(t),aa=n(e),c(gs.$$.fragment,e),na=n(e),E=i(e,"DIV",{class:!0});var ba=y(E);c(hs.$$.fragment,ba),Pa=n(ba),Es=i(ba,"DIV",{class:!0});var Ln=y(Es);c(fs.$$.fragment,Ln),Ln.forEach(t),ba.forEach(t),oa=n(e),c(us.$$.fragment,e),ra=n(e),I=i(e,"DIV",{class:!0});var G=y(I);c(_s.$$.fragment,G),Ya=n(G),Hs=i(G,"P",{"data-svelte-h":!0}),f(Hs)!=="svelte-1g2jbi1"&&(Hs.textContent=En),Da=n(G),Qs=i(G,"P",{"data-svelte-h":!0}),f(Qs)!=="svelte-q52n56"&&(Qs.innerHTML=Hn),Aa=n(G),Ps=i(G,"P",{"data-svelte-h":!0}),f(Ps)!=="svelte-hswkmf"&&(Ps.innerHTML=Qn),qa=n(G),Ys=i(G,"DIV",{class:!0});var Kn=y(Ys);c(bs.$$.fragment,Kn),Kn.forEach(t),G.forEach(t),la=n(e),c(Ms.$$.fragment,e),ia=n(e),k=i(e,"DIV",{class:!0});var X=y(k);c(ys.$$.fragment,X),La=n(X),Ds=i(X,"P",{"data-svelte-h":!0}),f(Ds)!=="svelte-8gz1ro"&&(Ds.textContent=Pn),Ka=n(X),As=i(X,"P",{"data-svelte-h":!0}),f(As)!=="svelte-q52n56"&&(As.innerHTML=Yn),Oa=n(X),qs=i(X,"P",{"data-svelte-h":!0}),f(qs)!=="svelte-hswkmf"&&(qs.innerHTML=Dn),en=n(X),S=i(X,"DIV",{class:!0});var te=y(S);c(Ts.$$.fragment,te),sn=n(te),Ls=i(te,"P",{"data-svelte-h":!0}),f(Ls)!=="svelte-sj6hmg"&&(Ls.innerHTML=An),tn=n(te),c(ee.$$.fragment,te),an=n(te),c(se.$$.fragment,te),te.forEach(t),X.forEach(t),ma=n(e),c(js.$$.fragment,e),ca=n(e),et=i(e,"P",{}),y(et).forEach(t),this.h()},h(){T(u,"name","hf:doc:metadata"),T(u,"content",go),oo(U,"float","right"),T($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Xs,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Es,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(Ys,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),T(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){r(document.head,u),o(e,z,s),o(e,j,s),o(e,M,s),o(e,U,s),o(e,x,s),p(ae,e,s),o(e,st,s),p(ne,e,s),o(e,tt,s),o(e,oe,s),o(e,at,s),o(e,re,s),o(e,nt,s),o(e,le,s),o(e,ot,s),o(e,ie,s),o(e,rt,s),o(e,me,s),o(e,lt,s),o(e,ce,s),o(e,it,s),o(e,pe,s),o(e,mt,s),o(e,de,s),o(e,ct,s),p(ge,e,s),o(e,pt,s),p(he,e,s),o(e,dt,s),o(e,fe,s),o(e,gt,s),p(ue,e,s),o(e,ht,s),p(_e,e,s),o(e,ft,s),p(be,e,s),o(e,ut,s),o(e,Me,s),o(e,_t,s),p(ye,e,s),o(e,bt,s),p(Te,e,s),o(e,Mt,s),o(e,je,s),o(e,yt,s),p(we,e,s),o(e,Tt,s),p(Je,e,s),o(e,jt,s),o(e,Ue,s),o(e,wt,s),p(ve,e,s),o(e,Jt,s),p(Ie,e,s),o(e,Ut,s),o(e,ke,s),o(e,vt,s),p(Ce,e,s),o(e,It,s),p(ze,e,s),o(e,kt,s),p($e,e,s),o(e,Ct,s),o(e,xe,s),o(e,zt,s),p(Se,e,s),o(e,$t,s),p(Be,e,s),o(e,xt,s),o(e,We,s),o(e,St,s),p(Ne,e,s),o(e,Bt,s),p(Ve,e,s),o(e,Wt,s),o(e,Ze,s),o(e,Nt,s),p(Re,e,s),o(e,Vt,s),p(Fe,e,s),o(e,Zt,s),o(e,Ge,s),o(e,Rt,s),p(Xe,e,s),o(e,Ft,s),p(Ee,e,s),o(e,Gt,s),o(e,He,s),o(e,Xt,s),p(Qe,e,s),o(e,Et,s),p(Pe,e,s),o(e,Ht,s),o(e,$,s),p(Ye,$,null),r($,Ma),r($,ws),r($,ya),r($,Js),r($,Ta),p(Y,$,null),o(e,Qt,s),p(De,e,s),o(e,Pt,s),o(e,W,s),p(Ae,W,null),r(W,ja),r(W,Us),r(W,wa),r(W,vs),o(e,Yt,s),p(qe,e,s),o(e,Dt,s),o(e,N,s),p(Le,N,null),r(N,Ja),r(N,Is),r(N,Ua),r(N,ks),o(e,At,s),p(Ke,e,s),o(e,qt,s),o(e,V,s),p(Oe,V,null),r(V,va),r(V,Cs),r(V,Ia),r(V,zs),o(e,Lt,s),p(es,e,s),o(e,Kt,s),o(e,Z,s),p(ss,Z,null),r(Z,ka),r(Z,$s),r(Z,Ca),r(Z,xs),o(e,Ot,s),p(ts,e,s),o(e,ea,s),o(e,v,s),p(as,v,null),r(v,za),r(v,Ss),r(v,$a),r(v,Bs),r(v,xa),r(v,D),p(ns,D,null),r(D,Sa),r(D,Ws),r(v,Ba),r(v,A),p(os,A,null),r(A,Wa),r(A,Ns),o(e,sa,s),p(rs,e,s),o(e,ta,s),o(e,J,s),p(ls,J,null),r(J,Na),r(J,Vs),r(J,Va),r(J,q),p(is,q,null),r(q,Za),r(q,Zs),r(J,Ra),r(J,L),p(ms,L,null),r(L,Fa),r(L,Rs),r(J,Ga),r(J,K),p(cs,K,null),r(K,Xa),r(K,Fs),r(J,Ea),r(J,O),p(ps,O,null),r(O,Ha),r(O,Gs),r(J,Qa),r(J,Xs),p(ds,Xs,null),o(e,aa,s),p(gs,e,s),o(e,na,s),o(e,E,s),p(hs,E,null),r(E,Pa),r(E,Es),p(fs,Es,null),o(e,oa,s),p(us,e,s),o(e,ra,s),o(e,I,s),p(_s,I,null),r(I,Ya),r(I,Hs),r(I,Da),r(I,Qs),r(I,Aa),r(I,Ps),r(I,qa),r(I,Ys),p(bs,Ys,null),o(e,la,s),p(Ms,e,s),o(e,ia,s),o(e,k,s),p(ys,k,null),r(k,La),r(k,Ds),r(k,Ka),r(k,As),r(k,Oa),r(k,qs),r(k,en),r(k,S),p(Ts,S,null),r(S,sn),r(S,Ls),r(S,tn),p(ee,S,null),r(S,an),p(se,S,null),o(e,ma,s),p(js,e,s),o(e,ca,s),o(e,et,s),pa=!0},p(e,[s]){const R={};s&2&&(R.$$scope={dirty:s,ctx:e}),Y.$set(R);const H={};s&2&&(H.$$scope={dirty:s,ctx:e}),ee.$set(H);const Q={};s&2&&(Q.$$scope={dirty:s,ctx:e}),se.$set(Q)},i(e){pa||(d(ae.$$.fragment,e),d(ne.$$.fragment,e),d(ge.$$.fragment,e),d(he.$$.fragment,e),d(ue.$$.fragment,e),d(_e.$$.fragment,e),d(be.$$.fragment,e),d(ye.$$.fragment,e),d(Te.$$.fragment,e),d(we.$$.fragment,e),d(Je.$$.fragment,e),d(ve.$$.fragment,e),d(Ie.$$.fragment,e),d(Ce.$$.fragment,e),d(ze.$$.fragment,e),d($e.$$.fragment,e),d(Se.$$.fragment,e),d(Be.$$.fragment,e),d(Ne.$$.fragment,e),d(Ve.$$.fragment,e),d(Re.$$.fragment,e),d(Fe.$$.fragment,e),d(Xe.$$.fragment,e),d(Ee.$$.fragment,e),d(Qe.$$.fragment,e),d(Pe.$$.fragment,e),d(Ye.$$.fragment,e),d(Y.$$.fragment,e),d(De.$$.fragment,e),d(Ae.$$.fragment,e),d(qe.$$.fragment,e),d(Le.$$.fragment,e),d(Ke.$$.fragment,e),d(Oe.$$.fragment,e),d(es.$$.fragment,e),d(ss.$$.fragment,e),d(ts.$$.fragment,e),d(as.$$.fragment,e),d(ns.$$.fragment,e),d(os.$$.fragment,e),d(rs.$$.fragment,e),d(ls.$$.fragment,e),d(is.$$.fragment,e),d(ms.$$.fragment,e),d(cs.$$.fragment,e),d(ps.$$.fragment,e),d(ds.$$.fragment,e),d(gs.$$.fragment,e),d(hs.$$.fragment,e),d(fs.$$.fragment,e),d(us.$$.fragment,e),d(_s.$$.fragment,e),d(bs.$$.fragment,e),d(Ms.$$.fragment,e),d(ys.$$.fragment,e),d(Ts.$$.fragment,e),d(ee.$$.fragment,e),d(se.$$.fragment,e),d(js.$$.fragment,e),pa=!0)},o(e){g(ae.$$.fragment,e),g(ne.$$.fragment,e),g(ge.$$.fragment,e),g(he.$$.fragment,e),g(ue.$$.fragment,e),g(_e.$$.fragment,e),g(be.$$.fragment,e),g(ye.$$.fragment,e),g(Te.$$.fragment,e),g(we.$$.fragment,e),g(Je.$$.fragment,e),g(ve.$$.fragment,e),g(Ie.$$.fragment,e),g(Ce.$$.fragment,e),g(ze.$$.fragment,e),g($e.$$.fragment,e),g(Se.$$.fragment,e),g(Be.$$.fragment,e),g(Ne.$$.fragment,e),g(Ve.$$.fragment,e),g(Re.$$.fragment,e),g(Fe.$$.fragment,e),g(Xe.$$.fragment,e),g(Ee.$$.fragment,e),g(Qe.$$.fragment,e),g(Pe.$$.fragment,e),g(Ye.$$.fragment,e),g(Y.$$.fragment,e),g(De.$$.fragment,e),g(Ae.$$.fragment,e),g(qe.$$.fragment,e),g(Le.$$.fragment,e),g(Ke.$$.fragment,e),g(Oe.$$.fragment,e),g(es.$$.fragment,e),g(ss.$$.fragment,e),g(ts.$$.fragment,e),g(as.$$.fragment,e),g(ns.$$.fragment,e),g(os.$$.fragment,e),g(rs.$$.fragment,e),g(ls.$$.fragment,e),g(is.$$.fragment,e),g(ms.$$.fragment,e),g(cs.$$.fragment,e),g(ps.$$.fragment,e),g(ds.$$.fragment,e),g(gs.$$.fragment,e),g(hs.$$.fragment,e),g(fs.$$.fragment,e),g(us.$$.fragment,e),g(_s.$$.fragment,e),g(bs.$$.fragment,e),g(Ms.$$.fragment,e),g(ys.$$.fragment,e),g(Ts.$$.fragment,e),g(ee.$$.fragment,e),g(se.$$.fragment,e),g(js.$$.fragment,e),pa=!1},d(e){e&&(t(z),t(j),t(M),t(U),t(x),t(st),t(tt),t(oe),t(at),t(re),t(nt),t(le),t(ot),t(ie),t(rt),t(me),t(lt),t(ce),t(it),t(pe),t(mt),t(de),t(ct),t(pt),t(dt),t(fe),t(gt),t(ht),t(ft),t(ut),t(Me),t(_t),t(bt),t(Mt),t(je),t(yt),t(Tt),t(jt),t(Ue),t(wt),t(Jt),t(Ut),t(ke),t(vt),t(It),t(kt),t(Ct),t(xe),t(zt),t($t),t(xt),t(We),t(St),t(Bt),t(Wt),t(Ze),t(Nt),t(Vt),t(Zt),t(Ge),t(Rt),t(Ft),t(Gt),t(He),t(Xt),t(Et),t(Ht),t($),t(Qt),t(Pt),t(W),t(Yt),t(Dt),t(N),t(At),t(qt),t(V),t(Lt),t(Kt),t(Z),t(Ot),t(ea),t(v),t(sa),t(ta),t(J),t(aa),t(na),t(E),t(oa),t(ra),t(I),t(la),t(ia),t(k),t(ma),t(ca),t(et)),t(u),h(ae,e),h(ne,e),h(ge,e),h(he,e),h(ue,e),h(_e,e),h(be,e),h(ye,e),h(Te,e),h(we,e),h(Je,e),h(ve,e),h(Ie,e),h(Ce,e),h(ze,e),h($e,e),h(Se,e),h(Be,e),h(Ne,e),h(Ve,e),h(Re,e),h(Fe,e),h(Xe,e),h(Ee,e),h(Qe,e),h(Pe,e),h(Ye),h(Y),h(De,e),h(Ae),h(qe,e),h(Le),h(Ke,e),h(Oe),h(es,e),h(ss),h(ts,e),h(as),h(ns),h(os),h(rs,e),h(ls),h(is),h(ms),h(cs),h(ps),h(ds),h(gs,e),h(hs),h(fs),h(us,e),h(_s),h(bs),h(Ms,e),h(ys),h(Ts),h(ee),h(se),h(js,e)}}}const go='{"title":"SAM2","local":"sam2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[{"title":"Automatic Mask Generation with Pipeline","local":"automatic-mask-generation-with-pipeline","sections":[],"depth":3},{"title":"Basic Image Segmentation","local":"basic-image-segmentation","sections":[{"title":"Single Point Click","local":"single-point-click","sections":[],"depth":4},{"title":"Multiple Points for Refinement","local":"multiple-points-for-refinement","sections":[],"depth":4},{"title":"Bounding Box Input","local":"bounding-box-input","sections":[],"depth":4},{"title":"Multiple Objects Segmentation","local":"multiple-objects-segmentation","sections":[],"depth":4}],"depth":3},{"title":"Batch Inference","local":"batch-inference","sections":[{"title":"Batched Images","local":"batched-images","sections":[],"depth":4},{"title":"Batched Objects per Image","local":"batched-objects-per-image","sections":[],"depth":4},{"title":"Batched Images with Batched Objects and Multiple Points","local":"batched-images-with-batched-objects-and-multiple-points","sections":[],"depth":4},{"title":"Batched Bounding Boxes","local":"batched-bounding-boxes","sections":[],"depth":4}],"depth":3},{"title":"Using Previous Masks as Input","local":"using-previous-masks-as-input","sections":[],"depth":3}],"depth":2},{"title":"Sam2Config","local":"transformers.Sam2Config","sections":[],"depth":2},{"title":"Sam2HieraDetConfig","local":"transformers.Sam2HieraDetConfig","sections":[],"depth":2},{"title":"Sam2VisionConfig","local":"transformers.Sam2VisionConfig","sections":[],"depth":2},{"title":"Sam2MaskDecoderConfig","local":"transformers.Sam2MaskDecoderConfig","sections":[],"depth":2},{"title":"Sam2PromptEncoderConfig","local":"transformers.Sam2PromptEncoderConfig","sections":[],"depth":2},{"title":"Sam2Processor","local":"transformers.Sam2Processor","sections":[],"depth":2},{"title":"Sam2ImageProcessorFast","local":"transformers.Sam2ImageProcessorFast","sections":[],"depth":2},{"title":"Sam2HieraDetModel","local":"transformers.Sam2HieraDetModel","sections":[],"depth":2},{"title":"Sam2VisionModel","local":"transformers.Sam2VisionModel","sections":[],"depth":2},{"title":"Sam2Model","local":"transformers.Sam2Model","sections":[],"depth":2}],"depth":1}';function ho(P){return so(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jo extends to{constructor(u){super(),ao(this,u,ho,po,eo,{})}}export{jo as component};
