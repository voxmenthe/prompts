import{s as hr,z as pt,o as ur,n as ut}from"../chunks/scheduler.18a86fab.js";import{S as _r,i as fr,g as l,s as a,r as g,A as gr,h as m,f as t,c as s,j as z,u as w,x as _,k as M,y as i,a as o,v as b,d as v,t as $,w as y}from"../chunks/index.98837b22.js";import{D as C}from"../chunks/Docstring.a1ef7999.js";import{C as _t}from"../chunks/CodeBlock.8d0c2e8a.js";import{E as dt}from"../chunks/ExampleCodeBlock.8c3ee1f9.js";import{H as ht,E as wr}from"../chunks/getInferenceSnippets.06c2775f.js";function br(U){let c,T="Example:",f,p,h;return p=new _t({props:{code:"QWRhZmFjdG9yKG1vZGVsLnBhcmFtZXRlcnMoKSUyQyUyMHNjYWxlX3BhcmFtZXRlciUzREZhbHNlJTJDJTIwcmVsYXRpdmVfc3RlcCUzREZhbHNlJTJDJTIwd2FybXVwX2luaXQlM0RGYWxzZSUyQyUyMGxyJTNEMWUtMyk=",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)',wrap:!1}}),{c(){c=l("p"),c.textContent=T,f=a(),g(p.$$.fragment)},l(r){c=m(r,"P",{"data-svelte-h":!0}),_(c)!=="svelte-11lpom8"&&(c.textContent=T),f=s(r),w(p.$$.fragment,r)},m(r,x){o(r,c,x),o(r,f,x),b(p,r,x),h=!0},p:ut,i(r){h||(v(p.$$.fragment,r),h=!0)},o(r){$(p.$$.fragment,r),h=!1},d(r){r&&(t(c),t(f)),y(p,r)}}}function vr(U){let c,T="Others reported the following combination to work well:",f,p,h;return p=new _t({props:{code:"QWRhZmFjdG9yKG1vZGVsLnBhcmFtZXRlcnMoKSUyQyUyMHNjYWxlX3BhcmFtZXRlciUzRFRydWUlMkMlMjByZWxhdGl2ZV9zdGVwJTNEVHJ1ZSUyQyUyMHdhcm11cF9pbml0JTNEVHJ1ZSUyQyUyMGxyJTNETm9uZSk=",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)',wrap:!1}}),{c(){c=l("p"),c.textContent=T,f=a(),g(p.$$.fragment)},l(r){c=m(r,"P",{"data-svelte-h":!0}),_(c)!=="svelte-mxeef1"&&(c.textContent=T),f=s(r),w(p.$$.fragment,r)},m(r,x){o(r,c,x),o(r,f,x),b(p,r,x),h=!0},p:ut,i(r){h||(v(p.$$.fragment,r),h=!0)},o(r){$(p.$$.fragment,r),h=!1},d(r){r&&(t(c),t(f)),y(p,r)}}}function $r(U){let c,T="scheduler as following:",f,p,h;return p=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vcHRpbWl6YXRpb24lMjBpbXBvcnQlMjBBZGFmYWN0b3IlMkMlMjBBZGFmYWN0b3JTY2hlZHVsZSUwQSUwQW9wdGltaXplciUyMCUzRCUyMEFkYWZhY3Rvcihtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMjBzY2FsZV9wYXJhbWV0ZXIlM0RUcnVlJTJDJTIwcmVsYXRpdmVfc3RlcCUzRFRydWUlMkMlMjB3YXJtdXBfaW5pdCUzRFRydWUlMkMlMjBsciUzRE5vbmUpJTBBbHJfc2NoZWR1bGVyJTIwJTNEJTIwQWRhZmFjdG9yU2NoZWR1bGUob3B0aW1pemVyKSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKC4uLiUyQyUyMG9wdGltaXplcnMlM0Qob3B0aW1pemVyJTJDJTIwbHJfc2NoZWR1bGVyKSk=",highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,wrap:!1}}),{c(){c=l("p"),c.textContent=T,f=a(),g(p.$$.fragment)},l(r){c=m(r,"P",{"data-svelte-h":!0}),_(c)!=="svelte-1eua222"&&(c.textContent=T),f=s(r),w(p.$$.fragment,r)},m(r,x){o(r,c,x),o(r,f,x),b(p,r,x),h=!0},p:ut,i(r){h||(v(p.$$.fragment,r),h=!0)},o(r){$(p.$$.fragment,r),h=!1},d(r){r&&(t(c),t(f)),y(p,r)}}}function yr(U){let c,T="Usage:",f,p,h;return p=new _t({props:{code:"JTIzJTIwcmVwbGFjZSUyMEFkYW1XJTIwd2l0aCUyMEFkYWZhY3RvciUwQW9wdGltaXplciUyMCUzRCUyMEFkYWZhY3RvciglMEElMjAlMjAlMjAlMjBtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMEElMjAlMjAlMjAlMjBsciUzRDFlLTMlMkMlMEElMjAlMjAlMjAlMjBlcHMlM0QoMWUtMzAlMkMlMjAxZS0zKSUyQyUwQSUyMCUyMCUyMCUyMGNsaXBfdGhyZXNob2xkJTNEMS4wJTJDJTBBJTIwJTIwJTIwJTIwZGVjYXlfcmF0ZSUzRC0wLjglMkMlMEElMjAlMjAlMjAlMjBiZXRhMSUzRE5vbmUlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAlMkMlMEElMjAlMjAlMjAlMjByZWxhdGl2ZV9zdGVwJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBzY2FsZV9wYXJhbWV0ZXIlM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMHdhcm11cF9pbml0JTNERmFsc2UlMkMlMEEp",highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`,wrap:!1}}),{c(){c=l("p"),c.textContent=T,f=a(),g(p.$$.fragment)},l(r){c=m(r,"P",{"data-svelte-h":!0}),_(c)!=="svelte-5wyjqd"&&(c.textContent=T),f=s(r),w(p.$$.fragment,r)},m(r,x){o(r,c,x),o(r,f,x),b(p,r,x),h=!0},p:ut,i(r){h||(v(p.$$.fragment,r),h=!0)},o(r){$(p.$$.fragment,r),h=!1},d(r){r&&(t(c),t(f)),y(p,r)}}}function xr(U){let c,T,f,p,h,r,x,It="The <code>.optimization</code> module provides:",De,q,St="<li>an optimizer with weight decay fixed that can be used to fine-tuned models, and</li> <li>several schedules in the form of schedule objects that inherit from <code>_LRSchedule</code>:</li> <li>a gradient accumulation class to accumulate the gradients of multiple batches</li>",Fe,P,Ve,d,B,ft,ue,Wt=`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
<a href="https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py" rel="nofollow">https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py</a>`,gt,_e,Nt=`Paper: <em>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</em> <a href="https://huggingface.co/papers/1804.04235" rel="nofollow">https://huggingface.co/papers/1804.04235</a> Note that
this optimizer internally adjusts the learning rate depending on the <code>scale_parameter</code>, <code>relative_step</code> and
<code>warmup_init</code> options. To use a manual (external) learning rate schedule you should set <code>scale_parameter=False</code> and
<code>relative_step=False</code>.`,wt,fe,qt="This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested.",bt,ge,Pt='Recommended T5 finetuning settings (<a href="https://discuss.huggingface.co/t/t5-finetuning-tips/684/3" rel="nofollow">https://discuss.huggingface.co/t/t5-finetuning-tips/684/3</a>):',vt,we,Bt='<li><p>Training without LR warmup or clip_threshold is not recommended.</p> <ul><li>use scheduled LR warm-up to fixed LR</li> <li>use clip_threshold=1.0 (<a href="https://huggingface.co/papers/1804.04235" rel="nofollow">https://huggingface.co/papers/1804.04235</a>)</li></ul></li> <li><p>Disable relative updates</p></li> <li><p>Use scale_parameter=False</p></li> <li><p>Additional optimizer operations like gradient clipping should not be used alongside Adafactor</p></li>',$t,H,yt,I,xt,be,Gt='When using <code>lr=None</code> with <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a> you will most likely need to use <code>AdafactorSchedule</code>',Mt,S,Tt,W,zt,N,G,Ct,ve,Xt="Performs a single optimization step",He,X,Ie,Y,Se,j,O,jt,$e,Yt=`Scheduler names for the parameter <code>lr_scheduler_type</code> in <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>.
By default, it uses “linear”. Internally, this retrieves <code>get_linear_schedule_with_warmup</code> scheduler from <a href="/docs/transformers/v4.56.2/en/main_classes/trainer#transformers.Trainer">Trainer</a>.
Scheduler types:`,Lt,ye,Ot="<li>“linear” = get_linear_schedule_with_warmup</li> <li>“cosine” = get_cosine_schedule_with_warmup</li> <li>“cosine_with_restarts” = get_cosine_with_hard_restarts_schedule_with_warmup</li> <li>“polynomial” = get_polynomial_decay_schedule_with_warmup</li> <li>“constant” =  get_constant_schedule</li> <li>“constant_with_warmup” = get_constant_schedule_with_warmup</li> <li>“inverse_sqrt” = get_inverse_sqrt_schedule</li> <li>“reduce_lr_on_plateau” = get_reduce_on_plateau_schedule</li> <li>“cosine_with_min_lr” = get_cosine_with_min_lr_schedule_with_warmup</li> <li>“warmup_stable_decay” = get_wsd_schedule</li>",We,R,Q,At,xe,Qt="Unified API to get any scheduler from its name.",Ne,E,K,Ut,Me,Kt="Create a schedule with a constant learning rate, using the learning rate set in optimizer.",qe,k,ee,Rt,Te,er=`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`,Pe,te,tr,Be,Z,re,Et,ze,rr=`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`,Ge,ne,nr,Xe,J,ae,kt,Ce,ar=`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`,Ye,se,sr,Oe,D,oe,Zt,je,or=`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`,Qe,ie,ir,Ke,L,le,Jt,Le,lr=`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by <em>lr_end</em>, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`,Dt,Ae,mr=`Note: <em>power</em> defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
<a href="https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37" rel="nofollow">https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</a>`,et,F,me,Ft,Ue,cr=`Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a
warmup period which increases lr linearly from 0 to the initial lr set in the optimizer.`,tt,A,ce,Vt,Re,pr="Create a schedule with a learning rate that has three stages:",Ht,Ee,dr="<li>warmup: increase from min_lr_ratio times the initial learning rate to the initial learning rate following a warmup_type.</li> <li>stable: constant learning rate.</li> <li>decay: decrease from the initial learning rate to min_lr_ratio times the initial learning rate following a decay_type.</li>",rt,pe,nt,Je,at;return h=new ht({props:{title:"Optimization",local:"optimization",headingTag:"h1"}}),P=new ht({props:{title:"AdaFactor",local:"transformers.Adafactor",headingTag:"h2"}}),B=new C({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>tuple[float, float]</code>, <em>optional</em>, defaults to <code>(1e-30, 0.001)</code>) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L688"}}),H=new dt({props:{anchor:"transformers.Adafactor.example",$$slots:{default:[br]},$$scope:{ctx:U}}}),I=new dt({props:{anchor:"transformers.Adafactor.example-2",$$slots:{default:[vr]},$$scope:{ctx:U}}}),S=new dt({props:{anchor:"transformers.Adafactor.example-3",$$slots:{default:[$r]},$$scope:{ctx:U}}}),W=new dt({props:{anchor:"transformers.Adafactor.example-4",$$slots:{default:[yr]},$$scope:{ctx:U}}}),G=new C({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L833"}}),X=new ht({props:{title:"Schedules",local:"schedules",headingTag:"h2"}}),Y=new ht({props:{title:"Learning Rate Schedules",local:"transformers.SchedulerType",headingTag:"h3"}}),O=new C({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/trainer_utils.py#L419"}}),Q=new C({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"},{name:"scheduler_specific_kwargs",val:": typing.Optional[dict] = None"}],parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"},{anchor:"transformers.get_scheduler.scheduler_specific_kwargs",description:`<strong>scheduler_specific_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Extra parameters for schedulers such as cosine with restarts. Mismatched scheduler types and scheduler
parameters will cause the scheduler function to raise a TypeError.`,name:"scheduler_specific_kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L594"}}),K=new C({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L37",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ee=new C({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L78",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),re=new C({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L141",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ae=new C({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L186",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),oe=new C({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L105",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),le=new C({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L240",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),me=new C({props:{name:"transformers.get_inverse_sqrt_schedule",anchor:"transformers.get_inverse_sqrt_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"timescale",val:": typing.Optional[int] = None"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_inverse_sqrt_schedule.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_inverse_sqrt_schedule.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_inverse_sqrt_schedule.timescale",description:`<strong>timescale</strong> (<code>int</code>, <em>optional</em>, defaults to <code>num_warmup_steps</code>) &#x2014;
Time scale.`,name:"timescale"},{anchor:"transformers.get_inverse_sqrt_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L294",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ce=new C({props:{name:"transformers.get_wsd_schedule",anchor:"transformers.get_wsd_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_decay_steps",val:": int"},{name:"num_training_steps",val:": typing.Optional[int] = None"},{name:"num_stable_steps",val:": typing.Optional[int] = None"},{name:"warmup_type",val:": str = 'linear'"},{name:"decay_type",val:": str = 'cosine'"},{name:"min_lr_ratio",val:": float = 0"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_wsd_schedule.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_wsd_schedule.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_wsd_schedule.num_decay_steps",description:`<strong>num_decay_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the decay phase.`,name:"num_decay_steps"},{anchor:"transformers.get_wsd_schedule.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of training steps. This is the sum of the warmup, stable and decay steps. If <code>num_stable_steps</code> is not provided, the stable phase will be <code>num_training_steps - num_warmup_steps - num_decay_steps</code>.`,name:"num_training_steps"},{anchor:"transformers.get_wsd_schedule.num_stable_steps",description:`<strong>num_stable_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of steps for the stable phase. Please ensure that <code>num_warmup_steps + num_stable_steps + num_decay_steps</code> equals <code>num_training_steps</code>, otherwise the other steps will default to the minimum learning rate.`,name:"num_stable_steps"},{anchor:"transformers.get_wsd_schedule.warmup_type",description:`<strong>warmup_type</strong> (<code>str</code>, <em>optional</em>, defaults to &#x201C;linear&#x201D;) &#x2014;
The type of warmup to use. Can be &#x2018;linear&#x2019;, &#x2018;cosine&#x2019; or &#x2018;1-sqrt&#x2019;.`,name:"warmup_type"},{anchor:"transformers.get_wsd_schedule.decay_type",description:`<strong>decay_type</strong> (<code>str</code>, <em>optional</em>, defaults to &#x201C;cosine&#x201D;) &#x2014;
The type of decay to use. Can be &#x2018;linear&#x2019;, &#x2018;cosine&#x2019; or &#x2018;1-sqrt&#x2019;.`,name:"decay_type"},{anchor:"transformers.get_wsd_schedule.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The minimum learning rate as a ratio of the initial learning rate.`,name:"min_lr_ratio"},{anchor:"transformers.get_wsd_schedule.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_wsd_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/optimization.py#L506",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),pe=new wr({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/optimizer_schedules.md"}}),{c(){c=l("meta"),T=a(),f=l("p"),p=a(),g(h.$$.fragment),r=a(),x=l("p"),x.innerHTML=It,De=a(),q=l("ul"),q.innerHTML=St,Fe=a(),g(P.$$.fragment),Ve=a(),d=l("div"),g(B.$$.fragment),ft=a(),ue=l("p"),ue.innerHTML=Wt,gt=a(),_e=l("p"),_e.innerHTML=Nt,wt=a(),fe=l("p"),fe.textContent=qt,bt=a(),ge=l("p"),ge.innerHTML=Pt,vt=a(),we=l("ul"),we.innerHTML=Bt,$t=a(),g(H.$$.fragment),yt=a(),g(I.$$.fragment),xt=a(),be=l("p"),be.innerHTML=Gt,Mt=a(),g(S.$$.fragment),Tt=a(),g(W.$$.fragment),zt=a(),N=l("div"),g(G.$$.fragment),Ct=a(),ve=l("p"),ve.textContent=Xt,He=a(),g(X.$$.fragment),Ie=a(),g(Y.$$.fragment),Se=a(),j=l("div"),g(O.$$.fragment),jt=a(),$e=l("p"),$e.innerHTML=Yt,Lt=a(),ye=l("ul"),ye.innerHTML=Ot,We=a(),R=l("div"),g(Q.$$.fragment),At=a(),xe=l("p"),xe.textContent=Qt,Ne=a(),E=l("div"),g(K.$$.fragment),Ut=a(),Me=l("p"),Me.textContent=Kt,qe=a(),k=l("div"),g(ee.$$.fragment),Rt=a(),Te=l("p"),Te.textContent=er,Pe=a(),te=l("img"),Be=a(),Z=l("div"),g(re.$$.fragment),Et=a(),ze=l("p"),ze.textContent=rr,Ge=a(),ne=l("img"),Xe=a(),J=l("div"),g(ae.$$.fragment),kt=a(),Ce=l("p"),Ce.textContent=ar,Ye=a(),se=l("img"),Oe=a(),D=l("div"),g(oe.$$.fragment),Zt=a(),je=l("p"),je.textContent=or,Qe=a(),ie=l("img"),Ke=a(),L=l("div"),g(le.$$.fragment),Jt=a(),Le=l("p"),Le.innerHTML=lr,Dt=a(),Ae=l("p"),Ae.innerHTML=mr,et=a(),F=l("div"),g(me.$$.fragment),Ft=a(),Ue=l("p"),Ue.textContent=cr,tt=a(),A=l("div"),g(ce.$$.fragment),Vt=a(),Re=l("p"),Re.textContent=pr,Ht=a(),Ee=l("ol"),Ee.innerHTML=dr,rt=a(),g(pe.$$.fragment),nt=a(),Je=l("p"),this.h()},l(e){const n=gr("svelte-u9bgzb",document.head);c=m(n,"META",{name:!0,content:!0}),n.forEach(t),T=s(e),f=m(e,"P",{}),z(f).forEach(t),p=s(e),w(h.$$.fragment,e),r=s(e),x=m(e,"P",{"data-svelte-h":!0}),_(x)!=="svelte-312sfx"&&(x.innerHTML=It),De=s(e),q=m(e,"UL",{"data-svelte-h":!0}),_(q)!=="svelte-ps1e52"&&(q.innerHTML=St),Fe=s(e),w(P.$$.fragment,e),Ve=s(e),d=m(e,"DIV",{class:!0});var u=z(d);w(B.$$.fragment,u),ft=s(u),ue=m(u,"P",{"data-svelte-h":!0}),_(ue)!=="svelte-114ll6r"&&(ue.innerHTML=Wt),gt=s(u),_e=m(u,"P",{"data-svelte-h":!0}),_(_e)!=="svelte-13ul7ru"&&(_e.innerHTML=Nt),wt=s(u),fe=m(u,"P",{"data-svelte-h":!0}),_(fe)!=="svelte-1dxk71o"&&(fe.textContent=qt),bt=s(u),ge=m(u,"P",{"data-svelte-h":!0}),_(ge)!=="svelte-kc6644"&&(ge.innerHTML=Pt),vt=s(u),we=m(u,"UL",{"data-svelte-h":!0}),_(we)!=="svelte-10mstfo"&&(we.innerHTML=Bt),$t=s(u),w(H.$$.fragment,u),yt=s(u),w(I.$$.fragment,u),xt=s(u),be=m(u,"P",{"data-svelte-h":!0}),_(be)!=="svelte-rvmj55"&&(be.innerHTML=Gt),Mt=s(u),w(S.$$.fragment,u),Tt=s(u),w(W.$$.fragment,u),zt=s(u),N=m(u,"DIV",{class:!0});var de=z(N);w(G.$$.fragment,de),Ct=s(de),ve=m(de,"P",{"data-svelte-h":!0}),_(ve)!=="svelte-6lkj7q"&&(ve.textContent=Xt),de.forEach(t),u.forEach(t),He=s(e),w(X.$$.fragment,e),Ie=s(e),w(Y.$$.fragment,e),Se=s(e),j=m(e,"DIV",{class:!0});var V=z(j);w(O.$$.fragment,V),jt=s(V),$e=m(V,"P",{"data-svelte-h":!0}),_($e)!=="svelte-w9nqv7"&&($e.innerHTML=Yt),Lt=s(V),ye=m(V,"UL",{"data-svelte-h":!0}),_(ye)!=="svelte-1i9daf3"&&(ye.innerHTML=Ot),V.forEach(t),We=s(e),R=m(e,"DIV",{class:!0});var he=z(R);w(Q.$$.fragment,he),At=s(he),xe=m(he,"P",{"data-svelte-h":!0}),_(xe)!=="svelte-1oh310h"&&(xe.textContent=Qt),he.forEach(t),Ne=s(e),E=m(e,"DIV",{class:!0});var st=z(E);w(K.$$.fragment,st),Ut=s(st),Me=m(st,"P",{"data-svelte-h":!0}),_(Me)!=="svelte-cadr4a"&&(Me.textContent=Kt),st.forEach(t),qe=s(e),k=m(e,"DIV",{class:!0});var ot=z(k);w(ee.$$.fragment,ot),Rt=s(ot),Te=m(ot,"P",{"data-svelte-h":!0}),_(Te)!=="svelte-1lj8vcj"&&(Te.textContent=er),ot.forEach(t),Pe=s(e),te=m(e,"IMG",{alt:!0,src:!0}),Be=s(e),Z=m(e,"DIV",{class:!0});var it=z(Z);w(re.$$.fragment,it),Et=s(it),ze=m(it,"P",{"data-svelte-h":!0}),_(ze)!=="svelte-1lhkhz0"&&(ze.textContent=rr),it.forEach(t),Ge=s(e),ne=m(e,"IMG",{alt:!0,src:!0}),Xe=s(e),J=m(e,"DIV",{class:!0});var lt=z(J);w(ae.$$.fragment,lt),kt=s(lt),Ce=m(lt,"P",{"data-svelte-h":!0}),_(Ce)!=="svelte-1pclsvt"&&(Ce.textContent=ar),lt.forEach(t),Ye=s(e),se=m(e,"IMG",{alt:!0,src:!0}),Oe=s(e),D=m(e,"DIV",{class:!0});var mt=z(D);w(oe.$$.fragment,mt),Zt=s(mt),je=m(mt,"P",{"data-svelte-h":!0}),_(je)!=="svelte-17q4ov5"&&(je.textContent=or),mt.forEach(t),Qe=s(e),ie=m(e,"IMG",{alt:!0,src:!0}),Ke=s(e),L=m(e,"DIV",{class:!0});var ke=z(L);w(le.$$.fragment,ke),Jt=s(ke),Le=m(ke,"P",{"data-svelte-h":!0}),_(Le)!=="svelte-1y55gjq"&&(Le.innerHTML=lr),Dt=s(ke),Ae=m(ke,"P",{"data-svelte-h":!0}),_(Ae)!=="svelte-1tn0l2l"&&(Ae.innerHTML=mr),ke.forEach(t),et=s(e),F=m(e,"DIV",{class:!0});var ct=z(F);w(me.$$.fragment,ct),Ft=s(ct),Ue=m(ct,"P",{"data-svelte-h":!0}),_(Ue)!=="svelte-yp2yn6"&&(Ue.textContent=cr),ct.forEach(t),tt=s(e),A=m(e,"DIV",{class:!0});var Ze=z(A);w(ce.$$.fragment,Ze),Vt=s(Ze),Re=m(Ze,"P",{"data-svelte-h":!0}),_(Re)!=="svelte-1g5u0qv"&&(Re.textContent=pr),Ht=s(Ze),Ee=m(Ze,"OL",{"data-svelte-h":!0}),_(Ee)!=="svelte-128o9uy"&&(Ee.innerHTML=dr),Ze.forEach(t),rt=s(e),w(pe.$$.fragment,e),nt=s(e),Je=m(e,"P",{}),z(Je).forEach(t),this.h()},h(){M(c,"name","hf:doc:metadata"),M(c,"content",Mr),M(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(te,"alt",""),pt(te.src,tr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||M(te,"src",tr),M(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ne,"alt",""),pt(ne.src,nr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||M(ne,"src",nr),M(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(se,"alt",""),pt(se.src,sr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||M(se,"src",sr),M(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(ie,"alt",""),pt(ie.src,ir="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||M(ie,"src",ir),M(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),M(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){i(document.head,c),o(e,T,n),o(e,f,n),o(e,p,n),b(h,e,n),o(e,r,n),o(e,x,n),o(e,De,n),o(e,q,n),o(e,Fe,n),b(P,e,n),o(e,Ve,n),o(e,d,n),b(B,d,null),i(d,ft),i(d,ue),i(d,gt),i(d,_e),i(d,wt),i(d,fe),i(d,bt),i(d,ge),i(d,vt),i(d,we),i(d,$t),b(H,d,null),i(d,yt),b(I,d,null),i(d,xt),i(d,be),i(d,Mt),b(S,d,null),i(d,Tt),b(W,d,null),i(d,zt),i(d,N),b(G,N,null),i(N,Ct),i(N,ve),o(e,He,n),b(X,e,n),o(e,Ie,n),b(Y,e,n),o(e,Se,n),o(e,j,n),b(O,j,null),i(j,jt),i(j,$e),i(j,Lt),i(j,ye),o(e,We,n),o(e,R,n),b(Q,R,null),i(R,At),i(R,xe),o(e,Ne,n),o(e,E,n),b(K,E,null),i(E,Ut),i(E,Me),o(e,qe,n),o(e,k,n),b(ee,k,null),i(k,Rt),i(k,Te),o(e,Pe,n),o(e,te,n),o(e,Be,n),o(e,Z,n),b(re,Z,null),i(Z,Et),i(Z,ze),o(e,Ge,n),o(e,ne,n),o(e,Xe,n),o(e,J,n),b(ae,J,null),i(J,kt),i(J,Ce),o(e,Ye,n),o(e,se,n),o(e,Oe,n),o(e,D,n),b(oe,D,null),i(D,Zt),i(D,je),o(e,Qe,n),o(e,ie,n),o(e,Ke,n),o(e,L,n),b(le,L,null),i(L,Jt),i(L,Le),i(L,Dt),i(L,Ae),o(e,et,n),o(e,F,n),b(me,F,null),i(F,Ft),i(F,Ue),o(e,tt,n),o(e,A,n),b(ce,A,null),i(A,Vt),i(A,Re),i(A,Ht),i(A,Ee),o(e,rt,n),b(pe,e,n),o(e,nt,n),o(e,Je,n),at=!0},p(e,[n]){const u={};n&2&&(u.$$scope={dirty:n,ctx:e}),H.$set(u);const de={};n&2&&(de.$$scope={dirty:n,ctx:e}),I.$set(de);const V={};n&2&&(V.$$scope={dirty:n,ctx:e}),S.$set(V);const he={};n&2&&(he.$$scope={dirty:n,ctx:e}),W.$set(he)},i(e){at||(v(h.$$.fragment,e),v(P.$$.fragment,e),v(B.$$.fragment,e),v(H.$$.fragment,e),v(I.$$.fragment,e),v(S.$$.fragment,e),v(W.$$.fragment,e),v(G.$$.fragment,e),v(X.$$.fragment,e),v(Y.$$.fragment,e),v(O.$$.fragment,e),v(Q.$$.fragment,e),v(K.$$.fragment,e),v(ee.$$.fragment,e),v(re.$$.fragment,e),v(ae.$$.fragment,e),v(oe.$$.fragment,e),v(le.$$.fragment,e),v(me.$$.fragment,e),v(ce.$$.fragment,e),v(pe.$$.fragment,e),at=!0)},o(e){$(h.$$.fragment,e),$(P.$$.fragment,e),$(B.$$.fragment,e),$(H.$$.fragment,e),$(I.$$.fragment,e),$(S.$$.fragment,e),$(W.$$.fragment,e),$(G.$$.fragment,e),$(X.$$.fragment,e),$(Y.$$.fragment,e),$(O.$$.fragment,e),$(Q.$$.fragment,e),$(K.$$.fragment,e),$(ee.$$.fragment,e),$(re.$$.fragment,e),$(ae.$$.fragment,e),$(oe.$$.fragment,e),$(le.$$.fragment,e),$(me.$$.fragment,e),$(ce.$$.fragment,e),$(pe.$$.fragment,e),at=!1},d(e){e&&(t(T),t(f),t(p),t(r),t(x),t(De),t(q),t(Fe),t(Ve),t(d),t(He),t(Ie),t(Se),t(j),t(We),t(R),t(Ne),t(E),t(qe),t(k),t(Pe),t(te),t(Be),t(Z),t(Ge),t(ne),t(Xe),t(J),t(Ye),t(se),t(Oe),t(D),t(Qe),t(ie),t(Ke),t(L),t(et),t(F),t(tt),t(A),t(rt),t(nt),t(Je)),t(c),y(h,e),y(P,e),y(B),y(H),y(I),y(S),y(W),y(G),y(X,e),y(Y,e),y(O),y(Q),y(K),y(ee),y(re),y(ae),y(oe),y(le),y(me),y(ce),y(pe,e)}}}const Mr='{"title":"Optimization","local":"optimization","sections":[{"title":"AdaFactor","local":"transformers.Adafactor","sections":[],"depth":2},{"title":"Schedules","local":"schedules","sections":[{"title":"Learning Rate Schedules","local":"transformers.SchedulerType","sections":[],"depth":3}],"depth":2}],"depth":1}';function Tr(U){return ur(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rr extends _r{constructor(c){super(),fr(this,c,Tr,xr,hr,{})}}export{Rr as component};
