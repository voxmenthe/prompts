import{s as Ke,o as et,n as tt}from"../chunks/scheduler.18a86fab.js";import{S as nt,i as ot,g as i,s as o,r as h,A as st,h as l,f as n,c as s,j as M,x as m,u as f,k as v,y as r,a,v as g,d as u,t as _,w as k}from"../chunks/index.98837b22.js";import{T as rt}from"../chunks/Tip.77304350.js";import{D as oe}from"../chunks/Docstring.a1ef7999.js";import{C as at}from"../chunks/CodeBlock.8d0c2e8a.js";import{H as xe,E as it}from"../chunks/getInferenceSnippets.06c2775f.js";function lt(re){let p,C=`PhoBERT implementation is the same as BERT, except for tokenization. Refer to <a href="bert">BERT documentation</a> for information on
configuration classes and their parameters. PhoBERT-specific tokenizer is documented below.`;return{c(){p=i("p"),p.innerHTML=C},l(b){p=l(b,"P",{"data-svelte-h":!0}),m(p)!=="svelte-9aqf1x"&&(p.innerHTML=C)},m(b,S){a(b,p,S)},p:tt,d(b){b&&n(p)}}}function dt(re){let p,C,b,S,z,De="<em>This model was released on 2020-03-02 and added to Hugging Face Transformers on 2020-11-16.</em>",ae,J,ie,$,je='<img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white"/>',le,E,de,U,He='The PhoBERT model was proposed in <a href="https://huggingface.co/papers/2003.00744" rel="nofollow">PhoBERT: Pre-trained language models for Vietnamese</a> by Dat Quoc Nguyen, Anh Tuan Nguyen.',ce,V,Ne="The abstract from the paper is the following:",pe,I,Qe=`<em>We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.</em>`,me,B,Fe='This model was contributed by <a href="https://huggingface.co/dqnguyen" rel="nofollow">dqnguyen</a>. The original code can be found <a href="https://github.com/VinAIResearch/PhoBERT" rel="nofollow">here</a>.',he,R,fe,L,ge,w,ue,D,_e,d,j,Pe,A,Ge="Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding.",qe,W,Xe=`This tokenizer inherits from <a href="/docs/transformers/v4.56.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Me,y,H,Ce,Z,Se="Loads a pre-existing dictionary from a text file and adds its symbols to this instance.",ze,T,N,Je,O,Ae=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`,Ee,Y,We="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Ue,x,Q,Ve,K,Ze="Converts a sequence of tokens (string) in a single string.",Ie,P,F,Be,ee,Oe=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`,Re,q,G,Le,te,Ye=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,ke,X,be,se,Te;return J=new xe({props:{title:"PhoBERT",local:"phobert",headingTag:"h1"}}),E=new xe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),R=new xe({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),L=new at({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXBob2JlcnQlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGcGhvYmVydC1iYXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGcGhvYmVydC1iYXNlJTIyKSUwQSUwQSUyMyUyMElOUFVUJTIwVEVYVCUyME1VU1QlMjBCRSUyMEFMUkVBRFklMjBXT1JELVNFR01FTlRFRCElMEFsaW5lJTIwJTNEJTIwJTIyVCVDMyVCNGklMjBsJUMzJUEwJTIwc2luaF92aSVDMyVBQW4lMjB0ciVDNiVCMCVFMSVCQiU5RG5nJTIwJUM0JTkxJUUxJUJBJUExaV9oJUUxJUJCJThEYyUyMEMlQzMlQjRuZ19uZ2glRTElQkIlODclMjAuJTIyJTBBJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QnRva2VuaXplci5lbmNvZGUobGluZSklNUQpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGZlYXR1cmVzJTIwJTNEJTIwcGhvYmVydChpbnB1dF9pZHMpJTIwJTIwJTIzJTIwTW9kZWxzJTIwb3V0cHV0cyUyMGFyZSUyMG5vdyUyMHR1cGxlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;Tôi là sinh_viên trường đại_học Công_nghệ .&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>`,wrap:!1}}),w=new rt({props:{$$slots:{default:[lt]},$$scope:{ctx:re}}}),D=new xe({props:{title:"PhobertTokenizer",local:"transformers.PhobertTokenizer",headingTag:"h2"}}),j=new oe({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.PhobertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.PhobertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.PhobertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.PhobertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.PhobertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PhobertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L51"}}),H=new oe({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L327"}}),N=new oe({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L146",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),Q=new oe({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L293"}}),F=new oe({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L200",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),G=new oe({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": list"},{name:"token_ids_1",val:": typing.Optional[list[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>list[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>list[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.56.2/src/transformers/models/phobert/tokenization_phobert.py#L172",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>list[int]</code></p>
`}}),X=new it({props:{source:"https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phobert.md"}}),{c(){p=i("meta"),C=o(),b=i("p"),S=o(),z=i("p"),z.innerHTML=De,ae=o(),h(J.$$.fragment),ie=o(),$=i("div"),$.innerHTML=je,le=o(),h(E.$$.fragment),de=o(),U=i("p"),U.innerHTML=He,ce=o(),V=i("p"),V.textContent=Ne,pe=o(),I=i("p"),I.innerHTML=Qe,me=o(),B=i("p"),B.innerHTML=Fe,he=o(),h(R.$$.fragment),fe=o(),h(L.$$.fragment),ge=o(),h(w.$$.fragment),ue=o(),h(D.$$.fragment),_e=o(),d=i("div"),h(j.$$.fragment),Pe=o(),A=i("p"),A.textContent=Ge,qe=o(),W=i("p"),W.innerHTML=Xe,Me=o(),y=i("div"),h(H.$$.fragment),Ce=o(),Z=i("p"),Z.textContent=Se,ze=o(),T=i("div"),h(N.$$.fragment),Je=o(),O=i("p"),O.textContent=Ae,Ee=o(),Y=i("ul"),Y.innerHTML=We,Ue=o(),x=i("div"),h(Q.$$.fragment),Ve=o(),K=i("p"),K.textContent=Ze,Ie=o(),P=i("div"),h(F.$$.fragment),Be=o(),ee=i("p"),ee.textContent=Oe,Re=o(),q=i("div"),h(G.$$.fragment),Le=o(),te=i("p"),te.innerHTML=Ye,ke=o(),h(X.$$.fragment),be=o(),se=i("p"),this.h()},l(e){const t=st("svelte-u9bgzb",document.head);p=l(t,"META",{name:!0,content:!0}),t.forEach(n),C=s(e),b=l(e,"P",{}),M(b).forEach(n),S=s(e),z=l(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-19qg5y0"&&(z.innerHTML=De),ae=s(e),f(J.$$.fragment,e),ie=s(e),$=l(e,"DIV",{class:!0,"data-svelte-h":!0}),m($)!=="svelte-13t8s2t"&&($.innerHTML=je),le=s(e),f(E.$$.fragment,e),de=s(e),U=l(e,"P",{"data-svelte-h":!0}),m(U)!=="svelte-1litcgl"&&(U.innerHTML=He),ce=s(e),V=l(e,"P",{"data-svelte-h":!0}),m(V)!=="svelte-vfdo9a"&&(V.textContent=Ne),pe=s(e),I=l(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-pnlemp"&&(I.innerHTML=Qe),me=s(e),B=l(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-cbf0gj"&&(B.innerHTML=Fe),he=s(e),f(R.$$.fragment,e),fe=s(e),f(L.$$.fragment,e),ge=s(e),f(w.$$.fragment,e),ue=s(e),f(D.$$.fragment,e),_e=s(e),d=l(e,"DIV",{class:!0});var c=M(d);f(j.$$.fragment,c),Pe=s(c),A=l(c,"P",{"data-svelte-h":!0}),m(A)!=="svelte-f1kmpq"&&(A.textContent=Ge),qe=s(c),W=l(c,"P",{"data-svelte-h":!0}),m(W)!=="svelte-ntrhio"&&(W.innerHTML=Xe),Me=s(c),y=l(c,"DIV",{class:!0});var ve=M(y);f(H.$$.fragment,ve),Ce=s(ve),Z=l(ve,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-ooaeix"&&(Z.textContent=Se),ve.forEach(n),ze=s(c),T=l(c,"DIV",{class:!0});var ne=M(T);f(N.$$.fragment,ne),Je=s(ne),O=l(ne,"P",{"data-svelte-h":!0}),m(O)!=="svelte-dhtv7"&&(O.textContent=Ae),Ee=s(ne),Y=l(ne,"UL",{"data-svelte-h":!0}),m(Y)!=="svelte-rq8uot"&&(Y.innerHTML=We),ne.forEach(n),Ue=s(c),x=l(c,"DIV",{class:!0});var $e=M(x);f(Q.$$.fragment,$e),Ve=s($e),K=l($e,"P",{"data-svelte-h":!0}),m(K)!=="svelte-b3k2yi"&&(K.textContent=Ze),$e.forEach(n),Ie=s(c),P=l(c,"DIV",{class:!0});var we=M(P);f(F.$$.fragment,we),Be=s(we),ee=l(we,"P",{"data-svelte-h":!0}),m(ee)!=="svelte-qg6q9n"&&(ee.textContent=Oe),we.forEach(n),Re=s(c),q=l(c,"DIV",{class:!0});var ye=M(q);f(G.$$.fragment,ye),Le=s(ye),te=l(ye,"P",{"data-svelte-h":!0}),m(te)!=="svelte-1f4f5kp"&&(te.innerHTML=Ye),ye.forEach(n),c.forEach(n),ke=s(e),f(X.$$.fragment,e),be=s(e),se=l(e,"P",{}),M(se).forEach(n),this.h()},h(){v(p,"name","hf:doc:metadata"),v(p,"content",ct),v($,"class","flex flex-wrap space-x-1"),v(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){r(document.head,p),a(e,C,t),a(e,b,t),a(e,S,t),a(e,z,t),a(e,ae,t),g(J,e,t),a(e,ie,t),a(e,$,t),a(e,le,t),g(E,e,t),a(e,de,t),a(e,U,t),a(e,ce,t),a(e,V,t),a(e,pe,t),a(e,I,t),a(e,me,t),a(e,B,t),a(e,he,t),g(R,e,t),a(e,fe,t),g(L,e,t),a(e,ge,t),g(w,e,t),a(e,ue,t),g(D,e,t),a(e,_e,t),a(e,d,t),g(j,d,null),r(d,Pe),r(d,A),r(d,qe),r(d,W),r(d,Me),r(d,y),g(H,y,null),r(y,Ce),r(y,Z),r(d,ze),r(d,T),g(N,T,null),r(T,Je),r(T,O),r(T,Ee),r(T,Y),r(d,Ue),r(d,x),g(Q,x,null),r(x,Ve),r(x,K),r(d,Ie),r(d,P),g(F,P,null),r(P,Be),r(P,ee),r(d,Re),r(d,q),g(G,q,null),r(q,Le),r(q,te),a(e,ke,t),g(X,e,t),a(e,be,t),a(e,se,t),Te=!0},p(e,[t]){const c={};t&2&&(c.$$scope={dirty:t,ctx:e}),w.$set(c)},i(e){Te||(u(J.$$.fragment,e),u(E.$$.fragment,e),u(R.$$.fragment,e),u(L.$$.fragment,e),u(w.$$.fragment,e),u(D.$$.fragment,e),u(j.$$.fragment,e),u(H.$$.fragment,e),u(N.$$.fragment,e),u(Q.$$.fragment,e),u(F.$$.fragment,e),u(G.$$.fragment,e),u(X.$$.fragment,e),Te=!0)},o(e){_(J.$$.fragment,e),_(E.$$.fragment,e),_(R.$$.fragment,e),_(L.$$.fragment,e),_(w.$$.fragment,e),_(D.$$.fragment,e),_(j.$$.fragment,e),_(H.$$.fragment,e),_(N.$$.fragment,e),_(Q.$$.fragment,e),_(F.$$.fragment,e),_(G.$$.fragment,e),_(X.$$.fragment,e),Te=!1},d(e){e&&(n(C),n(b),n(S),n(z),n(ae),n(ie),n($),n(le),n(de),n(U),n(ce),n(V),n(pe),n(I),n(me),n(B),n(he),n(fe),n(ge),n(ue),n(_e),n(d),n(ke),n(be),n(se)),n(p),k(J,e),k(E,e),k(R,e),k(L,e),k(w,e),k(D,e),k(j),k(H),k(N),k(Q),k(F),k(G),k(X,e)}}}const ct='{"title":"PhoBERT","local":"phobert","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"PhobertTokenizer","local":"transformers.PhobertTokenizer","sections":[],"depth":2}],"depth":1}';function pt(re){return et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kt extends nt{constructor(p){super(),ot(this,p,pt,dt,Ke,{})}}export{kt as component};
