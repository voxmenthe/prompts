<!-- Source: https://docs.anthropic.com/en/api/retrieving-message-batch-results -->

# Retrieve Message Batch results

get/v1/messages/batches/{message_batch_id}/results

Streams the results of a Message Batch as a `.jsonl` file.

Each line in the file is a JSON object containing the result of a single request in the Message Batch. Results are not guaranteed to be in the same order as requests. Use the `custom_id` field to match results to requests.

Learn more about the Message Batches API in our [user guide](<https://docs.claude.com/en/docs/build-with-claude/batch-processing>)

##### Path ParametersExpand Collapse 

message_batch_id: string

ID of the Message Batch.

##### ReturnsExpand Collapse 

MessageBatchIndividualResponse = object { custom_id, result } 

This is a single line in the response `.jsonl` file and does not represent the response as a whole.

custom_id: string

Developer-provided ID created for each request in a Message Batch. Useful for matching results to requests, as results may be given out of request order.

Must be unique for each request within the Message Batch.

result: [MessageBatchResult](</docs/en/api/messages#message_batch_result>)

Processing result for this request.

Contains a Message output if processing was successful, an error response if processing failed, or the reason why processing was not attempted, such as cancellation or expiration.

Accepts one of the following:

MessageBatchSucceededResult = object { message, type } 

message: [Message](</docs/en/api/messages#message>) { id, content, model, 5 more } 

id: string

Unique object identifier.

The format and length of IDs may change over time.

content: array of [ContentBlock](</docs/en/api/messages#content_block>)

Content generated by the model.

This is an array of content blocks, each of which has a `type` that determines its shape.

Example:

"type": "text", "text": "Hi, I'm Claude."}] `
[/code]

If the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.

For example, if the input `messages` were:

{"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"}, {"role": "assistant", "content": "The best answer is ("} ] `
[/code]

Then the response `content` might be:

"type": "text", "text": "B)"}] `
[/code]

Accepts one of the following:

TextBlock = object { citations, text, type } 

citations: array of [TextCitation](</docs/en/api/messages#text_citation>)

Citations supporting the text block.

The type of citation returned will depend on the type of document being cited. Citing a PDF results in `page_location`, plain text results in `char_location`, and content document results in `content_block_location`.

Accepts one of the following:

CitationCharLocation = object { cited_text, document_index, document_title, 4 more } 

cited_text: string

document_index: number

document_title: string

end_char_index: number

file_id: string

start_char_index: number

type: "char_location"

Accepts one of the following:

"char_location"

CitationPageLocation = object { cited_text, document_index, document_title, 4 more } 

cited_text: string

document_index: number

document_title: string

end_page_number: number

file_id: string

start_page_number: number

type: "page_location"

Accepts one of the following:

"page_location"

CitationContentBlockLocation = object { cited_text, document_index, document_title, 4 more } 

cited_text: string

document_index: number

document_title: string

end_block_index: number

file_id: string

start_block_index: number

type: "content_block_location"

Accepts one of the following:

"content_block_location"

CitationsWebSearchResultLocation = object { cited_text, encrypted_index, title, 2 more } 

cited_text: string

encrypted_index: string

title: string

type: "web_search_result_location"

Accepts one of the following:

"web_search_result_location"

url: string

CitationsSearchResultLocation = object { cited_text, end_block_index, search_result_index, 4 more } 

cited_text: string

end_block_index: number

search_result_index: number

source: string

start_block_index: number

title: string

type: "search_result_location"

Accepts one of the following:

"search_result_location"

text: string

type: "text"

Accepts one of the following:

"text"

ThinkingBlock = object { signature, thinking, type } 

signature: string

thinking: string

type: "thinking"

Accepts one of the following:

"thinking"

RedactedThinkingBlock = object { data, type } 

data: string

type: "redacted_thinking"

Accepts one of the following:

"redacted_thinking"

ToolUseBlock = object { id, input, name, type } 

id: string

input: map[unknown]

name: string

type: "tool_use"

Accepts one of the following:

"tool_use"

ServerToolUseBlock = object { id, input, name, type } 

id: string

input: map[unknown]

name: "web_search"

Accepts one of the following:

"web_search"

type: "server_tool_use"

Accepts one of the following:

"server_tool_use"

WebSearchToolResultBlock = object { content, tool_use_id, type } 

content: [WebSearchToolResultBlockContent](</docs/en/api/messages#web_search_tool_result_block_content>)

Accepts one of the following:

WebSearchToolResultError = object { error_code, type } 

error_code: "invalid_tool_input" or "unavailable" or "max_uses_exceeded" or 2 more

Accepts one of the following:

"invalid_tool_input"

"unavailable"

"max_uses_exceeded"

"too_many_requests"

"query_too_long"

type: "web_search_tool_result_error"

Accepts one of the following:

"web_search_tool_result_error"

UnionMember1 = array of [WebSearchResultBlock](</docs/en/api/messages#web_search_result_block>) { encrypted_content, page_age, title, 2 more } 

encrypted_content: string

page_age: string

title: string

type: "web_search_result"

Accepts one of the following:

"web_search_result"

url: string

tool_use_id: string

type: "web_search_tool_result"

Accepts one of the following:

"web_search_tool_result"

model: [Model](</docs/en/api/messages#model>)

The model that will complete your prompt.

See [models](<https://docs.anthropic.com/en/docs/models-overview>) for additional details and options.

Accepts one of the following:

UnionMember0 = "claude-opus-4-5-20251101" or "claude-opus-4-5" or "claude-3-7-sonnet-latest" or 17 more

The model that will complete your prompt.

See [models](<https://docs.anthropic.com/en/docs/models-overview>) for additional details and options.

Accepts one of the following:

"claude-opus-4-5-20251101"

Premium model combining maximum intelligence with practical performance

"claude-opus-4-5"

Premium model combining maximum intelligence with practical performance

"claude-3-7-sonnet-latest"

High-performance model with early extended thinking

"claude-3-7-sonnet-20250219"

High-performance model with early extended thinking

"claude-3-5-haiku-latest"

Fastest and most compact model for near-instant responsiveness

"claude-3-5-haiku-20241022"

Our fastest model

"claude-haiku-4-5"

Hybrid model, capable of near-instant responses and extended thinking

"claude-haiku-4-5-20251001"

Hybrid model, capable of near-instant responses and extended thinking

"claude-sonnet-4-20250514"

High-performance model with extended thinking

"claude-sonnet-4-0"

High-performance model with extended thinking

"claude-4-sonnet-20250514"

High-performance model with extended thinking

"claude-sonnet-4-5"

Our best model for real-world agents and coding

"claude-sonnet-4-5-20250929"

Our best model for real-world agents and coding

"claude-opus-4-0"

Our most capable model

"claude-opus-4-20250514"

Our most capable model

"claude-4-opus-20250514"

Our most capable model

"claude-opus-4-1-20250805"

Our most capable model

"claude-3-opus-latest"

Excels at writing and complex tasks

"claude-3-opus-20240229"

Excels at writing and complex tasks

"claude-3-haiku-20240307"

Our previous most fast and cost-effective

UnionMember1 = string

role: "assistant"

Conversational role of the generated message.

This will always be `"assistant"`.

Accepts one of the following:

"assistant"

stop_reason: [StopReason](</docs/en/api/messages#stop_reason>)

The reason that we stopped.

This may be one the following values:

  * `"end_turn"`: the model reached a natural stopping point
  * `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
  * `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
  * `"tool_use"`: the model invoked one or more tools
  * `"pause_turn"`: we paused a long-running turn. You may provide the response back as-is in a subsequent request to let the model continue.
  * `"refusal"`: when streaming classifiers intervene to handle potential policy violations

In non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.

Accepts one of the following:

"end_turn"

"max_tokens"

"stop_sequence"

"tool_use"

"pause_turn"

"refusal"

stop_sequence: string

Which custom stop sequence was generated, if any.

This value will be a non-null string if one of your custom stop sequences was generated.

type: "message"

Object type.

For Messages, this is always `"message"`.

Accepts one of the following:

"message"

usage: [Usage](</docs/en/api/messages#usage>) { cache_creation, cache_creation_input_tokens, cache_read_input_tokens, 4 more } 

Billing and rate-limit usage.

Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.

Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.

For example, `output_tokens` will be non-zero, even for an empty string response from Claude.

Total input tokens in a request is the summation of `input_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.

cache_creation: [CacheCreation](</docs/en/api/messages#cache_creation>) { ephemeral_1h_input_tokens, ephemeral_5m_input_tokens } 

Breakdown of cached tokens by TTL

ephemeral_1h_input_tokens: number

The number of input tokens used to create the 1 hour cache entry.

minimum0

ephemeral_5m_input_tokens: number

The number of input tokens used to create the 5 minute cache entry.

minimum0

cache_creation_input_tokens: number

The number of input tokens used to create the cache entry.

minimum0

cache_read_input_tokens: number

The number of input tokens read from the cache.

minimum0

input_tokens: number

The number of input tokens which were used.

minimum0

output_tokens: number

The number of output tokens which were used.

minimum0

server_tool_use: [ServerToolUsage](</docs/en/api/messages#server_tool_usage>) { web_search_requests } 

The number of server tool requests.

web_search_requests: number

The number of web search tool requests.

minimum0

service_tier: "standard" or "priority" or "batch"

If the request used the priority, standard, or batch tier.

Accepts one of the following:

"standard"

"priority"

"batch"

type: "succeeded"

Accepts one of the following:

"succeeded"

MessageBatchErroredResult = object { error, type } 

error: [ErrorResponse](</docs/en/api/$shared#error_response>) { error, request_id, type } 

error: [ErrorObject](</docs/en/api/$shared#error_object>)

Accepts one of the following:

InvalidRequestError = object { message, type } 

message: string

type: "invalid_request_error"

Accepts one of the following:

"invalid_request_error"

AuthenticationError = object { message, type } 

message: string

type: "authentication_error"

Accepts one of the following:

"authentication_error"

BillingError = object { message, type } 

message: string

type: "billing_error"

Accepts one of the following:

"billing_error"

PermissionError = object { message, type } 

message: string

type: "permission_error"

Accepts one of the following:

"permission_error"

NotFoundError = object { message, type } 

message: string

type: "not_found_error"

Accepts one of the following:

"not_found_error"

RateLimitError = object { message, type } 

message: string

type: "rate_limit_error"

Accepts one of the following:

"rate_limit_error"

GatewayTimeoutError = object { message, type } 

message: string

type: "timeout_error"

Accepts one of the following:

"timeout_error"

APIErrorObject = object { message, type } 

message: string

type: "api_error"

Accepts one of the following:

"api_error"

OverloadedError = object { message, type } 

message: string

type: "overloaded_error"

Accepts one of the following:

"overloaded_error"

request_id: string

type: "error"

Accepts one of the following:

"error"

type: "errored"

Accepts one of the following:

"errored"

MessageBatchCanceledResult = object { type } 

type: "canceled"

Accepts one of the following:

"canceled"

MessageBatchExpiredResult = object { type } 

type: "expired"

Accepts one of the following:

"expired"

Retrieve Message Batch results
[code]
    curl https://api.anthropic.com/v1/messages/batches/$MESSAGE_BATCH_ID/results \
        -H "X-Api-Key: $ANTHROPIC_API_KEY"
[/code]

##### Returns Examples